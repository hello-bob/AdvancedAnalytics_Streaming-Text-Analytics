{"aid": "40206821", "title": "5k word essay on understanding tokens and embeddings by jaju", "url": "https://msync.org/notes/llm-understanding-tokens-embeddings/", "domain": "msync.org", "votes": 1, "user": "bosky101", "posted_at": "2024-04-30 03:04:49", "comments": 0, "source_title": "LLMs: Understanding Tokens and Embeddings", "source_text": "LLMs: Understanding Tokens and Embeddings | Sync'ing from Memory\n\nSync'ing from Memory\n\n# LLMs: Understanding Tokens and Embeddings\n\nSat, 13 Apr, 2024 Updated: Mon, 29 Apr, 2024\n\n## The Background Story\n\n### Enabling Computers to Read\n\nIt\u2019s well understood that machine learning systems do not deal with text\ndirectly. Or for that matter, images, audio and video.\n\nFor text, we first convert them to numbers by some mechanism before we feed\nthem into our machine learning systems.\n\nThe strategies for converting into numbers exist in various forms. And\nchoosing one over the other depends entirely on the architecture of the\nconsuming system.\n\nUntil the deep-learning approaches to working with text came onto the scene,\nwe mostly dealt with words and converted words into numeric identifiers. Words\nare a group of characters delimited by spaces and punctuations. Something we\nare all familiar with. As we will see further, it\u2019s not necessary that we\nsegment text only at spaces and punctuations. We can even break down a single\nword into many units - these units are what we call as tokens.\n\n> Tokens are the fundamental unit we deal with as one of the outcomes of the\n> text preprocessing, and they are further mapped to numeric identifiers. Each\n> token maps to a unique numeric identifier.\n\nOnce the identifiers are associated with a token, dealing with them is as good\nas dealing with their associated tokens. Any statistical processing over\ntokens (like, their counts, or co-occurences) can be done by dealing with\ntheir associated identifiers.\n\nWords are natural candidates for processing as tokens because they are first-\nclass elements of our natural language. But tokens are more general - a single\nword represented as consisting of multiple tokens is perfectly normal. And a\ntoken can even span multiple words, not necessarily starting and ending at\nword boundaries. (This - the latter part - may not make complete sense just\nyet.)\n\nWe store the words to ids relationships in a table. Within the context of a\ngiven system, we are required to be consistent in using this mapping table\nthroughout.\n\n### But, Problems...\n\nUnlike the ideal world, the real world can throw a lot of surprises. Let\u2019s\nsay, we use the identifier 1048 for the word dog.\n\nBut, dog at the beginning of a sentence will appear as Dog. Are they the same\nwords?\n\nAlso, consider the possibility of mistyping by a human wherein the word was\ntyped as DOg. We humans will be able to process it, but the computer is\nunlikely to be able to deal with it. Unless, we treat all words in a case-\ninsensitive manner by down/up-casing everything.\n\nThat\u2019s great, you say, but what about Dawg? Oh, well. That\u2019s a spelling\nmistake and we can\u2019t handle it.\n\ntoken| id  \n---|---  \ndog| 1048  \nDog| ??  \nDOg| ??  \nDawg| ??  \n  \nThen, there are other problems too. What about words that don\u2019t exist in our\ndictionary? If we haven\u2019t seen a word, we\u2019d have no identifiers for it. Dawg\nis only one example, and it is a possible speeleeng myshtake. But new words\nget introduced into a language all the time.\n\n### Dealing with the Problems\n\nWhat if we process text by breaking it down into characters!?\n\nSo, how about splitting Dawg into four tokens - D, a, w, g.\n\nWe thus convert all text into character-level tokens and then, deal with their\nnumeric identifiers.\n\nSounds insane, and maybe it is! Until, we throw huge amounts of compute power\nat our problem. Given a very very large scale processing infrastructure, we\ncan indeed compute language level statistics to such level of detail that we\nmight be able to re-construct a language from statistical priors of just the\ncharacters!\n\nDon\u2019t believe? Go check Andrej Karpathy\u2019s demonstrating how to do this from\nscratch here.\n\nLLMs compute the probabilities of long sequences of characters. During\ntraining, they compute and remember the distribution from the training data.\nDuring generation, they predict - based on what they have learnt.\nConceptually, it\u2019s that simple!\n\nAssume a sequence length of 100 characters, and 26 characters, assuming lower-\ncase alphabets without numbers. That makes possible combinations - surely, an\noversimplification. And sure enough, all characters are not independently\nrandom. So, given enough training data, there is a great chance that a large\nenough computer can calculate prior probabilities and then use those to\npredict next character tokens when we input a new piece of text from the same\ndistribution.\n\nIn reality, we do wish to optimize here. On the one hand, working with\ncharacter tokens is very inefficient. On the other hand, working with entire\nwords - those space separated molecules of characters - troubles us with two\nproblems\n\n  * A very large dictionary covering the entire natural language\n  * An inability to deal with words never seen before - either new words or spelling mistakes.\n\nWe\u2019ll go into tokenization next, but before that, a few items of note.\n\n## Quick Supporting Glossary\n\nPython\n\n    Needs no explanation. Most of ML you do these days are accessible easily through Python\nNumpy\n\n    A vast Python library for numerical processing. ML is nothing if not numerical processing\nHuggingface\n\n    \n\nA company, a community, to discover and share models, data, information.\nMaybe, the Github of the ML world!? Most open models and datasets can be found\nhere, including those that common folks like us can publish. Loosely, the\nDocker Hub equivalent for ML.\n\n  * Transformers: In the context of Huggingface, a library they have published for generally dealing with the various models published on the hub.\n\nModel Card\n\n    A card detailing important information of models. Example - the Llama 3 model card.\nData Card\n\n    Likewise, for published datasets.\n\nWhen we refer to a model using an <org-id/model-name> convention - example,\nmistralai/Mistral-7B-v0.1. This points to the organization mistralai\u2019s Mistral\n7B v0.1 model repository on Huggingface.\n\nModels are generally very large in size, and you do not want to download them\nagain. More often than not, libraries aware of Huggingface manage downloading\n(and caching) model files from Huggingface and it\u2019s only the first time you\naccess a model do you see output on your screens showing transfers in\nprogress. The second time onwards, locally cached data is used. Cached files\nare typically under $HOME/.cache/huggingface.\n\nIf you\u2019d like to manage your own downloads and caching, you can use the\nofficial Huggingface tools like\n\n  * huggingface_hub\n  * huggingface-cli\n\nWhile the model data will be automatically downloaded when you run the code,\nyou can download them ahead of time using huggingface-cli, as shown in an\nexample below.\n\n    \n    \n    huggingface-cli download meta-llama/Meta-Llama-3-8B\n\nFinally, for getting the code used in this article, you\u2019ll find links to\ndownload them at the end of this article. The variable names should be self-\nexplanatory. Refer to the linked code towards the end for more details,\nincluding the various import statements.\n\n### And a Few Steps\n\nTo work with the code, you need to take care of a few things\n\n  * Install Python\n\n    * Install Poetry\n    * Optional, but strongly recommended - a virtual Python environment manager like virtualenv.\n  * Download the pyproject.toml file into a directory\n\n    * [Optional] Create and activate a virtual environment for Python\n    * Run `poetry update` within that directory\n    * Keep all python code within this directory\n  * Create an account on Huggingface.\n\n    * Run `huggingface-cli login` and follow instructions\n  * On Huggingface, visit the mistralai/Mistral-7B-v0.1 and meta-llama/Meta-Llama-3-8B pages and register/sign agreement to use those models.\n\n## Tokenization\n\nLet\u2019s jump into tokenization.\n\nTokenization creates a vocabulary or a dictionary for encoding the input text\nfor further processing. Our choice ranges between using individual characters\nto using word tokens. Even beyond (length-wise), actually, but that doesn\u2019t\nmake much sense.\n\nTokenization is an important tunable part of the entire process. Tokenization\nchoices will deeply impact the sizes of the network, the training effort, and\nthe quality of the outcomes.\n\n  * Character-level tokens will keep your dictionary small. Word-level tokens will bloat your dictionary.\n  * With character tokens, you lose larger structures and patterns. With word tokens, you capture more of them\n  * Again, with character tokens, you are able to deal with every input never seen before. With word tokens, you will not be able to handle tokens/inputs never seen before.\n\nThe smart folks in this area have devised ways to keep the best of both\nworlds. Conceptually in a manner similar to Huffman encoding - the famous\ntechnique for compression - tokenization of input text is done, using\nvariable-length chunks of text based on their frequencies as tokens. Thus, we\nhave tokens that range from individual characters to those that can be\ncomplete words, and everything in between. This takes care of handling out-of-\nvocabulary words, while also capturing word-formation structures with the\nlonger tokens. And since we use the longest-match, the size of the vocabulary\ndoes not affect the correctness of the tokenization activity. Better yet, you\ncan choose the size of the vocabulary depending on your resource estimates.\n\nIt\u2019s also interesting to note that with more tokens, we capture further\nstructures of the language - how characters combine to form larger and larger\nstructures. These can indirectly encapsulate patterns that represent notions\nlike grammar, or how words form. These enable the downstream network to even\ndeal with new words not seen before, and even misspellings!\n\n### Exploring Tokenization in Code\n\nLet\u2019s look at how mistralai/Mistral-7B-v0.1 does tokenization.\n\nWe use the transformers library from Huggingface. It has some pretty nifty\nutilities for working with a variety of models with literally no configuration\nsteps. In particular, we will be using the two Auto* classes\n\n  * AutoTokenizer\n  * AutoModel\n\nThe model data contains everything - metadata, token vocabulary, architecture,\nlayers information, and more. So, using the Huggingface coordinates as strings\n(e.g., mistralai/Mistral-7B-v0.1) while invoking class methods on these above\nclasses is enough - the transformers library pretty much does eveything else\nfor the most common use-cases we are interested in.\n\nOn my machine, the model directory has the following files\n\n    \n    \n    -rw-r--r-- 1 jaju staff 596 Apr 8 10:43 config.json -rw-r--r-- 1 jaju staff 111 Apr 8 10:43 generation_config.json -rw-r--r-- 1 jaju staff 4943162336 Apr 8 10:52 model-00001-of-00003.safetensors -rw-r--r-- 1 jaju staff 4999819336 Apr 8 10:49 model-00002-of-00003.safetensors -rw-r--r-- 1 jaju staff 4540516344 Apr 8 10:51 model-00003-of-00003.safetensors -rw-r--r-- 1 jaju staff 25125 Apr 8 10:43 model.safetensors.index.json -rw-r--r-- 1 jaju staff 23950 Apr 8 10:43 pytorch_model.bin.index.json -rw-r--r-- 1 jaju staff 72 Apr 8 10:43 special_tokens_map.json -rw-r--r-- 1 jaju staff 1795303 Apr 8 10:43 tokenizer.json -rw-r--r-- 1 jaju staff 493443 Apr 8 10:43 tokenizer.model -rw-r--r-- 1 jaju staff 1460 Apr 8 10:43 tokenizer_config.json\n\nYou can inspect the tokenizer.json file for more insights. In fact, there are\nother plain-text JSON files too and I\u2019d recommend scanning them.\n\nLet\u2019s pass some sentences to the tokenizer and see how it is broken into\ntokens. The transformers library gives you a pretty nifty API to deal with\nthis processing, as you can see below. The tokenizer object is a function\nobject.\n\n    \n    \n    tokenizer = AutoTokenizer.from_pretrained(mistral_model_name) print(f\"The vocabulary size is {len(tokenizer)}\") inputs = tokenizer(\"Who is Pani Puri?\", return_tensors=\"pt\") input_ids = inputs[\"input_ids\"] print_tokens(tokenizer, input_ids) inputs = tokenizer(\"Who is Katy Perry?\", return_tensors=\"pt\") input_ids = inputs[\"input_ids\"] print_tokens(tokenizer, input_ids)\n    \n    \n    The vocabulary size is 32000 ============================== Tokens and their IDs ============================== Token | ID -----------+------------------ | 1 Who | 6526 is | 349 P | 367 ani | 4499 P | 367 uri | 6395 ? | 28804 ============================== ============================== Tokens and their IDs ============================== Token | ID -----------+------------------ | 1 Who | 6526 is | 349 Kat | 14294 y | 28724 Perry | 24150 ? | 28804 ==============================\n\nLook at the tokens closely. Familiar word-level tokens are only incidental -\ndepending on how Mistral chose the size of the vocabulary and the strategy,\nthe tokens have been generated. You can observe the token vocabulary size -\nit\u2019s a round number. And that was a choice made ahead of time. Also note the\nfirst token is empty space with id 1 - the tokenization strategy decides how\n(and if) to encode markers like a sentence/content start. The next model does\nit differently, as you\u2019ll see.\n\nLet\u2019s run the above with the meta-llama/Meta-Llama-3-8B as well, for a\ncomparison. What differences do you notice?\n\n    \n    \n    tokenizer = AutoTokenizer.from_pretrained(llama3_model_name) print(f\"The vocabulary size is {len(tokenizer)}\") inputs = tokenizer(\"Who is Pani Puri?\", return_tensors=\"pt\") input_ids = inputs[\"input_ids\"] print_tokens(tokenizer, input_ids) inputs = tokenizer(\"Who is Katy Perry?\", return_tensors=\"pt\") input_ids = inputs[\"input_ids\"] print_tokens(tokenizer, input_ids)\n    \n    \n    Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained. The vocabulary size is 128256 ============================== Tokens and their IDs ============================== Token | ID -----------+------------------ Who | 15546 is | 374 P | 393 ani | 5676 P | 393 uri | 6198 ? | 30 ============================== ============================== Tokens and their IDs ============================== Token | ID -----------+------------------ Who | 15546 is | 374 Katy | 73227 Perry | 31421 ? | 30 ==============================\n\nThe tokens are different. But Llama 3, given that it has a higher token\nvocabulary size - about 4 times - and encodes with less tokens the sentence\nwith words it has possibly seen before. But both tokenizers handle the\npossibly new words \u201cPani Puri\u201d without a problem.\n\nAlso note that each token that covers a beginning of a word has a leading\nspace. These minor-looking choices impact how the network looks at data and\nwhat it learns very significantly.\n\nWith a larger vocabulary, you can encode more in less. There\u2019s more signal for\nless number of tokens.\n\n## Embeddings\n\nEmbeddings are an interesting next level to get to.\n\nTokens are only a preprocessing step. There are too many of them. If we wish\nto work with tokens, our neural network will probably need as many neurons in\nthe input layer to process them. The simplest manner in which you can encode\nthese tokens are via one-hot encoding. That makes for very sparse vectors, and\nvery inefficient utilization of the many neurons at your service, given all\nexcept one would be idle when a token is ingested.\n\nConsider the sentence - \u201cI love my pet\u201d - and this is our entire dataset.\n\nToken| Id  \n---|---  \nI| 1  \nlove| 2  \nmy| 3  \npet| 4  \n  \nThe sentence will be encoded into four vectors\n\n    \n    \n    [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]\n\nIf our dataset was larger and had more tokens, the above vectors would be\nlonger still with the additional elements all set to zero.\n\nWhile we can certainly feed these one-hot token vectors as \u201cfeatures\u201d to the\nneural network, they are very sparse and learning signals from them would be\ncomputationally very expensive - both in the processing cycles as well as\nmemory. We probably can do better by converting them into a representation\nthat captures better semantic relationships between them.\n\nThe denser the information content, the more you will be able to get from any\nmachine learning effort, for the same architecture. While we make this\nstatement very loosely, any claims of better semantic representations need to\nbe validated thoroughly.\n\n### A Brief Build-up\n\nThis section steps into the generative LLM domain superficially, to make a\ncase for embeddings. Please treat this section accordingly - as some\nbackground, rather than the depth.\n\nFor the moment, ignore the fact that tokens can be characters or just paired\nup bytes that don\u2019t mean a thing in our natural language.\n\nWith an extremely large dataset - our text corpus - that is made up of long\nsequences of such tokens, we hope to still find enough patterns.\n\nI\u2019d encourage you to read the previous sentence once more, because I don\u2019t\nknow how to express it better but it\u2019s profound yet very meh in how it comes\nout.\n\n> Humans can learn to speak without learning to read and write. They learn by\n> hearing. When they hear, they have no notion of words and their boundaries.\n> They have no notion of spellings, or the alphabet. We learn because we\u2019ve\n> heard for a very long time. Kids say their first words after being spoken to\n> for very long. Even then, they start very unskilled and take a very long\n> time before they can form complete sentences. But all along, they are still\n> learning because the world does not stop talking to them. We are wired to\n> decipher patterns and learn them. And then apply them.\n>\n> Machines are the same - because they are based on our current best\n> understanding of how the human neural network - the brain - works.\n\nIf we mimic the same processes by which humans learn, we might be able to make\nthe machines work too. It\u2019s funny I say that in a prophetic way because I\ndon\u2019t need to back myself up. The brave, smart folks in the past already\nhypothesized thus, and as of now (the year 2024), they have also proved\nthemselves right. We\u2019re only reading an article describing the current world -\nso, let\u2019s move on.\n\nImagine a generalized problem where given a set of tokens occuring together in\na sequence, we\u2019d like to train a network that can predict the next token.\nRemember that we are talking tokens - not words. Also, remember that words can\nbe tokens, but tokens need not be words.\n\nGiven a corpus of text, and tokens that form the vocabulary of that text, by\ntraining a neural network that can predict the next token well, we get a\nnetwork that intrinsically captures relationships between them.\n\n> Our training dataset is nothing if not a bucket load of samples of tokens\n> sequenced in specific manners that are particularly representative of the\n> natural languages we are hoping to model.\n\nThus, we can convert tokens into a vector form by passing them to this trained\nmodel, and what we hope to get is a vector that maps tokens into a space that\ncaptures notions of semantic and structural similarities in that space. What\u2019s\nmore, this space is much more compact as compared to the representation of the\ntokens. So, we get a higher signal density with a lower representation cost.\n\nWhen this new representation - that we call embeddings - is used as an input\nfor the next layer, we immediately see benefits of getting to deal with\nsmaller networks, if compared with what we\u2019d have to use if we used the raw\ntokens as vectors.\n\nTo remind - embeddings are created from tokens. Tokens can be our usual words,\nor they can be the more efficient forms described above.\n\n### Word Embeddings\n\nIn this section, we are going to look at embeddings created from word tokens.\nThis is to understand how the embeddings capture the essense of the input text\nfrom a human\u2019s point of view. Embeddings created as part of the training of\nLLMs are conceptually similar, but not understandable in the same way as word-\nbased embeddings.\n\nWe\u2019ll use a Python library called gensim. In this code sample, we use the in-\nbuilt model data downloader that gensim provides.\n\nThe similarity score you see is the dot-product of the normalized vectors for\nthe embeddings representing the corresponding words.\n\nPay attention to\n\n  * The vocabulary size. It is a round-figure - an indication that this is a choice made by the group working on computing this specific model-data.\n  * The dimensions of the embeddings vectors. This is another parameter chosen by the working group - to represent a very high-dimensional, sparse dataset in a lower-dimensional, more information-dense representation.\n\n    \n    \n    glove_model = downloader.load(glove_model_name) example_word = \"tower\" print(f\"Vector representation of the word {example_word}\") print(f\"Vector is of size {len(glove_model[example_word])} x 1\") print(glove_model[example_word]) vocabulary_size = len(glove_model.index_to_key) print(f\"First 10 words found in the model out of {vocabulary_size}\") for index, word in enumerate(glove_model.index_to_key): if index == 10: break print(f\"word {index} of {vocabulary_size}: {word}\") print(\"Words similar to 'sea'\") print(glove_model.most_similar(\"sea\", topn=5)) print(\"Words similar to 'dark'\") print(glove_model.most_similar(\"dark\", topn=5))\n    \n    \n    Vector representation of the word tower Vector is of size 100 x 1 [ 0.49798 -0.19195 -0.042257 0.30716 0.14201 -0.17802 -0.5812 0.099506 0.10369 0.34719 1.4765 0.29315 0.050309 0.38625 -0.010546 -0.48825 0.028371 0.37205 -0.054587 -0.97034 -0.2739 -0.17088 -0.40007 -0.82484 1.2213 -0.57755 -0.047156 0.42659 -0.81127 0.13567 0.24373 -0.017225 0.59778 0.88357 -0.031276 0.1912 0.09285 -0.34527 0.90167 -0.32842 -0.047498 -0.21357 -0.040807 0.18054 1.0713 0.41459 0.61106 0.41474 0.44509 0.14558 -0.21622 0.041226 0.0071143 0.87695 -0.036756 -2.6578 -0.24284 -0.10768 1.1065 0.39281 -0.4001 0.49402 0.061114 0.45835 -0.29885 -0.44187 -0.095089 0.56715 -0.27861 -0.1292 -0.39259 0.041889 0.21763 -0.15758 0.50181 -1.3226 0.98666 0.65784 0.13364 0.32398 -0.094106 -0.27393 -0.23881 0.26063 -0.15465 0.088721 0.50567 -0.75658 1.3782 0.40069 0.60617 -0.39039 0.45005 0.18642 -0.70215 -0.23439 -0.036533 -0.99066 0.66029 -0.17366 ] First 10 words found in the model out of 400000 word 0 of 400000: the word 1 of 400000: , word 2 of 400000: . word 3 of 400000: of word 4 of 400000: to word 5 of 400000: and word 6 of 400000: in word 7 of 400000: a word 8 of 400000: \" word 9 of 400000: 's Words similar to 'sea' [('ocean', 0.8386560678482056), ('waters', 0.8161073327064514), ('seas', 0.7600178122520447), ('mediterranean', 0.725997805595398), ('arctic', 0.6975978016853333)] Words similar to 'dark' [('bright', 0.7659975290298462), ('gray', 0.7474402189254761), ('black', 0.7343376278877258), ('darker', 0.7261934280395508), ('light', 0.7222751975059509)]\n\n#### Visualizing the Embeddings\n\nLet\u2019s try to plot the above. There is no easy way to visualize a 100-D space\nin which the embeddings lie, so we use a technique called the Principal\nComponent Analysis (PCA) to identify a transformed sub-space where the\ndimensions faithfully capture (within the limitation of the reduced\ndimensionality) the key distance metrics between our points of interest in the\nprojected space. Unsurprisingly, words that occur in similar contexts show up\ncloser.\n\nAs always, there\u2019s a Python package to do that. The code to plot is adapted\nfrom here.\n\n    \n    \n    plt.style.use('ggplot') def display_pca_scatterplot(model, words): word_vectors = np.array([model[w] for w in words]) twodim = PCA().fit_transform(word_vectors)[:,:2] plt.figure(figsize=(6, 6)) plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r') for word, (x, y) in zip(words, twodim): plt.text(x + 0.05, y + 0.05, word) display_pca_scatterplot(glove_model, [\"sea\", \"waters\", \"mediterranean\", \"arctic\", \"tea\", \"giant\", \"lion\", \"hockey\"]) plt.savefig(\"gensim-word-similarities.png\")\n\nIt\u2019s important to note that this representation is learnt from the training\ndataset. That the training dataset matches our expectations is purely a result\nof the fact that the training dataset is a generic dataset. But if we were to\nchoose a different, custom dataset, the embeddings can be expected to be\nplaced differently on the graph.\n\n### LLM Model Embeddings\n\nWord embeddings are great for human consumption, and then some more. But there\nare far too many words, and many words enter a language over time. Plus there\nare new Proper Nouns that get invented over time. And then, there are\nmisspellings. Word embeddings miss out on these.\n\nWhen looking to train more advanced networks using embeddings, we want our\nembeddings that capture further lower-level nuances of the language in\nquestion. Let\u2019s take an example. Which of the following (currently non-\nexistent in my knowledge) words are likely to be considered okay in a new\npiece of text you encounter?\n\n  * Zbwklttwq\n  * Queprayent\n\nI\u2019d guess you agree it\u2019s Queprayent. Why? There\u2019s something about our\nlanguages that makes us conclude so. We have no rules to explain, but one (of\nmultiple) possible explanations is how the vowels make the word pronounceable\nand hence more palatable to us. With techniques like subword tokenization or\nbyte-pair encoding, it\u2019s possible to capture such nuances of a language.\n\nWhen training LLMs, we could use the word embeddings. But if we trained our\nLLM with an embeddings model that captures further lower-level language\nnuances, we can hope to train networks better and also hope to get better\noutcomes for the various tasks we imagine using the LLMs for. That\u2019s where\nusing subword or byte-pair-encoded tokens come in.\n\nLLM models have their own embeddings that are distinct from embeddings created\nfrom more human-understandable semantic entities like dictionary words. In\nother words, each LLM gets to decide its own embeddings model to train and\nuse. When working with the model data, the embeddings model will be part of\nthe distribution and only that can and will be used.\n\nLet us look at an example embedding computed by meta-llama/Meta-Llama-3-8B\n\n    \n    \n    llama3_embeddings = load_embeddings(llama3_model_embeddings_extract_file) llama3_tokenizer = AutoTokenizer.from_pretrained(llama3_model_name) text_for_embeddings = \"The oceans and the seas are filled with salty water, cried the earth.\" # We need the return_tensors to be set to pt because the embeddings model expects a tensor in that format text_tokens = llama3_tokenizer(text_for_embeddings, truncation=True, return_tensors=\"pt\") input_ids = text_tokens[\"input_ids\"] print(f\"The input ids are: {input_ids}\") input_embeddings = llama3_embeddings(input_ids) print(\"===== Embeddings =====\") print(input_embeddings)\n    \n    \n    Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained. Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation. The input ids are: tensor([[ 791, 54280, 323, 279, 52840, 527, 10409, 449, 74975, 3090, 11, 39169, 279, 9578, 13]]) ===== Embeddings ===== tensor([[[ 3.3760e-04, -3.0670e-03, -6.7139e-04, ..., 7.0801e-03, -2.1057e-03, 2.6245e-03], [-1.1719e-02, -1.5259e-02, 1.9226e-03, ..., 3.6469e-03, 7.9346e-03, -9.8877e-03], [-5.5695e-04, -3.0365e-03, 8.2779e-04, ..., 2.0313e-04, 1.2589e-03, 5.0964e-03], ..., [ 7.1335e-04, -2.0294e-03, -1.2436e-03, ..., 1.0910e-03, -5.7373e-03, 2.9297e-03], [ 6.6223e-03, -9.5215e-03, -5.0964e-03, ..., 1.2741e-03, 3.4790e-03, 3.8147e-03], [-8.7738e-04, -5.5313e-04, -1.4603e-05, ..., 1.6403e-03, 3.3951e-04, 1.7166e-03]]], grad_fn=<EmbeddingBackward0>)\n\nTo check the model architecture, let\u2019s print out the structure of the meta-\nllama/Meta-Llama-3-8B model.\n\n    \n    \n    llama_model = AutoModel.from_pretrained(llama3_model_name) print(llama_model)\n    \n    \n    Loading checkpoint shards: 0% 0/4 [00:00<?, ?it/s] Loading checkpoint shards: 25% 1/4 [00:06<00:19, 6.61s/it] Loading checkpoint shards: 50% 2/4 [00:19<00:20, 10.18s/it] Loading checkpoint shards: 75% 3/4 [00:36<00:13, 13.40s/it] Loading checkpoint shards: 100% 4/4 [00:36<00:00, 8.24s/it] Loading checkpoint shards: 100% 4/4 [00:36<00:00, 9.21s/it] LlamaModel( (embed_tokens): Embedding(128256, 4096) (layers): ModuleList( (0-31): 32 x LlamaDecoderLayer( (self_attn): LlamaSdpaAttention( (q_proj): Linear(in_features=4096, out_features=4096, bias=False) (k_proj): Linear(in_features=4096, out_features=1024, bias=False) (v_proj): Linear(in_features=4096, out_features=1024, bias=False) (o_proj): Linear(in_features=4096, out_features=4096, bias=False) (rotary_emb): LlamaRotaryEmbedding() ) (mlp): LlamaMLP( (gate_proj): Linear(in_features=4096, out_features=14336, bias=False) (up_proj): Linear(in_features=4096, out_features=14336, bias=False) (down_proj): Linear(in_features=14336, out_features=4096, bias=False) (act_fn): SiLU() ) (input_layernorm): LlamaRMSNorm() (post_attention_layernorm): LlamaRMSNorm() ) ) (norm): LlamaRMSNorm() )\n\nThe embeddings layer is the first in sequence - its dimensions are . Recall\nthat the token vocabulary size of meta-llama/Meta-Llama-3-8B is 128256 and the\nembeddings dimension is 4096. The embeddings layer maps each token to a 4096\ndimensional vector.\n\nWe don\u2019t need to deal with the embeddings in LLMs directly, unlike GloVe. Both\nkinds of embeddings - those for LLMs, and those created from natural language\nwords - serve different purposes.\n\n## Summing up...\n\nTokens and embeddings are the initial phases that any text encounters as it\ntraverses through some network.\n\nIf you are looking to develop with, or build upon textual deep learning\nsystems, understanding both of these is certainly a big advantage. Knowledge\nof tokenization and embeddings can also help you tune performance, and exploit\nbetter, systems like vector databases too. Vector databases work with\nembeddings, and they provide means to store documents and retrieve based on\nsimilarity scores. The similarity is defined by some distance measure in the\nvector-space of the embeddings. Knowing how a particular embedding model works\nwill help you make better choices for your use-cases.\n\nThe next logical steps will be to explore how LLMs work - for our purposes -\nin code. If you liked this article so far, I hope you\u2019ll like the following\narticle too - which is in the works. Will link to it here when it\u2019s ready.\n\n## References\n\n  * Let\u2019s build GPT: from scratch, in code, spelled out.\n  * Inside the LLM: Visualizing the Embeddings Layer of Mistral-7B and Gemma-2B\n\n    * The code repository that goes along side the above video.\n\n## Supporting Code\n\nThe supporting code not found in the snippets above can be found below.\n\n### pyproject.toml\n\n    \n    \n    [tool.poetry] name = \"llm-for-the-experienced-beginner\" version = \"0.1.0\" authors = [\"Ravindra R. Jaju\"] description = \"Supporting code for the article - Understanding Tokens and Embeddings with Code\" [tool.poetry.dependencies] python = \"^3.12\" scikit-learn = \"^1.4.2\" transformers = \"^4.39.3\" gensim = \"^4.3.2\" numpy = \"^1.26.4\" matplotlib = \"^3.8.4\" torch = \"^2.2.2\" huggingface-hub = \"^0.22.2\" scipy = \"<1.13\" [build-system] requires = [\"poetry-core\"] build-backend = \"poetry.core.masonry.api\"\n\n### Utils\n\n    \n    \n    import os import torch import torch.nn as nn from functools import cache from transformers import AutoTokenizer, AutoModel class EmbeddingModel(nn.Module): def __init__(self, vocab_size, embedding_dim): super(EmbeddingModel, self).__init__() self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim) def forward(self, input_ids): return self.embedding(input_ids) def print_tokens(tokenizer, input_ids_tensor): token_texts = [tokenizer.decode([token_id], skip_special_tokens=True) for token_id in input_ids_tensor[0]] header = f\"{'Token':<10} | {'ID':<8}\" print(f\"{'='*30}\\n{'Tokens and their IDs':^30}\\n{'='*30}\") print(header) print(f\"{'-'*10}-+-{'-'*17}\") for idx, token_id in enumerate(input_ids_tensor[0]): token_text = token_texts[idx] print(f\"{token_text:<10} | {token_id:<20}\") print(f\"{'='*30}\") def extract_embeddings(model_name, embeddings_filename, **kwargs): trust_remote_code = kwargs.pop(\"trust_remote_code\", False) if not os.path.isfile(embeddings_filename): model = AutoModel.from_pretrained(model_name, trust_remote_code=trust_remote_code) embeddings = model.get_input_embeddings() print(f\"Extracted embeddings layer for {model_name}: {embeddings}\") torch.save(embeddings.state_dict(), embeddings_filename) else: print(f\"File {embeddings_filename} already exists...\") # Optimizing on load times for REPL-workflows - we cache @cache def load_embeddings(embeddings_filename): embeddings_data = torch.load(embeddings_filename) weights = embeddings_data[\"weight\"] vocab_size, dimensions = weights.size() embeddings = EmbeddingModel(vocab_size, dimensions) embeddings.embedding.weight.data = weights embeddings.eval() return embeddings\n\n### Code Files\n\n  * constants.py\n  * utils.py\n  * llm_tokens_embeddings.py\n\n## Acknowledgements\n\nIn no specific order, I\u2019d like to acknowledge specific and detailed feedback\non the content and style from my colleagues\n\n  * Vinayak Kadam\n  * Rhushikesh Apte\n  * Sudarsan Balaji\n  * Preethi Srinivasan\n\n\u00a9 Ravindra R. Jaju \u2014 2015-2024\n\njaju jaju\n\n", "frontpage": false}
