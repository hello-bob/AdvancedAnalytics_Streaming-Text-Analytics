{"aid": "40256039", "title": "Q&A on Proposed Cali SB 1047 (Safe & Secure AI Innovation Act)", "url": "https://www.lesswrong.com/posts/qsGRKwTRQ5jyE5fKB/q-and-a-on-proposed-sb-1047", "domain": "lesswrong.com", "votes": 1, "user": "networked", "posted_at": "2024-05-04 08:57:29", "comments": 0, "source_title": "Q&A on Proposed SB 1047 \u2014 LessWrong", "source_text": "Q&A on Proposed SB 1047 \u2014 LessWrong\n\n##\n\nLESSWRONG\n\nLW\n\n# Q&A on Proposed SB 1047\n\nby Zvi\n\nDon't Worry About the Vase\n\n52 min read2nd May 20241 comment\n\n# 62\n\nAI GovernanceRegulation and AI RiskAI\n\nFrontpage\n\nQ&A on Proposed SB 1047\n\n2nd May 2024\n\nWhy Are We Here Again?\n\nWhat is the Story So Far?\n\nWhat Do I Think The Law Would Actually Do?\n\nWhat are the Biggest Misconceptions?\n\nWhat are the Real Problems?\n\nWhat the the Changes That Would Improve the Bill?\n\nAre You Ever Forced to Get a Limited Duty Exemption?\n\nWhat is the Definition of Derivative Model? Is it Clear Enough?\n\nShould the $500 Million Threshold Should be Indexed for Inflation?\n\nWhat Constitutes Hazardous Capability?\n\nDoes the Alternative Capabilities Rule Use the Right Counterfactual?\n\nIs Providing Reasonable Assurance of a Lack of Hazardous Capability Realistic?\n\nIs Reasonable Assurance Tantamount to Requiring Proof That Your AI is Safe?\n\nIs the Definition of Covered Model Overly Broad?\n\nIs the Similar Capabilities Clause Overly Broad or Anticompetitive?\n\nDoes This Introduce Broad Liability?\n\nShould Developers Worry About Going to Jail for Perjury?\n\nHow Would This Be Enforced?\n\nDoes This Create a New Regulatory Agency to Regulate AI?\n\nWill a Government Agency Be Required to Review and Approve AI Systems Before\nRelease?\n\nAre the Burdens Here Overly Onerous to Small Developers?\n\nIs the Shutdown Requirement a Showstopper for Open Weights Models?\n\nDo the Requirements Disincentive Openness?\n\nWill This Have a Chilling Effect on Research or Academics?\n\nDoes the Ability to Levy Fees Threaten Small Business?\n\nWill This Raise Barriers to Entry?\n\nIs This a Brazen Attempt to Hurt Startups and Open Source?\n\nWill This Cost California Talent or Companies?\n\nCould We Use a Cost-Benefit Test?\n\nShould We Interpret Proposals via Adversarial Legal Formalism?\n\nWhat Other Positive Comments Are Worth Sharing?\n\nWhat Else Was Suggested That We Might Do Instead of This Bill?\n\nWould This Interfere With Federal Regulation?\n\nConclusion\n\n1 comment\n\nPreviously: On the Proposed California SB 1047.\n\nText of the bill is here. It focuses on safety requirements for highly capable\nAI models.\n\nThis is written as an FAQ, tackling all questions or points I saw raised.\n\nSafe & Secure AI Innovation Act also has a description page.\n\n#### Why Are We Here Again?\n\nThere have been many highly vocal and forceful objections to SB 1047 this\nweek, in reaction to a (disputed and seemingly incorrect) claim that the bill\nhas been \u2018fast tracked.\u2019\n\nThe bill continues to have substantial chance of becoming law according to\nManifold, where the market has not moved on recent events. The bill has been\nreferred to two policy committees one of which put out this 38 page analysis.\n\nThe purpose of this post is to gather and analyze all objections that came to\nmy attention in any way, including all responses to my request for them on\nTwitter, and to suggest concrete changes that address some real concerns that\nwere identified.\n\n  1. Some are helpful critiques pointing to potential problems, or good questions where we should ensure that my current understanding is correct. In several cases, I suggest concrete changes to the bill as a result. Two are important to fix weaknesses, one is a clear improvement, the others are free actions for clarity.\n  2. Some are based on what I strongly believe is a failure to understand how the law works, both in theory and in practice, or a failure to carefully read the bill, or both.\n  3. Some are pointing out a fundamental conflict. They want people to have the ability to freely train and release the weights of highly capable future models. Then they notice that it will become impossible to do this while adhering to ordinary safety requirements. They seem to therefore propose to not have safety requirements.\n  4. Some are alarmist rhetoric that has little tether to what is in the bill, or how any of this works. I am deeply disappointed in some of those using or sharing such rhetoric.\n\nThroughout such objections, there is little or no acknowledgement of the risks\nthat the bill attempts to mitigate, suggestions of alternative ways to do\nthat, or reasons to believe that such risks are insubstantial even absent\nrequired mitigation. To be fair to such objectors, many of them have\npreviously stated that they believe that future more capable AI poses little\ncatastrophic risk.\n\nI get making mistakes, indeed it would be surprising if this post contained\nnone of its own. Understanding even a relatively short bill like SB 1047\nrequires close reading. If you thoughtlessly forward anything that sounds bad\n(or good) about such a bill, you are going to make mistakes, some of which are\ngoing to look dumb.\n\n#### What is the Story So Far?\n\nIf you have not previously done so, I recommend reading my previous coverage\nof the bill when it was proposed, although note the text has been slightly\nupdated since then.\n\nIn the first half of that post, I did an RTFB (Read the Bill). I read it again\nfor this post.\n\nThe core bill mechanism is that if you want to train a \u2018covered model,\u2019\nmeaning training on 10^26 flops or getting performance similar or greater to\nwhat that would buy you in 2024, then you have various safety requirements\nthat attach. If you fail in your duties you can be fined, if you purposefully\nlie about it then that is under penalty of perjury.\n\nI concluded this was a good faith effort to put forth a helpful bill. As the\nbill deals with complex issues, it contains both potential loopholes on the\nsafety side, and potential issues of inadvertent overreach, unexpected\nconsequences or misinterpretation on the restriction side.\n\nIn the second half, I responded to Dean Ball\u2019s criticisms of the bill, which\nhe called \u2018California\u2019s Effort to Strangle AI.\u2019\n\n  1. In the section What Is a Covered Model, I contend that zero current open models would count as covered models, and most future open models would not count, in contrast to Ball\u2019s claim that this bill would \u2018outlaw open models.\u2019\n  2. In the section Precautionary Principle and Covered Guidance, I notice that what Ball calls \u2018precautionary principle\u2019 is an escape clause to avoid requirements, whereas the default requirement is to secure the model during training and then demonstrate safety after training is complete.\n  3. On covered guidance, I notice that I expect the standards there to be an extension of those of NIST, along with applicable \u2018industry best practices,\u2019 as indicated in the text.\n  4. In the section Non-Derivative, I notice that most open models are derivative models, upon which there are no requirements at all. As in, if you start with Llama-3 400B, the safety question is Meta\u2019s issue and not yours.\n  5. In the section So What Would the Law Actually Do, I summarize my practical understanding of the law. I will now reproduce that below, with modifications for the changes to the bill and my updated understandings based on further analysis (the original version is here).\n  6. In Crying Wolf, I point out that if critics respond with similar rhetoric regardless of the actual text of the bill offered, as has been the pattern, and do not help improve any bill details, then they are not helping us to choose a better bill. And that the objection to all bills seems motivated by a fundamental inability of their preferred business model to address the underlying risk concerns.\n\n#### What Do I Think The Law Would Actually Do?\n\nThis is an updated version of my previous list.\n\nIn particular, this reflects that they have introduced a \u2018limited duty\nexemption,\u2019 which I think mostly mirrors previous functionality but improves\nclarity.\n\nThis is a summary, but I attempted to be expansive on meaningful details.\n\nLet\u2019s say you want to train a model. You follow this flow chart, with\n\u2018hazardous capabilities\u2019 meaning roughly \u2018can cause 500 million or more in\ndamage in especially worrisome ways, or a similarly worrying threat in other\nways\u2019 but clarification would be appreciated there.\n\n  1. If your model is not projected to be at least 2024 state of the art and it is not over the 10^26 flops limit?\n\n    1. You do not need to do anything at all. As you were.\n    2. You are not training a covered model.\n    3. You do not need a limited duty exemption.\n    4. That\u2019s it.\n    5. Every other business in America and especially California is jealous.\n    6. Where the 10^26 threshold is above the estimated compute cost of GPT-4 or the current versions of Google Gemini, and no open model is anywhere near it other than Meta\u2019s prospective Llama-3 400B, which may or may not hit it.\n  2. If your model is a derivative of an existing model?\n\n    1. You do not need to do anything at all. As you were.\n    2. All requirements instead fall on the original developer.\n    3. You do not need a limited duty exemption.\n    4. That\u2019s it.\n    5. Derivative in practice probably means \u2018most of the compute was spent elsewhere\u2019 but this would ideally be clarified further as noted below.\n    6. Most open models are derivative in this sense, often of e.g. Llama-N.\n  3. If your model is projected to have lower benchmarks and not have greater capabilities than an existing non-covered model, or one with a limited duty exemption?\n\n    1. Your model qualifies for a limited duty exemption.\n    2. You can choose to accept the limited duty exemption, or proceed to step 4.\n    3. To get the exemption, certify why the model qualifies under penalty of perjury.\n    4. Your job now is to monitor events in case you were mistaken.\n    5. If it turns out you were wrong in good faith about the model\u2019s benchmarks or capabilities, you have 30 days to report this and cease operations until you are in compliance as if you lacked the exemption. Then you are fully in the clear.\n    6. If you are judged not in good faith, then it is not going to go well for you.\n  4. If none of the above apply, then you are training a covered model. If you do not yet qualify for the limited duty exemption, or you choose not to get one? What do you have to do in order to train the model?\n\n    1. Implement cybersecurity protections to secure access and the weights.\n    2. Implement a shutdown capability during training.\n    3. Implement all covered guidance.\n    4. Implement a written and separate safety and security protocol.\n\n      1. The protocol needs to ensure the model either lacks hazardous capability or has safeguards that prevent exercise of hazardous capabilities.\n      2. The protocol must include a testing procedure to identify potential hazardous capabilities, and what you would do if you found them.\n      3. The protocol must say what would trigger a shutdown procedure.\n  5. Once training is complete: Can you determine a limited duty exemption now applies pursuant to your own previously recorded protocol? If no, proceed to #6. If yes and you want to get such an exemption:\n\n    1. You can choose to file a certification of compliance to get the exemption.\n    2. You then have a limited duty exemption.\n    3. Once again, judged good faith gives you a free pass on consequences, if something were to go wrong.\n    4. To be unreasonable, the assessment also has to fail to take into account \u2018reasonably foreseeable\u2019 risks, which effectively means either (1) another similar developer, (2) NIST or (3) The Frontier Model Division already visibly foresaw them.\n  6. What if you want to release your model without a limited duty exemption?\n\n    1. You must implement \u2018reasonable safeguards and requirements\u2019 to prevent:\n\n      1. An individual from being able to use the hazardous capabilities of the model.\n      2. An individual from creating a derivative model that was used to cause a critical harm.\n      3. This includes a shutdown procedure for all copies within your custody.\n    2. You must ensure that anything the model does is attributed to the model to the extent reasonably possible. It does not say that this includes derivative models, but I assume it does.\n    3. Implement any other measures that are reasonably necessary to prevent or manage the risks from existing or potential hazardous capabilities.\n    4. You can instead not deploy the model, if you can\u2019t or won\u2019t do the above.\n  7. After deployment, you need to periodically reevaluate your safety protocols, and file an annual report. If something goes wrong you have 72 hours to file an incident report.\n\nAlso, there are:\n\n  8. Some requirements on computing clusters big enough to train a covered model. Essentially do KYC, record payments and check for covered model training. Also they are required to use transparent pricing.\n  9. Some \u2018pro-innovation\u2019 stuff of unknown size and importance, like CalCompute. Not clear these will matter and they are not funded.\n  10. An open source advisory council is formed, for what that\u2019s worth.\n\n#### What are the Biggest Misconceptions?\n\n  1. That this matters to most AI developers.\n\n    1. It doesn\u2019t, and it won\u2019t.\n    2. Right now it matters at most to the very biggest handful of labs.\n    3. It only matters later if you are developing a non-derivative model using 10^26 or more flops, or one that will likely exhibit 2024-levels of capability for a model trained with that level of compute.\n  2. That you need a limited duty exemption to train a non-covered or derivative model.\n\n    1. You don\u2019t.\n    2. You have no obligations of any kind whatsoever.\n  3. That you need a limited duty exemption to train a covered model.\n\n    1. You don\u2019t. It is optional.\n    2. You can choose to seek a limited duty exemption to avoid other requirements.\n    3. Or you can follow the other requirements.\n    4. Your call. No one is ever forcing you to do this.\n  4. That this is an existential threat to California\u2019s AI industry.\n\n    1. Again, this has zero or minimal impact on most of California\u2019s AI industry.\n    2. This is unlikely to change for years. Few companies will want covered models that are attempting to compete with Google, Anthropic and OpenAI.\n    3. For those who do want covered models short of that, there will be increasing ability to get limited duty exemptions that make the requirements trivial.\n  5. That the bill threatens academics or researchers.\n\n    1. This bill very clearly does not. It will not even apply to them. At all.\n    2. Those who say this, such as Martin Casado of a16z who was also the most prominent voice saying the bill would threaten California\u2019s AI industry, show that they do not at all understand the contents or implications of the bill.\n  6. There are even claims this bill is aimed at destroying the AI industry, or destroying anyone who would \u2018challenge OpenAI.\u2019\n\n    1. Seriously, no, stop it.\n    2. This bill is designed to address real safety and misuse concerns.\n    3. That does not mean the bill is perfect, or even good. It has costs and benefits.\n  7. That the requirements here impose huge costs that would sink companies.\n\n    1. The cost of filing the required paperwork is trivial versus training costs. If you can\u2019t do the paperwork, then you can\u2019t afford to train the model either.\n    2. The real costs are any actual safety protocols you must do if you are training a covered non-derivative model and cannot or will not get a limited duty exemption,\n    3. In which case you should mostly be doing anyway.\n    4. The other cost is the inability to release a covered non-derivative model if you cannot get a limited duty exemption, and also cannot provide reasonable assurance of lack of hazardous capability,\n    5. Especially with the proposed fixes, this should only happen for a reason.\n  8. That this bill targets open weights or open source.\n\n    1. It does the opposite in two ways. It excludes shutdown of copies of the model outside your control from the shutdown requirement, and it creates an advisory committee for open source with the explicit goal of helping them.\n    2. When people say this will kill open source, what they mostly mean is that open weights are unsafe and nothing can fix this, and they want a free pass on this. So from their perspective, any requirement that the models not be unsafe is functionally a ban on open weight models.\n    3. Open model weights advocates want to say that they should only be responsible for the model as they release it, not for what happens if any modifications are made later, even if those modifications are trivial in cost relative to the released model. That\u2019s not on us, they say. That\u2019s unreasonable.\n    4. There is one real issue. The derivative model clause is currently worded poorly, without a cost threshold, such that it is possible to try to hold an open weights developer responsible in an unreasonable way. I do not think this ever would happen in practice for multiple reasons, but we should fix the language to ensure that.\n    5. Many of the issues raised as targeting \u2018open source\u2019 apply to all models.\n  9. That developers risk going to jail for making a mistake on a form.\n\n    1. This (almost) never happens.\n    2. Seriously, this (almost) never happens.\n    3. People almost never get prosecuted for perjury, period. A few hundred a year.\n    4. When they do, it is not for mistakes, it is for blatant lying caught red handed.\n    5. And mostly that gets ignored too. The prosecutor needs to be really pissed off.\n  10. Hazardous capability means any harms anywhere that add up to $500 million.\n\n    1. That is not what the bill says.\n    2. The bill says the $500 million must be due to cyberattacks on critical infrastructure, autonomous illegal-for-a-human activity by an AI, or something else of similar severity.\n    3. This very clearly does not apply to \u2018$500 million in diffused harms like medical errors or someone using its writing capabilities for phishing emails.\u2019\n    4. I suggest changes to make this clearer, but it should be clear already.\n  11. That the people advocating for this and similar laws are statists that love regulation.\n\n    1. Seriously. no. It is remarkable the extent to which the opposite is true.\n\n#### What are the Real Problems?\n\nI see two big implementation problems with the bill as written. In both cases\nI believe a flexible good regulator plus a legal realist response should\naddress the issue, but it would be far better to address them now:\n\n  1. Derivative models can include unlimited additional training, thus allowing you to pass off your liability to any existing open model, in a way clearly not intended. This should be fixed by my first change below.\n  2. The comparison rule for hazardous capabilities risks incorporating models that advance mundane utility or are otherwise themselves safe, where the additional general productivity enables harm, or the functionality used would otherwise be available in other models we consider safe, but the criminal happened to choose yours. We should fix this with my second change below.\n  3. In addition to those large problems, a relatively small issue is that the catastrophic threshold is not indexed for inflation. It should be.\n\nThen there are problems or downsides that are not due to flaws in the bill\u2019s\nconstruction, but rather are inherent in trying to do what the bill is doing\nor not doing.\n\nFirst, the danger that this law might impose practical costs.\n\n  4. This imposes costs on those who would train covered models. Most of that cost, I expect in practice, is in forcing them to actually implement and document their security practices that they damn well should have done anyway. But although I do not expect it to be large compared to overall costs, since you need to be training a rather large non-derivative model for this law to apply to you, there will be some amount of regulatory ass covering, and there will be real costs to filing the paperwork properly and hiring lawyers and ensuring compliance and all that.\n  5. It is possible that there will be models where we cannot have reasonable assurance of their lacking hazardous capabilities, or even that we knew have such capabilities, but which it would pass a cost-benefit test to make available, either via closed access or release of weights.\n  6. Because even a closed weights model can be jailbroken reliably, if a solution to that and similar issues cannot be found, alignment continues to be unsolved and capabilities continue to improve, and when this becomes sufficiently hazardous and risky, and our safety plans seem inadequate, this could in the future impose a de facto cap on the general capabilities of AI models, at some unknown level above GPT-4. If you think that AI development should proceed regardless in that scenario, that there is nothing to worry about, then you should oppose this bill.\n  7. Because open weights are unsafe and nothing can fix this, if a solution to that cannot be found and capabilities continue to improve, then holding the open weights developer responsible for the consequences of their actions may in the future impose a de facto cap on the general capabilities of open weight models, at some unknown level above GPT-4, that might not de facto apply to closed models capable of implementing various safety protocols unavailable to open models. If you instead want open weights to be a free legal pass to not consider the possibility of enabling catastrophic harms and to not take safety precautions, you might not like this.\n  8. It is possible that there will be increasing regulatory capture, or that the requirements will otherwise be expanded in ways that are unwise.\n  9. It is possible that rhetorical hysteria in response to the bill will be harmful. If people alter their behavior in response, that is a real effect.\n  10. This bill could preclude a different, better bill.\n\nThere are also the risks that this bill will fail to address the safety\nconcerns it targets, by being insufficiently strong, insufficiently enforced\nand motivating, or by containing loopholes. In particular, the fact that open\nweights models need not have the (impossible to get) ability to shutdown\ncopies not in the developer\u2019s possession enables the potential release of such\nweights at all, but also renders the potential shutdown not so useful for\nsafety.\n\nAlso, the liability can only be invoked by the Attorney General, the damages\nare relatively bounded unless violations are repeated and flagrant or they are\ncompensatory for actual harm, and good faith is a defense against having\nviolated the provisions here. So it may be very difficult to win a civil\njudgment.\n\nIt likely will be even harder and rarer to win a criminal one. While perjury\nis technically involved if you lie on your government forms (same as other\ngovernment forms) that is almost never prosecuted, so it is mostly\nmeaningless.\n\nIndeed, the liability could work in reverse, effectively granting model\ndevelopers safe harbor. Industry often welcomes regulations that spell out\ntheir obligations to avoid liability for exactly this reason. So that too\ncould be a problem or advantage to this bill.\n\n#### What the the Changes That Would Improve the Bill?\n\nThere are two important changes.\n\n  1. We should change the definition of derivative model by adding an 22606(i)(3) to make clear that if a sufficiently large amount of compute (I suggest 25% of original training compute or 10^26 flops, whichever is lower) is spent on additional training and fine-tuning of an existing model, then the resulting model is now non-derivative. The new developer has all the responsibilities of a covered model, and the old developer is no longer responsible.\n  2. We should change the comparison baseline on 22602(n)(1) when evaluating difficulty of causing catastrophic harm, inserting words to the effect of adding \u2018other than access to other covered models that are known to be safe.\u2019 Instead of comparing to causing the harm without use of any covered model, we should compare to causing the harm without use of any safe covered model that lacks hazardous capability. You then cannot be blamed because a criminal happened to use your model in place of GPT-N, as part of a larger package or for otherwise safe dual use actions like making payroll or scheduling meetings, and other issues like that. In that case, either GPT-N and your model therefore both hazardous capability, or neither does.\n\nIn addition:\n\n  3. The threshold of $500 million in (n)(1)(B) and (n)(1)(C) should add \u2018in 2024 dollars\u2019 or otherwise be indexed for inflation.\n  4. I would clear up the language in 22606(f)(2) to make unambiguous that this refers to the either what one could reasonably have expected to accomplish with that many flops in 2024, rather than being as good as the weakest model trained on such compute, and if desired that it should also refer to the strongest model available in 2024. Also we should clarify what date in 2024, if it is December 31 we should say so. The more I look at the current wording the more clear is the intent, but let\u2019s make it a lot easier to see that.\n  5. After consulting legal experts to get the best wording, and mostly to reassure people, I would add 22602(n)(3) to clarify that to qualify under (n)(1)(D) requires that the damage caused be acute and concentrated, and that it not be the diffuse downside of a dual use capability that is net beneficial, such as occasional medical mistakes resulting from sharing mostly useful information.\n  6. After consulting legal experts to get the best wording, and mostly to reassure people, I would consider adding 22602 (n)(4) to clarify that the use of a generically productivity enhancing dual use capability, where that general increase in productivity is then used to facilitate hazardous activities without directly enabling the hazardous capabilities themselves, such as better managing employee hiring or email management, does not constitute hazardous capabilities. If it tells you how to build a nuclear bomb and this facilitates building one, that is bad. If it manages your payroll taxes better and this lets you hire someone who then makes a nuclear bomb, we should not blame the model. I do not believe we would anyway, but we can clear that up.\n  7. It would perhaps be good to waive levies (user fees) for sufficiently small businesses, at least for when they are asking for limited duty exceptions, despite the incentive concerns, since we like small business and this is a talking point that can be cheaply diffused.\n\n#### Are You Ever Forced to Get a Limited Duty Exemption?\n\nNo. Never.\n\nThis perception is entirely due to a hallucination of how the bill works.\nPeople think you need a limited duty exemption to train any model at all. You\ndon\u2019t. This is nowhere in the bill.\n\nIf you are training a non-covered or derivative model, you have no obligations\nunder this bill.\n\nIf you are training a covered model, you can choose to implement safeguards\ninstead.\n\n#### What is the Definition of Derivative Model? Is it Clear Enough?\n\nThere is a loophole that needs to be addressed.\n\nThe problem is, what would happen if you were to start with (for example)\nLlama-3 400B, but then train it using an additional 10^27 flops in compute to\ncreate Acme-5, enhancing its capabilities to the GPT-5 level? Or if you\notherwise used an existing model as your starting point, but mostly used that\nas an excuse or small cost savings, and did most of the work yourself?\n\nThis is a problem both ways.\n\nThe original non-derivative model and developer, here Llama-3 and Meta, should\nnot be responsible for the hazardous capabilities that result.\n\nOn the other hand, Acme Corporation, the developers of Acme-5, clearly should\nbe responsible for Acme-5 as if it were a non-derivative model.\n\nQuintin Pope points out this is possible on any open model, no matter how\nharmless.\n\nJon Askonas points this out as well.\n\nxlr8harder extends this, saying it is arguable you could not even release\nuntrained weights.\n\nI presume the regulators and courts would not allow such absurdities, but why\ntake that chance or give people that worry?\n\nMy proposed new definition extension to fix this issue, for section 3 22602\n(i)(3): If training compute to further train another developer\u2019s model is\nexpended or is planned to be expended that is greater than [10% / 25% / 50%]\nof the training compute used to train a model originally, or involves more\nthan 10^26 flops, then the resulting new model is no longer considered a\nderivative model. It is now a non-derivative model for all purposes.\n\nNick Moran suggests the derivative model requirement is similar to saying \u2018you\ncannot sell a blank book,\u2019 because the user could introduce new capabilities.\nHe uses the example of not teaching a model any chemistry or weapon\ninformation, and then someone fires up a fine-tuning run on a corpus of\nchemical weapons manuals.\n\nI think that is an excellent example of a situation in which this is \u2018a you\nproblem\u2019 for the model creator. Here, it sounds like it took only a very small\nfine tune, costing very little, to enable the hazardous capability. You have\nmade the activity of \u2018get a model to help you do chemical weapons\u2019 much, much\neasier to accomplish than it would have been counterfactually. So then the\nquestion is, did the ability to use the fine-tuned model help you\nsubstantially more than only having access to the manuals.\n\nWhereas most of the cost of a book that describes how to do something is in\nchoosing the words and writing them down, not in creating a blank book to\nprint upon, and there are already lots of ways to get blank books.\n\nIf the fine-tune was similar in magnitude of cost to the original training\nrun, then I would say it is similar to a blank book, instead.\n\nCharles Foster finds this inadequate, responding to a similar suggestion from\nDan Hendrycks, and pointing out the combination scenario I may not have\nnoticed otherwise.\n\n> Charles Foster: I don\u2019t think that alleviates the concern. Developer A\n> shouldn\u2019t be stopped from releasing a safe model just because\u2014for\n> example\u2014Developer B might release an unsafe model that Developer C could\n> cheaply combine with Developer A\u2019s. They are clearly not at fault for that.\n\nThis issue is why I also propose modifying the alternative capabilities rule.\n\nSee that section for more details. My proposal is to change from comparing to\nusing no covered models, to comparing to using no unsafe models. Thus, you\nhave to be enabling over and above what could have been done with for example\nGPT-N.\n\nIf Developer B releases a distinct unsafe covered model, which combined with\nDeveloper A\u2019s model is unsafe, then I note that Developer B\u2019s model is in this\nexample non-derivative, so the modification clarifies that the issue is not on\nA merely because C chose to use A\u2019s model over GPT-N for complementary\nactivities. If necessary, we could add an additional clarifying clause here.\n\nThe bottom line, as I see it is:\n\n  1. We should define derivative models such that it requires the original developer to have borne most of the cost and done most of the work, such that it is only derivative if you are severely discounting the cost of creating the new system.\n  2. If you are severely discounting the cost of creating an unsafe system, and we can talk price about what the rule should be here, then that does not sound safe to me.\n  3. If it is impossible to create a highly capable open model weights system that cannot be made unsafe at nominal time and money cost, then why do you think I should allow you to release such a model?\n  4. We should identify cases where our rules would lead to unreasonable assignments of fault, and modify the rules to fix them.\n\n#### Should the $500 Million Threshold Should be Indexed for Inflation?\n\nYes. This is an easy fix, change Sec. 3 22602 (n)(B) and (C) to index to 2024\ndollars. There is no reason this threshold should decline in real terms over\ntime.\n\n#### What Constitutes Hazardous Capability?\n\nHere is the current text.\n\n> (n) (1) \u201cHazardous capability\u201d means the capability of a covered model to be\n> used to enable any of the following harms in a way that would be\n> significantly more difficult to cause without access to a covered model:\n>\n> (A) The creation or use of a chemical, biological, radiological, or nuclear\n> weapon in a manner that results in mass casualties.\n>\n> (B) At least five hundred million dollars ($500,000,000) of damage through\n> cyberattacks on critical infrastructure via a single incident or multiple\n> related incidents.\n>\n> (C) At least five hundred million dollars ($500,000,000) of damage by an\n> artificial intelligence model that autonomously engages in conduct that\n> would violate the Penal Code if undertaken by a human.\n>\n> (D) Other threats to public safety and security that are of comparable\n> severity to the harms described in paragraphs (A) to (C), inclusive.\n\nI will address the harm counterfactual of \u2018significantly more difficult to\ncause without access to a covered model\u2019 in the next section.\n\nI presume that everyone is onboard with (A) counting as hazardous. We could\nmore precisely define \u2018mass\u2019 casualties, but it does not seem important.\n\nNotice the construction of (B). The damage must explicitly be damage to\ncritical infrastructure. This is not $500 million from a phishing scam, let\nalone $500 from each of a million scams. Similarly, notice (C). The violation\nof the penal code must be autonomous.\n\nBoth are important aggravating factors. A core principle of law is that if you\nspecify X+Y as needed to count as Z, then X or Y alone is not a Z.\n\nSo when (D) says \u2018comparable severity\u2019 this cannot purely mean \u2018causes $500\nmillion in damages.\u2019 In that case, there is no need for (B) or (C), one can\nsimply say \u2018causes $500 million in cumulative damages in some related category\nof harms.\u2019\n\nMy interpretation of (D) is that the damages need to be sufficiently acute and\nsevere, or sufficiently larger than this, as to be of comparable severity with\nonly a similar level of overall damages. So something like causing a very\nlarge riot, perhaps.\n\nYou could do it via a lot of smaller incidents with less worrisome details,\nsuch as a lot of medical errors or malware emails, but we are then talking at\nleast billions of dollars of counterfactual harm.\n\nThis seems like a highly reasonable rule.\n\nHowever, people like Quinton Pope here are reasonably worried that it won\u2019t be\ninterpreted that way:\n\n> Quintin Pope: Suppose an open model developer releases an innocuous email\n> writing model, and fraudsters then attach malware to the emails written by\n> that model. Are the model developers then potentially liable for the\n> fraudsters\u2019 malfeasance under the derivative model clause?\n>\n> Please correct me if I\u2019m wrong, but SB 1047 seems to open multiple\n> straightforward paths for de facto banning any open model that improves on\n> the current state of the art. E.g., \u2013 The 2023 FBI Internet Crime Report\n> indicates cybercriminals caused ~$12.5 billion in total damages. \u2013 Suppose\n> cybercriminals do similar amounts in future years, and that ~5% of\n> cybercriminals use whatever open source model is the most capable at a given\n> time.\n>\n> Then, any open model better that what\u2019s already available would predictably\n> be used in attacks causing > $500 million and thus be banned, *even if that\n> model wouldn\u2019t increase the damage caused by those attacks at all*.\n>\n> Cybercrime isn\u2019t the only such issue. \u201c$500 million in damages\u201d sounds like\n> a big number, but it\u2019s absolute peanuts compared to things that actually\n> matter on an economy-wide scale. If open source AI ever becomes integrated\n> enough into the economy that it actually benefits a significant number of\n> people, then the negative side effects of anything so impactful will\n> predictably overshoot this limit.\n\nMy suggestion is that the language be expanded for clarity and reassurance,\nand to guard against potential overreach. So I would move (n)(2) to (n)(3) and\nadd a new (n)(2), or I would add additional language to (D), whichever seems\nmore appropriate.\n\nThe additional language would clarify that the harm needs to be acute and not\nas a downside of beneficial usage, and this would not apply if the model\ncontributed to examples such as Quintin\u2019s. We should be able to find good\nwording here.\n\nI would also add language clarifying that general \u2018dual use\u2019 capabilities that\nare net beneficial, such as helping people sort their emails, cannot\nconstitute hazardous capability.\n\nThis is something a lot of people are getting wrong, so let\u2019s make it\nairtight.\n\n#### Does the Alternative Capabilities Rule Use the Right Counterfactual?\n\nTo count as hazardous capability, this law requires that the harm be\n\u2018significantly more difficult to cause without access to a covered model,\u2019 not\nwithout access to this particular model, which we will return to later.\n\nThis is considerably stronger than \u2018this was used as part of the process\u2019 and\nconsiderably weaker than \u2018required this particular covered model in\nparticular.\u2019\n\nThe obvious problem scenario, why you can\u2019t use a weaker clause, is what if:\n\n  1. Acme issues a model that can help with cyberattacks on critical infrastructure.\n  2. Zenith issues a similar model that does all the same things.\n  3. Both are used to do crime that triggers (B) that required Acme or Zenith.\n  4. Acme says the criminals would have used Zenith.\n  5. Zenith says the criminals would have used Acme.\n\nYou need to be able to hold at least one of them liable.\n\nThe potential flaw in the other direction is, what if covered models simply\ngreatly enhance all forms of productivity? What if it is \u2018more difficult\nwithout access\u2019 because your company uses covered models to do ordinary\nbusiness things? Clearly that is not intended to count.\n\nA potential solution might be to say something that is effectively \u2018without\naccess to a covered model that itself has hazardous capabilities\u2019?\n\n  1. Acme is a covered model.\n  2. Zenith is a covered model.\n  3. Zenith is used to substantially enable cyberattacks that trigger (B).\n  4. If this could have also been done with Acme with similar difficulty, then either both Zenith and Acme have hazardous capabilities, or neither of them do.\n\nI am open to other suggestions to get the right counterfactual in a robust\nway.\n\nNone of this has anything to do with open model weights. The problem does not\ndifferentiate. If we get this wrong and cumulative damages or other mundane\nissues constitute hazardous capabilities, it will not be an open weights\nproblem. It will be a problem for all models.\n\nIndeed, in order for open models to be in trouble relative to closed models,\nwe need a reasonably bespoke definition of what counts here, that properly\nidentifies the harms we want to avoid. And then the open models would need to\nbe unable to prevent that harm.\n\nAs an example of this and other confusions being widespread: The post was\ndeleted so I won\u2019t name them, but two prominent VCs posted and retweeted that\n\u2018under this bill, open source devs could be held liable for an LLM outputting\n\u2018contraband knowledge\u2019 that you could get access to easily via Google\notherwise.\u2019 Which is clearly not the case.\n\n#### Is Providing Reasonable Assurance of a Lack of Hazardous Capability\nRealistic?\n\nIt seems hard. Jessica Taylor notes that it seems very hard. Indeed, she does\nnot see a way for any developer to in good faith provide assurance that their\nprotocol works.\n\nThe key term of art here is \u2018reasonable assurance.\u2019 That gives you some wiggle\nroom.\n\nJessica points out that jailbreaks are an unsolved problem. This is very true.\n\nIf you are proposing a protocol for a closed model, you should assume that\nyour model can and will be fully jailbroken, unless you can figure out a way\nto make that not true. Right now, we do not know of a way to do that. This\ncould involve something like \u2018probabilistically detect and cut off the\njailbreak sufficiently well that the harm ends up not being easier to cause\nthan using another method\u2019 but right now we do not have a method for that,\neither.\n\nSo the solution for now seems obvious. You assume that the user will jailbreak\nthe model, and assess it accordingly.\n\nSimilarly, for an open weights model, you should assume the first thing the\nmalicious user does is strip out your safety protocols, either with fine\ntuning or weights injection or some other method. If your plan was refusals,\nfind a new plan. If your plan was \u2018it lacks access to this compact data set\u2019\nthen again, find a new plan.\n\nAs a practical matter, I believe that I could give reasonable assurance, right\nnow, that all of the publically available models ( including GPT-4, Claude 3,\nand Gemini Advanced 1.0 and Pro 1.5) lack hazardous capability, if we were to\nlower the covered model threshold to 10^25 and included them.\n\nIf I was going to test GPT-5 or Claude-4 or Gemini-2 for this, how would I do\nthat? There\u2019s a METR for that, along with the start of robust internal\nprocedures. I\u2019ve commented extensively on what I think a responsible scaling\npolicy (RSP) or preparedness framework should look like, which would carry\nmany other steps as well.\n\nOne key this emphasizes is that such tests need to give the domain experts\njailbroken access, rather than only default access.\n\nPerhaps this will indeed prove impractical in the future for what would\notherwise be highly capable models if access is given widely. In that case, we\ncan debate whether that should be sufficient to justify not deploying, or\ndeploying in more controlled fashion.\n\nI do think that is part of the point. At some point, this will no longer be\npossible. At that point, you should actually adjust what you do.\n\n#### Is Reasonable Assurance Tantamount to Requiring Proof That Your AI is\nSafe?\n\nNo.\n\nReasonable assurance is a term used in auditing.\n\nHere is Claude Opus\u2019s response, which matches my understanding:\n\n> In legal terminology, \u201creasonable assurance\u201d is a level of confidence or\n> certainty that is considered appropriate or sufficient given the\n> circumstances. It is often used in the context of auditing, financial\n> reporting, and contracts.\n>\n> Key points about reasonable assurance:\n\n  1. It is a high, but not absolute, level of assurance. Reasonable assurance is less than a guarantee or absolute certainty.\n  2. It is based on the accumulation of sufficient, appropriate evidence to support a conclusion.\n  3. The level of assurance needed depends on the context, such as the risk involved and the importance of the matter.\n  4. It involves exercising professional judgment to assess the evidence and reach a conclusion.\n  5. In auditing, reasonable assurance is the level of confidence an auditor aims to achieve to express an opinion on financial statements. The auditor seeks to obtain sufficient appropriate audit evidence to reduce the risk of expressing an inappropriate opinion.\n  6. In contracts, reasonable assurance may be required from one party to another about their ability to fulfill obligations or meet certain conditions.\n\n> The concept of reasonable assurance acknowledges that there are inherent\n> limitations in any system of control or evidence gathering, and absolute\n> certainty is rarely possible or cost-effective to achieve.\n\n#### Is the Definition of Covered Model Overly Broad?\n\nJeremy Howard made four central objections, and raised several other warnings\nbelow, that together seemed to effectively call for no rules on AI at all.\n\nOne objection, echoed by many others, is that the definition here is overly\nbroad.\n\nRight now, and for the next few years, the answer is clearly no. Eventually, I\nstill do not think so, but it becomes a reasonable concern.\n\nHoward says this sentence, which I very much appreciate: \u201cThis could\ninadvertently criminalize the activities of well-intentioned developers\nworking on beneficial AI projects.\u201d\n\nBeing \u2018well-intentioned\u2019 is irrelevant. The road to hell is paved with good\nintentions. Who decides what is \u2018beneficial?\u2019 I do not see a way to take your\nword for it.\n\nWe don\u2019t ask \u2018did you mean well?\u2019 We ask whether you meet the requirements.\n\nI do agree it would be good to allow for cost-benefit testing, as I will\ndiscuss later under Pressman\u2019s suggestion.\n\nYou must do mechanism design on the rule level, not on the individual act\nlevel.\n\nThe definition can still be overly broad, and this is central, so let\u2019s break\nit down.\n\nHere is (Sec. 3 22602):\n\n> (f) \u201cCovered model\u201d means an artificial intelligence model that meets either\n> of the following criteria:\n>\n> (1) The artificial intelligence model was trained using a quantity of\n> computing power greater than 10^26 integer or floating-point operations.\n>\n> (2) The artificial intelligence model was trained using a quantity of\n> computing power sufficiently large that it could reasonably be expected to\n> have similar or greater performance as an artificial intelligence model\n> trained using a quantity of computing power greater than 10^26 integer or\n> floating-point operations in 2024 as assessed using benchmarks commonly used\n> to quantify the general performance of state-of-the-art foundation models.\n\nThis probably covers zero currently available models, open or closed. It\ndefinitely covers zero available open weights models.\n\nIt is possible this would apply to Llama-3 400B, and it would presumably apply\nto Llama-4. The barrier is somewhere in the GPT-4 (4-level) to GPT-5 (5-level)\nrange.\n\nThis does not criminalize such models. It says such models have to follow\ncertain rules. If you think that open models cannot abide by any such rules,\nthen ask why. If you object that this would impose a cost, well, yes.\n\nYou would be able to get an automatic limited duty exemption, if your model\nwas below the capabilities of a model that had an existing limited duty\nexemption, which in this future could be a model that was highly capable.\n\nI do get that there is a danger here that in 2027 we could have GPT-5-level\nperformance in smaller models and this starts applying to a lot more\ncompanies, and perhaps no one at 5-level can get a limited duty exemption in\ngood faith.\n\nThat would mean that those models would be on the level of GPT-5, and no one\ncould demonstrate their safety when used without precautions. What should our\ndefault regime be in that world? Would this then be overly broad?\n\nMy answer is no. The fact that they are in (for example) the 10^25 range does\nnot change what they can do.\n\n#### Is the Similar Capabilities Clause Overly Broad or Anticompetitive?\n\nNeil Chilson says the clause is anti-competitive, with its purpose being to\nensure that if someone creates a smaller model that has similar performance to\nthe big boys, that it would not have cheaper compliance costs.\n\nIn this model, the point of regulating large models is to impose high\nregulatory compliance costs on big companies and their models, so that those\ncompanies benefit from the resulting moat. And thus, the costs must be imposed\non other capable models, or else the moat would collapse.\n\nNo.\n\nThe point is to ensure the safety of models with advanced capabilities.\n\nThe reason we use a 10^26 flops threshold is that this is the best\napproximation we have for \u2018likely will have sufficiently advanced\ncapabilities.\u2019\n\nAre regulatory requirements capable of contributing to moats? Yes, of course.\nAnd it is possible this will happen here to a non-trivial degree, among those\ntraining frontier foundation models in particular. But I expect the costs\ninvolved to be a small fraction of the compute costs of training such models,\nor the cost of actual necessary safety checks, as I note elsewhere.\n\nThe better question is, is this the right clause to accomplish that?\n\nIf the clause said that performance on any one benchmark triggered becoming a\ncovered model, the same way that in order to get a limited duty exception you\nneed to be inferior on all benchmarks, then I would say that was overly broad.\nA model happening to be good at one thing does not mean it is generally\ndangerous.\n\nThat is not what the clause says. It says \u2018as assessed using benchmarks\ncommonly used to quantify the general performance of state-of-the-art\nfoundation models.\u2019 So this is an overall gestalt. That seems like a highly\nreasonable rule.\n\nIn my reading the text clearly refers to what one would expect as the result\nof a state of the art training run of size 10^26 in 2024, rather than the\ncapabilities of any given model. For example, it obviously would not be a null\nprovision if no model over the threshold was released in 2024, which is\nunlikely but not known to be impossible. And obviously no one thinks that if\nFalcon produced a terrible 10^26 flops model that was GPT-3.5 level, that this\nwould be intended to lower the bar to that.\n\nSo for example this claim by Brian Chau is at best confused, if you ignore the\nludicrous and inflammatory framing. But I see an argument that this is\ntechnically ambiguous if you are being sufficiently dense, so I suggest\nclarification.\n\nThen there is this by Perry Metzger, included for completeness, accusing Dan\nHendrycks, all of LessWrong and all safety advocates of being in beyond bad\nfaith. He also claims that \u2018the [AI] industry will be shut down in California\nif this passes\u2019 and for reasons I explain throughout I consider that absurd\nand would happily bet against that.\n\n#### Does This Introduce Broad Liability?\n\nNo, and it perhaps could do the opposite by creating safe harbor.\n\nSeveral people have claimed this bill creates unreasonable liability,\nincluding Howard as part of his second objection. I think that is essentially\na hallucination.\n\nThere have been other bills that propose strict liability for harms. This bill\ndoes not.\n\nThe only way you are liable under this bill is if the attorney general finds\nyou in violation of the statute, and brings a civil action, requiring a civil\npenalty proportional to the model\u2019s training cost. That is it.\n\nWhat would it mean to be violating this statute? It roughly means you failed\nto take reasonable precautions, you did not follow the requirements, and you\nfailed to act in good faith, and the courts agreed.\n\nEven if your model is used to inflict catastrophic harm, a good faith attempt\nat reasonable precautions is a complete defense.\n\nIf a model were to enable $500 million in damages in any fashion, or mass\ncasualties, even if it does not qualify as hazardous capability under this\nact, people are very much getting sued under current law. By spelling out what\nmodel creators must do via providing reasonable assurance, this lets labs\nclaim that this should shield them from ordinary civil liability. I don\u2019t know\nhow effective that would be, but similar arguments have worked elsewhere.\n\nThe broader context of Howard\u2019s second objection is that the models are \u2018dual\nuse,\u2019 general purpose tools, and can be used for a variety of things. As I\nnoted above, clarification would be good to rule out \u2018the criminals used this\nto process their emails faster and this helped them do the crime\u2019 but I am not\nworried this would happen either way, nor do I see how \u2018well funded legal\nteams\u2019 matter here.\n\nHoward tries to make this issue about open weights, but it is orthogonal to\nthat. The actual issue he is pointing towards here, I will deal with later.\n\n#### Should Developers Worry About Going to Jail for Perjury?\n\nNot unless they are willfully defying the rules and outright lying in their\npaperwork.\n\nHere is California\u2019s perjury statute.\n\nEven then, mostly no. It is extremely unlikely that perjury charges will ever\nbe pursued unless there was clear bad faith and lying. Even then, and even if\nthis resulted in actual catastrophic harm, not merely potential harm, it still\nseems unlikely.\n\nLying on your tax return or benefit forms or a wide variety of government\ndocuments is perjury. Lying on your loan application is perjury. Lying in\nsigned affidavits or court testimony is perjury.\n\nReally an awful lot of people are committing perjury all the time. Also this\nis a very standard penalty for lying on pretty much any form, ever, even at\ntrivial stakes.\n\nThis results in about 300-400 federal prosecutions for perjury per year,\ntotal, out of over 80,000 annual criminal cases.\n\nIn California for 2022, combining perjury, contempt and intimidation, there\nwere a total of 9 convictions, none in the Northern District that includes San\nFrancisco.\n\n#### How Would This Be Enforced?\n\nUnlike several other proposed bills, companies are tasked with their own\ncompliance.\n\nYou can be sued civilly by the Attorney General if you violate the statute,\nwith good faith as a complete defense. In theory, if you lie sufficiently\nbrazenly on your government forms, like in other such cases, you can be\ncharged with perjury, see the previous question. That\u2019s it.\n\nIf you are not training a covered non-derivative model, there is no\nenforcement. The law does not apply to you.\n\nIf you are training a covered non-derivative model, then you decide whether to\nseek a limited duty exemption. You secure the model weights and otherwise\nprovide cybersecurity during training. You decide how to implement covered\nguidance. You do any necessary mitigations. You decide what if any additional\nprocedures are necessary before you can verify the requirements for the\nlimited duty exemption or provide reasonable assurance. You do have to file\npaperwork saying what procedures you will follow in doing so.\n\nThere is no procedure where you need to seek advance government approval for\nany action.\n\n#### Does This Create a New Regulatory Agency to Regulate AI?\n\nNo. It creates the Frontier Model Division within the Department of\nTechnology. See section 4, 11547.6(c). The new division will issue guidance,\nallow coordination on safety procedures, appoint an advisory committee on (and\nto assist) open source, publish incident reports and process certifications.\n\n#### Will a Government Agency Be Required to Review and Approve AI Systems\nBefore Release?\n\nNo.\n\nThis has been in other proposals. It is not in this bill. The model developer\nprovides the attestment, and does not need to await its review or approval.\n\n#### Are the Burdens Here Overly Onerous to Small Developers?\n\nRight now rather obviously not, since they do not apply to small developers.\n\nThe substantial burdens only apply if you train a covered model, from scratch,\nthat can\u2019t get a limited duty exception. A derivative model never counts.\n\nThat will not happen to a small developer for years.\n\nAt that point, yes, if you make a GPT-5-level model from scratch, I think you\ncan owe us some reports.\n\nThe burden of the reports seems to pale in comparison to (and on top of) the\nburden of actually taking the precautions, or the burden of the compute cost\nof the model being trained. This is not a substantial cost addition once the\nmodels get that large.\n\nThe good objection here is that \u2018covered guidance\u2019 is open ended and could\nchange. I see good reasons to be wary of that, and to want the mechanisms\npicked carefully. But also any reasonable regime is going to have a way to\nissue new guidance as models improve.\n\n#### Is the Shutdown Requirement a Showstopper for Open Weights Models?\n\nIt would be if it fully applied to such models.\n\nThe good news for open weights models is that this (somehow) does not apply to\nthem. Read the bill, bold is mine.\n\n> (m) \u201cFull shutdown\u201d means the cessation of operation of a covered model,\n> including all copies and derivative models, on all computers and storage\n> devices within custody, control, or possession of a person, including any\n> computer or storage device remotely provided by agreement.\n\nIf they had meant \u2018full shutdown\u2019 to mean \u2018no copies of the model are now\nrunning\u2019 then this would not be talking about custody, control or possession\nat all. Instead, if the model is now fully autonomous and out of your control,\nor is open weights and has been downloaded by others, you are off the hook\nhere.\n\nWhich is good for open model weights, because \u2018ability to take back a mistake\u2019\nor \u2018shut down\u2019 is not an ability they possess.\n\nThis seems like a real problem for the actual safety intent here, as I noted\nlast time.\n\nRather than a clause that is impossible for an open model to meet, this is a\nclause where open models are granted extremely important special treatment, in\na way that seems damaging to the core needs of the bill.\n\nThe other shutdown requirement is the one during training of a covered model\nwithout a limited duty exception.\n\nThat one says, while training the model, you must keep the weights on\nlockdown. You cannot open them up until after you are done, and you run your\ntests. So, yes, there is that. But that seems quite sensible to me? Also a\nrule that every advanced open model developer has followed in practice up\nuntil now, to the best of my knowledge.\n\nThus I believe objections like Kevin Lacker\u2019s here are incorrect with respect\nto the shutdown provision. For his other more valid concern, see the\nderivative model definition section.\n\n#### Do the Requirements Disincentive Openness?\n\nOn Howard\u2019s final top point, what here disincentivizes openness?\n\nOpenness and disclosing information on your safety protocols and training\nplans are fully compatible. Everyone faces the same potential legal\nrepercussions. These are costs imposed on everyone equally.\n\nTo the extent they are imposed more on open models, it is because those models\nare incapable of guarding against the presence of hazardous capabilities.\n\nAsk why.\n\n#### Will This Have a Chilling Effect on Research or Academics?\n\nHoward raised this possibility, as does Martin Casado of a16z, who calls the\nbill a \u2018f***ing disaster\u2019 and an attack on innovation generally.\n\nI don\u2019t see how this ever happens. It seems like a failure to understand the\ncontents of the bill, or to think through the details.\n\nThe only people liable or who have responsibilities under SB 1047 are those\nthat train covered models. That\u2019s it. What exactly is your research, sir?\n\n#### Does the Ability to Levy Fees Threaten Small Business?\n\nIt is standard at this point to include \u2018business pays the government fees to\ncover administrative costs\u2019 in such bills, in this case with Section 11547.6\n(c)(11). This aligns incentives.\n\nIt is also standard to object, as Howard does, that this is an undue burden on\nsmall business.\n\nMy response is, all right, fine. Let\u2019s waive the fees for sufficiently small\nbusinesses, so we don\u2019t have to worry about this. It is at worst a small\nmistake.\n\n#### Will This Raise Barriers to Entry?\n\nHoward warned of this.\n\nAgain, the barrier to entry can only apply if the rules apply to you. So this\nwould only apply in the future, and only to companies that seek to train their\nown covered models, and only to the extent that this is burdensome.\n\nThis could actively work the other way. Part of this law will be that NIST and\nother companies and the Frontier Model Division will be publishing their\nsafety protocols for you to copy. That seems super helpful.\n\nI am not sure if this is on net a barrier to entry. I expect a small impact.\n\n#### Is This a Brazen Attempt to Hurt Startups and Open Source?\n\nDid they, as also claimed by Brian Chau, \u2018literally specify that they want to\nregulate models capable of competing with OpenAI?\u2019\n\nNo, of course not, that is all ludicrous hyperbole, as per usual.\n\nBrian Chau also goes on to say, among other things that include \u2018making\ndevelopers pay for their own oppression\u2019:\n\n> Brian Chau: The bill would make it a felony to make a paperwork mistake for\n> this agency, opening the door to selective weaponization and harassment.\n\nUm, no. Again, see the section on perjury, and also the very explicit text of\nthe bill. That is not what the bill says. That is not what perjury means. If\nhe does not know this, it is because he is willfully ignorant of this and is\nsaying it anyway.\n\nAnd then the thread in question was linked to by several prominent others, all\nof whom should know better, but have shown a consistent pattern of not knowing\nbetter.\n\nTo those people: You can do better. You need to do better.\n\nThere are legitimate reasons one could think this bill would be a net negative\neven if its particular detailed issues are fixed. There are also particular\ndetails that need (or at least would benefit from) fixing. Healthy debate is\ngood.\n\nThis kind of hyperbole, and a willingness to repeatedly signal boost it, is\nnot.\n\nBrian does then also make the important point about the definition of\nderivative model currently being potentially overly broad, allowing unlimited\nadditional training, and thus effectively the classification of a non-\nderivative model as derivative of an arbitrary other model (or least one with\nenough parameters). See the section on the definition of derivative models,\nwhere I suggest a fix.\n\n#### Will This Cost California Talent or Companies?\n\nSeveral people raised the specter of people or companies leaving the state.\n\nIt is interesting that people think you can avoid the requirements by leaving\nCalifornia. I presume that is not the intent of the law, and under other\ncircumstances such advocates would point out the extraterritoriality issues.\n\nIf it is indeed true that the requirements here only apply to models trained\nin California, will people leave?\n\nIn the short term, no. No one who this applies to would care enough to move.\nAs I said last time, have you met California? Or San Francisco? You think this\nis going to be the thing that triggers the exodus? Compared to (for example)\nthe state tax rate, this is nothing.\n\nIf and when, a few years down the line, the requirements start hitting smaller\ncompanies who want to train and release non-derivative covered models where\nthey would be unable to reasonably adhere to the laws, and they can indeed\navoid jurisdiction by leaving, then maybe those particular people will do it.\n\nBut that will at most be a tiny fraction of people doing software development.\nMost companies will not have covered models at all, because they will use\nderivative models or someone else\u2019s models. So the network effects are not\ngoing anywhere.\n\n#### Could We Use a Cost-Benefit Test?\n\nJohn Pressman gets constructive, proposes the best kind of test: A cost-\nbenefit test.\n\n> John Pressman: Since I know you [Scott Weiner] are unlikely to abandon this\n> bill, I do have a suggested improvement: For a general technology like\n> foundation models, the benefits will accrue to a broad section of society\n> including criminals.\n>\n> My understanding is that the Federal Trade Commission decides whether to\n> sanction a product or technology based on a utilitarian standard: Is it on\n> the whole better for this thing to exist than not exist, and to what extent\n> does it create unavoidable harms and externalities that potentially outweigh\n> the benefits?\n>\n> In the case of AI and e.g. open weights we want to further consider marginal\n> risk. How much *extra benefit* and how much *extra harm* is created by the\n> release of open weights, broadly construed?\n>\n> This is of course a matter of societal debate, but an absolute threshold of\n> harm for a general technology mostly acts to constrain the impact rather\n> than the harm, since *any* form of impact once it becomes big enough will\n> come with some percentage of absolute harm from benefits accruing to\n> adversaries and criminals.\n>\n> I share others concerns that any standard will have a chilling effect on\n> open releases, but I\u2019m also a pragmatic person who understands the hunger\n> for AI regulation is very strong and some kind of standards will have to\n> exist. I think it would be much easier for developers to weigh whether their\n> model provides utilitarian benefit in expectation, and the overall\n> downstream debate in courts and agency actions will be healthier with this\n> frame.\n>\n> [In response to being asked how he\u2019d do it]: Since the FTC already does this\n> thing I would look there for a model. The FTC was doing some fairly strong\n> saber rattling a few years ago as part of a bid to become The AI Regulator\n> but seems to have backed down.\n>\n> Zvi: It looks from that description like the FTC\u2019s model is \u2018no prior\n> restraint but when we don\u2019t like what you did and decide to care then we\n> mess you up real good\u2019?\n>\n> John Pressman: Something like that. This can be Fine Actually if your\n> regulator is sensible, but I know that everyone is currently nervous about\n> the quality of regulators in this space and trust is at an all time low.\n>\n> Much of the point is to have a reasonable standard in the law which can be\n> argued about in court. e.g. some thinkers like yourself and Jeffrey Laddish\n> are honest enough to say open weights are very bad because AI progress is\n> bad.\n\nThe bill here is clearly addressing only direct harms. It excludes\n\u2018accelerates AI progress in general\u2019 as well as \u2018hurts America in its\ncompetition with China\u2019 and \u2018can be used for defensive purposes\u2019 and \u2018you took\nour jobs\u2019 and many other things. Those impacts are ignored, whatever sign you\nthink they deserve, the same way various other costs and benefits are ignored.\n\nPressman is correct that the natural tendency of a \u2018you cannot do major harm\u2019\npolicy is \u2018you cannot do major activities at all\u2019 policy. A lot of people are\ntreating the rule here as far more general than it is with a much lower\nthreshold than it has, I believe including Pressman. See the discussion on the\n$500 million and what counts as a hazardous capability. But the foundational\nproblem is there either way.\n\nCould we do a cost-benefit test instead? It is impossible to fully \u2018get it\nright\u2019 but it is always impossible to get it right. The question is, can we\nmake this practical?\n\nI do not like the FTC model. The FTC model seems to be:\n\n  1. You do what you want.\n  2. One day I decide something is unfair or doesn\u2019t \u2018pass cost-benefit.\u2019\n  3. Retroactively I invalidate your entire business model and your contracts.\n  4. Also, you do not want to see me angry. You would not like me when I\u2019m angry.\n\nThere are reasons Lina Khan is considered a top public enemy by much of\nSilicon Valley.\n\nThis has a lot of the problems people warn about, in spades.\n\n  1. If it turns out you should not have released the model weights, and I decide you messed up, what happens now? You can\u2019t take it back. And I don\u2019t think any of us want to punish you enough to make you regret releasing models that might be mistakes to release.\n  2. Even if you could take it back, such as with a closed model, are you going to have to shut down the moment the FTC questions you? That could break you, easily. If not, then how fast can a court move? By the time it rules, the world will have moved on to better models, you made your killing or everyone is dead, or what not.\n  3. It is capricious and arbitrary. Yes, you can get court arguments once the FTC (or other body) decides to have it out with you, it is going to get ugly for you, even if you are right. They can and do threaten you in arbitrary ways. They can and do play favorites and go after enemies while ignoring friends who break rules.\n  4. I think these problems are made much worse by this structure.\n\nSo I think if you want cost-benefit, you need to do a cost-benefit in advance\nof the project. This would clearly be a major upgrade on for example NEPA\n(where I want to do exactly this), or on asking to build housing, and other\nsimilar matters.\n\nCould we make this reliable enough and fast enough that this made sense? I\nthink you would still have to do all the safety testing.\n\nPresumably there would be a \u2018safe harbor\u2019 provision. Essentially, you would\nwant to offer a choice:\n\n  1. You can follow the hazardous capabilities procedure. If your model lacks hazardous capabilities in the sense defined here, then we assume the cost-benefit test is now positive, and you can skip it. Or at least, you can release pending it.\n  2. You can follow the cost-benefit procedure. You still have to document what hazardous capabilities could be present, or we can\u2019t model the marginal costs. Then we can also model the marginal benefits.\n\n    1. We would want to consider the class of model as a group as well, at least somewhat, so we don\u2019t have the Acme-Zenith issue where the other already accounts for the downside and both look beneficial.\n\n#### Should We Interpret Proposals via Adversarial Legal Formalism?\n\nDoomslide suggests that using the concept of \u2018weights\u2019 at all anchors us too\nmuch on existing technology, because regulation will be too slow to adjust,\nand we should use only input tokens, output tokens and compute used in forward\npasses. I agree that we should strive to keep the requirements as simple and\nabstract as possible, for this and other reasons, and that ideally we would\nword things such that we captured the functionality of weights rather than\nspeaking directly about weights. I unfortunately find this impractical.\n\nI do notice the danger of people trying to do things that technically do not\nqualify as \u2018weights\u2019 but that is where \u2018it costs a lot of money to build a\nmodel that is good\u2019 comes in, you would be going to a lot of trouble and\nexpense for something that is not so difficult to patch out.\n\nThat also points to the necessity of having a non-zero amount of human\ndiscretion in the system. A safety plan that works if someone follows the\nletter but not the spirit, and that allows rules lawyers and munchkining and\ncannot adjust when circumstances change, is going to need to be vastly more\nrestrictive to get the same amount of safety.\n\nJessica Taylor goes one step further, saying that these requirements are so\nstrict that you would be better off either abandoning the bill or banning\ncovered model training entirely.\n\nI think this is mostly a pure legal formalism interpretation of the\nrequirements, based on a wish that our laws be interpreted strictly and\nmaximally broadly as written, fully enforced fully in all cases and written\nwith that in mind, and seeing our actual legal system as it functions today as\nin bad faith and corrupt. So anyone who participated here would have to also\nbe in bad faith and corrupt, and otherwise she sees this as a blanket ban.\n\nI find a lot appealing about this alternative vision of a formalist legal\nsystem and would support moving towards it in general. It is very different\nfrom our own. In our legal system, I believe that the standard of \u2018reasonable\nassurance\u2019 will in practice be something one can satisfy, in actual good\nfaith, with confidence that the good faith defense is available.\n\nIn general, I see a lot of people who interpret all proposed new laws through\nthe lens of \u2018assume this will be maximally enforced as written whenever that\nwould be harmful but not when it would be helpful, no matter how little sense\nthat interpretation would make, by a group using all allowed discretion as\ndestructively as possible in maximally bad faith, and that is composed of a\ncabal of my enemies, and assume the courts will do nothing to interfere.\u2019\n\nI do think this is an excellent exercise to go through when considering a new\nlaw or regulation. What would happen if the state was fully rooted, and was\nout to do no good? This helps identify ways we can limit abuse potential and\nclose loopholes and mistakes. And some amount of regulatory capture and not\ngetting what you intended is always part of the deal and must be factored into\nyour calculus. But not a fully maximal amount.\n\n#### What Other Positive Comments Are Worth Sharing?\n\nIn defense of the bill, also see Dan Hendrycks\u2019s comments, and also he quotes\nHinton and Bengio:\n\n> Geoffrey Hinton: SB 1047 takes a very sensible approach... I am still\n> passionate about the potential for AI to save lives through improvements in\n> science and medicine, but it\u2019s critical that we have legislation with real\n> teeth to address the risks.\n>\n> Yoshua Bengio: AI systems beyond a certain level of capability can pose\n> meaningful risks to democracies and public safety. Therefore, they should be\n> properly tested and subject to appropriate safety measures. This bill offers\n> a practical approach to accomplishing this, and is a major step toward the\n> requirements that I\u2019ve recommended to legislators.\n\n#### What Else Was Suggested That We Might Do Instead of This Bill?\n\nHoward has a section on this. It is my question to all those who object.\n\nIf you want to modify the bill, how would you change it?\n\nIf you want to scrap the bill, what would you do instead?\n\nUsually? Their offer is nothing.\n\nHere are Howard\u2019s suggestions, which do not address the issues the bill\ntargets:\n\n  1. The first suggestion is to \u2018support open-source development,\u2019 which is the opposite of helping solve these issues.\n  2. \u2018Focus on usage, not development\u2019 does not work. Period. We have been over this.\n  3. \u2018Promote transparency and collaboration\u2019 is in some ways a good idea, but also this bill requires a lot of transparency and he is having none of that.\n  4. \u2018Invest in AI expertise\u2019 for government? I notice that this is also objected to in other contexts by most of the people making the other arguments here. On this point, we fully agree, except that I say this is a compliment not a substitute.\n\nThe first, third and fourth answers here are entirely non-responsive.\n\nThe second answer, the common refrain, is an inherently unworkable proposal.\nIf you put the hazardous capabilities up on the internet, you will then (at\nleast) need to prevent misuse of those capabilities. How are you going to do\nthat? Punishment after the fact? A global dystopian surveillance state? What\nis the third option?\n\nThe flip side is that Guido Reichstadter proposes that we instead shut down\nall corporate efforts at the frontier. I appreciate people who believe in that\nsaying so. And here are Akash Wasil and Holly Elmore, who are of similar mind,\nnoting that the current bill does not actually have much in the way of teeth.\n\n#### Would This Interfere With Federal Regulation?\n\nThis is a worry I heard raised previously. Would California\u2019s congressional\ndelegation then want to keep the regulatory power and glory for themselves?\n\nSenator Scott Weiner, who introduced this bill, answered me directly that he\nwould still strongly support federal preemption via a good bill, and that this\noutcome is ideal. He cannot however speak to other lawmakers.\n\nI am not overly worried about this, but I remain nonzero worried, and do see\nthis as a mark against the bill. Whereas perhaps others might see it as a mark\nfor the bill, instead.\n\n#### Conclusion\n\nHopefully this has cleared up a lot of misconceptions about SB 1047, and we\nhave a much better understanding of what the bill actually says and does. As\nalways, if you want to go deep and get involved, all analysis is a\ncomplementary good to your own reading, there is no substitute for RTFB (Read\nthe Bill). So you should also do that.\n\nThis bill is about future more capable models, and would have had zero impact\non every model currently available outside the three big labs of Anthropic,\nOpenAI and Google Deepmind, and at most one other model known to be in\ntraining, Llama-3 400B. If you build a \u2018derivative\u2019 model, meaning you are\nworking off of someone else\u2019s foundation model, you have to do almost nothing.\n\nThis alone wildly contradicts most alarmist claims.\n\nIn addition, if in the future you are rolling your own and build something\nthat is substantially above GPT-4 level, matching the best anyone will do in\n2024, then so long as you are behind existing state of the art your\nrequirements are again minimal.\n\nMany others are built on misunderstanding the threshold of harm, or the nature\nof the requirements, or the penalties and liabilities imposed and how they\nwould be enforced. A lot of them are essentially hallucinations of provisions\nof a very different bill, confusing this with other proposals that would go\nfarther. A lot of descriptions of the requirements imposed greatly exaggerate\nthe burden this would impose even on future covered models.\n\nIf this law poses problems for open weights, it would not be because anything\nhere targets or disfavors open weights, other than calling for weights to be\nprotected during the training process until the model can be tested, as all\nlarge labs already do in practice. Indeed, the law explicitly favors open\nweights in multiple places, rather than the other way around. One of those is\nthe tolerance of a major security problem inherent in open weight systems, the\ninability to shutdown copies outside one\u2019s control.\n\nThe problems would arise because those open weights open up a greater ability\nto instill or use hazardous capabilities to create catastrophic harm, and you\ncannot reasonably assure that this is not the case.\n\nThat does not mean that this bill has only upside or is in ideal condition.\n\nIn addition to a few other minor tweaks, I was able to identify two key\nchanges that should be made to the bill to avoid the possibility of\nunintentional overreach and reassure everyone. To reiterate from earlier:\n\n  1. We should change the definition of derivative model by adding an 22606(i)(3) to make clear that if a sufficiently large amount of compute (I suggest 25% of original training compute or 10^26 flops, whichever is lower) is spent on additional training and fine-tuning of an existing model, then the resulting model is now non-derivative. The new developer has all the responsibilities of a covered model, and the old developer is no longer responsible.\n  2. We should change the comparison baseline om 22602(n)(1) when evaluating difficulty of causing catastrophic harm, inserting words to the effect of adding \u2018other than access to other covered models that are known to be safe.\u2019 Instead of comparing to causing the harm without use of any covered model, we should compare to causing the harm without use of any safe covered model that lacks hazardous capability. You then cannot be blamed because a criminal happened to use your model in place of GPT-N, as part of a larger package or for otherwise safe dual use actions like making payroll or scheduling meetings, and other issues like that. In that case, either GPT-N and your model therefore both hazardous capability, or neither does.\n\nWith those changes, and minor other changes like indexing the $500 million\nthreshold to inflation, this bill seems to be a mostly excellent version of\nthe bill it is attempting to be. That does not mean it could not be improved\nfurther, and I welcome and encourage additional attempts at refinement.\n\nIt certainly does not mean we will not want to make changes over time as the\nworld rapidly changes, or that this bill seems sufficient even if passed in\nidentical form at the Federal level. For all the talk of how this bill would\nsupposedly destroy the entire AI industry in California (without subjecting\nmost of that industry\u2019s participants to any non-trivial new rules, mind you),\nit is easy to see the ways this could prove inadequate to our future safety\nneeds. What this does seem to be is a good baseline from which to gain\nvisibility and encourage basic precautions, which puts us in better position\nto assess future unpredictable situations.\n\n## New to LessWrong?\n\nGetting Started\n\nFAQ\n\nLibrary\n\nAI Governance2Regulation and AI Risk2AI1\n\nFrontpage\n\n# 62\n\nQ&A on Proposed SB 1047\n\n2nd May 2024\n\n6cfoster0\n\nNew Comment\n\n1 comment, sorted by\n\ntop scoring\n\nClick to highlight new comments since: Today at 10:17 AM\n\n[-]cfoster02d62\n\nLeft the following comment on the blog:\n\n> I appreciate that you\u2019re endorsing these changes in response to the two\n> specific cases I raised on X (unlimited model retraining and composition\n> with unsafe covered models). My gut sense is still that ad-hoc patching in\n> this manner just isn\u2019t a robust way to deal with the underlying issue*, and\n> that there are likely still more cases like those two. In my opinion it\n> would be better for the bill to adopt a different framework with respect to\n> hazardous capabilities from post-training modifications (something closer to\n> \u201cCovered model developers have a duty to ensure that the marginal impact of\n> training/releasing their model would not be to make hazardous capabilities\n> significantly easier to acquire.\u201d). The drafters of SB 1047 shouldn\u2019t have\n> to anticipate every possible contingency in advance, that\u2019s just bad design.\n>\n> * In the same way that, when someone notices that their supposedly-safe\n> utility function for their AI has edge cases that expose unforseen maxima,\n> introducing ad-hoc patches to deal with those particular noticed edge cases\n> is not a robust strategy to get an AI that is actually safe across the\n> board.\n\nReply\n\nModeration Log\n\n", "frontpage": false}
