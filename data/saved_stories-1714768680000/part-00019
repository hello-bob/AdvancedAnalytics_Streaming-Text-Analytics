{"aid": "40246671", "title": "Serverless GPUs: L4, L40S, V100, and more in private preview", "url": "https://www.koyeb.com/blog/serverless-gpus-in-private-preview-l4-l40s-v100-and-more", "domain": "koyeb.com", "votes": 2, "user": "riadsila", "posted_at": "2024-05-03 11:59:10", "comments": 0, "source_title": "Serverless GPUs in Private Preview: L4, L40S, V100, and more", "source_text": "Serverless GPUs in Private Preview: L4, L40S, V100, and more\n\nAll blog posts\n\nArticle\n\n# Serverless GPUs in Private Preview: L4, L40S, V100, and more\n\nApr 30, 20243 min\n\nToday, we\u2019re excited to share that Serverless GPUs are available for all your\nAI inference needs directly through the Koyeb platform!\n\nWe're starting with GPU Instances designed to support AI inference workloads\nincluding both heavy generative AI models and lighter computer vision models.\nThese GPUs provide up to 48GB of vRAM, 733 TFLOPS and 900GB/s of memory\nbandwidth to support large models including LLMs and text-to-image models.\n\nWe're starting with 4 Instances, starting at $0.50/hr and billed by the\nsecond:\n\nRTX 4000 SFF ADA| L4| V100| L40S  \n---|---|---|---  \nGPU vRAM| 20GB| 24GB| 16GB| 48GB  \nGPU Memory Bandwidth| 280GB/s| 300GB/s| 900GB/s| 864GB/s  \nFP32| 19.2 TFLOPS| 30.3 TFLOPS| 15.7 TFLOPS| 91.6 TFLOPS  \nFP8| 153 TFLOPS| 240 TFLOPS| 125 TFLOPS| 733 TFLOPS  \nRAM| 44GB| 44GB| 44GB| 96GB  \nvCPU| 6| 30| 8| 30  \nOn-demand price| $0.50/hr| $1/hr| $1.2/hr| $2/hr  \n  \nAll these Instances have dedicated vCPUs that are equivalent to a hyperthread.\n\nThese GPUs come with the same serverless deployment experience that you know\non the platform with one-click deployment of Docker containers, built-in load-\nbalancing, and seamless horizontal autoscaling, zero downtime deployments,\nauto-healing, vector databases, observability, and real-time monitoring.\n\nBy the way, you can pause your GPU Instances when they're not in use. This is\na great way to stretch your computing budget when you don't need to keep your\nGPU instances running 24/7.\n\nTo access these GPU Instances, join the preview on koyeb.com/ai. We're\ngradually onboarding users to ensure the best experience for everyone.\n\nIf you need GPUs in volume, let us know in your request or book a call.\n\n## Getting started with GPUs\n\nTo get started and deploy your first service backed by a GPU, you can use the\nKoyeb CLI or the Koyeb Dashboard.\n\nAs usual, you can deploy using pre-built containers or directly connect your\nGitHub repository and let Koyeb handle the build of your applications.\n\nHere is how you can deploy a Ollama service in one CLI command:\n\n    \n    \n    koyeb app init ollama \\ --docker ollama/ollama \\ --instance-type l4 \\ --regions fra \\ --port 11434:http \\ --route /:11434 \\ --docker-command serve\n\nThat's it! In less than 60 seconds, you will have Ollama running on Koyeb\nusing a L4 GPU.\n\nYou can then pull your favorite models and start interacting with them.\n\n## Need more? We\u2019re just getting started\n\nWe're super excited by this release and we believe this dramatically\nsimplifies deploying production-grade inference workloads, APIs, and endpoints\nthanks to the serverless capabilities of the Koyeb platform.\n\nOur goal is to let you easily access, build, experiment, and deploy on the\nbest accelerators from AMD, Intel, Furiosa, Qualcomm, and Nvidia using one\nunified platform.\n\nIf you're looking for specific configurations, GPUs, or accelerators, we'd\nlove to hear from you. We're currently adding more GPUs and accelerators to\nthe platform and are working closely with early users to design our offering.\n\nLet's get in touch! Whether you require high-performance GPUs, specialized\naccelerators, or unique hardware configurations, we want to hear from you.\n\nSign up for the platform today and join the GPU serverless private preview!\n\nKeep up with all the latest updates by joining our vibrant and friendly\nserverless community or follow us on X at @gokoyeb.\n\nShare this article\n\n  * Copied!\n\nAuthors\n\n  * Yann L\u00e9ger@yann_eu\n\n  * Edouard Bonlieu@edouardb_\n\n## GPUs for Inference\n\nDeploy AI inference workloads backed by high-end Serverless GPUs in minutes\n\nJoin now\n\nLatest posts\n\n  * A Software Engineer's Tips and Tricks #3: CPU Utilization Is Not Always What It SeemsMay 03, 20242 min\n\n  * A Software Engineer's Tips and Tricks #2: Template Databases in PostgreSQLApr 29, 20244 min\n\n  * What are LLMs? An intro into AI, models, tokens, parameters, weights, quantization and moreApr 25, 20247 min\n\n## Latest posts\n\n  * ### A Software Engineer's Tips and Tricks #3: CPU Utilization Is Not Always What It Seems\n\nJulien Castets\n\nMay 03, 2024\n\n  * ### A Software Engineer's Tips and Tricks #2: Template Databases in PostgreSQL\n\nJulien Castets\n\nApr 29, 2024\n\n  * ### What are LLMs? An intro into AI, models, tokens, parameters, weights, quantization and more\n\nAlisdair Broshar\n\nApr 25, 2024\n\n## Welcome to Koyeb\n\nKoyeb is a developer-friendly serverless platform to deploy any apps globally.\n\n  * Start for free, pay as you grow\n  * Deploy your first app in no time\n\nStart for free\n\nThe fastest way to deploy applications globally.\n\n", "frontpage": false}
