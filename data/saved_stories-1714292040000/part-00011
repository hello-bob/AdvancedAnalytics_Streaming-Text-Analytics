{"aid": "40184160", "title": "Counterintuitive Probabilities: Typical Sets from Information Theory", "url": "https://web.archive.org/web/20200830094206/http://www.jessicayung.com/counterintuitive-probabilities-typical-sets-from-information-theory/", "domain": "archive.org", "votes": 1, "user": "osmarks", "posted_at": "2024-04-27 22:26:58", "comments": 0, "source_title": "Counterintuitive Probabilities: Typical Sets from Information Theory", "source_text": "Counterintuitive Probabilities: Typical Sets from Information Theory | Jessica Yung\n\nThe Wayback Machine - http://www.jessicayung.com/counterintuitive-\nprobabilities-typical-sets-from-information-theory/\n\nJessica Yung Navigation\n\n  * Home\n  * About\n\n    * What I\u2019m doing now\n    * GitHub\n  * Projects\n\n    * Hello Motions\n    * How I\u2019ve been learning to code\n    * Machine Learning Nanodegree\n    * Maths Resources\n    * Tools\n  * Blog\n  * Contact\n\n# The Blog\n\nBlog Counterintuitive Probabilities: Typical Sets from Information Theory\n\n# Counterintuitive Probabilities: Typical Sets from Information Theory\n\nJessica Yung10.2017UncategorizedLeave a Comment\n\nSuppose we have a coin that has a 3/4 chance of landing on heads (call this 0)\nand a 1/4 chance of landing on tails (1). Which of the 16-toss sequences below\nis most likely?\n\n  1. 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  2. 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 0\n  3. 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\nWe came across this question in an Information Theory lecture last week, and\nmany of us thought the second sequence was most likely. It couldn\u2019t be the\nthird sequence since the coin is more likely to land on heads than on tails,\nso the first sequence is more likely than the third. Comparing the first and\nsecond sequences, we thought the second was more likely because it had 3/4\nheads and 1/4 tails, which is what you\u2019d expect from a coin that had a 3/4\nchance of landing on heads and 1/4 chance of landing on tails.\n\nWe were wrong.\n\n### Mathematical Explanation\n\nIf you work out the probability of seeing each of these sequences we have:\n\nSequence 1 =\n\nSequence 2 =\n\nSequence 3 =\n\nSo the first sequence is times more likely than the second one! How can this\nbe?\n\n### Intuitive Explanation\n\n#### It\u2019s a specific sequence, not the entire typical set\n\n(Think of the typical set as the set of outcomes where the fraction of heads\nis similar to the probability of getting heads.)\n\nThe mistake in our intuition was considering the probability of all sequences\nwith twelve heads and four tails as opposed to the specific sequence of twelve\nheads and four tails. The probability of getting any sequence with twelve\nheads and four tails is much larger:\n\nThis is times more likely than seeing the first sequence of all heads!\n\n#### The tosses are independent\n\nHere\u2019s a second way of thinking about it. Look again at our two sequences:\n\n  * 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  * 0 0 0 1 0 1 0 0 0 1 0 0 1 1 0 0\n\nEach toss is independent, i.e. the outcome of toss #3 is not affected by\nwhether toss #2 (or any other toss) was heads or tails. So if we consider toss\n#4, where we get tails in the second sequence and continue to get heads in the\nfirst sequence, we\u2019re actually three times as likely to get heads than tails.\n(since ). We have four tails in the second sequence, so the first sequence is\ntimes as likely as the second.\n\n### The remarkable thing about typical sets\n\nWhat we considered just now \u2013 the set of all 16-toss sequences with 12 heads\nand 4 tails \u2013 is an example of a typical set. It\u2019s \u2018typical\u2019 because the\nproportion of the heads is approximately equal to the probability of getting a\nhead, as you\u2019d expect.\n\nThe remarkable thing is even though the number of elements in the typical set\nis much smaller than the total number of possible sequences, the probability\nof a generated sequence being in the typical set is high.\n\n(If that seems confusing, think of it like there being fifty people in your\nclass. Each lesson you are allocated a partner randomly (perhaps not with\nequal probability), where your partner this lesson does not depend at all on\nwho you were partnered with in previous lessons. Somehow, out of fifty\nlessons, you get paired with person A eleven times. Weird, huh? There is\nlikely something interesting about the way students are paired.)\n\n  * In this example, there are possible sequences. There are sequences in our typical set. That means only around 2.8% of all sequences are in our typical set. But the probability a generated sequence you\u2019ll see belongs to our typical set is a whopping 22.5%!\n\nThis has implications for how we encode information and can help us compress\ndata more effectively. (More on that in a later post.)\n\n### A bit more technical: Typical sets and entropy (recognise that from neural\nnets?)\n\nDisclaimer: I\u2019ve been discussing \u2018the typical set\u2019, but what I actually mean\nis the typical set with and sequence length .\n\nA typical set is actually a set of sequences with length whose probability is\nconcentrated around , where\n\n  * is the entropy of the probability distribution\n\n    * The entropy of a random variable is the amount of uncertainty in the outcome of that random variable.\n    * Related: categorical cross-entropy is the loss function often used for neural network classification tasks!\n  * is related to the amount of deviation in probability from we allow the set of sequences to have.\n\nThus there are many typical sets we can consider.\n\nThere some beautiful results about typical sets. E.g. as the sequence length\ngets longer, the probability a generated sequence is in the typical set\nbecomes closer to 1. We can also put upper and lower bounds on the number of\nelements in the typical set in general.\n\n### In summary\n\nEach individual sequence has low probability, but the fraction-\nof-1s-proportional-to-probability-mass/density sequences as a set have high\nprobability, as you\u2019d expect.\n\nThe remarkable thing is even though the number of elements in the typical set\nis much smaller than the total number of possible sequences, the probability\nof a generated sequence being in the typical set is high.\n\nCredits to Dr. Ramji Venkataramanan for using this example in a 3F7\nInformation Theory lecture.\n\n### Share this:\n\n  * Click to share on Facebook (Opens in new window)\n  * Click to share on LinkedIn (Opens in new window)\n  * Click to share on Twitter (Opens in new window)\n  * Click to share on Google+ (Opens in new window)\n  * Click to email this to a friend (Opens in new window)\n  * Click to print (Opens in new window)\n\n### Like this:\n\nLike Loading...\n\n### Related\n\n### Leave a Reply Cancel reply\n\n#### Top Posts\n\n  * Automate running a script using crontab\n  * How to use pickle to save and load variables in Python\n  * LSTMs for Time Series in PyTorch\n  * Python Lists vs Dictionaries: The space-time tradeoff\n  * How to run scripts in the background\n  * What makes Numpy Arrays Fast: Memory and Strides\n  * How Python implements dictionaries\n  * Code, Explained: Training a model in TensorFlow\n  * Explaining TensorFlow code for a Multilayer Perceptron\n\n#### Recent Comments\n\n  * Jessica Yung on Explaining Tensorflow Code for a Convolutional Neural Network\n  * Jessica Yung on Self-Driving Car Engineer Nanodegree\n  * KL Tah on Self-Driving Car Engineer Nanodegree\n  * Jessica Yung on Self-Driving Car Engineer Nanodegree Term 1 Review\n  * Jessica Yung on Self-Driving Car Engineer Nanodegree Term 1 Review\n\n#### Archives\n\n  * October 2018\n  * September 2018\n  * August 2018\n  * June 2018\n  * March 2018\n  * January 2018\n  * December 2017\n  * October 2017\n  * September 2017\n  * June 2017\n  * May 2017\n  * April 2017\n  * March 2017\n  * February 2017\n  * January 2017\n  * December 2016\n  * November 2016\n  * October 2016\n  * September 2016\n  * August 2016\n  * July 2016\n  * May 2016\n  * April 2016\n  * March 2016\n  * September 2015\n  * July 2015\n  * May 2015\n  * January 2015\n  * November 2014\n  * March 2014\n  * September 2013\n  * December 2012\n  * June 2012\n  * February 2012\n  * June 2010\n  * March 2010\n  * November 2009\n  * October 2009\n\n#### Categories\n\n  * Artificial Intelligence\n  * Careers\n  * Data Science\n  * Economics\n  * Education\n  * Engineering\n  * Entrepreneurship\n  * Highlights\n  * Life\n  * Machine Learning\n  * Mathematics\n  * Poetry\n  * Poetry 2009\n  * Poetry 2010\n  * Programming\n  * Python\n  * Reflections\n  * Self-Driving Car ND\n  * Statistics\n  * Studying\n  * Talk Reviews\n  * Technology Article Summaries\n  * Uncategorized\n  * Writing\n\n#### Meta\n\n  * Log in\n  * Entries feed\n  * Comments feed\n  * WordPress.org\n\n%d bloggers like this:\n\n", "frontpage": false}
