{"aid": "40131900", "title": "Working with Antithesis at MongoDB", "url": "https://antithesis.com/blog/mongo_bug/", "domain": "antithesis.com", "votes": 18, "user": "Klaster_1", "posted_at": "2024-04-23 13:45:10", "comments": 0, "source_title": "Working with Antithesis at MongoDB", "source_text": "Working with Antithesis at MongoDB\n\nWhat is Antithesis? How we\u2019re different How it works See an example\n\nProblems we solve Case studies Working with Antithesis\n\nBackstory Leadership Careers Brand\n\nPricing Docs Blog\n\nLet's talk\n\nDan Gottlieb\n\nFormer Senior Staff Engineer, MongoDB\n\n# Working with Antithesis at MongoDB\n\nApril 22, 2024\n\nThis is a guest post from an Antithesis customer, writing about their own\nexperience testing with us. If you are an Antithesis customer and interested\nin talking about your impressions of our platform, please reach out to us!\n\nMongoDB runs lots of tests: Unit tests and integration tests. Tests which run\nentire tests (they call them \u201cpassthroughs\u201d). Tests that idealize the\nenvironment and tests that inject faults. Tests of client side behavior. Tests\nfor internal consistency after a test is verified as well as dozens of times\nduring a test. Tests for each MongoDB process in a three node replica set.\nTests for each process in a sharded cluster.\n\nThe testing matrix goes on and on (and on).\n\nMongoDB is heavily invested into improving productivity through excellent\ntesting. There\u2019s the normal stuff, like running tests in the cloud against a\npatch and providing back the results in a slick UI. But there\u2019s also cool\ncustom stuff as well: An online test log viewer that can filter and highlight\ninformation, complete with shareable links; or there\u2019s the ability to click a\nbutton that spawns an appropriate cloud machine complete with a coredump and\nthe binaries/debug information for that exact build to inspect it in gdb (with\ncustom data structure walking scripts); or a background service that compares\ntest failures in a patch run with the known failures.\n\nHowever, when so many tests are run, there are also lots of test failures\u2014\u201cred\nboxes.\u201d Many are false positives (bugs in the test itself), others fail to\ncomplete in time due to excessive resource usage or a faulty cloud machine. Of\ncourse, tests also fail because there\u2019s a real code bug. It can be hard to\ntease out which category a red box falls into.\n\nOnly one failure mode is dead simple to identify: data corruption. Data is\nduplicated within a node in the form of indexes and the operation log\u2014as well\nas across nodes in a replica set. Because so much data is derivative, it\u2019s\ntrivial to see when something has gone wrong. An error message saying an index\nkey is missing (or that nodes in a replica set have diverged) is rarely a\nfalse positive. Data corruption is a symptom; the root cause problem already\nhappened. It\u2019s that we\u2019re just now observing the mistake. Unfortunately, the\ntooling for finding the reason for data corruption can only go so far.\n\nDue to the nature of data corruption, all of the fancy tools to inspect test\nlogs and data files provide clues, but not necessarily the answer. Those tools\ncan speed up the process, and remove barriers for gleaning details, but\nthey\u2019ll rarely provide the crucial detail required to fully understand the\nproblem. And until there\u2019s a reproducer to identify the bug and assess its\nimpact, engineers are left investigating a bug that could be infinitely\nbad\u2014and could also take an infinite amount of resources to find the root cause\nof.\n\n## Investigating with Antithesis\n\nI led multiple data corruption investigations during my tenure at MongoDB, but\nthe most memorable was one that was found by our testing partner, Antithesis.\n\nSince 2021, MongoDB has used Antithesis to rigorously test MongoDB's\napplication code. This includes testing MongoDB\u2019s Storage Engine, Core Server,\nCluster-to-Cluster Syncing Utility, and upgrade/downgrade compatibility. The\nAntithesis platform places software in a custom, deterministic hypervisor,\ninjects faults, and uses fuzzing-like techniques to explore code and uncover\nbugs. Antithesis is then integrated into MongoDB at the CI level, to be run as\npart of their release process. This helps MongoDB find bugs before they are\nintroduced in production and impact customers. The Antithesis simulation\nenvironment gives engineers and developers a perfect reproduction of any found\nissues, making triaging failures far more productive for them.\n\nIn this particular test failure, the symptom was an index inconsistency. An\nindex entry existed, but the document it referred to was missing. This can\nhappen for, broadly, two reasons:\n\n  * There can be a bug with index builds. Particularly when the index is being built concurrently with writes to the collection being indexed.\n  * There can be a bug in the storage engine. Outside of index builds, index entries are written transactionally with writes to documents. Thus being able to see them out of sync implies the storage engine lost a write.\n\nThis bug was the latter case. The index in question was the _id index. In\nMongoDB, the _id field is the primary key; that index is created along with\nthe collection. Thus, the index build code was exonerated from any wrongdoing.\n\nThe other observations about this failure were:\n\n  * It happened on the config.transaction collection.\n\n    * This was notable as config.transaction is an internal collection. The rules about data and indexes being updated in transactions still hold.\n    * But, writes to config.transaction happen in different code paths and it has some other peculiar properties. For instance, some updates to documents in the config.transaction collection are elided.\n  * The error was observed soon after a replication rollback.\n\n    * A replication rollback is performed when a node learns it has accepted writes that are not in the leader\u2019s history.\n    * A replication rollback has its own special code paths in the storage engine and replication layers, as well as all the layers in between.\n  * The data files from the end of the test were available, but there were no clues about the missing data. This bug did a good job hiding its tracks.\n\nThis test failure came at an inconvenient time, businesswise. The MongoDB\nserver team was preparing for the next release and was rapidly working through\nloose ends. Still, we immediately reached out and got on a call with\nAntithesis to see what our options were for getting to the bottom of this.\n\nAntithesis came prepared. They had rerun their simulations, and had not only\nreproduced the bug, but found the exact time at which it had occurred. How was\nthis possible?\n\nAntithesis doesn\u2019t just run a test from start to finish one time\u2014and it\ndoesn\u2019t just run a test from start to finish multiple times, either. It runs a\nsimulation for some amount of time and takes a \u201ccheckpoint.\u201d It will then add\nentropy and explore multiple paths rooted at that checkpoint. If one path\nkeeps leading to the same error state, it can record that as a problem and\nspend resources exploring other paths instead.\n\nThis testing technique is super powerful for software like MongoDB. The most\nconcerning bugs that will affect customers are those that happen when there\u2019s\nlots of data. And inserting data not only takes time, it takes time away from\ntesting the operations we want to do on that data. Longer-running tests can\nfind more problems, but reproducing those problems becomes harder as the\niteration cycle goes from a couple minutes to an entire workday. Antithesis\u2019s\ncheckpointing and branching algorithm naturally spends more resources\nexploring branches after the MongoDB database has been running long enough to\nget itself into more interesting states.\n\nUltimately, what Antithesis told us was that they had an analysis that\nsummarized the probability of a failure from any given checkpoint. They\nexplained that when the probability spikes between adjacent checkpoints, we\nknow with confidence that the bug happened in that time window.\n\nThis was useful but unfortunately, it still wasn\u2019t good enough. Based on the\nobservations I had made from the logs and prior experience of where we\u2019d had\nthese storage engine bugs before, I already knew the issue would be in the\nstorage engine code related to replication rollback\u2014and that was indeed what\nAntithesis\u2019s analysis was pointing at. So, while I was super impressed with\nthe quality of the results Antithesis\u2019s software produced, I still didn\u2019t know\nwhy the bug was occuring, even if we now were sure of when and where.\n\nSo what else could we do? Could we add logging to the database, recompile and\nrerun the experiment? Not exactly. Antithesis reproduces failures by\nreproducing the exact (simulated) timing of every instruction the CPU\nprocesses, such that context switching happens at precisely the same time.\nAdding or removing instructions would have a butterfly effect on the outcome.\nTherefore, instrumenting the binary with diagnostics was off the table... What\nelse was there? Hint: the answer wasn\u2019t glamorous.\n\nWe asked for core dumps for every 100 milliseconds within a two second range\naround the bug.\n\nThe ask wasn\u2019t crazy for their software, but that didn\u2019t mean it was simple to\ndo. At that time, each run had to be done by hand (and we were heading into\nthe weekend). It took a while, but we got them.\n\nAnd those core dumps were gold.\n\n## Diving into the core dumps\n\nSo what information was in these core dumps that would provide a clue? First,\nwe had to identify what to look for, and when to look for it. First, we\nanalyzed the data files left behind after the test exited. The failure we were\nseeing was an index inconsistency with a collection on the _id index\n(MongoDB's primary key). What the error was telling us was that the index\nentry was referencing a document that was no longer in the system. Looking at\nthe oplog (the operation log, or history of changes) from the leftover data\nfiles, we could tell the document was never intentionally deleted. But our\nfirst interesting clue was that this document was updated a lot\u2014thousands of\ntimes in rapid succession, a state that doesn't happen often in our normal\ntesting operation. Lots of writes slow down tests; and our test suite is very\nlarge and slow to fully execute.\n\nThe other information we had before examining the core dumps were the MongoDB\nlogs that the affected node generated before the failure. It had just gone\nthrough a replication rollback. Due to the distributed nature of a replica\nset, data is first committed locally on a primary node, then replicated to\nother secondaries. When a majority of nodes have locally committed the same\nwrite, we say that write is now majority committed. If a failure happens after\ndata is locally committed, but before becoming majority committed, that data\nis subject to being erased via a replication rollback. The heavy lifting for\nthis is also supported by WiredTiger and is heavily optimized by necessity .\nWhich incurs some correctness risks.\n\nArmed with those observations, we wanted to get core dumps right around that\nreplication rollback event\u2014and we wanted to check the state of the document\nthat went missing. Getting the exact time was challenging but the people at\nAntithesis did a remarkable job working with us running multiple experiments a\nday to catch the bug in the act. A core dump that came in a fraction of a\nsecond before the replication rollback event looked completely fine. A core\ndump that came in a fraction of a second after the replication rollback had\nlost the data and there was no trace where it went.\n\nBut, we did get a core dump at just the right time. There was something a bit\ndifferent. But to understand, we first need to talk about MVCC, multiversion\nconcurrency control.\n\nIn a simpler database, when a document is updated one could lock the document,\nupdate it in place, then release the lock. But there are two frequent\ncomplaints about that strategy:\n\n  * Taking a lock blocks other operations trying to access that document. (There may be older readers in the system that need access to the old version of the document.)\n  * Updating documents in place is (broadly) incompatible with offering snapshot isolation.\n\nSo, many database storage engines instead do a \"copy on write\" algorithm for\nupdates to a document. This can be done without requiring locks on the\ndocument, so that the previous version of the document is available for older\nreaders that want it.\n\nBut every advanced technique tends to introduce new complexity. The new\nproblem MVCC introduces is that the database has to keep all of these old\ndocuments around and delete them when they are no longer needed. What if the\ndatabase process is running out of memory? The answer may seem obvious\u2014we\nwrite it to disk of course!\n\nThis core dump in the Goldilocks zone caught WiredTiger in this memory\nreclamation state. It had moved the thousands of versions of this affected\ndocument into a structure suitable for writing it to disk. And in that state\nwe made an observation: the historical contents of this document was being\nsplit across multiple WiredTiger containers. With these last key ingredients,\nwe were able to create an isolated reproducer.\n\n#### A solution is found\n\nAfter some calibration on timing, we had a great lead. We caught the storage\nengine replication rollback algorithm in a window no wider than 10ms. It had\ncompleted its first phase and the data in question was still present and\ncorrect. We narrowed the problem down to the second half of the algorithm and\nwe had the exact input that would lead to an error.\n\nThe analysis Antithesis had provided us was so useful, that they ended up\nturning it into a full-fledged product. What they had created with us was a\nprototype version of what has become their bug report.\n\nThis wasn\u2019t my bug to diagnose; that code wasn\u2019t my expertise. I handed off\nthe information to Hari (a rockstar) in the Sydney office. My JIRA inbox\nwelcomed me Monday morning with a diagnosis, a reproducer and an example fix,\nconfirming the diagnosis. See here for the gory details on this gnarly bug.\n\nSince that investigation, through to my last days at MongoDB, Antithesis was\nan incredible partner to work with. They continued to improve how their\ntesting explores paths to maximize coverage. They helped onboard more of our\ntests into their system. They came to the MongoDB office and talked about\ntheir technology (which our engineers loved hearing about). But the other\n(arguably larger) half of what makes Antithesis great is the company's\ncommitment to their customers' success.\n\n\u00a9 Antithesis Operations LLC Privacy policy Terms of use Brand\n\n", "frontpage": true}
