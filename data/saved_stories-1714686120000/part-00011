{"aid": "40235449", "title": "Is Model Collapse Inevitable?", "url": "https://arxiv.org/abs/2404.01413", "domain": "arxiv.org", "votes": 2, "user": "tosh", "posted_at": "2024-05-02 12:36:02", "comments": 0, "source_title": "Is Model Collapse Inevitable? Breaking the Curse of Recursion by Accumulating Real and Synthetic Data", "source_text": "[2404.01413] Is Model Collapse Inevitable? Breaking the Curse of Recursion by\nAccumulating Real and Synthetic Data\n\nSkip to main content\n\nWe gratefully acknowledge support from the Simons Foundation, member\ninstitutions, and all contributors. Donate\n\n> cs > arXiv:2404.01413\n\n# Computer Science > Machine Learning\n\narXiv:2404.01413 (cs)\n\n[Submitted on 1 Apr 2024 (v1), last revised 29 Apr 2024 (this version, v2)]\n\n# Title:Is Model Collapse Inevitable? Breaking the Curse of Recursion by\nAccumulating Real and Synthetic Data\n\nAuthors:Matthias Gerstgrasser, Rylan Schaeffer, Apratim Dey, Rafael Rafailov,\nHenry Sleight, John Hughes, Tomasz Korbak, Rajashree Agrawal, Dhruv Pai,\nAndrey Gromov, Daniel A. Roberts, Diyi Yang, David L. Donoho, Sanmi Koyejo\n\nView a PDF of the paper titled Is Model Collapse Inevitable? Breaking the\nCurse of Recursion by Accumulating Real and Synthetic Data, by Matthias\nGerstgrasser and 13 other authors\n\nView PDF HTML (experimental)\n\n> Abstract:The proliferation of generative models, combined with pretraining\n> on web-scale data, raises a timely question: what happens when these models\n> are trained on their own generated outputs? Recent investigations into\n> model-data feedback loops proposed that such loops would lead to a\n> phenomenon termed model collapse, under which performance progressively\n> degrades with each model-data feedback iteration until fitted models become\n> useless. However, those studies largely assumed that new data replace old\n> data over time, where an arguably more realistic assumption is that data\n> accumulate over time. In this paper, we ask: what effect does accumulating\n> data have on model collapse? We empirically study this question by\n> pretraining sequences of language models on text corpora. We confirm that\n> replacing the original real data by each generation's synthetic data does\n> indeed tend towards model collapse, then demonstrate that accumulating the\n> successive generations of synthetic data alongside the original real data\n> avoids model collapse; these results hold across a range of model sizes,\n> architectures, and hyperparameters. We obtain similar results for deep\n> generative models on other types of real data: diffusion models for molecule\n> conformation generation and variational autoencoders for image generation.\n> To understand why accumulating data can avoid model collapse, we use an\n> analytically tractable framework introduced by prior work in which a\n> sequence of linear models are fit to the previous models' outputs. Previous\n> work used this framework to show that if data are replaced, the test error\n> increases with the number of model-fitting iterations; we extend this\n> argument to prove that if data instead accumulate, the test error has a\n> finite upper bound independent of the number of iterations, meaning model\n> collapse no longer occurs.\n\nSubjects:| Machine Learning (cs.LG); Artificial Intelligence (cs.AI);\nComputation and Language (cs.CL); Emerging Technologies (cs.ET); Machine\nLearning (stat.ML)  \n---|---  \nCite as:| arXiv:2404.01413 [cs.LG]  \n(or arXiv:2404.01413v2 [cs.LG] for this version)  \nhttps://doi.org/10.48550/arXiv.2404.01413arXiv-issued DOI via DataCite  \n  \n## Submission history\n\nFrom: Matthias Gerstgrasser [view email] [v1] Mon, 1 Apr 2024 18:31:24 UTC\n(5,804 KB) [v2] Mon, 29 Apr 2024 23:13:42 UTC (6,472 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Is Model Collapse Inevitable? Breaking the\nCurse of Recursion by Accumulating Real and Synthetic Data, by Matthias\nGerstgrasser and 13 other authors\n\n  * View PDF\n  * HTML (experimental)\n  * TeX Source\n  * Other Formats\n\nview license\n\nCurrent browse context:\n\ncs.LG\n\n< prev | next >\n\nnew | recent | 2404\n\nChange to browse by:\n\ncs cs.AI cs.CL cs.ET stat stat.ML\n\n### References & Citations\n\n  * NASA ADS\n  * Google Scholar\n  * Semantic Scholar\n\na export BibTeX citation Loading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer (What is the Explorer?)\n\nLitmaps (What is Litmaps?)\n\nscite Smart Citations (What are Smart Citations?)\n\n# Code, Data and Media Associated with this Article\n\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\n\nDagsHub (What is DagsHub?)\n\nGotit.pub (What is GotitPub?)\n\nPapers with Code (What is Papers with Code?)\n\nScienceCast (What is ScienceCast?)\n\n# Demos\n\nReplicate (What is Replicate?)\n\nHugging Face Spaces (What is Spaces?)\n\nTXYZ.AI (What is TXYZ.AI?)\n\n# Recommenders and Search Tools\n\nInfluence Flower (What are Influence Flowers?)\n\nConnected Papers (What is Connected Papers?)\n\nCORE Recommender (What is CORE?)\n\nIArxiv Recommender (What is IArxiv?)\n\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new\narXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and\naccepted our values of openness, community, excellence, and user data privacy.\narXiv is committed to these values and only works with partners that adhere to\nthem.\n\nHave an idea for a project that will add value for arXiv's community? Learn\nmore about arXivLabs.\n\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n\n  * About\n  * Help\n\n  * Contact\n  * Subscribe\n\n  * Copyright\n  * Privacy Policy\n\n  * Web Accessibility Assistance\n  * arXiv Operational Status Get status notifications via email or slack\n\n", "frontpage": false}
