{"aid": "40130317", "title": "AWS for Data Engineers: Conquer the Cloud in 90 Days", "url": "https://datagibberish.com/p/aws-in-90-days", "domain": "datagibberish.com", "votes": 1, "user": "ivanovyordan", "posted_at": "2024-04-23 10:26:29", "comments": 0, "source_title": "AWS for Data Engineers: Conquer the Cloud in 90 Days", "source_text": "AWS for Data Engineers: Conquer the Cloud in 90 Days\n\n# Data Gibberish\n\nShare this post\n\n#### AWS for Data Engineers: Conquer the Cloud in 90 Days\n\ndatagibberish.com\n\n# AWS for Data Engineers: Conquer the Cloud in 90 Days\n\n### A 30-60-90 plan to kickstart your cloud journey with key AWS tools\n\nYordan Ivanov\n\nApr 17, 2024\n\n30\n\nShare this post\n\n#### AWS for Data Engineers: Conquer the Cloud in 90 Days\n\ndatagibberish.com\n\n4\n\nShare\n\nThe cloud has transformed how we handle data, and AWS has been at the\nforefront of this change. With a market share of more than 30% and a suite of\n200+ services, AWS has become the go-to platform for data engineers.\n\nBut where do you begin? How do you navigate this ecosystem and harness its\npower?\n\nI crafted a comprehensive 30-60-90 day plan to help you conquer AWS dive in\nthe modern cloud.\n\nThis is not a course, not even a tutorial. What you will read here is a guide.\nA game plan if you wish. I am leaving all the reading and practice to you.\nAfter all, there's no sense in bragging about a project developed by somebody\nelse.\n\nThat said, please let me know how it is going for you. Ask questions, and\nshare your progress. I want you to succeed in your learning.\n\nReading time: 7 minutes\n\nThanks for reading Data Gibberish! Subscribe for free to receive new posts and\nsupport my work.\n\n##\n\n\ud83d\uddfa\ufe0f The Plan\n\nNow, let's be clear. You cannot become an AWS expert in only 90 days. But I\ncan promise you can become competent. You will need to dedicate a significant\namount of your time. I recommend spending at least 10 hours per week.\n\nBut wait, there is more! I prepared a FREE Notion template for you to\nduplicate and track your learning progress.\n\nSteal Your AWS Learning Plan Now\n\nAlso, remember that not every service in AWS has a free tier. It might cost\nyou some cash to run everything. There are free tools like LocalStack that\nemulate AWS well. Yet, I recommend using the real thing and shutting down your\nservices when you finish using them for the day.\n\n##\n\n\ud83c\udf92 Prerequisites\n\nThere's nothing you need to know upfront. Yet, it would help if you know how\nto use a text editor and have some understanding of Python and SQL.\n\nAlso, it would help if you already know about a project you want to build. In\na perfect world, you'd have a way to produce some data. You can use IFTTT to\ntrack your GitHub activity, phone calls, or even how often you go to the\nbathroom.\n\nIf you don't want to spend time producing data or wish to avoid tracking\npersonal data, you can rely on AWS. Browse the Registry of Open Data on AWS,\npick some sources, and think of an exciting project to build.\n\nNow, let's discuss the plan.\n\n##\n\n\ud83e\udd5a Days 1-30: AWS Basics\n\nThe first month of your project is about understanding the basics. Here, you\nwill learn some core services and understand how AWS works.\n\n###\n\nProject Part #1: Build a Data Lake on S3 \ud83e\udea3\n\nThe first and most important service you must learn is S3. S3 is not yet\nanother service for file storage. It is de facto the standard for cloud\nstorage. No matter what you do in your career, you will use the S3 standard.\n\nHere is what you need to do to learn S3:\n\n  * Learn S3 basics - buckets, objects, storage classes, permissions\n\n  * Create an S3 bucket and upload sample datasets\n\n  * Define a folder structure to organise data like a data lake\n\n  * Configure lifecycle policies to transition data to cheaper storage\n\n###\n\nProject Part #2: Analyse Data with Athena \ud83d\udd0e\n\nThat's why you should start analysing the data you stored in S3. You will\nbegin with Athena for the analyses. Later, you will transition to QuickSight\nto make a pretty dashboard.\n\nThe second part of your 30 days is a bit easier from a technical point of view\nbut more complex from a UX standpoint:\n\n  * Learn Athena basics - databases, tables, querying data\n\n  * Create an Athena database and define table schemas on top of the S3 data\n\n  * Write SQL queries to analyse the data and output results back to S3\n\n  * Visualise query results in QuickSight\n\nOnce you finish the first 30 days of your project, you will have a functioning\ndata platform. You will have a way to store data and a way to analyse it.\nLet's meet at the intermediate level.\n\n##\n\n\ud83d\udc23 Days 31-60: Intermediate Skills\n\nDuring the second month of your AWS journey, you will explore data engineering\nterritories. You will learn everything you need to know about ETL and\nwarehouse services.\n\n###\n\nProject Part #3: Data Processing Pipelines with Glue and Kinesis \ud83d\udeb0\n\nHere, you will learn about catalogues and transformations with Glue. You will\nalso learn how to stream data and process events Kinesis.\n\nHere's Here'san for Part 3:\n\n  * Learn Glue basics - crawlers, jobs, workflows\n\n  * Use Glue Crawlers to discover schemas and create a Data Catalog\n\n  * Write Glue ETL jobs in Python to transform data from landing to processed zone\n\n  * Learn Kinesis basics - streams, shards, producers, consumers\n\n  * Ingest streaming data into Kinesis and use Kinesis Firehose to land it in S3\n\n  * Trigger Glue jobs to process the streaming data using Kinesis Analytics\n\n###\n\nProject Part #4: Data Warehousing with Redshift \ud83d\uddc4\ufe0f\n\nPart 4 is all about warehousing. With Redshift, you can make your reports\nfaster and cheaper. It's not the best warehouse for larger organisations, but\nit is great for smaller projects.\n\nCheck the plan for Part 4:\n\n  * Learn Redshift basics - clusters, nodes, distribution styles\n\n  * Provision a Redshift cluster and create tables in it\n\n  * Use Glue to ETL data from S3 into Redshift\n\n  * Write efficient SQL to query and join large tables\n\n  * Connect QuickSight to Redshift for reporting and dashboarding\n\nFinishing the second 30 days of your AWS learning means you understand AWS\nwell. You can be confident in your skills and showcase your project.\n\n#### Migrating from Redshift to Snowflake\n\nYordan Ivanov\n\n\u00b7\n\nJune 7, 2020\n\nRead full story\n\n##\n\n\ud83d\udc25 Days 61-90: Advanced Topics\n\nYou will step into advanced areas during the last month of your learning. Most\nof what you will do here is to improve efficiency and decrease costs.\n\n###\n\nProject Part #5: Big Data Processing with EMR \u2728\n\nIn Part 5 of your AWS learning project, you must focus on ERM and Spark. As\nmentioned, you can run Spark with Glue, but EMR gives you much more control\nover the infrastructure.\n\nLet's cLet'syour learning plan:\n\n  * Learn EMR basics - clusters, nodes, steps\n\n  * Provision an EMR cluster with Spark\n\n  * Write PySpark to process data in S3\n\n  * Optimise Spark for performance through partitioning, file formats, caching\n\n  * Output aggregated results to S3 and visualise in QuickSight\n\n###\n\nProject Part #6: Cost Optimisation \ud83d\udcb8\n\nYou know how people say the devil is in the details? This saying applies to\nthe last part of your project. This part is about taking small steps to reduce\ncosts and show your AWS mastery.\n\n  * Learn to track and analyse costs using Cost Explorer and Budgets\n\n  * Identify the most significant cost drivers and implement optimisations\n\n  * Right-size Redshift and EMR clusters based on workload\n\n  * Leverage Spot instances and Auto Scaling to reduce compute costs\n\n  * Delete unused resources and automate cost-saving practices\n\n##\n\n\ud83c\udfc1 Summary\n\nBy the end of the 90 days, you will have built an end-to-end data platform on\nAWS. You will have everything from ingestion to storage to processing to\nvisualisation. You'll have hands-on experience with the core services. You\nwill also know how to optimise for cost and performance.\n\nBut this is the beginning of your AWS journey. The cloud evolves, and there's\nmore to learn and explore. Keep pushing yourself to stay up-to-date with the\nlatest services and best practices.\n\nTake the skills and knowledge you gain over these 90 days and put them into\npractice. Build something unique, solve real-world problems, and showcase your\nexpertise. The cloud is your playground, and the possibilities are endless.\n\nThank you for reading Data Gibberish. This post is public so feel free to\nshare it.\n\nShare\n\n##\n\n\ud83d\udcda Picks of the Week\n\n  * I\u2019ve been planning an article about setting up your data engineering dev environment. This piece by\n\nJordan Cutler\n\nis exceptionally good. (link)\n\n  * Also, the latest article by\n\nElla Pham\n\nis focused on DataOps. This is all we talk about here, too. (link)\n\n  * And, as we are talking about learning roadmaps, here is an outstanding text by\n\nZach Wilson\n\n. It\u2019s a must-read if you want to break into data in 2024. (link)\n\n##\n\n\ud83d\ude0d How Am I Doing?\n\nI love hearing from readers and am always looking for feedback. How am I doing\nwith Data Gibberish? Is there anything you\u2019d like to see more or less of*?\nWhich aspects of the newsletter do you enjoy the most?*\n\nHit reply and say hello. I\u2019d love to hear from you!\n\n### Subscribe to Data Gibberish\n\nBy Yordan Ivanov\n\nLearn valuable DataOps skills through actionable frameworks delivered to your\ninbox every week.\n\n30 Likes\n\n\u00b7\n\n1 Restack\n\n30\n\nShare this post\n\n#### AWS for Data Engineers: Conquer the Cloud in 90 Days\n\ndatagibberish.com\n\n4\n\nShare\n\n4 Comments\n\nJordan CutlerHigh Growth EngineerApr 19Liked by Yordan IvanovThank you so much\nfor the shoutout, Yordan!Expand full commentLike (1)ReplyShare  \n---  \n  \n1 reply by Yordan Ivanov\n\nElla PhamData & BeyondApr 17Liked by Yordan IvanovGreat comprehensive article!\nAnd thanks for the mention as well!Expand full commentLike (1)ReplyShare  \n---  \n  \n1 reply by Yordan Ivanov\n\n2 more comments...\n\nHow I Interview Data Engineers\n\nHere\u2019s what you want to do if you want to work with me\n\nDec 13, 2023 \u2022\n\nYordan Ivanov\n\n15\n\nShare this post\n\n#### How I Interview Data Engineers\n\ndatagibberish.com\n\nSelf-Service BI Is a Lie: 3 Problems You Can Resolve Today And Improve It\n\nEnhancing collaboration between data teams and stakeholders\n\nApr 3 \u2022\n\nYordan Ivanov\n\n21\n\nShare this post\n\n#### Self-Service BI Is a Lie: 3 Problems You Can Resolve Today And Improve It\n\ndatagibberish.com\n\nZero to Hero: Mastering Change Data Capture For Remarkable Database\nIntegrations\n\nAll you need to know to talk about CDC confidently\n\nFeb 28 \u2022\n\nYordan Ivanov\n\n19\n\nShare this post\n\n#### Zero to Hero: Mastering Change Data Capture For Remarkable Database\nIntegrations\n\ndatagibberish.com\n\n2\n\nReady for more?\n\n\u00a9 2024 Yordan Ivanov\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
