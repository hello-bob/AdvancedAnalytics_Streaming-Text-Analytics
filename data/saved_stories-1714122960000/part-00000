{"aid": "40165638", "title": "Amazon Virtual Try-all: A way to virtually try out products", "url": "https://www.amazon.science/blog/virtual-try-all-visualizing-any-product-in-any-personal-setting", "domain": "amazon.science", "votes": 1, "user": "dkpk", "posted_at": "2024-04-26 03:14:58", "comments": 0, "source_title": "Virtual try-all: Visualizing any product in any personal setting", "source_text": "Virtual try-all: Visualizing any product in any personal setting - Amazon\nScience\n\nSubscribe\n\nFollow Us\n\n  * twitter\n  * instagram\n  * youtube\n  * facebook\n  * linkedin\n  * github\n  * rss\n\n  * Research areas\n\n    * Automated reasoning\n    * Cloud and systems\n    * Computer vision\n    * Conversational AI\n    * Economics\n    * Information and knowledge management\n    * Machine learning\n    * Operations research and optimization\n    * Quantum technologies\n    * Robotics\n    * Search and information retrieval\n    * Security, privacy, and abuse prevention\n    * Sustainability\n    * Automated reasoning\n    * Cloud and systems\n    * Computer vision\n    * Conversational AI\n    * Economics\n    * Information and knowledge management\n    * Machine learning\n    * Operations research and optimization\n    * Quantum technologies\n    * Robotics\n    * Search and information retrieval\n    * Security, privacy, and abuse prevention\n    * Sustainability\n\n  * Blog\n\n  * Publications\n\n  * Conferences\n\n  * Code and datasets\n\n  * Academia\n\n    * Alexa Prize\n    * Academics at Amazon\n    * Amazon Research Awards\n    * Research collaborations\n    * Alexa Prize\n    * Academics at Amazon\n    * Amazon Research Awards\n    * Research collaborations\n\n  * Careers\n\nSubscribe\n\nComputer vision\n\n# Virtual try-all: Visualizing any product in any personal setting\n\n## First model to work across a wide range of products uses a second U-Net\nencoder to capture fine-grained product details.\n\nBy Karim Bouyarmane\n\nApril 16, 2024\n\nShare\n\nShare\n\n  * Copy link\n  * Email\n  * Twitter\n  * LinkedIn\n  * Facebook\n  * Line\n  * Reddit\n  * QZone\n  * Sina Weibo\n  * WeChat\n  * WhatsApp\n\n\u5206\u4eab\u5230\u5fae\u4fe1\n\nA way for online shoppers to virtually try out products is a sought-after\ntechnology that can create a more immersive shopping experience. Examples\ninclude realistically draping clothes on an image of the shopper or inserting\npieces of furniture into images of the shopper\u2019s living space.\n\nRelated content\n\nAdapting neural radiance fields (NeRFs) to dynamic scenes\n\nRepresenting light and density fields as weighted sums over basis functions,\nwhose weights vary over time, improves motion capture, texture, and lighting.\n\nIn the clothing category, this problem is traditionally known as virtual try-\non; we call the more general problem, which targets any category of product in\nany personal setting, the virtual try-all problem.\n\nIn a paper we recently posted in arXiv, we presented a solution to the\nvirtual-try-all problem called Diffuse-to-Choose (DTC). Diffuse-to-Choose is a\nnovel generative-AI model that allows users to seamlessly insert any product\nat any location in any scene.\n\nThe customer starts with a personal scene image and a product and draws a mask\nin the scene to tell the model where to insert the object. The model then\nintegrates the item into the scene, with realistic angles, lighting, shadows,\nand so on. If necessary, the model infers new perspectives on the item, and it\npreserves the item\u2019s fine-grained visual-identity details.\n\nVideo Player is loading.\n\nCurrent Time 0:00\n\n/\n\nDuration 1:27\n\nLoaded: 0%\n\nStream Type LIVE\n\nRemaining Time -1:27\n\n1x\n\n  * Chapters\n\n  * descriptions off, selected\n\n  * captions settings, opens captions settings dialog\n  * captions off, selected\n\nThis is a modal window.\n\nBeginning of dialog window. Escape will cancel and close the window.\n\nEnd of dialog window.\n\nDiffuse-to-choose\n\nNew \"virtual try-all\" method works with any product, in any personal setting,\nand enables precise control of image regions to be modified.\n\nThe Diffuse-to-Choose model has a number of characteristics that set it apart\nfrom existing work on related problems. First, it is the first model to\naddress the virtual-try-all problem, as opposed to the virtual-try-on problem:\nit is a single model that works across a wide range of product categories.\nSecond, it doesn\u2019t require 3-D models or multiple views of the product, just a\nsingle 2-D reference image. Nor does it require sanitized, white-background,\nor professional-studio-grade images: it works with \u201cin the wild\u201d images, such\nas regular cellphone pictures. Finally, it is fast, cost effective, and\nscalable, generating an image in approximately 6.4 seconds on a single AWS\ng5.xlarge instance (NVIDIA A10G with 24GB of GPU memory).\n\n1 of 5\n\n\u2014 Sofas, superimposed on a source image\n\n2 of 5\n\n\u2014 Dresses, superimposed on a source image behind the model's crossed arms,\nwhich remain in the foreground\n\n3 of 5\n\n\u2014 Easy chairs, rotated to preserve perspective, and with the appearance of\ntheir backs inferred, superimposed on a source image\n\n4 of 5\n\n\u2014 Men's pants, superimposed on a source image\n\n5 of 5\n\n\u2014 Women's tops, superimposed on a source image\n\nUnder the hood, Diffuse-to-Choose is an inpainting latent-diffusion model,\nwith architectural enhancements that allow it to preserve products\u2019 fine-\ngrained visual details. A diffusion model is one that\u2019s incrementally trained\nto denoise increasingly noisy inputs, and a latent-diffusion model is one in\nwhich the denoising happens in the model\u2019s representation (latent) space.\nInpainting is a technique in which part of an image is masked, and the latent-\ndiffusion inpainting model is trained to fill in (\u201cinpaint\u201d) the masked region\nwith a realistic reconstruction, sometimes guided by a text prompt or an image\nreference.\n\nDiffuse-to-choose allows customers to control virtual-try-on features such as\nsleeve length and whether shirts are worn tucked or untucked, simply by\nspecifying the region of the image to be modified.\n\nLike most inpainting models, DTC uses an encoder-decoder model known as a\nU-Net to do the diffusion modeling. The U-Net\u2019s encoder consists of a\nconvolutional neural network, which divides the input image into small blocks\nof pixels and applies a battery of filters to each block, looking for\nparticular image features. Each layer of the encoder steps down the resolution\nof the image representation; the decoder steps the resolution back up. (The\nU-shaped curve describing the resolution of the representation over successive\nlayers gives the network its name.)\n\nRelated content\n\nKnowledge distillation method for better vision-language models\n\nMethod preserves knowledge encoded in teacher model\u2019s attention heads even\nwhen student model has fewer of them.\n\nOur main innovation is to introduce a secondary U-Net encoder into the\ndiffusion process. The input to this encoder is a rough copy-paste collage in\nwhich the product image, resized to match the scale of the background scene,\nhas been inserted into the mask created by the customer. It\u2019s a very crude\napproximation of the desired output, but the idea is that the encoding will\npreserve fine-grained details of the product image, which the final image\nreconstruction will incorporate.\n\nWe call the secondary encoder\u2019s output a \u201chint signal\u201d. Both it and the output\nof the primary U-Net\u2019s encoder pass to a feature-wise linear-modulation (FiLM)\nmodule, which aligns the features of the two encodings. Then the encodings\npass to the U-Net decoder.\n\nThe Diffuse-to-Choose (DTC) architecture, with sample input and output. The\nmain difference between DTC and a typical inpainting diffusion model is the\nsecond U-Net encoder that produces a \u201chint signal\u201d that carries additional\ninformation about details of the product image.\n\nWe trained Diffuse-to-Choose on AWS p4d.24xlarge instances (with NVIDIA A100\n40GB GPUs), with a dataset of a few million pairs of public images. In\nexperiments, we compared its performance on the virtual-try-all task to those\nof four different versions of a traditional image-conditioned inpainting\nmodel, and we compared it to the state-of-the-art model on the more-\nspecialized virtual-try-on task.\n\nRelated content\n\nBuilding geospatial foundation models via continual pretraining\n\nNew approach enables sustainable machine learning for remote-sensing\napplications.\n\nIn addition to human-based qualitative evaluation of similarity and semantic\nblending, we used two quantitative metrics to assess performance: CLIP\n(contrastive language-image pretraining) score and the Fr\u00e9chet inception\ndistance (FID), which measures the realism and diversity of generated images.\nOn the virtual-try-all task, DTC outperformed all four image-conditioned\ninpainting baselines on both metrics, with a margin of 9% in FID over the\nbest-performing baseline.\n\nOn the virtual-try-on task, DTC was comparable to the baseline \u2014 slightly\nhigher in CLIP score (90.14 vs. 90.11), but also slightly higher in FID, where\nlower is better (5.39 vs. 5.28). But given DTC\u2019s generality, performing\ncomparably to a special-purpose model on its specialized task is a substantial\nachievement. Finally, we demonstrate that DTC\u2019s results are comparable in\nquality to those of order-of-magnitude more-expensive methods based on few-\nshot fine-tuning on every product, like our previous DreamPaint method.\n\nResearch areas\n\n  * Computer vision\n\nTags\n\n  * Diffusion modeling\n  * Generative AI\n\nAbout the Author\n\nKarim Bouyarmane\n\nKarim Bouyarmane is a senior manager of applied science for the Amazon Store.\n\n## Related content\n\n  * Adapting neural radiance fields (NeRFs) to dynamic scenes\n\nSameera Ramasinghe\n\nFebruary 26, 2024\n\nRepresenting light and density fields as weighted sums over basis functions,\nwhose weights vary over time, improves motion capture, texture, and lighting.\n\nComputer vision\n\n  * Amazon Web Services releases two new Titan vision-language models\n\nLarry Hardesty\n\nDecember 20, 2023\n\nNovel architectures and carefully prepared training data enable state-of-the-\nart performance.\n\nComputer vision\n\n  * Do large language models understand the world?\n\nMatthew Trager, Stefano Soatto\n\nFebruary 15, 2024\n\nIn addition to its practical implications, recent work on \u201cmeaning\nrepresentations\u201d could shed light on some old philosophical questions.\n\nConversational AI\n\n## Work with us\n\nSee more jobs See more jobs\n\nIntern - Economics, JP Retail\n\nUS, WA, Seattle\n\nThe JP Economics and Decision Science Team is looking for an Intern Economist\nwith experience in empirical... Read more\n\nSenior Applied Scientist, Fulfillment by Amazon\n\nUS, WA, Bellevue\n\nThe Fulfillment by Amazon (FBA) team is looking for a passionate, curious, and\ncreative Senior Applied Scientist, with... Read more\n\nApplied Scientist, FBA Science\n\nUS, WA, Bellevue\n\nThe Fulfillment by Amazon (FBA) team is looking for a passionate, curious, and\ncreative Applied Scientist, with expertise... Read more\n\nApplied Scientist, Outbound Communications, Traffic and Marketing Tech\n\nUS, WA, Seattle\n\nOutbound Communications own the worldwide charter for delighting our customers\nwith timely, relevant notifications (... Read more\n\nApplied Scientist, Artificial General Intelligence\n\nUS, WA, Seattle\n\nThe Artificial General Intelligence (AGI) team is looking for a passionate,\ntalented, and inventive Applied... Read more\n\nSenior Economist, Economic Decision Science\n\nGB, London\n\nEconomic Decision Science is a central science team working across a variety\nof topics in the EU Stores business and... Read more\n\nApplied Scientist, Alexa Shopping\n\nUS, WA, Seattle\n\nWe\u2019re working to improve shopping on Amazon using the conversational\ncapabilities of LLMs, and are searching for... Read more\n\nApplied Scientist, AWS Transcribe\n\nUS, WA, Seattle\n\nAmazon is looking for a passionate, talented, and inventive Applied Scientist\nwith background in Natural Language... Read more\n\nApplied Scientist, Automated Reasoning Group\n\nUS, WA, Seattle\n\nThe Automated Reasoning Group in AWS Platform is looking for an Applied\nScientist with experience in building... Read more\n\nApplied Scientist*, Japan Store Tech\n\nCN, 11, Beijing\n\nAmazon Search JP builds features powering product search on the Amazon JP\nshopping site and expands the innovations... Read more\n\nGet more from Amazon Science\n\nSubscribe to our monthly newsletter\n\nAmazon.com | Conditions of Use | Privacy | \u00a9 1996-2024 Amazon.com, Inc. or its affiliates\n\nFollow Us\n\n  * twitter\n  * instagram\n  * youtube\n  * facebook\n  * linkedin\n  * github\n  * rss\n\n", "frontpage": false}
