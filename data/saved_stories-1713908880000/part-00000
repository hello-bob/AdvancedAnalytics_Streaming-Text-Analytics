{"aid": "40133257", "title": "Quantum Computers Can Now Run Powerful AI That Works Like the Brain", "url": "https://www.scientificamerican.com/article/quantum-computers-can-run-powerful-ai-that-works-like-the-brain/", "domain": "scientificamerican.com", "votes": 1, "user": "LinuxBender", "posted_at": "2024-04-23 15:45:36", "comments": 0, "source_title": "Quantum Computers Can Run Powerful AI That Works like the Brain", "source_text": "Quantum Computers Can Run Powerful AI That Works like the Brain | Scientific American\n\nSkip to main content\n\nScientific American\n\nSign Up for Our Daily Newsletter\n\nApril 22, 2024\n\n5 min read\n\n#\n\nQuantum Computers Can Now Run Powerful AI That Works like the Brain\n\nThe influential AI design that makes chatbots tick now runs on quantum\ncomputers\n\nBy Rahul Rao\n\nBlackJack3D/Getty Images\n\nFew computer science breakthroughs have done so much in so little time as the\nartificial intelligence design known as a transformer. A transformer is a form\nof deep learning\u2014a machine model based on networks in the brain\u2014that\nresearchers at Google first proposed in 2017. Seven years later the\ntransformer, which enables ChatGPT and other chatbots to quickly generate\nsophisticated outputs in reply to user prompts, is the dynamo powering the\nongoing AI boom. As remarkable as this AI design has already proved to be,\nwhat if you could run it on a quantum computer?\n\nThat might sound like some breathless mash-up proposed by an excitable tech\ninvestor. But quantum-computing researchers are now in fact asking this very\nquestion out of sheer curiosity and the relentless desire to make computers do\nnew things. A new study published recently in Quantum used simple hardware to\nshow that rudimentary quantum transformers could indeed work, hinting that\nmore developed quantum-AI combinations might solve crucial problems in areas\nincluding encryption and chemistry\u2014at least in theory.\n\nA transformer\u2019s superpower is its ability to discern which parts of its input\nare more important than others and how strongly those parts connect. Take the\nsentence \u201cShe is eating a green apple.\u201d A transformer could pick out the\nsentence\u2019s key words: \u201ceating,\u201d \u201cgreen\u201d and \u201capple.\u201d Then, based on patterns\nidentified in its training data, it would judge that the action \u201ceating\u201d has\nlittle to do with the color \u201cgreen\u201d but a great deal more to do with the\nobject \u201capple.\u201d Computer scientists call this feature an \u201cattention\nmechanism,\u201d meaning it pays the most attention to the most important words in\na sentence, pixels in an image or proteins in a sequence. The attention\nmechanism mimics how humans process language, performing a task that is\nelementary for most young children but that\u2014until the ChatGPT era\u2014computers\nhad struggled with.\n\nAttention mechanisms currently run on supercomputers with powerful processors,\nbut they still use basic binary bits that hold values of either 0 or 1.\nPhysicists describe these as \u201cclassical\u201d machines, which also include\nsmartphones and PCs. Quantum hardware, on the other hand, taps into the\nweirdness of quantum mechanics to solve problems too impractical for classical\ncomputers. That\u2019s because quantum bits, aka qubits, can exist as a 0, a 1 or a\nspectrum of other possible states. So could developers build a superior\nattention mechanism using qubits? \u201cQuantum computers are not expected to be a\ncomputational panacea, but we won\u2019t know until we try,\u201d says quantum computing\nresearcher Christopher Ferrie at the University of Technology Sydney, who\nwasn\u2019t involved with the new study.\n\nAn author of the study, Jonas Landman, had previously crafted quantum\nfacsimiles of other brainlike AI designs to run on quantum hardware. \u201cWe\nwanted to look at transformers because they seemed to be the state of the art\nof deep learning,\u201d says Landman, a quantum computing researcher at the\nUniversity of Edinburgh and a computing firm called QC Ware. In the new\nresearch, he and his colleagues adapted a transformer designed for medical\nanalysis. From a database of images of 1,600 people\u2019s retinas, some in healthy\neyes and some in people with diabetes-induced blindness, the quantum model\nsorted each image into one of five levels from no damage to the most severe.\n\nDeveloping their quantum transformer was a three-step process. First, before\neven touching any quantum hardware, they needed to design a quantum circuit\u2014a\nquantum program\u2019s \u201ccode,\u201d in other words\u2014for a transformer. They made three\nversions, each of which could theoretically pay attention more efficiently\nthan a classical transformer, as demonstrated by mathematical proofs.\n\nBolstered by confidence from the math, the study authors tested their designs\non a quantum simulator\u2014a qubit emulator that runs on classical hardware.\nEmulators avoid a problem plaguing today\u2019s real quantum computers, which are\nstill so sensitive to heat, electromagnetic waves and other interference that\nqubits can become muddled or entirely useless.\n\n## Curated by Our Editors\n\n  * ### AI Chatbot Brains Are Going Inside Robot Bodies. What Could Possibly Go Wrong?\n\nDavid Berreby\n\n  * ### New AI Circuitry That Mimics Human Brains Makes Models Smarter\n\nAnna Mattson\n\n  * ### Quantum Computing Is the Future, and Schools Need to Catch Up\n\nOlivia Lanes\n\n  * ### Are Quantum Computers about to Break Online Privacy?\n\nDavide Castelvecchi & Nature magazine\n\nOn the simulator, each quantum transformer categorized a set of retinal images\nwith between 50 and 55 percent accuracy\u2014greater than the 20 percent accuracy\nthat randomly sorting retinas into one of five categories would have achieved.\nThe 50- to 55-percent range was about the same accuracy level (53 to 56\npercent) achieved by two classical transformers with vastly more complex\nnetworks.\n\nOnly after this could the scientists move on to the third step: operating\ntheir transformers on real IBM-made quantum computers, using up to six qubits\nat a time. The three quantum transformers still performed with between 45 and\n55 percent accuracy.\n\nSix qubits is not very many. For a viable quantum transformer to match the\nchatbot giants of Google\u2019s Gemini or OpenAI\u2019s ChatGPT, some researchers think\ncomputer scientists would have to create a code that uses hundreds of qubits.\nQuantum computers of that size already exist, but designing a comparatively\ncolossal quantum transformer isn\u2019t yet practical because of the interference\nand potential errors involved. (The researchers tried higher qubit numbers but\ndid not see the same success.)\n\nThe group isn\u2019t alone in its work on transformers. Last year researchers at\nIBM\u2019s Thomas J. Watson Research Center proposed a quantum version of a\ntransformer type known as a graph transformer. And in Australia, Ferrie\u2019s\ngroup has designed its own transformer quantum circuit concept. That team is\nstill working on the first step that QC Ware passed: mathematically testing\nthe design before trying it out.\n\nBut suppose a reliable quantum computer existed\u2014one with more than 1,000\nqubits and where interference is somehow kept to a minimum. Would a quantum\ntransformer, then, always have the advantage? Maybe not. Head-to-head\ncomparisons between quantum and classical transformers are not the right\napproach because the two probably have different strengths.\n\nFor one thing, classical computers have the benefit of investment and\nfamiliarity. Even as quantum-computing technology matures, \u201cIt will take many\nyears for quantum computers to scale up to that regime, and classical\ncomputers won\u2019t stop growing in the meantime,\u201d says Nathan Killoran, head of\nsoftware at quantum computing firm Xanadu, who was not involved with the new\nresearch. \u201cClassical machine learning is just so powerful and so well financed\nthat it may just not be worth it to replace it entirely with an emerging\ntechnology like quantum computing in our lifetimes.\u201d\n\nAdditionally, quantum computers and classical machine learning each excel at\ndifferent kinds of problems. Modern deep-learning algorithms detect patterns\nwithin their training data. It\u2019s possible that qubits can learn to encode the\nsame patterns, but it is not clear if they are optimal for the task. That\u2019s\nbecause qubits offer the greatest advantage when a problem is \u201cunstructured,\u201d\nmeaning its data have no clear patterns to find in the first place. Imagine\ntrying to find a name in a phone book with no alphabetization or order of any\nkind; a quantum computer can find that word in the square root of the time a\nclassical computer would take.\n\nBut the two options are not exclusive. Many quantum researchers believe a\nquantum transformer\u2019s ideal place will be as part of a classical-quantum\nhybrid system. Quantum computers could handle the trickier problems of\nchemistry and materials science, while a classical system crunches through\nvolumes of data. Quantum systems might also prove valuable at generating\ndata\u2014decrypted cryptographic keys, for example, or the properties of materials\nthat don\u2019t yet exist, both of which are hard for classical computers to\ndo\u2014that could in turn help train classical transformers to perform tasks that\nnow remain largely inaccessible.\n\nAnd quantum transformers may bring other bonuses. Classical transformers, at\nthe scales they are now being used, consume so much energy that U.S. utilities\nare keeping carbon-spewing coal plants operational just to meet new data\ncenters\u2019 power demands. The dream of a quantum transformer is also the dream\nof a leaner, more efficient machine that lightens the energy load.\n\nRights & Permissions\n\nRahul Rao is a London-based freelance science writer covering physics, space,\ntechnology and their intersections with one another and everything else. He\nlikes snakes, old genre fiction, trains and classic Doctor Who, in no\nparticular order.\n\nMore by Rahul Rao\n\n# Popular Stories\n\nParticle Physics April 12, 2024\n\n##\n\nThe Secret to the Strongest Force in the Universe\n\nNew discoveries demystify the bizarre force that binds atomic nuclei together\n\nStanley J. Brodsky, Alexandre Deur, Craig D. Roberts\n\nOpinion April 15, 2024\n\n##\n\nHow Parents Can Heal Rifts with Their Adult Children\n\nRepairing a broken parent-adult child relationship is possible if both sides\napproach it earnestly and honestly\n\nJoshua Coleman\n\nClimate Change April 21, 2024\n\n##\n\nThe U.S. Spends a Fortune on Beach Sand That Storms Just Wash Away\n\nThe U.S. is paying hundreds of millions of dollars to replenish storm-ravaged\nbeaches in a losing battle against rising seas and erosion\n\nDaniel Cusick, E&E News\n\nAnimals April 16, 2024\n\n##\n\nWhy Feathers Are One of Evolution\u2019s Cleverest Inventions\n\nFossil and living birds reveal the dazzling biology of feathers\n\nMichael B. Habib\n\nPharmaceuticals April 19, 2024\n\n##\n\nWhat Happens When You Quit Ozempic or Wegovy?\n\nMany researchers think that Wegovy and Ozempic should be taken for life, but\nmyriad factors can force people off the drugs\n\nMcKenzie Prillaman, Nature magazine\n\nPlanetary Science April 18, 2024\n\n##\n\nSpaceX\u2019s Starship Could Save NASA\u2019s Beleaguered Mars Sample Return Mission\n\nFacing budgetary pressure for its Mars Sample Return program, NASA has turned\nto private industry for ideas\u2014perhaps with one specific company in mind\n\nJonathan O'Callaghan\n\n## Expand Your World with Science\n\nLearn and share the most exciting discoveries, innovations and ideas shaping\nour world today.\n\nSubscribeSign up for our newslettersSee the latest storiesRead the latest\nissueGive a Gift Subscription\n\nFollow Us:\n\n  * Return & Refund Policy\n  * About\n  * Press Room\n\n  * FAQs\n  * Contact Us\n  * International Editions\n\n  * Advertise\n  * Accessibility Statement\n  * Terms of Use\n\n  * Privacy Policy\n  * California Consumer Privacy Statement\n  * Use of cookies/Do not sell my data\n\nScientific American is part of Springer Nature, which owns or has commercial\nrelations with thousands of scientific publications (many of them can be found\nat www.springernature.com/us). Scientific American maintains a strict policy\nof editorial independence in reporting developments in science to our readers.\n\n\u00a9 2024 SCIENTIFIC AMERICAN, A DIVISION OF SPRINGER NATURE AMERICA, INC. ALL\nRIGHTS RESERVED.\n\n## We Value Your Privacy\n\nWe use cookies to enhance site navigation, analyze site usage & personalize\ncontent to provide social media features and to improve our marketing efforts.\nWe also share information about your use of our site with our social media,\nadvertising and analytics partners. By clicking \u201cAccept All Cookies\u201d, you\nagree to the storing of cookies on your device for the described purposes.\nView Our Privacy Policy\n\n## Privacy Preference Center\n\n  * ### Your Privacy\n\n  * ### Performance Cookies\n\n  * ### Social Media Cookies\n\n  * ### Targeting Cookies\n\n  * ### Strictly Necessary Cookies\n\n  * ### Functional Cookies\n\n  * ### Targeting (1st Party)\n\n  * ### Google & IAB TCF 2 Purposes of Processing\n\n  * ### Targeting (3rd Party)\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your\nbrowser, mostly in the form of cookies. This information might be about you,\nyour preferences or your device and is mostly used to make the site work as\nyou expect it to. The information does not usually directly identify you, but\nit can give you a more personalized web experience. Because we respect your\nright to privacy, you can choose not to allow some types of cookies. Click on\nthe different category headings to find out more and change our default\nsettings. However, blocking some types of cookies may impact your experience\nof the site and the services we are able to offer.\n\n#### Performance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure\nand improve the performance of our site. They help us to know which pages are\nthe most and least popular and see how visitors move around the site. All\ninformation these cookies collect is aggregated and therefore anonymous. If\nyou do not allow these cookies we will not know when you have visited our\nsite, and will not be able to monitor its performance.\n\n#### Social Media Cookies\n\nThese cookies are set by a range of social media services that we have added\nto the site to enable you to share our content with your friends and networks.\nThey are capable of tracking your browser across other sites and building up a\nprofile of your interests. This may impact the content and messages you see on\nother websites you visit. If you do not allow these cookies you may not be\nable to use or see these sharing tools.\n\n#### Targeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They\nmay be used by those companies to build a profile of your interests and show\nyou relevant adverts on other sites. They do not store directly personal\ninformation, but are based on uniquely identifying your browser and internet\ndevice. If you do not allow these cookies, you will experience less targeted\nadvertising.\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched\noff in our systems. They are usually only set in response to actions made by\nyou which amount to a request for services, such as setting your privacy\npreferences, logging in or filling in forms. You can set your browser to block\nor alert you about these cookies, but some parts of the site will not then\nwork. These cookies do not store any personally identifiable information.\n\n#### Functional Cookies\n\nThese cookies enable the website to provide enhanced functionality and\npersonalisation. They may be set by us or by third party providers whose\nservices we have added to our pages. If you do not allow these cookies then\nsome or all of these services may not function properly.\n\n#### Targeting (1st Party)\n\nThese cookies may be set through our site by ourselves. They may be used by\nourselves to build a profile of your interests and show you relevant content\nor adverts on our sites. They do not store directly personal information, but\nare based on uniquely identifying your browser and internet device. If you do\nnot allow these cookies, you may experience less personalised content and/or\nadvertising.\n\n#### Google & IAB TCF 2 Purposes of Processing\n\nAllowing third-party ad tracking and third-party ad serving through Google and\nother vendors to occur. Please see more information on Google Ads\n\n#### Targeting (3rd Party)\n\nThese cookies may be set through our site by our advertising partners. They\nmay be used by those companies to build a profile of your interests and show\nyou relevant adverts on other sites. They do not store directly personal\ninformation, but are based on uniquely identifying your browser and internet\ndevice. If you do not allow these cookies, you will experience less targeted\nadvertising.\n\n### Back\n\nConsent Leg.Interest\n\nlabel\n\nlabel\n\nlabel\n\n  *     * Name\n\ncookie name\n\nlabel\n\n", "frontpage": false}
