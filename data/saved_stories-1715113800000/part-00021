{"aid": "40285323", "title": "FinRAG Datasets and Study", "url": "https://www.parsee.ai/en/blog/finrag-dataset-and-study/", "domain": "parsee.ai", "votes": 1, "user": "thf24", "posted_at": "2024-05-07 13:40:13", "comments": 0, "source_title": "finRAG Study and Dataset", "source_text": "finRAG Study and Dataset\n\n## Cookies\n\nWe use our own cookies and third-party cookies so that we can display this\nwebsite correctly and better understand how this website is used, with a view\nto improving the services we offer. A decision on cookie usage permissions can\nbe changed anytime using the cookie button that will appear after a selection\nhas been made on this banner.\n\nHome\n\nBack To Blog\n\nData Extraction\n\n# finRAG Datasets & Study\n\nMay 7, 2024 - 10 min\n\nWe wanted to investigate how good the current state of the art (M)LLMs are at\nsolving the relatively simple problem of extracting revenue figures from\npublicly available financial reports. To test this, we created 3 different\ndatasets, all based on the same selection of 1,156 randomly selected annual\nreports for the year 2023 of publicly listed US companies. The resulting\ndatasets contain a combined total of 10,404 rows, 37,536,847 tokens and 1,156\nimages. For our study, we are evaluating 8 state-of-the-art (M)LLMs on a\nsubset of 100 reports.\n\n## finRAG Datasets\n\nWe are publishing 3 datasets:\n\n  * \u201cSelection-text\u201d: this dataset contains only the relevant profit & loss statement with the revenue numbers that we are looking for. It can be considered our \u201cbase-case\u201d, as extracting the revenue numbers from this table only should be the easiest (8.5M tokens).\n\n  * \u201cRAG-text\u201d: this dataset tries to simulate a real-world RAG-application, where we chunk the original document into pieces, perform a vector search based on the question that we want to answer, and present the LLMs with the most relevant chunks. We cut off all prompts at 8k tokens for this exercise, so in case the relevant table was not contained in the prompt, we inserted it at the \u201cfirst position\u201d, to simulate a \u201chappy path\u201d for the vector search, as the goal of this study is not to examine how good or bad vector search is working, but rather to focus on the capabilities of the LLMs if we can guarantee that all required information to solve a task is presented to the model (28M tokens).\n\n  * \u201cSelection-image\u201d: this dataset is similar to the \u201cSelection-text\u201d dataset in the sense that we feed to the models only an image of the relevant profit & loss statement, that contains all the necessary information to solve the problem (1M tokens and 1,156 images).\n\nAll data can be found both on Github and Huggingface.\n\n### Major Results:\n\n  * If we feed the models just the relevant table in text form (\u201cselection-text\u201d dataset), which contains all the required information to solve the problem, a lot of the state-of-the-art LLMs we tested can solve the task with near 100% accuracy, although some even struggle with this most simple exercise (notably Databricks DBRX and Snowflake Arctic, which is by far the worst model we tested for our task).\n\n  * RAG is still a major problem for ALL models as even at a relatively small context size of 8k which we used for the \u201cRAG-text\u201d dataset, performance drops significantly for all models, between 10-30% as compared to the \u201cselection-text\u201d dataset.\n\n  * State of the art vision models (Claude 3 and Chat GPT 4 vision) are performing much worse than their pure-textual counterparts, achieving only around 60% accuracy on the \u201cselection-image\u201d dataset, which is a drop of almost 40% compared to the \u201cselection-text\u201d dataset\n\n  * Looking at the results of the \u201cSelection-text\u201d dataset, especially the smaller models (e.g. Llama 3 70b, Command R plus) are showing a significant drop in performance with a more complex prompt (and more complex expected answer), compared to a more precisely posed question (and simpler answer), whereas the more complex prompt and expected answer was not a problem for the leading proprietary models (Claude 3, Mistral Large and Chat GPT 4). So for smaller models it is still important to keep the task as \u201ceasy\u201d as possible, and better to use multiple prompts rather than trying to do everything in one prompt/request.\n\n## Introduction\n\nThe market for \u201centerprise-grade\u201d (M)LLMs is gaining new competitors on a\nweekly basis and as such, models are being marketed at being already excellent\nat solving real-world problems for a variety of use-cases. We decided to put\nthe current state of the art (M)LLMs to the test and evaluate their\ncapabilities in the domain of simple data extraction from financial reports.\nWe imagined the use-case of a financial analyst wanting to retrieve the\nrevenue numbers of a company, based on the most recent annual report. This\ntask typically involves finding the profit & loss statement inside the report,\nfinding the row with the \u201cTotal Revenues\u201d (or similarly named), and then\nreturning the number from one or all columns, depending on the task (if all\nnumbers should be extracted or just numbers from a single year, e.g. 2023). We\nalso always need to return a unit (thousands, millions, billions or \u201cnone\u201d)\nand a currency, as this information is also crucial to being able to work with\nthe numbers.\n\nThis task can be considered \u201ceasy\u201d for a human, and anyone with basic\nfinancial knowledge will be able to solve the task with near 100% accuracy\ngiven the annual report of the company. We wanted to evaluate the models in-\ndepth on a relatively easy task, in order to fully understand what they are\ncapable of and where their current weaknesses lie.\n\n## Methodology\n\nWe created two versions of the data, in V1 we explored the best prompts for\nour task and in V2 we only preceded with the \u201cwinning\u201d prompts from V1. For\nboth versions (V1 and V2) we created 3 different datasets:\n\n  * \u201cSelection-text\u201d: this dataset contains only the relevant profit & loss statement with the revenue numbers that we are looking for. It can be considered our \u201cbase-case\u201d, as extracting the revenue numbers from this table only should be the easiest.\n\n  * \u201cRAG-text\u201d: this dataset tries to simulate a real-world RAG-application, where we chunk the original document into pieces, perform a vector search based on the question that we want to solve, and present the LLMs with the chunks. We cut off all prompts at 4k tokens for the V1 dataset and at 8k tokens for the V2 dataset, so in case the relevant table was not contained in the prompt, we inserted it at the \u201cfirst position\u201d, to simulate a \u201chappy path\u201d for the vector search, as the goal of this study is not to examine how good or bad vector search is working, but rather to focus on the capabilities of the LLMs if we can guarantee that all required information to solve a task is presented to the model.\n\n  * \u201cSelection-image\u201d: this dataset is similar to the \u201cSelection-text\u201d dataset in the sense that we feed to the models only an image of the relevant profit & loss statement, that contains all the necessary information to solve the problem.\n\nFor both V1 and V2 datasets, we set up separate \u201cextraction templates\u201d, using\nthe parsee-core Python library (more info about the extraction templates can\nbe found here). The parsee-core library enables us to create prompts (based on\nthe questions we are asking, more on these below) which have been battle-\ntested and also makes the evaluation of the results easy, as all answers of\nthe models are parsed into a format that allows for direct comparison with our\nmanually \u201cassigned\u201d labels. The evaluation methods in the parsee-core library\nalso don\u2019t just assign a right or wrong value to an answer, but rather allow\nfor more subtle comparisons, as each answer is scored based on the correctness\nof the \u201cmain\u201d answer and also the meta info returned (time periods,\ncurrencies, units in our case). For example, let\u2019s say we want to extract the\n2023 revenue numbers from a financial report and the revenues of a company are\nEUR 104M for 2023. If the model returns the 104M but also says the currency is\nUSD, while actually the currency is EUR, we still count the \u201cmain\u201d answer as\ncorrect (as the correct number was returned), but the \u201cmeta\u201d score will be 0\nfor this question. For our study, we chose a weight of 3\u20444 for the \u201cmain\u201d\nquestion, and 1\u20444 of the meta items, so in total in this case the model would\nhave a score of 75% for this question, given that the wrong currency was\nreturned but the number was correct.\n\nThe full prompts contain by default an example, so this is basically a one-\nshot exercise.\n\n### V1 Dataset\n\n#### Setup\n\nWe created a first version of the dataset where we tried to explore which\nprompt format is the best for our task of extracting the revenues figure from\nthe original filings. The V1 dataset contained therefore 6 different\n\u201cquestions\u201d, worded slightly differently and with different expected output\nvalues. For instance we wanted to know if we would ask the model to return the\nnumber in a specific unit, if it would be able to do so (applying some\nnecessary addition of zeros or removal of decimal places).\n\nThe questions of the V1 dataset were the following:\n\n  1. What are the revenues for the 12 month period ending December 2023 (in thousand USD)? (ID: rev23_thousands_no_hint)\n\n  2. What are the revenues for the 12 month period ending December 2023 (in thousand USD)? Please double check your answer and make sure the unit of the number is in thousands, format the number if necessary (add or remove digits). (ID: rev23_thousands_hint)\n\n  3. What are the revenues for the 12 month period ending December 2023 (in million USD)? (ID: rev23_millions_no_hint)\n\n  4. What are the revenues for the 12 month period ending December 2023? [meta items: unit, currency] (ID: rev23_meta)\n\n  5. What are the revenues for the 12 month period ending December 2022 (in million USD)? (ID: rev22_millions_no_hint)\n\n  6. What are the revenues of the company? [meta items: unit, currency, time period in months, period ending date] (ID: rev_meta)\n\nNote: the actual prompts incorporate these \u201cquestions\u201d but are of course more\ncomplex, you can see the full prompts in the dataset files.\n\nThe full extraction template can be seen here visually also (requires free\nlog-in).\n\nFor questions 1-3 (rev23_thousands_no_hint, rev23_millions_no_hint,\nrev23_millions_no_hint) we are asking for the revenue numbers of a specific\nyear (2023) and in a specific unit (thousands or millions). For question 2 we\nare also giving a hint that the number might have to be adjusted in order to\nreturn the number in the requested format.\n\nFor question 4 we are not asking for a specific unit and currency but rather\nwant to extract the unit and currency as meta-info (this is done with meta\nitems in a Parsee extraction template).\n\nFor question 5 we are asking for the 2022 revenues instead of the 2023\nnumbers, as all reports are from 2023, it should be a little \u201charder\u201d to find\nthe number, as it is not in the first column usually.\n\nFor question 6 (rev_meta) we did not specify a unit or specific year, but\nrather want to extract all the available revenue figures and their units, time\nperiods etc.\n\n#### Results\n\nWe ran the extraction using the parsee-core library for a selection of 50\nannual reports and with a token limit of 4k for all datasets. The 4k tokens\nare only relevant for the RAG-text dataset, as the other 2 datasets should\nnever touch the 4k token limit anyway.\n\nFor the evaluation of the results we also used the parsee-core library but\nadded a custom function for comparing the results, which would basically\nreflect the unit chosen by the model and check if the model prediction and the\ncorrect value differ by more than 0.1% (full code is on Github). So for\nexample, if a model replies (simplified): {\u201cunit\u201d: \u201cmillions\u201d,\n\u201cmain_question\u201d: 1400}\n\nAnd the \u201cassigned\u201d (correct) value is {\u201cunit\u201d: \u201cthousands\u201d, \u201cmain_question\u201d:\n1399000}, this would still evaluate as \u201ctrue\u201d, as we compare 1,400,000,000\nwith 1,399,000,000 and the difference is only 0.07%. As discussed before, the\ntotal scores are always also considering the \u201cmeta items\u201d if present (see\nbeginning of \u201cMethodology\u201d section).\n\nThe results are summarized in a Jupyter Notebook.\n\nSummary of results by dataset and task:\n\nLooking at the \u201cSelection-text\u201d dataset only (in the table), we can clearly\nsee that the question with the highest accuracy for almost all models is\n\u201crev23_meta\u201d. Only for Llama 3 70B, the rev23_meta question is not the highest\nranking. Surprisingly also, Llama 3 is beating Claude 3 and ChatGPT 4 at the\nrev22_millions_no_hint question. As Llama 3 is the only \u201coutlier\u201d here, we\ndecided to proceed with the rev23_meta question, which is not asking for a\nspecific unit but rather requires the model to extract the correct unit and\nreturn it as part of the answer. It seems also that even state of the art\nmodels like Claude 3 or ChatGPT 4 struggle with returning the right unit, if\nthe unit requested differs from the one present in the document. For most\nfilings the unit is in millions, so asking for the answer in thousands (e.g.\nrev23_thousands_no_hint) leads to an accuracy of only ~80% for Claude 3 and\nChatGPT 4, almost 20% lower than for the rev23_meta question which is not\nasking for a specific unit.\n\nWe are not going to discuss the results of the V1 run further, as we adjusted\nthe extraction template to only use questions in the format of rev23_meta, as\nopposed to asking for a specific unit. There are also some smaller edge cases\nin the data for the V1 run (basically some pro-forma restatements which can\nlead to two possible numbers for the same time-period), which lead to overall\na lower performance than on the V2 dataset, especially for the Selection-text\ndatasets. In the V2 run we made sure that no such cases are present.\n\n### V2 Dataset\n\n#### Setup\n\nThe V2 dataset is the one we uploaded in full also to Github and Huggingface\n(with 1k+ annual reports).\n\nIn the V2 dataset we are asking the following three questions, 2 are taken\nfrom V1 and one new one was added, to improve the comparison of the results:\n\n  1. What are the revenues of the company? [meta items: unit, currency, time period in months, period ending date] (ID: rev_meta)\n\n  2. What are the revenues for the 12 month period ending December 2023? [meta items: unit, currency] (ID: rev23_meta)\n\n  3. What are the revenues for the 12 month period ending December 2022? [meta items: unit, currency] (ID: rev22_meta) [NEW]\n\nNote: the actual prompts incorporate these \u201cquestions\u201d but are of course more\ncomplex, you can see the full prompts in the dataset files.\n\nThe full extraction template can be seen here visually also (requires free\nlog-in).\n\n#### Models Used\n\nFor the V2 run we compared the following models:\n\n  * Claude 3 Opus (id: claude-3-opus-20240229)\n\n  * ChatGPT 4 (id: gpt-4-1106-preview)\n\n  * Llama 3 70B via replicate (id: meta/meta-llama-3-70b-instruct)\n\n  * Mixtral 8x22B Instruct via together.ai (id: mistralai/Mixtral-8x22B-Instruct-v0.1)\n\n  * Mistral Large (id: mistral-large-latest)\n\n  * Databricks DBRX Instruct via together.ai (id: databricks/dbrx-instruct)\n\n  * Cohere Command R Plus via cohere.com (id: command-r-plus)\n\n  * Snowflake Arctic Instruct (id: Snowflake/snowflake-arctic-instruct)\n\n#### Results\n\nWe ran the extraction using the parsee-core library for a selection of 100\nannual reports and with a token limit of 8k for all datasets. The 8k tokens\nare only relevant for the RAG-text dataset, as the other 2 datasets should\nnever touch the 8k token limit anyway.\n\nFor the evaluation of the results we again use the custom compare function, as\ndiscussed in the V1 results.\n\nThe results are summarized in a Jupyter Notebook.\n\nSummary of results by dataset and task:\n\n##### Selection-text Dataset\n\nWe can see that 3 of the models can solve the exercise in theory with almost\n100% accuracy (green bars in the chart). These models are: Claude 3 Opus,\nMistral Large and ChatGPT 4. Afterwards we have Llama 3 as best open &\navailable weights model, with more than 80% accuracy. Afterwards the accuracy\ngradually declines and reaches a low with ~30% for Snowflake Arctic.\n\n##### RAG-text Dataset\n\nFor all models we see a significant drop in performance when the 8k context is\nfully used. The drop is less noticeable for some models (for Claude 3 and\nCommand R Plus around 10%), but some other models are heavily deteriorating\nwith the addition of other elements to the data. As such, Mistral Large\u2019s\naccuracy drops by almost 20% and Llama 3 accuracy drops even by more than 30%.\n\n##### Selection-image Dataset\n\nFor the selection image datasets we can also see that the vision models are\nnot up-to par with the pure text models, as the accuracy for both Claude 3 and\nChatGPT is almost halved, as compared to the pure-text case.\n\n## Conclusion\n\nNo model can achieve a total accuracy of over 90% on this \u201crelatively simple\u201d\ntask (on the RAG-text dataset, as this is the closest to our real world use-\ncase we wanted to test), with Claude 3 Opus being the leader at 87%, followed\nby ChatGPT 4 at 82%. The only other model that managed to achieve relatively\ngood results is Mistral Large, at an overall score of 76%. Command R Plus is\nbarely suited for our task, with an overall score of 67% and high variance\ndepending on how the question is asked. Llama 3 is giving good results when\ndata from a single column is supposed to be retrieved, but is scoring the\nlowest from all models in extracting all columns correctly (only 13%\naccuracy), which drags down its overall score to 51%. DBRX Instruct is clearly\nnot suited for RAG in the case we tested with an overall score of 25%.\nSnowflake Arctic is by far though the worst model we tested, as even for the\neasiest dataset (Selection-text), where state of the art models score close to\n100%, it only achieved a score of 28% (vs. 53% for DBRX Instruct, the second\nworst model in our tests), making it basically unusable for our test-case.\n\nDoes this mean that LLMs are not ready for the enterprise yet? Well, it\ndepends. For our task (precise data extraction), training any model (not\nlimited to LLMs) with 90+% accuracy is not easy at all, so having a model that\nout of the box can almost achieve 90% accuracy is already quite a feat. Then\nagain, the task can be solved by an intern with very basic financial knowledge\nwith 100% accuracy, so we are also still far away from AGI levels. There is\nalso the problem of \u201cfinding\u201d the necessary piece of information, as the 90%\naccuracy does not factor in cases where the vector search (or any other search\nmethod) is not retrieving the correct chunks. This can of course partly be\nmitigated by larger contexts of the models, but as we can see looking at the\nRAG-text results vs. the Selection-text results, increasing the context leads\nto a significant drop in performance for all models still. It would be\ninteresting for further research to compare the results depending on the\nnumber of tokens used in more detail (e.g. 8k vs. 16k vs. 100k vs. 200k). In a\nfuture study we will also examine the costs associated with running the models\nand incorporate that into our results.\n\n### Try Parsee Cloud for free\n\nExplore Parsee Cloud's Document Processing Capabilities at No Cost\n\nRelated posts\n\n  * Data Extraction\n\nComparing Parsee Document Loader vs. Langchain Document Loaders for PDFs\n\nIn the following we will be comparing the results of the Parsee Document\nLoader vs. the PyPDF Langchain Document Loader for various datasets. All\ndatasets that are used here can be found on Huggingface (links below), so the\nresults are all reproducible.\n\n  * Data Extraction\n\nExtraction Templates\n\nThe core functionality of the Parsee Extraction Templates explained.\n\nHome\n\n  * Contact\n  * Blog\n\n  * Imprint & Terms of Service\n  * Privacy Policy\n\nParsee.ai is developed and maintained by SimFin Analytics GmbH\n\n", "frontpage": false}
