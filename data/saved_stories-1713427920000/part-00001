{"aid": "40070157", "title": "Kubernetes v1.30: Uwubernetes", "url": "https://kubernetes.io/blog/2024/04/17/kubernetes-v1-30-release/", "domain": "kubernetes.io", "votes": 11, "user": "gmemstr", "posted_at": "2024-04-17 21:19:51", "comments": 0, "source_title": "Kubernetes v1.30: Uwubernetes", "source_text": "Kubernetes v1.30: Uwubernetes | Kubernetes\n\nKubernetes v1.30: Uwubernetes | Kubernetes\n\n## Kubernetes Blog\n\n# Kubernetes v1.30: Uwubernetes\n\nBy Kubernetes v1.30 Release Team | Wednesday, April 17, 2024\n\nEditors: Amit Dsouza, Frederick Kautz, Kristin Martin, Abigail McCarthy,\nNatali Vlatko\n\nAnnouncing the release of Kubernetes v1.30: Uwubernetes, the cutest release!\n\nSimilar to previous releases, the release of Kubernetes v1.30 introduces new\nstable, beta, and alpha features. The consistent delivery of top-notch\nreleases underscores the strength of our development cycle and the vibrant\nsupport from our community.\n\nThis release consists of 45 enhancements. Of those enhancements, 17 have\ngraduated to Stable, 18 are entering Beta, and 10 have graduated to Alpha.\n\n## Release theme and logo\n\nKubernetes v1.30: Uwubernetes\n\nKubernetes v1.30 makes your clusters cuter!\n\nKubernetes is built and released by thousands of people from all over the\nworld and all walks of life. Most contributors are not being paid to do this;\nwe build it for fun, to solve a problem, to learn something, or for the simple\nlove of the community. Many of us found our homes, our friends, and our\ncareers here. The Release Team is honored to be a part of the continued growth\nof Kubernetes.\n\nFor the people who built it, for the people who release it, and for the\nfurries who keep all of our clusters online, we present to you Kubernetes\nv1.30: Uwubernetes, the cutest release to date. The name is a portmanteau of\n\u201ckubernetes\u201d and \u201cUwU,\u201d an emoticon used to indicate happiness or cuteness.\nWe\u2019ve found joy here, but we\u2019ve also brought joy from our outside lives that\nhelps to make this community as weird and wonderful and welcoming as it is.\nWe\u2019re so happy to share our work with you.\n\nUwU \u2665\ufe0f\n\n## Improvements that graduated to stable in Kubernetes v1.30\n\nThis is a selection of some of the improvements that are now stable following\nthe v1.30 release.\n\n### Robust VolumeManager reconstruction after kubelet restart (SIG Storage)\n\nThis is a volume manager refactoring that allows the kubelet to populate\nadditional information about how existing volumes are mounted during the\nkubelet startup. In general, this makes volume cleanup after kubelet restart\nor machine reboot more robust.\n\nThis does not bring any changes for user or cluster administrators. We used\nthe feature process and feature gate NewVolumeManagerReconstruction to be able\nto fall back to the previous behavior in case something goes wrong. Now that\nthe feature is stable, the feature gate is locked and cannot be disabled.\n\n### Prevent unauthorized volume mode conversion during volume restore (SIG\nStorage)\n\nFor Kubernetes 1.30, the control plane always prevents unauthorized changes to\nvolume modes when restoring a snapshot into a PersistentVolume. As a cluster\nadministrator, you'll need to grant permissions to the appropriate identity\nprincipals (for example: ServiceAccounts representing a storage integration)\nif you need to allow that kind of change at restore time.\n\nWarning: Action required before upgrading. The prevent-volume-mode-conversion\nfeature flag is enabled by default in the external-provisioner v4.0.0 and\nexternal-snapshotter v7.0.0. Volume mode change will be rejected when creating\na PVC from a VolumeSnapshot unless you perform the steps described in the the\n\"Urgent Upgrade Notes\" sections for the external-provisioner 4.0.0 and the\nexternal-snapshotter v7.0.0.\n\nFor more information on this feature also read converting the volume mode of a\nSnapshot.\n\n### Pod Scheduling Readiness (SIG Scheduling)\n\nPod scheduling readiness graduates to stable this release, after being\npromoted to beta in Kubernetes v1.27.\n\nThis now-stable feature lets Kubernetes avoid trying to schedule a Pod that\nhas been defined, when the cluster doesn't yet have the resources provisioned\nto allow actually binding that Pod to a node. That's not the only use case;\nthe custom control on whether a Pod can be allowed to schedule also lets you\nimplement quota mechanisms, security controls, and more.\n\nCrucially, marking these Pods as exempt from scheduling cuts the work that the\nscheduler would otherwise do, churning through Pods that can't or won't\nschedule onto the nodes your cluster currently has. If you have cluster\nautoscaling active, using scheduling gates doesn't just cut the load on the\nscheduler, it can also save money. Without scheduling gates, the autoscaler\nmight otherwise launch a node that doesn't need to be started.\n\nIn Kubernetes v1.30, by specifying (or removing) a Pod's\n.spec.schedulingGates, you can control when a Pod is ready to be considered\nfor scheduling. This is a stable feature and is now formally part of the\nKubernetes API definition for Pod.\n\n### Min domains in PodTopologySpread (SIG Scheduling)\n\nThe minDomains parameter for PodTopologySpread constraints graduates to stable\nthis release, which allows you to define the minimum number of domains. This\nfeature is designed to be used with Cluster Autoscaler.\n\nIf you previously attempted use and there weren't enough domains already\npresent, Pods would be marked as unschedulable. The Cluster Autoscaler would\nthen provision node(s) in new domain(s), and you'd eventually get Pods\nspreading over enough domains.\n\n### Go workspaces for k/k (SIG Architecture)\n\nThe Kubernetes repo now uses Go workspaces. This should not impact end users\nat all, but does have a impact for developers of downstream projects.\nSwitching to workspaces caused some breaking changes in the flags to the\nvarious k8s.io/code-generator tools. Downstream consumers should look at\nstaging/src/k8s.io/code-generator/kube_codegen.sh to see the changes.\n\nFor full details on the changes and reasons why Go workspaces was introduced,\nread Using Go workspaces in Kubernetes.\n\n## Improvements that graduated to beta in Kubernetes v1.30\n\nThis is a selection of some of the improvements that are now beta following\nthe v1.30 release.\n\n### Node log query (SIG Windows)\n\nTo help with debugging issues on nodes, Kubernetes v1.27 introduced a feature\nthat allows fetching logs of services running on the node. To use the feature,\nensure that the NodeLogQuery feature gate is enabled for that node, and that\nthe kubelet configuration options enableSystemLogHandler and\nenableSystemLogQuery are both set to true.\n\nFollowing the v1.30 release, this is now beta (you still need to enable the\nfeature to use it, though).\n\nOn Linux the assumption is that service logs are available via journald. On\nWindows the assumption is that service logs are available in the application\nlog provider. Logs are also available by reading files within /var/log/\n(Linux) or C:\\var\\log\\ (Windows). For more information, see the log query\ndocumentation.\n\n### CRD validation ratcheting (SIG API Machinery)\n\nYou need to enable the CRDValidationRatcheting feature gate to use this\nbehavior, which then applies to all CustomResourceDefinitions in your cluster.\n\nProvided you enabled the feature gate, Kubernetes implements validation\nracheting for CustomResourceDefinitions. The API server is willing to accept\nupdates to resources that are not valid after the update, provided that each\npart of the resource that failed to validate was not changed by the update\noperation. In other words, any invalid part of the resource that remains\ninvalid must have already been wrong. You cannot use this mechanism to update\na valid resource so that it becomes invalid.\n\nThis feature allows authors of CRDs to confidently add new validations to the\nOpenAPIV3 schema under certain conditions. Users can update to the new schema\nsafely without bumping the version of the object or breaking workflows.\n\n### Contextual logging (SIG Instrumentation)\n\nContextual Logging advances to beta in this release, empowering developers and\noperators to inject customizable, correlatable contextual details like service\nnames and transaction IDs into logs through WithValues and WithName. This\nenhancement simplifies the correlation and analysis of log data across\ndistributed systems, significantly improving the efficiency of troubleshooting\nefforts. By offering a clearer insight into the workings of your Kubernetes\nenvironments, Contextual Logging ensures that operational challenges are more\nmanageable, marking a notable step forward in Kubernetes observability.\n\n### Make Kubernetes aware of the LoadBalancer behaviour (SIG Network)\n\nThe LoadBalancerIPMode feature gate is now beta and is now enabled by default.\nThis feature allows you to set the .status.loadBalancer.ingress.ipMode for a\nService with type set to LoadBalancer. The .status.loadBalancer.ingress.ipMode\nspecifies how the load-balancer IP behaves. It may be specified only when the\n.status.loadBalancer.ingress.ip field is also specified. See more details\nabout specifying IPMode of load balancer status.\n\n## New alpha features\n\n### Speed up recursive SELinux label change (SIG Storage)\n\nFrom the v1.27 release, Kubernetes already included an optimization that sets\nSELinux labels on the contents of volumes, using only constant time.\nKubernetes achieves that speed up using a mount option. The slower legacy\nbehavior requires the container runtime to recursively walk through the whole\nvolumes and apply SELinux labelling individually to each file and directory;\nthis is especially noticable for volumes with large amount of files and\ndirectories.\n\nKubernetes 1.27 graduated this feature as beta, but limited it to\nReadWriteOncePod volumes. The corresponding feature gate is\nSELinuxMountReadWriteOncePod. It's still enabled by default and remains beta\nin 1.30.\n\nKubernetes 1.30 extends support for SELinux mount option to all volumes as\nalpha, with a separate feature gate: SELinuxMount. This feature gate\nintroduces a behavioral change when multiple Pods with different SELinux\nlabels share the same volume. See KEP for details.\n\nWe strongly encourage users that run Kubernetes with SELinux enabled to test\nthis feature and provide any feedback on the KEP issue.\n\nFeature gate| Stage in v1.30| Behavior change  \n---|---|---  \nSELinuxMountReadWriteOncePod| Beta| No  \nSELinuxMount| Alpha| Yes  \n  \nBoth feature gates SELinuxMountReadWriteOncePod and SELinuxMount must be\nenabled to test this feature on all volumes.\n\nThis feature has no effect on Windows nodes or on Linux nodes without SELinux\nsupport.\n\n### Recursive Read-only (RRO) mounts (SIG Node)\n\nIntroducing Recursive Read-Only (RRO) Mounts in alpha this release, you'll\nfind a new layer of security for your data. This feature lets you set volumes\nand their submounts as read-only, preventing accidental modifications. Imagine\ndeploying a critical application where data integrity is key\u2014RRO Mounts ensure\nthat your data stays untouched, reinforcing your cluster's security with an\nextra safeguard. This is especially crucial in tightly controlled\nenvironments, where even the slightest change can have significant\nimplications.\n\n### Job success/completion policy (SIG Apps)\n\nFrom Kubernetes v1.30, indexed Jobs support .spec.successPolicy to define when\na Job can be declared succeeded based on succeeded Pods. This allows you to\ndefine two types of criteria:\n\n  * succeededIndexes indicates that the Job can be declared succeeded when these indexes succeeded, even if other indexes failed.\n  * succeededCount indicates that the Job can be declared succeeded when the number of succeeded Indexes reaches this criterion.\n\nAfter the Job meets the success policy, the Job controller terminates the\nlingering Pods.\n\n### Traffic distribution for services (SIG Network)\n\nKubernetes v1.30 introduces the spec.trafficDistribution field within a\nKubernetes Service as alpha. This allows you to express preferences for how\ntraffic should be routed to Service endpoints. While traffic policies focus on\nstrict semantic guarantees, traffic distribution allows you to express\npreferences (such as routing to topologically closer endpoints). This can help\noptimize for performance, cost, or reliability. You can use this field by\nenabling the ServiceTrafficDistribution feature gate for your cluster and all\nof its nodes. In Kubernetes v1.30, the following field value is supported:\n\nPreferClose: Indicates a preference for routing traffic to endpoints that are\ntopologically proximate to the client. The interpretation of \"topologically\nproximate\" may vary across implementations and could encompass endpoints\nwithin the same node, rack, zone, or even region. Setting this value gives\nimplementations permission to make different tradeoffs, for example optimizing\nfor proximity rather than equal distribution of load. You should not set this\nvalue if such tradeoffs are not acceptable.\n\nIf the field is not set, the implementation (like kube-proxy) will apply its\ndefault routing strategy.\n\nSee Traffic Distribution for more details.\n\n## Graduations, deprecations and removals for Kubernetes v1.30\n\n### Graduated to stable\n\nThis lists all the features that graduated to stable (also known as general\navailability). For a full list of updates including new features and\ngraduations from alpha to beta, see the release notes.\n\nThis release includes a total of 17 enhancements promoted to Stable:\n\n  * Container Resource based Pod Autoscaling\n  * Remove transient node predicates from KCCM's service controller\n  * Go workspaces for k/k\n  * Reduction of Secret-based Service Account Tokens\n  * CEL for Admission Control\n  * CEL-based admission webhook match conditions\n  * Pod Scheduling Readiness\n  * Min domains in PodTopologySpread\n  * Prevent unauthorised volume mode conversion during volume restore\n  * API Server Tracing\n  * Cloud Dual-Stack --node-ip Handling\n  * AppArmor support\n  * Robust VolumeManager reconstruction after kubelet restart\n  * kubectl delete: Add interactive(-i) flag\n  * Metric cardinality enforcement\n  * Field status.hostIPs added for Pod\n  * Aggregated Discovery\n\n### Deprecations and removals\n\n#### Removed the SecurityContextDeny admission plugin, deprecated since v1.27\n\n(SIG Auth, SIG Security, and SIG Testing) With the removal of the\nSecurityContextDeny admission plugin, the Pod Security Admission plugin,\navailable since v1.25, is recommended instead.\n\n## Release notes\n\nCheck out the full details of the Kubernetes 1.30 release in our release\nnotes.\n\n## Availability\n\nKubernetes 1.30 is available for download on GitHub. To get started with\nKubernetes, check out these interactive tutorials or run local Kubernetes\nclusters using minikube. You can also easily install 1.30 using kubeadm.\n\n## Release team\n\nKubernetes is only possible with the support, commitment, and hard work of its\ncommunity. Each release team is made up of dedicated community volunteers who\nwork together to build the many pieces that make up the Kubernetes releases\nyou rely on. This requires the specialized skills of people from all corners\nof our community, from the code itself to its documentation and project\nmanagement.\n\nWe would like to thank the entire release team for the hours spent hard at\nwork to deliver the Kubernetes v1.30 release to our community. The Release\nTeam's membership ranges from first-time shadows to returning team leads with\nexperience forged over several release cycles. A very special thanks goes out\nour release lead, Kat Cosgrove, for supporting us through a successful release\ncycle, advocating for us, making sure that we could all contribute in the best\nway possible, and challenging us to improve the release process.\n\n## Project velocity\n\nThe CNCF K8s DevStats project aggregates a number of interesting data points\nrelated to the velocity of Kubernetes and various sub-projects. This includes\neverything from individual contributions to the number of companies that are\ncontributing and is an illustration of the depth and breadth of effort that\ngoes into evolving this ecosystem.\n\nIn the v1.30 release cycle, which ran for 14 weeks (January 8 to April 17), we\nsaw contributions from 863 companies and 1391 individuals.\n\n## Event update\n\n  * KubeCon + CloudNativeCon China 2024 will take place in Hong Kong, from 21 \u2013 23 August 2024! You can find more information about the conference and registration on the event site.\n  * KubeCon + CloudNativeCon North America 2024 will take place in Salt Lake City, Utah, The United States of America, from 12 \u2013 15 November 2024! You can find more information about the conference and registration on the eventsite.\n\n## Upcoming release webinar\n\nJoin members of the Kubernetes v1.30 release team on Thursday, May 23rd, 2024,\nat 9 A.M. PT to learn about the major features of this release, as well as\ndeprecations and removals to help plan for upgrades. For more information and\nregistration, visit the event page on the CNCF Online Programs site.\n\nJoin members of the Kubernetes v1.30 release team on DATE AND TIME TBA to\nlearn about the major features of this release, as well as deprecations and\nremovals to help plan for upgrades. For more information and registration,\nvisit the event page on the CNCF Online Programs site.\n\n## Get involved\n\nThe simplest way to get involved with Kubernetes is by joining one of the many\nSpecial Interest Groups (SIGs) that align with your interests. Have something\nyou\u2019d like to broadcast to the Kubernetes community? Share your voice at our\nweekly community meeting, and through the channels below. Thank you for your\ncontinued feedback and support.\n\n  * Follow us on X @Kubernetesio for latest updates\n  * Join the community discussion on Discuss\n  * Join the community on Slack\n  * Post questions (or answer questions) on Stack Overflow\n  * Share your Kubernetes story\n  * Read more about what\u2019s happening with Kubernetes on the blog\n  * Learn more about the Kubernetes Release Team\n\n  * \u2190Previous\n\nNext\u2192\n\nRSS Feed\n\nSubmit a Post\n\n@Kubernetesio\n\non GitHub\n\n#kubernetes-users\n\nStack Overflow\n\nForum\n\nKubernetes\n\n\u00a9 2024 The Kubernetes Authors | Documentation Distributed under CC BY 4.0 Copyright \u00a9 2024 The Linux Foundation \u00ae. All rights reserved. The Linux Foundation has registered trademarks and uses trademarks. For a list of trademarks of The Linux Foundation, please see our Trademark Usage page ICP license: \u4eacICP\u590717074266\u53f7-3\n\n  * Documentation\n  * Kubernetes Blog\n  * Training\n  * Partners\n  * Community\n  * Case Studies\n  * Versions\n  * Release Information\n  * v1.30\n  * v1.29\n  * v1.28\n  * v1.27\n  * v1.26\n  * English\n  * \u4e2d\u6587 (Chinese)\n  * Fran\u00e7ais (French)\n  * Deutsch (German)\n  * \u0939\u093f\u0928\u094d\u0926\u0940 (Hindi)\n  * Bahasa Indonesia (Indonesian)\n  * Italiano (Italian)\n  * \u65e5\u672c\u8a9e (Japanese)\n  * \ud55c\uad6d\uc5b4 (Korean)\n  * Polski (Polish)\n  * Portugu\u00eas (Portuguese)\n  * \u0420\u0443\u0441\u0441\u043a\u0438\u0439 (Russian)\n  * Espa\u00f1ol (Spanish)\n  * \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 (Ukrainian)\n  * Ti\u1ebfng Vi\u1ec7t (Vietnamese)\n\n", "frontpage": true}
