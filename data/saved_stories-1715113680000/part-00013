{"aid": "40285134", "title": "Aider LLM Leaderboards", "url": "https://aider.chat/docs/leaderboards/", "domain": "aider.chat", "votes": 3, "user": "anotherpaulg", "posted_at": "2024-05-07 13:22:39", "comments": 0, "source_title": "Aider LLM leaderboards", "source_text": "Aider LLM leaderboards | aider\n\nSkip to the content.\n\n# Aider LLM leaderboards\n\n## aider is AI pair programming in your terminal\n\nHome Blog GitHub\n\n# Aider LLM leaderboards\n\nAider works best with LLMs which are good at editing code, not just good at\nwriting code. To evaluate an LLM\u2019s editing skill, aider uses a pair of\nbenchmarks that assess a model\u2019s ability to consistently follow the system\nprompt to successfully edit code.\n\nThe leaderboards below report the results from a number of popular LLMs. While\naider can connect to almost any LLM, it works best with models that score well\non the benchmarks.\n\n## Code editing leaderboard\n\nAider\u2019s code editing benchmark asks the LLM to edit python source files to\ncomplete 133 small coding exercises. This benchmark measures the LLM\u2019s coding\nability, but also whether it can consistently emit code edits in the format\nspecified in the system prompt.\n\nModel| Percent completed correctly| Percent using correct edit format|\nCommand| Edit format  \n---|---|---|---|---  \nclaude-3-opus-20240229| 68.4%| 100.0%| aider --opus| diff  \ngpt-4-0613| 67.7%| 100.0%| aider -4| diff  \ngpt-4-0314| 66.2%| 93.2%| aider --model gpt-4-0314| diff  \ngpt-4-0125-preview| 66.2%| 97.7%| aider --model gpt-4-0125-preview| udiff  \ngpt-4-turbo-2024-04-09| 63.9%| 97.0%| aider --gpt-4-turbo| udiff  \ngpt-4-1106-preview| 63.2%| 94.0%| aider| udiff  \ndeepseek-chat v2| 60.2%| 100.0%| aider --model openai/deepseek-chat| whole  \ngpt-3.5-turbo-0301| 57.9%| 100.0%| aider --model gpt-3.5-turbo-0301| whole  \ngemini/gemini-1.5-pro-latest| 57.1%| 87.2%| aider --model\ngemini/gemini-1.5-pro-latest| diff-fenced  \ngpt-3.5-turbo-1106| 56.1%| 100.0%| aider --model gpt-3.5-turbo-1106| whole  \nclaude-3-sonnet-20240229| 54.9%| 100.0%| aider --sonnet| whole  \ndeepseek-coder| 54.5%| 100.0%| aider --model openai/deepseek-coder| whole  \ngpt-3.5-turbo-0613| 50.4%| 100.0%| aider --model gpt-3.5-turbo-0613| whole  \ngpt-3.5-turbo-0125| 49.6%| 100.0%| aider -3| whole  \ngroq/llama3-70b-8192| 49.2%| 73.5%| aider --model groq/llama3-70b-8192| diff  \ncommand-r-plus| 31.6%| 100.0%| aider --model command-r-plus| whole  \n  \n## Code refactoring leaderboard\n\nAider\u2019s refactoring benchmark asks the LLM to refactor 89 large methods from\nlarge python classes. This is a more challenging benchmark, which tests the\nmodel\u2019s ability to output long chunks of code without skipping sections or\nmaking mistakes. It was developed to provoke and measure GPT-4 Turbo\u2019s \u201clazy\ncoding\u201d habit.\n\nThe refactoring benchmark requires a large context window to work with large\nsource files. Therefore, results are available for fewer models.\n\nModel| Percent completed correctly| Percent using correct edit format|\nCommand| Edit format  \n---|---|---|---|---  \nclaude-3-opus-20240229| 72.3%| 79.5%| aider --opus| diff  \ngpt-4-1106-preview| 57.3%| 31.5%| aider| udiff  \ngemini/gemini-1.5-pro-latest| 49.4%| 7.9%| aider --model\ngemini/gemini-1.5-pro-latest| diff-fenced  \ngpt-4-0125-preview| 43.8%| 74.2%| aider --model gpt-4-0125-preview| udiff  \ngpt-4-turbo-2024-04-09| 34.1%| 30.7%| aider --gpt-4-turbo| udiff  \n  \n## Notes on benchmarking results\n\nThe key benchmarking results are:\n\n  * Percent completed correctly - Measures what percentage of the coding tasks that the LLM completed successfully. To complete a task, the LLM must solve the programming assignment and edit the code to implement that solution.\n  * Percent using correct edit format - Measures the percent of coding tasks where the LLM complied with the edit format specified in the system prompt. If the LLM makes edit mistakes, aider will give it feedback and ask for a fixed copy of the edit. The best models can reliably conform to the edit format, without making errors.\n\n## Notes on the edit format\n\nAider uses different \u201cedit formats\u201d to collect code edits from different LLMs.\nThe \u201cwhole\u201d format is the easiest for an LLM to use, but it uses a lot of\ntokens and may limit how large a file can be edited. Models which can use one\nof the diff formats are much more efficient, using far fewer tokens. Models\nthat use a diff-like format are able to edit larger files with less cost and\nwithout hitting token limits.\n\nAider is configured to use the best edit format for the popular OpenAI and\nAnthropic models and the other models recommended on the LLM page. For lesser\nknown models aider will default to using the \u201cwhole\u201d editing format since it\nis the easiest format for an LLM to use.\n\n## Contributing benchmark results\n\nContributions of benchmark results are welcome! See the benchmark README for\ninformation on running aider\u2019s code editing benchmarks. Submit results by\nopening a PR with edits to the benchmark results data files.\n\naider is maintained by paul-gauthier.\n\n", "frontpage": false}
