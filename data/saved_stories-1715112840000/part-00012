{"aid": "40283517", "title": "Mojo 24.3", "url": "https://www.modular.com/blog/whats-new-in-mojo-24-3-community-contributions-pythonic-collections-and-core-language-enhancements", "domain": "modular.com", "votes": 2, "user": "tosh", "posted_at": "2024-05-07 08:36:13", "comments": 0, "source_title": "Modular: What\u2019s New in Mojo 24.3: Community Contributions, Pythonic Collections and Core Language Enhancements", "source_text": "Modular: What\u2019s New in Mojo 24.3: Community Contributions, Pythonic\nCollections and Core Language Enhancements\n\nBlog\n\n/\n\nDeveloper\n\n## What\u2019s New in Mojo 24.3: Community Contributions, Pythonic Collections and\nCore Language Enhancements\n\nMay 2, 2024\n\nShashank Prasanna\n\nAI Developer Advocate\n\nShashank Prasanna\n\nAI Developer Advocate\n\nMojo\ud83d\udd25 24.3 is now available for download and this is a very special release.\nThis is the first major release since Mojo\ud83d\udd25 standard library was open sourced\nand it is packed with the wholesome goodness of community contributions! The\nenthusiasm from the Mojo community to enhance the standard library has been\ntruly remarkable. And on behalf of the entire Mojo team, we\u2019d like to thank\nyou for all your feedback, discussion and, contributions to Mojo, helping\nshape it into a stronger and more inclusive platform for all.\n\n> Special thanks to our contributors. Thank you for your PRs: @LJ-9801\n> @mikowals @whym1here @StandinKP @gabrieldemarmiesse @arvindavoudi @helehex\n> @jayzhan211 @mzaks @StandinKP @artemiogr97 @bgreni @zhoujingya\n> @leandrolcampos @soraros @lsh\n\nIn addition to standard library enhancements, this release also includes\nseveral new core language features and enhancements to built-in types and\ncollections that make them more Pythonic. Through the rest of the blog post,\nI\u2019ll share many of the new features with code examples that you can copy/paste\nand follow along. You can also access all the code samples in this blog post\nin a Jupyter Notebook on GitHub. As always, the official changelog has an\nexhaustive list of new features, what\u2019s changed, what\u2019s removed, and what\u2019s\nfixed. And before we continue, don\u2019t forget to upgrade your Mojo\ud83d\udd25. Let\u2019s dive\ninto the new features.\n\n### Enhancements to List, Dict, and Tuple\n\nIn Mojo 24.3 collections (List, Dict, Set, Tuple) are more Pythonic than ever\nand easier to use if you\u2019re coming from Python. Many of these enhancements\nhave come directly from the community:\n\n  * List has several new methods that mirror Python API, thanks to community contributions from @LJ-9801 @mikowals @whym1here @StandinKP.\n  * Dict can now be updated thanks to contributions from @gabrieldemarmiesse.\n  * Tuple now works with memory-only element types like String and allows you to directly index into it with a parameter expression.\n\nOne of the best ways to learn new features is to see them in action. Let\u2019s\ntake a look at an example that makes use of these types. In the example below\nwe implement a simple gradient descent algorithm, which is an iterative\nalgorithm used to find the minima of a function. Gradient descent is also used\nin most machine learning algorithms to minimize the training loss function by\nupdating the values of function parameters (i.e. weights) iteratively until\nsome convergence criterion is met.\n\nFor this example, we choose the famous Rosenbrock function to optimize, i.e.\nfind its minima. Rosenbrock is an important function in optimization because\nit represents a challenging landscape with a global minimum at ((1,1) for 2-D\nRosenbrock) that is difficult to find. Let\u2019s take a look at its implementation\nand how we make use of Mojo\u2019s shiny new List, Dict, and Tuple enhancements.\nFirst, we define the Rosenbrock function and its gradient:\n\nMojo\n\nfrom python import Python from testing import assert_true np =\nPython.import_module(\"numpy\") plt = Python.import_module(\"matplotlib.pyplot\")\n# Define Rosenbrock function def rosenbrock(x: Float64, y: Float64) ->\nFloat64: return (1 - x)**2 + 100 * (y - x**2)**2 def rosenbrock_gradient(x:\nFloat64, y: Float64) -> Tuple[Float64, Float64]: dx = -2 * (1 - x) - 400 * x *\n(y - x**2) dy = 200 * (y - x**2) return (dx, dy) # Return as a tuple\n\nWe use Tuple to return the gradients using (dx, dy). Notice that for the\nreturn type we use Tuple[Float64, Float64], we can also write it more simply\nas (Float64, Float64) using parentheses just like in Python. Now we can write\nthe gradient descent iteration loop and we'll use the parentheses style for\nTuple. This simplifies compare List[Tuple[Float64, Float64, Float64]]() vs\nList[(Float64, Float64, Float64)]() below:\n\nMojo\n\n# Gradient Descent function def gradient_descent(params: Dict[String,Float64])\n-> List[(Float64, Float64, Float64)]: assert_true(params, \"Optimization\nparameters are empty\") x = params['initial_x'] y = params['initial_y'] history\n= List[(Float64, Float64, Float64)]() history.append((x, y, rosenbrock(x, y)))\nfor _ in range(params['num_iterations']): grad = rosenbrock_gradient(x, y) x\n-= params['learning_rate'] * grad[0] y -= params['learning_rate'] * grad[1] fx\n= rosenbrock(x, y) history.append((x, y, fx)) return history\n\nHere we use List to store gradients and function evaluation at each iteration\nusing history.append((x, y, rosenbrock(x, y)))\n\nWe also capture the Tuple output of rosenbrock_gradient in grad. You can index\ninto grad to access dx = grad[0] and dy = grad[1] which we use to update x and\ny.\n\nFinally, we call the gradient_descent function with a dictionary of parameters\nparams:\n\nMojo\n\n# Parameters stored in a dictionary params = Dict[String,Float64]()\nparams['initial_x'] = 0.0 params['initial_y'] = 3.0 params['learning_rate'] =\n0.001 params['num_iterations'] = 10000 # Run gradient descent history =\ngradient_descent(params) # Calculate minimum of Rosenbrock function min_val =\nhistory[-1] # Print results print(\"Minimum of Rosenbrock function:\") print(\"x\n=\", min_val[0]) print(\"y =\", min_val[1]) print(\"Function value at minimum\npoint:\", min_val[2]) np_arr1 = np.empty((len(history), 3)) for i in\nrange(len(history)): np_arr1[i]=history[i] plot_results(np_arr1, min_val)\n\nOutput\n\nOutput\n\nMinimum of Rosenbrock function: x = 0.99466952190364388 y =\n0.98934605887632932 Function value at minimum point: 2.8459788146378422e-05\n\nNote: plot_results() is a Python function that I call from Mojo using Mojo-\nPython interop. Since we capture all the iterations of (x,y) in our\nList[Tuple] variable history we have all the information we need to generate\nthese plots below. We do, however need to covert history into a NumPy array\nbefore we call our Python function, which is what we do in the for loop at the\nend. You can find the implementation of this function on GitHub along with\nrest of the code.\n\nIn the plot above (click to zoom) you can see that we start at an initial\npoint (0,3) and gradient descent takes us to the global minima at (1,1)\n\nWe can use the new update() function in Dict to update using\nparams.update(new_params) to change our initial point to (-1.5,3) and re-run\nthe optimization:\n\nMojo\n\nnew_params = Dict[String,Float64]() new_params['initial_x'] = -1.5\nnew_params['initial_y'] = 3 params.update(new_params) # Run gradient descent\nhistory = gradient_descent(params) # Calculate minimum of Rosenbrock function\nmin_val = history[-1] # Print results print(\"Minimum of Rosenbrock function:\")\nprint(\"x =\", min_val[0]) print(\"y =\", min_val[1]) print(\"Function value at\nminimum point:\", min_val[2]) np_arr2 = np.empty((len(history), 3)) for i in\nrange(len(history)): np_arr2[i]=history[i] plot_results(np_arr2, min_val)\n\nOutput\n\nOutput\n\nMinimum of Rosenbrock function: x = 0.98963752635944247 y =\n0.97934070791671202 Function value at minimum point: 0.00010755496303921971\n\nWith the new initial point you can see in the contour plot (click to zoom),\nthat due to the narrowness of the valley, the gradient descent algorithm\novershoots the minimum and bounces back and forth across the valley, causing\noscillations. Such problems are common in numerical optimization problems and\ncan lead to slow or premature convergence. This tells us that we can explore\ndifferent learning parameters (or hyperparameters in machine learning) or\nother types of optimizers to converge faster.\n\n### Enhancements to Set\n\nSets are unordered collections of unique elements, allowing for efficient\nmembership tests and mathematical set operations. An example of using Sets can\nbe to identify unique genetic markers or species from a large dataset of DNA\nsequences. Sets can automatically handle duplicates, and offer efficient\noperations for mathematical set concepts like unions, intersections, and\ndifferences.\n\nIn this release, Set introduces named methods that mirror operators, thanks to\ncontributions from @arvindavoudi\n\nLet\u2019s take a look at a simple example that compares both operator based and\nthe new method based operations on the set. Let\u2019s define two sets with\ndifferent genetic markers:\n\nMojo\n\nfrom collections import Set fn print_set(set: Set[String]): for element in\nset: print(element[],end=\" \") print() # Define sets of genetic markers for two\npopulations set_A_markers = Set[String]('ATGC', 'CGTA', 'GCTA', 'TACG',\n'AAGC') set_B_markers = Set[String]('CGTA', 'CAGT', 'GGCA', 'ATGC', 'TTAG')\n\nWe can use both difference method and difference operator to subtract both\nsets:\n\nMojo\n\n# Using methods # Difference using difference() print(\"Difference:\")\nprint_set(set_A_markers.difference(set_B_markers)) print_set(set_A_markers -\nset_B_markers) print()\n\nOutput\n\nOutput\n\nDifference: GCTA TACG AAGC GCTA TACG AAGC\n\nSimilarly, we can perform intersection_update using the method and the\noperator &= :\n\nMojo\n\nprint(\"Intersection update:\") common_markers_1 = Set[String](set_A_markers) #\nCopy common_markers_2 = Set[String](set_A_markers) # Copy\ncommon_markers_1.intersection_update(set_B_markers) common_markers_2 &=\nset_B_markers print_set(common_markers_1) print_set(common_markers_2) print()\n\nOutput\n\nOutput\n\nIntersection update: ATGC CGTA ATGC CGTA\n\nFinally, we can use the new update method to update a set:\n\nMojo\n\nprint(\"Update:\") updated_A = Set[String](set_A_markers) # Copy new_markers =\nSet[String]('AACG', 'TTAG') updated_A.update(new_markers) print_set(updated_A)\nprint()\n\nOutput\n\nOutput\n\nUpdate: ATGC CGTA GCTA TACG AAGC AACG TTAG\n\n### New reversed() function for reversed iterator\n\nThis release includes a new reversed() function for reversed iterators thanks\nto community contribution from @helehex @jayzhan211. In this example below, we\nreverse the words in a sentence using the new reversed iterator and by using\nList\u2019s reverse() method and compare their results:\n\nMojo\n\ndef reverse_list(sentence: String)->String: words = sentence.split(\" \")\nwords.reverse() reversed_sentence = String(\"\") for w in words:\nreversed_sentence += w[]+\" \" return reversed_sentence def\nreverse_iterator(sentence: String)->String: words = sentence.split(\" \")\nreversed_sentence = String(\"\") for w in reversed(words): reversed_sentence +=\nw[]+\" \" return reversed_sentence original_sentence = \"Hello world, this is a\ntest sentence.\" print(\"Original sentence:\", original_sentence)\nreversed_sentence = reverse_list(original_sentence) print(\"Reversed list\nsentence:\", reversed_sentence) reversed_sentence =\nreverse_iterator(original_sentence) print(\"Reversed iterator sentence:\",\nreversed_sentence)\n\nOutput\n\nOutput\n\nOriginal sentence: Hello world, this is a test sentence. Reversed list\nsentence: sentence. test a is this world, Hello Reversed iterator sentence:\nsentence. test a is this world, Hello\n\nreversed() function for reversed iterator also supports Dicts.\n\n### New parametric indices in __getitem__() and __setitem__(), and Dict, List,\nand Set conform to the new Boolable trait.\n\nMojo 24.3 also includes new core language enhancements and introduces a new\nBoolable trait. In the example below we\u2019ll explore both these features. We\u2019ll\ncreate a struct called MyStaticArray whose size is known at compile time. For\nthe MyStaticArray struct we can choose to define parametric indices\n__getitem__[idx: Int](self) and use compile-time checks on the requested\nindex. This is in contrast to using __getitem__(self, idx: Int) which can be\nused when the size of the array is unknown at compile time. Let\u2019s take a\ncloser look at the struct:\n\nMojo\n\nfrom memory import memset_zero struct MyStaticArray[size:Int,\ndtype:DType=DType.float64](Boolable): var _ptr: DTypePointer[dtype]\n@always_inline fn __init__(inout self): self._ptr = DTypePointer[dtype]() fn\n__init__(inout self, *data: Scalar[dtype]): self._ptr =\nDTypePointer[dtype].alloc(size) memset_zero[dtype](self._ptr, size) for i in\nrange(size): self._ptr[i] = data[i] if len(data)>size: print(\"Ignoring all\nvalues >\",size,\"number of values\") fn __getitem__[idx: Int](self) ->\nScalar[dtype]: constrained[idx<size, \"Index value must be less than static\narray size\"]() return self._ptr.load(idx) fn __del__(owned self):\nself._ptr.free() fn __len__(self) -> Int: return size fn __bool__(self) ->\nBool: return Bool(self._ptr) fn __str__(self) -> String: var s: String = \"\" s\n+= \"[\" for i in range(len(self)): if i>0: s+=\" \" s+=self._ptr[i] s+=\"]\" return\ns\n\nLet\u2019s instantiate the MyStaticArray and since it conforms to Boolable trait,\nwe can check if the array is empty. We can use Bool(arr) or just arr in the if\ncondition, we show both approaches below:\n\nMojo\n\nvar arr = MyStaticArray[6](1,2,3,4,5,6) if arr: print(arr) print(\"1st element\narr[0]:\", arr[0]) else: print(\"This array is empty :( \") var empty_arr =\nMyStaticArray[4]() if Bool(empty_arr): print(empty_arr) else: print(\"This\narray is empty :( \")\n\nOutput\n\nMojo\n\n[1.0 2.0 3.0 4.0 5.0 6.0] 1st element arr[0]: 1.0 This array is empty :(\n\nNow let\u2019s try to get an item from the MyStaticArray whose index is larger than\nthe size of the array.\n\nMojo\n\nprint(arr[50])\n\nOutput\n\nOutput\n\nconstraint failed: Index value must be less than static array size\n\nThis fails the constrained[idx<size, ...]() test in __getitem__[]() function.\n\n### But wait, there is so much more!\n\nMojo 24.3 is a huge release and I barely scratched the surface in this blog\npost. While this blog post was focused on community contributions, standard\nlibrary enhancements, and a few core language enhancements, there is a lot\nmore in this release that also caters to low-level system programming. I\nencourage you to check out the detailed list of what\u2019s new, changed, moved,\nrenamed, and fixed, check out the changelog in the documentation. A few other\nnotable features from the changelog:\n\n  * Core Language: Improvements to variadic arguments support.\n  * Core language: Allows users to capture the source location of code and call the location of functions dynamically using the __source_location() and __call_location() functions.\n  * Standard Library: FileHandle.seek() now has a \"whence\" argument similar to Python.\n  * Docs: New Types page.\n\nMAX 24.3 is also available for download today and includes several\nenhancements including preview of Custom Operator Extensibility support which\nallows you to write custom operators for MAX models using the Mojo for\nintuitive and performant extensibility. Read more in the What\u2019s new in MAX\n24.3 blog post.\n\nAll the examples I used in this blog post are available in a Jupyter Notebook\non GitHub, check it out!\n\n  * Download MAX and Mojo.\n  * Head over to the docs to read the Mojo\ud83d\udd25 manual and learn about APIs.\n  * Explore the examples on GitHub.\n  * Join our Discord community.\n  * Contribute to discussions on the Mojo GitHub.\n  * Read and subscribe to Modverse Newsletter.\n  * Read Mojo blog posts, watch developer videos and past live streams.\n  * Report feedback, including issues on our GitHub tracker.\n\nUntil next time! \ud83d\udd25\n\nProduct\n\n## MAX 24.3 - Introducing MAX Engine Extensibility\n\nMay 2, 2024\n\nRead post\n\nMAX Platform\n\nMAX Engine \ud83c\udfce\ufe0f\n\nMAX Serving \u26a1\ufe0f\n\nMojo \ud83d\udd25\n\nSign Up\n\nBlog\n\nCareers\n\nReport a security issue\n\nCopyright \u00a9\n\n2024\n\nModular Inc\n\nTerms\n\n,\n\nPrivacy\n\n&\n\nAcceptable Use\n\nShashank Prasanna\n\n,\n\nAI Developer Advocate\n\nShashank Prasanna\n\n,\n\nAI Developer Advocate\n\n", "frontpage": false}
