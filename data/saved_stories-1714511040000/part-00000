{"aid": "40211754", "title": "Can LLM's produce useful code?", "url": "https://Kshitij-Banerjee.github.io/2024/04/30/can-llms-produce-better-code/", "domain": "kshitij-banerjee.github.io", "votes": 2, "user": "kshitij_libra", "posted_at": "2024-04-30 15:02:10", "comments": 0, "source_title": "Can LLM's produce better code?", "source_text": "Can LLM's produce better code? | KiloBytes by KB\n\n# Can LLM's produce better code?\n\nApril 30, 2024\n\n# Introduction#\n\nIn my previous post, I tested a coding LLM on its ability to write React code.\nSpecifically, I tried the currently leading open source model in the\nHumanEval+ benchmark leaderboard - DeepseekCoder:33b-instruct.\n\nI used this model in development for a few weeks, and published a subset of\nexamples in the post. Even though I tried this on a relatively small problem\nsize, there were some obvious issues that were recognisable to me, namely:-\n\nThe randomness problem: LLMs are unable to produce correct code in the first\nattempt, however a few attempts (sometimes) leads to the correct code output.\n\nThe complexity problem: Smaller, more manageable problem with lesser\nconstraints are more feasible, than complex multi-constraint problem.\n\nFor example, if I would ask it to code a component and gave both styling and\nlogic constraints in the prompt, it would frequently solve the logic but miss\nthe styling part of the solution.\n\n(Hunch) Out of training problem: I also noticed that it spectacularly fails in\nsmaller sized problems for specific types. For example, while it can write\nreact code pretty well. It\u2019s ability of writing test cases was quite horrid,\nand will typically just write the test case name, and leave the implementation\nas a \u201cTODO: Fill this implementation...\u201d.\n\nSo I spent some time researching existing literature that could explain the\nreasoning, and potential solutions to these problems.\n\nHow the rest of the post is structured.\n\n  1. HumanEval+ - A summary on this rigorous evaluation of CodeLLMs and how they fair in this extended benchmark. There are some interesting insights and learnings about LLM behavior here.\n\n  2. The effect of Pre-Planning in code LLMs: Insights from this paper on how pre-planning helps produce better code\n\n  3. The effect of using a planning-algorithm (Monte Carlo Tree Search) in the LLM decoding process: Insights from this paper, that suggest using a planning algorithm can improve the probability of producing \u201ccorrect\u201d code, while also improving efficiency (when compared to traditional beam search / greedy search).\n\n  4. The effect of using a higher-level planning algorithm (like MCTS) to solve more complex problems: Insights from this paper, on using LLMs to make common sense decisions to improve on a traditional MCTS planning algorithm.\n\nOverall - I believe using a combination of these concepts can be viable\napproach to solving complex coding problems, with higher accuracy than using\nvanilla implementation of current code LLMs. I\u2019ll detail more insights and\nsummarise the key findings in the end.\n\n# Human Eval+#\n\nPaper: HumanEval+\n\n## Core Problem:#\n\nExisting code LLM benchmarks are insufficient, and lead to wrong evaluation of\nmodels. The authors found, that by adding new test cases to the HumanEval\nbenchmark, the rankings of some open source LLM\u2019s (Phind, WizardCoder)\novershot the scores for ChatGPT (GPT 3.5, not GPT4), which was previously\nincorrectly ranked higher than the others.\n\n## How they create tests cases#\n\nLiu et.al augmented the existing HumanEval test suite by\n\n  1. generating some seed inputs by prompting ChatGPT.\n\n  2. Using type-based mutations to generate more test inputs\n\n  3. Comparing the results on these additional inputs on the ground-truth solution to the LLM generated solutions\n\n  4. Adding these new (minimal-set-of) inputs into a new benchmark.\n\n# The results#\n\n## Insights from the paper#\n\n### 1) Increasing K in pass@k , almost always leads to improved benchmark\nresults.#\n\nThis proves that the correct solution does exist in the solution space of the\nLLM outputs most of the times, however it may not be the first one that the\nLLM spits out. Using a strategy that can guide the LLM towards the reward has\nthe potential to lead to better outcomes.\n\n### 2) Choosing a temperature value#\n\nWhen using a pass@1 (or single greedy output), choose a low temperate of 0.2\nor below\n\nFor a larger number of passes, a higher temperature value of 0.6 -> 0.8, will\nlead to good results.\n\n### 3) Practically, k=10 is a decent default to pick#\n\nThe improvement between k=1, and k=10 is pretty large. However this\nimprovement, is not really extrapolated in the same degree when moving from\nk=10, to k=100.\n\n### 4) New code models are coming up#\n\nComparing the results from the paper, to the current eval board, its clear\nthat the space is rapidly changing and new open source models are gaining\ntraction. (Deepseek-coder wasn\u2019t even present in the original paper)\n\n### 5) Llama3 is still behind#\n\nThis one was surprising to me, I thought the 70B LLama3-instruct model, being\nlarger and also trained on 15T tokens, would perform quite well.\n\nHowever, the benchmark shows its still behind deepseek, wizard and other open\nsource models\n\n# Pre-Planning in Code LLMs#\n\nPaper: Self-Planning Code Generation with LLM\n\n## Core Problem#\n\nWhile chain-of-thought adds some limited reasoning abilities to LLMs, it does\nnot work properly for code-outputs.\n\nTypically, CoT in code is done via creating sequences of comments interspersed\nwith code output.\n\nThis is because, while mentally reasoning step-by-step works for problems that\nmimic human chain of though, coding requires more overall planning than simply\nstep-by-step thinking.\n\n## How they solve it#\n\nAn obvious solution is to make the LLM think about a high level plan first,\nbefore it writes the code. This is precisely the subject of evaluation for\nthis paper.\n\nTo create such a plan the authors use few-shot learning examples to create\nplans.\n\n### What is a good plan ?#\n\nThe authors expect the plans to be in a specific fashion.\n\nNamely that it is a number list, and each item is a step that is executable as\na subtask.\n\nThe plan should always conclude with a return statement.\n\n## Results#\n\nAdding a self planning step, that adds a high-level plan before the\nimplementation starts-creates a 25% improvement in benchmark results.\n\nInterestingly, the authors also evaluate a multi-turn self planning step, and\nfind it inferior.\n\nIn the multi-turn approach, the LM Takes iterative turns to create a final\ncode output as opposed to producing the output in one-turn.\n\nThis seems counter-intuitive to me, given all the recent progress in Agentic\nLLMs.\n\nThey offer some clues:-\n\nThey find that the multi turn approach does not work as well as a one-shot\napproach because:-\n\n> This can be ascribed to two possible causes: 1) there is a lack of one-to-\n> one correspondence between the code snippets and steps, with the\n> implementation of a solution step possibly interspersed with multiple code\n> snippets; 2) LLM faces challenges in determining the termination point for\n> code generation with a sub-plan. Since the final goal or intent is specified\n> at the outset, this often results in the model persistently generating the\n> entire code without considering the indicated end of a step, making it\n> difficult to determine where to truncate the code. When implemented as a\n> one-phase process, the self-planning approach has been shown to yield\n> slightly improved performance compared to the two-phase way.\n\n### Insights and recommendations from the paper#\n\nConsidering limited LLM context windows.\n\nThe authors suggest a 2-phase plan + 8 shot examples approach produces best\nresults\n\n(2 phase in this context, does not mean 2 turns. It simply means, the LLM is\nprompted to prepare the plan first, and then the plan is concatenated to\nproduce the final output)\n\n> we generally recommend using either 8-shot or 4-shot for self-planning in\n> LLMs.\n\n# Planning algorithms in LLM Decoder#\n\nPaper: Planning with LLM for code gen\n\n## Problem#\n\nLLMs being probabilistic machines, they do not always create correct programs\nin a single run. However, if we sample the code outputs from an LLM enough\ntimes, usually the correct program lies somewhere in the sample set. The task\nof finding the correct output by sampling and filtering is costly.\nIntuitively, in sampling + filtering, we are not making use of any objectives\nto focus the search on the \u201ccorrect\u201d outputs, but merely hoping that the\ncorrect output lies somewhere in a large sample.\n\n> Can we integrate a planning algorithm with a pre-trained code generation\n> Transformer, achieving an algorithm that generates better programs than the\n> conventional Transformer generation algorithms and the well-accepted\n> sampling + filtering scheme in the literature?\n\n## Core Idea#\n\nThe core idea here is that we can search for optimal code outputs from a\ntransformer effectively by integrating a planning algorithm, like Monte Carlo\ntree search, into the decoding process as compared to a standard beam search\nalgorithm that is typically used.\n\n### Catch#\n\nFor this to work, we need to create a reward function with which to evaluate\ndifferent code outputs produced during the search of each branch in the\nsolution space. The reward function here is based on evaluating test-cases. So\nan explicit need for \u201ctestable\u201d code is required for this approach to work.\n\nBut assuming we can create tests, by providing such an explicit reward - we\ncan focus the tree search on finding higher pass-rate code outputs, instead of\nthe typical beam search of finding high token probability code outputs.\n\nIntuitively, transformers are built to produce outputs that match previously\nseen completions - which may not be the same as a program that is correct and\nsolves the overall problem.\n\nThe paper shows, that using a planning algorithm like MCTS can not only create\nbetter quality code outputs. But it is also more resource efficient as we do\nnot have to create a large amount of samples to use for filtering.\n\nTo achieve this efficiency, a caching mechanism is implemented, that ensures\nthe intermediate results of beam search and the planning MCTS do not compute\nthe same output sequence multiple times.\n\nAnother interesting idea, is to use these planner-guided solutions to fine-\ntune the LLM to improve its future outputs\n\n# LLM reasoning for MCTS#\n\nPaper: LLM-MCTS - Zhao et.al\n\n## The Question#\n\nThe core concept of this paper intrigues me. In essence, the paper tries to\nanswer the question - \u201cCan the reasoning abilities of LLM models, be used to\nguide a Monte Carlo search in finding the optimal answer to a higher-level\nproblem like object rearrangement in household\"\n\n## The 3 options#\n\nThe authors conduct comparison between 3 solutions\n\n  1. A pure LLM solution (LLM-Policy)\n\n  2. A planner guided solution called LLM-model (LLM guided MCTS), and\n\n  3. A hybrid approach (LLM-MCTS) where the LLMs reasoning is not used as a hard action, but used as part of a heuristic to continue the search process.\n\n## How it works#\n\n## Insights from the paper#\n\n#### 1\\. LLM-MCTS outperforms both the LLM-policy, and LLM-model#\n\n#### 2\\. The improvement is much more noticeable when the problem is\nComplicated / Novel as opposed to simple.#\n\nThis quantifies the initial intuition of this post, that LLM\u2019s are unable to\nsolve more complex/novel problems but perform much better for smaller/simpler\nproblems\n\nAn intuitive understanding of this, is best explained by the authors via an\nanalogy to the multiplication example. i.e: Multiplying large numbers is hard,\nbut using a algorithm that works on top of foundational concepts becomes a\nsimpler solve.\n\n> A decimal number is described as a sequence of n digits,(dn\u22121, dn\u22122, . . . ,\n> d0). There are two methods of implementing multiplication with an LLM. The\n> first one corresponds to L-Policy. We represent the multiplication function\n> as a table. Each row or column corresponds to a number. The table entry is\n> the multiplication of two numbers, obtained by querying an LLM.\n> Experimentally, GPT4 performs single-digit multiplication perfectly with\n> 100% accuracy, 2-digit multiplication with 99% accuracy, 4-digit\n> multiplication with merely 4% accuracy, and fails almost completely on\n> 5-digit multiplication [ 10 ]. The second approach uses LLM-derived small\n> single-digit multiplication tables, which GPT4 performs with 100% accuracy.\n> To multiply multi-digit numbers, it applies the long multiplication\n> algorithm with the single-digit table. This method corresponds to L-Model.\n> While long multiplication differs from planning in algorithmic details, it\n> plays the same role in L-Model and is highly efficient. Clearly, this second\n> method achieves100% accuracy for arbitrarily large numbers, provided that\n> the single-digit multiplication table is accurate. So, the L-Model\n> outperforms L-Policy for the multiplication task, contrary to the finding\n> for object rearrangement tasks\n\n## My take#\n\nIn scenarios where the problem is complex, it\u2019s solution space is large, AND\nthere exist some known planning algorithms that can solve the larger problem\nat play - its beneficial to let the planning algorithms take control of the\noverall process, but let the LLM guide the intelligent search process to\nutilise its inherent common sense and reasoning abilities.\n\nIn essence, when the problem becomes sufficiently complicated and large, it is\nhard for LLMs to solve the entire problem in one go. This is because, LLMs\ncan\u2019t stop, rethink and evaluate before they answer and lack any second-order\nthinking. Also, in complicated situations like path finding, a certain level\nof trial and error search is required to find the answer.\n\n# Summarising some key points#\n\n### LLM\u2019s can solve simpler problems well, but struggle with complex/novel\nproblems#\n\nFinding ways to break a complicated problem into smaller pieces, and using\nknown algorithms to combine the pieces is a viable idea.\n\n### Tests can be used to focus the LLM decoding process to optimise for\ncorrectness without fine-tuning.#\n\nIn problem spaces where we can use tests to judge program correctness (eg:\nTest cases, Visual comparison, comparison with ground-truth solutions), we can\nguide the LLM outputs towards correctness more efficiently.\n\nFurther, a planner-guided LLM output can be used to fine-tune the base model\nto improve its base accuracy\n\n### Pre-Planning and Chain-of-thought are cost-effective default solutions to\ntry#\n\nIt is intuitive, that making these models build a plan and working through the\nsolutions step-by-step would increase correctness, without increasing\ncomputation cost much, and as such should probably be defaulted into most\ndeveloper workflows.\n\n# Conclusion#\n\nThere are some promising ideas in the field of planner-augmented problem\nsolving that can be applied to code generation.\n\nRigorous study and evaluation is needed to confirm these ideas, and conduct\nfurther research.\n\nIf there are other papers related to the same field, or you have some\ninsights, I\u2019d would love to know more. Please reach out, or comment to share\nthem with me.\n\n  * Machine-Learning\n  * AI\n\n\u00a9 2024 KiloBytes by KB Powered by Hugo & PaperMod\n\n", "frontpage": false}
