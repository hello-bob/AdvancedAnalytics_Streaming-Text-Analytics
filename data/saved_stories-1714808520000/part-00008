{"aid": "40252402", "title": "Realtime Video Stream Analysis with Computer Vision", "url": "https://blog.roboflow.com/video-stream-analysis/", "domain": "roboflow.com", "votes": 1, "user": "jonbaer", "posted_at": "2024-05-03 21:07:04", "comments": 0, "source_title": "Realtime Video Stream Analysis with Computer Vision", "source_text": "Realtime Video Stream Analysis with Computer Vision\n\nProduct\n\nSolutions\n\nResources\n\nPricing\n\nDocs\n\nSign In\n\nSign Up\n\nCase Studies Object Detection Deployment\n\n# Realtime Video Stream Analysis with Computer Vision\n\nWritten by\n\nLeo Ueno\n\nMay 3, 2024\n\n5 min read\n\nRemote camera streams can be an effective way to monitor multiple locations in\nreal-time. Computer vision can quickly amplify the value of those streams by\nproviding analytics to maximize the value of each camera. Using existing\ncameras and making them AI-enabled is a great way to begin using AI in any\nphysical location whether for occupancy analytics, security augmentation,\ninfrastructure monitoring, or workplace safety.\n\n0:00\n\n/0:05\n\nIn this guide, you will learn how to build a real-time road congestion system\nusing computer vision. We will start with visualizing boxes for one camera,\nthen work our way up to determining congestion across multiple cameras, and\nend with a system that can monitor multiple streams from an NYC road camera\ndata source.\n\n\ud83d\udcd3\n\nAll the code used for this project is available in this Google Colab notebook\nwhich includes some code omitted or changed from this blog post for\nreadability.\n\n## Create or Use a Computer Vision Model\n\nFor our project, we will use a vehicle detection model already available on\nRoboflow Universe. You can use stream processing for almost any vision model,\nincluding pre-trained Universe models, your own Roboflow models, or YOLO\nmodels trained on the COCO dataset.\n\n\ud83d\udca1\n\nYou can also use foundation models like YOLO-World. See how to use\nInferencePipeline with YOLO-World and to build a custom model.\n\nFor a different use case or production use, it may be better to train a new\nmodel or fine-tune it with your own data. See guides on how you can train\nYOLOv8 or YOLOv9 on your custom data.\n\nOnce you\u2019ve selected a pre-trained model or trained your own, you\u2019re ready for\nthe next step.\n\n## Single Camera Stream Processing\n\nBefore we tackle multiple cameras, let\u2019s start by processing one video stream.\nTo do this, we will use the Inference Pipeline feature of the Inference\npackage. InferencePipeline works on a system of input stream(s) (called video\nreference(s)) and output processes. (called \u201csink(s)\u201d)\n\nThe video references can be local camera devices, video files, or URLs to\nlinks or streams. In this project, we will be using live webcam stream URLs\nprovided by New York City\u2019s Department of Transportation.\n\nFirst, we will try running InferencePipeline using a default sink,\nrender_boxes, to render and output bounding boxes.\n\n    \n    \n    from inference import InferencePipeline from inference.core.interfaces.stream.sinks import render_boxes pipeline = InferencePipeline.init( model_id=\"vehicle-detection-3mmwj/1\", max_fps=0.5, confidence=0.3, video_reference=\"https://webcams.nyctmc.org/api/cameras/053e8995-f8cb-4d02-a659-70ac7c7da5db/image\", on_prediction=render_boxes, api_key=\"*ROBOFLOW_API_KEY*\" ) pipeline.start() pipeline.join()\n\nAfter starting the InferencePipeline, we start seeing model predictions on\nlive frames straight from the webcam, where we can see the packed streets near\nTimes Square.\n\n0:00\n\n/0:05\n\nNow, let\u2019s create a custom output sink. For this project, we want our output\nsink to do two things:\n\n  1. Create a timelapse video of the live stream to review later\n  2. Count and record the number of vehicles detected at any one time\n\nFirst, we will do some setup:\n\n  * To keep a record of the vehicles, we will create and add to a Google Sheet (using the gspread package).\n  * For recording our video, we will use VideoWriter from the OpenCV (cv2) package.\n  * Since the live stream will go on indefinitely, when we want to stop processing, we need a way to release the VideoWriter so that it produces a valid video file. To do this, we will use the signal package to intercept a keyboard interrupt and close out the video writer.\n\n    \n    \n    # Imports from inference import InferencePipeline from datetime import datetime import pytz import supervision as sv # Google Colab from google.colab import auth, userdata api_key = userdata.get('ROBOFLOW_API_KEY') # Google Sheet Setup import gspread from google.auth import default auth.authenticate_user() creds, _ = default() googlesheets = gspread.authorize(creds) document = googlesheets.open_by_key('1tNGjQSJQqQ7j9BoIw4VcxPn_DIcai8zxv_IwcSRlh34') worksheet = document.worksheet('SingleCameraTest') # VideoWriter Setup import cv2 video_info = (352, 240, 60) # The size of the stream fourcc = cv2.VideoWriter_fourcc(*'mp4v') writer = cv2.VideoWriter(\"nyc_traffic_timelapse.mp4\", fourcc, video_info[2], video_info[:2]) # Interrupt Handling import signal import sys def signal_handler(sig, frame): writer.release() sys.exit(0) signal.signal(signal.SIGINT, signal_handler)\n\nThen, we will define our callback function, then our InferencePipeline again,\nand replace our on_prediction default sink with our new custom sink.\n\n    \n    \n    from inference import InferencePipeline from datetime import datetime import pytz def on_prediction(predictions, video_frame, writer): # Process Results detections = sv.Detections.from_inference(predictions) annotated_frame = sv.BoundingBoxAnnotator( thickness=1 ).annotate(video_frame.image, detections) # Add Frame To Timelapse writer.write(annotated_frame) # Format data for Google Sheets ET = pytz.timezone('America/New_York') time = datetime.now(ET).strftime(\"%H:%M\") fields = [time, len(detections)] print(fields) # Add to Google Sheet worksheet.append_rows([fields], \"USER_ENTERED\") pipeline = InferencePipeline.init( model_id=\"vehicle-detection-3mmwj/1\", max_fps=0.5, confidence=0.3, video_reference=\"https://webcams.nyctmc.org/api/cameras/053e8995-f8cb-4d02-a659-70ac7c7da5db/image\", on_prediction=lambda predictions, video_frame: on_prediction(predictions, video_frame, writer), api_key=api_key ) pipeline.start() pipeline.join()\n\nAs the stream starts being processed, we see the Google Sheet start populating\nwith vehicle counts.\n\nA screenshot of the previously referenced Google Sheet recording the time and\nvehicles detected\n\nAfter we stop the stream, we can check out the time-lapsed video along with\nthe complete graph.\n\n0:00\n\n/0:05\n\nThe timelapse video playing at 5x speed (left) and the graph generated from\nthe vehicle counts in the spreadsheet (right) (bar was added in post-\nprocessing)\n\n## Multi-Camera Stream Processing\n\nInferencePipeline makes it simple to drop in a camera stream and run computer\nvision models on it, and with some modifications to our code, we can make it\nrun on several different streams.\n\n\ud83d\udca1\n\nThe following code omits some repetitive parts from the previous section. See\nthe Colab linked earlier for the full code.\n\nFor this project, we will use three different street cameras to keep track of\nthe stream URLs and street locations. We will also need to create separate\nVideoWriter instances for each camera.\n\n    \n    \n    cameras = { \"5th Ave @ 34 St\": \"https://webcams.nyctmc.org/api/cameras/3a3d7bc0-7f35-46ba-9cca-75fe83aac34d/image\", \"2 Ave @ 74 St\": \"https://webcams.nyctmc.org/api/cameras/6316453d-6161-4b98-a8e7-0e36c69d267c/image\", \"E 14 St @ Irving Pl\": \"https://webcams.nyctmc.org/api/cameras/f9cb9d4c-10ad-42e4-8997-dbc9e12bd55a/image\" } camera_writers = [ cv2.VideoWriter(f\"{location}.mp4\", fourcc, video_info[2], video_info[:2]) for location in cameras.keys() ]\n\nThen, we will modify our sink and create a camera processing function.\n\n    \n    \n    from inference.core.interfaces.stream.inference_pipeline import SinkMode def process_camera(predictions, video_frame, location): # Process Results detections = sv.Detections.from_inference(predictions) annotated_frame = sv.BoundingBoxAnnotator( thickness=1 ).annotate(video_frame.image, detections) vehicles = len(detections) # Add to Google Sheet ET = pytz.timezone('America/New_York') time = datetime.now(ET).strftime(\"%H:%M\") worksheet = document.worksheet(location) print(location,\"has\",vehicles,\"cars\") fields = [time, vehicles] worksheet.append_rows([fields], \"USER_ENTERED\") return annotated_frame def on_prediction(predictions, video_frame, camera_writers): idx = video_frame.source_id annotated_frame = process_camera(predictions,video_frame,list(cameras.keys())[idx]) camera_writers[idx].write(annotated_frame) pipeline = InferencePipeline.init( model_id=\"vehicle-detection-3mmwj/1\", max_fps=0.5, confidence=0.3, video_reference=list(cameras.values()), on_prediction=lambda predictions, video_frame: on_prediction(predictions, video_frame, camera_writers), api_key=api_key, sink_mode=SinkMode.SEQUENTIAL # Sequential mode means each prediction will trigger one sink call ) pipeline.start() pipeline.join()\n\nOnce we start the pipeline, the sheet will start populating again and once we\nstop it, we can combine the generated graphs with the time-lapsed videos.\n\n0:00\n\n/0:05\n\n## Conclusion\n\nUsing the Inference package, we were able to analyze several live streams from\nvarious locations around New York City. This project can be adapted and\nextended to monitor video streams from almost any source for any use case from\nretail camera analysis to insights for occupancy statistics.\n\n## Cite this Post\n\nUse the following entry to cite this post in your research:\n\nLeo Ueno. (May 3, 2024). Realtime Video Stream Analysis with Computer Vision.\nRoboflow Blog: https://blog.roboflow.com/video-stream-analysis/\n\n## Discuss this Post\n\nIf you have any questions about this blog post, start a discussion on the\nRoboflow Forum.\n\n### Leo Ueno\n\nML Growth Associate @ Roboflow | Sharing the magic of computer vision | leoueno.com\n\nVIEW MORE POSTS\n\nTOPICS:\n\nCase Studies, Object Detection, Deployment\n\nLeo Ueno\n\nMay 3, 2024\n\n5 min read\n\nSearch blog\n\n###### Table of Contents\n\n  1. Create or Use a Computer Vision Model\n  2. Single Camera Stream Processing\n  3. Multi-Camera Stream Processing\n  4. Conclusion\n\n#### MORE ABOUT\n\n#### Case Studies\n\nView All\n\n##### Detect and Describe Flowers with Computer Vision and Generative AI\n\nContributing Writer\n\nApr 30, 2024\n\n##### Using Computer Vision to Create AR Experiences\n\nTrevor Lynn\n\nApr 10, 2024\n\n##### Counting Rebar with Computer Vision\n\nContributing Writer\n\nApr 3, 2024\n\n##### Coffee Bean Inspection with Computer Vision\n\nJames Gallagher\n\nMar 18, 2024\n\n##### Monitor and Analyze Retail Queues Using Computer Vision\n\nContributing Writer\n\nMar 13, 2024\n\n##### Assess Car Damage with Computer Vision\n\nContributing Writer\n\nMar 11, 2024\n\nWant to learn more about Roboflow? Email sales@roboflow.com or talk to sales\nwith our sales team.\n\n\u00a9 2024 Roboflow, Inc. All rights reserved.\n\n#### For sales inquiries:\n\nsales@roboflow.comBook a demo\n\n#### PRODUCT\n\n  * Sign In / Sign Up\n  * Universe\n  * Annotate\n  * Train\n  * Deploy\n  * Integrations\n  * Pricing\n\n#### ECosystem\n\n  * Notebooks\n  * Autodistill\n  * Supervision\n  * Inference\n  * Roboflow 100\n  * Open Source\n\n#### DEVELOPERS\n\n  * User Forum\n  * Templates\n  * Blog\n  * Learn Computer Vision\n  * Convert Annotation Formats\n  * Computer Vision Models\n\n#### Industries\n\n  * Manufacturing\n  * Oil and Gas\n  * Retail\n  * Safety & Security\n  * Transportation\n  * All Industries\n\n## MODELS\n\n  * YOLOv9\n  * YOLOv8\n  * YOLOv5\n  * YOLO-World\n  * YOLO-NAS\n  * CLIP\n  * Grounding DINO\n  * Multimodal Models\n  * Explore More Models\n\n## COMPANY\n\n  * About Us\n  * Careers\n  * Press\n  * Terms of Service\n  * Privacy Policy\n  * Roboflow Sitemap\n  * Blog Sitemap\n  * Status\n\n", "frontpage": false}
