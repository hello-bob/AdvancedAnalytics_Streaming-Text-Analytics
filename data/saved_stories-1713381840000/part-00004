{"aid": "40062679", "title": "Lingo-2: Driving with Natural Language", "url": "https://wayve.ai/thinking/lingo-2-driving-with-language/", "domain": "wayve.ai", "votes": 1, "user": "Chirono", "posted_at": "2024-04-17 10:27:41", "comments": 0, "source_title": "LINGO-2: Driving with Natural Language", "source_text": "LINGO-2: Driving with Natural Language - Wayve\n\n  * Technology\n  * Product\n  * Science\n  * Careers\n  * Company\n  * Blog\n\nTechnology\n\nPioneering Embodied AI for Autonomy\n\nAV2.0\n\nNext-gen autonomous driving technology based on end-to-end AI\n\nFleet learning loop\n\nTraining, evaluating and deploying foundation models for autonomy\n\nSafety 2.0 framework\n\nEmbracing a new paradigm in AV safety\n\nBack\n\nProduct\n\nAdvanced AI technology for safer, smarter driving\n\nWayve AI Driver\n\nEnabling automakers to unlock all levels of driving automation at scale\n\nFleet solutions\n\nWayve AI Driver for last-mile delivery and ridehail\n\nMeet our fleet\n\nData collection and development vehicles for assisted and automated driving\n\nBack\n\nScience\n\nAdvancing end-to-end autonomous driving research\n\nGAIA\n\nGenerative AI world model that can generate realistic driving videos from text\n\nLINGO\n\nUsing natural language to train and explain AI driving models\n\nCVPR 2024\n\nJoin Wayve at CVPR 2024. Don\u2019t miss our tutorial on end-to-end autonomy.\n\nBack\n\nCareers\n\nWayve is a place where you can shape the future of autonomous mobility\n\nOur people\n\nLearn more about our team and why they joined Wayve\n\nOur journey\n\nHow we started and where we are today\n\nDiversity, Equity & Inclusion\n\nWe strive to create a more equitable world\n\nJoin us\n\nAll of our current open roles\n\nBack\n\nCompany\n\nWe are proudly reimagining autonomous mobility\n\nLeadership\n\nMeet the people leading Wayve\n\nLocations\n\nSee where Wayve operates\n\nInvestors\n\nWe\u2019re proud to be backed by investors who share our vision\n\nData Privacy\n\nHow we protect individuals\u2019 data\n\nBack\n\nBlog\n\nCompany blog\n\nInsights. Ideas. Research.\n\nCompany news\n\nStay up-to-date with what\u2019s new at Wayve\n\nPress kit\n\nAccess branding elements, images and videos for press\n\nBack\n\nTechnology\n\nPioneering Embodied AI for Autonomy\n\nAV2.0\n\nNext-gen autonomous driving technology based on end-to-end AI\n\nFleet learning loop\n\nTraining, evaluating and deploying foundation models for autonomy\n\nSafety 2.0 framework\n\nEmbracing a new paradigm in AV safety\n\nBack\n\nProduct\n\nAdvanced AI technology for safer, smarter driving\n\nWayve AI Driver\n\nEnabling automakers to unlock all levels of driving automation at scale\n\nFleet solutions\n\nWayve AI Driver for last-mile delivery and ridehail\n\nMeet our fleet\n\nData collection and development vehicles for assisted and automated driving\n\nBack\n\nScience\n\nAdvancing end-to-end autonomous driving research\n\nGAIA\n\nGenerative AI world model that can generate realistic driving videos from text\n\nLINGO\n\nUsing natural language to train and explain AI driving models\n\nCVPR 2024\n\nJoin Wayve at CVPR 2024. Don\u2019t miss our tutorial on end-to-end autonomy.\n\nBack\n\nCareers\n\nWayve is a place where you can shape the future of autonomous mobility\n\nOur people\n\nLearn more about our team and why they joined Wayve\n\nOur journey\n\nHow we started and where we are today\n\nDiversity, Equity & Inclusion\n\nWe strive to create a more equitable world\n\nJoin us\n\nAll of our current open roles\n\nBack\n\nCompany\n\nWe are proudly reimagining autonomous mobility\n\nLeadership\n\nMeet the people leading Wayve\n\nLocations\n\nSee where Wayve operates\n\nInvestors\n\nWe\u2019re proud to be backed by investors who share our vision\n\nData Privacy\n\nHow we protect individuals\u2019 data\n\nBack\n\nBlog\n\nCompany blog\n\nInsights. Ideas. Research.\n\nCompany news\n\nStay up-to-date with what\u2019s new at Wayve\n\nPress kit\n\nAccess branding elements, images and videos for press\n\nBack\n\n  * Technology\n  * Product\n  * Company news\n  * Contact\n\ntwitter linkedin youtube\n\n17 April 2024 | Research\n\n# LINGO-2: Driving with Natural Language\n\nThis blog introduces LINGO-2, a driving model that links vision, language, and\naction to explain and determine driving behavior, opening up a new dimension\nof control and customization for an autonomous driving experience. LINGO-2 is\nthe first closed-loop vision-language-action driving model (VLAM) tested on\npublic roads.\n\n9 minute read\n\nIn September 2023, we introduced natural language for autonomous driving in\nour blog on LINGO-1, an open-loop driving commentator that was a first step\ntowards trustworthy autonomous driving technology. In November 2023, we\nfurther improved the accuracy and trustworthiness of LINGO-1\u2019s responses by\nadding a \u201cshow and tell\u201d capability through referential segmentation. Today,\nwe are excited to present the next step in Wayve\u2019s pioneering work\nincorporating natural language to enhance our driving models: introducing\nLINGO-2, a closed-loop vision-language-action driving model (VLAM) that is the\nfirst driving model trained on language tested on public roads. In this blog\npost, we share the technical details of our approach and examples of LINGO-2\u2019s\ncapability to combine language and action to accelerate the safe development\nof Wayve\u2019s AI driving models.\n\n## Introducing LINGO-2, a closed-loop Vision-Language-Action-Model (VLAM)\n\nOur previous model, LINGO-1, was an open-loop driving commentator that\nleveraged vision-language inputs to perform visual question answering (VQA)\nand driving commentary on tasks such as describing scene understanding,\nreasoning, and attention\u2014providing only language as an output. This research\nmodel was an important first step in using language to understand what the\nmodel comprehends about the driving scene. LINGO-2 takes that one step\nfurther, providing visibility into the decision-making process of a driving\nmodel. LINGO-2 combines vision and language as inputs and outputs, both\ndriving action and language, to provide a continuous driving commentary of its\nmotion planning decisions. LINGO-2 adapts its actions and explanations in\naccordance with various scene elements and is a strong first indication of the\nalignment between explanations and decision-making. By linking language and\naction directly, LINGO-2 sheds light on how AI systems make decisions and\nopens up a new level of control and customization for driving.\n\nPlease change your cookie settings to view embedded content, or view this\nvideo on YouTube.\n\nThe above video is taken from a LINGO-2 drive through Central London. The same\ndeep learning model generates the driving behavior and textual predictions in\nreal-time.\n\nWhile LINGO-1 could retrospectively generate commentary on driving scenarios,\nits commentary was not integrated with the driving model. Therefore, its\nobservations were not informed by actual driving decisions. However, LINGO-2\ncan both generate real-time driving commentary and control a car. The linking\nof these fundamental modalities underscores the model\u2019s profound understanding\nof the contextual semantics of the situation, for example, explaining that\nit\u2019s slowing down for pedestrians on the road or executing an overtaking\nmaneuver. It\u2019s a crucial step towards enhancing trust in our assisted and\nautonomous driving systems. It opens up new possibilities for accelerating\nlearning with natural language by incorporating a description of driving\nactions and causal reasoning into the model\u2019s training. Natural language\ninterfaces could, even in the future, allow users to engage in conversations\nwith the driving model, making it easier for people to understand these\nsystems and build trust.\n\n### LINGO-2 Architecture: Multi-modal Transformer for Driving\n\nLINGO-2 architecture\n\nLINGO-2 consists of two modules: the Wayve vision model and the auto-\nregressive language model. The vision model processes camera images of\nconsecutive timestamps into a sequence of tokens. These tokens and additional\nconditioning variables \u2013 such as route, current speed, and speed limit \u2013 are\nfed into the language model. Equipped with these inputs, the language model is\ntrained to predict a driving trajectory and commentary text. Then, the car\u2019s\ncontroller executes the driving trajectory.\n\n### LINGO-2\u2019s New Capabilities\n\nThe integration of language and driving opens up new capabilities for\nautonomous driving and human-vehicle interaction, including:\n\n  1. Adapting driving behavior through language prompts: We can prompt LINGO-2 with constrained navigation commands (e.g., \u201cpull over,\u201d \u201cturn right,\u201d etc.) and adapt the vehicle\u2019s behavior. This has the potential to aid model training or, in some cases, enhance human-vehicle interaction.\n  2. Interrogating the AI model in real-time: LINGO-2 can predict and respond to questions about the scene and its decisions while driving.\n  3. Capturing real-time driving commentary: By linking vision, language, and action, LINGO-2 can leverage language to explain what it\u2019s doing and why, shedding light on the AI\u2019s decision-making process.\n\nWe\u2019ll explore these use cases in the sections below, showing examples of how\nwe\u2019ve tested LINGO-2 in our neural simulator Ghost Gym. Ghost Gym creates\nphotorealistic 4D worlds for training, testing, and debugging our end-to-end\nAI driving models. Given the speed and complexity of real-world driving, we\nleverage offline simulation tools like Ghost Gym to evaluate the robustness of\nLINGO-2\u2019s features first. In this setup, LINGO-2 can freely navigate through\nan ever-changing synthetic environment, where we can run our model against the\nsame scenarios with different language instructions and observe how it adapts\nits behavior. We can gain deep insights and rigorously test how the model\nbehaves in complex driving scenarios, communicates its actions, and responds\nto linguistic instructions.\n\n## Adapting Driving Behavior through Linguistic Instructions\n\nLINGO-2 uniquely allows driving instruction through natural language. To do\nthis, we swap the order of text tokens and driving action, which means\nlanguage becomes a prompt for the driving behavior. This section demonstrates\nthe model\u2019s ability to change its behavior in our neural simulator in response\nto language prompts for training purposes. This new capability opens up a new\ndimension of control and customization. The user can give commands or suggest\nalternative actions to the model. This is of particular value for training our\nAI and offers promise to enhance human-vehicle interaction for applications\nrelated to advanced driver assistance systems. In the examples below, we\nobserve the same scenes repeated, with LINGO-2 adapting its behavior to follow\nlinguistic instructions.\n\n### Example 1: Navigating a junction\n\nIn the three videos below, LINGO-2 navigates the same junction but is given\ndifferent instructions: \u201cturning left, clear road,\u201d \u201cturning right, clear\nroad,\u201d and \u201cstopping at the give way line.\u201d We observe that LINGO-2 can follow\nthe instructions, reflected by different driving behaviors at the\nintersection.\n\nExample of LINGO-2 driving in Ghost Gym and being prompted to turn left on a\nclear road.\n\nExample of LINGO-2 driving in Ghost Gym and being prompted to turn right on a\nclear road.\n\nExample of LINGO-2 driving in Ghost Gym and being prompted to stop at the\ngive-way line.\n\n### Example 2: Navigating a bus\n\nIn the two videos below, LINGO-2 navigates around a bus. We can observe that\nLINGO-2 can follow the instructions to either hold back and \u201cstop behind the\nbus\u201d or \u201caccelerate and overtake the bus.\u201d\n\nExample of LINGO-2 in Wayve\u2019s Ghost Gym stopping behind the bus when\ninstructed.\n\nExample of LINGO-2 in Wayve\u2019s Ghost Gym overtaking a bus when instructed by\ntext.\n\n### Example 3: Driving in a residential area\n\nIn the two videos below, LINGO-2 responds to linguistic instruction when\ndriving in a residential area. It can correctly respond to the prompts\n\u201ccontinue straight to follow the route\u201d or \u201cslow down for an upcoming turn.\u201d\n\nExample of LINGO-2 in Wayve\u2019s Ghost Gym driving straight when instructed by\ntext.\n\nExample of LINGO-2 in Wayve\u2019s Ghost Gym turning right when instructed by text.\n\n## Interrogating an AI model in real-time: Video Question Answering (VQA)\n\nAnother possibility for language is to develop a layer of interaction between\nthe robot car and the user that can give confidence in the decision-making\ncapability of the driving model. Unlike our previous LINGO-1 research model,\nwhich could only answer questions retrospectively and was not directly\nconnected to decision-making, LINGO-2 allows us to interrogate and prompt the\nactual model that is driving.\n\n### Example 4: Traffic Lights\n\nIn this example, we show LINGO-2 driving through an intersection. When we ask\nthe model, \u201cWhat is the color of the traffic lights?\u201d it correctly responds,\n\u201cThe traffic lights are green.\u201d\n\nExample of LINGO-2 VQA in Ghost Gym\n\n### Example 5: Hazard Identification\n\nIn this example, LINGO-2 is prompted by the question, \u201cAre there any hazards\nahead of you?\u201d It correctly identifies that \u201cYes, there is a cyclist ahead of\nme, which is why I am decelerating.\u201d\n\nExample of LINGO-2 VQA in Ghost Gym\n\n### Example 6: Weather\n\nIn the following three examples, we ask LINGO-2 to describe \u201cWhat is the\nweather like?\u201d It can correctly identify that the weather ranges from \u201cvery\ncloudy, there is no sign of the sun\u201d to \u201csunny\u201d to \u201cthe weather is clear with\na blue sky and scattered clouds.\u201d\n\nExample of LINGO-2 VQA in Ghost Gym\n\n## Limitations\n\nLINGO-2 marks a step-change in our progress to leverage natural language to\nenhance our AI driving models. While we are excited about the progress we are\nmaking, we also want to describe the current limitations of the model.\n\nLanguage explanations from the driving model give us a strong idea of what the\nmodel might be thinking. However, more work is needed to quantify the\nalignment between explanations and decision-making. Future work will quantify\nand strengthen the connection between language, vision, and driving to\nreliably debug and explain model decisions. We expect to show in the real\nworld that adding intermediate language reasoning in \u201cchain-of-thought\u201d\ndriving helps solve edge cases and counterfactuals.\n\nAdditionally, we plan to investigate whether controlling the car\u2019s behavior\nwith language in real-world settings can be done reliably and safely. Ghost\nGym provides a safe off-road environment for testing, but more work needs to\nbe done to ensure the model is robust to noise and misinterpretation of the\ncommands. It should understand the context of human instructions while never\nviolating appropriate limits of safe and responsible driving behavior. This\nfunctionality will be more suited to aid model testing and training for fully\nautomated driving systems.\n\n## Conclusion\n\nIn this post, we have introduced LINGO-2, the first driving model trained on\nlanguage that has driven on public roads. We are excited to showcase how\nLINGO-2 can respond to language instruction and explain its driving actions in\nreal-time. This is a first step towards building embodied AI that can perform\nmultiple tasks, starting with language and driving. Join us at CVPR 2024 to\nexplore this and other topics at our \u201cEnd-to-End Autonomy: A New Era of Self-\nDriving\u201d Tutorial.\n\n14 March 2024 | Leadership\n\n### Solving the long-tail with e2e AI: \u201cThe revolution will not be supervised\u201d\n\nErez Dagan, President of Wayve, shares his thoughts on how end-to-end (e2e)\nEmbodied AI is, by design, uniquely equipped to solve the long-tail problem of\ndriving automation.\n\nRead morerr\n\n21 December 2023 | Research\n\n### Ghost Gym: A Neural Simulator for Autonomous Driving\n\nDiscover Wayve Ghost Gym, a neural simulator that creates photorealistic 4D\nworlds and thousands of simulated scenarios for training, testing, and\ndebugging our end-to-end AI driving models thoroughly in an offline, virtual\nenvironment.\n\nRead morerr\n\n15 November 2023 | Research\n\n### Unveiling LINGO-1's Show and Tell Capability with Referential Segmentation\n\nWe've developed a new \"show and tell\" feature in LINGO-1, which uses\nreferential segmentation to visually highlight the focus of LINGO-1's\nattention and enhances the connection between language and vision tasks,\nultimately allowing for more accurate and trustworthy responses.\n\nRead morerr\n\n###### Contact\n\nHeadquarters, UK 230-238 York Way London, N7 9AG United Kingdom\n\n+44 20 4574 5971\n\nhello@wayve.ai\n\n###### Careers\n\n  * Careers\n  * Jobs at Wayve\n  * Our people\n\n###### Company\n\n  * Technology\n  * Product\n  * Leadership\n  * Investors\n  * Company news\n  * Contact\n\ntwitter linkedin youtube\n\n  * Terms of use\n  * Privacy & Cookies\n\n\u00a9 Wayve 2024. All rights reserved Wayve Technologies Ltd. Registered in\nEngland Number: 10924127\n\nWe use necessary cookies to make our site work and if you agree, analytics\ncookies to analyse our website traffic. The optional cookies are from Google.\nFor more detail, please see our Cookies Policy\n\nEssential onlyrr Accept allrr Settings\n\n", "frontpage": false}
