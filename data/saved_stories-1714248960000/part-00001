{"aid": "40180093", "title": "The Convenience of System.io (2023)", "url": "https://devblogs.microsoft.com/dotnet/the-convenience-of-system-io/", "domain": "microsoft.com", "votes": 1, "user": "Rexxar", "posted_at": "2024-04-27 14:12:05", "comments": 0, "source_title": "The convenience of System.IO", "source_text": "The convenience of System.IO - .NET Blog\n\nSkip to main content\n\nMicrosoft\n\n.NET Blog\n\n.NET Blog\n\n  * Light\n  * Dark\n\nLogin\n\nAzure Developers .NET Day 2024!\n\nAzure Developers .NET Day is back on April 30th! Join the .NET community to\nlearn cutting-edge cloud development techniques from experts on cloud services\nfor AI, data, cloud-native, and developer productivity. Elevate your cloud\ndevelopment skills today!\n\nRegister Today!\n\n# The convenience of System.IO\n\nRichard Lander\n\nNovember 6th, 20236 5\n\nReading and writing files is very common, just like other forms of I/O. File\nAPIs are needed for reading application configuration, caching content, and\nloading data (from disk) into memory to do some computation like (today\u2019s\ntopic) word counting. File, FileInfo, FileStream, and related types do a lot\nof the heavy lifting for .NET developers needing to access files. In this\npost, we\u2019re going to look at the convenience and performance of reading text\nfiles with System.IO, with some help from System.Text APIs.\n\nWe recently kicked off a series on the Convenience of .NET that describes our\napproach for providing convenient solutions to common tasks. The convenience\nof System.Text.Json is another post in the series, about reading and writing\nJSON documents. Why .NET? describes the architectural choices that enable the\nsolutions covered in these posts.\n\nThis post analyzes the convenience and performance of file I/O and text APIs\nbeing put to the task of counting lines, words, and bytes in a large novel.\nThe results show that the high-level APIs are straightforward to use and\ndeliver great performance, while the lower-level APIs require a little more\neffort and deliver excellent results. You\u2019ll also see how native AOT shifts\n.NET into a new class of performance for application startup.\n\n## The APIs\n\nThe following File APIs (with their companions) are used in the benchmarks.\n\n  1. File.OpenHandle with RandomAccess.Read\n  2. File.Open with FileStream.Read\n  3. File.OpenText with StreamReader.Read and StreamReader.ReadLine\n  4. File.ReadLines with IEnumerable<string>\n  5. File.ReadAllLines with string[]\n\nThe APIs are listed from highest-control to most convenient. It\u2019s OK if they\nare new to you. It should still be an interesting read.\n\nThe lower-level benchmarks rely on the following System.Text types:\n\n  * Encoding\n  * Rune\n\nI also used the new SearchValues class to see if it provided a significant\nbenefit over passing a Span<char> to Span<char>.IndexOfAny. It pre-computes\nthe search strategy to avoid the upfront costs of IndexOfAny. Spoiler: the\nimpact is dramatic.\n\nNext, we\u2019ll look at an app that has been implemented multiple times \u2014 for each\nof those APIs \u2014 testing approachability and efficiency.\n\n## The App\n\nThe app counts lines, words, and bytes in a text file. It is modeled on the\nbehavior of wc, a popular tool available on Unix-like systems.\n\nWord counting is an algorithm that requires looking at every character in a\nfile. The counting is done by counting spaces and line breaks.\n\n> A word is a non-zero-length sequence of printable characters delimited by\n> white space.\n\nThat\u2019s from wc --help. The app code need to follows that recipe. Seems\nstraightforward.\n\nThe benchmarks count words in Clarissa Harlowe; or the history of a young lady\nby Samuel Richardson. This text was chosen because it is apparently one of the\nlongest books in the English language and is freely available on Project\nGutenberg. There\u2019s even a BBC TV adaption of it from 1991.\n\nI also did some testing with Les Miserables, another long text. Sadly, 24601\ndidn\u2019t come up as a word count.\n\n## Results\n\nEach implementation is measured in terms of:\n\n  * Lines of code\n  * Speed of execution\n  * Memory use\n\nI\u2019m using a build of .NET 8 very close to the final GA build. While writing\nthis post, I saw that there was another .NET 8 build still coming, however,\nthe build I used is probably within the last two or three builds of final for\nthe release.\n\nI used BenchmarkDotNet for performance testing. It\u2019s a great tool if you\u2019ve\nnever used it. Writing a benchmark is similar to writing a unit test.\n\nThe following use of wc lists the cores on my Linux machine. Each core gets\nits own line in the /proc/cpuinfo file with \u201cmodel name\u201d appearing in each of\nthose lines and -l counts lines.\n\n    \n    \n    $ cat /proc/cpuinfo | grep \"model name\" | wc -l 8 $ cat /proc/cpuinfo | grep \"model name\" | head -n 1 model name : Intel(R) Core(TM) i7-7700K CPU @ 4.20GHz $ cat /etc/os-release | head -n 1 NAME=\"Manjaro Linux\"\n\nI used that machine for the performance testing in this post. You can see I\u2019m\nusing Manjaro Linux, which is part of the Arch Linux family. .NET 8 is already\navailable in the Arch User Repository (which is also available to Manjaro\nusers).\n\n### Lines of code\n\nI love solutions that are easy and approachable. Lines of code is our best\nproxy metric for that.\n\nThere are two clusters in this chart, at ~35 and ~75 lines. You\u2019ll see that\nthese benchmarks boil down to two algorithms with some small differences to\naccomodate the different APIs. In contrast, the wc implementation is quite a\nbit longer, nearing 1000 lines. It does more, however.\n\nI used wc to calculate the Benchmark line counts, again with -l.\n\n    \n    \n    $ wc -l *Benchmark.cs 73 FileOpenCharSearchValuesBenchmark.cs 71 FileOpenHandleAsciiCheatBenchmark.cs 74 FileOpenHandleCharSearchValuesBenchmark.cs 60 FileOpenHandleRuneBenchmark.cs 45 FileOpenTextCharBenchmark.cs 65 FileOpenTextCharIndexOfAnyBenchmark.cs 84 FileOpenTextCharLinesBenchmark.cs 65 FileOpenTextCharSearchValuesBenchmark.cs 34 FileOpenTextReadLineBenchmark.cs 36 FileOpenTextReadLineSearchValuesBenchmark.cs 32 FileReadAllLinesBenchmark.cs 32 FileReadLinesBenchmark.cs 671 total\n\nI wrote several benchmarks. I summarized those in the image above, using the\nbest performing benchmark for each file API (and then shortened the name for\nsimplicity). The full set of benchmarks will get covered later.\n\n### Functional parity with wc\n\nLet\u2019s validate that my C# implementation matches wc.\n\nwc:\n\n    \n    \n    $ wc ../Clarissa_Harlowe/* 11716 110023 610515 ../Clarissa_Harlowe/clarissa_volume1.txt 12124 110407 610557 ../Clarissa_Harlowe/clarissa_volume2.txt 11961 109622 606948 ../Clarissa_Harlowe/clarissa_volume3.txt 12168 111908 625888 ../Clarissa_Harlowe/clarissa_volume4.txt 12626 108592 614062 ../Clarissa_Harlowe/clarissa_volume5.txt 12434 107576 607619 ../Clarissa_Harlowe/clarissa_volume6.txt 12818 112713 628322 ../Clarissa_Harlowe/clarissa_volume7.txt 12331 109785 611792 ../Clarissa_Harlowe/clarissa_volume8.txt 11771 104934 598265 ../Clarissa_Harlowe/clarissa_volume9.txt 9 153 1044 ../Clarissa_Harlowe/summary.md 109958 985713 5515012 total\n\nAnd with count, a standalone copy of FileOpenHandleCharSearchValuesBenchmark:\n\n    \n    \n    $ dotnet run ../Clarissa_Harlowe/ 11716 110023 610515 ../Clarissa_Harlowe/clarissa_volume1.txt 12124 110407 610557 ../Clarissa_Harlowe/clarissa_volume2.txt 11961 109622 606948 ../Clarissa_Harlowe/clarissa_volume3.txt 12168 111908 625888 ../Clarissa_Harlowe/clarissa_volume4.txt 12626 108593 614062 ../Clarissa_Harlowe/clarissa_volume5.txt 12434 107576 607619 ../Clarissa_Harlowe/clarissa_volume6.txt 12818 112713 628322 ../Clarissa_Harlowe/clarissa_volume7.txt 12331 109785 611792 ../Clarissa_Harlowe/clarissa_volume8.txt 11771 104934 598265 ../Clarissa_Harlowe/clarissa_volume9.txt 9 153 1044 ../Clarissa_Harlowe/summary.md 109958 985714 5515012 total\n\nThe results are effectively identical, with a one word difference in total\nwordcount. Here, you are seeing a Linux version of wc. The macOS version\nreported 985716 words, three words different than the Linux implementation. I\nnoticed that there were some special characters in two of the files that were\ncausing these differences. I didn\u2019t spend more time investigating them since\nits outside of the scope of the post.\n\n### Scan the summary (in 10 microseconds)\n\nI started by testing a short summary of the novel. It\u2019s just 1 kilobyte (with\n9 lines and 153 words).\n\n    \n    \n    $ dotnet run ../Clarissa_Harlowe/summary.md 9 153 1044 ../Clarissa_Harlowe/summary.md\n\nLet\u2019s count some words.\n\nI\u2019m going to call this result a tie. There are not a lot of apps where a 1\nmicrosecond gap in performance matters. I wouldn\u2019t write tens of additional\nlines of code for (only) that win.\n\n### Team byte wins the memory race with Team string\n\nLet\u2019s look at memory usage for the same small document.\n\nNote: 1_048_576 bytes is 1 megabyte (mebibyte). 10_000 bytes is 1% of that.\nNote: I\u2019m using the integer literal format.\n\nYou are seeing one cluster of APIs that return bytes and another that returns\nheap-allocated strings. In the middle, File.OpenText returns char values.\n\nFile.OpenText relies on the StreamReader and FileStream classes to do the\nrequired processing. The string returning APIs rely on the same types. The\nStreamReader object used by these APIs allocate several buffers, including a\n1k buffer. It also creates a FileStream object, which by default allocates a\n4k buffer. For File.OpenText (when using StreamReader.Read), those buffers are\na fixed cost while File.ReadLines and File.ReadAllLines also allocate strings\n(one per line; a variable cost).\n\n## Speed reading the book (in 1 millisecond)\n\nLet\u2019s see how long it takes to count lines, words, and bytes in\nClarissa_Harlowe volume one.\n\n    \n    \n    $ dotnet run ../Clarissa_Harlowe/clarissa_volume1.txt 11716 110023 610515 ../Clarissa_Harlowe/clarissa_volume1.txt\n\nPerhaps we\u2019ll see a larger separation in performance, grinding through 610_515\nbytes of text.\n\nAnd we do. The byte and char returning APIs cluster together, just above 1ms.\nWe\u2019re also now seeing a difference between File.ReadLine and\nFile.ReadAllLines. However, we should put in perspective that the gap is only\n2ms for 600k of text. The high-level APIs are doing a great job of delivering\ncompetitive performance with much simpler algorithms (in the code I wrote).\n\nThe difference in File.ReadLine and File.ReadAllLines is worth a bit more\nexplanation.\n\n  * All the APIs start with bytes. File.ReadLines reads bytes into char values, looks for the next line break, then converts that block of text to a string, returning one at a time.\n  * File.ReadAllLines does the same and additionally creates all string lines at once and packages them all up into a string[]. That\u2019s a LOT of upfront work that requires a lot of additional memory that frequently offers no additional value.\n\nFile.OpenText returns a StreamReader, which exposes ReadLine and Read APIs.\nThe former returns a string and the latter one or one char values. The\nReadLine option is very similar to using File.ReadLines, which is built on the\nsame API. In the chart, I\u2019ve shown File.OpenText using StreamReader.Read. It\u2019s\na lot more efficient.\n\n## Memory: It\u2019s best to read a page at a time\n\nBased on the speed differences, we\u2019re likely to see big memory differences,\ntoo.\n\nLet\u2019s be char-itable. That\u2019s a dramatic difference. The low-level APIs have a\nfixed cost, while the memory requirements of the string APIs scale with the\nsize of the document.\n\nThe FileOpenHandle and FileOpen benchmarks I wrote use ArrayPool arrays, whose\ncost doesn\u2019t show up in the benchmark.\n\n    \n    \n    Encoding encoding = Encoding.UTF8; Decoder decoder = encoding.GetDecoder(); // BenchmarkValues.Size = 4 * 1024 // charBufferSize = 4097 int charBufferSize = encoding.GetMaxCharCount(BenchmarkValues.Size); char[] charBuffer = ArrayPool<char>.Shared.Rent(charBufferSize); byte[] buffer = ArrayPool<byte>.Shared.Rent(BenchmarkValues.Size);\n\nThis code shows the two ArrayPool arrays that are used (and their sizes).\nBased on observation, there is a significant performance benefit with a 4k\nbuffer and limited (or none) past that. A 4k buffer seems reasonable to\nprocess a 600k file.\n\nI could have used private arrays (or accepted a buffer from the caller). My\nuse of ArrayPool arrays demonstrates the memory use difference in the\nunderlying APIs. As you can see, the cost of File.Open and File.OpenHandle is\neffectively zero (at least, relatively).\n\nAll that said, the memory use of my FileOpen and FileOpenHandle benchmarks\nwould show up as very similar to FileOpenText if I wasn\u2019t using ArrayPool.\nThat should give you the idea that FileOpenText is pretty good (when not using\nStreamReader.ReadLine). Certainly, my implementations could be updated to use\nmuch smaller buffers, but they would run slower.\n\n## Performance parity with wc\n\nI\u2019ve demonstrated that System.IO can be used to produce the same results as\nwc. I should similarly compare performance, using my best-performing\nbenchmark. Here, I\u2019ll use the time command to record the entire invocation\n(process start to termination), processing both a single volume (of the novel)\nand all volumes. You\u2019ll see that the entirety of the novel (all 9 volumes)\ncomprises >5MB of text and just shy of 1M words.\n\nLet\u2019s start with wc.\n\n    \n    \n    $ time wc ../Clarissa_Harlowe/clarissa_volume1.txt 11716 110023 610515 ../Clarissa_Harlowe/clarissa_volume1.txt real 0m0.009s user 0m0.006s sys 0m0.003s $ time wc ../Clarissa_Harlowe/* 11716 110023 610515 ../Clarissa_Harlowe/clarissa_volume1.txt 12124 110407 610557 ../Clarissa_Harlowe/clarissa_volume2.txt 11961 109622 606948 ../Clarissa_Harlowe/clarissa_volume3.txt 12168 111908 625888 ../Clarissa_Harlowe/clarissa_volume4.txt 12626 108592 614062 ../Clarissa_Harlowe/clarissa_volume5.txt 12434 107576 607619 ../Clarissa_Harlowe/clarissa_volume6.txt 12818 112713 628322 ../Clarissa_Harlowe/clarissa_volume7.txt 12331 109785 611792 ../Clarissa_Harlowe/clarissa_volume8.txt 11771 104934 598265 ../Clarissa_Harlowe/clarissa_volume9.txt 9 153 1044 ../Clarissa_Harlowe/summary.md 109958 985713 5515012 total real 0m0.026s user 0m0.026s sys 0m0.000s\n\nThat\u2019s pretty fast. That\u2019s 9 and 26 milliseconds.\n\nLet\u2019s try with .NET, using my FileOpenHandleCharSearchValuesBenchmark\nimplementation.\n\n    \n    \n    $ time ./app/count ../Clarissa_Harlowe/clarissa_volume1.txt 11716 110023 610515 ../Clarissa_Harlowe/clarissa_volume1.txt real 0m0.070s user 0m0.033s sys 0m0.016s $ time ./app/count ../Clarissa_Harlowe/ 11716 110023 610515 ../Clarissa_Harlowe/clarissa_volume1.txt 12124 110407 610557 ../Clarissa_Harlowe/clarissa_volume2.txt 11961 109622 606948 ../Clarissa_Harlowe/clarissa_volume3.txt 12168 111908 625888 ../Clarissa_Harlowe/clarissa_volume4.txt 12626 108593 614062 ../Clarissa_Harlowe/clarissa_volume5.txt 12434 107576 607619 ../Clarissa_Harlowe/clarissa_volume6.txt 12818 112713 628322 ../Clarissa_Harlowe/clarissa_volume7.txt 12331 109785 611792 ../Clarissa_Harlowe/clarissa_volume8.txt 11771 104934 598265 ../Clarissa_Harlowe/clarissa_volume9.txt 9 153 1044 ../Clarissa_Harlowe/summary.md 109958 985714 5515012 total real 0m0.124s user 0m0.095s sys 0m0.010s\n\nThat\u2019s no good! Wasn\u2019t even close.\n\nThat\u2019s 70 and 124 milliseconds with .NET compared to 9 and 26 milliseconds\nwith wc. It\u2019s really interesting that the duration doesn\u2019t scale with the size\nof the content, particularly with the .NET implementation. The runtime startup\ncost is clearly dominant.\n\nEveryone knows that a managed language runtime cannot keep up with native code\non startup. The numbers validate that. If only we had a native managed\nruntime.\n\nOh! We do. We have native AOT. Let\u2019s try it.\n\nSince I enjoy using containers, I used one of our SDK container images (with\nvolume mounting) to do the compilation so that I don\u2019t have install a native\ntoolchain on my machine.\n\n    \n    \n    $ docker run --rm mcr.microsoft.com/dotnet/nightly/sdk:8.0-jammy-aot dotnet --version 8.0.100-rtm.23523.2 $ docker run --rm -v $(pwd):/source -w /source mcr.microsoft.com/dotnet/nightly/sdk:8.0-jammy-aot dotnet publish -o /source/napp $ ls -l napp/ total 4936 -rwxr-xr-x 1 root root 1944896 Oct 30 11:57 count -rwxr-xr-x 1 root root 3107720 Oct 30 11:57 count.dbg\n\nIf you are looking closely, you\u2019ll see the benchmark app compiles down to <\n2MB (1_944_896) with native AOT. That\u2019s the runtime, libraries, and app code.\nEverything. In fact the symbols (count.dbg) file is larger. I can take that\nexecutable to an Ubuntu 22.04 x64 machine, for example, and just run it.\n\nLet\u2019s test native AOT.\n\n    \n    \n    $ time ./napp/count ../Clarissa_Harlowe/clarissa_volume1.txt 11716 110023 610515 ../Clarissa_Harlowe/clarissa_volume1.txt real 0m0.004s user 0m0.005s sys 0m0.000s $ time ./napp/count ../Clarissa_Harlowe/ 11716 110023 610515 ../Clarissa_Harlowe/clarissa_volume1.txt 12124 110407 610557 ../Clarissa_Harlowe/clarissa_volume2.txt 11961 109622 606948 ../Clarissa_Harlowe/clarissa_volume3.txt 12168 111908 625888 ../Clarissa_Harlowe/clarissa_volume4.txt 12626 108593 614062 ../Clarissa_Harlowe/clarissa_volume5.txt 12434 107576 607619 ../Clarissa_Harlowe/clarissa_volume6.txt 12818 112713 628322 ../Clarissa_Harlowe/clarissa_volume7.txt 12331 109785 611792 ../Clarissa_Harlowe/clarissa_volume8.txt 11771 104934 598265 ../Clarissa_Harlowe/clarissa_volume9.txt 9 153 1044 ../Clarissa_Harlowe/summary.md 109958 985714 5515012 total real 0m0.022s user 0m0.025s sys 0m0.007s\n\nThat\u2019s 4 and 22 milliseconds with native AOT compared to 9 and 25 with wc.\nThose are excellent results and quite competitive! The numbers are so good\nthat I\u2019d almost have to double check, but the counts validate the computation.\n\nNote: I configured the app with\n<OptimizationPreference>Speed</OptimizationPreference>. It provided a small\nbenefit.\n\n## Text, Runes, and Unicode\n\nText is everywhere. In fact, you are reading it right now. .NET includes\nmultiple types for processing and storing text, including Char, Encoding,\nRune, and String.\n\nUnicode encodes over a million characters, including emoji. The first 128\ncharacters of ASCII and Unicode match. There are three Unicode encodings:\nUTF8, UTF16, and UTF32, with varying numbers of bytes used to encode each\ncharacter.\n\nHere\u2019s some (semi-relevant) text from The Hobbit.\n\n> \u201cMoon-letters are rune-letters, but you cannot see them,\u201d said Elrond\n\nI cannot help but think that moon-letters are fantastic whitespace characters.\n\nHere are the results of a small utility that prints out information about each\nUnicode character, using that text. The byte-length, and bytes are specific to\nbeing a UTF8 representation.\n\n    \n    \n    $ dotnet run elrond.txt | head -n 16 char, codepoint, byte-length, bytes, notes \u201c, 8220, 3, 11100010_10000000_10011100, M, 77, 1, 01001101, o, 111, 1, 01101111, o, 111, 1, 01101111, n, 110, 1, 01101110, -, 45, 1, 00101101, l, 108, 1, 01101100, e, 101, 1, 01100101, t, 116, 1, 01110100, t, 116, 1, 01110100, e, 101, 1, 01100101, r, 114, 1, 01110010, s, 115, 1, 01110011, , 32, 1, 00100000,whitespace a, 97, 1, 01100001,\n\nThe opening quotation mark character requires three bytes to encode. The\nremaining characters all require one byte since they are within the ASCII\ncharacter range. We also see one whitespace character, the space character.\n\nThe binary representation of the characters that use the one-byte encoding\nexactly match their codepoint integer values. For example, the binary\nrepresentation of codepoint \u201cM\u201d (77) is 0b01001101, the same as integer 77. In\ncontrast, the binary representation of integer 8220 is 0b_100000_00011100, not\nthe three-byte binary value we see above for \u201c. That\u2019s because Unicode\nencodings describe more than just the codepoint value.\n\nHere\u2019s another program that should provide even more insight.\n\n    \n    \n    using System.Text; char englishLetter = 'A'; char fancyQuote = '\u201c'; // char emoji = (char)0x1f600; // won't compile string emoji = \"\\U0001f600\"; Encoding encoding = Encoding.UTF8; PrintChar(englishLetter); PrintChar(fancyQuote); PrintChar(emoji[0]); PrintUnicodeCharacter(emoji); void PrintChar(char c) { int value = (int)c; // Rune rune = new Rune(c); // will throw since emoji[0] is an invalid rune Console.WriteLine($\"{c}; bytes: {encoding.GetByteCount([c])}; integer value: {(int)c}; round-trip: {(char)value}\"); } void PrintUnicodeCharacter(string s) { char[] chars = s.ToCharArray(); int value = char.ConvertToUtf32(s, 0); Rune r1 = (Rune)value; Rune r2 = new Rune(chars[0], chars[1]); Console.WriteLine($\"{s}; chars: {chars.Length}; bytes: {encoding.GetByteCount(chars)}; integer value: {value}; round-trip {char.ConvertFromUtf32(value)};\"); Console.WriteLine($\"{s}; Runes match: {r1 == r2 && r1.Value == value}; {nameof(Rune.Utf8SequenceLength)}: {r1.Utf8SequenceLength}; {nameof(Rune.Utf16SequenceLength)}: {r1.Utf16SequenceLength}\"); }\n\nIt prints out the following:\n\n    \n    \n    A; bytes: 1; integer value: 65; round-trip: A \u201c; bytes: 3; integer value: 8220; round-trip: \u201c \ufffd; bytes: 3; integer value: 55357; round-trip: \ufffd ; chars: 2; bytes: 4; integer value: 128512; round-trip ; ; Runes match: True; Utf8SequenceLength: 4; Utf16SequenceLength: 2\n\nI can run the app again, switching the encoding to UTF16. I switched the value\nof encoding to Encoding.Unicode.\n\n    \n    \n    A; bytes: 2; integer value: 65; round-trip: A \u201c; bytes: 2; integer value: 8220; round-trip: \u201c \ufffd; bytes: 2; integer value: 55357; round-trip: \ufffd ; chars: 2; bytes: 4; integer value: 128512; round-trip ; ; Runes match: True; Utf8SequenceLength: 4; Utf16SequenceLength: 2\n\nThat tells us a few things:\n\n  * The UTF8 encoding has a non-uniform byte encoding.\n  * The UTF16 encoding is more uniform.\n  * Characters that require a single codepoint can interoperate with int, enabling patterns like (char)8220 or (char)0x201C.\n  * Characters that require two codepoints can be stored in a string, an (UTF32) integer value, or as a Rune, enabling patterns like (Rune)128512.\n  * It is easy to write software with bugs if the code directly handles characters or (even worse) bytes. For example, imagine writing a text search algorithm that supports emoji search terms.\n  * Multi-codepoint characters are enough to rune any developer.\n  * My terminal supports emoji (and I\u2019m very happy about that).\n\nWe can connect those Unicode concepts back to .NET types.\n\n  * string and char use the UTF16 encoding.\n  * Encoding classes enable processing text between the encodings and byte values.\n  * string supports Unicode characters that require one or two codepoints.\n  * Rune can represent all Unicode characters (including surrogate pairs), unlike char.\n\nAll of these types are used in the benchmarks. All of the benchmarks (except\none that cheats) properly use these types so that Unicode text is correctly\nprocessed.\n\nLet\u2019s look at the benchmarks.\n\n## File.ReadLines and File.ReadAllLines\n\nThe following benchmarks implement a high-level algorithm based on string\nlines:\n\n  * FileReadLines\n  * FileReadAllLinesBenchmark\n\nThe performance charts in the results section include both of these benchmarks\nso there is no need to show these results again.\n\nThe FileReadLines benchmark sets the baseline for our analysis. It uses\nforeach over an IEnumerable<string>.\n\n    \n    \n    public static Count Count(string path) { long wordCount = 0, lineCount = 0, charCount = 0; foreach (string line in File.ReadLines(path)) { lineCount++; charCount += line.Length; bool wasSpace = true; foreach (char c in line) { bool isSpace = char.IsWhiteSpace(c); if (!isSpace && wasSpace) { wordCount++; } wasSpace = isSpace; } } return new(lineCount, wordCount, charCount, path); }\n\nThe code counts lines and character counts via the outer foreach. The inner\nforeach counts words after spaces, looking at every character in the line. It\nuses char.IsWhiteSpace to determine if a character is whitespace. This\nalgorithm is about as simple as it gets for word counting.\n\n    \n    \n    $ wc ../Clarissa_Harlowe/clarissa_volume1.txt 11716 110023 610515 ../Clarissa_Harlowe/clarissa_volume1.txt $ dotnet run -c Release 1 11 FileReadLinesBenchmark 11716 110023 587080 /Users/rich/git/convenience/wordcount/wordcount/bin/Release/net8.0/Clarissa_Harlowe/clarissa_volume1.txt\n\nNote: The app launches in the benchmarks in several different ways, for my own\ntesting. That\u2019s the reason for the strange commandline arguments.\n\nThe results largely match the wc tool. The byte counts don\u2019t match since this\ncode works on characters not bytes. That means that byte order marks, multi-\nbyte encodings, and line termination characters have been hidden from view. I\ncould have added +1 to the charCount per line, but that didn\u2019t seem useful to\nme, particularly since there are multiple newline schemes. I decided to\naccurately count characters or bytes and not attempt to approximate the\ndifferences between them.\n\n> Bottom line: These APIs are great for small documents or when memory use\n> isn\u2019t a strong constraint. I\u2019d only use File.ReadAllLines if my algorithm\n> relied on knowing the number of lines in a document up front and only for\n> small documents. For larger documents, I\u2019d adopt a better algorithm to count\n> line break characters to avoid using that API.\n\n## File.OpenText\n\nThe following benchmarks implement a variety of approaches, all based on\nStreamReader, which File.OpenText is simply a wrapper around. Some of the\nStreamReader APIs expose string lines and others expose char values. This is\nwhere we\u2019re going to see a larger separation in performance.\n\n  * FileOpenTextReadLineBenchmark\n  * FileOpenTextReadLineSearchValuesBenchmark\n  * FileOpenTextCharBenchmark\n  * FileOpenTextCharLinesBenchmark\n  * FileOpenTextCharIndexOfAnyBenchmark\n  * FileOpenTextCharSearchValuesBenchmark\n\nThe goal of these benchmarks is to determine the benefit of SearchValues and\nof char vs string. I also included the FileReadLinesBenchmark benchmark as a\nbaseline from the previous set of benchmarks.\n\nYou might wonder about memory. The memory use with StreamReader is a function\nof char vs string, which you can see in the initial memory charts earlier in\nthe post. The differences in these algorithms affect speed, but not the\nmemory.\n\nThe FileOpenTextReadLineBenchmark benchmark is effectively identical to\nFileReadLines, only without the IEnumerable<string> abstraction\n\nThe FileOpenTextReadLineSearchValuesBenchmark benchmark starts to get a bit\nmore fancy.\n\n    \n    \n    public static Count Count(string path) { long wordCount = 0, lineCount = 0, charCount = 0; using StreamReader stream = File.OpenText(path); string? line = null; while ((line = stream.ReadLine()) is not null) { lineCount++; charCount += line.Length; ReadOnlySpan<char> text = line.AsSpan().TrimStart(); if (text.Length is 0) { continue; } int index = 0; while ((index = text.IndexOfAny(BenchmarkValues.WhitespaceSearchValuesNoLineBreak)) > 0) { wordCount++; text = text.Slice(index).TrimStart(); } wordCount++; } return new(lineCount, wordCount, charCount, path); }\n\nThis benchmark is simply counting spaces (that it doesn\u2019t trim). It is taking\nadvantage of the new SearchValues type, which can speed up IndexOfAny when\nsearching for more than just a few values. The SearchValues object is\nconstructed with whitespace characters except (most) line break characters. We\ncan assume that line break characters are no longer present, since the code is\nrelying on StreamReader.ReadLine for that.\n\nI could have used this same algorithm for the previous benchmark\nimplementations, however, I wanted to match the most approachable APIs with\nthe most approachable benchmark implementations.\n\nA big part of the reason that IndexOfAny performs so well is vectorization.\n\n    \n    \n    $ dotnet run -c Release 2 Vector64.IsHardwareAccelerated: False Vector128.IsHardwareAccelerated: True Vector256.IsHardwareAccelerated: True Vector512.IsHardwareAccelerated: False\n\n.NET 8 includes vector APIs all the way up to 512 bits. You can use them in\nyour own algorithms or rely on built-in APIs like IndexOfAny to take adantage\nof the improved processing power. The handy IsHardwareAccelerated API tells\nyou how large the vector registers are on a given CPU. This is the result on\nmy Intel machine. I experimented with some newer Intel hardware available in\nAzure, which reported Vector512.IsHardwareAccelerated as True. My MacBook M1\nmachine reports as Vector128.IsHardwareAccelerated as the highest available.\n\nWe can now leave the land of string and switch to char values. There are two\nexpected big benefits. The first is that the underlying API doesn\u2019t need to\nread ahead to find a line break character and there won\u2019t be any more strings\nto heap allocate and garbage collect. We should see a marked improvement in\nspeed and we already know from previous charts that there is a significant\nreduction in memory.\n\nI constructed the following benchmarks to tease apart the value of various\nstrategies.\n\n  * FileOpenTextCharBenchmark \u2014 Same basic algorithm as FileReadLines with the addition of a check for line breaks.\n  * FileOpenTextCharLinesBenchmark \u2014 An attempt to simplify the core algorithm by synthesizing lines of chars.\n  * FileOpenTextCharSearchValuesBenchmark \u2014 Similar use of SearchValues as FileOpenTextReadLineSearchValuesBenchmark to speed up the space searching, but without pre-computed lines.\n  * FileOpenTextCharIndexOfAnyBenchmark \u2014 Exact same algorithm but uses IndexOfAny with a Span<char> instead of the new SearchValues types.\n\nThese benchmarks (as demonstrated in the chart above) tell us that IndexOfAny\nwith SearchValues<char> is very beneficial. It\u2019s interesting to see how poorly\nIndexOfAny does when given so many values (25) to check. It\u2019s a lot slower\nthan simply interating over every character with a char.IsWhiteSpace check.\nThese results should give you pause if you are using a large set of search\nterms with IndexOfAny.\n\nI did some testing on some other machines. I noticed that\nFileOpenTextCharLinesBenchmark performed quite well on an AVX512 machine (with\na lower clock speed). That\u2019s possibly because it is relying more heavily on\nIndexOfAny (with only two search terms) and is otherwise a pretty lean\nalgorithm.\n\nHere\u2019s the FileOpenTextCharSearchValuesBenchmark implementation.\n\n    \n    \n    public static Count Count(string path) { long wordCount = 0, lineCount = 0, charCount = 0; bool wasSpace = true; char[] buffer = ArrayPool<char>.Shared.Rent(BenchmarkValues.Size); using StreamReader reader = File.OpenText(path); int count = 0; while ((count = reader.Read(buffer)) > 0) { charCount += count; Span<char> chars = buffer.AsSpan(0, count); while (chars.Length > 0) { if (char.IsWhiteSpace(chars[0])) { if (chars[0] is '\\n') { lineCount++; } wasSpace = true; chars = chars.Slice(1); continue; } else if (wasSpace) { wordCount++; wasSpace = false; chars = chars.Slice(1); } int index = chars.IndexOfAny(BenchmarkValues.WhitespaceSearchValues); if (index > -1) { if (chars[index] is '\\n') { lineCount++; } wasSpace = true; chars = chars.Slice(index + 1); } else { wasSpace = false; chars = []; } } } ArrayPool<char>.Shared.Return(buffer); return new(lineCount, wordCount, charCount, path); }\n\nIt isn\u2019t that different to the original implementation. The first block needs\nto account for line breaks within the char.IsWhiteSpace check. After that,\nIndexOfAny is used with a SearchValue<char> to find the next whitespace\ncharacter so that the next check can be done. If IndexOfAny returns -1, we\nknow that there are no more whitespace characters so there is no need to read\nany further into the buffer.\n\nSpan<T> is used pervasively in this implementation. Spans provide a cheap way\nof creating window on an underlying array. They are so cheap that its fine for\nthe implementation to continue slicing all the way to when chars.Length > 0 is\nno longer true. I only used that approach with algorithms that required slices\n>1 characters at once. Otherwise, I used a for loop to iterate over a Span,\nwhich was faster.\n\nNote: Visual Studio will suggest that chars.Slice(1) can be simplified to\nchars[1..]. I discovered that the simplication isn\u2019t equivalent and shows up\nas a performance regression in benchmarks. It\u2019s much less likely to be a\nproblem in apps.\n\n    \n    \n    $ wc ../Clarissa_Harlowe/clarissa_volume1.txt 11716 110023 610515 ../Clarissa_Harlowe/clarissa_volume1.txt $ dotnet run -c Release 1 4 FileOpenTextCharBenchmark 11716 110023 610512 /Users/rich/git/convenience/wordcount/wordcount/bin/Release/net8.0/Clarissa_Harlowe/clarissa_volume1.txt\n\nThe FileOpenTextChar* benchmarks are a lot closer to matching wc for the byte\nresults (for ASCII text). The Byte Order Mark (BOM) is consumed before these\nAPIs start returning values. As a result, the byte counts for the the char\nreturning APIs are consistently off by three bytes (the size of the BOM).\nUnlike the string returning APIs, all of the line break characters are\ncounted.\n\n> Bottom line: StreamReader (which is the basis of File.OpenText) offers a\n> flexible set of APIs covering a broad range of approachability and\n> performance. For most use cases (if File.ReadLines isn\u2019t appropriate),\n> StreamReader is a great default choice.\n\n## File.Open and File.OpenHandle\n\nThe following benchmarks implement the lowest-level algorithms, based on\nbytes. File.Open is wrapper on FileStream. File.OpenHandle returns an\noperating system handle, which requires RandomAccess.Read to access.\n\n  * FileOpenCharSearchValuesBenchmark\n  * FileOpenHandleCharSearchValuesBenchmark\n  * FileOpenHandleRuneBenchmark\n  * FileOpenHandleAsciiCheatBenchmark\n\nThese APIs offer a lot more control. Lines and chars are now gone and we\u2019re\nleft with bytes. The goal of these benchmarks is to get the best performance\npossible and to explore the facilities for correctly reading Unicode text\ngiven that the APIs return bytes.\n\nOne last attempt to match the results of wc.\n\n    \n    \n    $ wc ../Clarissa_Harlowe/clarissa_volume1.txt 11716 110023 610515 ../Clarissa_Harlowe/clarissa_volume1.txt $ dotnet run -c Release 1 0 FileOpenHandleCharSearchValuesBenchmark 11716 110023 610515 /Users/rich/git/convenience/wordcount/wordcount/bin/Release/net8.0/Clarissa_Harlowe/clarissa_volume1.txt\n\nThe byte counts now match. We\u2019re now looking at every byte in a given file.\n\nThe FileOpenHandleCharSearchValuesBenchmark adds some new concepts.\nFileOpenCharSearchValuesBenchmark is effectively identical.\n\n    \n    \n    public static Count Count(string path) { long wordCount = 0, lineCount = 0, byteCount = 0; bool wasSpace = true; Encoding encoding = Encoding.UTF8; Decoder decoder = encoding.GetDecoder(); int charBufferSize = encoding.GetMaxCharCount(BenchmarkValues.Size); char[] charBuffer = ArrayPool<char>.Shared.Rent(charBufferSize); byte[] buffer = ArrayPool<byte>.Shared.Rent(BenchmarkValues.Size); using Microsoft.Win32.SafeHandles.SafeFileHandle handle = File.OpenHandle(path, FileMode.Open, FileAccess.Read, FileShare.Read, FileOptions.SequentialScan); // Read content in chunks, in buffer, at count length, starting at byteCount int count = 0; while ((count = RandomAccess.Read(handle, buffer, byteCount)) > 0) { byteCount += count; int charCount = decoder.GetChars(buffer.AsSpan(0, count), charBuffer, false); ReadOnlySpan<char> chars = charBuffer.AsSpan(0, charCount); while (chars.Length > 0) { if (char.IsWhiteSpace(chars[0])) { if (chars[0] is '\\n') { lineCount++; } wasSpace = true; chars = chars.Slice(1); continue; } else if (wasSpace) { wordCount++; wasSpace = false; chars = chars.Slice(1); } int index = chars.IndexOfAny(BenchmarkValues.WhitespaceSearchValues); if (index > -1) { if (chars[index] is '\\n') { lineCount++; } wasSpace = true; chars = chars.Slice(index + 1); } else { wasSpace = false; chars = []; } } } ArrayPool<char>.Shared.Return(charBuffer); ArrayPool<byte>.Shared.Return(buffer); return new(lineCount, wordCount, byteCount, path); }\n\nThe body of this algorithm is effectively identical to\nFileOpenTextCharSearchValuesBenchmark implementation we just saw. What\u2019s\ndifferent is the initial setup.\n\nThe following two blocks of code are new.\n\n    \n    \n    Encoding encoding = Encoding.UTF8; Decoder decoder = encoding.GetDecoder(); int charBufferSize = encoding.GetMaxCharCount(BenchmarkValues.Size);\n\nThis code gets a UTF8 decoder for converting bytes to chars. It also gets the\nmaximum number of characters that the decoder might produce given the size of\nbyte buffer that will be used. This implementation is hard-coded to use UTF8.\nIt could be made dynamic (by reading the byte order mark) to use another\nUnicode encodings.\n\n    \n    \n    int charCount = decoder.GetChars(buffer.AsSpan(0, count), charBuffer, false); ReadOnlySpan<char> chars = charBuffer.AsSpan(0, charCount);\n\nThis block decodes the buffer of bytes into the character buffer. Both buffers\nare correctly sized (with AsSpan) per the reported byte and char count values.\nAfter that, the code adopts a more familiar char-based algorithm. There is no\nobvious way to use SearchValues<byte> that plays nicely with the multi-byte\nUnicode encodings. This approach works fine, so that doesn\u2019t matter much.\n\nThis post is about convenience. I found Decoder.GetChars to be incredibly\nconvenient. it\u2019s a perfect example of a low-level API that does exactly what\nis needed and sort of saves the day down in the trenches. I found this pattern\nby reading how File.ReadLines (indirectly) solves this same problem. All that\ncode is there to be read. It\u2019s open source!\n\nFileOpenHandleRuneBenchmark uses the Rune class instead of Encoding. It turns\nout to be slower, in part because I returned to a more basic algorithm. It\nwasn\u2019t obvious how to use IndexOfAny or SearchValues with Rune, in part\nbecause there is no analog to decoder.GetChars for Rune.\n\n    \n    \n    public static Count Count(string path) { long wordCount = 0, lineCount = 0, byteCount = 0; bool wasSpace = true; byte[] buffer = ArrayPool<byte>.Shared.Rent(BenchmarkValues.Size); using Microsoft.Win32.SafeHandles.SafeFileHandle handle = File.OpenHandle(path, FileMode.Open, FileAccess.Read, FileShare.Read, FileOptions.SequentialScan); int index = 0; // Read content in chunks, in buffer, at count length, starting at byteCount int count = 0; while ((count = RandomAccess.Read(handle, buffer.AsSpan(index), byteCount)) > 0 || index > 0) { byteCount += count; Span<byte> bytes = buffer.AsSpan(0, count + index); index = 0; while (bytes.Length > 0) { OperationStatus status = Rune.DecodeFromUtf8(bytes, out Rune rune, out int bytesConsumed); // bad read due to low buffer length if (status == OperationStatus.NeedMoreData && count > 0) { bytes[..bytesConsumed].CopyTo(buffer); // move the partial Rune to the start of the buffer before next read index = bytesConsumed; break; } if (Rune.IsWhiteSpace(rune)) { if (rune.Value is '\\n') { lineCount++; } wasSpace = true; } else if (wasSpace) { wordCount++; wasSpace = false; } bytes = bytes.Slice(bytesConsumed); } } ArrayPool<byte>.Shared.Return(buffer); return new(lineCount, wordCount, byteCount, path); }\n\nThere isn\u2019t a lot different here and that\u2019s a good thing. Rune is largely a\ndrop-in replacement for char.\n\nThis line is the key difference.\n\n    \n    \n    var status = Rune.DecodeFromUtf8(bytes, out Rune rune, out int bytesConsumed);\n\nI wanted an API that returns a Unicode character from a Span<byte> and reports\nhow many bytes were read. It could be 1 to 4 bytes. Rune.DecodeFromUtf8 does\nprecisely that. For my purposes, I don\u2019t care if I get a Rune or a char back.\nThey are both structs.\n\nI left FileOpenHandleAsciiCheatBenchmark for last. I wanted to see how much\nfaster the code could be made to run if it could apply the maxiumum number of\nassumptions. In short, what would an ASCII-only algorithm look like?\n\n    \n    \n    public static Count Count(string path) { const byte NEWLINE = (byte)'\\n'; const byte SPACE = (byte)' '; ReadOnlySpan<byte> searchValues = [SPACE, NEWLINE]; long wordCount = 0, lineCount = 0, byteCount = 0; bool wasSpace = true; byte[] buffer = ArrayPool<byte>.Shared.Rent(BenchmarkValues.Size); using Microsoft.Win32.SafeHandles.SafeFileHandle handle = File.OpenHandle(path, FileMode.Open, FileAccess.Read, FileShare.Read, FileOptions.SequentialScan); // Read content in chunks, in buffer, at count length, starting at byteCount int count = 0; while ((count = RandomAccess.Read(handle, buffer, byteCount)) > 0) { byteCount += count; Span<byte> bytes = buffer.AsSpan(0, count); while (bytes.Length > 0) { // what's this character? if (bytes[0] <= SPACE) { if (bytes[0] is NEWLINE) { lineCount++; } wasSpace = true; bytes = bytes.Slice(1); continue; } else if (wasSpace) { wordCount++; } // Look ahead for next space or newline // this logic assumes that preceding char was non-whitespace int index = bytes.IndexOfAny(searchValues); if (index > -1) { if (bytes[index] is NEWLINE) { lineCount++; } wasSpace = true; bytes = bytes.Slice(index + 1); } else { wasSpace = false; bytes = []; } } } ArrayPool<byte>.Shared.Return(buffer); return new(lineCount, wordCount, byteCount, path); }\n\nThis code is nearly identical to what you\u2019ve seen before except it searches\nfor far fewer characters, which \u2014 SURPRISE \u2014 speeds up the algorithm. You can\nsee that in the chart earlier in this section. SearchValues isn\u2019t used here\nsince it\u2019s not optimized for only two values.\n\n    \n    \n    $ dotnet run -c Release 1 3 FileOpenHandleAsciiCheatBenchmark 11716 110023 610515 /Users/rich/git/convenience/wordcount/wordcount/bin/Release/net8.0/Clarissa_Harlowe/clarissa_volume1.txt\n\nThis algorithm is still able to produce the expected results. That\u2019s only\nbecause the text file satisfies the assumption of the code.\n\n> Bottom line: File.Open and File.OpenHandle offer the highest control and\n> performance. In the case of text data, it\u2019s not obvious that it is worth the\n> extra effort over File.OpenText (with char) even though they can deliver\n> higher performance. In this case, these APIs were required to match the byte\n> count baseline. For non-text data, these APIs are a more obvious choice.\n\n## Summary\n\nSystem.IO provides effective APIs that cover many use cases. I like how easy\nit is to create straightforward algorithms with File.ReadLines. It works very\nwell for content that is line-based. File.OpenText enables writing faster\nalgorithms without a big step in complexity. Last, File.Open and\nFile.OpenHandle are great for getting access to the binary content of files\nand to enable writing the most high-performance and accurate algorithms.\n\nI didn\u2019t set out to explore .NET globalization APIs or Unicode in quite so\nmuch depth. I\u2019d used the encoding APIs before, but never tried Rune. I was\nimpressed to see how well those APIs suited my project and how well they were\nable to perform. These APIs were a surprise case-in-point example of the\nconvenience premise of the post. Convenience doesn\u2019t mean \u201chigh-level\u201d, but\n\u201cright and approachable tool for the job\u201d.\n\nAnother insight was that for this problem, the high-level APIs were\napproachable and effective, however, only the low-level APIs were up to the\ntask of exactly matching the results of wc. I didn\u2019t understand that dynamic\nwhen I started the project, however, I was happy that the required APIs were\nwell within reach.\n\nThanks to Levi Broderick for reviewing the benchmarks and helping me\nunderstand the finer points of Unicode a little better. Thanks to David\nFowler, Jan Kotas, and Stephen Toub for their help contributing to this\nseries.\n\n### Richard Lander Product Manager, .NET Team\n\nFollow\n\nPosted in .NET C# PerformanceTagged Convenience of .NET\n\n### Read next\n\nJoin us for the Great .NET 8 Hack\n\nJoin us for a virtual hackathon, November 20 to December 4, 2023 to learn how\nto build awesome apps with .NET, AI, or Cloud Native. And a chance to win\nexciting prizes!\n\nAaron Powell November 7, 2023\n\n6 comments\n\nAnnouncing F# 8\n\nRead what is new in F# 8 - the language, compiler tooling and FSharp.Core\nstandard library\n\nRNDr. Tom\u00e1\u0161 Gro\u0161up, Ph.D. November 14, 2023\n\n5 comments\n\n## 6 comments\n\nDiscussion is closed. Login to edit/delete existing comments.\n\n  * Max Mustermueller November 6, 2023 10:21 pm 1\n\nUnlike System.Text.Json, the System.IO is definitely one of my favorites when\nit comes to convenience. It has so many one-liners that saves me so much time\nbut it also goes all the way down if I ever want to. The flexibility is\nincredible but it never felt overloaded for me. I started with .NET because of\nthings like this. Every API in System.IO feels very well thought out.\n\nThe only thing I\u2019d say I\u2019m missing is the lack of async IO copy/move\noperations. There is a tracking issue from 2017\nhttps://github.com/dotnet/runtime/issues/20695\n\n  * Paulo Pinto November 6, 2023 11:54 pm 2\n\nThe whole set of .NET APIs are mostly great, there is enough stuff to write\nendless \u201cThe convenience of ....\u201d articles.\n\nUnfortunely we will never see a \u201cThe convenience of COM\u201d or \u201cThe convenience\nof WinRT\u201d, given how much of the tooling still focus in C++ as main customer,\nand the .NET Core did not carry over all the nice COM interop tooling from\n.NET Framework or .NET Native, now we have to deal with raw IDL tooling, code\ngeneration libraries like CsWinRT, and boilerplate code that it wasn\u2019t\nrequired previously, and this for the scenarios where .NET is allowed to play\nalongside C++ to begin with.\n\n    * Aaron Robinson November 7, 2023 11:49 am 2\n\nHi Paulo, the changing of COM and WinRT are definitely painful in many ways.\nThe goal though is longer term and is to get back to a better place but with\nmore alignment with modern .NET. The first is AOT friendly. The built-in COM\ninterop was not AOT friendly, but with the new COM source generator this means\nCOM interop going forward can be trimmer and AOT friendly. This has huge\nimplications for WinForms and other Windows focused UI Frameworks that run\nwith .NET. The second is evolution. Built-in COM put a heavy burden on the\nruntime Interop team to rarely innovate or change anything due to how the\nsystem was built. As we are evolving these systems to be more flexible we are\nrebuilding the tooling to improve the developer UX.\n\nPlease file issues and suggestions at\nhttps://github.com/dotnet/runtime/issues/new/choose so know how to prioritize\nwork.\n\n      * Paulo Pinto November 8, 2023 4:43 am 0\n\nThanks for jumping in Aaron, in regards to submitting tickets, the experience\nregarding on how WinRT, WinUI and C++/WinRT pain points were respectfully\nignored or declined, until C++/WinRT was brought into maintenance without\nanything comparable to C++/CX development experience, and .NET seems to be on\nthe same track regarding .NET Native\u2019s deprecation, kind of doesn\u2019t make me\nspend the effort on tickets that will anyway be ignored.\n\nAdditionally it is quite clear what is missing, even Visual Basic 6 provided a\nbetter experience, and Microsoft certainly don\u2019t need me submiting tickets to\ndiscover what features are missing from the current development experience.\n\n  * MgSam November 7, 2023 7:41 am 0\n\nI wish a blog could be written about the convenience of System.Text.Csv.\n\n  * Gauthier M. November 8, 2023 2:05 am 2\n\nSystem.IO is a nice API. Manage files is extremly simple in .NET, never had\nproblem with it.\n\n##### Code Block\n\nFeedback\n\nYour Privacy Choices Consumer Health Privacy\n\n", "frontpage": false}
