{"aid": "40091696", "title": "Design patterns for extracting from REST APIs", "url": "https://blog.sequin.io/design-patterns-for-extracting-from-rest-apis/", "domain": "sequin.io", "votes": 2, "user": "todsacerdoti", "posted_at": "2024-04-19 20:43:36", "comments": 0, "source_title": "Design patterns for extracting from REST APIs", "source_text": "Design patterns for extracting from REST APIs\n\nDocs Pricing Integrations Blog Login Start free\n\nDocs Pricing Integrations Blog Sign up Login\n\nBack to posts\n\n# Design patterns for extracting from REST APIs\n\nAnthony Accomazzo\n\n\u2022\n\nApr 19, 2024\n\n\u2022\n\n8 min read\n\nWhen your integration is small or just getting started, you can sometimes get\naway with calling API endpoints just in time.\n\nBut often, that's not sufficient. When it's not, you need to perform an\nextraction process to get data out of the API first. With the data extracted,\nthen you can build your integration.\n\nExtractions are tricky, though. Most extractions I\u2019ve seen in the wild have\nmajor flaws. If your extraction process has a flaw, it will cause holes in\nyour data, and those can be hard to track down.\n\nThe two primary requirements for an extraction are accuracy and efficiency.\nYou need your extraction to be 100% accurate. But you also want it to be\nefficient. Ideally, you don't need your extraction process to paginate an API\nfrom front-to-back every time it runs. (Both expensive and slow.)\n\nThis post covers design patterns to hopefully help you build a reliable\nextraction process on the first shot.\n\n## Why extract?\n\nAn API's limitations are usually what drive a team to extract. These\nlimitations include:\n\n  * rate limit is too low\n  * throughput is too slow\n  * limited query params (can't query the data the way you want)\n  * limited eventing or webhook support\n\nBut extracting is also a good way to create a boundary between an API and your\nsystem. By extracting API data into systems like Kafka and Postgres, you\ndecouple the rest of your system from the quirks and limitations of the API.\nYou can design schemas that fit your use case and optimize for your access\npatterns. And you gain the ability to trigger side effects based on the data\nflowing through the extraction process.\n\n## What to extract?\n\nTypically, an extraction process is trying to extract two things from the\nupstream API:\n\n  * A record: information about a single entity, a row in a database.\n  * An event: a specific thing that happened in the API.\n\nFor example, take Stripe. A record would be a JSON representation of a Stripe\nsubscription. An event would be a JSON object that indicated that a\nsubscription was canceled or crossed some threshold of value.\n\nYou extract records when you want to cache or store API state locally.\nSometimes, you need to cache API objects to avoid rate limit issues or\nincrease throughput. Other times, you are folding API data into your internal\ntables, like updating org.subscription_status in your org table based on the\ncurrent state in Stripe.\n\nYou extract events to trigger side effects. Returning to Stripe, you might\ntrigger a workflow whenever a customer signs up for a new subscription. You\nwant to trigger that workflow only when the subscription is created, not when\nit's updated or deleted. The \"rising edge\" of the subscription being created\nis important.\n\nIn an extraction process, you're extracting records, because the current state\nof records is what most REST APIs return. You can then use changes to records\nto generate events.\n\n## Backfilling\n\nYour extraction process most likely will begin with a backfill. In a backfill,\nyou sweep over every API table, pulling every record into your messaging\nsystem or database. You paginate an API endpoint from the beginning to the\nend.\n\nThe stream of records will be changing as you paginate through it. In order to\nbackfill safely, you need to ensure you don't skip over any records.\n\nYou need to identify two things:\n\n  1. A safe strategy for ordering the stream\n  2. A safe strategy for traversing the stream\n\nThe best-case scenario is that the API has one of the following properties:\n\n  * a \"default order\" which starts at the beginning of time\n  * auto-incrementing IDs and allows for order by ID asc\n  * another unique, increasing field you can order by asc (e.g. an order or invoice number)\n\nAnd supports either filtering (so you can do where ID > ?) or has pagination\ntokens.\n\nOften, APIs don't have one of these properties, so you have to settle for\nanother option. These other options can have some pitfalls.\n\nOne common pitfall is using a created_at timestamp as the cursor for your\nbackfill. It's tempting, and often supported, but has a severe limitation:\nwhat if lots of objects can be created at the same time? For example, what if\nyou can bulk import Contacts into your CRM or an Order's Items are created en\nmasse when the order is created?\n\nIf you naively paginate the stream with where created_at > ?, you risk\nskipping items. If page sizes are 100, and the API returns a page with 100\nitems that all have the same created_at \u2013 what do you do? You can't increment\ncreated_at, because you don't know if there are 101+ items that have the same\ncreated_at as your current cursor. And you can't re-use the same cursor, as\nyou'll just get back the same page of results.\n\nIf the API supports pagination tokens, usually that means you can use\ncreated_at safely. But it's worth checking how it handles the situation of\nlots of items being created with the same created_at timestamp.\n\nAnother way to navigate this situation is if the API supports compound\nqueries. If it does, you can use a compound query to traverse the stream in\nthese situations.\n\n## Detecting changes\n\nAfter the backfill, your extraction process needs to detect changes (or\nperform an \"incremental\" sync).\n\nIdeally, the API supports an events stream you can use to detect changes. But\nthese are rare. Webhooks can be helpful, but have some limitations.\n\nYour best option is almost always to sort and paginate by updated_at asc.\nThen, you'll want to take into account two design considerations:\n\n1\\. Can I paginate safely when lots of records have the same updated_at\ntimestamp?\n\nThe same challenge specified in \"Backfilling\" applies here. What happens if\nhundreds of records can have the same updated_at, but the page size maxes out\nat less than that?\n\n2\\. Does updated_at get updated for the changes I care about?\n\nSometimes, APIs don't update the updated_at field for a record when you'd\nexpect them to. For example, in Salesforce, a record's formula field might\nchange, but that won't update the record's updated_at.\n\n### What if you can't order and paginate by updated_at?\n\nSome APIs make change detection very difficult (looking at you, HubSpot\nAssociations). These APIs provide little in the way of ordering the stream or\npaginating it. Or, sometimes, they return objects that don't even have a\nupdated_at property at all.\n\nThese APIs force you to continuously re-backfill them.\n\nA cache can help alleviate some load on your system. You can store a map of\nIDs to record hashes in memory or in Redis. A record hash can be a simple MD5\nof each record \u2013 just be sure that your method for hashing is consistent. As\nyou pull records, you can filter for changes based on which hashes changed.\n\n### Webhooks\n\nIf the API you're extracting from supports webhooks, those can be a great\noption for detecting incremental updates.\n\nThere are three things you want to be mindful of when receiving webhooks:\n\n  1. Webhooks may have reliability issues.\n  2. You don't want your system to crash and \"lose\" the webhook.\n  3. Webhooks may arrive out-of-order.\n\nFor #1, webhooks are hard to audit. If the provider is struggling to send\nthem, it's harder to detect. If the provider fails to deliver some, it might\nbe impossible for you to find out. Polling, in contrast, can be a lot \"safer.\"\nIf the API is strongly consistent, you rest easy at night knowing you're not\n\"missing\" anything.\n\nFor #2, you need to be mindful of the reliability of your own system. If you\ncrash before processing a webhook, there's no telling if the provider will\nresend it.\n\nThis is why it's usually wise to have a very simple webhook receiver. The only\njob of the receiver is to put the webhook in a messaging system like Kafka for\nfurther processing. That way, it's unlikely that you'll drop a webhook.\n\nFor #3, the provider might send webhooks out-of-order. Even if they don't,\nthere's no telling which of your webhook receivers will \"finish first\" in the\ninstance where you're processing multiple webhooks for the same record at the\nsame time.\n\nTo turn an out-of-order webhook stream into a consistently ordered one, you'll\nneed to use a data store. For example, you can keep track of the last\nupdated_at you saw for a given record ID in Postgres. When you receive a\nwebhook, you can lock that record's row, check if the updated_at of the\nwebhook is newer, and then let the webhook through if it is.\n\nLast, it\u2019s worth mentioning that most APIs only support webhooks for a subset\nof all objects. In many extractions, the team will still set up a foundation\nof polling for changes (to cover all objects needed). Then use webhooks where\nsupported to augment the speed of the sync.\n\n## Generating events\n\nSo far, I've outlined how to extract records. After you've built record\nextraction, you can build event generation on top of that.\n\nTo generate events, you'll typically need to keep track of the current state\nof records somewhere. That's because an event indicates a specific change to a\nrecord. And APIs usually just return the latest version of a record.\n\nFor example, let's say you want to generate an event whenever a Stripe\nsubscription's value passes $10,000. You'll need some persistent local state\nthat indicates what its last value was. When the value exceeds your threshold,\nyou can update your local state and generate an event. This ensures that you\nonly generate an event on the \"rising edge\" \u2013 when the subscription first\npasses the threshold.\n\nIt's a good idea to keep event generation at the edge of your system, near\nwhere you're performing the extraction. That way, downstream consumers don't\nneed to bother with caching records and deducing changes on their own.\n\nPostgres is a great fit for this. If you cache a record or a partial record in\nPostgres, every time you receive the latest version of a record from your\nextraction process you can perform a procedure like this:\n\n  1. Begin a transaction\n  2. Select the record, including the for update clause\n  3. Upsert the record\n\nFor example:\n\n    \n    \n    begin; -- select the stripe subscription record with for update to lock the record select * from stripe_subscriptions where id = $1 for update; -- upsert the stripe subscription record -- assuming the table has columns id, value, and other necessary fields insert into stripe_subscriptions (id, created_at, status, [...columns]) values ($1, $2, $3, ...) on conflict (id) do update set value = excluded.value, created_at = excluded.created_at status = excluded.status, [...];\n\nAt this point, you have the last version of the record in-memory. And Postgres\nhas the latest version of the record, albeit in an uncommitted transaction.\nNow, you can perform your business logic to determine if you should generate\nan event. After generating your event and pushing it to your messaging system,\nyou can commit the transaction.\n\nThis approach has a high degree of safety. Because we \"lock\" the record before\ngenerating the event, we ensure that your system won't produce two of the same\nevents. And doing this in a transaction means that if we fail to produce the\nevent to our messaging system, we'll rollback the change in Postgres. This\nmeans that when the record's update is picked up again, we'll attempt to\ngenerate the event again.\n\n## Conclusion\n\nAs you can see, when building an extraction pipeline, the devil's in the\ndetails. Hopefully these design patterns help you build a pipeline that is\nboth accurate and efficient.\n\nA lot of people confuse the idea of an API integration with an extraction\nprocess. As I hope this post makes clear, extraction is often just the first\nstep. Once your data is flowing from the API, then you can build your\nintegration: triggering off of events and caching data in your database.\n\n### Product\n\nPricing Integrations Status\n\n### Company\n\nCareers Blog Contact\n\n### Developers\n\nDocs Postgres Kafka NATS Webhooks Streams Mutations\n\n### Playbooks\n\nMaterialize Synadia Upstash Neon Supabase More...\n\n\u00a9 Sequin Labs, Inc. 2024\n\nTerms of Service Privacy Policy Security Data Processing\n\nAll systems nominal\n\n", "frontpage": false}
