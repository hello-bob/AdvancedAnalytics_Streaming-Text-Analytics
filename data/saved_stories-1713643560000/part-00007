{"aid": "40096022", "title": "Alchemy is all you need", "url": "https://press.airstreet.com/p/alchemy-is-all-you-need", "domain": "airstreet.com", "votes": 1, "user": "tosh", "posted_at": "2024-04-20 09:29:23", "comments": 0, "source_title": "Alchemy is all you need", "source_text": "Alchemy is all you need\n\nShare this post\n\n#### Alchemy is all you need\n\npress.airstreet.com\n\n#### Discover more from Air Street Press\n\nIdeas worth propagating.\n\nContinue reading\n\nSign in\n\nAnalysis\n\n# Alchemy is all you need\n\n### On the economics of frontier models\n\nAir Street Capital\n\n,\n\nNathan Benaich\n\n, and\n\nAlex Chalmers\n\nApr 18, 2024\n\n10\n\nShare this post\n\n#### Alchemy is all you need\n\npress.airstreet.com\n\nShare\n\n###\n\nIntroduction\n\nNVIDIA\u2019s market capitalization crosses the $2T mark. Sam Altman begins laying\nplans for a $7T global chip empire. The largest sovereign wealth funds compete\nto buy FTX\u2019s Anthropic stake.\n\nIt\u2019s easy to think the AI world is headed in one direction - bigger, badder\nand faster.\n\nBut in the last few months, we\u2019ve heard of multiple generative AI and\ninfrastructure companies stockpiling expensive GPUs, before struggling to find\na use for them. Start-ups we know have been removing powerful frontier models\nfrom their tech stacks, in favor of small, open-source alternatives. And, of\ncourse, there are the fresh reports of frontier model providers that are\nstruggling with revenue growth and margins.\n\nThis isn\u2019t because \u2018deep learning has hit a wall\u2019 or that the technology is\nanything other than incredibly impressive. Instead, we believe that the\nfollowing features can all be true:\n\n  1. the capabilities of large-scale AI systems have improved rapidly,\n\n  2. eye watering capex investments to drive 1) continues apace,\n\n  3. the exact shape of many commercial applications remains unclear,\n\n  4. while the AI sector itself remains in its early innings,\n\n  5. and the economics of large AI models don\u2019t currently work.\n\nThere are also specific dynamics at play in the frontier model space that will\nlikely prevent the market from correcting in the near future. These have the\npotential to impact the entire AI value chain, from the chipmakers through to\ninvestors and founders.\n\n###\n\n2014 SaaS vs 2024 jumbo jets\n\nThe meteoric rise of AI-related VC investment that started in 2014/15 was\nprimarily driven by start-ups applying deep learning to well-established pain\npoints for enterprise. These companies had low capex needs, namely some\nlaptops and a cloud service. While it took 1-2 years to collect labelled data\nto build and train a model and move to a product, it was possible for\nrelatively lean teams to have an outsized impact, while spending what we\u2019d now\nconsider small sums of money. These companies also maintained stable and\npredictable unit economics as they scaled. While they made less general\npurpose models, they made up for it with palatable economics and were often\nable to raise at generous valuations that offered them several paths to\nultimately generating returns for themselves and investors.\n\nWhen we enter the frontier model world, however, we see valuations that\nordinarily would only make sense for a company that had invented a literal,\nrather than a metaphorical, money printer. To take some recent examples:\n\n  * Elon Musk\u2019s xAI is allegedly in talks to raise $3B at an $18B valuation, despite the company not having firmed up a clear business model;\n\n  * Holistic, started by former DeepMinders, is raising a \u20ac200M maiden round;\n\n  * Before being hollowed out by recent events, Inflection had raised $1.5B at a $4B valuation, despite revenue in the low millions.\n\nEven in the calm waters of SaaS, eyebrows would be raised at these kinds of\nmultiples. In fact, frontier model providers are operating in a space that\ncouldn\u2019t be designed to make it harder to earn money predictably.\n\nFirstly, there\u2019s the obviously significant capex required to build a model.\n\nWhile Sam Altman has suggested that GPT-4 cost a mere $100M to train, this is\nonly true in a very narrow sense. It\u2019s unlikely that anyone reading this post,\nif handed $100M, could go away and create an equally capable model, unless\nthey have already invested significantly in previous models, projects, and\ninfrastructure that could be repurposed. The true cost is orders of magnitude\nhigher.\n\nSecondly, the unit economics remain messy. At the moment, providers are all\nlosing money on their models, with growing revenues making little dent in\ncompute costs and R&D expenses. However, SaaS economies of scale don\u2019t apply\nhere - growth in adoption does not necessarily mean the relative cost of\ninfrastructure decreases.\n\nAdditional users, especially if they opt to use the largest, most resource-\nintensive models, can significantly increase the compute requirement. While\nthere are efficiency gains from optimally using GPU resources, these plateau\nafter a certain point. For a company charging users a fixed rate per API call\nor by usage tier, this becomes a tightrope walk.\n\nWhen we start digging into the numbers, it gets ugly quickly. If we take\nAnthropic as an example, after paying for customer support and servers, its\ngross margin works out in the 50-55% range (versus an average of 77% for cloud\nsoftware). Of course, gross margin does not reflect the cost of model training\nor R&D. It\u2019s currently unclear what catalyst would change these margins\nradically.\n\nThe economics of open source pose their own challenges.\n\nWhile the new Databricks model DBRX cost $10M to build and Llama 2 is thought\nto have cost $20M, the same caveats about carrying over investments apply. As\ncosts inevitably grow with model size, very few organizations or individuals\nare likely to be prepared to pump ever larger sums of money into something\nthat is ultimately freely available.\n\nOpen source tools can sometimes create a \u2018lock-in\u2019 effect, but it\u2019s not\napparent that the same will necessarily happen with open frontier models. If\nwe take another Meta open source offering, React, which is used for building\nuser interfaces, it\u2019s easy to see why switching can be challenging. Once\ndevelopers build applications around React, the team\u2019s skills, tooling, and\ncodebase become tied to it. Switching to another UI framework becomes a\nsignificant logistical pain point that could also involve changing the\ncomposition of a team.\n\nMeanwhile, switching LLMs is a more straightforward process. It usually\ninvolves updates to specific features and components that directly interact\nwith the model, which are comparatively isolated, and then running relevant\nbenchmarks to ensure behaviors are consistent. The process becomes easier\nstill for API-only models.\n\nGiven the gradual convergence in performance we\u2019re seeing between different\nfrontier models for economically useful tasks, this poses a challenge to the\nentire sector. We\u2019re slowly entering into a capex intensive arms race to\nproduce progressively bigger models with ever smaller relative performance\nadvantages.\n\nShare\n\n###\n\nThe airliners of the technology world?\n\nThis combination of a relatively undifferentiated offering and high capex\nbefore earning a cent of revenue is highly unusual for software. The IBM\nSystem/360 mainframe, which cost $5B in today\u2019s money, is likely the only\nprecedent. However, it marked the birth of a 50-year long hardware and\nsoftware lock-in for the company. For frontier model providers, these trends\nare only likely to become more unfavorable.\n\nSource: Center for a New American Security.\n\nWarren Buffet warned that \u201cthe worst sort of business is the one that grows\nrapidly, requires significant capital to engender the growth, and then earns\nlittle or no money\u201d, one where \u201cinvestors have poured money into a bottomless\npit, attracted by growth when they should have been repelled by it\u201d. He was,\nof course, talking about airliners.\n\nUnfortunately, airline operators have certain levers at their disposal that\nfrontier model builders lack.\n\nFirstly, airlines drive as much efficiency as possible in their service,\neroding the customer experience as they try to eke out individual planes for\n20 to 30 years.\n\nSecondly, the industry engages in periodic bouts of suicidal price warfare to\nincrease market share at the expense of margin - much to the chagrin of\ninstitutional investors.\n\nHowever, foundation model providers don\u2019t currently have the first option\navailable to them, with the exception of persistent rate-limiting.\n\nThey struggle to eke models out for 20-30 weeks, let alone years. The economic\nvalue of legacy models can plummet within months of their release.\n\nThis is stark when you look at OpenAI\u2019s GPT pricing options:\n\nMeanwhile, there\u2019s a parallel war playing out in API pricing, driving the cost\nof serving some new models down to essentially zero - sometimes within days of\nlaunch:\n\nThis turns the market into a competition to raise as much money as possible\nfrom deep-pocketed big tech companies and investors to, in turn, incinerate in\nthe pursuit of market and mind share.\n\nIn industries like this, where the offering is similar and the economics are\nrough, consolidation is the norm. Historically it occurred across airlines,\nthe automotive industry, telecoms, along with banking and financial services.\nConsidering the compute and data requirements, the narrow talent pool, and the\nneed to price sustainably, the frontier model space should be ripe for this.\n\nHowever, regulators on both sides of the Atlantic have all but guaranteed that\nthis is not going to happen. Something as trivial as Microsoft\u2019s $15M non-\nequity investment in Mistral is considered worthy of a competition\ninvestigation by the European Commission, while the FTC is already probing the\nrelationships between a number of labs and big tech companies. Even innocuous\ndata licensing deals are being placed under the microscope. This is the\nclimate that produced the bizarre pseudo-acquisition of Inflection by\nMicrosoft.\n\n###\n\nBut what about the customers?\n\nSo far, we\u2019ve focused on the challenges faced by the providers of foundation\nmodels, but this is only part of the picture. Adoption of large models relies\non users seeing economic upside that justifies the costs involved, whether\nthat\u2019s through API access or hosting.\n\nHosting a large model can quickly become prohibitively expensive. For example,\na Falcon 180B deployment on AWS, using the cheapest recommended instance, will\nwork out at approximately $33 an hour or $23,000 a month. This stands in\ncontrast to a Falcon 7B deployment, which will set you back $1.20 an hour.\n\nIt\u2019s harder to forecast prices for closed models, as there will be significant\nvariation in token requirements and there can be multiple hidden costs (e.g.\nbackground API calls for different libraries and frameworks). Experimenting\nwith different pricing tools shows how the numbers can begin to add up very\nquickly, even while remaining loss-making for the model builders.\n\n###\n\nSmall is beautiful\n\nCurrently, we believe most economically useful foundation model applications\ndo not require high powered multi-trillion parameter frontier models. The\nreason why many applications have struggled in our view is less about whether\na model scores 82 or 86 on MMLU, but in how they\u2019re applied.\n\nRecently, Shyam Sankar, the CTO of Palantir, wrote a searing critique of the\n\u201ccargo cult\u201d mentality in software. He argued that companies focused far more\non the process of acquiring technology than on deeply understanding or\nintegrating it into their real-world operations. As decision-makers get caught\nup in layers of abstraction, software solutions become disconnected from the\nmeasurable outcomes they\u2019re meant to be driving.\n\nIn the context of foundation models, this usually results in either the\nineffectual layering of LLMs onto decrepit software stacks or the\ninappropriate use of off-the-shelf models with little to no fine-tuning.\n\nAs well as education levels increasing, there will likely be potential\ntechnical fixes. Our friends at Interloom are already redefining process\nautomation, while longer-term, we can see context length playing a role. For\nexample, there are already promising experiments that use massive custom\nprompts (e.g. a company\u2019s code base or documentation, along with better prompt\nengineering) as an alternative to either fine-tuning on knowledge bases or\nRAG.\n\nIt could be that a world of smaller models with long context lengths end up\nbecoming the go-to for the vast majority of common applications. Smaller\nmodels, in the 7B range, are compatible with a broad spectrum of GPUs,\nincluding the older V100 and A100, which are less powerful, but more cost\nefficient than the newer H100. The Apple M-series chip, with its integrated\nCPU/GPU unified memory architecture, means it\u2019s already possible to run\nMistral-7B on some Macs.\n\nWe can already see the future seeds of this being sewn. Google\u2019s lightweight\nCodeGemma family is available openly in 2B and 7B form, while Haiku, the\nsmallest of the new Claude 3 family, offers 1 million input tokens for only\n$0.25. Despite likely being an approximately 7B parameter model, it surpasses\nthe 2023 version of GPT-4 (a likely trillion parameter model) in the LMSYS\nleaderboard.\n\nAlternatively, we\u2019ve written before about the benefits of open source models\nfor innovators, including control, predictability, and removing reliance on a\nsingle external point of failure.\n\nWhile this work is still in its early stages and has its skeptics, a future\nshift to on-device deployment, whether on a high-end desktop computer or\npowerful phone, is already beginning to take place. Microsoft Research\u2019s\nBitNet paper outlined a language model architecture that uses very low-\nprecision 1-bit weights and 8-bit activations, instead of the typical high-\nprecision weights employed by most LLMs.\n\nThis produced significant efficiencies in memory usage and energy consumption.\nThe researchers demonstrated that BitNet could maintain competitive\nperformance on language tasks, while providing substantial reductions in the\ncompute and power requirements. It was also able to mimic the scaling laws of\nhigh-precision models while maintaining the same efficiency benefits. We\u2019re\nalready beginning to see others train models using the same method.\n\nShare\n\n###\n\nClosing thoughts\n\nThe shift to on-device will be a slow one. Generalizing this work to domains\nbeyond language and figuring out some of the complexities around real-world\ndeployment and engineering is complex. We\u2019ve also not entered the aggressive\nsystem optimisation era.\n\nThat said, there is cause to be optimistic - often models used for image\ngeneration or speech recognition and text-to-speech are smaller than text-\ngeneration models. While they require more computational power, this is easier\nto scale on-device than memory capacity or bandwidth.\n\nIf we one day moved to on-device hosting for most commercial applications of\ngenerative models, the implications could be seismic. We could see demand for\nhigh-end GPUs shrink significantly, a market open up for edge chips, while VC\nfirms would contribute less to the bottom-line of cloud providers.\n\nWhile highly powerful frontier models would remain available for the small\nsubset of users who need their advanced capabilities, others could redirect\ninvestment away from ever higher cloud bills to more productive ends. It would\nalso end the unhealthy dynamic of equity being used for capex and the\naggressive dilution of founders and early investors to pay GPU bills.\n\nOf course, the economics of this space remain in flux. The potential of the\nlargest frontier models to develop agentic capabilities and take on more\nautonomous decision-making and problem-solving may lead to a re-evaluation of\ntheir economic value. But we suspect that, for the majority of commercial\napplications, end customers will still index on predictability and reliability\nover unbounded autonomy for the foreseeable future. A sudden advance in\ncapabilities also does not magic away upfront capex or operational costs; if\nanything, it likely worsens them.\n\nWe\u2019re unashamedly optimistic about the potential of AI to transform the\neconomy and believe that industries will be rebuilt around it. None of this is\nan attempt to pour cold water on frontier models or the progress of recent\nyears. Instead, it\u2019s an acknowledgement of a simultaneously obvious but often\nforgotten reality: that companies eventually need to make money.\n\nThis means that we believe that we\u2019ll increasingly see a split in the AI\nworld: between i) predominantly closed frontier models charging economically\nviable prices to a small subset of deep-pocketed users that need their\ncapabilities ii) efficient, cheaper open source models with a specific task\nfocus. This diversity of providers is the ideal outcome for the ecosystem, but\nit\u2019ll only happen if it\u2019s allowed to. Whether it\u2019s on safety or competition,\nbadly designed regulation poses the biggest threat to the future function of\nthe market. If we get this wrong, a future of decaying frontier AI airliners\nawaits us.\n\n### Subscribe to Air Street Press\n\nLaunched 4 years ago\n\nIdeas worth propagating.\n\n11 Likes\n\n\u00b7\n\n4 Restacks\n\n10\n\nShare this post\n\n#### Alchemy is all you need\n\npress.airstreet.com\n\nShare\n\nComments\n\nData acquisition strategies for AI-first start-ups\n\nBack in 2016, I came across a prescient guide on data acquisition strategies\nfor AI startups written by Moritz Mueller-Freitag, then co-founder of...\n\nApr 4 \u2022\n\nAir Street Capital\n\n,\n\nNathan Benaich\n\n,\n\nMoritz Mueller-Freitag\n\n, and\n\nAlex Chalmers\n\n11\n\nShare this post\n\n#### Data acquisition strategies for AI-first start-ups\n\npress.airstreet.com\n\n2\n\nThe State of State of AI Report\n\nIntroduction The State of AI Report is our comprehensive round-up of the most\nimportant developments of the year in AI research, industry, safety, and...\n\nFeb 1 \u2022\n\nAir Street Capital\n\nand\n\nNathan Benaich\n\n10\n\nShare this post\n\n#### The State of State of AI Report\n\npress.airstreet.com\n\nThe Case for Open Source AI\n\nArm the rebels\n\nFeb 8 \u2022\n\nAir Street Capital\n\n,\n\nNathan Benaich\n\n, and\n\nAlex Chalmers\n\n15\n\nShare this post\n\n#### The Case for Open Source AI\n\npress.airstreet.com\n\nReady for more?\n\n\u00a9 2024 Air Street Capital Management Ltd.\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
