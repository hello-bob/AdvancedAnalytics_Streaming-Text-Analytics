{"aid": "40070850", "title": "The Chinese Room Argument", "url": "https://plato.stanford.edu/entries/chinese-room/", "domain": "stanford.edu", "votes": 2, "user": "louislang", "posted_at": "2024-04-17 22:41:46", "comments": 0, "source_title": "The Chinese Room Argument (Stanford Encyclopedia of Philosophy)", "source_text": "The Chinese Room Argument (Stanford Encyclopedia of Philosophy)\n\nStanford Encyclopedia of Philosophy\n\n  * Browse\n\n    * Table of Contents\n    * What's New\n    * Random Entry\n    * Chronological\n    * Archives\n  * About\n\n    * Editorial Information\n    * About the SEP\n    * Editorial Board\n    * How to Cite the SEP\n    * Special Characters\n    * Advanced Tools\n    * Contact\n  * Support SEP\n\n    * Support the SEP\n    * PDFs for SEP Friends\n    * Make a Donation\n    * SEPIA for Libraries\n\n  * Entry Contents\n  * Bibliography\n  * Academic Tools\n  * Friends PDF Preview\n  * Author and Citation Info\n  * Back to Top\n\n# The Chinese Room Argument\n\nFirst published Fri Mar 19, 2004; substantive revision Thu Feb 20, 2020\n\nThe argument and thought-experiment now generally known as the Chinese Room\nArgument was first published in a 1980 article by American philosopher John\nSearle (1932\u2013 ). It has become one of the best-known arguments in recent\nphilosophy. Searle imagines himself alone in a room following a computer\nprogram for responding to Chinese characters slipped under the door. Searle\nunderstands nothing of Chinese, and yet, by following the program for\nmanipulating symbols and numerals just as a computer does, he sends\nappropriate strings of Chinese characters back out under the door, and this\nleads those outside to mistakenly suppose there is a Chinese speaker in the\nroom.\n\nThe narrow conclusion of the argument is that programming a digital computer\nmay make it appear to understand language but could not produce real\nunderstanding. Hence the \u201cTuring Test\u201d is inadequate. Searle argues that the\nthought experiment underscores the fact that computers merely use syntactic\nrules to manipulate symbol strings, but have no understanding of meaning or\nsemantics. The broader conclusion of the argument is that the theory that\nhuman minds are computer-like computational or information processing systems\nis refuted. Instead minds must result from biological processes; computers can\nat best simulate these biological processes. Thus the argument has large\nimplications for semantics, philosophy of language and mind, theories of\nconsciousness, computer science and cognitive science generally. As a result,\nthere have been many critical replies to the argument.\n\n  * 1\\. Overview\n  * 2\\. Historical Background\n\n    * 2.1 Leibniz\u2019 Mill\n    * 2.2 Turing\u2019s Paper Machine\n    * 2.3 The Chinese Nation\n  * 3\\. The Chinese Room Argument\n  * 4\\. Replies to the Chinese Room Argument\n\n    * 4.1 The Systems Reply\n    * 4.2 The Robot Reply\n    * 4.3 The Brain Simulator Reply\n    * 4.4 The Other Minds Reply\n    * 4.5 The Intuition Reply\n  * 5\\. The Larger Philosophical Issues\n\n    * 5.1 Syntax and Semantics\n    * 5.2 Intentionality\n    * 5.3 Mind and Body\n    * 5.4 Simulation, duplication and evolution\n  * Conclusion\n  * Bibliography\n  * Academic Tools\n  * Other Internet Resources\n  * Related Entries\n\n## 1\\. Overview\n\nWork in Artificial Intelligence (AI) has produced computer programs that can\nbeat the world chess champion, control autonomous vehicles, complete our email\nsentences, and defeat the best human players on the television quiz show\nJeopardy. AI has also produced programs with which one can converse in natural\nlanguage, including customer service \u201cvirtual agents\u201d, and Amazon\u2019s Alexa and\nApple\u2019s Siri. Our experience shows that playing chess or Jeopardy, and\ncarrying on a conversation, are activities that require understanding and\nintelligence. Does computer prowess at conversation and challenging games then\nshow that computers can understand language and be intelligent? Will further\ndevelopment result in digital computers that fully match or even exceed human\nintelligence? Alan Turing (1950), one of the pioneer theoreticians of\ncomputing, believed the answer to these questions was \u201cyes\u201d. Turing proposed\nwhat is now known as \u2018The Turing Test\u2019: if a computer can pass for human in\nonline chat, we should grant that it is intelligent. By the late 1970s some AI\nresearchers claimed that computers already understood at least some natural\nlanguage. In 1980 U.C. Berkeley philosopher John Searle introduced a short and\nwidely-discussed argument intended to show conclusively that it is impossible\nfor digital computers to understand language or think.\n\nSearle argues that a good way to test a theory of mind, say a theory that\nholds that understanding can be created by doing such and such, is to imagine\nwhat it would be like to actually do what the theory says will create\nunderstanding. Searle (1999) summarized his Chinese Room Argument (herinafter,\nCRA) concisely:\n\n> Imagine a native English speaker who knows no Chinese locked in a room full\n> of boxes of Chinese symbols (a data base) together with a book of\n> instructions for manipulating the symbols (the program). Imagine that people\n> outside the room send in other Chinese symbols which, unknown to the person\n> in the room, are questions in Chinese (the input). And imagine that by\n> following the instructions in the program the man in the room is able to\n> pass out Chinese symbols which are correct answers to the questions (the\n> output). The program enables the person in the room to pass the Turing Test\n> for understanding Chinese but he does not understand a word of Chinese.\n\nSearle goes on to say, \u201cThe point of the argument is this: if the man in the\nroom does not understand Chinese on the basis of implementing the appropriate\nprogram for understanding Chinese then neither does any other digital computer\nsolely on that basis because no computer, qua computer, has anything the man\ndoes not have.\u201d\n\nThirty years after introducing the CRA Searle 2010 describes the conclusion in\nterms of consciousness and intentionality:\n\n> I demonstrated years ago with the so-called Chinese Room Argument that the\n> implementation of the computer program is not by itself sufficient for\n> consciousness or intentionality (Searle 1980). Computation is defined purely\n> formally or syntactically, whereas minds have actual mental or semantic\n> contents, and we cannot get from syntactical to the semantic just by having\n> the syntactical operations and nothing else. To put this point slightly more\n> technically, the notion \u201csame implemented program\u201d defines an equivalence\n> class that is specified independently of any specific physical realization.\n> But such a specification necessarily leaves out the biologically specific\n> powers of the brain to cause cognitive processes. A system, me, for example,\n> would not acquire an understanding of Chinese just by going through the\n> steps of a computer program that simulated the behavior of a Chinese speaker\n> (p.17).\n\n\u201cIntentionality\u201d is a technical term for a feature of mental and certain other\nthings, namely being about something. Thus a desire for a piece of chocolate\nand thoughts about real Manhattan or fictional Harry Potter all display\nintentionality, as will be discussed in more detail in section 5.2 below.\n\nSearle\u2019s shift from machine understanding to consciousness and intentionality\nis not directly supported by the original 1980 argument. However the re-\ndescription of the conclusion indicates the close connection between\nunderstanding and consciousness in Searle\u2019s later accounts of meaning and\nintentionality. Those who don\u2019t accept Searle\u2019s linking account might hold\nthat running a program can create understanding without necessarily creating\nconsciousness, and conversely a fancy robot might have dog level\nconsciousness, desires, and beliefs, without necessarily understanding natural\nlanguage.\n\nIn moving to discussion of intentionality Searle seeks to develop the broader\nimplications of his argument. It aims to refute the functionalist approach to\nunderstanding minds, that is, the approach that holds that mental states are\ndefined by their causal roles, not by the stuff (neurons, transistors) that\nplays those roles. The argument counts especially against that form of\nfunctionalism known as the Computational Theory of Mind that treats minds as\ninformation processing systems. As a result of its scope, as well as Searle\u2019s\nclear and forceful writing style, the Chinese Room argument has probably been\nthe most widely discussed philosophical argument in cognitive science to\nappear since the Turing Test. By 1991 computer scientist Pat Hayes had defined\nCognitive Science as the ongoing research project of refuting Searle\u2019s\nargument. Cognitive psychologist Steven Pinker (1997) pointed out that by the\nmid-1990s well over 100 articles had been published on Searle\u2019s thought\nexperiment \u2013 and that discussion of it was so pervasive on the Internet that\nPinker found it a compelling reason to remove his name from all Internet\ndiscussion lists.\n\nThis interest has not subsided, and the range of connections with the argument\nhas broadened. A search on Google Scholar for \u201cSearle Chinese Room\u201d limited to\nthe period from 2010 through 2019 produced over 2000 results, including papers\nmaking connections between the argument and topics ranging from embodied\ncognition to theater to talk psychotherapy to postmodern views of truth and\n\u201cour post-human future\u201d \u2013 as well as discussions of group or collective minds\nand discussions of the role of intuitions in philosophy. In 2007 a game\ncompany took the name \u201cThe Chinese Room\u201d in joking honor of \u201c...Searle\u2019s\ncritique of AI \u2013 that you could create a system that gave the impression of\nintelligence without any actual internal smarts.\u201d This wide-range of\ndiscussion and implications is a tribute to the argument\u2019s simple clarity and\ncentrality.\n\n## 2\\. Historical Background\n\n### 2.1 Leibniz\u2019 Mill\n\nSearle\u2019s argument has four important antecedents. The first of these is an\nargument set out by the philosopher and mathematician Gottfried Leibniz\n(1646\u20131716). This argument, often known as \u201cLeibniz\u2019 Mill\u201d, appears as section\n17 of Leibniz\u2019 Monadology. Like Searle\u2019s argument, Leibniz\u2019 argument takes the\nform of a thought experiment. Leibniz asks us to imagine a physical system, a\nmachine, that behaves in such a way that it supposedly thinks and has\nexperiences (\u201cperception\u201d).\n\n> 17\\. Moreover, it must be confessed that perception and that which depends\n> upon it are inexplicable on mechanical grounds, that is to say, by means of\n> figures and motions. And supposing there were a machine, so constructed as\n> to think, feel, and have perception, it might be conceived as increased in\n> size, while keeping the same proportions, so that one might go into it as\n> into a mill. That being so, we should, on examining its interior, find only\n> parts which work one upon another, and never anything by which to explain a\n> perception. Thus it is in a simple substance, and not in a compound or in a\n> machine, that perception must be sought for. [Robert Latta translation]\n\nNotice that Leibniz\u2019s strategy here is to contrast the overt behavior of the\nmachine, which might appear to be the product of conscious thought, with the\nway the machine operates internally. He points out that these internal\nmechanical operations are just parts moving from point to point, hence there\nis nothing that is conscious or that can explain thinking, feeling or\nperceiving. For Leibniz physical states are not sufficient for, nor\nconstitutive of, mental states.\n\n### 2.2 Turing\u2019s Paper Machine\n\nA second antecedent to the Chinese Room argument is the idea of a paper\nmachine, a computer implemented by a human. This idea is found in the work of\nAlan Turing, for example in \u201cIntelligent Machinery\u201d (1948). Turing writes\nthere that he wrote a program for a \u201cpaper machine\u201d to play chess. A paper\nmachine is a kind of program, a series of simple steps like a computer\nprogram, but written in natural language (e.g., English), and implemented by a\nhuman. The human operator of the paper chess-playing machine need not\n(otherwise) know how to play chess. All the operator does is follow the\ninstructions for generating moves on the chess board. In fact, the operator\nneed not even know that he or she is involved in playing chess \u2013 the input and\noutput strings, such as \u201cN\u2013QB7\u201d need mean nothing to the operator of the paper\nmachine.\n\nAs part of the WWII project to decipher German military encryption, Turing had\nwritten English-language programs for human \u201ccomputers\u201d, as these specialized\nworkers were then known, and these human computers did not need to know what\nthe programs that they implemented were doing.\n\nOne reason the idea of a human-plus-paper machine is important is that it\nalready raises questions about agency and understanding similar to those in\nthe CRA. Suppose I am alone in a closed room and follow an instruction book\nfor manipulating strings of symbols. I thereby implement a paper machine that\ngenerates symbol strings such as \u201cN-KB3\u201d that I write on pieces of paper and\nslip under the door to someone ouside the room. Suppose further that prior to\ngoing into the room I don\u2019t know how to play chess, or even that there is such\na game. However, unbeknownst to me, in the room I am running Turing\u2019s chess\nprogram and the symbol strings I generate are chess notation and are taken as\nchess moves by those outside the room. They reply by sliding the symbols for\ntheir own moves back under the door into the room. If all you see is the\nresulting sequence of moves displayed on a chess board outside the room, you\nmight think that someone in the room knows how to play chess very well. Do I\nnow know how to play chess? Or is it the system (consisting of me, the\nmanuals, and the paper on which I manipulate strings of symbols) that is\nplaying chess? If I memorize the program and do the symbol manipulations\ninside my head, do I then know how to play chess, albeit with an odd\nphenomenology? Does someone\u2019s conscious states matter for whether or not they\nknow how to play chess? If a digital computer implements the same program,\ndoes the computer then play chess, or merely simulate this?\n\nBy mid-century Turing was optimistic that the newly developed electronic\ncomputers themselves would soon be able to exhibit apparently intelligent\nbehavior, answering questions posed in English and carrying on conversations.\nTuring (1950) proposed what is now known as the Turing Test: if a computer\ncould pass for human in on-line chat, it should be counted as intelligent.\n\nA third antecedent of Searle\u2019s argument was the work of Searle\u2019s colleague at\nBerkeley, Hubert Dreyfus. Dreyfus was an early critic of the optimistic claims\nmade by AI researchers. In 1965, when Dreyfus was at MIT, he published a circa\nhundred page report titled \u201cAlchemy and Artificial Intelligence\u201d. Dreyfus\nargued that key features of human mental life could not be captured by formal\nrules for manipulating symbols. Dreyfus moved to Berkeley in 1968 and in 1972\npublished his extended critique, \u201cWhat Computers Can\u2019t Do\u201d. Dreyfus\u2019 primary\nresearch interests were in Continental philosophy, with its focus on\nconsciousness, intentionality, and the role of intuition and the inarticulated\nbackground in shaping our understandings. Dreyfus identified several\nproblematic assumptions in AI, including the view that brains are like digital\ncomputers, and, again, the assumption that understanding can be codified as\nexplicit rules.\n\nHowever by the late 1970s, as computers became faster and less expensive, some\nin the burgeoning AI community started to claim that their programs could\nunderstand English sentences, using a database of background information. The\nwork of one of these, Yale researcher Roger Schank (Schank & Abelson 1977)\ncame to Searle\u2019s attention.Schank developed a technique called \u201cconceptual\nrepresentation\u201d that used \u201cscripts\u201d to represent conceptual relations (related\nto Conceptual Role Semantics). Searle\u2019s argument was originally presented as a\nresponse to the claim that AI programs such as Schank\u2019s literally understand\nthe sentences that they respond to.\n\n### 2.3 The Chinese Nation\n\nA fourth antecedent to the Chinese Room argument are thought experiments\ninvolving myriad humans acting as a computer. In 1961 Anatoly Mickevich\n(pseudonym A. Dneprov) published \u201cThe Game\u201d, a story in which a stadium full\nof 1400 math students are arranged to function as a digital computer (see\nDneprov 1961 and the English translation listed at Mickevich 1961, Other\nInternet Resources). For 4 hours each repeatedly does a bit of calculation on\nbinary numbers received from someone near them, then passes the binary result\nonto someone nearby. They learn the next day that they collectively translated\na sentence from Portuguese into their native Russian. Mickevich\u2019s protagonist\nconcludes \u201cWe\u2019ve proven that even the most perfect simulation of machine\nthinking is not the thinking process itself, which is a higher form of motion\nof living matter.\u201d Apparently independently, a similar consideration emerged\nin early discussion of functionalist theories of minds and cognition (see\nfurther discussion in section 5.3 below), Functionalists hold that mental\nstates are defined by the causal role they play in a system (just as a door\nstop is defined by what it does, not by what it is made out of). Critics of\nfunctionalism were quick to turn its proclaimed virtue of multiple\nrealizability against it. While functionalism was consistent with a\nmaterialist or biological understanding of mental states (arguably a virtue),\nit did not identify types of mental states (such as experiencing pain, or\nwondering about OZ) with particular types of neurophysiological states, as\n\u201ctype-type identity theory\u201d did. In contrast with type-type identity theory,\nfunctionalism allowed sentient beings with different physiology to have the\nsame types of mental states as humans \u2013 pains, for example. But it was pointed\nout that if extraterrestrial aliens, with some other complex system in place\nof brains, could realize the functional properties that constituted mental\nstates, then, presumably so could systems even less like human brains. The\ncomputational form of functionalism, which holds that the defining role of\neach mental state is its role in information processing or computation, is\nparticularly vulnerable to this maneuver, since a wide variety of systems with\nsimple components are computationally equivalent (see e.g., Maudlin 1989 for\ndiscussion of a computer built from buckets of water). Critics asked if it was\nreally plausible that these inorganic systems could have mental states or feel\npain.\n\nDaniel Dennett (1978) reports that in 1974 Lawrence Davis gave a colloquium at\nMIT in which he presented one such unorthodox implementation. Dennett\nsummarizes Davis\u2019 thought experiment as follows:\n\n> Let a functionalist theory of pain (whatever its details) be instantiated by\n> a system the subassemblies of which are not such things as C-fibers and\n> reticular systems but telephone lines and offices staffed by people. Perhaps\n> it is a giant robot controlled by an army of human beings that inhabit it.\n> When the theory\u2019s functionally characterized conditions for pain are now met\n> we must say, if the theory is true, that the robot is in pain. That is, real\n> pain, as real as our own, would exist in virtue of the perhaps disinterested\n> and businesslike activities of these bureaucratic teams, executing their\n> proper functions.\n\nIn \u201cTroubles with Functionalism\u201d, also published in 1978, Ned Block envisions\nthe entire population of China implementing the functions of neurons in the\nbrain. This scenario has subsequently been called \u201cThe Chinese Nation\u201d or \u201cThe\nChinese Gym\u201d. We can suppose that every Chinese citizen would be given a call-\nlist of phone numbers, and at a preset time on implementation day, designated\n\u201cinput\u201d citizens would initiate the process by calling those on their call-\nlist. When any citizen\u2019s phone rang, he or she would then phone those on his\nor her list, who would in turn contact yet others. No phone message need be\nexchanged; all that is required is the pattern of calling. The call-lists\nwould be constructed in such a way that the patterns of calls implemented the\nsame patterns of activation that occur between neurons in someone\u2019s brain when\nthat person is in a mental state \u2013 pain, for example. The phone calls play the\nsame functional role as neurons causing one another to fire. Block was\nprimarily interested in qualia, and in particular, whether it is plausible to\nhold that the population of China might collectively be in pain, while no\nindividual member of the population experienced any pain, but the thought\nexperiment applies to any mental states and operations, including\nunderstanding language.\n\nThus Block\u2019s precursor thought experiment, as with those of Davis and Dennett,\nis a system of many humans rather than one. The focus is on consciousness, but\nto the extent that Searle\u2019s argument also involves consciousness, the thought\nexperiment is closely related to Searle\u2019s. Cole (1984) tries to pump\nintuitions in the reverse direction by setting out a thought experiment in\nwhich each of his neurons is itself conscious, and fully aware of its actions\nincluding being doused with neurotransmitters, undergoing action potentials,\nand squirting neurotransmitters at its neighbors. Cole argues that his\nconscious neurons would find it implausible that their collective activity\nproduced a consciousness and other cognitive competences, including\nunderstanding English, that the neurons lack. Cole suggests the intuitions of\nimplementing systems are not to be trusted.\n\n## 3\\. The Chinese Room Argument\n\nIn 1980 John Searle published \u201cMinds, Brains and Programs\u201d in the journal The\nBehavioral and Brain Sciences. In this article, Searle sets out the argument,\nand then replies to the half-dozen main objections that had been raised during\nhis earlier presentations at various university campuses (see next section).\nIn addition, Searle\u2019s article in BBS was published along with comments and\ncriticisms by 27 cognitive science researchers. These 27 comments were\nfollowed by Searle\u2019s replies to his critics.\n\nIn the decades following its publication, the Chinese Room argument was the\nsubject of very many discussions. By 1984, Searle presented the Chinese Room\nargument in a book, Minds, Brains and Science. In January 1990, the popular\nperiodical Scientific American took the debate to a general scientific\naudience. Searle included the Chinese Room Argument in his contribution, \u201cIs\nthe Brain\u2019s Mind a Computer Program?\u201d, and Searle\u2019s piece was followed by a\nresponding article, \u201cCould a Machine Think?\u201d, written by philosophers Paul and\nPatricia Churchland. Soon thereafter Searle had a published exchange about the\nChinese Room with another leading philosopher, Jerry Fodor (in Rosenthal (ed.)\n1991).\n\nThe heart of the argument is Searle imagining himself following a symbol-\nprocessing program written in English (which is what Turing called \u201ca paper\nmachine\u201d). The English speaker (Searle) sitting in the room follows English\ninstructions for manipulating Chinese symbols, whereas a computer \u201cfollows\u201d\n(in some sense) a program written in a computing language. The human produces\nthe appearance of understanding Chinese by following the symbol manipulating\ninstructions, but does not thereby come to understand Chinese. Since a\ncomputer just does what the human does \u2013 manipulate symbols on the basis of\ntheir syntax alone \u2013 no computer, merely by following a program, comes to\ngenuinely understand Chinese.\n\nThis narrow argument, based closely on the Chinese Room scenario, is\nspecifically directed at a position Searle calls \u201cStrong AI\u201d. Strong AI is the\nview that suitably programmed computers (or the programs themselves) can\nunderstand natural language and actually have other mental capabilities\nsimilar to the humans whose behavior they mimic. According to Strong AI, these\ncomputers really play chess intelligently, make clever moves, or understand\nlanguage. By contrast, \u201cweak AI\u201d is the much more modest claim that computers\nare merely useful in psychology, linguistics, and other areas, in part because\nthey can simulate mental abilities. But weak AI makes no claim that computers\nactually understand or are intelligent. The Chinese Room argument is not\ndirected at weak AI, nor does it purport to show that no machine can think \u2013\nSearle says that brains are machines, and brains think. The argument is\ndirected at the view that formal computations on symbols can produce thought.\n\nWe might summarize the narrow argument as a reductio ad absurdum against\nStrong AI as follows. Let L be a natural language, and let us say that a\n\u201cprogram for L\u201d is a program for conversing fluently in L. A computing system\nis any system, human or otherwise, that can run a program.\n\n  1. If Strong AI is true, then there is a program for Chinese such that if any computing system runs that program, that system thereby comes to understand Chinese.\n  2. I could run a program for Chinese without thereby coming to understand Chinese.\n  3. Therefore Strong AI is false.\n\nThe first premise elucidates the claim of Strong AI. The second premise is\nsupported by the Chinese Room thought experiment. The conclusion of this\nnarrow argument is that running a program cannot endow the system with\nlanguage understanding. (There are other ways of understanding the structure\nof the argument. It may be relevant to understand some of the claims as\ncounterfactual: e.g. \u201cthere is a program\u201d in premise 1 as meaning there could\nbe a program, etc. On this construal the argument involves modal logic, the\nlogic of possibility and necessity (see Damper 2006 and Shaffer 2009)).\n\nIt is also worth noting that the first premise above attributes understanding\nto \u201cthe system\u201d. Exactly what Strong-AI supposes will acquire understanding\nwhen the program runs is crucial to the success or failure of the CRA. Schank\n1978 has a title that claims their group\u2019s computer, a physical device,\nunderstands, but in the body of the paper he claims that the program [\u201cSAM\u201d]\nis doing the understanding: SAM, Schank says \u201c...understands stories about\ndomains about which it has knowledge\u201d (p. 133). As we will see in the next\nsection (4), these issues about the identity of the understander (the cpu? the\nprogram? the system? something else?) quickly came to the fore for critics of\nthe CRA. Searle\u2019s wider argument includes the claim that the thought\nexperiment shows more generally that one cannot get semantics (meaning) from\nsyntax (formal symbol manipulation). That and related issues are discussed in\nsection 5: The Larger Philosophical Issues.\n\n## 4\\. Replies to the Chinese Room Argument\n\nCriticisms of the narrow Chinese Room argument against Strong AI have often\nfollowed three main lines, which can be distinguished by how much they\nconcede:\n\n(1) Some critics concede that the man in the room doesn\u2019t understand Chinese,\nbut hold that nevertheless running the program may create comprehension of\nChinese by something other than the room operator. These critics object to the\ninference from the claim that the man in the room does not understand Chinese\nto the conclusion that no understanding has been created. There might be\nunderstanding by a larger, smaller, or different, entity. This is the strategy\nof The Systems Reply and the Virtual Mind Reply. These replies hold that the\noutput of the room might reflect real understanding of Chinese, but the\nunderstanding would not be that of the room operator. Thus Searle\u2019s claim that\nhe doesn\u2019t understand Chinese while running the room is conceded, but his\nclaim that there is no understanding of the questions in Chinese, and that\ncomputationalism is false, is denied.\n\n(2) Other critics concede Searle\u2019s claim that just running a natural language\nprocessing program as described in the CR scenario does not create any\nunderstanding, whether by a human or a computer system. But these critics hold\nthat a variation on the computer system could understand. The variant might be\na computer embedded in a robotic body, having interaction with the physical\nworld via sensors and motors (\u201cThe Robot Reply\u201d), or it might be a system that\nsimulated the detailed operation of an entire human brain, neuron by neuron\n(\u201cthe Brain Simulator Reply\u201d).\n\n(3) Finally, some critics do not concede even the narrow point against AI.\nThese critics hold that the man in the original Chinese Room scenario might\nunderstand Chinese, despite Searle\u2019s denials, or that the scenario is\nimpossible. For example, critics have argued that our intuitions in such cases\nare unreliable. Other critics have held that it all depends on what one means\nby \u201cunderstand\u201d \u2013 points discussed in the section on The Intuition Reply.\nOthers (e.g. Sprevak 2007) object to the assumption that any system (e.g.\nSearle in the room) can run any computer program. And finally some have argued\nthat if it is not reasonable to attribute understanding on the basis of the\nbehavior exhibited by the Chinese Room, then it would not be reasonable to\nattribute understanding to humans on the basis of similar behavioral evidence\n(Searle calls this last the \u201cOther Minds Reply\u201d). The objection is that we\nshould be willing to attribute understanding in the Chinese Room on the basis\nof the overt behavior, just as we do with other humans (and some animals), and\nas we would do with extra-terrestrial Aliens (or burning bushes or angels)\nthat spoke our language. This position is close to Turing\u2019s own, when he\nproposed his behavioral test for machine intelligence.\n\nIn addition to these responses specifically to the Chinese Room scenario and\nthe narrow argument to be discussed here, some critics also independently\nargue against Searle\u2019s larger claim, and hold that one can get semantics (that\nis, meaning) from syntactic symbol manipulation, including the sort that takes\nplace inside a digital computer, a question discussed in the section below on\nSyntax and Semantics.\n\n### 4.1 The Systems Reply\n\nIn the original BBS article, Searle identified and discussed several responses\nto the argument that he had come across in giving the argument in talks at\nvarious places. As a result, these early responses have received the most\nattention in subsequent discussion. What Searle 1980 calls \u201cperhaps the most\ncommon reply\u201d is the Systems Reply.\n\nThe Systems Reply (which Searle says was originally associated with Yale, the\nhome of Schank\u2019s AI work) concedes that the man in the room does not\nunderstand Chinese. But, the reply continues, the man is but a part, a central\nprocessing unit (CPU), in a larger system. The larger system includes the huge\ndatabase, the memory (scratchpads) containing intermediate states, and the\ninstructions \u2013 the complete system that is required for answering the Chinese\nquestions. So the Sytems Reply is that while the man running the program does\nnot understand Chinese, the system as a whole does.\n\nNed Block was one of the first to press the Systems Reply, along with many\nothers including Jack Copeland, Daniel Dennett, Douglas Hofstadter, Jerry\nFodor, John Haugeland, Ray Kurzweil and Georges Rey. Rey (1986) says the\nperson in the room is just the CPU of the system. Kurzweil (2002) says that\nthe human being is just an implementer and of no significance (presumably\nmeaning that the properties of the implementer are not necessarily those of\nthe system). Kurzweil hews to the spirit of the Turing Test and holds that if\nthe system displays the apparent capacity to understand Chinese \u201cit would have\nto, indeed, understand Chinese\u201d \u2013 Searle is contradicting himself in saying in\neffect, \u201cthe machine speaks Chinese but doesn\u2019t understand Chinese\u201d.\n\nMargaret Boden (1988) raises levels considerations. \u201cComputational psychology\ndoes not credit the brain with seeing bean-sprouts or understanding English:\nintentional states such as these are properties of people, not of brains\u201d\n(244). \u201cIn short, Searle\u2019s description of the robot\u2019s pseudo-brain (that is,\nof Searle-in-the-robot) as understanding English involves a category-mistake\ncomparable to treating the brain as the bearer, as opposed to the causal\nbasis, of intelligence\u201d. Boden (1988) points out that the room operator is a\nconscious agent, while the CPU in a computer is not \u2013 the Chinese Room\nscenario asks us to take the perspective of the implementer, and not\nsurprisingly fails to see the larger picture.\n\nSearle\u2019s response to the Systems Reply is simple: in principle, he could\ninternalize the entire system, memorizing all the instructions and the\ndatabase, and doing all the calculations in his head. He could then leave the\nroom and wander outdoors, perhaps even conversing in Chinese. But he still\nwould have no way to attach \u201cany meaning to the formal symbols\u201d. The man would\nnow be the entire system, yet he still would not understand Chinese. For\nexample, he would not know the meaning of the Chinese word for hamburger. He\nstill cannot get semantics from syntax.\n\nIn some ways Searle\u2019s response here anticipates later extended mind views\n(e.g. Clark and Chalmers 1998): if Otto, who suffers loss of memory, can\nregain those recall abilities by externalizing some of the information to his\nnotebooks, then Searle arguably can do the reverse: by internalizing the\ninstructions and notebooks he should acquire any abilities had by the extended\nsystem. And so Searle in effect concludes that since he doesn\u2019t acquire\nunderstanding of Chinese by internalizing the external components of the\nentire system (e.g. he still doesn\u2019t know what the Chinese word for hamburger\nmeans), understanding was never there in the partially externalized system of\nthe original Chinese Room.\n\nIn his 2002 paper \u201cThe Chinese Room from a Logical Point of View\u201d, Jack\nCopeland considers Searle\u2019s response to the Systems Reply and argues that a\nhomunculus inside Searle\u2019s head might understand even though the room operator\nhimself does not, just as modules in minds solve tensor equations that enable\nus to catch cricket balls. Copeland then turns to consider the Chinese Gym,\nand again appears to endorse the Systems Reply: \u201c...the individual players [do\nnot] understand Chinese. But there is no entailment from this to the claim\nthat the simulation as a whole does not come to understand Chinese. The\nfallacy involved in moving from part to whole is even more glaring here than\nin the original version of the Chinese Room Argument\u201d. Copeland denies that\nconnectionism implies that a room of people can simulate the brain.\n\nJohn Haugeland writes (2002) that Searle\u2019s response to the Systems Reply is\nflawed: \u201c...what he now asks is what it would be like if he, in his own mind,\nwere consciously to implement the underlying formal structures and operations\nthat the theory says are sufficient to implement another mind\u201d. According to\nHaugeland, his failure to understand Chinese is irrelevant: he is just the\nimplementer. The larger system implemented would understand \u2013 there is a\nlevel-of-description fallacy.\n\nShaffer 2009 examines modal aspects of the logic of the CRA and argues that\nfamiliar versions of the System Reply are question-begging. But, Shaffer\nclaims, a modalized version of the System Reply succeeds because there are\npossible worlds in which understanding is an emergent property of complex\nsyntax manipulation. Nute 2011 is a reply to Shaffer.\n\nStevan Harnad has defended Searle\u2019s argument against Systems Reply critics in\ntwo papers. In his 1989 paper, Harnad writes \u201cSearle formulates the problem as\nfollows: Is the mind a computer program? Or, more specifically, if a computer\nprogram simulates or imitates activities of ours that seem to require\nunderstanding (such as communicating in language), can the program itself be\nsaid to understand in so doing?\u201d (Note the specific claim: the issue is taken\nto be whether the program itself understands.) Harnad concludes: \u201cOn the face\nof it, [the CR argument] looks valid. It certainly works against the most\ncommon rejoinder, the \u2018Systems Reply\u2019....\u201d Harnad appears to follow Searle in\nlinking understanding and states of consciousness: Harnad 2012 (Other Internet\nResources) argues that Searle shows that the core problem of conscious\n\u201cfeeling\u201d requires sensory connections to the real world. (See sections below\n\u201cThe Robot Reply\u201d and \u201cIntentionality\u201d for discussion.)\n\nFinally some have argued that even if the room operator memorizes the rules\nand does all the operations inside his head, the room operator does not become\nthe system. Cole (1984) and Block (1998) both argue that the result would not\nbe identity of Searle with the system but much more like a case of multiple\npersonality \u2013 distinct persons in a single head. The Chinese responding system\nwould not be Searle, but a sub-part of him. In the CR case, one person\n(Searle) is an English monoglot and the other is a Chinese monoglot. The\nEnglish-speaking person\u2019s total unawareness of the meaning of the Chinese\nresponses does not show that they are not understood. This line, of distinct\npersons, leads to the Virtual Mind Reply.\n\n#### 4.1.1 The Virtual Mind Reply\n\nThe Virtual Mind reply concedes, as does the System Reply, that the operator\nof the Chinese Room does not understand Chinese merely by running the paper\nmachine. However the Virtual Mind reply holds that what is important is\nwhether understanding is created, not whether the Room operator is the agent\nthat understands. Unlike the Systems Reply, the Virtual Mind reply (VMR) holds\nthat a running system may create new, virtual, entities that are distinct from\nboth the system as a whole, as well as from the sub-systems such as the CPU or\noperator. In particular, a running system might create a distinct agent that\nunderstands Chinese. This virtual agent would be distinct from both the room\noperator and the entire system. The psychological traits, including linguistic\nabilities, of any mind created by artificial intelligence will depend entirely\nupon the program and the Chinese database, and will not be identical with the\npsychological traits and abilities of a CPU or the operator of a paper\nmachine, such as Searle in the Chinese Room scenario. According to the VMR the\nmistake in the Chinese Room Argument is to make the claim of strong AI to be\n\u201cthe computer understands Chinese\u201d or \u201cthe System understands Chinese\u201d. The\nclaim at issue for AI should simply be whether \u201cthe running computer creates\nunderstanding of Chinese\u201d.\n\nA familiar model of virtual agents are characters in computer or video games,\nand personal digital assistants, such as Apple\u2019s Siri and Microsoft\u2019s Cortana.\nThese characters have various abilities and personalities, and the characters\nare not identical with the system hardware or program that creates them. A\nsingle running system might control two distinct agents, or physical robots,\nsimultaneously, one of which converses only in Chinese and one of which can\nconverse only in English, and which otherwise manifest very different\npersonalities, memories, and cognitive abilities. Thus the VM reply asks us to\ndistinguish between minds and their realizing systems.\n\nMinsky (1980) and Sloman and Croucher (1980) suggested a Virtual Mind reply\nwhen the Chinese Room argument first appeared. In his widely-read 1989 paper\n\u201cComputation and Consciousness\u201d, Tim Maudlin considers minimal physical\nsystems that might implement a computational system running a program. His\ndiscussion revolves around his imaginary Olympia machine, a system of buckets\nthat transfers water, implementing a Turing machine. Maudlin\u2019s main target is\nthe computationalists\u2019 claim that such a machine could have phenomenal\nconsciousness. However in the course of his discussion, Maudlin considers the\nChinese Room argument. Maudlin (citing Minsky, and Sloman and Croucher) points\nout a Virtual Mind reply that the agent that understands could be distinct\nfrom the physical system (414). Thus \u201cSearle has done nothing to discount the\npossibility of simultaneously existing disjoint mentalities\u201d (414\u20135).\n\nPerlis (1992), Chalmers (1996) and Block (2002) have apparently endorsed\nversions of a Virtual Mind reply as well, as has Richard Hanley in The\nMetaphysics of Star Trek (1997). Penrose (2002) is a critic of this strategy,\nand Stevan Harnad scornfully dismisses such heroic resorts to metaphysics.\nHarnad defended Searle\u2019s position in a \u201cVirtual Symposium on Virtual Minds\u201d\n(1992) against Patrick Hayes and Don Perlis. Perlis pressed a virtual minds\nargument derived, he says, from Maudlin. Chalmers (1996) notes that the room\noperator is just a causal facilitator, a \u201cdemon\u201d, so that his states of\nconsciousness are irrelevant to the properties of the system as a whole. Like\nMaudlin, Chalmers raises issues of personal identity \u2013 we might regard the\nChinese Room as \u201ctwo mental systems realized within the same physical space.\nThe organization that gives rise to the Chinese experiences is quite distinct\nfrom the organization that gives rise to the demon\u2019s [= room operator\u2019s]\nexperiences\u201d(326).\n\nCole (1991, 1994) develops the reply and argues as follows: Searle\u2019s argument\nrequires that the agent of understanding be the computer itself or, in the\nChinese Room parallel, the person in the room. However Searle\u2019s failure to\nunderstand Chinese in the room does not show that there is no understanding\nbeing created. One of the key considerations is that in Searle\u2019s discussion\nthe actual conversation with the Chinese Room is always seriously under\nspecified. Searle was considering Schank\u2019s programs, which can only respond to\na few questions about what happened in a restaurant, all in third person. But\nSearle wishes his conclusions to apply to any AI-produced responses, including\nthose that would pass the toughest unrestricted Turing Test, i.e. they would\nbe just the sort of conversations real people have with each other. If we\nflesh out the conversation in the original CR scenario to include questions in\nChinese such as \u201cHow tall are you?\u201d, \u201cWhere do you live?\u201d, \u201cWhat did you have\nfor breakfast?\u201d, \u201cWhat is your attitude toward Mao?\u201d, and so forth, it\nimmediately becomes clear that the answers in Chinese are not Searle\u2019s\nanswers. Searle is not the author of the answers, and his beliefs and desires,\nmemories and personality traits (apart from his industriousness!) are not\nreflected in the answers and in general Searle\u2019s traits are causally inert in\nproducing the answers to the Chinese questions. This suggests the following\nconditional is true: if there is understanding of Chinese created by running\nthe program, the mind understanding the Chinese would not be the computer,\nwhether the computer is human or electronic. The person understanding the\nChinese would be a distinct person from the room operator, with beliefs and\ndesires bestowed by the program and its database. Hence Searle\u2019s failure to\nunderstand Chinese while operating the room does not show that understanding\nis not being created.\n\nCole (1991) offers an additional argument that the mind doing the\nunderstanding is neither the mind of the room operator nor the system\nconsisting of the operator and the program: running a suitably structured\ncomputer program might produce answers submitted in Chinese and also answers\nto questions submitted in Korean. Yet the Chinese answers might apparently\ndisplay completely different knowledge and memories, beliefs and desires than\nthe answers to the Korean questions \u2013 along with a denial that the Chinese\nanswerer knows any Korean, and vice versa. Thus the behavioral evidence would\nbe that there were two non-identical minds (one understanding Chinese only,\nand one understanding Korean only). Since these might have mutually exclusive\nproperties, they cannot be identical, and ipso facto, cannot be identical with\nthe mind of the implementer in the room. Analogously, a video game might\ninclude a character with one set of cognitive abilities (smart, understands\nChinese) as well as another character with an incompatible set (stupid,\nEnglish monoglot). These inconsistent cognitive traits cannot be traits of the\nXBOX system that realizes them. Cole argues that the implication is that minds\ngenerally are more abstract than the systems that realize them (see Mind and\nBody in the Larger Philosophical Issues section).\n\nIn short, the Virtual Mind argument is that since the evidence that Searle\nprovides that there is no understanding of Chinese was that he wouldn\u2019t\nunderstand Chinese in the room, the Chinese Room Argument cannot refute a\ndifferently formulated equally strong AI claim, asserting the possibility of\ncreating understanding using a programmed digital computer. Maudlin (1989)\nsays that Searle has not adequately responded to this criticism.\n\nOthers however have replied to the VMR, including Stevan Harnad and\nmathematical physicist Roger Penrose. Penrose is generally sympathetic to the\npoints Searle raises with the Chinese Room argument, and has argued against\nthe Virtual Mind reply. Penrose does not believe that computational processes\ncan account for consciousness, both on Chinese Room grounds, as well as\nbecause of limitations on formal systems revealed by Kurt G\u00f6del\u2019s\nincompleteness proof. (Penrose has two books on mind and consciousness;\nChalmers and others have responded to Penrose\u2019s appeals to G\u00f6del.) In his 2002\narticle \u201cConsciousness, Computation, and the Chinese Room\u201d that specifically\naddresses the Chinese Room argument, Penrose argues that the Chinese Gym\nvariation \u2013 with a room expanded to the size of India, with Indians doing the\nprocessing \u2013 shows it is very implausible to hold there is \u201csome kind of\ndisembodied \u2018understanding\u2019 associated with the person\u2019s carrying out of that\nalgorithm, and whose presence does not impinge in any way upon his own\nconsciousness\u201d (230\u20131). Penrose concludes the Chinese Room argument refutes\nStrong AI. Christian Kaernbach (2005) reports that he subjected the virtual\nmind theory to an empirical test, with negative results.\n\n### 4.2 The Robot Reply\n\nThe Robot Reply concedes Searle is right about the Chinese Room scenario: it\nshows that a computer trapped in a computer room cannot understand language,\nor know what words mean. The Robot reply is responsive to the problem of\nknowing the meaning of the Chinese word for hamburger \u2013 Searle\u2019s example of\nsomething the room operator would not know. It seems reasonable to hold that\nmost of us know what a hamburger is because we have seen one, and perhaps even\nmade one, or tasted one, or at least heard people talk about hamburgers and\nunderstood what they are by relating them to things we do know by seeing,\nmaking, and tasting. Given this is how one might come to know what hamburgers\nare, the Robot Reply suggests that we put a digital computer in a robot body,\nwith sensors, such as video cameras and microphones, and add effectors, such\nas wheels to move around with, and arms with which to manipulate things in the\nworld. Such a robot \u2013 a computer with a body \u2013 might do what a child does,\nlearn by seeing and doing. The Robot Reply holds that such a digital computer\nin a robot body, freed from the room, could attach meanings to symbols and\nactually understand natural language. Margaret Boden, Tim Crane, Daniel\nDennett, Jerry Fodor, Stevan Harnad, Hans Moravec and Georges Rey are among\nthose who have endorsed versions of this reply at one time or another. The\nRobot Reply in effect appeals to \u201cwide content\u201d or \u201cexternalist semantics\u201d.\nThis can agree with Searle that syntax and internal connections in isolation\nfrom the world are insufficient for semantics, while holding that suitable\ncausal connections with the world can provide content to the internal symbols.\n\nAbout the time Searle was pressing the CRA, many in philosophy of language and\nmind were recognizing the importance of causal connections to the world as the\nsource of meaning or reference for words and concepts. Hilary Putnam 1981\nargued that a Brain in a Vat, isolated from the world, might speak or think in\na language that sounded like English, but it would not be English \u2013 hence a\nbrain in a vat could not wonder if it was a brain in a vat (because of its\nsensory isolation, its words \u201cbrain\u201d and \u201cvat\u201d do not refer to brains or\nvats). The view that meaning was determined by connections with the world\nbecame widespread. Searle resisted this turn outward and continued to think of\nmeaning as subjective and connected with consciousness.\n\nA related view that minds are best understood as embodied or embedded in the\nworld has gained many supporters since the 1990s, contra Cartesian solipsistic\nintuitions. Organisms rely on environmental features for the success of their\nbehavior. So whether one takes a mind to be a symbol processing system, with\nthe symbols getting their content from sensory connections with the world, or\na non-symbolic system that succeeds by being embedded in a particular\nenvironment, the important of things outside the head have come to the fore.\nHence many are sympathetic to some form of the Robot Reply: a computational\nsystem might understand, provided it is acting in the world. E.g Carter 2007\nin a textbook on philosophy and AI concludes \u201cThe lesson to draw from the\nChinese Room thought experiment is that embodied experience is necessary for\nthe development of semantics.\u201d\n\nHowever Searle does not think that the Robot Reply to the Chinese Room\nargument is any stronger than the Systems Reply. All the sensors can do is\nprovide additional input to the computer \u2013 and it will be just syntactic\ninput. We can see this by making a parallel change to the Chinese Room\nscenario. Suppose the man in the Chinese Room receives, in addition to the\nChinese characters slipped under the door, a stream of binary digits that\nappear, say, on a ticker tape in a corner of the room. The instruction books\nare augmented to use the numerals from the tape as input, along with the\nChinese characters. Unbeknownst to the man in the room, the symbols on the\ntape are the digitized output of a video camera (and possibly other sensors).\nSearle argues that additional syntactic inputs will do nothing to allow the\nman to associate meanings with the Chinese characters. It is just more work\nfor the man in the room.\n\nJerry Fodor, Hilary Putnam, and David Lewis, were principle architects of the\ncomputational theory of mind that Searle\u2019s wider argument attacks. In his\noriginal 1980 reply to Searle, Fodor allows Searle is certainly right that\n\u201cinstantiating the same program as the brain does is not, in and of itself,\nsufficient for having those propositional attitudes characteristic of the\norganism that has the brain.\u201d But Fodor holds that Searle is wrong about the\nrobot reply. A computer might have propositional attitudes if it has the right\ncausal connections to the world \u2013 but those are not ones mediated by a man\nsitting in the head of the robot. We don\u2019t know what the right causal\nconnections are. Searle commits the fallacy of inferring from \u201cthe little man\nis not the right causal connection\u201d to conclude that no causal linkage would\nsucceed. There is considerable empirical evidence that mental processes\ninvolve \u201cmanipulation of symbols\u201d; Searle gives us no alternative explanation\n(this is sometimes called Fodor\u2019s \u201cOnly Game in Town\u201d argument for\ncomputational approaches). In the 1980s and 1990s Fodor wrote extensively on\nwhat the connections must be between a brain state and the world for the state\nto have intentional (representational) properties, while also emphasizing that\ncomputationalism has limits because the computations are intrinsically local\nand so cannot account for abductive reasoning.\n\nIn a later piece, \u201cYin and Yang in the Chinese Room\u201d (in Rosenthal 1991\npp.524\u2013525), Fodor substantially revises his 1980 view. He distances himself\nfrom his earlier version of the robot reply, and holds instead that\n\u201cinstantiation\u201d should be defined in such a way that the symbol must be the\nproximate cause of the effect \u2013 no intervening guys in a room. So Searle in\nthe room is not an instantiation of a Turing Machine, and \u201cSearle\u2019s setup does\nnot instantiate the machine that the brain instantiates.\u201d He concludes:\n\u201c...Searle\u2019s setup is irrelevant to the claim that strong equivalence to a\nChinese speaker\u2019s brain is ipso facto sufficient for speaking Chinese.\u201d Searle\nsays of Fodor\u2019s move, \u201cOf all the zillions of criticisms of the Chinese Room\nargument, Fodor\u2019s is perhaps the most desperate. He claims that precisely\nbecause the man in the Chinese room sets out to implement the steps in the\ncomputer program, he is not implementing the steps in the computer program. He\noffers no argument for this extraordinary claim.\u201d (in Rosenthal 1991, p. 525)\n\nIn a 1986 paper, Georges Rey advocated a combination of the system and robot\nreply, after noting that the original Turing Test is insufficient as a test of\nintelligence and understanding, and that the isolated system Searle describes\nin the room is certainly not functionally equivalent to a real Chinese speaker\nsensing and acting in the world. In a 2002 second look, \u201cSearle\u2019s\nMisunderstandings of Functionalism and Strong AI\u201d, Rey again defends\nfunctionalism against Searle, and in the particular form Rey calls the\n\u201ccomputational-representational theory of thought \u2013 CRTT\u201d. CRTT is not\ncommitted to attributing thought to just any system that passes the Turing\nTest (like the Chinese Room). Nor is it committed to a conversation manual\nmodel of understanding natural language. Rather, CRTT is concerned with\nintentionality, natural and artificial (the representations in the system are\nsemantically evaluable \u2013 they are true or false, hence have aboutness). Searle\nsaddles functionalism with the \u201cblackbox\u201d character of behaviorism, but\nfunctionalism cares how things are done. Rey sketches \u201ca modest mind\u201d \u2013 a CRTT\nsystem that has perception, can make deductive and inductive inferences, makes\ndecisions on basis of goals and representations of how the world is, and can\nprocess natural language by converting to and from its native representations.\nTo explain the behavior of such a system we would need to use the same\nattributions needed to explain the behavior of a normal Chinese speaker.\n\nIf we flesh out the Chinese conversation in the context of the Robot Reply, we\nmay again see evidence that the entity that understands is not the operator\ninside the room. Suppose we ask the robot system Chinese translations of \u201cwhat\ndo you see?\u201d, we might get the answer \u201cMy old friend Shakey\u201d, or \u201cI see you!\u201d.\nWhereas if we phone Searle in the room and ask the same questions in English\nwe might get \u201cThese same four walls\u201d or \u201cthese damn endless instruction books\nand notebooks.\u201d Again this is evidence that we have distinct responders here,\nan English speaker and a Chinese speaker, who see and do quite different\nthings. If the giant robot goes on a rampage and smashes much of Tokyo, and\nall the while oblivious Searle is just following the program in his notebooks\nin the room, Searle is not guilty of homicide and mayhem, because he is not\nthe agent committing the acts.\n\nTim Crane discusses the Chinese Room argument in his 1991 book, The Mechanical\nMind. He cites the Churchlands\u2019 luminous room analogy, but then goes on to\nargue that in the course of operating the room, Searle would learn the meaning\nof the Chinese: \u201c...if Searle had not just memorized the rules and the data,\nbut also started acting in the world of Chinese people, then it is plausible\nthat he would before too long come to realize what these symbols mean.\u201d(127).\n(Rapaport 2006 presses an analogy between Helen Keller and the Chinese Room.)\nCrane appears to end with a version of the Robot Reply: \u201cSearle\u2019s argument\nitself begs the question by (in effect) just denying the central thesis of AI\n\u2013 that thinking is formal symbol manipulation. But Searle\u2019s assumption, none\nthe less, seems to me correct ... the proper response to Searle\u2019s argument is:\nsure, Searle-in-the-room, or the room alone, cannot understand Chinese. But if\nyou let the outside world have some impact on the room, meaning or \u2018semantics\u2019\nmight begin to get a foothold. But of course, this concedes that thinking\ncannot be simply symbol manipulation.\u201d (129) The idea that learning grounds\nunderstanding has led to work in developmental robotics (a.k.a. epigenetic\nrobotics). This AI research area seeks to replicate key human learning\nabilities, such as robots that are shown an object from several angles while\nbeing told in natural language the name of the object.\n\nMargaret Boden 1988 also argues that Searle mistakenly supposes programs are\npure syntax. But programs bring about the activity of certain machines: \u201cThe\ninherent procedural consequences of any computer program give it a toehold in\nsemantics, where the semantics in question is not denotational, but causal.\u201d\n(250) Thus a robot might have causal powers that enable it to refer to a\nhamburger.\n\nStevan Harnad also finds important our sensory and motor capabilities: \u201cWho is\nto say that the Turing Test, whether conducted in Chinese or in any other\nlanguage, could be successfully passed without operations that draw on our\nsensory, motor, and other higher cognitive capacities as well? Where does the\ncapacity to comprehend Chinese begin and the rest of our mental competence\nleave off?\u201d Harnad believes that symbolic functions must be grounded in\n\u201crobotic\u201d functions that connect a system with the world. And he thinks this\ncounts against symbolic accounts of mentality, such as Jerry Fodor\u2019s, and, one\nsuspects, the approach of Roger Schank that was Searle\u2019s original target.\nHarnad 2012 (Other Internet Resources) argues that the CRA shows that even\nwith a robot with symbols grounded in the external world, there is still\nsomething missing: feeling, such as the feeling of understanding.\n\nHowever Ziemke 2016 argues a robotic embodiment with layered systems of bodily\nregulation may ground emotion and meaning, and Seligman 2019 argues that\n\u201cperceptually grounded\u201d approaches to natural language processing (NLP) have\nthe \u201cpotential to display intentionality, and thus after all to foster a truly\nmeaningful semantics that, in the view of Searle and other skeptics, is\nintrinsically beyond computers\u2019 capacity.\u201d\n\n### 4.3 The Brain Simulator Reply\n\nConsider a computer that operates in quite a different manner than the usual\nAI program with scripts and operations on sentence-like strings of symbols.\nThe Brain Simulator reply asks us to suppose instead the program simulates the\nactual sequence of nerve firings that occur in the brain of a native Chinese\nlanguage speaker when that person understands Chinese \u2013 every nerve, every\nfiring. Since the computer then works the very same way as the brain of a\nnative Chinese speaker, processing information in just the same way, it will\nunderstand Chinese. Paul and Patricia Churchland have set out a reply along\nthese lines, discussed below.\n\nIn response to this, Searle argues that it makes no difference. He suggests a\nvariation on the brain simulator scenario: suppose that in the room the man\nhas a huge set of valves and water pipes, in the same arrangement as the\nneurons in a native Chinese speaker\u2019s brain. The program now tells the man\nwhich valves to open in response to input. Searle claims that it is obvious\nthat there would be no understanding of Chinese. (Note however that the basis\nfor this claim is no longer simply that Searle himself wouldn\u2019t understand\nChinese \u2013 it seems clear that now he is just facilitating the causal operation\nof the system and so we rely on our Leibnizian intuition that water-works\ndon\u2019t understand (see also Maudlin 1989).) Searle concludes that a simulation\nof brain activity is not the real thing.\n\nHowever, following Pylyshyn 1980, Cole and Foelber 1984, Chalmers 1996, we\nmight wonder about hybrid systems. Pylyshyn writes:\n\n> If more and more of the cells in your brain were to be replaced by\n> integrated circuit chips, programmed in such a way as to keep the input-\n> output function each unit identical to that of the unit being replaced, you\n> would in all likelihood just keep right on speaking exactly as you are doing\n> now except that you would eventually stop meaning anything by it. What we\n> outside observers might take to be words would become for you just certain\n> noises that circuits caused you to make.\n\nThese cyborgization thought experiments can be linked to the Chinese Room.\nSuppose Otto has a neural disease that causes one of the neurons in my brain\nto fail, but surgeons install a tiny remotely controlled artificial neuron, a\nsynron, along side his disabled neuron. The control of Otto\u2019s neuron is by\nJohn Searle in the Chinese Room, unbeknownst to both Searle and Otto. Tiny\nwires connect the artificial neuron to the synapses on the cell-body of his\ndisabled neuron. When his artificial neuron is stimulated by neurons that\nsynapse on his disabled neuron, a light goes on in the Chinese Room. Searle\nthen manipulates some valves and switches in accord with a program. That, via\nthe radio link, causes Otto\u2019s artificial neuron to release neuro-transmitters\nfrom its tiny artificial vesicles. If Searle\u2019s programmed activity causes\nOtto\u2019s artificial neuron to behave just as his disabled natural neuron once\ndid, the behavior of the rest of his nervous system will be unchanged. Alas,\nOtto\u2019s disease progresses; more neurons are replaced by synrons controlled by\nSearle. Ex hypothesi the rest of the world will not notice the difference;\nwill Otto? If so, when? And why?\n\nUnder the rubric \u201cThe Combination Reply\u201d, Searle also considers a system with\nthe features of all three of the preceding: a robot with a digital brain\nsimulating computer in its cranium, such that the system as a whole behaves\nindistinguishably from a human. Since the normal input to the brain is from\nsense organs, it is natural to suppose that most advocates of the Brain\nSimulator Reply have in mind such a combination of brain simulation, Robot,\nand Systems Reply. Some (e.g. Rey 1986) argue it is reasonable to attribute\nintentionality to such a system as a whole. Searle agrees that it would indeed\nbe reasonable to attribute understanding to such an android system \u2013 but only\nas long as you don\u2019t know how it works. As soon as you know the truth \u2013 it is\na computer, uncomprehendingly manipulating symbols on the basis of syntax, not\nmeaning \u2013 you would cease to attribute intentionality to it.\n\n(One assumes this would be true even if it were one\u2019s spouse, with whom one\nhad built a life-long relationship, that was revealed to hide a silicon\nsecret. Science fiction stories, including episodes of Rod Serling\u2019s\ntelevision series The Twilight Zone, have been based on such possibilities\n(the face of the beloved peels away to reveal the awful android truth);\nhowever, Steven Pinker (1997) mentions one episode in which the android\u2019s\nsecret was known from the start, but the protagonist developed a romantic\nrelationship with the android.)\n\nOn its tenth anniversary the Chinese Room argument was featured in the general\nscience periodical Scientific American. Leading the opposition to Searle\u2019s\nlead article in that issue were philosophers Paul and Patricia Churchland. The\nChurchlands agree with Searle that the Chinese Room does not understand\nChinese, but hold that the argument itself exploits our ignorance of cognitive\nand semantic phenomena. They raise a parallel case of \u201cThe Luminous Room\u201d\nwhere someone waves a magnet and argues that the absence of resulting visible\nlight shows that Maxwell\u2019s electromagnetic theory is false. The Churchlands\nadvocate a view of the brain as a connectionist system, a vector transformer,\nnot a system manipulating symbols according to structure-sensitive rules. The\nsystem in the Chinese Room uses the wrong computational strategies. Thus they\nagree with Searle against traditional AI, but they presumably would endorse\nwhat Searle calls \u201cthe Brain Simulator Reply\u201d, arguing that, as with the\nLuminous Room, our intuitions fail us when considering such a complex system,\nand it is a fallacy to move from part to whole: \u201c... no neuron in my brain\nunderstands English, although my whole brain does.\u201d\n\nIn his 1991 book, Microcognition. Andy Clark holds that Searle is right that a\ncomputer running Schank\u2019s program does not know anything about restaurants,\n\u201cat least if by \u2018know\u2019 we mean anything like \u2018understand\u2019\u201d. But Searle thinks\nthat this would apply to any computational model, while Clark, like the\nChurchlands, holds that Searle is wrong about connectionist models. Clark\u2019s\ninterest is thus in the brain-simulator reply. The brain thinks in virtue of\nits physical properties. What physical properties of the brain are important?\nClark answers that what is important about brains are \u201cvariable and flexible\nsubstructures\u201d which conventional AI systems lack. But that doesn\u2019t mean\ncomputationalism or functionalism is false. It depends on what level you take\nthe functional units to be. Clark defends \u201cmicrofunctionalism\u201d \u2013 one should\nlook to a fine-grained functional description, e.g. neural net level. Clark\ncites William Lycan approvingly contra Block\u2019s absent qualia objection \u2013 yes,\nthere can be absent qualia, if the functional units are made large. But that\ndoes not constitute a refutation of functionalism generally. So Clark\u2019s views\nare not unlike the Churchlands\u2019, conceding that Searle is right about Schank\nand symbolic-level processing systems, but holding that he is mistaken about\nconnectionist systems.\n\nSimilarly Ray Kurzweil (2002) argues that Searle\u2019s argument could be turned\naround to show that human brains cannot understand \u2013 the brain succeeds by\nmanipulating neurotransmitter concentrations and other mechanisms that are in\nthemselves meaningless. In criticism of Searle\u2019s response to the Brain\nSimulator Reply, Kurzweil says: \u201cSo if we scale up Searle\u2019s Chinese Room to be\nthe rather massive \u2018room\u2019 it needs to be, who\u2019s to say that the entire system\nof a hundred trillion people simulating a Chinese Brain that knows Chinese\nisn\u2019t conscious? Certainly, it would be correct to say that such a system\nknows Chinese. And we can\u2019t say that it is not conscious anymore than we can\nsay that about any other process. We can\u2019t know the subjective experience of\nanother entity....\u201d\n\n### 4.4 The Other Minds Reply\n\nRelated to the preceding is The Other Minds Reply: \u201cHow do you know that other\npeople understand Chinese or anything else? Only by their behavior. Now the\ncomputer can pass the behavioral tests as well as they can (in principle), so\nif you are going to attribute cognition to other people you must in principle\nalso attribute it to computers.\u201d\n\nSearle\u2019s (1980) reply to this is very short:\n\n> The problem in this discussion is not about how I know that other people\n> have cognitive states, but rather what it is that I am attributing to them\n> when I attribute cognitive states to them. The thrust of the argument is\n> that it couldn\u2019t be just computational processes and their output because\n> the computational processes and their output can exist without the cognitive\n> state. It is no answer to this argument to feign anesthesia. In \u2018cognitive\n> sciences\u2019 one presupposes the reality and knowability of the mental in the\n> same way that in physical sciences one has to presuppose the reality and\n> knowability of physical objects.\n\nCritics hold that if the evidence we have that humans understand is the same\nas the evidence we might have that a visiting extra-terrestrial alien\nunderstands, which is the same as the evidence that a robot understands, the\npresuppositions we may make in the case of our own species are not relevant,\nfor presuppositions are sometimes false. For similar reasons, Turing, in\nproposing the Turing Test, is specifically worried about our presuppositions\nand chauvinism. If the reasons for the presuppositions regarding humans are\npragmatic, in that they enable us to predict the behavior of humans and to\ninteract effectively with them, perhaps the presupposition could apply equally\nto computers (similar considerations are pressed by Dennett, in his\ndiscussions of what he calls the Intentional Stance).\n\nSearle raises the question of just what we are attributing in attributing\nunderstanding to other minds, saying that it is more than complex behavioral\ndispositions. For Searle the additional seems to be certain states of\nconsciousness, as is seen in his 2010 summary of the CRA conclusions. Terry\nHorgan (2013) endorses this claim: \u201cthe real moral of Searle\u2019s Chinese room\nthought experiment is that genuine original intentionality requires the\npresence of internal states with intrinsic phenomenal character that is\ninherently intentional...\u201d But this tying of understanding to phenomenal\nconsciousness raises a host of issues.\n\nWe attribute limited understanding of language to toddlers, dogs, and other\nanimals, but it is not clear that we are ipso facto attributing unseen states\nof subjective consciousness \u2013 what do we know of the hidden states of exotic\ncreatures? Ludwig Wittgenstein (the Private Language Argument) and his\nfollowers pressed similar points. Altered qualia possibilities, analogous to\nthe inverted spectrum, arise: suppose I ask \u201cwhat\u2019s the sum of 5 and 7\u201d and\nyou respond \u201cthe sum of 5 and 7 is 12\u201d, but as you heard my question you had\nthe conscious experience of hearing and understanding \u201cwhat is the sum of 10\nand 14\u201d, though you were in the computational states appropriate for producing\nthe correct sum and so said \u201c12\u201d. Are there certain conscious states that are\n\u201ccorrect\u201d for certain functional states? Wittgenstein\u2019s considerations appear\nto be that the subjective state is irrelevant, at best epiphenomenal, if a\nlanguage user displays appropriate linguistic behavior. Afterall, we are\ntaught language on the basis of our overt responses, not our qualia. The\nmathematical savant Daniel Tammet reports that when he generates the decimal\nexpansion of pi to thousands of digits he experiences colors that reveal the\nnext digit, but even here it may be that Tennant\u2019s performance is likely not\nproduced by the colors he experiences, but rather by unconscious neural\ncomputation. The possible importance of subjective states is further\nconsidered in the section on Intentionality, below.\n\nIn the 30 years since the CRA there has been philosophical interest in zombies\n\u2013 creatures that look like and behave just as normal humans, including\nlinguistic behavior, yet have no subjective consciousness. A difficulty for\nclaiming that subjective states of consciousness are crucial for understanding\nmeaning will arise in these cases of absent qualia: we can\u2019t tell the\ndifference between zombies and non-zombies, and so on Searle\u2019s account we\ncan\u2019t tell the difference between those that really understand English and\nthose that don\u2019t. And if you and I can\u2019t tell the difference between those who\nunderstand language and Zombies who behave like they do but don\u2019t really, than\nneither can any selection factor in the history of human evolution \u2013 to\npredators, prey, and mates, zombies and true understanders, with the \u201cright\u201d\nconscious experience, have been indistinguishable. But then there appears to\nbe a distinction without a difference. In any case, Searle\u2019s short reply to\nthe Other Minds Reply may be too short.\n\nDescartes famously argued that speech was sufficient for attributing minds and\nconsciousness to others, and infamously argued that it was necessary. Turing\nwas in effect endorsing Descartes\u2019 sufficiency condition, at least for\nintelligence, while substituting written for oral linguistic behavior. Since\nmost of us use dialog as a sufficient condition for attributing understanding,\nSearle\u2019s argument, which holds that speech is a sufficient condition for\nattributing understanding to humans but not for anything that doesn\u2019t share\nour biology, an account would appear to be required of what additionally is\nbeing attributed, and what can justify the additional attribution. Further, if\nbeing con-specific is key on Searle\u2019s account, a natural question arises as to\nwhat circumstances would justify us in attributing understanding (or\nconsciousness) to extra-terrestrial aliens who do not share our biology?\nOffending ET\u2019s by withholding attributions of understanding until after doing\na post-mortem may be risky.\n\nHans Moravec, director of the Robotics laboratory at Carnegie Mellon\nUniversity, and author of Robot: Mere Machine to Transcendent Mind, argues\nthat Searle\u2019s position merely reflects intuitions from traditional philosophy\nof mind that are out of step with the new cognitive science. Moravec endorses\na version of the Other Minds reply. It makes sense to attribute intentionality\nto machines for the same reasons it makes sense to attribute them to humans;\nhis \u201cinterpretative position\u201d is similar to the views of Daniel Dennett.\nMoravec goes on to note that one of the things we attribute to others is the\nability to make attributions of intentionality, and then we make such\nattributions to ourselves. It is such self-representation that is at the heart\nof consciousness. These capacities appear to be implementation independent,\nand hence possible for aliens and suitably programmed computers.\n\nAs we have seen, the reason that Searle thinks we can disregard the evidence\nin the case of robots and computers is that we know that their processing is\nsyntactic, and this fact trumps all other considerations. Indeed, Searle\nbelieves this is the larger point that the Chinese Room merely illustrates.\nThis larger point is addressed in the Syntax and Semantics section below.\n\n### 4.5 The Intuition Reply\n\nMany responses to the Chinese Room argument have noted that, as with Leibniz\u2019\nMill, the argument appears to be based on intuition: the intuition that a\ncomputer (or the man in the room) cannot think or have understanding. For\nexample, Ned Block (1980) in his original BBS commentary says \u201cSearle\u2019s\nargument depends for its force on intuitions that certain entities do not\nthink.\u201d But, Block argues, (1) intuitions sometimes can and should be trumped\nand (2) perhaps we need to bring our concept of understanding in line with a\nreality in which certain computer robots belong to the same natural kind as\nhumans. Similarly Margaret Boden (1988) points out that we can\u2019t trust our\nuntutored intuitions about how mind depends on matter; developments in science\nmay change our intuitions. Indeed, elimination of bias in our intuitions was\nprecisely what motivated Turing (1950) to propose the Turing Test, a test that\nwas blind to the physical character of the system replying to questions. Some\nof Searle\u2019s critics in effect argue that he has merely pushed the reliance on\nintuition back, into the room.\n\nFor example, one can hold that despite Searle\u2019s intuition that he would not\nunderstand Chinese while in the room, perhaps he is mistaken and does, albeit\nunconsciously. Hauser (2002) accuses Searle of Cartesian bias in his inference\nfrom \u201cit seems to me quite obvious that I understand nothing\u201d to the\nconclusion that I really understand nothing. Normally, if one understands\nEnglish or Chinese, one knows that one does \u2013 but not necessarily. Searle\nlacks the normal introspective awareness of understanding \u2013 but this, while\nabnormal, is not conclusive.\n\nCritics of the CRA note that our intuitions about intelligence, understanding\nand meaning may all be unreliable. With regard to meaning, Wakefield 2003,\nfollowing Block 1998, defends what Wakefield calls \u201cthe essentialist\nobjection\u201d to the CRA, namely that a computational account of meaning is not\nanalysis of ordinary concepts and their related intuitions. Rather we are\nbuilding a scientific theory of meaning that may require revising our\nintuitions. As a theory, it gets its evidence from its explanatory power, not\nits accord with pre-theoretic intuitions (however Wakefield himself argues\nthat computational accounts of meaning are afflicted by a pernicious\nindeterminacy (pp. 308ff)).\n\nOther critics focusing on the role of intuitions in the CRA argue that our\nintuitions regarding both intelligence and understanding may also be\nunreliable, and perhaps incompatible even with current science. With regard to\nunderstanding, Steven Pinker, in How the Mind Works (1997), holds that \u201c...\nSearle is merely exploring facts about the English word understand.... People\nare reluctant to use the word unless certain stereotypical conditions\napply...\u201d But, Pinker claims, nothing scientifically speaking is at stake.\nPinker objects to Searle\u2019s appeal to the \u201ccausal powers of the brain\u201d by\nnoting that the apparent locus of the causal powers is the \u201cpatterns of\ninterconnectivity that carry out the right information processing\u201d. Pinker\nends his discussion by citing a science fiction story in which Aliens,\nanatomically quite unlike humans, cannot believe that humans think when they\ndiscover that our heads are filled with meat. The Aliens\u2019 intuitions are\nunreliable \u2013 presumably ours may be so as well.\n\nClearly the CRA turns on what is required to understand language. Schank 1978\nclarifies his claim about what he thinks his programs can do: \u201cBy\n\u2018understand\u2019, we mean SAM [one of his programs] can create a linked causal\nchain of conceptualizations that represent what took place in each story.\u201d\nThis is a nuanced understanding of \u201cunderstanding\u201d, whereas the Chinese Room\nthought experiment does not turn on a technical understanding of\n\u201cunderstanding\u201d, but rather intuitions about our ordinary competence when we\nunderstand a word like \u201chamburger\u201d. Indeed by 2015 Schank distances himself\nfrom weak senses of \u201cunderstand\u201d, holding that no computer can \u201cunderstand\nwhen you tell it something\u201d, and that IBM\u2019s WATSON \u201cdoesn\u2019t know what it is\nsaying\u201d. Schank\u2019s program may get links right, but arguably does not know what\nthe linked entities are. Whether it does or not depends on what concepts are,\nsee section 5.1. Furthermore it is possible that when it comes to attributing\nunderstanding of language we have different standards for different things \u2013\nmore relaxed for dogs and toddlers. Some things understand a language \u201cun\npoco\u201d. Searle (1980)concedes that there are degrees of understanding, but says\nthat all that matters that there are clear cases of no understanding, and AI\nprograms are an example: \u201cThe computer understanding is not just (like my\nunderstanding of German) partial or incomplete; it is zero.\u201d\n\nSome defenders of AI are also concerned with how our understanding of\nunderstanding bears on the Chinese Room argument. In their paper \u201cA Chinese\nRoom that Understands\u201d AI researchers Simon and Eisenstadt (2002) argue that\nwhereas Searle refutes \u201clogical strong AI\u201d, the thesis that a program that\npasses the Turing Test will necessarily understand, Searle\u2019s argument does not\nimpugn \u201cEmpirical Strong AI\u201d \u2013 the thesis that it is possible to program a\ncomputer that convincingly satisfies ordinary criteria of understanding. They\nhold however that it is impossible to settle these questions \u201cwithout\nemploying a definition of the term \u2018understand\u2019 that can provide a test for\njudging whether the hypothesis is true or false\u201d. They cite W.V.O. Quine\u2019s\nWord and Object as showing that there is always empirical uncertainty in\nattributing understanding to humans. The Chinese Room is a Clever Hans trick\n(Clever Hans was a horse who appeared to clomp out the answers to simple\narithmetic questions, but it was discovered that Hans could detect unconscious\ncues from his trainer). Similarly, the man in the room doesn\u2019t understand\nChinese, and could be exposed by watching him closely. (Simon and Eisenstadt\ndo not explain just how this would be done, or how it would affect the\nargument.) Citing the work of Rudolf Carnap, Simon and Eisenstadt argue that\nto understand is not just to exhibit certain behavior, but to use \u201cintensions\u201d\nthat determine extensions, and that one can see in actual programs that they\ndo use appropriate intensions. They discuss three actual AI programs, and\ndefend various attributions of mentality to them, including understanding, and\nconclude that computers understand; they learn \u201cintensions by associating\nwords and other linguistic structure with their denotations, as detected\nthrough sensory stimuli\u201d. And since we can see exactly how the machines work,\n\u201cit is, in fact, easier to establish that a machine exhibits understanding\nthat to establish that a human exhibits understanding....\u201d Thus, they\nconclude, the evidence for empirical strong AI is overwhelming.\n\nSimilarly, Daniel Dennett in his original 1980 response to Searle\u2019s argument\ncalled it \u201can intuition pump\u201d, a term he came up with in discussing the CRA\nwith Hofstader. Sharvy 1983 echoes the complaint. Dennett\u2019s considered view\n(2013) is that the CRA is \u201cclearly a fallacious and misleading argument ....\u201d\n(p. 320). Paul Thagard (2013) proposes that for every thought experiment in\nphilosophy there is an equal and opposite thought experiment. Thagard holds\nthat intuitions are unreliable, and the CRA is an example (and that in fact\nthe CRA has now been refuted by the technology of autonomous robotic cars).\nDennett has elaborated on concerns about our intuitions regarding\nintelligence. Dennett 1987 (\u201cFast Thinking\u201d) expressed concerns about the slow\nspeed at which the Chinese Room would operate, and he has been joined by\nseveral other commentators, including Tim Maudlin, David Chalmers, and Steven\nPinker. The operator of the Chinese Room may eventually produce appropriate\nanswers to Chinese questions. But slow thinkers are stupid, not intelligent \u2013\nand in the wild, they may well end up dead. Dennett argues that \u201cspeed ... is\n\u2018of the essence\u2019 for intelligence. If you can\u2019t figure out the relevant\nportions of the changing environment fast enough to fend for yourself, you are\nnot practically intelligent, however complex you are\u201d (326). Thus Dennett\nrelativizes intelligence to processing speed relative to current environment.\n\nTim Maudlin (1989) disagrees. Maudlin considers the time-scale problem pointed\nto by other writers, and concludes, contra Dennett, that the extreme slowness\nof a computational system does not violate any necessary conditions on\nthinking or consciousness. Furthermore, Searle\u2019s main claim is about\nunderstanding, not intelligence or being quick-witted. If we were to encounter\nextra-terrestrials that could process information a thousand times more\nquickly than we do, it seems that would show nothing about our own slow-poke\nability to understand the languages we speak.\n\nSteven Pinker (1997) also holds that Searle relies on untutored intuitions.\nPinker endorses the Churchlands\u2019 (1990) counterexample of an analogous thought\nexperiment of waving a magnet and not generating light, noting that this\noutcome would not disprove Maxwell\u2019s theory that light consists of\nelectromagnetic waves. Pinker holds that the key issue is speed: \u201cThe thought\nexperiment slows down the waves to a range to which we humans no longer see\nthem as light. By trusting our intuitions in the thought experiment, we\nfalsely conclude that rapid waves cannot be light either. Similarly, Searle\nhas slowed down the mental computations to a range in which we humans no\nlonger think of it as understanding (since understanding is ordinarily much\nfaster)\u201d (94\u201395). Howard Gardiner, a supporter of Searle\u2019s conclusions\nregarding the room, makes a similar point about understanding. Gardiner\naddresses the Chinese Room argument in his book The Mind\u2019s New Science (1985,\n171\u2013177). Gardiner considers all the standard replies to the Chinese Room\nargument and concludes that Searle is correct about the room: \u201c...the word\nunderstand has been unduly stretched in the case of the Chinese room ....\u201d\n(175).\n\nThus several in this group of critics argue that speed affects our willingness\nto attribute intelligence and understanding to a slow system, such as that in\nthe Chinese Room. The result may simply be that our intuitions regarding the\nChinese Room are unreliable, and thus the man in the room, in implementing the\nprogram, may understand Chinese despite intuitions to the contrary (Maudlin\nand Pinker). Or it may be that the slowness marks a crucial difference between\nthe simulation in the room and what a fast computer does, such that the man is\nnot intelligent while the computer system is (Dennett).\n\n## 5\\. The Larger Philosophical Issues\n\n### 5.1 Syntax and Semantics\n\nSearle believes the Chinese Room argument supports a larger point, which\nexplains the failure of the Chinese Room to produce understanding. Searle\nargued that programs implemented by computers are just syntactical. Computer\noperations are \u201cformal\u201d in that they respond only to the physical form of the\nstrings of symbols, not to the meaning of the symbols. Minds on the other hand\nhave states with meaning, mental contents. We associate meanings with the\nwords or signs in language. We respond to signs because of their meaning, not\njust their physical appearance. In short, we understand. But, and according to\nSearle this is the key point, \u201cSyntax is not by itself sufficient for, nor\nconstitutive of, semantics.\u201d So although computers may be able to manipulate\nsyntax to produce appropriate responses to natural language input, they do not\nunderstand the sentences they receive or output, for they cannot associate\nmeanings with the words.\n\nSearle (1984) presents a three premise argument that because syntax is not\nsufficient for semantics, programs cannot produce minds.\n\n  1. Programs are purely formal (syntactic).\n  2. Human minds have mental contents (semantics).\n  3. Syntax by itself is neither constitutive of, nor sufficient for, semantic content.\n  4. Therefore, programs by themselves are not constitutive of nor sufficient for minds.\n\nThe Chinese Room thought experiment itself is the support for the third\npremise. The claim that syntactic manipulation is not sufficient for meaning\nor thought is a significant issue, with wider implications than AI, or\nattributions of understanding. Prominent theories of mind hold that human\ncognition generally is computational. In one form, it is held that thought\ninvolves operations on symbols in virtue of their physical properties. On an\nalternative connectionist account, the computations are on \u201csubsymbolic\u201d\nstates. If Searle is right, not only Strong AI but also these main approaches\nto understanding human cognition are misguided.\n\nAs we have seen, Searle holds that the Chinese Room scenario shows that one\ncannot get semantics from syntax alone. In a symbolic logic system, a kind of\nartificial language, rules are given for syntax. A semantics, if any, comes\nlater. The logician specifies the basic symbol set and some rules for\nmanipulating strings to produce new ones. These rules are purely syntactic \u2013\nthey are applied to strings of symbols solely in virtue of their syntax or\nform. A semantics, if any, for the symbol system must be provided separately.\nAnd if one wishes to show that interesting additional relationships hold\nbetween the syntactic operations and semantics, such as that the symbol\nmanipulations preserve truth, one must provide sometimes complex meta-proofs\nto show this. So on the face of it, semantics is quite independent of syntax\nfor artificial languages, and one cannot get semantics from syntax alone.\n\u201cFormal symbols by themselves can never be enough for mental contents, because\nthe symbols, by definition, have no meaning (or interpretation, or semantics)\nexcept insofar as someone outside the system gives it to them\u201d (Searle 1989,\n45).\n\nSearle\u2019s identification of meaning with interpretation in this passage is\nimportant. Searle\u2019s point is clearly true of the causally inert formal systems\nof logicians. A semantic interpretation has to be given to those symbols by a\nlogician. When we move from formal systems to computational systems, the\nsituation is more complex. As many of Searle\u2019s critics (e.g. Cole 1984,\nDennett 1987, Boden 1988, and Chalmers 1996) have noted, a computer running a\nprogram is not the same as \u201csyntax alone\u201d. A computer is an enormously complex\nelectronic causal system. State changes in the system are physical. One can\ninterpret the physical states, e.g. voltages, as syntactic 1\u2019s and 0\u2019s, but\nthe intrinsic reality is electronic and the syntax is \u201cderived\u201d, a product of\ninterpretation. The states are syntactically specified by programmers, but\nwhen implemented in a running machine they are electronic states of a complex\ncausal system embedded in the real world. This is quite different from the\nabstract formal systems that logicians study. Dennett notes that no \u201ccomputer\nprogram by itself\u201d (Searle\u2019s language) \u2013 e.g. a program lying on a shelf \u2013 can\ncause anything, even simple addition, let alone mental states. The program\nmust be running. Chalmers (1996) offers a parody in which it is reasoned that\nrecipes are syntactic, syntax is not sufficient for crumbliness, cakes are\ncrumbly, so implementation of a recipe is not sufficient for making a cake.\nImplementation makes all the difference; an abstract entity (recipe, program)\ndetermines the causal powers of a physical system embedded in the larger\ncausal nexus of the world.\n\nDennett (1987) sums up the issue: \u201cSearle\u2019s view, then, comes to this: take a\nmaterial object (any material object) that does not have the power of causing\nmental phenomena; you cannot turn it in to an object that does have the power\nof producing mental phenomena simply by programming it \u2013 reorganizing the\nconditional dependencies of transitions between its states.\u201d Dennett\u2019s view is\nthe opposite: programming \u201cis precisely what could give something a mind\u201d. But\nDennett claims that in fact it is \u201cempirically unlikely that the right sorts\nof programs can be run on anything but organic, human brains\u201d (325\u20136).\n\nA further related complication is that it is not clear that computers perform\nsyntactic operations in quite the same sense that a human does \u2013 it is not\nclear that a computer understands syntax or syntactic operations. A computer\ndoes not know that it is manipulating 1\u2019s and 0\u2019s. A computer does not\nrecognize that its binary data strings have a certain form, and thus that\ncertain syntactic rules may be applied to them, unlike the man inside the\nChinese Room. Inside a computer, there is nothing that literally reads input\ndata, or that \u201cknows\u201d what symbols are. Instead, there are millions of\ntransistors that change states. A sequence of voltages causes operations to be\nperformed. We humans may choose to interpret these voltages as binary numerals\nand the voltage changes as syntactic operations, but a computer does not\ninterpret its operations as syntactic or any other way. So perhaps a computer\ndoes not need to make the move from syntax to semantics that Searle objects\nto; it needs to move from complex causal connections to semantics.\nFurthermore, perhaps any causal system is describable as performing syntactic\noperations \u2013 if we interpret a light square as logical \u201c0\u201d and a dark square\nas logical \u201c1\u201d, then a kitchen toaster may be described as a device that\nrewrites logical \u201c0\u201ds as logical \u201c1\u201ds. But there is no philosophical problem\nabout getting from syntax to breakfast.\n\nIn the 1990s, Searle began to use considerations related to these to argue\nthat computational views are not just false, but lack a clear sense.\nComputation, or syntax, is \u201cobserver-relative\u201d, not an intrinsic feature of\nreality: \u201c...you can assign a computational interpretation to anything\u201d\n(Searle 2002b, p. 17), even the molecules in the paint on the wall. Since\nnothing is intrinsically computational, one cannot have a scientific theory\nthat reduces the mental, which is not observer-relative, to computation, which\nis. \u201cComputation exists only relative to some agent or observer who imposes a\ncomputational interpretation on some phenomenon. This is an obvious point. I\nshould have seen it ten years ago, but I did not.\u201d (Searle 2002b, p.17,\noriginally published 1993).\n\nCritics note that walls are not computers; unlike a wall, a computer goes\nthrough state-transitions that are counterfactually described by a program\n(Chalmers 1996, Block 2002, Haugeland 2002). In his 2002 paper, Block\naddresses the question of whether a wall is a computer (in reply to Searle\u2019s\ncharge that anything that maps onto a formal system is a formal system,\nwhereas minds are quite different). Block denies that whether or not something\nis a computer depends entirely on our interpretation. Block notes that Searle\nignores the counterfactuals that must be true of an implementing system.\nHaugeland (2002) makes the similar point that an implementation will be a\ncausal process that reliably carries out the operations \u2013 and they must be the\nright causal powers. Block concludes that Searle\u2019s arguments fail, but he\nconcedes that they \u201cdo succeed in sharpening our understanding of the nature\nof intentionality and its relation to computation and representation\u201d (78).\n\nRey (2002) also addresses Searle\u2019s arguments that syntax and symbols are\nobserver-relative properties, not physical. Searle infers this from the fact\nthat syntactic properties (e.g. being a logical \u201c1\u201d)are not defined in\nphysics; however Rey holds that it does not follow that they are observer-\nrelative. Rey argues that Searle also misunderstands what it is to realize a\nprogram. Rey endorses Chalmers\u2019 reply to Putnam: a realization is not just a\nstructural mapping, but involves causation, supporting counterfactuals. \u201cThis\npoint is missed so often, it bears repeating: the syntactically specifiable\nobjects over which computations are defined can and standardly do possess a\nsemantics; it\u2019s just that the semantics is not involved in the specification.\u201d\nStates of a person have their semantics in virtue of computational\norganization and their causal relations to the world. Rey concludes: Searle\n\u201csimply does not consider the substantial resources of functionalism and\nStrong AI.\u201d (222) A plausibly detailed story would defuse negative conclusions\ndrawn from the superficial sketch of the system in the Chinese Room.\n\nJohn Haugeland (2002) argues that there is a sense in which a processor must\nintrinsically understand the commands in the programs it runs: it executes\nthem in accord with the specifications. \u201cThe only way that we can make sense\nof a computer as executing a program is by understanding its processor as\nresponding to the program prescriptions as meaningful\u201d (385). Thus operation\nsymbols have meaning to a system. Haugeland goes on to draw a distinction\nbetween narrow and wide system. He argues that data can have semantics in the\nwide system that includes representations of external objects produced by\ntransducers. In passing, Haugeland makes the unusual claim, argued for\nelsewhere, that genuine intelligence and semantics presuppose \u201cthe capacity\nfor a kind of commitment in how one lives\u201d which is non-propositional \u2013 that\nis, love (cp. Steven Spielberg\u2019s 2001 film Artificial Intelligence: AI).\n\nTo Searle\u2019s claim that syntax is observer-relative, that the molecules in a\nwall might be interpreted as implementing the Wordstar program (an early word\nprocessing program) because \u201cthere is some pattern in the molecule movements\nwhich is isomorphic with the formal structure of Wordstar\u201d (Searle 1990b, p.\n27), Haugeland counters that \u201cthe very idea of a complex syntactical token ...\npresupposes specified processes of writing and reading....\u201d The tokens must be\nsystematically producible and retrievable. So no random isomorphism or pattern\nsomewhere (e.g. on some wall) is going to count, and hence syntax is not\nobserver-relative.\n\nWith regard to the question of whether one can get semantics from syntax,\nWilliam Rapaport has for many years argued for \u201csyntactic semantics\u201d, a view\nin which understanding is a special form of syntactic structure in which\nsymbols (such as Chinese words) are linked to concepts, themselves represented\nsyntactically. Others believe we are not there yet. AI futurist (The Age of\nSpiritual Machines) Ray Kurzweil holds in a 2002 follow-up book that it is red\nherring to focus on traditional symbol-manipulating computers. Kurzweil agrees\nwith Searle that existent computers do not understand language \u2013 as evidenced\nby the fact that they can\u2019t engage in convincing dialog. But that failure does\nnot bear on the capacity of future computers based on different technology.\nKurzweil claims that Searle fails to understand that future machines will use\n\u201cchaotic emergent methods that are massively parallel\u201d. This claim appears to\nbe similar to that of connectionists, such as Andy Clark, and the position\ntaken by the Churchlands in their 1990 Scientific American article.\n\nApart from Haugeland\u2019s claim that processors understand program instructions,\nSearle\u2019s critics can agree that computers no more understand syntax than they\nunderstand semantics, although, like all causal engines, a computer has\nsyntactic descriptions. And while it is often useful to programmers to treat\nthe machine as if it performed syntactic operations, it is not always so:\nsometimes the characters programmers use are just switches that make the\nmachine do something, for example, make a given pixel on the computer display\nturn red, or make a car transmission shift gears. Thus it is not clear that\nSearle is correct when he says a digital computer is just \u201ca device which\nmanipulates symbols\u201d. Computers are complex causal engines, and syntactic\ndescriptions are useful in order to structure the causal interconnections in\nthe machine. AI programmers face many tough problems, but one can hold that\nthey do not have to get semantics from syntax. If they are to get semantics,\nthey must get it from causality.\n\nTwo main approaches have developed that explain meaning in terms of causal\nconnections. The internalist approaches, such as Schank\u2019s and Rapaport\u2019s\nconceptual representation approaches, and also Conceptual Role Semantics, hold\nthat a state of a physical system gets its semantics from causal connections\nto other states of the same system. Thus a state of a computer might represent\n\u201ckiwi\u201d because it is connected to \u201cbird\u201d and \u201cflightless\u201d nodes, and perhaps\nalso to images of prototypical kiwis. The state that represents the property\nof being \u201cflightless\u201d might get its content from a Negation-operator modifying\na representation of \u201ccapable of airborne self-propulsion\u201d, and so forth, to\nform a vast connected conceptual network, a kind of mental dictionary.\n\nExternalist approaches developed by Dennis Stampe, Fred Dretske, Hilary\nPutnam, Jerry Fodor, Ruth Millikan, and others, hold that states of a physical\nsystem get their content through causal connections to the external reality\nthey represent. Thus, roughly, a system with a KIWI concept is one that has a\nstate it uses to represent the presence of kiwis in the external environment.\nThis kiwi-representing state can be any state that is appropriately causally\nconnected to the presence of kiwis. Depending on the system, the kiwi\nrepresenting state could be a state of a brain, or of an electrical device\nsuch as a computer, or even of a hydraulic system. The internal representing\nstate can then in turn play a causal role in the determining the behavior of\nthe system. For example, Rey (1986) endorses an indicator semantics along the\nlines of the work of Dennis Stampe (1977) and Fodor\u2019s Psychosemantics. These\nsemantic theories that locate content or meaning in appropriate causal\nrelations to the world fit well with the Robot Reply. A computer in a robot\nbody might have just the causal connections that could allow its inner\nsyntactic states to have the semantic property of representing states of\nthings in its environment.\n\nThus there are at least two families of theories (and marriages of the two, as\nin Block 1986) about how semantics might depend upon causal connections. Both\nof these attempt to provide accounts that are substance neutral: states of\nsuitably organized causal systems can have content, no matter what the systems\nare made of. On these theories a computer could have states that have meaning.\nIt is not necessary that the computer be aware of its own states and know that\nthey have meaning, nor that any outsider appreciate the meaning of the states.\nOn either of these accounts meaning depends upon the (possibly complex) causal\nconnections, and digital computers are systems designed to have states that\nhave just such complex causal dependencies. It should be noted that Searle\ndoes not subscribe to these theories of semantics. Instead, Searle\u2019s\ndiscussions of linguistic meaning have often centered on the notion of\nintentionality.\n\n### 5.2 Intentionality\n\nIntentionality is the property of being about something, having content. In\nthe 19th Century, psychologist Franz Brentano re-introduced this term from\nMedieval philosophy and held that intentionality was the \u201cmark of the mental\u201d.\nBeliefs and desires are intentional states: they have propositional content\n(one believes that p, one desires that p, where sentences that represent\npropositions substitute for \u201cp\u201d). Searle\u2019s views regarding intentionality are\ncomplex; of relevance here is that he makes a distinction between the original\nor intrinsic intentionality of genuine mental states, and the derived\nintentionality of language. A written or spoken sentence only has derivative\nintentionality insofar as it is interpreted by someone. It appears that on\nSearle\u2019s view, original intentionality can at least potentially be conscious.\nSearle then argues that the distinction between original and derived\nintentionality applies to computers. We can interpret the states of a computer\nas having content, but the states themselves do not have original\nintentionality. Many philosophers endorse this intentionality dualism,\nincluding Sayre (1986) and even Fodor (2009), despite Fodor\u2019s many differences\nwith Searle.\n\nIn a section of her 1988 book, Computer Models of the Mind, Margaret Boden\nnotes that intentionality is not well-understood \u2013 reason to not put too much\nweight on arguments that turn on intentionality. Furthermore, insofar as we\nunderstand the brain, we focus on informational functions, not unspecified\ncausal powers of the brain: \u201c...from the psychological point of view, it is\nnot the biochemistry as such which matters but the information-bearing\nfunctions grounded in it.\u201d (241) Searle sees intentionality as a causal power\nof the brain, uniquely produced by biological processes. Dale Jacquette 1989\nargues against a reduction of intentionality \u2013 intentionality, he says, is an\n\u201cineliminable, irreducible primitive concept.\u201d However most AI sympathizers\nhave seen intentionality, aboutness, as bound up with information, and non-\nbiological states can bear information as well as can brain states. Hence many\nresponders to Searle have argued that he displays substance chauvinism, in\nholding that brains understand but systems made of silicon with comparable\ninformation processing capabilities cannot, even in principle. Papers on both\nsides of the issue appeared, such as J. Maloney\u2019s 1987 paper \u201cThe Right\nStuff\u201d, defending Searle, and R. Sharvy\u2019s 1983 critique, \u201cIt Ain\u2019t the Meat,\nit\u2019s the Motion\u201d. AI proponents such as Kurzweil (1999, see also Richards\n2002) have continued to hold that AI systems can potentially have such mental\nproperties as understanding, intelligence, consciousness and intentionality,\nand will exceed human abilities in these areas.\n\nOther critics of Searle\u2019s position take intentionality more seriously than\nBoden does, but deny his dualistic distinction between original and derived\nintentionality. Dennett (1987, e.g.) argues that all intentionality is\nderived, in that attributions of intentionality \u2013 to animals, other people,\nand even ourselves \u2013 are instrumental and allow us to predict behavior, but\nthey are not descriptions of intrinsic properties. As we have seen, Dennett is\nconcerned about the slow speed of things in the Chinese Room, but he argues\nthat once a system is working up to speed, it has all that is needed for\nintelligence and derived intentionality \u2013 and derived intentionality is the\nonly kind that there is, according to Dennett. A machine can be an intentional\nsystem because intentional explanations work in predicting the machine\u2019s\nbehavior. Dennett also suggests that Searle conflates intentionality with\nawareness of intentionality. In his syntax-semantic arguments, \u201cSearle has\napparently confused a claim about the underivability of semantics from syntax\nwith a claim about the underivability of the consciousness of semantics from\nsyntax\u201d (336). The emphasis on consciousness forces us to think about things\nfrom a first-person point of view, but Dennett 2017 continues to press the\nclaim that this is a fundamental mistake if we want to understand the mental.\n\nWe might also worry that Searle conflates meaning and interpretation, and that\nSearle\u2019s original or underived intentionality is just second-order\nintentionality, a representation of what an intentional object represents or\nmeans. Dretske and others have seen intentionality as information-based. One\nstate of the world, including a state in a computer, may carry information\nabout other states in the world, and this informational aboutness is a mind-\nindependent feature of states. Hence it is a mistake to hold that conscious\nattributions of meaning are the source of intentionality.\n\nOthers have noted that Searle\u2019s discussion has shown a shift over time from\nissues of intentionality and understanding to issues of consciousness. Searle\nlinks intentionality to awareness of intentionality, in holding that\nintentional states are at least potentially conscious. In his 1996 book, The\nConscious Mind, David Chalmers notes that although Searle originally directs\nhis argument against machine intentionality, it is clear from later writings\nthat the real issue is consciousness, which Searle holds is a necessary\ncondition of intentionality. It is consciousness that is lacking in digital\ncomputers. Chalmers uses thought experiments to argue that it is implausible\nthat one system has some basic mental property (such as having qualia) that\nanother system lacks, if it is possible to imagine transforming one system\ninto the other, either gradually (as replacing neurons one at a time by\ndigital circuits), or all at once, switching back and forth between flesh and\nsilicon.\n\nA second strategy regarding the attribution of intentionality is taken by\ncritics who in effect argue that intentionality is an intrinsic feature of\nstates of physical systems that are causally connected with the world in the\nright way, independently of interpretation (see the preceding Syntax and\nSemantics section). Fodor\u2019s semantic externalism is influenced by Fred\nDretske, but they come to different conclusions with regard to the semantics\nof states of computers. Over a period of years, Dretske developed an\nhistorical account of meaning or mental content that would preclude\nattributing beliefs and understanding to most machines. Dretske (1985) agrees\nwith Searle that adding machines don\u2019t literally add; we do the adding, using\nthe machines. Dretske emphasizes the crucial role of natural selection and\nlearning in producing states that have genuine content. Human built systems\nwill be, at best, like Swampmen (beings that result from a lightning strike in\na swamp and by chance happen to be a molecule by molecule copy of some human\nbeing, say, you) \u2013 they appear to have intentionality or mental states, but do\nnot, because such states require the right history. AI states will generally\nbe counterfeits of real mental states; like counterfeit money, they may appear\nperfectly identical but lack the right pedigree. But Dretske\u2019s account of\nbelief appears to make it distinct from conscious awareness of the belief or\nintentional state (if that is taken to require a higher order thought), and so\nwould apparently allow attribution of intentionality to artificial systems\nthat can get the right history by learning.\n\nHoward Gardiner endorses Zenon Pylyshyn\u2019s criticisms of Searle\u2019s view of the\nrelation of brain and intentionality, as supposing that intentionality is\nsomehow a stuff \u201csecreted by the brain\u201d, and Pylyshyn\u2019s own counter-thought\nexperiment in which one\u2019s neurons are replaced one by one with integrated\ncircuit workalikes (see also Cole and Foelber (1984) and Chalmers (1996) for\nexploration of neuron replacement scenarios). Gardiner holds that Searle owes\nus a more precise account of intentionality than Searle has given so far, and\nuntil then it is an open question whether AI can produce it, or whether it is\nbeyond its scope. Gardiner concludes with the possibility that the dispute\nbetween Searle and his critics is not scientific, but (quasi?) religious.\n\n### 5.3 Mind and Body\n\nSeveral critics have noted that there are metaphysical issues at stake in the\noriginal argument. The Systems Reply draws attention to the metaphysical\nproblem of the relation of mind to body. It does this in holding that\nunderstanding is a property of the system as a whole, not the physical\nimplementer. The Virtual Mind Reply holds that minds or persons \u2013 the entities\nthat understand and are conscious \u2013 are more abstract than any physical\nsystem, and that there could be a many-to-one relation between minds and\nphysical systems. (Even if everything is physical, in principle a single body\ncould be shared by multiple minds, and a single mind could have a sequence of\nbodies over time.) Thus larger issues about personal identity and the relation\nof mind and body are in play in the debate between Searle and some of his\ncritics.\n\nSearle\u2019s view is that the problem the relation of mind and body \u201chas a rather\nsimple solution. Here it is: Conscious states are caused by lower level\nneurobiological processes in the brain and are themselves higher level\nfeatures of the brain\u201d (Searle 2002b, p. 9). In his early discussion of the\nCRA, Searle spoke of the causal powers of the brain. Thus his view appears to\nbe that brain states cause consciousness and understanding, and \u201cconsciousness\nis just a feature of the brain\u201d (ibid). However, as we have seen, even if this\nis true it begs the question of just whose consciousness a brain creates.\nRoger Sperry\u2019s split-brain experiments suggest that perhaps there can be two\ncenters of consciousness, and so in that sense two minds, implemented by a\nsingle brain. While both display at least some language comprehension, only\none (typically created by the left hemisphere) controls language production.\nThus many current approaches to understanding the relation of brain and\nconsciousness emphasize connectedness and information flow (see e.g. Dehaene\n2014).\n\nConsciousness and understanding are features of persons, so it appears that\nSearle accepts a metaphysics in which I, my conscious self, am identical with\nmy brain \u2013 a form of mind-brain identity theory. This very concrete\nmetaphysics is reflected in Searle\u2019s original presentation of the CR argument,\nin which Strong AI was described by him as the claim that \u201cthe appropriately\nprogrammed computer really is a mind\u201d (Searle 1980). This is an identity\nclaim, and has odd consequences. If A and B are identical, any property of A\nis a property of B. Computers are physical objects. Some computers weigh 6 lbs\nand have stereo speakers. So the claim that Searle called Strong AI would\nentail that some minds weigh 6 lbs and have stereo speakers. However it seems\nto be clear that while humans may weigh 150 pounds; human minds do not weigh\n150 pounds. This suggests that neither bodies nor machines can literally be\nminds. Such considerations support the view that minds are more abstract that\nbrains, and if so that at least one version of the claim that Searle calls\nStrong AI, the version that says that computers literally are minds, is\nmetaphysically untenable on the face of it, apart from any thought-\nexperiments.\n\nSearle\u2019s CR argument was thus directed against the claim that a computer is a\nmind, that a suitably programmed digital computer understands language, or\nthat its program does. Searle\u2019s thought experiment appeals to our strong\nintuition that someone who did exactly what the computer does would not\nthereby come to understand Chinese. As noted above, many critics have held\nthat Searle is quite right on this point \u2013 no matter how you program a\ncomputer, the computer will not literally be a mind and the computer will not\nunderstand natural language. But if minds are not physical objects this\ninability of a computer to be a mind does not show that running an AI program\ncannot produce understanding of natural language, by something other than the\ncomputer (See section 4.1 above.)\n\nFunctionalism is a theory of the relation of minds to bodies that was\ndeveloped in the two decades prior to Searle\u2019s CRA. Functionalism is an\nalternative to the identity theory that is implicit in much of Searle\u2019s\ndiscussion, as well as to the dominant behaviorism of the mid-Twentieth\nCentury. If functionalism is correct, there appears to be no intrinsic reason\nwhy a computer couldn\u2019t have mental states. Hence the CRA\u2019s conclusion that a\ncomputer is intrinsically incapable of mental states is an important\nconsideration against functionalism. Julian Baggini (2009, 37) writes that\nSearle \u201ccame up with perhaps the most famous counter-example in history \u2013 the\nChinese room argument \u2013 and in one intellectual punch inflicted so much damage\non the then dominant theory of functionalism that many would argue it has\nnever recovered.\u201d\n\nFunctionalists hold that a mental state is what a mental state does \u2013 the\ncausal (or \u201cfunctional\u201d) role that the state plays determines what state it\nis. A functionalist might hold that pain, for example, is a state that is\ntypically caused by damage to the body, is located in a body-image, and is\naversive. Functionalists distance themselves both from behaviorists and\nidentity theorists. In contrast with the former, functionalists hold that the\ninternal causal processes are important for the possession of mental states.\nThus functionalists may agree with Searle in rejecting the Turing Test as too\nbehavioristic. In contrast with identity theorists (who might e.g. hold \u201cpain\nis identical with C-fiber firing\u201d), functionalists hold that mental states\nmight be had by a variety of physical systems (or non-physical, as in Cole and\nFoelber 1984, in which a mind changes from a material to an immaterial\nimplementation, neuron by neuron). Thus while an identity theorist will\nidentify pain with certain neuron firings, a functionalist will identify pain\nwith something more abstract and higher level, a functional role that might be\nhad by many different types of underlying system.\n\nFunctionalists accuse identity theorists of substance chauvinism. However,\nfunctionalism remains controversial: functionalism is vulnerable to the\nChinese Nation type objections discussed above, and functionalists notoriously\nhave trouble explaining qualia, a problem highlighted by the apparent\npossibility of an inverted spectrum, where qualitatively different states\nmight have the same functional role (e.g. Block 1978, Maudlin 1989, Cole\n1990).\n\nComputationalism is the sub-species of functionalism that holds that the\nimportant causal role of brain processes is information processing. Milkowski\n2017 notes that computational approaches have been fruitful in cognitive\nscience; he surveys objections to computationalism and concludes that the\nmajority target a strawman version. However Jerry Fodor, an early proponent of\ncomputational approaches, argues in Fodor 2005 that key mental processes, such\nas inference to the best explanation, which depend on non-local properties of\nrepresentations, cannot be explained by computational modules in the brain. If\nFodor is right, understanding language and interpretation appear to involve\nglobal considerations such as linguistic and non-linguistic context and theory\nof mind and so might resist computational explanation. If so, we reach\nSearle\u2019s conclusion on the basis of different considerations.\n\nSearle\u2019s 2010 statement of the conclusion of the CRA has it showing that\ncomputational accounts cannot explain consciousness. There has been\nconsiderable interest in the decades since 1980 in determining what does\nexplain consciousness, and this has been an extremely active research area\nacross disciplines. One interest has been in the neural correlates of\nconsciousness. This bears directly on Searle\u2019s claim that consciousness is\nintrinsically biological and not computational or information processing.\nThere is no definitive answer yet, though some recent work on anesthesia\nsuggests that consciousness is lost when cortical (and cortico-thalamic)\nconnections and information flow are disrupted (e.g.Hudetz 2012, a review\narticle).\n\nIn general, if the basis of consciousness is confirmed to be at the relatively\nabstract level of information flow through neural networks, it will be\nfriendly to functionalism, and if it is turns out to be lower and more\nbiological (or sub-neuronal), it will be friendly to Searle\u2019s account.\n\nThese controversial biological and metaphysical issues bear on the central\ninference in the Chinese Room argument. From the intuition that in the CR\nthought experiment he would not understand Chinese by running a program,\nSearle infers that there is no understanding created by running a program.\nClearly, whether that inference is valid or not turns on a metaphysical\nquestion about the identity of persons and minds. If the person understanding\nis not identical with the room operator, then the inference is unsound.\n\n### 5.4 Simulation, duplication and evolution\n\nIn discussing the CRA, Searle argues that there is an important distinction\nbetween simulation and duplication. No one would mistake a computer simulation\nof the weather for weather, or a computer simulation of digestion for real\ndigestion. Searle concludes that it is just as serious a mistake to confuse a\ncomputer simulation of understanding with understanding.\n\nOn the face of it, there is generally an important distinction between a\nsimulation and the real thing. But two problems emerge. It is not clear that\nthe distinction can always be made. Hearts are biological if anything is. Are\nartificial hearts simulations of hearts? Or are they functional duplicates of\nhearts, hearts made from different materials? Walking is normally a biological\nphenomenon performed using limbs. Do those with artificial limbs walk? Or do\nthey simulate walking? Do robots walk? If the properties that are needed to be\ncertain kind of thing are high-level properties, anything sharing those\nproperties will be a thing of that kind, even if it differs in its lower level\nproperties. Chalmers (1996) offers a principle governing when simulation is\nreplication. Chalmers suggests that, contra Searle and Harnad (1989), a\nsimulation of X can be an X, namely when the property of being an X is an\norganizational invariant, a property that depends only on the functional\norganization of the underlying system, and not on any other details.\n\nCopeland (2002) argues that the Church-Turing thesis does not entail that the\nbrain (or every machine) can be simulated by a universal Turing machine, for\nthe brain (or other machine) might have primitive operations that are not\nsimple clerical routines that can be carried out by hand. (An example might be\nthat human brains likely display genuine low-level randomness, whereas\ncomputers are carefully designed not to do that, and so computers resort to\npseudo-random numbers when apparent randomness is needed.) Sprevak 2007 raises\na related point. Turing\u2019s 1938 Princeton thesis described such machines\n(\u201cO-machines\u201d). O-machines are machines that include functions of natural\nnumbers that are not Turing-machine computable. If the brain is such a\nmachine, then, says Sprevak,: \u201cThere is no possibility of Searle\u2019s Chinese\nRoom Argument being successfully deployed against the functionalist hypothesis\nthat the brain instantiates an O-machine....\u201d (120).\n\nCopeland discusses the simulation / duplication distinction in connection with\nthe Brain Simulator Reply. He argues that Searle correctly notes that one\ncannot infer from X simulates Y, and Y has property P, to the conclusion that\ntherefore X has Y\u2019s property P for arbitrary P. But Copeland claims that\nSearle himself commits the simulation fallacy in extending the CR argument\nfrom traditional AI to apply against computationalism. The contrapositive of\nthe inference is logically equivalent \u2013 X simulates Y, X does not have P\ntherefore Y does not \u2013 where P is understands Chinese. The faulty step is: the\nCR operator S simulates a neural net N, it is not the case that S understands\nChinese, therefore it is not the case that N understands Chinese. Copeland\nalso notes results by Siegelmann and Sontag (1994) showing that some\nconnectionist networks cannot be simulated by a universal Turing Machine (in\nparticular, where connection weights are real numbers).\n\nThere is another problem with the simulation-duplication distinction, arising\nfrom the process of evolution. Searle wishes to see original intentionality\nand genuine understanding as properties only of certain biological systems,\npresumably the product of evolution. Computers merely simulate these\nproperties. At the same time, in the Chinese Room scenario, Searle maintains\nthat a system can exhibit behavior just as complex as human behavior,\nsimulating any degree of intelligence and language comprehension that one can\nimagine, and simulating any ability to deal with the world, yet not understand\na thing. He also says that such behaviorally complex systems might be\nimplemented with very ordinary materials, for example with tubes of water and\nvalves.\n\nThis creates a biological problem, beyond the Other Minds problem noted by\nearly critics of the CR argument. While we may presuppose that others have\nminds, evolution makes no such presuppositions. The selection forces that\ndrive biological evolution select on the basis of behavior. Evolution can\nselect for the ability to use information about the environment creatively and\nintelligently, as long as this is manifest in the behavior of the organism. If\nthere is no overt difference in behavior in any set of circumstances between a\nsystem that understands and one that does not, evolution cannot select for\ngenuine understanding. And so it seems that on Searle\u2019s account, minds that\ngenuinely understand meaning have no advantage over creatures that merely\nprocess information, using purely computational processes. Thus a position\nthat implies that simulations of understanding can be just as biologically\nadaptive as the real thing, leaves us with a puzzle about how and why systems\nwith \u201cgenuine\u201d understanding could evolve. Original intentionality and genuine\nunderstanding become epiphenomenal.\n\n## Conclusion\n\nAs we have seen, since its appearance in 1980 the Chinese Room argument has\nsparked discussion across disciplines. Despite the extensive discussion there\nis still no consensus as to whether the argument is sound. At one end we have\nJulian Baggini\u2019s (2009) assessment that Searle \u201ccame up with perhaps the most\nfamous counter-example in history \u2013 the Chinese room argument \u2013 and in one\nintellectual punch inflicted so much damage on the then dominant theory of\nfunctionalism that many would argue it has never recovered.\u201d Whereas\nphilosopher Daniel Dennett (2013, p. 320) concludes that the Chinese Room\nargument is \u201cclearly a fallacious and misleading argument\u201d. Hence there is no\nconsensus as to whether the argument is a proof that limits the aspirations of\nArtificial Intelligence or computational accounts of mind.\n\nMeanwhile work in artificial intelligence and natural language processing has\ncontinued. The CRA led Stevan Harnad and others on a quest for \u201csymbol\ngrounding\u201d in AI. Many in philosophy (Dretske, Fodor, Millikan) worked on\nnaturalistic theories of mental content. Speculation about the nature of\nconsciousness continues in many disciplines. And computers have moved from the\nlab to the pocket and the wrist.\n\nAt the time of Searle\u2019s construction of the argument, personal computers were\nvery limited hobbyist devices. Weizenbaum\u2019s \u2018Eliza\u2019 and a few text \u2018adventure\u2019\ngames were played on DEC computers; these included limited parsers. More\nadvanced parsing of language was limited to computer researchers such as\nSchank. Much changed in the next quarter century; billions now use natural\nlanguage to interrogate and command virtual agents via computers they carry in\ntheir pockets. Has the Chinese Room argument moderated claims by those who\nproduce AI and natural language systems? Some manufacturers linking devices to\nthe \u201cinternet of things\u201d make modest claims: appliance manufacturer LG says\nthe second decade of the 21st century brings the \u201cexperience of conversing\u201d\nwith major appliances. That may or may not be the same as conversing. Apple is\nless cautious than LG in describing the capabilities of its \u201cvirtual personal\nassistant\u201d application called \u2018Siri\u2019: Apple says of Siri that \u201cIt understands\nwhat you say. It knows what you mean.\u201d IBM is quick to claim its much larger\n\u2018Watson\u2019 system is superior in language abilities to Siri. In 2011 Watson beat\nhuman champions on the television game show \u2018Jeopardy\u2019, a feat that relies\nheavily on language abilities and inference. IBM goes on to claim that what\ndistinguishes Watson is that it \u201cknows what it knows, and knows what it does\nnot know.\u201d This appears to be claiming a form of reflexive self-awareness or\nconsciousness for the Watson computer system. Thus the claims of strong AI now\nare hardly chastened, and if anything some are stronger and more exuberant. At\nthe same time, as we have seen, many others believe that the Chinese Room\nArgument showed once and for all that at best computers can simulate human\ncognition.\n\nThough separated by three centuries, Leibniz and Searle had similar intuitions\nabout the systems they consider in their respective thought experiments,\nLeibniz\u2019 Mill and the Chinese Room. In both cases they consider a complex\nsystem composed of relatively simple operations, and note that it is\nimpossible to see how understanding or consciousness could result. These\nsimple arguments do us the service of highlighting the serious problems we\nface in understanding meaning and minds. The many issues raised by the Chinese\nRoom argument may not be settled until there is a consensus about the nature\nof meaning, its relation to syntax, and about the biological basis of\nconsciousness. There continues to be significant disagreement about what\nprocesses create meaning, understanding, and consciousness, as well as what\ncan be proven a priori by thought experiments.\n\n## Bibliography\n\n  * Apple Inc., 2014, \u2018IOS 7 Siri\u2019, accessed 1/10/2014.\n  * Baggini, J., 2009, \u2018Painting the bigger picture\u2019, The Philosopher\u2019s Magazine, 8: 37\u201339.\n  * Block, N., 1978, \u2018Troubles with Functionalism\u2019, in C. W. Savage (ed.), Perception and Cognition: Issues in the Foundations of Psychology, Minneapolis: University of Minnesota Press. (Reprinted in many anthologies on philosophy of mind and psychology.)\n  * \u2013\u2013\u2013, 1986, \u2018Advertisement for a Semantics for Psychology\u2019, Midwest Studies in Philosophy (Volume X), P.A. French, et al. (eds.), Minneapolis: University of Minnesota Press, 615\u2013678.\n  * \u2013\u2013\u2013, 2002, \u2018Searle\u2019s Arguments Against Cognitive Science\u2019, in Preston and Bishop (eds.) 2002.\n  * Boden, M., 1988, Computer Models of the Mind, Cambridge: Cambridge University Press; pp. 238\u2013251 were excerpted and published as \u2018Escaping from the Chinese Room\u2019, in The Philosophy of Artificial Intelligence, ed M. A. Boden, New York: Oxford University Press, 1990.\n  * Cam, P., 1990, \u2018Searle on Strong AI\u2019, Australasian Journal of Philosophy, 68: 103\u20138.\n  * Chalmers, D., 1992, \u2018Subsymbolic Computation and the Chinese Room\u2019, in J. Dinsmore (ed.), The Symbolic and Connectionist Paradigms: Closing the Gap, Hillsdale, NJ: Lawrence Erlbaum.\n  * \u2013\u2013\u2013, 1996, The Conscious Mind, Oxford: Oxford University Press.\n  * \u2013\u2013\u2013, 1996a, \u201cDoes a Rock Implement Every Finite-State Automaton\u2019, Synthese 108: 309\u201333.\n  * \u2013\u2013\u2013, 1996b, \u2018Minds, machines, and mathematics\u2019, Psyche, 2: 11\u201320.\n  * Churchland, P., 1985, \u2018Reductionism, Qualia, and the Direct Introspection of Brain States\u2019, The Journal of Philosophy, LXXXII: 8\u201328.\n  * Churchland, P. and Churchland, P., 1990, \u2018Could a machine think?\u2019, Scientific American, 262(1): 32\u201337.\n  * Clark, A., 1991, Microcognition: Philosophy, Cognitive Science, and Parallel Distributed Processing, Cambridge, MA: MIT Press.\n  * Cole, D., 1984, \u2018Thought and Thought Experiments\u2019, Philosophical Studies, 45: 431\u201344.\n  * \u2013\u2013\u2013, 1990, \u2018Functionalism and Inverted Spectra\u2019, Synthese, 82: 202\u2013222.\n  * \u2013\u2013\u2013, 1991a, \u2018Artificial Intelligence and Personal Identity\u2019, Synthese, 88: 399\u2013417.\n  * \u2013\u2013\u2013, 1991b, \u2018Artificial Minds: Cam on Searle\u2019, Australasian Journal of Philosophy, 69: 329\u201333.\n  * \u2013\u2013\u2013, 1994, \u2018The Causal Powers of CPUs\u2019, in E. Dietrich (ed.), Thinking Computers and Virtual Persons, New York: Academic Press\n  * Cole, D. and Foelber, R., 1984, Contingent Materialism\u2019, Pacific Philosophical Quarterly, 65(1): 74\u201385.\n  * Copeland, J., 2002, \u2018The Chinese Room from a Logical Point of View\u2019, in Preston and Bishop (eds.) 2002, 104\u2013122.\n  * Crane, Tim., 1996, The Mechanical Mind: A Philosophical Introduction to Minds, Machines and Mental Representation, London: Penguin.\n  * Davis, Lawrence, 2001, \u2018Functionalism, the Brain, and Personal Identity\u2019, Philosophical Studies, 102(3): 259\u2013279.\n  * Dehaene, S., 2014, Consciousness and the Brain: Deciphering How the Brain Codes Our Thoughts, New York: Viking Penquin.\n  * Dennett, D., 1978, \u2018Toward a Cognitive Theory of Consciousness\u2019, in Brainstorms: Philosophical Essays on Mind and Psychology, Cambridge, MA: MIT Press.\n  * \u2013\u2013\u2013, 1981, \u2018Where am I?\u2019 in Brainstorms: Philosophical Essays on Mind and Psychology, Cambridge, MA: MIT Press, pp. 310\u2013323.\n  * \u2013\u2013\u2013, 1987, \u2018Fast Thinking\u2019, in The Intentional Stance, Cambridge, MA: MIT Press, 324\u2013337.\n  * \u2013\u2013\u2013, 1997, \u2018Consciousness in Humans and Robot Minds,\u2019 in M. Ito, Y. Miyashita and E.T. Rolls (eds.), Cognition, computation, and consciousness, New York: Oxford University Press, pp. 17\u201329.\n  * \u2013\u2013\u2013, 2013, Intuition Pumps and Other Tools for Thinking, New York: W.W. Norton and Co.\n  * Dneprov, A., 1961, \u2018\u0418\u0433\u0440\u0430\u2019 (\u2018The Game\u2019), \u0417\u043d\u0430\u043d\u0438\u0435-\u0441\u0438\u043b\u0430 (Knowledge is Power), 5: 39\u201342; for a link to the translation, see Mickevich 1961, Other Internet Resources.\n  * Double, R., 1983, \u2018Searle, Programs and Functionalism\u2019, Nature and System, 5: 107\u201314.\n  * Dretske, F. 1985, \u2018Presidential Address\u2019 (Central Division Meetings of the American Philosophical Association), Proceedings and Addresses of the American Philosophical Association, 59(1): 23\u201333.\n  * Dreyfus, H. 1965, \u2018Alchemy and Artificial Intelligence\u2019, Boston, MA: Rand Corporation.\n  * \u2013\u2013\u2013, 1972, What Computers Can\u2019t Do, New York: Harper & Row.\n  * Fodor, J., 1987, Psychosemantics, Cambridge, MA: MIT Press.\n  * \u2013\u2013\u2013, 1991, \u2018Yin and Yang in the Chinese Room\u2019, in D. Rosenthal (ed.), The Nature of Mind, New York: Oxford University Press.\n  * \u2013\u2013\u2013, 1992, A Theory of Content and other essays, Cambridge, MA: MIT Press.\n  * \u2013\u2013\u2013, 2009, \u2018Where is my Mind?\u2019, London Review of Books, (31)3: 13\u201315.\n  * Ford, J., 2010, \u2018Helen Keller was never in a Chinese Room\u2019, Minds and Machines, VOLUME: PAGES.\n  * Gardiner, H., 1987, The Mind\u2019s New Science: A History of the Cognitive Revolution, New York: Basic Books.\n  * Hanley, R., 1997, The Metaphysics of Star Trek, New York: Basic Books.\n  * Harnad, S., 1989, \u2018Minds, Machines and Searle\u2019, Journal of Experimental and Theoretical Artificial Intelligence, 1: 5\u201325.\n  * \u2013\u2013\u2013, 2002, \u2018Minds, Machines, and Searle2: What\u2019s Right and Wrong about the Chinese Room Argument\u2019, in Preston and Bishop (eds.) 2002, 294\u2013307.\n  * Haugeland, J., 2002, \u2018Syntax, Semantics, Physics\u2019, in Preston and Bishop (eds.) 2002, 379\u2013392.\n  * Hauser, L., 1997, \u2018Searle\u2019s Chinese Box: Debunking the Chinese Room Argument\u2019, Minds and Machines, 7: 199\u2013226.\n  * \u2013\u2013\u2013, 2002, \u2018Nixin\u2019 Goes to China\u2019, in Preston and Bishop (eds.) 2002, 123\u2013143.\n  * Hayes, P., Harnad, S., Perlis, D. & Block, N., 1992, \u2018Virtual Symposium on Virtual Mind\u2019, Minds and Machines, 2(3): 217\u2013238.\n  * Hofstadter, D., 1981, \u2018Reflections on Searle\u2019, in Hofstadter and Dennett (eds.), The Mind\u2019s I, New York: Basic Books, pp. 373\u2013382.\n  * Horgan, T., 2013, \u2018Original Intentionality is Phenomenal Intentionality\u2019, The Monist 96: 232\u2013251.\n  * Hudetz, A., 2012, \u2018General Anesthesia and Human Brain Connectivity\u2019, Brain Connect, 2(6): 291\u2013302.\n  * Jackson, F., 1986, \u2018What Mary Didn\u2019t Know\u2019, Journal of Philosophy, LXXXIII: 291\u20135.\n  * Kaernbach, C., 2005, \u2018No Virtual Mind in the Chinese Room\u2019, Journal of Consciousness Studies, 12(11): 31\u201342.\n  * Kim, J., 2010, The Philosophy of Mind, (3rd edition), Boulder, CO: Westview Press.\n  * Kurzweil, R., 2000, The Age of Spiritual Machines: When Computers Exceed Human Intelligence, New York: Penguin.\n  * \u2013\u2013\u2013, 2002, \u2018Locked in his Chinese Room\u2019, in Richards 2002, 128\u2013171.\n  * Maloney, J., 1987, \u2018The Right Stuff\u2019, Synthese, 70: 349\u201372.\n  * Maudlin, T., 1989, \u2018Computation and Consciousness\u2019, Journal of Philosophy, LXXXVI: 407\u2013432.\n  * Milkowski, M. 2017, \u2018Why think that the brain is not a computer?\u2019, APA Newsletter on Philosophy and Computers, 16(2), 22\u201328.\n  * Millikan, R., 1984, Language, Thought, and other Biological Categories, Cambridge, MA: MIT Press.\n  * Moravec, H., 1999, Robot: Mere Machine to Transcendent Mind, New York: Oxford University Press.\n  * Nute, D., 2011, \u2018A Logical Hole the Chinese Room Avoids\u2019, Minds and Machines, 21: 431\u20133; this is a reply to Shaffer 2009.\n  * Penrose, R., 2002, \u2018Consciousness, Computation, and the Chinese Room\u2019 in Preston and Bishop (eds.) 2002, 226\u2013249.\n  * Pinker, S., 1997, How the Mind Works, New York: Norton.\n  * Preston, J. and M. Bishop (eds.), 2002, Views into the Chinese Room: New Essays on Searle and Artificial Intelligence, New York: Oxford University Press.\n  * Pylyshyn, Z., 1980, Reply to Searle,Behavioral and Brain Sciences, 3.\n  * Rapaport, W., 1984, \u2018Searle\u2019s Experiments with Thought\u2019, Philosophy of Science, 53: 271\u20139.\n  * \u2013\u2013\u2013 2006, \u2018How Helen Keller Used Syntactic Semantics to Escape from a Chinese Room\u2019, Minds and Machines, 16(4): 381\u2013436.\n  * Rey, G., 1986, \u2018What\u2019s Really Going on in Searle\u2019s \u201cChinese Room\u201d \u2019, Philosophical Studies, 50: 169\u201385.\n  * \u2013\u2013\u2013, 2002, \u2018Searle\u2019s Misunderstandings of Functionalism and Strong AI\u2019, in Preston and Bishop (eds.) 2002, 201\u2013225.\n  * Richards, J. W. (ed.), 2002, Are We Spiritual Machines: Ray Kurzweil vs. the Critics of Strong AI, Seattle: Discovery Institute.\n  * Rosenthal, D. (ed), 1991, The Nature of Mind, Oxford and NY: Oxford University Press.\n  * Schank, R., 2015, \u2018Machines that Think are in the Movies\u2019, in Brockman, J. (ed.), What to Think About Machines that Think, New York: Harper Collins\n  * Schank, R. and Abelson, R., 1977, Scripts, Plans, Goals, and Understanding, Hillsdale, NJ: Lawrence Erlbaum.\n  * Schank, R. and P. Childers, 1985, The Cognitive Computer: On Language, Learning, and Artificial Intelligence, New York: Addison-Wesley.\n  * Schweizer, P., 2012, \u2018The Externalist Foundations of a Truly Total Turing Test\u2019, Minds and Machines, 22: 191\u2013212.\n  * Searle, J., 1980, \u2018Minds, Brains and Programs\u2019, Behavioral and Brain Sciences, 3: 417\u201357 [Preprint available online]\n  * \u2013\u2013\u2013, 1984, Minds, Brains and Science, Cambridge, MA: Harvard University Press.\n  * \u2013\u2013\u2013, 1989, \u2018Artificial Intelligence and the Chinese Room: An Exchange\u2019, New York Review of Books, 36: 2 (February 16, 1989).\n  * \u2013\u2013\u2013, 1990a, \u2018Is the Brain\u2019s Mind a Computer Program?\u2019, Scientific American, 262(1): 26\u201331.\n  * \u2013\u2013\u2013, 1990b, \u2018Presidential Address\u2019, Proceedings and Addresses of the American Philosophical Association, 64: 21\u201337.\n  * \u2013\u2013\u2013, 1998, \u2018Do We Understand Consciousness?\u2019 (Interview with Walter Freeman), Journal of Consciousness Studies, 6: 5\u20136.\n  * \u2013\u2013\u2013, 1999, \u2018The Chinese Room\u2019, in R.A. Wilson and F. Keil (eds.), The MIT Encyclopedia of the Cognitive Sciences, Cambridge, MA: MIT Press.\n  * \u2013\u2013\u2013, 2002a, \u2018Twenty-one Years in the Chinese Room\u2019, in Preston and Bishop (eds.) 2002, 51\u201369.\n  * \u2013\u2013\u2013, 2002b, \u2018The Problem of Consciousness\u2019, in Consciousness and Language, Cambridge: Cambridge University Press, 7\u201317.\n  * \u2013\u2013\u2013, 2004, Mind: a Brief Introduction, Oxford: Oxford University Press.\n  * \u2013\u2013\u2013, 2010, \u2018Why Dualism (and Materialism) Fail to Account for Consciousness\u2019, in Richard E. Lee (ed.), Questioning Nineteenth Century Assumptions about Knowledge (III: Dualism), New York: SUNY Press.\n  * Seligman, M., 2019, \u2018The Evolving Treatment of Semantics in Machine Translation\u2019, in M. Ji and M. Oakes (eds.), Advances in Empirical Translation Studies: Developing Translation Resources and Technologies, Cambridge: Cambridge University Press.\n  * Shaffer, M., 2009, \u2018A Logical Hole in the Chinese Room\u2019, Minds and Machines, 19(2): 229\u2013235.\n  * Sharvy, R., 1983, \u2018It Ain\u2019t the Meat It\u2019s the Motion\u2019, Inquiry, 26: 125\u2013134.\n  * Simon, H. and Eisenstadt, S., 2002, \u2018A Chinese Room that Understands\u2019, in Preston and Bishop (eds.) 2002, 95\u2013108.\n  * Sloman, A. and Croucher, M., 1980, \u2018How to turn an information processor into an understanding\u2019, Brain and Behavioral Sciences, 3: 447\u20138.\n  * Sprevak, M., 2007, \u2018Chinese Rooms and Program Portability\u2019, British Journal for the Philosophy of Science, 58(4): 755\u2013776.\n  * Stampe, Dennis, 1977, \u2018Towards a Causal Theory of Linguistic Representation\u2019, in P. French, T. Uehling, H. Wettstein, (eds.) Contemporary Perspectives in the Philosophy of Language, (Midwest Studies in Philosophy, Volume 2), Minneapolis: University of Minnesota Press, pp. 42\u201363.\n  * Thagard, P., 1986, \u2018The Emergence of Meaning: An Escape from Searle\u2019s Chinese Room\u2019, Behaviorism, 14: 139\u201346.\n  * \u2013\u2013\u2013, 2013, \u2018Thought Experiments Considered Harmful\u2018, Perspectives on Science, 21: 122\u2013139.\n  * Turing, A., 1948, \u2018Intelligent Machinery: A Report\u2019, London: National Physical Laboratory.\n  * \u2013\u2013\u2013, 1950, \u2018Computing Machinery and Intelligence\u2019, Mind, 59: 433\u2013460.\n  * Weiss, T., 1990, \u2018Closing the Chinese Room\u2019, Ratio, 3: 165\u201381.\n  * Ziemke, T., 2016, \u2018The Body of Knowledge: on the role of the living body in grounding embodied cognition\u2019, Biosystems, 148: 4\u201311.\n\n## Academic Tools\n\n> How to cite this entry.  \n> ---  \n> Preview the PDF version of this entry at the Friends of the SEP Society.  \n> Look up topics and thinkers related to this entry at the Internet Philosophy\n> Ontology Project (InPhO).  \n> Enhanced bibliography for this entry at PhilPapers, with links to its\n> database.  \n  \n## Other Internet Resources\n\n  * Harnad, S., 2012, \u2018Alan Turing and the \u2018Hard\u2019 and \u2018Easy\u2019 Problem of Cognition: Doing and Feeling,\u201d Turing100: Essays in Honour of Centenary Turing Year 2012, available online.\n  * Mickevich, A., 1961, \u2018The Game\u2019, translation of Dneprov 1961, at Center for Consciousness Studies (Philosophy Department, Moscow State University).\n  * Searle, J., Failures of Computationalism (Searle\u2019s reply to Harnad, and Harnad\u2019s response)\n  * Papers on the Chinese Room Argument, at PhilPapers.org.\n  * Annotated Chinese Room Bibliography, by L. Hauser.\n\n## Related Entries\n\ncomputation: in physical systems | consciousness: and intentionality | consciousness: representational theories of | emergent properties | epiphenomenalism | externalism about the mind | functionalism | information: biological | information: semantic conceptions of | intentionality | mental content: causal theories of | mental content: teleological theories of | mental representation | mind: computational theory of | multiple realizability | neuroscience, philosophy of | other minds | thought experiments | Turing, Alan | Turing test | zombies\n\nCopyright \u00a9 2020 by David Cole <dcole@d.umn.edu>\n\nOpen access to the SEP is made possible by a world-wide funding initiative.\nPlease Read How You Can Help Support the Growth and Development of the\nEncyclopedia\n\n#### Browse\n\n  * Table of Contents\n  * What's New\n  * Random Entry\n  * Chronological\n  * Archives\n\n#### About\n\n  * Editorial Information\n  * About the SEP\n  * Editorial Board\n  * How to Cite the SEP\n  * Special Characters\n  * Advanced Tools\n  * Accessibility\n  * Contact\n\n#### Support SEP\n\n  * Support the SEP\n  * PDFs for SEP Friends\n  * Make a Donation\n  * SEPIA for Libraries\n\n#### Mirror Sites\n\nView this site from another server:\n\nUSA (Main Site) Philosophy, Stanford University\n\n  * Australia Library, University of Sydney\n  * Netherlands ILLC, University of Amsterdam\n\nThe Stanford Encyclopedia of Philosophy is copyright \u00a9 2023 by The Metaphysics\nResearch Lab, Department of Philosophy, Stanford University\n\nLibrary of Congress Catalog Data: ISSN 1095-5054\n\n", "frontpage": false}
