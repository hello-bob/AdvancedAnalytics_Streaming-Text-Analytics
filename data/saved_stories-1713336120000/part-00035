{"aid": "40056916", "title": "What makes concurrency so hard?", "url": "https://buttondown.email/hillelwayne/archive/what-makes-concurrency-so-hard/", "domain": "buttondown.email/hillelwayne", "votes": 5, "user": "BerislavLopac", "posted_at": "2024-04-16 20:40:37", "comments": 0, "source_title": "What makes concurrency so hard?", "source_text": "What makes concurrency so hard? \u2022 Buttondown\n\nComputer Things\n\nApril 16, 2024\n\n# What makes concurrency so hard?\n\n## Is it something about human brains, or something about the problem domain?\n\nA lot of my formal specification projects involve concurrent or distributed\nsystem. That's in the sweet spot of \"difficult to get right\" and \"severe costs\nto getting it wrong\" that leads to people spending time and money on writing\nspecifications. Given its relevance to my job, I spend an awful lot of time\nthinking about the nature of concurrency.\n\nAs the old joke goes, concurrency one of the two hardest things in computer\nscience. There are lots of \"accidental\" reasons why: it's hard to test, it's\nnot composable, bugs can stay latent for a long time, etc. Is there anything\nthat makes it essentially hard? Something that makes concurrent software, by\nits very nature, more difficult to write than synchronous software?\n\nThe reason I hear most often is that humans think linearly, not concurrently,\nso are ill-equipped to reason about race conditions. I disagree: in my\nexperience, humans are very good at concurrent reasoning. We do concurrent\nreasoning every time we drive a car!\n\nMore generally, some studies find that if you frame concurrent systems in\nhuman terms (\"meatspace modeling\"), people get quite good at finding the race\nconditions. So while concurrency might be difficult to reason about, I don't\nthink it's because of a fault in our brains.\n\nIn my opinion, a better basis is state space explosion. Concurrency is hard\nbecause concurrent systems can be in a lot of different possible states, and\nthe number of states grows much faster than anyone is prepared for.\n\n### Behaviors, Interleavings, and Nondeterminism\n\nTake agents^1 {1, 2, ... n}, which each executes a linear sequence of steps.\nDifferent agents may have different algorithms. Think writing or reading from\na queue, incrementing a counter, anything like that. The first process takes\np1 atomic steps to complete, the second p2, etc. Agents complete their\nprograms strictly linearly, but another process can interleave after every\natomic step. If we have algorithms A1A2 and B1B2, they could execute as\nA1B1A2B2 or A1B1B2A2, but not A1B2B1A2.\n\nUnder those conditions, here's an equation for how many possible orderings of\nexecution (behaviors) can happen:\n\n    \n    \n    (p1+p2+...)! ------------ p1!*p2!*...\n\nIf we have three agents all executing the same 2-step algorithm, that's 6!/8 =\n90 distinct behaviors. Each step in the sequence can potentially lead to a\ndistinct state, so there's 6*90=540 maximum distinct states (MDS). Any one of\nthose can potentially be the \"buggy\" one.\n\nIn most cases, the actual state space will be significantly smaller than the\nMDS, as different behaviors will cross the same states. Counterbalancing this\nis just how fast this formula grows. Three 3-step agents gives us 1700\npossible behaviors (15K MDS), four 2-step agents instead have 2500 (20K MDS).\nAnd this is all without any kind of nondeterminism! If one step in one of the\nagents can nondeterministically do one of three things (send message M1, send\nmessage M2, crash), that triples the number of behaviors and the MDS.\n\nIt's pretty common for complex concurrent systems to have millions or tens of\nmillions of states. Even the toy model I use in this article on model\noptimization has about 8 million distinct states (though with some work you\ncan get it much lower). I mostly think about state spaces in terms of\nperformance because large state spaces take a lot longer to model-check. But\nit's also why concurrency is essentially hard to reason about. If my\ntheoretical MDS is two million states, my practical state space is just 1%\nthat size, and my human brain can reason through 99.9% of the remaining\nstates... that still leaves 20 edge cases I missed.\n\n### Shrinking the State Space\n\nHere's a heuristic I use a lot:\n\n> All means of making concurrency 'easier' are concerned first and foremost\n> with managing the state space.\n\nIt's not 100% true (no heuristic is) but it's like 60% true, and that's good\nenough. State spaces grow quickly and bigger state spaces cause problems. If\nwe want to make maintainable concurrent systems, we need to start by shrinking\nthe space.\n\nLike look at threads. Threads share memory, and the thread scheduler has a lot\nof freedom to suspend threads. So you have lots of steps (potentially one per\nline of code)^2 and any interleaving can lead to a distinct state. I can use\nprogramming constructs like mutexes and barriers to \"prune\" the state space\nand give me the behaviors I want, but given how big the state space can be, I\nhave to do a lot of pruning to get the right behaviors. I can make mistakes in\nimplementation, \"misshape\" the space (like by adding a deadlock), or not\nnotice a buggy state I need to remove. Threads are very error prone.\n\nSubscribe\n\nI could instead switch to memory-isolated processes. The scheduler can still\nschedule whatever interleavings it wants but the processes can't muck with\neach other's internal memory. Internally a process is still executing\nA1A2A3A4, but if the only steps that involve external resources (or\ninterprocess communication) are A2 and A4, then we only need to think about\nhow A2A4 affects the state space. Three four-step agents have 35k\ninterleavings, three two-step agents have only 90. That's a big improvement!\n\nWhat else can we do? Low-level atomic instructions do more in a single step,\nso there's no room for interleaving. Database transactions take a lot of\nphysical time but represent only one logical time step. Data mutations create\nnew steps, which immutable data structures avoid by definition.\n\nLanguages have constructs to better prune the resulting state space: go's\nchannels, promises/futures/async-await, nurseries, etc. I think you can also\ntreat promises as a way of forcing \"noninterleaving\": wait until a future is\nready to execute in full (or to the next yield point) and before execution.\nPlease don't quote me on this.\n\nI think CRDTs reduce state space by permitting interleavings, but arranging\nthings so that external changes are commutative: A1B1 and B1A1 lead to the\nsame final result, so there are not distinct states.\n\nAgain, this is all a very rough heuristic.\n\n### Limits\n\nTo a first-order approximation, smaller state space == good. But this doesn't\naccount for the topology of the space: some spaces are gnarlier than others.\nOne that has lots of different cycles will be harder to work with than one\nthat's acyclic.^3 Different forms of nondeterminism also matter: \"the agent\ncontinues or restarts\" leads to more complex behavior than \"the writer sends\nmessage M1 or M2.\" These are both places where \"humans are bad at reasoning\nabout concurrency\" appears again. We often work through concurrency by reduces\ngroups of states to \"state equivalence classes\" that all have the same\nproperties. complicated state spaces have more equivalence classes to work\nthrough.\n\n(Another way \"humans are bad at reasoning about concurrency\" can be a real\nthing: we might not notice that something is nonatomic.)\n\nSome high-level paradigms can lead to particular state space topologies that\nhave fewer interleavings or ones that have more equivalent states. I've heard\npeople claim that fork-join and pipe-filter are especially easy for humans to\nreason about, which I take to mean \"doesn't lead to a gnarly state space\".\nMaybe also event-loops? Where does the actor model fit into all of this?\n\nAnother limit: there's a difference between buggy states and buggy behaviors.\nSome behaviors can go entirely through safe states but still cause an bug like\n\"never reaches consistency\". This is called a \"liveness bug\", and I talk more\nabout them here. Liveness bugs are much harder to reason about.\n\nOkay, that's enough philosophical rambling about concurrency. Concurrency is\nhard, don't feel bad if you struggle with it, it's not you, it's\ncombinatorics.\n\n### Video Appearance\n\nI was on David Girard's Technology and Friends talking about TLA+. Check it\nout here!\n\n  1. An agent is any concurrent actor, like a thread, a process, a future, a human, etc. Concurrency literature uses \"processes\" a lot but that clashes with OS processes. \u21a9\n\n  2. Or more! If writes are nonatomic, you can get data races, where two threads write to different bits in a byte at the same time. Then a variable assignment can be multiple steps in one line. \u21a9\n\n  3. Is there any research correlating unusual subgraphs in state spaces with probable bugs? \u21a9\n\nIf you're reading this on the web, you can subscribe here. Updates are once a\nweek. My main website is here.\n\nDon't miss what's next. Subscribe to Computer Things:\n\nSubscribe\n\nBrought to you by Buttondown, the easiest way to start and grow your\nnewsletter.\n\n", "frontpage": true}
