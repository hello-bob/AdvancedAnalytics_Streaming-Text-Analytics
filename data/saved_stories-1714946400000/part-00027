{"aid": "40263940", "title": "Why governments need to take a more active role in regulating AI", "url": "https://themarkup.org/hello-world/2024/04/27/why-governments-need-to-take-a-more-active-role-in-regulating-ai", "domain": "themarkup.org", "votes": 1, "user": "atg_abhishek", "posted_at": "2024-05-05 11:11:10", "comments": 0, "source_title": "Why Governments Need To Take a More Active Role in Regulating AI \u2013 The Markup", "source_text": "Why Governments Need To Take a More Active Role in Regulating AI \u2013 The Markup\n\nSkip navigation\n\nMenu The Markup Donate\n\n  * About Us\n  * Donate\n\nChallenging technology to serve the public good.\n\n# Why Governments Need To Take a More Active Role in Regulating AI\n\nBy Ross Teixeira\n\nApril 27, 2024 08:00 ET\n\nViewable online at https://themarkup.org/hello-world/2024/04/27/why-\ngovernments-need-to-take-a-more-active-role-in-regulating-ai\n\nHello World\n\n# Why Governments Need To Take a More Active Role in Regulating AI\n\nChristelle Tessono talks with the Markup about how AI systems are still mostly\nmonitored by the companies that build them By Ross Teixeira\n\nApril 27, 2024 08:00 ET\n\nIn October 2023, US President Joe Biden signed America\u2019s first executive order\non AI (Credit: Demetrius Freeman/The Washington Post via Getty Images)\n\n### Share This Article\n\nCopy Link Copied! Republish\n\nHi everyone,\n\nRoss here\u2014I\u2019m an Investigative Data Journalist at The Markup. We publish a lot\nof words telling you what AI can (and often shouldn\u2019t) do, but how can public\npolicy keep AI in check?\n\nGovernments are ramping up their role in keeping AI algorithms accountable and\nlimiting their harms. We covered President Joe Biden\u2019s AI order back in\nOctober; last month, a new executive order required all federal agencies to\ndesignate a Chief AI Officer, and by December agencies must implement AI\nsafeguards and publish transparency reports on federal AI deployments. Many\nindividual states like New Jersey, Colorado, Massachusetts, and\nCalifornia\u2014which The Markup will be looking at closely in the future as we\njoin forces with CalMatters\u2014are proposing and passing legislation regulating\nAI to combat discrimination.\n\nThe EU recently passed the world\u2019s first comprehensive regulation on AI\nsystems, and Canada\u2019s AI safety bill has been working its way through\nParliament since 2022. Canada also announced $2.4 billion CAD earlier this\nmonth towards public computing resources for AI models, while the U.S.\nrecently launched its own pilot in January (projected to cost $2.6 billion to\nbe fully operational).\n\nTo get more insight on upcoming AI policy, I reached out to my good friend\nChristelle Tessono, a graduate student at the University of Toronto\u2019s Faculty\nof Information and a policy and research assistant at the Dais, a public\npolicy and leadership think tank at Toronto Metropolitan University. Her work\nfocuses on tackling the relationship between racial inequality and digital\ntechnology, with special attention to AI deployments.\n\nCaption: Credit:Christian Diotte, House of Commons Photo Services HOC-CDC 2020\n\nRoss: What are the main things you\u2019re working on?\n\nChristelle: I\u2019ve been focusing for the past couple of years on AI governance\nin Canada, discussed in public policy processes. I\u2019ve also acquired expertise\non facial recognition technology, gig work in the Canadian context, as well as\nlooking at social media platform governance.\n\nRoss: So I know that there\u2019s a bunch of new legislation being proposed in\nCanada and the US as well, about regulating AI. And one of the big issues,\nsomething you mentioned, is how do you even define AI? So, to you, how should\nwe define AI?\n\nChristelle: I define AI as the set of computational tools used to process\nlarge amounts of data to identify patterns, make inferences, and generate\nassessments and recommendations.\n\nRoss: Tell me what\u2019s happening in AI legislation in the policy space in Canada\nand the U.S., or anywhere else in the world that you\u2019re tracking now.\n\nNews\n\n### NYC\u2019s AI Chatbot Tells Businesses to Break the Law\n\nThe Microsoft-powered bot says bosses can take workers\u2019 tips and that\nlandlords can discriminate based on source of income\n\nMarch 29, 2024 06:00 ET\n\nChristelle: I\u2019ve been tracking what\u2019s happening in the US, Canada, and Europe.\nBut let\u2019s talk about Canada, because I feel like people at the international\nlevel don\u2019t really know what\u2019s happening. Canada was the first country to\ndevelop a national AI strategy back in 2017, the Pan Canadian Artificial\nIntelligence Strategy, which in recent years has received over $125 million in\nfunding to conduct research and drive AI adoption in the country. However, we\nhave been very slow about developing enforceable regulation to address the\nharms and risks caused by AI systems.\n\nCanada introduced the AI and Data Act (AIDA) back in June 2022 to regulate the\nuse of AI systems in the private sector. But it never received public\nconsultation, has an overly narrow definition for systems in scope, and\ndoesn\u2019t have prohibitions on certain types of AI systems like the EU AI Act.\n\nRoss: Can you talk more about the difference between different AI\nlegislations?\n\nChristelle: The EU has a \u201crisk based\u201d framework, meaning that they\u2019ve taken\nthe time to outline the different types of systems that would fall under\ndifferent types of risks, such as \u201chigh risk\u201d, \u201climited risk\u201d, \u201cminimal risk\u201d,\nand \u201cno risk\u201d in the law itself. Whereas here in Canada, the legislation\nstates that this will apply to \u201chigh impact\u201d systems, but it remains unclear\nwhether the government will determine if a given product is considered \u201chigh\nimpact\u201d or if it is at the discretion of the developer. So in short, the\nCanadian proposed framework is an empty shell.\n\nRoss: And do you know which framework the U.S. uses in its proposed law?\n\nChristelle: The U.S. approach is very decentralized, with multiple initiatives\nacross different agencies. Multiple bills have been introduced in the past,\nbut none of them have gotten enough traction to be considered the singular\napproach that the U.S. will undertake at the federal level. At the state level\nthere are a lot of initiatives, some have become law, such as the Artificial\nIntelligence Video Interview Act in Illinois that regulates AI in employment\ncontexts. There are several that are also slowly making their way through\nlegislative houses, such as algorithmic discrimination acts in Oklahoma\n(HB3835) and Washington (HB 1951), so I would say it\u2019s a very decentralized\napproach. Then there is the AI Bill of Rights, which is a guiding document \u2013\nso not enforceable.\n\nRoss: What are some good properties of an AI system that you think systems\nthat are deployed out in the world should have?\n\nChristelle: When I think about a system, about an AI system, I don\u2019t only\nthink about the physical machinery, the data, the computing. I think about the\ncontext in which it is designed, developed and deployed.\n\nFirst, an AI system should have a clear accountability framework. That is, do\nwe know who\u2019s responsible for what? And how can people sort of complain or\nalert authorities that there is a problem? If to me there\u2019s no accountability,\nthen the AI system is simply doomed to fail.\n\nThen there\u2019s transparency. As a researcher, I\u2019m curious about learning how\nthese products are not only developed, but the procurement process. Who\ndecided to make the call for offers? How many people were provided with mock-\nups of the product? What was the decision-making that led to this choice? Why\nare we using this specific product?\n\nI [also] think about functionality: does the system work? Can it even achieve\nits intended goal? If there\u2019s no match between the task and the capabilities\nof the system, then the system shouldn\u2019t be operating at all. That is the case\nwith many facial recognition systems used for categorizing people or even\nidentifying their emotions. Facial recognition works in verification\ncontexts... but... when you\u2019re using it to try to categorize people and make\npredictions based on them... the functionality piece is not there.\n\nI think a lot of people talk about fairness, like ensuring that the system is\nrobust and not perpetuating bias. That to me is good as a property, but that\u2019s\nnot the first one I think about when it comes to the robustness of a system.\nAs a human person, I cannot be 100% fair. So how can I impose that on a\nsystem? I think it\u2019s better to figure out whether [a system] is able to\ncomplete the desired tasks we wanted to do.\n\nRoss: You\u2019ve talked about premature AI deployments. How can/should an agency\ndecide whether a technology is ready to be deployed in the real world?\n\nNews\n\n### With AI, Anyone Can Be a Victim of Nonconsensual Porn. Can Laws Keep Up?\n\nStates around the country are scrambling to respond to the dramatic rise in\ndeepfakes, a result of little regulation and easy-to-use apps\n\nMarch 11, 2024 06:00 ET\n\nChristelle: First, there should be public consultations as to whether this is\nthe right approach to dealing with a problem that the agency has identified. A\nlot of the issue right now is that we\u2019re seeing technology being deployed\nwithout consultation, without regard for prior consultations on a variety of\nmatters. Is this a real need, or are we using technology to [solve] a problem\nthat doesn\u2019t really need a technological intervention?\n\nThe second thing is functionality and ensuring that the system is robust. What\nare the metrics that the company is using in order to prove that their\ntechnology is up to task? What standardization bodies are they following? What\ntypes of regulations are they respecting? Like, has the company proven that\nthey\u2019re following standards that are followed everywhere else in the world?\n\nThe third piece is, again, accountability. How are we gonna responsibly use\n[this technology]? Are we making sure that we\u2019re not firing people and using\ntechnology as a replacement for labor? Who\u2019s gonna be supervising the\ntechnology?\n\nRoss: On the topic of accountability, what does good accountability look like?\nHow can the public actually raise concerns or fight back against tech that\nthey think is being improperly deployed?\n\nChristelle: The Canadian framework for AI doesn\u2019t have a complaint mechanism.\nAnd that to me is like the first step with regards to accountability. For\nexample, I\u2019m a student, and [let\u2019s say] there\u2019s a problem with one of the\nassignments, I cannot upload it onto the website. I can send an email to the\nprofessor and say \u201cHey, I couldn\u2019t submit the assignment because the online\nplatform doesn\u2019t work,\u201d and so on, that works. And if the professor doesn\u2019t\nrespond, then I can go to the dean or other student representative\norganizations. Like, there are mechanisms for me to alert of an issue.\n\nFor AI systems, it\u2019s hard because you can tell the company \u201cHey, your product\nis faulty,\u201d but what if the product already removed all the money in your bank\naccount because it assumed that you were making fraudulent transactions? Who\ndo you actually complain to? And who\u2019s gonna listen to you and make sure that\nthis is dealt with in a timely fashion and it\u2019s not burdensome on the person\ncomplaining? So in more simple terms, the properties of a good accountability\nframework rely on making it easier for people to complain once something goes\nwrong, and also ensure that there are [complaint] options beyond the\ntechnology.\n\nRoss: How do you design law to make sure that companies will actually abide by\nit?\n\nChristelle: That\u2019s something that we\u2019ve been struggling a lot conceptually in\nCanada. Some companies say that providing criminal penalties for\ncontraventions to the act is a heavy penalty. Others say that a small\nfinancial penalty just incentivizes companies to factor it in the operating\ncosts of the product. So they\u2019re just paying a small bill compared to what\nthey can make if they continue producing and deploying that product.\n\nI think that a way to answer those two challenges while also respecting human\nrights and building trust is having a flexible framework that has a regulator\n[who is] empowered to conduct proactive audits, impose fines, and draft\nregulations.\n\nRoss: How should AI systems be audited?\n\nChristelle: The proposed AI and Data Act in Canada says that if the minister\nsuspects a violation, they can request the company to conduct an audit and\ndeliver them the results. And the company itself has the choice of conducting\nthe audit themselves or procuring the auditing service from a third party that\nthey choose and pay. Deb Raji [a fellow accountability researcher argues]...\nwhen you let a company audit themselves, then you\u2019re not getting... an\nimpartial assessment of the problem.\n\nI believe a way forward is to build specialized auditing teams within\ngovernment that include [a variety of] expertise that understand the socio-\ntechnical implications of AI. From lawyers, technologists, sociologists,\nphilosophers, [and others].\n\nA lot of industry actors are rapidly developing [infrastructure for auditing\nAI]. While this is a positive thing for companies who want to use those\nservices to assess their products, the government shouldn\u2019t rely on them for\naudits. We shouldn\u2019t be outsourcing expertise that we can develop in-house.\n\nRoss: Given that Canada and the US are about to spend billions of dollars on\nAI, can you talk a bit about what that money will be used for, and what you\nsee as any gaps in the funding?\n\nNews\n\n### The Future of Border Patrol: AI Is Always Watching\n\nHuman rights advocates warn of algorithmic bias, legal violations, and other\ndire consequences of relying on AI to monitor the border\n\nMarch 22, 2024 11:00 ET\n\nChristelle: We need more money for regulatory infrastructure, and I really\nemphasize regulatory infrastructure as a term, because how can we be able to\naudit or even develop guidelines on how to use systems if we don\u2019t have public\nservants thinking about these things. We shouldn\u2019t let industry dictate how\ntechnologies are used, when they\u2019re used, and whether they should be used. I\nthink this is a responsibility that the government needs to take on.\n\n[There is] a meager $5.1 million [CAD] for the office of the AI and data\ncommissioner of the country.\n\nThe office of the Privacy Commissioner of Canada has five times that budget.\nSo $5 million [CAD] is nothing. If you have $2 billion [CAD] for computing\ninfrastructure, who\u2019s gonna regulate it? We need money for that.\n\nThere\u2019s [also] $50 million [CAD] for upskilling and \u201ctraining\u201d people who are\nimpacted by AI. The government didn\u2019t give too much detail, but they alluded\nthat this would be for, for example, content creators, artists, creative\nartists who might be impacted by AI. They specifically use the word\n\u201ctraining\u201d, which is very interesting because creative artists, creative\nworkers, artists, [they] don\u2019t need more skills to use AI, they just want\ntheir intellectual property/copyright to be respected, and to not see their\nwork stolen.\n\nRoss: There was a new bill just introduced in the US by a senator, just by a\nsenator that would require all companies that use AI to disclose any\ncopyrighted works that are included in their training data and to have a\ndatabase of that. Do you have thoughts on this? Is it feasible?\n\nChristelle: It\u2019s an interesting idea, but I don\u2019t know much about copyright\nand whether... just disclosing that you use copyrighted work is enough to\nprevent harms for workers who rely on copyright for their income. When it\ncomes to AI policy, we need to think about these types of interventions as\npart of a broader puzzle.\n\nRoss: Do you have advice for readers to [make their voices heard about AI\npolicy]?\n\nChristelle: I highly encourage people to learn more about how government\noperates, how laws are made, even if it\u2019s at the municipal or state level in\nthe US. I highly encourage you, because a lot of people benefit from the\nmajority not knowing how bills are passed. So I highly encourage you all to do\nthat.\n\nI want people to be excited about finding new ways to deal with issues and\nalso building community. Talk to your neighbors, talk to your friends, talk to\nyour parents.\n\nWant to learn more? Check out the OECD tracker of over 1,000 AI initiatives\nfrom 69 countries and territories. Want to get involved? Learn more about AI\nharms and contact your elected officials in the U.S., Canada, or wherever you\nmight be.\n\nThanks for reading!\n\nRoss Teixeira Investigative Data Journalist The Markup\n\nYour donations power our award-winning reporting and our tools. Together we\ncan do more. Give now.\n\nDonate Now\n\n## You just read\n\nWhy Governments Need To Take a More Active Role in Regulating AI\n\nFrom the series \u2014 Hello World\n\n## Share This Article\n\nCopy Link Copied! Republish\n\n### Credits\n\n  * Ross Teixeira Investigative Data Journalist\n\n### Editing\n\n  * Michael Reilly\n\n### Design and Graphics\n\n  * Gabriel Hongsdusit\n\n### Engagement\n\n  * Maria Puertas\n\nClose\n\nRepublish\n\n## Why Governments Need To Take a More Active Role in Regulating AI\n\nWe\u2019re happy to make this story available to republish for free under the\nconditions of an Attribution\u2013NonCommercial\u2013No Derivatives Creative Commons\nlicense. Please adhere to the following:\n\n  * Notify us: Please email us at republish@themarkup.org to let us know if you\u2019ve republished the story.\n  * Give prominent credit to The Markup and its journalists: Credit our authors at the top of the article and any other byline areas of your publication.\n  * Do not edit the article: The complete, unaltered article text must be published. If you wish to translate the article, please contact us for approval.\n  * Access to the article must remain free: Do not sell access to this article or place it behind a paywall, but you can republish our articles on sites with ads.\n  * Images may not be available for republication: Not all of the imagery used in articles published on our site are licensed under Creative Commons. Some images are from commercial providers who do not allow their images to be republished without permission or payment. If you wish to use any image from our articles, email us at republish@themarkup.org for guidance.\n  * Use the provided HTML to republish this article on your site: Simply copy the HTML that we have provided and publish it as is on your website. The provided HTML snippet includes all text formatting and hyperlinks, the author byline, and credit to The Markup. If the HTML code for The Markup credit image is incompatible with your CMS, let us know if you remove it.\n\nCopy HTML\n\n## The Latest\n\nThe Breakdown\n\n### How Do I Prepare My Phone for a Protest? (Updated 2024)\n\nSimple steps to take before hitting the streets\n\nMay 4, 2024 08:00 ET\n\nStill Loading\n\n### The Affordable Connectivity Program Was a Connectivity Lifeline for\nMillions. Congress Is Letting It Die\n\nMore than half of the House supports a bill to extend funds. But it can\u2019t get\nout of committee\n\nMay 4, 2024 08:00 ET\n\nPrivacy\n\n### Car Tracking Can Enable Domestic Abuse. Turning It Off Is Easier Said Than\nDone\n\nInternet-connected cars allow abusers to track domestic violence survivors\nafter they leave\n\nApril 30, 2024 11:01 ET\n\nReturn to The Markup's homepage\n\nYour contributions help us investigate how technology influences our society.\n\nDonate\n\n  * About Us\n  * Our Donors\n  * Ethics Policy\n  * Events\n  * Board of Directors\n  * Jobs\n  * Team\n  * Have a Tip?\n  * Newsletters\n  * A Letter from the President\n  * Awards\n  * Privacy Policy\n  * Terms of Use\n\nSign up to get the Hello World newsletter in your inbox every Saturday.\n\n", "frontpage": false}
