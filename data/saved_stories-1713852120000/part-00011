{"aid": "40117553", "title": "Learn to use WebSockets with Django by building your own ChatGPT", "url": "https://www.saaspegasus.com/guides/django-websockets-chatgpt-channels-htmx/", "domain": "saaspegasus.com", "votes": 1, "user": "czue", "posted_at": "2024-04-22 18:55:38", "comments": 0, "source_title": "Learn to use Websockets with Django by building your own ChatGPT", "source_text": "Learn to use Websockets with Django by building your own ChatGPT\n\nLearn to use Websockets with Django by building your own ChatGPT\n\n  1. A whirlwind overview of websockets\n\n    1. Websockets and channels\n    2. Applying websockets to a chatbot application\n  2. Creating your Chat UI View\n  3. Setting up a new Channels route and consumer\n  4. HTMX and Websockets: an overview\n  5. Integrating HTMX Websockets into our Chat UI\n\n    1. Installation and establishing the websocket connection\n    2. Sending messages from the client to the server with ws-send\n    3. How HTMX swaps messages from the server into the UI\n    4. Sending messages from our websocket consumer\n  6. Adding responses from ChatGPT\n\n    1. Sending back the system's message all at once\n    2. Adding streaming responses\n  7. Adding history\n  8. Making it suitable for production with asynchronous support\n  9. Get the code\n\nLearn to use Websockets with Django by building your own ChatGPT\n\nEverything you need to know about websockets to use them in your applications,\nwith Django, channels, and HTMX.\n\nApr 22, 2024\n\nAnyone who has used ChatGPT\u2014which is likely nearly everyone reading\nthis\u2014should be familiar with the experience of watching it write out its\nresponses to you in real time\u2014almost as if the machine on the other end of the\nwire is typing its words to you one at a time.\n\nHave you ever wondered how this works? How do you make a chatbot that types\nlike that in a web browser?\n\nThere are a few ways to do this, but in this post we'll focus on the most\ncommon option: websockets. By the end of this post you should have a good\nunderstanding of what websockets are and how you can use them in your\nprojects. We'll do this by building our own chatbot with Django and OpenAI.\n\nWe'll also use two other libraries:\n\n  1. Channels\u2014the most popular library for using websockets with Django.\n  2. HTMX\u2014the lightweight front end library that works great with Django templates.\n\nFirst we'll cover the basics of websockets, then show how you can use these\ntechnologies to build a real-time, streaming, asynchronous chatbot in your\napplication in just a handful of steps.\n\nHere's the outline:\n\n  * A whirlwind overview of websockets\n\n    * Websockets and channels\n    * Applying websockets to a chatbot application\n  * Creating your Chat UI View\n  * Setting up a new Channels route and consumer\n  * HTMX and Websockets: an overview\n  * Integrating HTMX Websockets into our Chat UI\n\n    * Installation and establishing the websocket connection\n    * Sending messages from the client to the server with ws-send\n    * How HTMX swaps messages from the server into the UI\n    * Sending messages from our websocket consumer\n  * Adding responses from ChatGPT\n\n    * Sending back the system's message all at once\n    * Adding streaming responses\n  * Adding history\n  * Making it suitable for production with asynchronous support\n  * Get the code\n\n## A whirlwind overview of websockets\n\nWebsockets sound fancy, and anyone who has never used them will probably think\nof them as black magic. But, conceptually, they are quite simple!\n\nLet's first talk about how a traditional HTTP request works. In this world you\nhave a client (usually the browser), and a server (usually the...well,\nserver). The client starts off by sending a request to the server (e.g.\n\"please load this webpage\"). The server then processes the request and sends a\nresponse to the client (e.g. \"here it is!\"). Once the content has been\ndelivered to the client, the connection is closed and the request is\ncompleted.\n\nA traditional HTTP Request / Response workflow.\n\nYou can think of this HTTP interaction like writing a letter to a pen pal. You\nsend the letter, it gets delivered, your pen pal reads it, and then they send\nyou a response. Each letter gets one and only one response, and everything\nhappens serially.\n\nIf HTTP is like letter-writing, then websockets is like making a phone call.\nInstead of sending requests and getting responses, the client first opens a\nconnection to the server (i.e. calls someone). Once the connection is\nestablished (call is answered), both the client and server can send messages\nover the channel as much as they want (the conversation). Then, at some point\nthe channel is closed (hanging up) and the conversation is over.\n\nA websocket-based workflow.\n\nThe main difference between normal HTTP requests and web sockets is the \"open\nchannel\" where both client and server can send and receive messages at any\ntime. This makes websockets a good choice when you are expecting multiple\nmessages from the server in a row\u2014for example in a typing UI.\n\nWhat about SSE?\n\nServer-side events, or SSE is another good choice for this type of interface.\nLike websockets, SSE allows sending multiple messages from the server back to\nthe client. The main difference between SSE and websockets is that SSE is one-\ndirection only (server to client). You can also implement a streaming Chat UI\nwith SSE, though the code ends up a bit more complicated.\n\n### Websockets and channels\n\nChannels is the most popular library for using websockets with Django, and\nit's what we'll use in this guide.\n\nWe won't cover channels in depth, but we can quickly map our understanding of\nwebsockets to the core component of channels which is called a consumer.\n\nHere are the key functions available on channels' WebsocketConsumer class,\nwhich we'll be using for this project. You can clearly see how each function\nmaps to the various operations of establishing a connection, sending and\nreceiving messages, and ending the session.\n\n    \n    \n    class WebsocketConsumer: \"\"\" A WebSocket consumer. \"\"\" def connect(self): \"\"\" Called when a new websocket connection is established. \"\"\" def accept(self, subprotocol=None): \"\"\" Accepts a socket request \"\"\" def receive(self, text_data=None, bytes_data=None): \"\"\" Called with a decoded WebSocket frame. \"\"\" def send(self, text_data=None, bytes_data=None, close=False): \"\"\" Sends a reply down the WebSocket. \"\"\" def close(self, code=None): \"\"\" Closes the WebSocket from the server end. \"\"\" def disconnect(self, code): \"\"\" Called when a WebSocket connection is closed. \"\"\"\n\nThe main Consumer methods for handling websockets.\n\nHere's how these functions are represented in our diagram:\n\nHow the above methods are used in our websocket workflow.\n\nUnderstanding this structure will make it clear how to write our consumer code\nlater!\n\n### Applying websockets to a chatbot application\n\nNow that we understand websockets, we can map the concepts onto our chatbot\napplication. Big picture, here\u2019s how it will work.\n\nFirst, we load our chat page in a normal HTTP request, which is served by a\ntraditional Django view. This view returns a page with our UI scaffolding (a\nchat window and an input bar).\n\nOnce the page is loaded, the browser opens a websocket connection to our\nserver. When the connection is established, the client and server can start\nsending each other messages. New messages from the user will be sent from the\nclient to the server, and responses from system will be sent from the server\nto client. Importantly, since the server can now send arbitrary messages to\nthe client at any time, we can implement streaming responses.\n\nIn diagram form, it will look something like this, with one message from the\nserver for every word:\n\nNow let's go through these steps one-by-one to see how it works in practice.\n\n## Creating your Chat UI View\n\nAs we mentioned, the first step is to create a standard Django view to hold\nour chat UI. You can add this view to a new Django app or any existing one.\n\nThis guide won't go through all the steps of setting up a Django project. If\nyou'd like to do that, we recommend first doing the Django tutorial or using a\nstarter project like SaaS Pegasus.\n\nThis guide assumes the view is added to an existing app called \"chat\", though\nyou could also of course create a new app. Either way, we need to add a URL, a\nview and a template:\n\nIn urls.py:\n\n    \n    \n    urlpatterns = [ # your other URLs path(\"chat/\", views.chat, name=\"chat\"), ]\n\nIn views.py:\n\n    \n    \n    @login_required def chat(request): return TemplateResponse(request, \"chat/single_chat.html\")\n\nAnd then in the single_chat.html template:\n\n    \n    \n    {% extends \"web/base.html\" %} {% block body %} <div id=\"chat-wrapper\"> <div id=\"message-list\"> <div class=\"chat-message is-system\"> Hello, what can I help you with today? </div> </div> <form id=\"chat-input-bar\"> <input name=\"message\" type=\"text\" placeholder=\"Type your message...\" <button type=\"submit\">{% translate \"Send\" %}</button> </form> </div> {% endblock %}\n\nWe aren't including styling information in the template to avoid clutter, but\nonce styles are added this will produce a page that looks something like this:\n\nNote that this template doesn't do anything yet, it's just the skin that we'll\nadd our functionality to. So let's do that!\n\nIf you'd like the code for a fully working ChatGPT application you can drop\ndirectly into your Django projects, enter your email address below to get a\nlink to the repository.\n\n## Setting up a new Channels route and consumer\n\nBefore updating the front end, we need to set up our Channels consumer. We'll\nassume you've already got a working Channels project and just show the changes\nto add a new route and consumer.\n\nIf you need to get channels integrated into your application, I recommend\nstarting with the first two parts of the excellent Channels tutorial which\nwill help you create your first routes and consumers and get you to the point\nwhere you can run the code here.\n\nAdding channels endpoints to your application is similar to Django views, just\nwith slightly different conventions. Instead of adding the route to a urls.py\nfile, we put it in routing.py:\n\n    \n    \n    from django.urls import path from . import consumers websocket_urlpatterns = [ # other websocket URLs here path(r\"ws/chatgpt-demo/\", consumers.ChatConsumer.as_asgi(), name=\"chatgpt_demo\"), ]\n\nAnd then instead of creating a view, we create a consumer.\n\nIn consumers.py:\n\n    \n    \n    from channels.generic.websocket import WebsocketConsumer class ChatConsumer(WebsocketConsumer): def receive(self, text_data): # our webhook handling code will go here. print(text_data)\n\nThis gives us a basic websocket endpoint that prints out anything we send it.\n\nWe finally need to add our websocket_urls to our root channels router.\n\nIn asgi.py:\n\n    \n    \n    from chat.routing import websocket_urlpatterns application = ProtocolTypeRouter( { \"http\": django_asgi_app, \"websocket\": AllowedHostsOriginValidator( AuthMiddlewareStack( URLRouter( websocket_url_patterns, # add this alongside other routes. ) ) ), } )\n\nFor more on channels routing see this page.\n\n## HTMX and Websockets: an overview\n\nNext up, we want to wire our front end template to talk to our new consumer.\nWe'll use HTMX and the websockets extension for this.\n\nFirst, we need to understand a bit about HTMX and websockets work together.\n\nThis guide doesn't assume any prior knowledge of HTMX, but we cover the core\nconcepts quite quickly. For a longer treatment of using HTMX and Django, see\n\"Django, HTMX and Alpine.js: Modern websites, JavaScript optional\".\n\nThe core workflow of traditional HTMX-based UIs is:\n\n  1. The client makes an HTTP request.\n  2. The server sends back some HTML.\n  3. The client swaps it somewhere onto the page.\n\nWith websockets, things work similarly, except we can no longer guarantee that\nthings happen in the normal call-and-response HTTP way. Instead, the client\nneeds to be able to receive messages at any time.\n\nThis means that the above steps get split out into the websocket primitives of\nsending and receiving. So the HTTP requests in step 1 get replaced by the\nwebsocket send event (called ws-send in htmx). And steps 2 and 3 happen\ntogether every time a message is received. We also have an additional initial\nstep of establishing the websocket connection before anything else can happen\n(ws-connect).\n\nHow the HTMX events map to our websocket workflow.\n\nConfused? Don't worry, it'll be clearer with an example.\n\n## Integrating HTMX Websockets into our Chat UI\n\nBig picture, we want to do three things:\n\n  1. Establish a connection to a websocket endpoint.\n  2. Send messages to that endpoint when we have new chat messages.\n  3. Display messages from that endpoint when we receive responses.\n\nWe'll go through these step-by-step.\n\n### Installation and establishing the websocket connection\n\nFirst we have to add HTMX and the websocket extension to our page head.\n\n    \n    \n    <head> <!-- other stuff here --> <script src=\"https://unpkg.com/htmx.org@1.9.11\"></script> <script src=\"https://unpkg.com/htmx.org/dist/ext/ws.js\"></script> </head>\n\nNext we'll use the HTMX markup to connect to our websocket endpoint, by making\nthe following change to our template:\n\n    \n    \n    -<div class=\"chat-wrapper\"> +<div class=\"chat-wrapper\" hx-ext=\"ws\" ws-connect=\"/ws/chatgpt-demo/\"> <div id=\"message-list\" class=\"chat-pane\"> <div class=\"chat-message-system\">\n\nHere we have added two things to our chat-wrapper component. The hx-ext=\"ws\"\ntells the extension that we are using websockets for this component, and the\nws-connect=\"/ws/chatgpt-demo/\" tells it where to connect. This URL should\nmatch the route we set up in routing.py above. Now when we load the page, HTMX\nwill open a connection to this websocket\u2014which will be handled by our consumer\nbackend.\n\nNow we're ready to send and receive messages over the websocket.\n\n### Sending messages from the client to the server with ws-send\n\nWith the connection established, we can move on to sending messages.\nThankfully, HTMX makes this ridiculously easy. All we have to do is add the\nws-send attribute to the form where we want our messages to go. Here's the\nupdated code for our chat input bar:\n\n    \n    \n    -<form class=\"chat-input-bar\"> +<form class=\"chat-input-bar\" ws-send> <input name=\"message\" type=\"text\" placeholder=\"Type your message...\" > <button type=\"submit\">Send<button> </form>\n\nWith the addition of those seven characters, our form submissions will now\nsend the messages over the websocket to our consumer backend. HTMX will\nautomatically serialize these messages as JSON\u2014so our form input of \"message\"\ngets turned into a JSON structure that looks like:\n\n    \n    \n    { \"message\": \"our message\" }\n\nWe can then parse and process these incoming messages in our consumer code:\n\n    \n    \n    import json class ChatConsumer(WebsocketConsumer): def receive(self, text_data): # our webhook handling code goes here text_data_json = json.loads(text_data) message_text = text_data_json[\"message\"] # do something with the user's message\n\nNow our messages are successfully being sent to our chat backend. But how do\nwe respond?\n\nBefore we get to that, we need to understand how the front end will handle our\nresponses.\n\n### How HTMX swaps messages from the server into the UI\n\nRecall that the core workflow we will use when pushing messages back to the UI\nis:\n\n  1. Send across some HTML.\n  2. Swap it somewhere into our page.\n\nLet's first look at how the swapping logic is handled. Recall our original\ntemplate, which has a chat wrapper, and a list of messages containing our\noriginal system greeting.\n\nWhat we want to do is append new messages to the message list\u2014so they show up\nbelow like this:\n\nIn other words we want to insert the messages into the template where the\ncomment is below:\n\n    \n    \n    <div class=\"chat-wrapper\" hx-ext=\"ws\" ws-connect=\"/ws/chatgpt-demo/\"> <div id=\"message-list\" class=\"chat-pane\"> <div class=\"chat-message is-system\"> Hello, what can I help you with today? </div> <!-- we want to add our new messages here --> </div> </div>\n\nThe way we can do this from our websocket events is by using the same\nmarkup/workflow as HTMX's out-of-band swaps\u2014which is just a fancy way of\nsaying \"swap the content into the page element with the same ID\".\n\nHere's the HTML we want to send back:\n\n    \n    \n    <div id=\"message-list\" hx-swap-oob=\"beforeend\"> <div class=\"chat-message\"> <!-- our user's input message --> </div> </div>\n\nLet's see how this works.\n\nThe first thing to note is that we're sending back an element with an id of\n\"message-list\". This tells HTMX to swap our response into the element with the\nID of \"message-list\"\u2014i.e. the div we want to add our messages to.\n\nThe other thing we need to tell HTMX is where in the div they should go. This\nis handled by the special attribute called hx-swap-oob. The hx-swap-oob\nattribute can take any valid hx-swap value, which tells HTMX how to swap the\ncontent in, e.g. by replacing the whole element, putting it at the beginning,\nthe end, and so on. In this case we want to append our message to the end of\nthe message-list, so we tell HTMX to swap it before the end of the element\nwith a value of \"beforeend\".\n\nOnce HTMX swaps the HTML into the page, the final output will look like this:\n\n    \n    \n    <div class=\"chat-wrapper\" hx-ext=\"ws\" ws-connect=\"/ws/chatgpt-demo/\"> <div id=\"message-list\" class=\"chat-pane\"> <div class=\"chat-message is-system\"> Hello, what can I help you with today? </div> <div class=\"chat-message\"> write me a haiku </div> </div> </div>\n\nCool!\n\nNow we're ready to update our consumer backend to send these messages. Let's\nsee how that works.\n\n### Sending messages from our websocket consumer\n\nThankfully, sending messages over our websocket is basically as simple as a\nfunction call\u2014specifically consumer.send().\n\nFirst we'll update our consumer to show the user's own message back to them.\nHere's the updated code of our receive function, which uses Django's\nrender_to_string function to render a template to a string and then sends the\nresult over the websocket channel:\n\n    \n    \n    class ChatConsumer(WebsocketConsumer): def receive(self, text_data): text_data_json = json.loads(text_data) message_text = text_data_json[\"message\"] # show user's message user_message_html = render_to_string( \"chat/ws/chat_message.html\", { \"message_text\": message_text, \"is_system\": False, }, ) self.send(text_data=user_message_html)\n\nAnd the updated contents of our user message template, where we've added our\ncontext variables.\n\n    \n    \n    <div id=\"message-list\" hx-swap-oob=\"beforeend\"> <div class=\"chat-message{% if is_system %} is-system{% endif %}\"> {{ message_text }} </div> </div>\n\nThe chat/ws/chat_message.html template. System (AI) messages will have an\nadditional \"is-system\" class that we can use for styling them differently from\nthe user's messages.\n\n## Adding responses from ChatGPT\n\nOk, we're getting close. We have our Chat UI, our websocket connection, and we\nhave sent and displayed the user's input message. Now for the main\nevent\u2014shipping the message to our language model and sending back the\nresponse. Let's dive into how that works.\n\n### Sending back the system's message all at once\n\nBefore we get into streaming, let's start by sending back the system's\nresponse as a single message.\n\nFirst we have to call OpenAI:\n\n    \n    \n    client = OpenAI(api_key=settings.OPENAI_API_KEY) openai_response = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=[ { \"role\": \"user\", \"content\": message_text, } ], )\n\nThen extract the response:\n\n    \n    \n    system_message = openai_response.choices[0].message.content\n\nAnd finally, send it back over the websocket using the same swap logic and\ntemplate as our user's message:\n\n    \n    \n    system_message_html = render_to_string( \"chat/ws/chat_message.html\", { \"message_text\": system_message, \"is_system\": True, }, ) self.send(text_data=system_message_html)\n\nThis will produce the resulting final message list:\n\n    \n    \n    <div class=\"chat-wrapper\" hx-ext=\"ws\" ws-connect=\"/ws/chatgpt-demo/\"> <div id=\"message-list\" class=\"chat-pane\"> <div class=\"chat-message is-system\"> Hello, what can I help you with today? </div> <div class=\"chat-message\"> write me a haiku </div> <div class=\"chat-message is-system\"> Whispers of the wind... </div> </div> </div>\n\nWhich renders as:\n\nWhat about rendering?\n\nIn order to avoid distracting from the core point of the article, we're\nskipping over some details, including how the text from ChatGPT gets converted\nto html in your template. We recommend using a markdown-to-HTML renderer like\nthis library. This will add support for line breaks, code blocks, lists, and\nother common text formats.\n\n### Adding streaming responses\n\nOk, almost there! Let's add streaming.\n\nNow, instead of getting the whole response form OpenAI and swapping it into\nthe template at the end, we will instead:\n\n  1. Return (and swap in) in an empty system message component immediately.\n  2. Call OpenAI with a streaming option.\n  3. Swap each token in the response stream into our system message contents.\n\nAn important change is that we are now going to swap different server messages\ninto different places in our UI\u2014the messages go into the message list, but the\nstreaming response goes into an individual message element.\n\nFirst, before calling OpenAI at all, we'll send back the system message\ntemplate with an empty body. We can use CSS to style it as \"loading\".\n\nWe also need to generate a unique ID for the message so that we can swap\nfuture messages into this element:\n\n    \n    \n    message_id = uuid.uuid4().hex\n\nWe'll pass this ID in as the message_id variable to our template like so:\n\n    \n    \n    system_message_html = render_to_string( \"chat/ws/chat_message.html\", { \"message_text\": \"\", # we don't send message text since we haven't gotten a response yet \"is_system\": True, \"message_id\": message_id, # and we add an ID }, ) self.send(text_data=system_message_html)\n\nThen we can set the ID on the message div in the template:\n\n    \n    \n    <div id=\"message-list\" hx-swap-oob=\"beforeend\"> <div class=\"chat-message{% if is_system %} is-system{% endif %}\" {% if message_id %}id=\"{{ message_id }}\"{% endif %}> <!-- we added this line --> {{ message_text }} </div> </div>\n\nAfter calling this code, the message list will look like this (choosing a\nvalue of 12345 for message_id):\n\n    \n    \n    <div id=\"message-list\" class=\"chat-pane\"> <div class=\"chat-message is-system\"> Hello, what can I help you with today? </div> <div class=\"chat-message\"> write me a haiku </div> <div class=\"chat-message is-system\" id=\"12345\"> <!-- our empty div where we'll swap the streaming response --> </div> </div>\n\nNext we need to use the stream=True OpenAI API option to stream our response\nback from ChatGPT:\n\n    \n    \n    client = OpenAI(api_key=settings.OPENAI_API_KEY) openai_response = client.chat.completions.create( model=settings.OPENAI_MODEL, messages=[ { \"role\": \"user\", \"content\": message_text, } ], stream=True, # add this line to tell OpenAI to stream. )\n\nWhen we use the streaming API, we also now have to iterate through the\nresponse. For each chunk we get back, we want to swap the content at the end\nof the element with an ID of message_id. We can use the same hx-swap-\noob=\"beforeend\" value, with a different ID, like so:\n\n    \n    \n    for chunk in openai_response: message_chunk = chunk.choices[0].delta.content self.send(text_data=f'<div id=\"{message_id}\" hx-swap-oob=\"beforeend\">{message_chunk}</div>')\n\nNow, the content coming back from ChatGPT will get inserted at the end of the\nrelevant message, finally resulting in our nice streaming experience.\n\nWe did it!\n\n## Adding history\n\nOkay, before we take our victory lap there's one more detail we forgot:\nhistory. Right now every separate message is treated as a standalone\ninteraction with our AI, but we actually want the bot to remember what we've\nasked it so we can ask followup questions in our chat session.\n\nTo do this we'll have to plug into some of the other methods in our consumer\nclass. In particular, whenever a new client connects, we want to initialize an\nempty list of messages:\n\n    \n    \n    class ChatConsumer(WebsocketConsumer): def connect(self): self.messages = [] super().connect()\n\nThen, when we get a new message from the user, we'll append it to our message\nlist in the format expected by OpenAI:\n\n    \n    \n    # in the receive() function self.messages.append( { \"role\": \"user\", \"content\": message_text, } )\n\nNext, we have to update our API call to pass the entire list of messages like\nso:\n\n    \n    \n    openai_response = client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=self.messages, # pass the entire list here stream=True, )\n\nAnd finally, we have to add our system response to the message list once we're\ndone:\n\n    \n    \n    chunks = [] # collect all the chunks in this list for chunk in openai_response: message_chunk = chunk.choices[0].delta.content self.send(text_data=f'<div id=\"{message_id}\" hx-swap-oob=\"beforeend\">{message_chunk}</div>') chunks.append(message_chunk) # and when we're done, join them together when we add the system message in our message list self.messages.append({ \"role\": \"system\", \"content\": \"\".join(chunks) })\n\nNow our bot will have a unique history for every different user session!\n\n## Making it suitable for production with asynchronous support\n\nAre we done? Well, the bot works great, but there's one final problem.\nEverything is still happening synchronously.\n\nWhat does that mean? Without going into a full explanation of asynchronous\nversus synchronous code, it means our calls are blocking. Specifically our\ncalls to OpenAI. This means that while we are waiting for OpenAI to generate\nand stream its responses to us, our application threads are sitting there,\nblocked, and not able to do any other work.\n\nIn practice, it means that our app won't support very many concurrent sessions\nbefore things start getting slow. So, finally, lets look at how to make this\ncode asynchronous.\n\nAsynchronous Django is a big topic, and giving it a proper treatment would\ntake an entire separate article. But\u2014it's not too difficult to make the above\nexample async, so we will quickly run through those changes.\n\nThe first step is to use channels' AsyncWebsocketConsumer class instead of\nWebsocketConsumer:\n\n    \n    \n    -from channels.generic.websocket import WebsocketConsumer +from channels.generic.websocket import AsyncWebsocketConsumer # other code here -class ChatConsumer(WebsocketConsumer): +class ChatConsumer(AsyncWebsocketConsumer):\n\nUsing the AsyncWebsocketConsumer class.\n\nNext we have to declare all our methods\u2014and specifically the receive method\u2014as\nasynchronous, using the async keyword:\n\n    \n    \n    - def receive(self, text_data): + async def receive(self, text_data): # our webhook handling code goes here\n\nMarking our methods as asynchronous.\n\nThen, whenever we send messages to the client we have to await them:\n\n    \n    \n    - self.send(text_data=user_message_html) + await self.send(text_data=user_message_html)\n\nAwaiting our send functions.\n\nWe also need to use the asynchronous OpenAI client, and await our API calls:\n\n    \n    \n    - client = OpenAI(api_key=settings.OPENAI_API_KEY) + openai_response = client.chat.completions.create( - client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY) + openai_response = await client.chat.completions.create( model=\"gpt-3.5-turbo\", messages=self.messages, stream=True, )\n\nMaking OpenAI calls asynchronous.\n\nAnd finally, we need to iterate through the response asynchronously:\n\n    \n    \n    - for chunk in openai_response: + async for chunk in openai_response:\n\nAnd voila! Now our requests won't block and our application should be handle\nway more users at the same time without breaking a sweat.\n\n## Get the code\n\nHooray, we're finally done! We've built an asynchronous, streaming chatbot in\njust a few steps.\n\nIf you would like the complete source code of a working example, that includes\neverything we discussed here and some basic styling with Tailwind CSS, just\nfill in the form below.\n\nAnd if you'd like a version that also includes user-based sessions,\nasynchronous database access, chat history, and loads more functionality\u2014check\nout SaaS Pegasus: the Django boilerplate for AI apps.\n\nI hope this has been helpful, and let me know if you have any questions or\nfeedback by sending a mail to cory@saaspegasus.com.\n\nIf you'd like the code for a fully working ChatGPT application you can drop\ndirectly into your Django projects, enter your email address below to get a\nlink to the repository.\n\n# Subscribe for Updates\n\n## Sign up to get notified when I publish new articles about building SaaS\napplications with Django.\n\nI don't spam and you can unsubscribe anytime.\n\nPowered by EmailOctopus\n\nSaaS Pegasus \u2014 Copyright 2024\n\n", "frontpage": false}
