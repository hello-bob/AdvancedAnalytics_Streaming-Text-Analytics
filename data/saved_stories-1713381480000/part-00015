{"aid": "40061605", "title": "The state of AI for hand-drawn animation inbetweening", "url": "https://yosefk.com/blog/the-state-of-ai-for-hand-drawn-animation-inbetweening.html", "domain": "yosefk.com", "votes": 2, "user": "luu", "posted_at": "2024-04-17 07:35:40", "comments": 0, "source_title": "The state of AI for hand-drawn animation inbetweening", "source_text": "The state of AI for hand-drawn animation inbetweening\n\nBlog Site X Feed\n\n# The state of AI for hand-drawn animation inbetweening\n\nApril 17th, 2024\n\nThere are many potential ways to use AI^1 (and computers in general) for 2D\nanimation. I\u2019m currently interested in a seemingly conservative goal: to\nimprove the productivity of a traditional hand-drawn full animation workflow\nby AI assuming responsibilities similar to those of a human assistant.\n\nAs a \u201csub-goal\u201d of that larger goal, we\u2019ll take a look at two recently\npublished papers on animation \u201cinbetweening\u201d \u2013 the automatic generation of\nintermediate frames between given keyframes. AFAIK these papers represent the\ncurrent state of the art. We\u2019ll see how these papers and a commercial frame\ninterpolation tool perform on some test sequences. We\u2019ll then briefly discuss\nthe future of the broad family of techniques in these papers versus some\nsubstantially different emerging approaches.\n\nThere\u2019s a lot of other relevant research to look into, which I\u2019m trying to do\n- this is just the start. I should say that I\u2019m not \u201can AI guy\u201d - or rather I\nam if you\u2019re building an inference chip, but not if you\u2019re training a neural\nnet. I\u2019m interested in this as a programmer who could incorporate the latest\ntech into an animation program, and as an animator who could use that program.\nBut I\u2019m no expert on this, and so I\u2019ll be very happy to get\nfeedback/suggestions through email or comments.\n\nI\u2019ve been into animation tech since forever, and what\u2019s possible these days is\nexciting. Specifically with inbetweening tech, I think we\u2019re still \u201cnot there\nyet\u201d, and I think you\u2019ll agree after seeing the results below. But we might\nwell get there within a decade, and maybe much sooner.\n\nI think this stuff is very, very interesting! If you think so, too, we should\nget in touch. Doubly so if you want to work on this. I am going to work on\nexactly this!\n\n## Motivation and scope\n\nWhy is it interesting to make AI a 2D animator\u2019s assistant, of all the things\nwe could have it do (text to video, image to video, image style transfer onto\na video, etc.)?\n\n  * An animator is an actor. The motion of a character reflects the implied physical and mental state of that character. If the motion of a character, even one designed by a human, is fully machine-generated, it means that human control over acting is limited; the machine is now the actor, and the human\u2019s influence is limited to \u201cdirecting\u201d at best. It is interesting to develop AI-assisted workflows where the human is still the actor.\n  * To control motion, the animator needs to draw several keyframes (or perhaps edit a machine-generated draft - but with a possibility to erase and redraw it fully.) The range of ways to do \u201ca sad walk\u201d or \u201can angry, surprised head turn\u201d and the range of character traits influencing the acting is too wide for acting to be controlled via cues other than actually drawing the pose.\n  * If a human is to be in control, \u201cmoving line art\u201d is the necessary basis for any innovations in the appearance of the characters. That\u2019s because humans use a \u201clight table\u201d, aka \u201conion skin\u201d, to draw moving characters, where you see several frames overlaid on top of each other (like the frames of a bouncing ball sequence below). And it\u2019s roughly not humanly possible to \u201cread\u201d a light table unless the frames have the sharp edges of line art (believe me, I spent more time trying than I should have.) Any workflow with human animators in control of motion needs to have line art at its basis, even if the final rendered film looks very differently from the traditional line art style.\n\n  * The above gives the human a role similar to a traditional key animator, so it\u2019s natural to give the machine the roles of assistants. It could be that AI can additionally do some of the key animator\u2019s work, so that less keyframes are provided in some cases than you\u2019d have to give a human assistant (and one reason for this could be your ability to quickly get the AI to complete your work in 10-20 possible ways, and choose the best option, which is impractical with a human assistant.) But the basic role of the human as a key animator would remain, and so the first thing to explore is the machine taking over the assistant\u2019s role.\n\nSo I\u2019m not saying that we can\u2019t improve productivity beyond the \u201cmachine as\nthe assistant\u201d arrangement, nor that we must limit ourselves to the\ntraditional appearance of hand-drawn animation. I\u2019m just saying that our\nconservative scope is likely the right starting point, even if our final goals\nare more ambitious - at least as long as we want the human to remain the\nactor.\n\nWhat would the machine do in an assistant\u2019s role? Traditionally, assistants\u2019\njobs include:\n\n  * Inbetweening (drawing frames between the key frames)\n  * Cleanup (taking rough \u201cpencil\u201d sketches and \u201cinking\u201d them)\n  * Coloring (\u201cshould\u201d be trivial with a paint bucket tool, but surprisingly annoying around small gaps in the lines)\n\nOur scope here is narrowed further by focusing exclusively on inbetweening.\nThere\u2019s no deep reason for this beyond having to start somewhere, and\ninbetweening being the most \u201canimation-y\u201d assistant\u2019s job, because it\u2019s about\nmovement. So focusing our search on inbetweening is most likely to give\nresults relevant to animation and not just \u201cstill\u201d line art.\n\nFinally, in this installment, we\u2019re going to focus on papers which call\nthemselves \u201cAI for animation inbetweening\u201d papers. It\u2019s not obvious that any\nrelevant \u201ckiller technique\u201d has to come from a paper focusing on this problem\nexplicitly. We could end up borrowing ideas from papers on video frame\ninterpolation, or video/animation generation not designed for inbetweening,\netc. In fact, I\u2019m looking at some things like this. But again, let\u2019s start\nsomewhere.\n\n## Preamble: testing Runway\n\nBefore looking at papers for the latest ideas, let\u2019s check out Runway Frame\nInterpolation. Together with Stability AI and the CompVis group, Runway\nresearchers were behind Stable Diffusion, and Runway is at the forefront of\ndeploying generative AI for video.\n\nLet\u2019s test frame interpolation on a sneaky cartoony rabbit sequence. It\u2019s good\nas a test sequence because it has both fast/large and slower/smaller movement\n(so both harder and easier parts.) It also has both \u201cflat 2D\u201d body movement\nand \u201c3D\u201d head rotation - one might say too much rotation... But rotation is\ngood to test because it\u2019s a big reason for doing full hand-drawn animation.\nAbsent rotation, you can split your character into \u201ccut-out\u201d parts, and\nanimate it by moving and stretching these parts.\n\nWe throw away every second frame, ask Runway to interpolate the sequence, and\nafter some conversions and a frame rate adjustment (don\u2019t ask), we get\nsomething like this:\n\nThis tool definitely isn\u2019t currently optimized for cartoony motion. Here\u2019s an\nexample inbetween:\n\nNow let\u2019s try a similar sequence with a sneaky me instead of a sneaky rabbit.\nIncidentally, this is one of several styles I\u2019m interested in - something\nbetween live action and Looney Tunes, with this self-portrait taking live\naction maybe 15% towards Looney Tunes:\n\nFrame interpolation looks somewhat better here, but it\u2019s still more morphing\nthan moving from pose to pose:\n\nAn example inbetween:\n\nWhile the Frame Interpolation tool currently doesn\u2019t work for this use case,\nI\u2019d bet that Runway could solve the problem quicker and better than most if\nthey wanted to. Whether there\u2019s a large enough market for this is another\nquestion, and it might depend on the exact definition of \u201cthis.\u201d Personally, I\nbelieve that a lot of good things in life cannot be \u201cmonetized\u201d, a lot of art-\nrelated things are in this unfortunate category, and I\u2019m very prepared to\ninvest time and effort into this without clear, or even any prospects of\nmaking money.\n\nIn any case, we\u2019ve got our test sequences, and we\u2019ve got our motivation to\nlook for better performance in recent papers.\n\n## Raster frame representation\n\nThere\u2019s a lot of work on AI for image processing/computer vision. It\u2019s natural\nto borrow techniques from this deeply researched space and apply them to line\nart represented as raster images.\n\nThere are a few papers doing this; AFAIK the state of the art with this\napproach is currently Improving the Perceptual Quality of 2D Animation\nInterpolation (2022). Their EISAI GitHub repo points to a colab demo and a\nDocker image for running locally, which I did, and things basically Just\nWorked.\n\nThat this can even happen blows my mind. I remember how things worked 25 years\nago, when you rarely had the code published, and people implementing computer\nvision papers would occasionally swear that the paper is outright lying,\nbecause the described algorithms don\u2019t do and couldn\u2019t possibly do what the\npaper says.\n\nThe sequence below shows just inbetweens produced by EISAI. Meaning, frame N\nis produced from the original frames N-1 and N+1; there\u2019s not a single\noriginal frame here. So this sequence isn\u2019t directly comparable to Runway\u2019s\noutput.\n\nI couldn\u2019t quite produce the same output with Runway as with the papers (don\u2019t\nask.) If you care, this sequence is closer to being comparable to Runway\u2019s, if\nnot fully apples to apples:\n\nIf you look at individual inbetweens, you\u2019ll see that EISAI and Runway have\nsimilar difficulties - big changes between frames, occlusion and deformation,\nand both do their best and worst in about the same places. One of the best\ninbetweens by EISAI:\n\nOne of the worst:\n\nThe inbetweens are produced by forward-warping based on bidirectional flow\nestimation. \u201cFlow estimation\u201d means computing, per pixel or region in the\nfirst keyframe, its most likely corresponding location in the other keyframe -\n\u201cfinding where it went to\u201d in the other image (if you have \u201ctwo images of\nmostly the same thing,\u201d you can hope to find parts from one in the other.)\n\u201cWarping\u201d means transforming pixel data - for example, scaling, translating\nand rotating a region. \u201cForward-warping by bidirectional flow estimation\u201d\nmeans taking regions from both keyframes and warping them to put them \u201cwhere\nthey belong\u201d in the inbetween - which is halfway between a region\u2019s position\nin the source image, and the position in the other image that the flow\nestimation says this region corresponds to.\n\nWarping by flow explains the occasional 3-4 arms and legs and 2 heads (it\nwarps a left hand from both input images into two far-away places in the\noutput image, since the flow estimator found a wrong match, instead of\nmatching the hands to each other.) This also explains \u201cempty space\u201d patches of\nvarious sizes in the otherwise flat background.\n\nNotably, warping by flow \u201cgives up\u201d on cases of occlusion up front (I mean\ncases where something is visible in one frame and not in the other due to\nrotation or any other reason.) If your problem formulation is \u201clet\u2019s find\nparts of one image in the other image, and warp each part to the middle\nposition between where it was in the first and where we found it in the\nsecond\u201d - then the correct answer to \u201cwhere did the occluded part move?\u201d is \u201cI\ndon\u2019t know; I can\u2019t track something that isn\u2019t there.\u201d\n\n(Note that the system being an \u201cAI\u201d has no impact on this point. You could\nhave a \u201ctraditional,\u201d \u201chardcoded\u201d system for warping based on optical flow, or\na differentiable one with trainable parameters (\u201cAI\u201d.) Let\u2019s say we believe\nthe trainable one is likely to achieve better results. But training does not\nsidestep the question the parameters of what are being trained, and what the\nmodel can, or can\u2019t possibly do once trained.)\n\nWhen the optical flow matches \u201clarge parts\u201d between images correctly, you\nstill have occasional issues due to both images being warped into the result,\nwith \u201cghosting\u201d of details of fingers or noses or what-not (meaning, you see\ntwo slightly different drawings of a hand at roughly the same place, and you\nsee one drawing through the other, as if that other drawing was a semi-\ntransparent \u201cghost\u201d.) A dumb question coming to my mind is if this could be\nimproved through brute force, by \u201cincreasing the resolution of the image\u201d /\nhaving a \u201chigher-resolution flow estimation,\u201d so you have a larger number of\nsmaller patches capable of representing the deformations of details, because\neach patch is tracked and warped separately.\n\nAn interesting thing in this paper is the use of distance transform to\n\u201ccreate\u201d texture for convolutional neural networks to work with for feature\nextraction. The distance transform replaces every pixel value with the\ndistance from that pixel\u2019s coordinates to the closest black pixel. If you\ninterpret distances as black & white pixel values, this gives \u201ctexture\u201d to\nyour line art in a way. The paper cites \u201cOptical flow based line drawing frame\ninterpolation using distance transform to support inbetweenings\u201d (2019) which\nalso used distance transform for this purpose.\n\nIf you\u2019re dealing with 2D animation and you\u2019re borrowing image\nprocessing/computer vision neural networks (hyperparameters and maybe even\npretrained weights, as this paper does with a few layers of ResNet), you will\nhave the problem of \u201clack of texture\u201d - you have these large flat-color\nregions, and the output of every convolution on each pixel within the region\nis obviously exactly the same. Distance transform gives some texture for the\nconvolutions to \u201crespond\u201d to.\n\nThis amuses me in a \u201cmachine learning inside joke\u201d sort of way. \u201cBut they told\nme that manual feature engineering was over in the era of Deep Learning!\u201d I\nmean, sure, a lot of it is over - you won\u2019t see a paper on \u201cthe next SIFT or\nHOG.\u201d But, apart from the \u201chyperparameters\u201d (a name for, basically, the entire\nnetwork architecture) being manually engineered, and the various manual data\naugmentation and what-not, what\u2019s Kornia, if not \u201ca tool for manual feature\nengineering in a differentiable programming context\u201d? And I\u2019m not implying\nthat there\u2019s anything wrong with it - quite the contrary, my point is that\npeople still do this because it works, or at least makes some things work\nbetter.\n\nBefore we move on to other approaches, let\u2019s check how EISAI does on the\nrabbit sequence. I don\u2019t care for the rabbit sequence; I\u2019m selfishly\ninterested in the me sequence. But since unlike Runway, EISAI was trained on\nanimation data, it seems fair to feed it something more like the training\ndata:\n\nBoth Runway and EISAI do worse on the rabbit, which has more change in hands\nand ears and walks a bit faster. It seems that large movements, deformations\nand rotations affect performance more than \u201csimilarity to training data,\u201d or\nat least similarity in a naive sense.\n\n## Vector frame representation\n\nInstead of treating the input as images, you could work on a vector\nrepresentation of the lines. AFAIK the most recent paper in this category is\nDeep Geometrized Cartoon Line Inbetweening (2023). Their AnimeInbet GitHub\nrepo lets you reproduce the paper\u2019s results. To run on your own data, you need\nto hack the code a bit (at least I didn\u2019t manage without some code changes.)\nMore importantly, you need to vectorize your input data somehow.\n\nThe paper doesn\u2019t come with its own input drawing vectorization system, and\narguably shouldn\u2019t, since vector drawing programs exist, and vectorizing\nraster drawings is a problem in its own right and outside the paper\u2019s scope.\nThe code in the paper has no trouble getting input data in a vector\nrepresentation because their line art dataset is produced from their dataset\nof moving 3D characters, rendered with a \u201ctoon shader\u201d or whatever the thing\nrendering lines instead of shaded surfaces is called. And since the 2D\npoints/lines come from 3D vertices/edges, you\u2019re basically projecting a 3D\nvector representation into a 2D space and it\u2019s still a vector representation.\n\nWhat\u2019s more, this data set provides a kind of ground truth that you don\u2019t get\nfrom 2D animation data sets - namely, detailed correspondence between the\npoints in both input frames and the ground truth inbetween frame. If your\nground truth is a frame from an animated movie, you only know that this frame\nis \u201cthe inbetween you expect between the previous frame and the next.\u201d But\nhere, you know where every 3D vertex ended up in every image!\n\nThis correspondence information is used at training time - and omitted at\ninference time, or it would be cheating. So if you want to feed data into\nAnimeInbet, you only need to vectorize this data into points connected by\nstraight lines, without worrying about vertex correspondence. The paper itself\ncites Virtual Sketching, itself a deep learning based system, as the\nvectorization tool they used for their own experiments in one of the \u201cablation\nstudies\u201d (I know it\u2019s idiomatic scientific language, but can I just say that I\nlove this expression? \u201cPlease don\u2019t contribute to the project during the next\nmonth. We\u2019re performing an ablation study of individual productivity. If the\nstudy proves successful, you shall be ablated from the company by the end of\nthe month.\u201d)\n\nThere are comments in the AnimeInbet repo about issues using Virtual\nSketching; mine was that some lines partially disappeared (could be my fault\nfor not using it properly.) I ended up writing some neanderthal-style image\nprocessing code skeletonizing the raster lines, and then flood-filling the\nskeleton and connecting the points while flood-filling. I\u2019d explain this at\nmore length if it was more than a one-off hack; for what it\u2019s worth, I think\nit\u2019s reasonably correct for present purposes. (My \u201ctesting\u201d is that when I\nrender my vertices and the lines connecting them and eyeball the result, no\nobviously stupid line connecting unrelated things appears, and no big thing\nfrom the input raster image is clearly missing.)\n\nThis hacky \u201cvectorization\u201d code (might need more hacking to actually use) is\nin Animation Papers GitHub repo, together with other code you might use to run\nAnimeInbet on your data.\n\nResults on our test sequences:\n\nThe rabbit is harder for AnimeInbet, similarly to the others. For example, the\nears are completely destroyed by the head turn, as usual:\n\nThe worst and the best inbetweens occur in pretty much the same frames:\n\nVisually notable aspects of AnimeInbet\u2019s output compared to the previous\nsystems we\u2019ve seen:\n\n  * AnimeInbet doesn\u2019t blur lines. It might shred lines on occasion, but you don\u2019t blur vector lines like you blur pixels. (You very much can put a bunch of garbage lines into the output, and AnimeInbet is pretty good at not doing that, but this capability belongs to our next item. Here we\u2019ll just note that raster-based systems didn\u2019t quite \u201clearn\u201d to avoid line blurring, which this system avoids by design.)\n  * AnimeInbet seems quite good at matching small details and avoiding ghosting/copying the same thing twice from both images. This is not something that can salvage bad inbetweens, but it makes good inbetweens better; in the one above, the pants and the hands are examples where small detail is matched better than in the raster systems.\n  * For every part, AnimeInbet either finds a match or removes it from the output. The paper formulates inbetweening as a graph matching problem (where vertices are the nodes and the lines connecting them are edges.) Parts without a match are marked as invisible. This doesn\u2019t \u201csolve\u201d occlusion or rotation, but it tends to keep you from putting stuff into the output that the animator needs to erase and redraw afterwards. This makes good inbetweens marginally better; for bad inbetweens, it makes them \u201cless funny\u201d but probably not much more usable (you get 2 legs instead of 4, but they\u2019re often not the right legs; and you can still get a head with two foreheads as in the bad inbetween above.)\n\nAnimeInbet has a comprehensive evaluation of their system vs other systems\n(EISAI and VFIformer as well as FILM and RIFE, video interpolation rather than\nspecifically animation inbetweening systems.) According to their methodology\n(where they use their own test dataset), their system comes out ahead by a\nlarge margin. In my extremely small-scale and qualitative testing, I\u2019d say\nthat it looks better, too, though perhaps less dramatically.\n\nHere we have deep learning with a model and input data set tailored carefully\nto the problem - something I think you won\u2019t see as often as papers reusing\none or several pretrained networks, and combining them with various\nadaptations to apply to the problem at hand. My emotional reaction to this\napproach appearing to do better than ideas borrowed from \u201cgeneral image/video\nAI research\u201d is mixed.\n\nI like \u201cbeing right\u201d (well, vaguely) about AI not being \u201cgeneral artificial\nintelligence\u201d but a set of techniques that you need to apply carefully to\nbuild a system for your needs, instead of just throwing data into some giant\ngeneral-purpose black box - this is something I like going on about, maybe\nmore than I should given my level of understanding. As a prospective\nuser/implementer looking for \u201cthe next breakthrough paper,\u201d however, it would\nbe better for me if ideas borrowed from \u201cgeneral video research\u201d worked great,\nbecause there\u2019s so many of them compared to the volume of \u201canimation-focused\nresearch.\u201d\n\nI mean, Disney already fired its hand-drawn animation department years ago. If\nthe medium is to be revived (and people even caring about it aren\u2019t getting\nany younger), it\u2019s less likely to happen through direct investment into\nanimation than as a byproduct of other, more profitable things. I guess we\u2019ll\nsee how it goes.\n\n## Applicability of \u201c2D feature matching\u201d techniques\n\nNo future improvement of the techniques in both papers can possibly take care\nof \u201call of inbetweening,\u201d because occlusion and rotation happen a lot, and do\nnot fit these papers\u2019 basic approach of matching 2D features in the input\nframes. And even the best inbetweens aren\u2019t quite usable as is. But they could\nbe used with some editing, and it could be easier to edit them than draw the\nwhole thing from scratch.\n\nAn encouraging observation is that machines struggle with big changes and\npeople struggle with small changes, so they can complement each other well. A\nhuman is better at (and less bored by) drawing an inbetween between two\nkeyframes which look very different than drawing something very close to both\ninput frames and putting every line at juuuuust the right place. If machines\ncan help handle the latter kind of work, even with some editing required,\nthat\u2019s great!\n\nIt\u2019s very interesting to look into approaches that can in fact handle more\nchange between input frames. For example, check out the middle frame below,\ngenerated from the frames on its left and right:\n\nThis is from Explorative Inbetweening of Time and Space (2024); they say the\ncode is coming soon. It does have some problems with occlusion (look at the\nright arm in the middle image.) But it seems to only struggle when showing\nsomething that is occluded in both input frames (for example, the right leg is\nfine, though it\u2019s largely occluded in the image on the left.) This is a big\nimprovement over what we\u2019ve seen above, or right below (this is one frame of\nRunway\u2019s output, where one right leg slowly merges into the left leg, while\nanother right leg is growing):\n\nBut what\u2019s even more impressive - extremely impressive - is that the system\ndecided that the body would go up before going back down between these two\nposes! (Which is why it\u2019s having trouble with the right arm in the first\nplace! A feature matching system wouldn\u2019t have this problem, because it\nwouldn\u2019t realize that in the middle position, the body would go up, and the\nright arm would have to be somewhere. Struggling with things not visible in\neither input keyframe is a good problem to have - it\u2019s evidence of knowing\nthese things exist, which demonstrates quite the capabilities!)\n\nThis system clearly learned a lot about three-dimensional real-world movement\nbehind the 2D images it\u2019s asked to interpolate between. Let\u2019s call approaches\ngoing in this direction \u201c3D motion reconstruction\u201d techniques (and I apologize\nif there\u2019s better, standard terminology / taxonomy; I\u2019d use it if I knew it.)\n\nMy point here, beyond eagerly waiting for the code in this paper, is that\nfeature matching techniques might remain interesting in the long term,\nprecisely because \u201cthey don\u2019t understand what\u2019s going on in the scene.\u201d Sure,\nthey clearly don\u2019t learn \u201chow a figure moves or looks like.\u201d But this gives\nsome hope that what they can do - handling small changes - will work on more\nkinds of inputs. Meaning, a system that \u201clearned human movement\u201d might be less\nuseful for an octopus sequence than a system that \u201clearned to match patches of\npixels, or graphs of points connected by lines.\u201d So falling back on 2D feature\nmatching could remain useful for a long time, even once 3D motion\nreconstruction works great on the kinds of characters it was trained on.\n\n## Conclusion\n\nI think we can agree that animation inbetweening doesn\u2019t quite work at the\nmoment, though it might already be useful for inbetweening small movements,\nwhich is otherwise a painstaking process for a human. I think we can also\nagree that it\u2019s reasonable to hope it will be production-ready quite soon, and\nemerging inbetweening systems which \u201cunderstand and reconstruct movement,\u201d\nbeyond \u201cmatching image features,\u201d are one reason to be hopeful.\n\nIn future installments, I hope to look into more techniques for inbetweening,\nand the closely related question of what animators need to control\ninbetweening, beyond just giving the system two keyframes. Human inbetweeners\ncertainly get more input than pairs of keyframes. This makes me believe that\nit\u2019s not just the plausibility of the inbetweens you produce, but their\ncontrollability which is going to determine \u201cthe winning technique.\u201d\n\nThanks to Dan Luu for reviewing a draft of this post.\n\n  1. I miss the time when they called it machine learning rather than artificial intelligence, and the milder, calmer economic conditions which were a moderating influence on terminology (in the end, whether it\u2019s called ML or AI is an investors\u2019 preference.) But I\u2019m giving up and calling it AI, since at this point calling it ML is more a readability issue than anything else.\u21a9\ufe0e\n\nComments Blog Site X Feed\n\n", "frontpage": false}
