{"aid": "40154501", "title": "Tutorial for streaming processing in deep learning", "url": "https://balacoon.com/blog/streaming_inference/", "domain": "balacoon.com", "votes": 1, "user": "balacoon", "posted_at": "2024-04-25 07:23:36", "comments": 0, "source_title": "Streaming Inference with Convolutional Layers", "source_text": "Streaming Inference with Convolutional Layers - Balacoon\n\n### Balacoon\n\n  * e-mail\n\n# Streaming Inference with Convolutional Layers\n\n10 minute read\n\nIn this post, we explore how to apply convolutional layers to infinitely long\ninputs, specifically focusing on how to process inputs in chunks to minimize\nlatency. For instance, in text-to-speech applications, instead of synthesizing\nan entire sentence at once, we prefer to generate and play back audio in\nsegments. While recurrent or autoregressive networks are inherently causal and\nthus well-suited for streaming processing, convolutional layers present more\nchallenges and require careful handling.\n\n# Conv1dPermalink\n\nFirst, let\u2019s examine a standard convolutional layer. By default, convolutions\nare non-causal, meaning the output at any given time may depend on both past\nand future input values.\n\nNon-causal convolution\n\nTo achieve output of the same size as the input, we pad the input on both\nsides by the receptive_field of the convolution layer, defined as kernel_size\n// 2:\n\n    \n    \n    import torch x = torch.randn(1, 1, 100) # (batch x channels x time) kernel_size = 7 receptive_field = kernel_size // 2 non_causal_conv_layer = torch.nn.Conv1d( 1, # input channels 1, # output channels kernel_size, bias=False, padding=receptive_field, ) y = non_causal_conv_layer(x) assert x.shape == y.shape\n\nFor chunked inference, padding must be applied manually, and the input shifted\nby chunk_size - 2 * receptive_field for each subsequent chunk.\n\nNon-causal convolution in chunks\n\nThis can be implemented as follows:\n\n    \n    \n    non_causal_chunk_conv_layer = torch.nn.Conv1d( 1, # input channels 1, # output channels kernel_size, bias=False, padding=0, # we will do padding manually ) # copy the weights from the original conv layer non_causal_chunk_conv_layer.weight = non_causal_conv_layer.weight # pad the input by receptive field on both sides padded_x = torch.nn.functional.pad(x, (receptive_field, receptive_field)) # run inference in a loop on chunk_size chunk_outputs = [] chunk_size = 20 i = 0 while i < padded_x.size(2) - 2 * receptive_field: chunk = padded_x[:, :, i: i + chunk_size + 2 * receptive_field] chunk_outputs.append( non_causal_chunk_conv_layer(chunk) ) i += chunk_size chunked_y = torch.cat(chunk_outputs, 2) assert chunked_y.shape == y.shape assert torch.all(chunked_y == y)\n\nIf you have a stack of convolutional layers, their receptive fields simply add\nup, but the method remains the same.\n\n# Causal Conv1dPermalink\n\nFor online processing (such as live denoising or voice conversion), latency is\ninfluenced by both chunk_size and the receptive_field of the convolutional\nkernel on the right, also known as lookahead. While chunk size is adjustable,\nthe receptive field is limited by the architecture. To reduce latency, one\nshould aim to design a convolution with an asymmetrical receptive field. In\nthe extreme case, with no lookahead, this results in a causal convolutional\nlayer:\n\nCausal convolution\n\nThis is achieved by asymmetrically padding the convolution, padding only on\nthe left by kernel_size - 1:\n\n    \n    \n    causal_conv_layer = torch.nn.Conv1d( 1, # input channels 1, # output channels kernel_size, bias=False, padding=0, # need to do padding manually for assymetric case ) padded_x = torch.nn.functional.pad(x, (kernel_size - 1, 0)) y = causal_conv_layer(padded_x) assert x.shape == y.shape\n\nInference in chunks does not differ significantly from a regular convolution,\nexcept that there is only one receptive field located on the left of the\ninput.\n\n    \n    \n    # run inference in a loop on chunk_size chunk_outputs = [] chunk_size = 20 i = 0 receptive_field = kernel_size - 1 while i < padded_x.size(2) - receptive_field: chunk = padded_x[:, :, i: i + chunk_size + receptive_field] chunk_outputs.append( causal_conv_layer(chunk) ) i += chunk_size chunked_y = torch.cat(chunk_outputs, 2) assert chunked_y.shape == y.shape assert torch.all(chunked_y == y)\n\n# Transposed Conv1dPermalink\n\nIn audio or image processing, low-dimensional latent representations often\nneed to be upsampled back to samples or pixels. This is achieved through\ntransposed convolution with strides. A detailed explanation of this can be\nfound in a blogpost on the topic. In short, each input point expands into\nmultiple output points. The stride determines the degree of upsampling\nperformed by the transposed convolution, usually set so kernel_size = stride *\n2 to prevent checkboard artifacts. Two neighboring input points contribute to\neach output point. Padding in this case actually reduces the number of output\npoints at the edges, ensuring that stride * len(input) output points are\nproduced.\n\nTransposed convolution with stride\n\n    \n    \n    import torch upsample_rate = 4 kernel_size = upsample_rate * 2 + upsample_rate % 2 padding = (kernel_size - upsample_rate) // 2 transposed_conv_layer = torch.nn.ConvTranspose1d( in_channels=1, out_channels=1, kernel_size=kernel_size, stride=upsample_rate, padding=padding, bias=False, ) y = transposed_conv_layer(x) # (1, 1, 400) print(y.shape) assert y.shape == (x.size(0), x.size(1), x.size(2) * upsample_rate)\n\nRunning transposed convolution in chunks is similar to regular convolution:\nedges of the output are trimmed, input is padded, and inference is performed\non overlapping chunks.\n\nTransposed convolution with stride in chunks\n\nComputing parameters for streaming inference differs from regular convolution:\n\n    \n    \n    # we will run inference with overlap, # which needs to be taken into account # in the slicing extra_samples = (kernel_size - upsample_rate) * 3 // 2 - upsample_rate % 2 # how much extra output samples on the left and right transposed_chunk_conv_layer = torch.nn.ConvTranspose1d( in_channels=1, out_channels=1, kernel_size=kernel_size, stride=upsample_rate, padding=extra_samples, bias=False ) transposed_chunk_conv_layer.weight = transposed_conv_layer.weight chunk_outputs = [] chunk_size = 20 i = 0 # each output contributed by 2 inputs, so overlap is 1 overlap = kernel_size // (2 * upsample_rate) # need to pad so edges are handled correctly, # this padding is taken into account in slicing padded_x = torch.nn.functional.pad(x, (overlap, overlap)) while i < padded_x.size(2) - 2 * overlap: chunk = padded_x[:, :, i: i + chunk_size + 2 * overlap] res = transposed_chunk_conv_layer(chunk) chunk_outputs.append(res) i += chunk_size chunked_y = torch.cat(chunk_outputs, 2) assert chunked_y.shape == y.shape assert torch.all(chunked_y == y)\n\n# Fourier transformPermalink\n\nMany image and audio processing techniques still incorporate elements from\nclassical signal processing. For audio, it\u2019s common to extract a spectrogram\nto downsample the redundant audio signal while preserving the most relevant\ninformation. During training, this can be achieved using torch.stft. When\ndeploying the model, however, there are challenges in tracing this operation\nacross different CPU and GPU precisions. A workaround involves reformulating\nspectrogram extraction as a convolution with strides. This approach is already\nimplemented in nnAudio. Here, the STFT is executed with a precomputed\nconvolution where the kernel size matches the number of FFT points and the\nstride equals the hop size between windows.\n\nExtracting spectrogram looks like this:\n\n    \n    \n    import torch from nnAudio.features.stft import STFT win_length = 1024 downsample_rate = 320 stft = STFT( n_fft=win_length, win_length=win_length, hop_length=downsample_rate, # disabling padding # https://github.com/KinWaiCheuk/nnAudio/blob/9e9a4bad230d175f7ad541309829483f1274a3e5/Installation/nnAudio/features/stft.py#L278 center=False, output_format=\"Magnitude\", pad_mode=\"constant\" ) total_frames = 30 total_samples = win_length + (total_frames - 1) * downsample_rate x = torch.randn(1, total_samples) y = stft(x) assert y.size(2) == total_frames\n\nWhen computing the spectrogram in chunks, the same approach is applied as with\ncausal convolution:\n\n    \n    \n    chunk_size = 5 chunk_size_samples = chunk_size * downsample_rate # overlap between the frames receptive = win_length - downsample_rate start = 0 chunked_y_lst = [] while start <= x.size(1) - chunk_size_samples - receptive: chunk = x[:, start:start + chunk_size_samples + receptive] chunked_y_lst.append(stft(chunk)) start += chunk_size_samples chunked_y = torch.cat(chunked_y_lst, dim=2) assert chunked_y.shape == y.shape assert torch.all(torch.abs(chunked_y - y) < 1e-3)\n\n# Putting it all togetherPermalink\n\nLet\u2019s integrate everything and examine how layers might interact in a typical\naudio-to-audio stack, where audio is first downsampled to a latent\nrepresentation and then upsampled back. The model might look something like\nthis:\n\n    \n    \n    from typing import List import torch from nnAudio.features.stft import STFT def create_conv_stack(kernels: List[int], in_channels: int = 1) -> torch.nn.Sequential: \"\"\" Creates a dummy convolutional stack \"\"\" lst = [] for i, k in enumerate(kernels): ic = in_channels if i == 0 else 1 lst.append( torch.nn.Conv1d( ic, # input channels 1, # output channels k, bias=False, padding=0, ) ) return torch.nn.Sequential(*lst) def create_transpose_conv(upsample_rate: int) -> torch.nn.ConvTranspose1d: \"\"\" Creates dummy transposed convolutional layer that upsamples the input signal by given ratio \"\"\" kernel_size = upsample_rate * 2 + upsample_rate % 2 extra_samples = (kernel_size - upsample_rate) * 3 // 2 - upsample_rate % 2 return torch.nn.ConvTranspose1d( in_channels=1, out_channels=1, kernel_size=kernel_size, stride=upsample_rate, padding=extra_samples, bias=False ) \"\"\" Finally the model, which is a stack of STFT -> conv_stack -> in_conv -> upsampling -> conv_stack -> upsampling -> conv_stack -> out_conv \"\"\" model = torch.nn.Sequential( STFT( n_fft=1024, win_length=1024, hop_length=320, center=False, # disables the padding output_format=\"Magnitude\", pad_mode=\"constant\" ), create_conv_stack([5, 5, 5], 513), create_conv_stack([7]), create_transpose_conv(5), create_conv_stack([3, 5, 11]), create_transpose_conv(64), create_conv_stack([3, 5, 11]), create_conv_stack([7]), )\n\nGiven what we\u2019ve learnt so far, lets define the receptive field for each\nlayer, to understand how much context on the left and on the right our model\nrequires\n\n    \n    \n    # define receptive fields of each layer in the stack # for each layer, specify (left_receptive, right_receptive, resolution) # notice how STFT and transposed conv layers change the resolution receptives_with_resolutions = [ (1024 - 320, 0, 1), # STFT: requires (win_len - hop_size) on the left ((5 - 1) * 3, 0, 320), # conv_stack: 3 causal layers with receptive of (kernel_size - 1) (7 // 2, 7 // 2, 320), # in_conv: non-causal conv layer with symmetric repective of kernel_size // 2 (1, 1, 320), # transposed conv: receptive is defined by overlap = kernel_size // (2 * upsample_rate) == 1 (2 + 4 + 10, 0, 64), # conv_stack: 3 causal layers with varying kernel_size (1, 1, 64), # transposed conv: different upsample rate, but it doesn't affect overlap (2 + 4 + 10, 0, 1), # conv_stack: another 3 causal layers with varying kernel_size (7 // 2, 7 // 2, 1), # out_conv: another non-causal conv layer with symmetric repective ] # bring all to the same resolution (samples) receptives = [(left * res, right * res) for left, right, res in receptives_with_resolutions] # this is our overlap in case of chunked synthesis left = sum([left for left, _ in receptives]) # 6931 # and this is our architectural latency, in case of online processing right = sum([right for _, right in receptives]) # 1347\n\nTo confirm that we calculated the receptive field correctly, we can run the\nmodel on a dummy input and check the input/output dimensionality. The input\nlength should be compatible with model downsampling. In our case, the input\nlength should generate a whole number of outputs for the STFT layer. This is\nthe case if input_length = hop_length * N + win_length.\n\n    \n    \n    input_length = 320 * 50 + 1024 x = torch.zeros(1, input_length) y = model(x) # if we calculated receptive field correctly, # model should strip off left/right receptives assert input_length - left - right == y.size(2)\n\nFrom previous examples, for inference in chunks, we need to shift the input by\nchunk_size, which is the input size without receptive fields. In our case, the\nchunk_size is 320 * N + 1024 - left - right. For N==50, it is 8746. There is a\nproblem, however. We can only shift the input by the stride of the\ndownsampling layer(s), in our case by M * 320. For most architectures, there\nis no way to satisfy both requirements:\n\n  * Shift by chunk_size\n  * Shift by M * downsampling_stride To overcome this issue, we\u2019ll have to drop some extra samples from the output, in order to be able to do inference in chunks:\n\n    \n    \n    chunk_size = input_length - left - right # 8746 extra_to_drop = chunk_size % 320 # 106\n\nNow we are all set to check if inference in chunks works. As before, for a\ndummy input we will run inference on the whole input, and then on chunks of\nthe input and compare the results.\n\n    \n    \n    # some random input x = torch.randn((1, 100000)) # inference on the whole input y = model(x) # running inference on the first chunk y_chunk_1 = model(x[:, :input_length]) # running inference on the second chunk, carefully shifting input start = chunk_size - extra_to_drop y_chunk_2 = model(x[:, start:(start + input_length)]) # now we need to slice off the `extra_to_drop` from both outputs y_chunk_list = [y_chunk_1, y_chunk_2] y_chunk_list = [chunk[:, :, :-extra_to_drop] for chunk in y_chunk_list] y_chunk = torch.cat(y_chunk_list, dim=2) # finally compare the chunked inference to the original one. # we run inference only on two chunks, omitting handling the # padding on the right for simplicity. So the comparison # is done only for beginning of the output. diff = y[:, :, :y_chunk.size(2)] - y_chunk # should be the same assert torch.all(torch.abs(diff) < 1e-3)\n\n# TakeawaysPermalink\n\nIn this post, we delved into how to perform streaming inference (a.k.a.,\ninference in chunks) for models consisting of various convolutional layers.\nThis insight is crucial for building online audio or image processing\napplications. It boils down to carefully computing the receptive field of the\nresulting model and managing overlap between input chunks. When multiple\nlayers are combined, however, additional attention should be paid to feeding\nyour model with input of proper length, which in turn requires more\nsophisticated input/output handling. Hopefully, the explanations and code\nsnippets provided will help you navigate these challenges in your own\narchitectural designs.\n\nUpdated: April 20, 2024\n\nTwitter Facebook LinkedIn\n\n## You May Also Enjoy\n\n## Dissecting BARK\n\n5 minute read\n\nThings started to get stale after the ubiquitous switch to Neural Text-to-\nSpeech. A long-awaited leap forward was introduced thanks to ideas from the\nblossom...\n\n## Zero-shot speech generation benchmark\n\n3 minute read\n\nSynthesizing speech with a speaker identity not seen during training presents\na significant challenge. Traditionally, achieving this required extensive\ntrain...\n\n## \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u043c\u043e\u0432\u0430 \u0432 Balacoon\n\n1 minute read\n\n\u0428\u0432\u0438\u0434\u043a\u0438\u0439, \u0437\u0440\u0443\u0447\u043d\u0438\u0439 \u0442\u0430 \u044f\u043a\u0456\u0441\u043d\u0438\u0439 \u043d\u0435\u0439\u0440\u043e\u043c\u0435\u0440\u0435\u0436\u0435\u0432\u0438\u0439 \u0441\u0438\u043d\u0442\u0435\u0437 \u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u043e\u0433\u043e \u043c\u043e\u0432\u043b\u0435\u043d\u043d\u044f \u0442\u0435\u043f\u0435\u0440\n\u0432 Balacoon. \u0406\u043d\u0442\u0435\u0433\u0440\u0430\u0446\u0456\u044f \u0431\u0456\u0431\u043b\u0456\u043e\u0442\u0435\u043a\u0438 \u0441\u0438\u043d\u0442\u0435\u0437\u0443 \u0449\u0435 \u043d\u0456\u043a\u043e\u043b\u0438 \u043d\u0435 \u0431\u0443\u043b\u0430 \u0442\u0430\u043a\u043e\u044e \u043f\u0440\u043e\u0441\u0442\u043e\u044e:\nPyth...\n\n## Balacoon TTS on-device\n\n3 minute read\n\nNeural text-to-speech brought unprecedented improvements in the naturalness of\nsynthetic speech. But it came with a cost. While parametric and concatenative\n...\n\n  * GitHub\n  * Slack\n  * Feed\n\n\u00a9 2024 Balacoon. Powered by Jekyll & Minimal Mistakes.\n\n", "frontpage": false}
