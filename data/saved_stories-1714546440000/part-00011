{"aid": "40217778", "title": "LMMs as Zero-Shot Dialogue State Tracker Through Function Calling", "url": "https://github.com/facebookresearch/FnCTOD", "domain": "github.com/facebookresearch", "votes": 1, "user": "zerojames", "posted_at": "2024-04-30 23:36:18", "comments": 0, "source_title": "GitHub - facebookresearch/FnCTOD: Official code for the publication \"Large Language Models as Zero-shot Dialogue State Tracker through Function Calling\" https//arxiv.org/abs/2402.10466", "source_text": "GitHub - facebookresearch/FnCTOD: Official code for the publication \"Large\nLanguage Models as Zero-shot Dialogue State Tracker through Function Calling\"\nhttps//arxiv.org/abs/2402.10466\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nfacebookresearch / FnCTOD Public\n\n  * Notifications\n  * Fork 0\n  * Star 3\n\nOfficial code for the publication \"Large Language Models as Zero-shot Dialogue\nState Tracker through Function Calling\" https//arxiv.org/abs/2402.10466\n\n### License\n\nView license\n\n3 stars 0 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# facebookresearch/FnCTOD\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nskiingpacmanInitial commitApr 30, 2024311ca47 \u00b7 Apr 30, 2024Apr 30, 2024\n\n## History\n\n1 Commits  \n  \n### chatbots\n\n|\n\n### chatbots\n\n| Initial commit| Apr 30, 2024  \n  \n### data\n\n|\n\n### data\n\n| Initial commit| Apr 30, 2024  \n  \n### pics\n\n|\n\n### pics\n\n| Initial commit| Apr 30, 2024  \n  \n### sh_folders\n\n|\n\n### sh_folders\n\n| Initial commit| Apr 30, 2024  \n  \n### src\n\n|\n\n### src\n\n| Initial commit| Apr 30, 2024  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| Initial commit| Apr 30, 2024  \n  \n### CODE_OF_CONDUCT.md\n\n|\n\n### CODE_OF_CONDUCT.md\n\n| Initial commit| Apr 30, 2024  \n  \n### CONTRIBUTING.md\n\n|\n\n### CONTRIBUTING.md\n\n| Initial commit| Apr 30, 2024  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Initial commit| Apr 30, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| Initial commit| Apr 30, 2024  \n  \n### create_finetunedata.py\n\n|\n\n### create_finetunedata.py\n\n| Initial commit| Apr 30, 2024  \n  \n### finetune.py\n\n|\n\n### finetune.py\n\n| Initial commit| Apr 30, 2024  \n  \n### merge.py\n\n|\n\n### merge.py\n\n| Initial commit| Apr 30, 2024  \n  \n### requirements.txt\n\n|\n\n### requirements.txt\n\n| Initial commit| Apr 30, 2024  \n  \n## Repository files navigation\n\n# Large Language Models as Zero-shot Dialogue State Tracker through Function\nCalling\n\nThis repository provides the official PyTorch implementation of the following\npaper:\n\n> Large Language Models as Zero-shot Dialogue State Tracker through Function\n> Calling\n\n## Overview\n\nWe introduce a novel approach FnCTOD, to address zero-shot DST with LLMs. Our\nmethod seamlessly integrates DST as a part of the assistant's output during\nchat completion. Specifically, we treat the schema of each task-oriented\ndialogue domain as a specific function, and DST for this domain as the process\nof ``calling'' the corresponding function. We thus instruct LLMs to generate\nfunction calls along with the response in the assistant's output. To achieve\nthis, we convert the domain schema into function specifications, which include\nthe function's description and required arguments, and incorporate them into\nthe system prompt of the LLM. Additionally, we integrate these function calls\ninto the assistant's output within the dialogue context.\n\nZero-shot DST performance comparison among (1) previous domain transfer\napproaches using small models; (2) previous prompting approaches exclusively\nrelying on advanced proprietary LLMs; and (3) our approach, compatible with\nvarious LLMs, empowers various 7B and 13B models for superior performance and\nsets new state-of-the-art with GPT-4.\n\n## Data Preparation\n\nThe detailed instruction for preparing the benchmark dataset MultiWOZ and pre-\ntraining corpora (optional) are provided in the ./data folder.\n\n## Environment Setup\n\n  1. Requires Python 3.8 \u2013 3.11\n  2. Conda Environment Setup: pip install -r requirements.txt\n  3. Environment Variable Configuration: Set the following environment variables for local model inference in each evaluation script:\n\n    \n    \n    export TRANSFORMERS_CACHE='/HOME_PATH/.cache/huggingface/transformers' export HF_HOME='/HOME_PATH/.cache/huggingface' export OPENAI_API_KEY='XXXX'\n\n## In-context Prompting\n\nExecute the following scripts located in the ./sh_folders/ directory to run\ninference with different models.\n\n  1. inference_chatgpt.sh\n  2. inference_fnctod-llama.sh\n  3. inference_oss_models.sh\n\n## Prompt-based Fine-tuning\n\n  1. For each dataset used in the training, including CamRest676, MSE2E, SGD, Taskmaster, and WoZ, first process the data, then format it in our dialogue prompt for training. Here is an example for the SGD dataset:\n\n    \n    \n    cd sh_folders sh processing-sgd.sh sh prompting-sgd.sh\n\n  2. Collect the data from different datasets:\n\n    \n    \n    cd sh_folders sh create_finetunedata.sh\n\n  3. Finetune FnCTOD-Llama2:\n\n    \n    \n    cd sh_folders sh finetune.sh\n\n## Acknowledgements\n\n  1. UBAR: upon which our data processing code is built.\n  2. PPTOD: upon which our evaluation code is built.\n  3. FastChat: we borrowed the chat templates from this repository.\n\nWe thank the authors for their wonderful work.\n\n## License\n\nSee the LICENSE file for details about the license under which this code is\nmade available.\n\n## Citation\n\nIf you find this work useful, please cite our paper:\n\n    \n    \n    @article{li2024large, title={Large Language Models as Zero-shot Dialogue State Tracker through Function Calling}, author={Li, Zekun and Chen, Zhiyu Zoey and Ross, Mike and Huber, Patrick and Moon, Seungwhan and Lin, Zhaojiang and Dong, Xin Luna and Sagar, Adithya and Yan, Xifeng and Crook, Paul A}, journal={arXiv preprint arXiv:2402.10466}, year={2024} }\n\n## About\n\nOfficial code for the publication \"Large Language Models as Zero-shot Dialogue\nState Tracker through Function Calling\" https//arxiv.org/abs/2402.10466\n\n### Resources\n\nReadme\n\n### License\n\nView license\n\n### Code of conduct\n\nCode of conduct\n\n### Security policy\n\nSecurity policy\n\nActivity\n\nCustom properties\n\n### Stars\n\n3 stars\n\n### Watchers\n\n5 watching\n\n### Forks\n\n0 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Contributors 3\n\n  * Leezekun Zekun Li\n  * skiingpacman Paul Crook\n  * facebook-github-bot Facebook Community Bot\n\n## Languages\n\n  * Python 96.3%\n  * Shell 3.7%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
