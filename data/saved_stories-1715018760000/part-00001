{"aid": "40273638", "title": "Counting of wheezing events from lung sounds using deep learning algorithms", "url": "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0294447", "domain": "plos.org", "votes": 1, "user": "PaulHoule", "posted_at": "2024-05-06 12:04:04", "comments": 0, "source_title": "Real-time counting of wheezing events from lung sounds using deep learning algorithms: Implications for disease prediction and early intervention", "source_text": "Real-time counting of wheezing events from lung sounds using deep learning algorithms: Implications for disease prediction and early intervention | PLOS ONE\n\nSkip to main content\n\nAdvertisement\n\n  * plos.org\n  * create account\n  * sign in\n\nBrowse Subject Areas\n\n?\n\nClick through the PLOS taxonomy to find articles in your field.\n\nFor more information about PLOS Subject Areas, click here.\n\n  * 6\n\nSave\n\nTotal Mendeley and Citeulike bookmarks.\n\n  * 0\n\nCitation\n\nPaper's citation count computed by Dimensions.\n\n  * 2,568\n\nView\n\nPLOS views and downloads.\n\n  * 0\n\nShare\n\nSum of Facebook, Twitter, Reddit and Wikipedia activity.\n\nOpen Access\n\nPeer-reviewed\n\nResearch Article\n\n# Real-time counting of wheezing events from lung sounds using deep learning\nalgorithms: Implications for disease prediction and early intervention\n\n  * Sunghoon Im ,\n\nContributed equally to this work with: Sunghoon Im, Taewi Kim\n\nRoles Conceptualization, Data curation, Investigation, Methodology,\nValidation, Writing \u2013 original draft, Writing \u2013 review & editing\n\nAffiliation Department of Mechanical Engineering, Ajou University, Suwon-si,\nGyeonggi-do, Republic of Korea\n\n\u2a2f\n\n  * Taewi Kim ,\n\nContributed equally to this work with: Sunghoon Im, Taewi Kim\n\nRoles Conceptualization, Formal analysis, Investigation, Methodology\n\nAffiliation Department of Mechanical Engineering, Ajou University, Suwon-si,\nGyeonggi-do, Republic of Korea\n\n\u2a2f\n\n  * Choongki Min,\n\nRoles Software, Visualization\n\nAffiliation Waycen, Inc., Seoul, Republic of Korea\n\n\u2a2f\n\n  * Sanghun Kang,\n\nRoles Resources, Software\n\nAffiliation Department of Mechanical Engineering, Ajou University, Suwon-si,\nGyeonggi-do, Republic of Korea\n\n\u2a2f\n\n  * Yeonwook Roh,\n\nRoles Investigation, Methodology\n\nAffiliation Department of Mechanical Engineering, Ajou University, Suwon-si,\nGyeonggi-do, Republic of Korea\n\n\u2a2f\n\n  * Changhwan Kim,\n\nRoles Investigation, Methodology\n\nAffiliation Department of Mechanical Engineering, Ajou University, Suwon-si,\nGyeonggi-do, Republic of Korea\n\n\u2a2f\n\n  * Minho Kim,\n\nRoles Investigation, Methodology\n\nAffiliation Department of Mechanical Engineering, Ajou University, Suwon-si,\nGyeonggi-do, Republic of Korea\n\n\u2a2f\n\n  * Seung Hyun Kim,\n\nRoles Validation\n\nAffiliation Department of Medical Humanities, Korea University College of\nMedicine, Seoul, Republic of Korea\n\n\u2a2f\n\n  * KyungMin Shim,\n\nRoles Investigation\n\nAffiliation Industry-University Cooperation Foundation, Seogyeong University,\nSeoul, Republic of Korea\n\n\u2a2f\n\n  * Je-sung Koh,\n\nRoles Methodology\n\nAffiliation Department of Mechanical Engineering, Ajou University, Suwon-si,\nGyeonggi-do, Republic of Korea\n\n\u2a2f\n\n  * Seungyong Han,\n\nRoles Methodology\n\nAffiliation Department of Mechanical Engineering, Ajou University, Suwon-si,\nGyeonggi-do, Republic of Korea\n\nhttps://orcid.org/0000-0003-2870-4088\n\n\u2a2f\n\n  * JaeWang Lee,\n\nRoles Methodology\n\nAffiliation Department of Biomedical Laboratory Science, College of Health\nScience, Eulji University, Seongnam-si, Gyeonggi-do, Republic of Korea\n\nhttps://orcid.org/0000-0001-6801-7149\n\n\u2a2f\n\n  * Dohyeong Kim ,\n\nRoles Conceptualization, Methodology, Supervision, Writing \u2013 review & editing\n\n* E-mail: dohyeong.kim@utdallas.edu (DK); dskang@ajou.ac.kr (DK); haha0694@gmail.com (SS)\n\nAffiliation University of Texas at Dallas, Richardson, TX, United States of\nAmerica\n\n\u2a2f\n\n  * Daeshik Kang ,\n\nRoles Conceptualization, Funding acquisition, Methodology, Project\nadministration, Supervision\n\n* E-mail: dohyeong.kim@utdallas.edu (DK); dskang@ajou.ac.kr (DK); haha0694@gmail.com (SS)\n\nAffiliation Department of Mechanical Engineering, Ajou University, Suwon-si,\nGyeonggi-do, Republic of Korea\n\nhttps://orcid.org/0000-0002-7490-9079\n\n\u2a2f\n\n  * SungChul Seo\n\nRoles Conceptualization, Funding acquisition, Methodology, Project\nadministration, Supervision\n\n* E-mail: dohyeong.kim@utdallas.edu (DK); dskang@ajou.ac.kr (DK); haha0694@gmail.com (SS)\n\nAffiliation Department of Nano-Chemical, Biological and Environmental\nEngineering, Seogyeong University, Seoul, Republic of Korea\n\n\u2a2f\n\n# Real-time counting of wheezing events from lung sounds using deep learning\nalgorithms: Implications for disease prediction and early intervention\n\n  * Sunghoon Im,\n  * Taewi Kim,\n  * Choongki Min,\n  * Sanghun Kang,\n  * Yeonwook Roh,\n  * Changhwan Kim,\n  * Minho Kim,\n  * Seung Hyun Kim,\n  * KyungMin Shim, ...\n  * Je-sung Koh\n\nx\n\n  * Published: November 20, 2023\n  * https://doi.org/10.1371/journal.pone.0294447\n\n  * Article\n  * Authors\n  * Metrics\n  * Comments\n  * Media Coverage\n  * Peer Review\n\n  * Abstract\n  * Introduction\n  * Methods\n  * Results\n  * Discussion\n  * Conclusion\n  * Supporting information\n  * References\n\n  * Reader Comments\n  * Figures\n\nAccessible Data\n\nSee the data\n\nThis article includes the Accessible Data icon, an experimental feature to\nencourage data sharing and reuse. Find out how research articles qualify for\nthis feature.\n\n## Abstract\n\nThis pioneering study aims to revolutionize self-symptom management and\ntelemedicine-based remote monitoring through the development of a real-time\nwheeze counting algorithm. Leveraging a novel approach that includes the\ndetailed labeling of one breathing cycle into three types: break, normal, and\nwheeze, this study not only identifies abnormal sounds within each breath but\nalso captures comprehensive data on their location, duration, and\nrelationships within entire respiratory cycles, including atypical patterns.\nThis innovative strategy is based on a combination of a one-dimensional\nconvolutional neural network (1D-CNN) and a long short-term memory (LSTM)\nnetwork model, enabling real-time analysis of respiratory sounds. Notably, it\nstands out for its capacity to handle continuous data, distinguishing it from\nconventional lung sound classification algorithms. The study utilizes a\nsubstantial dataset consisting of 535 respiration cycles from diverse sources,\nincluding the Child Sim Lung Sound Simulator, the EMTprep Open-Source\nDatabase, Clinical Patient Records, and the ICBHI 2017 Challenge Database.\nAchieving a classification accuracy of 90%, the exceptional result metrics\nencompass the identification of each breath cycle and simultaneous detection\nof the abnormal sound, enabling the real-time wheeze counting of all\nrespirations. This innovative wheeze counter holds the promise of\nrevolutionizing research on predicting lung diseases based on long-term\nbreathing patterns and offers applicability in clinical and non-clinical\nsettings for on-the-go detection and remote intervention of exacerbated\nrespiratory symptoms.\n\n## Figures\n\nCitation: Im S, Kim T, Min C, Kang S, Roh Y, Kim C, et al. (2023) Real-time\ncounting of wheezing events from lung sounds using deep learning algorithms:\nImplications for disease prediction and early intervention. PLoS ONE 18(11):\ne0294447. https://doi.org/10.1371/journal.pone.0294447\n\nEditor: Mohammad Amin Fraiwan, Jordan University of Science and Technology,\nJORDAN\n\nReceived: May 17, 2023; Accepted: October 23, 2023; Published: November 20,\n2023\n\nCopyright: \u00a9 2023 Im et al. This is an open access article distributed under\nthe terms of the Creative Commons Attribution License, which permits\nunrestricted use, distribution, and reproduction in any medium, provided the\noriginal author and source are credited.\n\nData Availability: Codes and utilized data are available in our open\nrepository (https://github.com/sunghoon-most/Wheeze_Counter).\n\nFunding: This work was supported by the Korea Environment Industry &\nTechnology Institute (KEITI) through the Environmental Health Digital Program\nfunded by the Korea Ministry of Environment (MOE) (2021003330010,\n2021003330009) The funders had no role in study design, data collection and\nanalysis, decision to publish, or preparation of the manuscript.\n\nCompeting interests: The authors have declared that no competing interests\nexist.\n\n## Introduction\n\nLung diseases are a major cause of global morbidity and mortality, including\nasthma, COPD, lung infections like pneumonia, lung cancer, bronchitis, and\nother breathing problems [1, 2]. Lung sounds can be indicative of most lung\nand respiratory diseases [3]. When there is no respiratory disorder, normal\nbreathing sounds are heard, whereas abnormal breathing sounds such as wheezing\nor crackling are detected when there is a lung disease [4, 5]. For this\nreason, regular or routine monitoring of breathing sounds is essential for\nsymptom prevention and alleviation, as well as for the early detection of\nvarious respiratory diseases [6, 7]. Typically, respiratory abnormalities are\ndiagnosed by spirometry and auscultation [8]. While spirometry is impossible\nfor certain groups, such as children, and is difficult to use practically to\nmonitor a long-term pattern of patient condition in non-clinical settings [9,\n10], auscultation is non-invasive, inexpensive, and easy to use [11, 12].\nMedical professionals listen to these sounds to evaluate and diagnose patients\n[13]; however, conventional auscultation requires considerable training and\nexpertise, and its quality depends on the doctor\u2019s experience and hearing\n[14]. The misunderstanding of breathing sounds and making incorrect diagnoses\nis not rare among medical students [15, 16].\n\nTo overcome the limitation of conventional auscultation, various methods such\nas neural networks [17], classifiers [18, 19], and NMF [20] are suggested in\nmany cases in order to assist in the automatic detection and classification of\nadventitious lung sounds [21]. Among them, deep learning algorithms train the\nmachine to automatically learn the characteristics of the signals or waveforms\nof lung sounds to recognize abnormal lung or breathing sounds (wheezing,\ncrackling) [22]. The most common deep learning algorithm used for lung sound\nclassification is a convolutional neural network (CNN) [11, 15, 23, 24] or\nrecurrent neural network (RNN) model [25, 26] that extracts breathing sound\nfeatures from a two-dimensional spectrogram image, or a combination of the\ntwo, a convolutional-recurrent neural network (CRNN) [22, 27]. The accuracy of\nthe models ranges from 63% [11] to 99% [28], and in general, the CNN-based\nmodel has the highest accuracy [5]. Incorporating AI-based lung sound analysis\ninto automated diagnosis systems has been suggested to determine the degree of\nairway inflammation [29] or the risk of a number of lung diseases [30].\nRecently, efforts have been made to collect breathing sounds from smartphones\nor real-time lung sounds from wearable devices to develop automated AI-based\nsolutions for lung sound analysis and classification [31\u201334]. Through this\ntechnological advancement, abnormal respiratory and asthmatic symptoms could\nbe detected or diagnosed at an early stage via real-time self-monitoring or\ntelemedicine [35, 36].\n\nHowever, most existing models focus on the automatic diagnosis of single\nrecorded data, and applications to real-time monitoring data are still limited\n[21, 37]. They tended to be developed based on the learning data collected by\nauscultation for a short period of 10 to 70 s and labeled by clinicians [38,\n39]. Much of the previous work focused on addressing methodological challenges\nassociated with noise cancellation or reduction [40, 41], detection of the\nbreathing section, or binary classification of an individual cycle of\nrespiration [11, 22, 23, 42, 43]. Due to a lack of adaptability for real-time,\ncontinuous long-term signals, most lung sound classification algorithms have\nnot been widely implemented in practice, with limited applicability in self-\nsymptom management or telemedicine [2, 44]. Considering that respiratory\npatterns represent the holistic physical and psychological state of humans,\nnot only the presence of abnormal sounds but also the location, duration, and\nrelationships of a sequence of respiration cycles, including atypical\nbreathing activities, could serve as important reference data for clinicians\nand patients to diagnose and monitor lung diseases [45]. To provide\ncomprehensive information about the lung\u2019s breathing functionality, which may\nnot be well noticed or recognized in a clinical setting, the pattern and\nfrequency of abnormal lung sounds within a relatively long time must be\nanalyzed rather than most of the existing models for determining the presence\nor absence of abnormalities at each respiratory unit [46, 47]. The real-time\ndata collection and automated pre-processing system would be critical for\nlong-term monitoring and intervention [48]. We have summarized the relevant\npapers in a table and included them in the S1 Table.\n\nConsidering this loophole, in this exploratory study, we have developed a\nreal-time event counting algorithm to identify abnormal breathing sounds,\nespecially wheezing, and record their frequency to determine the pattern over\na certain period and present this information in real-time. We utilize a\nunique method that involves the meticulous categorization of a single\nbreathing cycle into three types: break, normal, and wheeze. The algorithm not\nonly detects abnormal sounds in each breath but also collects extensive data\non their location, duration, and connections within the entire respiratory\ncycle, including unusual patterns. This counting algorithm may improve\nexisting studies that aim to predict lung diseases based on long-term\nbreathing patterns [49\u201351], going beyond simply classifying respiratory units.\nIn addition, when integrated with wearable devices that are being actively\ndeveloped, its utility will be maximized [52, 53]. Using three types of\nlabeled lung sound data, we trained a one-dimensional convolutional neural\nnetwork and a long short-term memory (1D-CNN-LSTM) network model for\ndiscriminating three breathing statuses (break, normal, and wheezing) and then\ndeveloped a \u201creal-time wheezing counter\u201d as a pilot; we suggested the\npossibility of its application for early diagnosis or the remote treatment of\nrespiratory diseases. Our research demonstrates the potential of AI-based\ntechnology for diagnosing and monitoring lung diseases in real-time, offering\nthe prospect of earlier detection and improved treatment outcomes. Existing\nresearch gaps include limitations in real-time applications and a focus on\nshort-term data. We address these gaps with a real-time event counting\nalgorithm designed for continuous, long-term signals, emphasizing the pattern\nand frequency of abnormal lung sounds over time, rather than just detecting\ntheir presence or absence at individual respiratory units. This advancement\nholds promise for enhancing the diagnosis and monitoring of lung diseases.\n\n## Methods\n\n### Overview\n\nThe procedure of the developed wheeze counting algorithm is illustrated in Fig\n1. We first obtained multiple reference lung sound data sets from open sources\nand clinical data. We augmented the data using the pitch shift method to\novercome the limited quantity of training samples. We then extracted the\nfeatures of the augmented lung sound data using a Mel frequency spectrogram,\nwhich is widely used in sound analysis [54]. The preprocessed data were fed\ninto a combined model of 1D-CNN-LSTM, which has been shown to be effective for\nlung disease recognition [55]. After sufficient training to ensure reliable\naccuracy using validation datasets, we tested the trained model with the test\ndataset. Finally, we developed and improved a wheeze counting algorithm that\nanalyzes lung sound data to count the number of wheezes from clinical lung\nsound data. The algorithm could be applied to the long-term monitoring of\nbreathing functions in clinical and non-clinical settings.\n\nDownload:\n\n  * PPT\n\nPowerPoint slide\n\n  * PNG\n\nlarger image\n\n  * TIFF\n\noriginal image\n\nFig 1. Overall procedure of wheeze counting algorithm development and\napplications.\n\nhttps://doi.org/10.1371/journal.pone.0294447.g001\n\n### Clinical lung sound data in this study\n\nWe employed a subset of the clinical lung sound data collected on November 30,\n2021. for both training and testing purposes in our study. The first time we\naccessed the data was on March 10, 2022. To ensure the privacy and\nconfidentiality of the participants, none of the authors had access to any\ninformation that could potentially reveal their identity. The Institutional\nReview Board at Eulji University approved the study, affirming that it was\nconducted in compliance with all relevant ethical standards.\n\n### Lung sound databases\n\nWe obtained reference lung sound signals from three databases: 1) lung sound\nsimulator, 2) EMTprep, 3) clinical patient records, and 4) ICBHI 2017\ndatasets. First, using a commercial microphone, ten and seven cycles of\ntypical breathing and wheezing data were taken from the pediatric lung sound\nsimulator known as Child Sim (SimulAIDS Inc, UK). Second, the open-source lung\nsound database EMTprep (https://www.EMTprep.com) provided three cycles of\ntypical breathing and nine cycles of wheezing sound data. In the case of\nclinical data, we used 17 cycles of wheeze breathing using a commercial\nmicrophone that was affixed to the anterior right lung region. And last, we\nutilized additional diagnostic data from the ICBHI 2017 challenge database\n[31]. In the database, we used cases of asthma, COPD, and healthy patients.\nThe various lung sound signals that were employed in this study are listed in\nTable 1.\n\nDownload:\n\n  * PPT\n\nPowerPoint slide\n\n  * PNG\n\nlarger image\n\n  * TIFF\n\noriginal image\n\nTable 1. Number of lung sound signals by database.\n\nhttps://doi.org/10.1371/journal.pone.0294447.t001\n\n### Soft labeling and data augmentation\n\nSince some lung sounds annotated as \"expiratory wheezing\" contain both normal\nsound and adventitious sound in the isolated breathing cycle (S1 Fig), we\nsoft-labeled the data manually before augmenting it. Based on the clinician\u2019s\ndiagnosis, we used the free software Audacity to index the shifting boundary\nbetween normal and wheeze breathing during one breath cycle. We designated\nthem specifically as \"normal,\" \"wheeze,\" and \"break,\" which are depicted in\nblue, red, and green, respectively, in the left graph of Fig 2. Then, we\naugmented the soft labeled data using the Librosa Python library\u2019s \u201cpitch\nshifting\u201d function to get around data constraints [56]. In accordance with an\nearlier study, we changed the pitch by four semitone values (-3.5, -2, 2, and\n3.5) [57, 58].\n\nDownload:\n\n  * PPT\n\nPowerPoint slide\n\n  * PNG\n\nlarger image\n\n  * TIFF\n\noriginal image\n\nFig 2. Schematics of soft-labeling, and hard-labeling process.\n\nThe left graph shows 9 seconds of example lung sound and soft labeled\nannotation. The right graph shows the magnified scope (from 7 to 8.2 seconds)\nof the left graph with a hard labeled annotation.\n\nhttps://doi.org/10.1371/journal.pone.0294447.g002\n\n### Hard labeling and feature extraction\n\nFor each augmented lung sound, we sliced it into 25-ms segments with 10-ms\noverlaps. This allowed us to determine one breath cycle and, in addition, to\ndetermine which part of the breath contains accidental sound. Then we hard-\nlabeled the pre-processed lung sound data based on soft labels. As illustrated\nin the right graph in Fig 2, all segments were hard labeled to 0, 1, or 2. The\nlabel ratio (the ratio of hard labeled segments among the soft labeled range)\nis chosen at 1 for high accuracy (S2 Fig). The segments in the \u2018break\u2019 range\nwere labeled to 0, and in the same manner, the segments in the \u2018normal\u2019 range\nand \u2018wheeze\u2019 range were labeled into 1 and 2 each. Table 2 shows the total\nnumber of hard-labeled segments after hard labeling. From 535 breathing cycles\nin four databases, 332,720 segments were prepared as a machine learning\ndataset. Finally, we converted the segments into Mel spectrograms in order to\nextract acoustic features. 128 Mel bands were employed. Based on the\neffectiveness of the results from the counting algorithm, we select the proper\nparameter values of segment length and the number of Mel frequency bins (S2\nFig) for feature extraction.\n\nDownload:\n\n  * PPT\n\nPowerPoint slide\n\n  * PNG\n\nlarger image\n\n  * TIFF\n\noriginal image\n\nTable 2. Number of hard-labeled segments by database.\n\nhttps://doi.org/10.1371/journal.pone.0294447.t002\n\n## Results\n\n### Model structure\n\nWe used a combination 1D-CNN-LSTM model including two blocks: CNN and LSTM\nblocks. The CNN block consists of 1d CNN layers, MaxPooling, and a dropout\nlayer, while the LSTM block, connected directly to the CNN block, consists of\nLSTM, fully connected, and dropout layer. The MaxPooling and dropout layers\nwere used to prevent overfitting. We used the SoftMax function [59] as an\nactivation function for the final output layer, finally yielding 3 dimensional\noutputs. Fig 3 shows the entire model structure. We did not shuffle the\ndataset during the training procedure, so the association over time could be\nsufficiently reflected. As a result, 25-ms segmented lung sounds were fed into\nthe model, and the model predicted the probability of each class label. To\ncreate the model architecture, we employed the TensorFlow framework [60]. S2\nTable lists the parameters of the neural network model, including the filter,\nkernel, and layer-specific unit sizes. We used a kernel size of 16 (about 1/10\nof the input length) and 32 sets of filters, which is a regularly employed\nnumber, considering the size of the input data. The ReLu function is used as\nthe activation function for each 1D CNN layer. We extracted the dominating\nweights through the MaxPooling layer, followed by 256 units of the bi-\ndirectional LSTM layer, after passing two layers of the 1D CNN. Then, ReLu was\napplied to 128 dense layer units for the following fully connected layers.\n\nDownload:\n\n  * PPT\n\nPowerPoint slide\n\n  * PNG\n\nlarger image\n\n  * TIFF\n\noriginal image\n\nFig 3. 1D CNN and LSTM modeling structure and procedure.\n\nhttps://doi.org/10.1371/journal.pone.0294447.g003\n\n### Model training and validation\n\nThe hard-labeled dataset was divided into training and validation data. We\nused the 10-fold cross-validation method. The k-fold cross-validation method\nis beneficial as it ensures every data point is tested at least once and\nminimizes bias, providing a more accurate measure of the model\u2019s performance.\nThis method also helps mitigate overfitting by using most of the data for\nfitting and testing each data point at least once. We utilized the model with\nthe highest accuracy among the trained models. Then we evaluated the trained\nmodel\u2019s performance using test data by calculating its accuracy score, F1\nscore, and ROC-AUC (Area Under Receiver Operating Characteristic Curve) score.\nThe test data is extracted from reference audio signals, which are totally\nunseen during the training process (Fig 5A). The F1 score is widely used as a\nperformance measure in multi-label classification problems [61], and the ROC-\nAUC score is generally used for evaluating the performance of multi-label\nclassifiers [62], that should be calculated using either \"one-vs-rest\" or\n\"one-vs-one\" methods [63]. In this study, we used it as \"one-vs-rest\". The\nprediction results of the trained model are shown in Fig 4. The model\u2019s\naccuracy score was 0.9, the F1 score was 0.91, and the ROC-AUC score was 0.98\n(Fig 4A). We made use of the Sci-kit Learn Metrics Libraries to determine each\nscore. The model\u2019s accuracy for each label is displayed via the model\u2019s\nnormalized confusion matrix (Fig 4B). The model\u2019s accuracy for \"wheeze\" labels\nwas 0.82, and for \"normal\" and \"break\" labels, it was 0.84 and 0.96. Each\nlabel\u2019s sensitivity and specificity were also determined using a confusion\nmatrix without normalization (Fig 4C). The \"wheeze\" label has a sensitivity of\n0.83 and a specificity of 0.94 for the test datasets. At the label of\n\"normal,\" the specificity was at its highest, 0.97. Additionally, prior to\napplying the trained model to the counting algorithm, we also compared it to\nthree popular and verified classifiers from the Sci-kit Learn library: Random\nForest Classifier, K-Nearest Neighbors, and Multi-layer Perceptron classifier\n(S3 and S4 Figs). To further accurately assess the performance of those\nmodels, we used identical test datasets for each. In every indicator, the\nresults demonstrated that the 1dCNN + LSTM model performed better than the\nothers. Because of this, we used the model to implement our algorithm.\nFurthermore, in order to evaluate its durability in noisy environments, we\nalso overlapped clinical environment noise from free open source\n(https://freesound.org/people/bassboybg/sounds/201638/, S5 Fig). to the test\ndata. As shown in S6 Fig, the model misidentified the noise sound as wheezing,\nmaking it difficult to count the number of wheezings. However, after\nnoisereduce-Python-library preprocessing, the model closely identified the\ntrue labels of the test data with less error. These findings show possible\neffectiveness in noisy environments when the model is combined with a proper\nnoise cancellation method.\n\nDownload:\n\n  * PPT\n\nPowerPoint slide\n\n  * PNG\n\nlarger image\n\n  * TIFF\n\noriginal image\n\nFig 4. Performance of the trained 1D CNN+LSTM model.\n\n(A) Evaluation in three indicators: Accuracy, F1-score, and ROC-AUC score. (B)\nConfusion matrix of test data with normalization, and (C) without\nnormalization and calculated sensitivity and specificity of each label.\n\nhttps://doi.org/10.1371/journal.pone.0294447.g004\n\n### Validation through visualizing predicted probabilities\n\nThe trained model successfully predicted each class label (\"break,\" \"normal,\"\nand \"wheeze\"), as shown in Fig 5. For the test, 10-second-long clinical lung\nsound recordings within four cycles of breaths were used (Fig 5A). The first\nbreath (0 to 1.2 seconds) is normal, while the following breaths are abnormal,\nas can be seen in the Mel spectrogram image below the raw signal (Fig 5B).\nThen the input was segmented into 25-ms segments with 10-ms of overlap, and\neach segment was pre-processed for feature extraction by the same method as\ndescribed in the preceding sections. Predict-to-break probabilities are shown\nas green, Predict-to-normal probabilities are shown as blue, and Predict-to-\nwheeze probabilities are shown as red in Fig 5C, while the reference\nprobabilities for each label are shown as a dotted line. The findings\ndemonstrate that the 1D-CNN-LSTM model\u2019s predicted probability accurately\ntracked the references\u2019 paths.\n\nDownload:\n\n  * PPT\n\nPowerPoint slide\n\n  * PNG\n\nlarger image\n\n  * TIFF\n\noriginal image\n\nFig 5. Predicted probabilities of the 1d CNN+LSTM model.\n\n(a) Raw data from the input and (b) Mel spectrogram. (c) Predicted\nprobabilities from the trained 1D-CNN-LSTM model.\n\nhttps://doi.org/10.1371/journal.pone.0294447.g005\n\n### Counting algorithm\n\nWe counted the frequencies of wheeze throughout the full lung sound recording\nusing the predicted probabilities from the trained model. A (N, 128, 1) tensor\nencoding the recorded lung sound signals was generated, and it was then fed\ninto the trained model. The trained model predicted probabilities in the shape\nof (N, 3), with N being the number of lung sound segments after receiving the\npre-processed input data. Our proposed counting algorithm is described as\npseudo-code in Algorithm 1. Following are the steps: the segments were\npredicted in a (N, 3) shape of outputs, and by using the argmax function in\nthe NumPy library [64], the results were converted into a (N, 1) shape of\nhighest predicted labels (0 or 1 or 2); then, the peaks from the highest\npredicted labels were found by using the \u2019find peaks\u2019 function in the SciPy\nLibrary [65]; one breath cycle was considered complete when the value of d\n(peak interval) was longer than 100 (about one second); finally, isolated\nbreathing was classified as \"wheeze\" if the average peak heights from\nseparated breathing\u2019s range is higher than 1. If the average of peak is equal\nto 1, we considered the breath to be \u201cnormal.\"; In this manner, the total\nnumber of breath cycles as well as the total number of normal and wheeze\nevents throughout the full record were captured.\n\nAlgorithm 1. Wheeze counting algorithm for the recorded lung sound.\n\ndef wheeze counter (calculated probabilities = (N,3))\n\n(N, 1) \u2190 argmax (N, 3)\n\nd = peak interval\n\nif d \u2265 100\n\ncount +1 respiration\n\navg_p \u2190 average value of peaks\n\nif avg_p > 1.0\n\ncount +1 wheeze\n\nelse if avg_p = 1.0\n\ncount +1 normal\n\nAdditionally, a second algorithm for real-time counting to track wheeze events\nwas created. We defined a Boolean variable named \"Wheeze toggle\" that is\ninitially set to \"False\" at the beginning. The real-time wheeze counting\nprocedure is described as pseudo-code in Algorithm 2 and illustrated in Fig 6:\nThe algorithm takes raw signals with a 0.5-second duration as input, and the\n\"Wheeze toggle\" maintained its state even as the input\u2019s value changed over\ntime in order to link the prior signals with the current signal (Fig 6A). The\nraw input signal is converted into the Mel spectrogram, then the scale is\nchanged to dBs before being fed into the trained model; The trained model\npredicts the probabilities of the 0.25-ms segments as a (N, 3) shape of output\nfrom 0.5-s long raw input signals. The first index value of the output denotes\nthe probability that the 25-ms segment would \"break,\" the second index value\ndenotes the probability that it will \"normal,\" and the third index value\ndenotes the probability that it will \"wheeze.\" The total of the three\nprobabilities is 1 since SoftMax is the activation function of the final fully\nconnected layer. From the sequentially predicted probabilities, if predict-to-\nwheeze probability is highest 3 times in a row (Fig 6B), the moment is\nrecognized as a wheeze occurrence and \"Wheeze toggle\" changes to \"True.\" The\nbreath cycle is then recognized to terminate, if probability of predict-to-\nbreak is also highest 3 times in a row, while the \"Wheeze toggle\" is \"True.\"\nThen we reset \"Wheeze toggle\" to \"False\" for consecutive wheeze counting (Fig\n6C). For real-time application, 0.5-s long lung sounds are sequentially fed\ninto Algorithm 2 for wheeze occurrences to be continuously counted and\npresented to users.\n\nDownload:\n\n  * PPT\n\nPowerPoint slide\n\n  * PNG\n\nlarger image\n\n  * TIFF\n\noriginal image\n\nFig 6. Illustration of the real-time wheeze counting process.\n\n(A) Raw signal of the clinical lung sound and Mel spectrogram. 0.5 seconds of\ninput data is given to the model without overlap. (B) Score accumulates as\nprobabilities are higher than threshold. The score resets to 0 after it\nreaches 3 points. (C) When the \"Wheeze score\" reaches 3 points, the \"Wheeze\ntoggle\" changes to \"True,\" and when the \"break score\" reaches 3 points, it\nturns back to \"False\".\n\nhttps://doi.org/10.1371/journal.pone.0294447.g006\n\nAlgorithm 2. Wheeze counting algorithm for real-time lung sound.\n\ndef real-time wheeze counter (raw audio signal)\n\npred \u2190 model (converted input)\n\n(break prob, normal prob, wheeze prob) \u2190 pred\n\nif wheeze prob \u2265 0.6 more than 3 times\n\nwheeze toggle \u2018True\u2019\n\nwhile wheeze toggle \u2018False\u2019\n\nif break pred \u2265 0.6 more than 3 times\n\nwheeze toggle \u2018Off\u2019\n\ncount +1 wheeze\n\n### Application to clinical data\n\nWe monitored test data of relatively long-term clinical lung sounds and\ncounted the number of wheeze occurrences using the real-time wheeze counting\nmethod established in this work (Fig 7A). The trained model was continuously\nfed lung sounds with 0.5-second durations, and using the counting algorithm,\nwheeze events were calculated and logged across the dataset. For 0.5 seconds\nof input, lengths of 8,000 samples were used to reshape the (51, 128, 1) shape\nof the input because the raw signal\u2019s sample rate was 16,000 Hz. Therefore, a\n(5, 128, 1) shape of input is fed into the model at intervals of 0.5 seconds,\nand the model predicts 51 sets of probabilities as outputs. As a result, the\ncounting algorithm detected 33 wheezing occurrences from entire clinical lung\nsounds. From the 77 breaths detected by the algorithm, the wheeze rate was\ncalculated at 0.43 (= 33/77) and was close to the wheeze rate noted by the\ndoctor at 0.41 (= 32/77), that has only 2% of an error rate. Utilizing the\nresults, our counting algorithm is simulated in real-time autonomous\ndetection, as seen in the S1 Video.\n\nDownload:\n\n  * PPT\n\nPowerPoint slide\n\n  * PNG\n\nlarger image\n\n  * TIFF\n\noriginal image\n\nFig 7. Illustration of real-time wheeze counting using long-term clinical\ndata.\n\n(A) Raw signals of lung sound and Mel spectrogram for visualizing the acoustic\nfeatures. (B) The results of our counting algorithm. The frequency of wheeze\nsymptoms is counted, and time stamped.\n\nhttps://doi.org/10.1371/journal.pone.0294447.g007\n\n## Discussion\n\nLong-term monitoring of lung sounds assembled via a wearable device and AI-\nbased diagnosis without doctor involvement would be essential to developing\nadvanced computerized monitoring that may be used for self-symptom management\nor remote monitoring such as telemedicine. There have been a few research\nstudies on these issues recently [16, 53]. However, the methods typically\nclassify whether a signal is abnormal or normal or what kind of inadvertent\nsound it is. Their suggested method just distinguishes the subject\u2019s\nartificially induced inadvertent lung sound as a real-life applicable\ndemonstration, indicating a practical usage limitation. Otherwise, we present\nan applicable method for implementing long-term monitoring in clinical\nsettings by counting the number of wheeze occurrences over time. Our method\ndiffers from previous research in that it counts both the number of normal\nbreaths and the number of wheezes, that is helpful for monitoring respiratory\ndisease patients in dynamic environments. We utilize a segment-based\nclassification AI model, which is normally used in speech recognition [66] or\nrare sound detection [67, 68]. To be able to detect not only wheezing events\nbut also the isolated breath cycle, we sliced lung sound signals into segments\nand utilized the predicted probability of each segment. As a result, the\ncounting algorithm we developed could report the frequency of wheezing during\nentire clinical lung sounds without any additional information, such as\nrespiration volume.\n\nDespite our contributions, several methodological limitations must be\naddressed. First, due to the limited availability of reference lung sound data\ncollected in good quality, it is not sufficiently verified whether the\ndeveloped algorithm would function well on the data collected from various\npatients in diverse recording environments. With useful methods such as\nassessment of lung sound quality, we would utilize the fine quality of lung\nsound data, resulting in a more sophisticated and accurate AI model.\nFurthermore, the algorithm must be enhanced and adjusted based on the clinical\ntrials of long-term lung sound monitoring with a broad patient group in order\nto assure the validity, reliability, and applicability of preventive treatment\nin clinical and non-clinical settings. Second, this study does not provide\nempirical evidence on how sensitive the proposed algorithm is to different\ntypes of lung sounds. The frequency of wheezes was the focus of this study\nbecause it is known that wheezes might exacerbate asthma and COPD. A recently\ndeveloped wearable stethoscope including a de-nosing function [53] could\nenable the widespread practical use of our counting algorithm when more\nadventitious sounds such as crackling, rhonchi, stridor, and pleural rub are\naccumulated by the device. In general, automated long-term monitoring via AI-\nbased algorithms could assist preventative medicine by acquiring precursory\ninformation from numerous signals and images from the human body for the\nrelevant bad health impacts. As so, long-term monitoring of wheezing\noccurrences and patterns may shed light on the development of various\nrespiratory illness outcomes if combined with a patient\u2019s clinical records,\nsuch as symptom exacerbation and response to treatment. The integration of AI-\nbased algorithms into long-term monitoring could revolutionize preventative\nmedicine. By acquiring precursory information from numerous signals and images\nfrom the human body, AI could potentially predict adverse health impacts. In\nthe context of respiratory health, long-term monitoring of wheezing\noccurrences and patterns, combined with a patient\u2019s clinical records, could\nprovide invaluable insights into the development of various respiratory\nillness outcomes. This could lead to more personalized treatment plans and\nimproved patient outcomes.\n\n## Conclusion\n\nThis research presents a deep learning-based algorithm for counting wheezes,\nutilizing a 1D-CNN-LSTM model. The model is trained on a variety of reference\nlung sound databases to predict the probability of abnormal sounds in each\nsegment. Our algorithm then uses this model to count wheeze instances from\nrecorded lung sounds and validates in real-time lung sound simulation.\n\nOur wheeze counting method is straightforward yet effective, with potential\nfor expansion into automatic symptom monitoring. This could be crucial in\npredicting the onset or severity of future abnormalities, as well as detecting\ncurrent symptoms. Given the possible link between wheeze occurrence trends and\nsymptom exacerbations, our approach could aid in preventing urgent emergencies\nlike asthma attacks. Unlike traditional lung sound classification algorithms,\nour method can handle continuous data. With a detection accuracy of 90%, the\nresults include identifying the number of total breath cycles and the\nproportion of abnormal sounds, along with real-time counting and visualization\nof these events throughout whole respiration. This could revolutionize\nresearch on predicting lung diseases based on long-term breathing patterns and\noffers utility in both clinical and non-clinical settings for immediate\ndetection and remote intervention of worsened respiratory symptoms. Moreover,\nour counting algorithm can easily adapt to other bio-signals. For instance,\nwhen used with ECG (Electrocardiogram) or EMG (Electromyography) signals, it\ncould automatically detect the intensity of heart or muscle anomaly patterns.\n\nIn conclusion, our study introduces a novel and effective approach to real-\ntime wheeze detection and counting, which has significant potential for\nimproving self-symptom management and telemedicine-based remote monitoring.\nThis innovative wheeze counter, with its high detection accuracy and ability\nto handle continuous data, could play a crucial role in predicting lung\ndiseases based on long-term breathing patterns. Furthermore, its adaptability\nto other bio-signals suggests a wide range of potential applications in both\nclinical and non-clinical settings. Future research should focus on further\nrefining the algorithm and exploring its potential in various healthcare\ncontexts.\n\n## Supporting information\n\nResearch trends in lung sound analysis and related papers.\n\nShowing 1/9: pone.0294447.s001.docx\n\n### S1 Table. Research trends in lung sound analysis and related papers.\n\nhttps://doi.org/10.1371/journal.pone.0294447.s001\n\n(DOCX)\n\n### S2 Table. Model parameter values of the 1D CNN + LSTM model.\n\nhttps://doi.org/10.1371/journal.pone.0294447.s002\n\n(DOCX)\n\n### S1 Fig. Comparison of wheeze between normal by raw signal and Mel\nspectrogram.\n\nIn some cases, there is coexistence of normal and wheeze sound in isolated\nbreathing cycle.\n\nhttps://doi.org/10.1371/journal.pone.0294447.s003\n\n(TIF)\n\n### S2 Fig. Parametric study in number of Mel frequency bin, frame length, and\nlabeling ratio.\n\nAmong 9 cases of parametric study, we choose parameters of case 4 to utilize\nin counting algorithm.\n\nhttps://doi.org/10.1371/journal.pone.0294447.s004\n\n(TIF)\n\n### S3 Fig. Comparison of Sci-kit Learn classifiers between 1D CNN+LSTM model.\n\nThe classifier also trained by 10-fold cross-validation method.\n\nhttps://doi.org/10.1371/journal.pone.0294447.s005\n\n(TIF)\n\n### S4 Fig. Probability visualization of 3 comparative classifiers.\n\n(A) Random Forest classifier, (B) K-Nearest Neighbors, (C) Multi-layer\nperceptron.\n\nhttps://doi.org/10.1371/journal.pone.0294447.s006\n\n(TIF)\n\n### S5 Fig. Test data overlapped with hospital noise (waiting room).\n\n(A) original raw signal of test data, (B) test data with overlapped noise (SNR\n-20dB) is depicted in blue line, and result after noise reduce is plotted in\norange line (The number of standard deviations above the noise is set to\n\u20180.1\u2019, and mode of stationary set to \u2018True\u2019), and green line (default setting\nfrom library).\n\nhttps://doi.org/10.1371/journal.pone.0294447.s007\n\n(TIF)\n\n### S6 Fig. Probability visualization of noisy test data.\n\n(A) prediction probabilities of original noisy data, (B) predictions of noise\nreduced data (The number of standard deviations above the noise is set to\n\u20180.1\u2019, and mode of stationary set to \u2018True\u2019), and (C) different setting of\nnoise reduced data (default setting from library).\n\nhttps://doi.org/10.1371/journal.pone.0294447.s008\n\n(TIF)\n\n### S1 Video. Real-time counting in simulation.\n\nhttps://doi.org/10.1371/journal.pone.0294447.s009\n\n(MP4)\n\n## References\n\n  1. 1\\. Labaki W, Han MK. Chronic respiratory diseases: a global view. The Lancet Respiratory Medicine. 2020;8(6):531\u20133. pmid:32526184\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  2. 2\\. Fenton TR, Pasterkamp H, Tai A, Chemick V. Automated spectral characterization of wheezing in asthmatic children. IEEE Transactions on Biomedical Engineering. 1985;1:50\u20135. pmid:3980029\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  3. 3\\. Haider NS, Singh BK, Periyasamy R, Behera AK. Respiratory sound based classification of chronic obstructive pulmonary disease: a risk stratification approach in machine learning paradigm. Journal of Medical Systems. 2019;43(8):1\u201313. pmid:31254141\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  4. 4\\. Rocha BM, Pessoa D, Marquest A, Carvalho P, Paiva RP. Automatic classification of adventitious respiratory sounds: A (un) solved problem? Sensors. 2020;21(1):57. pmid:33374363\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  5. 5\\. Aykanat M, Kilic O, Kurt B, Saryal S. Classification of lung sounds using convolutional neural networks. EURASIP Journal on Image and Video Processing. 2017;2017(1):1\u20139.\n\n     * View Article\n     * Google Scholar\n  6. 6\\. Sahgal N. Monitoring and analysis of lung sounds remotely. International Journal of Chronic Obstructive Pulmonary Disease. 2011;6:407\u201312. pmid:21857780\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  7. 7\\. Jayalakshmy S, Sudha GF. Scalogram based prediction model for respiratory disorders using optimized convolutional neural networks. Artificial Intelligence in Medicine. 2020;103(101809). pmid:32143805\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  8. 8\\. Mukherjee H, Sreerama P, Dhar A, Obaidullah S, Roy K, Mahmud M, et al. Automatic lung health screening using respiratory sounds. Journal of Medical Systems. 2021;45(2):160\u20139. pmid:33426615\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  9. 9\\. Swarnkar V, Abeyratne U, Tan J, Ng TW, Brisbane JM, Choveaux J, et al. Stratifying asthma severity in children using cough sound analytic technology. Journal of Asthma. 2021;58(2):160\u20139. pmid:31638844\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  10. 10\\. Islam MA, Bandyopadhyaya I, Bhattacharyya P, Saha G. Multichannel lung sound analysis for asthma detection. Computer methods and programs in biomedicine. 2018;159:111\u201323. pmid:29650306\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  11. 11\\. Demir F, Sengur A, Bajaj V. Convolutional neural networks based efficient approach for classification of lung diseases. Health information science and systems. 2020;8(1):1\u20138.\n\n     * View Article\n     * Google Scholar\n  12. 12\\. Li SH, Lin BS, Tsai CH, Yang CT, Lin BS. Design of wearable breathing sound monitoring system for real-time wheeze detection. Sensors. 2017;17(1):171. pmid:28106747\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  13. 13\\. Sarkar M, Madabhavi I, Niranjan N, Dogra M. Ausculation of the respiratory system. Annals of Thoracic Medicine. 2015;10(3):158\u201368.\n\n     * View Article\n     * Google Scholar\n  14. 14\\. Kiyokawa H, Greenberg M, Shirota K, Pasterkamp H. Auditory detection of simulated crackles in breath sounds. Chest. 2001;119(6):1886\u201392. pmid:11399719\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  15. 15\\. Bardou D, Zhang K, Ahmad SM. Lung sounds classification using convolutional neural networks. Artificial Intelligence in Medicine. 2018;88:58\u201369. pmid:29724435\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  16. 16\\. Kim Y, Hyon Y, Jung SS, Lee S, Yoo G, Chung C, et al. Respiratory sound classification for crackles, wheezes, and rhonchi in the clinical field using deep learning. Scientific Reports. 2021;11(1):1\u201311.\n\n     * View Article\n     * Google Scholar\n  17. 17\\. Kochetov K, Putin E, Azizov S, Skorobogatov I, Filchenkov A, editors. Wheeze detection using convolutional neural networks. Progress in Artificial Intelligence: 18th EPIA Conference on Artificial Intelligence, EPIA 2017, Porto, Portugal, September 5\u20138, 2017, Proceedings 18; 2017: Springer.\n\n     * View Article\n     * Google Scholar\n  18. 18\\. Rani A, Sehrawat H, editors. Role Of Machine Learning and Random Forest in Accuracy Enhancement During Asthma Prediction. 2022 10th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions)(ICRITO); 2022: IEEE.\n\n  19. 19\\. Oletic D, Bilas V. Asthmatic wheeze detection from compressively sensed respiratory sound spectra. IEEE journal of biomedical and health informatics. 2017;22(5):1406\u201314. pmid:29990246\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  20. 20\\. Torre-Cruz J, Canadas-Quesada F, Carabias-Orti J, Vera-Candeas P, Ruiz-Reyes N. A novel wheezing detection approach based on constrained non-negative matrix factorization. Applied Acoustics. 2019;148:276\u201388.\n\n     * View Article\n     * Google Scholar\n  21. 21\\. Pramono RXA, Bowyer S, Rodriguez-Villegas E. Automatic adventitious respiratory sound analysis: A systematic review. PloS one. 2017;12(5):e0177926. pmid:28552969\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  22. 22\\. Acharya J, Basu A. Deep neural network for respiratory sound classification in wearable devices enabled by patient specific model tuning. IEEE transactions on biomedical circuits and systems. 2020;14(3):535\u201344. pmid:32191898\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  23. 23\\. Demir F, Ismael AM, Sengur A. Classification of lung sounds with CNN model using parallel pooling structure. IEEE Access. 2020;8:105376\u201383.\n\n     * View Article\n     * Google Scholar\n  24. 24\\. Shuvo SB, Ali SN, Swapnil SI, Hasan T, Bhuiyan MIH. A lightweight cnn model for detecting respiratory diseases from lung auscultation sounds using emd-cwt-based hybrid scalogram. IEEE Journal of Biomedical and Health Informatics. 2020;25(7):2595\u2013603.\n\n     * View Article\n     * Google Scholar\n  25. 25\\. Perna D, Tagarelli A, editors. Deep auscultation: Predicting respiratory anomalies and diseases via recurrent neural networks. 2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS); 2019: IEEE.\n\n     * View Article\n     * Google Scholar\n  26. 26\\. Kochetov K, Putin E, Balashov M, Filchenkov A, Shalyto A, editors. Noise masking recurrent neural network for respiratory sound classification. International Conference on Artificial Neural Networks; 2018: Springer, Cham.\n\n  27. 27\\. Hsu FS, Huang SR, Huang CW, Huang CJ, Cheng YR, Chen CC, et al. Benchmarking of eight recurrent neural network variants for breath phase and adventitious sound detection on a self-developed open-access lung sound database. PLoS One. 2021;16(7):e0254134.\n\n     * View Article\n     * Google Scholar\n  28. 28\\. Tariq Z, Shah SK, Lee Y. Feature-based Fusion using CNN for Lung and Heart Sound Classification. Sensors. 2022;22(4):1521. pmid:35214424\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  29. 29\\. Shimoda T, Obase Y, Nagasaka Y, Nakano H, Ishimatsu A, Kishikawa R, et al. Lung sound analysis helps localize airway inflammation in patients with bronchial asthma. Journal of Asthma and Allergy. 2017;10:99\u2013108. pmid:28392708\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  30. 30\\. Aziz S, Khan MU, Shakeel M, Mushtaq Z, Khan AZ. An Automated System towards Diagnosis of Pneumonia using Pulmonary Auscultations. 2019 13th International Conference on Mathematics, Actuarial Science, Computer Science and Statistics (MACS) 2019. p. 1\u20137,\n\n     * View Article\n     * Google Scholar\n  31. 31\\. Rocha BM, Filos D, Mendes L, Serbes G, Ulukaya S, Kahya YP, et al. An open access database for the evaluation of respiratory sound classification algorithms. Physiological Measurement. 2019;40(3):035001. pmid:30708353\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  32. 32\\. Faezipour M, Abuzneid A. Smartphone-based self-testing of COVID-19 using breathing sounds. Telemedicine and e-Health. 2020;26(10):1202\u20135. pmid:32487005\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  33. 33\\. Lella KK, Pja A. Automatic diagnosis of COVID-19 disease using deep convolutional neural network with multi-feature channel from respiratory sound data: cough, voice, and breath. Alexandria Engineering Journal. 2022;61(2):1319\u201334.\n\n     * View Article\n     * Google Scholar\n  34. 34\\. Gupta P, Moghimi MJ, Jeong Y, Gupta D, Inan OT, Ayazi F. Precision wearable accelerometer contact microphones for longitudinal monitoring of mechano-acoustic cardiopulmonary signals. NPJ Digital Medicine. 2020;3(1):1\u20138. pmid:32128449\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  35. 35\\. Srivastava A, Jain S, Miranda R, Patil S, Pandya S, Kotecha K. Deep learning based respiratory sound analysis for detection of chronic obstructive pulmonary disease. PeerJ Computer Science. 2021;7:e369. pmid:33817019\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  36. 36\\. Jaber MM, Abd SK, Shakeel PM, Burhanuddin MA, Mohammed MA, Yussof S. A telemedicine tool framework for lung sounds classification using ensemble classifier algorithms. Measurement. 2020;162(107883).\n\n     * View Article\n     * Google Scholar\n  37. 37\\. Corbishley P, Rodriguez-Villegas E. Breathing detection: towards a miniaturized, wearable, battery-operated monitoring system. IEEE Transactions on Biomedical Engineering. 2007;55(1):196\u2013204.\n\n     * View Article\n     * Google Scholar\n  38. 38\\. Rocha BM, Filos D, Mendes L, Vogiatzis I, Perantoni E, Kaimakamis E, et al., editors. \u0391 respiratory sound database for the development of automated classification. International Conference on Biomedical and Health Informatics; 2017; Singapore: Springer.\n\n  39. 39\\. Chambres G, Hanna P, Desainte-Catherine M, editors. Automatic detection of patient with respiratory diseases using lung sound analysis. International Conference on Content-Based Multimedia Indexing (CBMI) 2018: IEEE.\n\n     * View Article\n     * Google Scholar\n  40. 40\\. Meng F, Wang Y, Shi Y, Zhao H. A kind of integrated serial algorithms for noise reduction and characteristics expanding in respiratory sound. International Journal of Biological Sciences. 2019;15(9):1921. pmid:31523193\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  41. 41\\. Munoz-Montoro AJ, Revuelta-Sanz P, Martinez-Munoz D, Torre-Cruz J, Ranilla J. An ambient denoising method based on multi-channel non-negative matrix factorization for wheezing detection. The Journal of Supercomputing. 2022;(https://doi.org/10.1007/s11227-022-04706-x).\n\n     * View Article\n     * Google Scholar\n  42. 42\\. Garcia-Ordas MT, Benitez-Andrades JA, Garcia-Rodriguez I, Benavides C, Alaiz-Moreton H. Detecting respiratory pathologies using convolutional neural networks and variational autoencoders for unbalancing data. Sensors. 2020;20(4):1214. pmid:32098446\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  43. 43\\. Koehler U, Hildebrandt O, Fischer P, Gross V, Sohrabi K, Timmesfeld N, et al. Time course of nocturnal cough and wheezing in children with acute bronchitis monitored by lung sound analysis. European Journal of Pediatrics. 2019;178(9):1385\u201394. pmid:31321530\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  44. 44\\. Joyashiki T, Wada C. Validation of a body-conducted sound sensor for respiratory sound monitoring and a comparison with several sensors. Sensors. 2020;20(3):942. pmid:32050716\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  45. 45\\. Wang Y, Hu M, Zhou Y, Li Q, Yao N, Zhai G, et al. Unobtrusive and automatic classification of multiple people\u2019s abnormal respiratory patterns in real time using deep neural network and depth camera. IEEE Internet of Things Journal. 2020;7(9):8559\u201371.\n\n     * View Article\n     * Google Scholar\n  46. 46\\. Xue B, Shi W, Chotirmall SH, Koh VCA, Ang YY, Tan RX, et al. Distance-Based Detection of Cough, Wheeze, and Breath Sounds on Wearable Devices. Sensors. 2022;22(6):2167. pmid:35336338\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  47. 47\\. Monaco A, Amoroso N, Bellantuono L, Pantaleo E, Tangaro S, Bellotti R. Multi-time-scale features for accurate respiratory sound classification. Applied Sciences. 2020;10(23):8606.\n\n     * View Article\n     * Google Scholar\n  48. 48\\. Naqvi SZH, Choudhry MA. An automated system for classification of chronic obstructive pulmonary disease and pneumonia patients using lung sound analysis. Sensors. 2020;20(22):6512. pmid:33202613\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  49. 49\\. Fernandez-Granero MA, Sanchez-Morillo D, Leon-Jimenez A. An artificial intelligence approach to early predict symptom-based exacerbations of COPD. Biotechnology & Biotechnological Equipment. 2018;32(3):778\u201384.\n\n     * View Article\n     * Google Scholar\n  50. 50\\. Gurung A, Scrafford CG, Tielsch JM, Levine OS, Checkley W. Computerized lung sound analysis as diagnostic aid for the detection of abnormal lung sounds: a systematic review and meta-analysis. Respiratory Medicine. 2011;105(9):1396\u2013403. pmid:21676606\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  51. 51\\. Abaza AA, Day JB, Reynolds JS, Mahmoud AM, Goldsmith WT, McKinney WG, et al. Classification of voluntary cough sound and airflow patterns for detecting abnormal pulmonary function. Cough. 2009;5(1):1\u201312. pmid:19930559\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  52. 52\\. George UZ, Moon KS, Lee SQ. Extraction and analysis of respiratory motion using a comprehensive wearable health monitoring system. Sensors. 2021;21(4):1393. pmid:33671202\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  53. 53\\. Lee S, Kim Y, Yeo M, Mahmood M, Zavanelli N, Chung C, et al. Fully portable continuous real-time auscultation with a soft wearable stethoscope designed for automated disease diagnosis. Science Advances. 2022;8(21):eabo5867. pmid:35613271\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  54. 54\\. Stevens SS, Volkmann J, Newman EB. A scale for the measurement of the psychological magnitude pitch. The journal of the acoustical society of america. 1937;8(3):185\u201390.\n\n     * View Article\n     * Google Scholar\n  55. 55\\. Fraiwan M, Fraiwan L, Alkhodari M, Hassanin O. Recognition of pulmonary diseases from lung sounds using convolutional neural networks and long short-term memory. Journal of Ambient Intelligence and Humanized Computing. 2022;13(10):4759\u201371. pmid:33841584\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  56. 56\\. McFee B, Raffel C, Liang D, Ellis DP, McVicar M, Battenberg E, et al., editors. librosa: Audio and music signal analysis in python. Proceedings of the 14th python in science conference; 2015.\n\n  57. 57\\. Jayalakshmy S, Sudha GF. Conditional gan based augmentation for predictive modeling of respiratory signals. Computers in Biology and Medicine. 2021;138:104930. pmid:34638019\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  58. 58\\. Tariq Z, Shah SK, Lee Y, editors. Lung disease classification using deep convolutional neural network. 2019 IEEE international conference on bioinformatics and biomedicine (BIBM); 2019: IEEE.\n\n     * View Article\n     * Google Scholar\n  59. 59\\. Bridle JS. Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. Neurocomputing: Springer; 1990. p. 227\u201336.\n\n  60. 60\\. Abadi M, Barham P, Chen J, Chen Z, Davis A, Dean J, et al., editors. {TensorFlow}: a system for {Large-Scale} machine learning. 12th USENIX symposium on operating systems design and implementation (OSDI 16); 2016.\n\n     * View Article\n     * Google Scholar\n  61. 61\\. Pillai I, Fumera G, Roli F. Designing multi-label classifiers that maximize F measures: State of the art. Pattern Recognition. 2017;61:394\u2013404.\n\n     * View Article\n     * Google Scholar\n  62. 62\\. Fawcett T. An introduction to ROC analysis. Pattern recognition letters. 2006;27(8):861\u201374.\n\n     * View Article\n     * Google Scholar\n  63. 63\\. Hand DJ, Till RJ. A simple generalisation of the area under the ROC curve for multiple class classification problems. Machine learning. 2001;45(2):171\u201386.\n\n     * View Article\n     * Google Scholar\n  64. 64\\. Harris CR, Millman KJ, Van Der Walt SJ, Gommers R, Virtanen P, Cournapeau D, et al. Array programming with NumPy. Nature. 2020;585(7825):357\u201362. pmid:32939066\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  65. 65\\. Virtanen P, Gommers R, Oliphant TE, Haberland M, Reddy T, Cournapeau D, et al. SciPy 1.0: fundamental algorithms for scientific computing in Python. Nature methods. 2020;17(3):261\u201372. pmid:32015543\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  66. 66\\. Amodei D, Ananthanarayanan S, Anubhai R, Bai J, Battenberg E, Case C, et al., editors. Deep speech 2: End-to-end speech recognition in english and mandarin. International conference on machine learning; 2016: PMLR.\n\n     * View Article\n     * Google Scholar\n  67. 67\\. Stables R, Hockman J, Southall C, editors. Automatic Drum Transcription using Bi-directional Recurrent Neural Networks 2016: dblp.\n\n     * View Article\n     * Google Scholar\n  68. 68\\. Lim H, Park J-S, Han Y, editors. Rare Sound Event Detection Using 1D Convolutional Recurrent Neural Networks. DCASE; 2017.\n\n     * View Article\n     * Google Scholar\n\nDownload PDF\n\n  * Citation\n  * XML\n\nPrint\n\nShare\n\n  * Reddit\n  * Facebook\n  * LinkedIn\n  * Mendeley\n  * Twitter\n  * Email\n\nAdvertisement\n\n###\n\nSubject Areas\n\n?\n\nFor more information about PLOS Subject Areas, click here.\n\nWe want your feedback. Do these Subject Areas make sense for this article?\nClick the target next to the incorrect Subject Area and let us know. Thanks\nfor your help!\n\n  * Breathing\n\nIs the Subject Area \"Breathing\" applicable to this article?\n\nThanks for your feedback.\n\n  * Respiratory physiology\n\nIs the Subject Area \"Respiratory physiology\" applicable to this article?\n\nThanks for your feedback.\n\n  * Algorithms\n\nIs the Subject Area \"Algorithms\" applicable to this article?\n\nThanks for your feedback.\n\n  * Acoustic signals\n\nIs the Subject Area \"Acoustic signals\" applicable to this article?\n\nThanks for your feedback.\n\n  * Data visualization\n\nIs the Subject Area \"Data visualization\" applicable to this article?\n\nThanks for your feedback.\n\n  * Auscultation\n\nIs the Subject Area \"Auscultation\" applicable to this article?\n\nThanks for your feedback.\n\n  * Neural networks\n\nIs the Subject Area \"Neural networks\" applicable to this article?\n\nThanks for your feedback.\n\n  * Telemedicine\n\nIs the Subject Area \"Telemedicine\" applicable to this article?\n\nThanks for your feedback.\n\n  * Publications\n  * PLOS Biology\n  * PLOS Climate\n  * PLOS Complex Systems\n  * PLOS Computational Biology\n  * PLOS Digital Health\n  * PLOS Genetics\n  * PLOS Global Public Health\n\n  * PLOS Medicine\n  * PLOS Mental Health\n  * PLOS Neglected Tropical Diseases\n  * PLOS ONE\n  * PLOS Pathogens\n  * PLOS Sustainability and Transformation\n  * PLOS Water\n\n  * Home\n  * Blogs\n  * Collections\n  * Give feedback\n  * LOCKSS\n\n  * Privacy Policy\n  * Terms of Use\n  * Advertise\n  * Media Inquiries\n  * Contact\n\nPLOS is a nonprofit 501(c)(3) corporation, #C2354500, based in San Francisco,\nCalifornia, US\n\n### Cookie Preference Center\n\nOur website uses different types of cookies. Optional cookies will only be set\nwith your consent and you may withdraw this consent at any time. Below you can\nlearn more about the types of cookies PLOS uses and register your cookie\npreferences.\n\n### Customize Your Cookie Preference\n\n+Strictly Necessary\n\nAlways On\n\n+Functional\n\n+Performance and Analytics\n\n+Marketing\n\nFor more information about the cookies and other technologies used by us,\nplease read our Cookie Policy.\n\n", "frontpage": false}
