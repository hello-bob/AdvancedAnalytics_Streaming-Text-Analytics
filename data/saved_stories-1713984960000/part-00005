{"aid": "40142886", "title": "Diving Deeper into AI Package Hallucinations", "url": "https://www.lasso.security/blog/ai-package-hallucinations", "domain": "lasso.security", "votes": 1, "user": "irememberu", "posted_at": "2024-04-24 11:00:48", "comments": 0, "source_title": "Diving Deeper into AI Package Hallucinations", "source_text": "Diving Deeper into AI Package Hallucinations\n\nBar Lanyado\n\nBar Lanyado\n\nBack to all blog posts\n\n# Diving Deeper into AI Package Hallucinations\n\nBar Lanyado\n\nThursday\n\n,\n\nMarch\n\n28\n\n9\n\nmin read\n\nOn this page\n\nWhy - Research Motivation\n\nHow- Research Process\n\nWhat - Research Results\n\nGPT4\n\nGPT3.5\n\nGEMINI\n\nCOHERE\n\nCross models hallucinations\n\nHeads up: Hallucinated packages in the wild?\n\nHallucinated Package Case Study\n\nOur Hallucinated Package Adoption\n\nRecommendations (What\u2019s Next)\n\nSix months ago (forever in Generative AI terms) while working at Vulcan Cyber,\nI conducted research regarding LLM hallucinations on open source package\nrecommendations.\n\nIn my previous research I have exposed a new attack technique: AI Package\nHallucination. This Attack technique uses LLM tools such as ChatGPT, Gemini,\nand more, in order to spread malicious packages that do not exist, based on\nmodel outputs provided to the end-user.\n\nI also tested the technique on GPT-3.5 turbo model and used 457 questions for\nover 40 subjects in 2 programming languages. I found out that for almost 30%\nof my questions, the model recommended at least one hallucinated package that\nattackers could use for malicious purposes.\n\nThis time, I aimed to take it to the next level and scale up everything from\nmy previous research: from the amount of questions asked, number of languages\nchecked and the models we tested.\n\n## Why - Research Motivation\n\nI kicked start on this follow-up research for several reasons:\n\n1\ufe0f\u20e3 I investigate whether package hallucinations persist in the current\nlandscape, six months after the initial findings. I wanted to reaffirm the\nsignificance of this problem and to check whether model providers have dealt\nwith this issue by now.\n\n2\ufe0f\u20e3 I intended to expand the scope of my previous investigation by utilizing a\nlarger number of questions, thereby validating whether the percentages of\nhallucinated packages remain consistent with my earlier results. Additionally,\nI was looking to assess the resilience of this technique across different\nLLMs, as it is crucial to understand its adaptability.\n\n3\ufe0f\u20e3 I was keen on exploring the possibility of cross-model hallucinations,\nwhere the same hallucinated package appears in different models, shedding\nlight on potential shared hallucinations.\n\n4\ufe0f\u20e3 I aim to gather more comprehensive statistics regarding the repetitiveness\nof package hallucinations, to further enrich our understanding of the breadth\nof this security concern.\n\n5\ufe0f\u20e3 I wanted to test the attack effectiveness in the wild, and to see if the\ntechnique I discovered could actually be exploited, to confirm the practical\napplicability of my findings.\n\n## How- Research Process\n\nIn order to simulate the hallucinated package scenario, I gathered and asked\nthe different models a set of generic questions that developers are likely to\nask in real-life situations.\n\nThis time I have collected 2500 questions related to over 100 subjects and in\n5 programming languages (python, node.js, go, .net, ruby). After collecting\nand analyzing the set of questions, I\u2019ve kept only the \u201chow to\u201d questions in\norder to simulate the workflow of developers with the models and probe to\nmodel to offer a solution that includes a package. In total I have collected\n47,803 of \u201chow to questions\u201d, and asked four different models via their API:\nGPT-3.5-Turbo, GPT-4, Gemini Pro (Bard), and Coral (Cohere).\n\nAnother difference from the previous research, is that this time I used\nLangchain framework to handle the model interactions. I kept its default\nprompt which tells the model to say if it doesn\u2019t know the answer (see the\npicture below). We expected this mechanism to make it harder for the models to\nprovide an hallucinated answer.\n\nLangchain Default Prompt\n\nIn order to check the repetitiveness, I randomly chose 20 questions with zero-\nshot hallucinations (which means we received an hallucinated package the first\ntime we asked the model our questions). I then took each of these questions\nand asked them 100 times on each model. My goal was to check if I receive the\nsame hallucinated package every time.\n\n## What - Research Results\n\n### GPT4\n\nIn total we received 24.2% of hallucinations, with 19.6% of repetitiveness.\n\n### GPT3.5\n\nIn total we received 22.2% of hallucinations, with 13.6% of repetitiveness.\n\n### GEMINI\n\nIn total we received 64.5% of hallucinations, with 14% percentages of\nrepetitiveness.\n\n### COHERE\n\nIn total we received 29.1% of hallucinations, with 24.2% percentages of\nrepetitiveness.\n\nIn GO and .NET we received many hallucinated packages across all models.\nAlthough the numbers are high, not all of the hallucinated packages were\nexploitable.\n\n1\\. In GO, there is no centralized package repository, and all the packages\nare stored in different places such as private domains or repositories in\nGitHub, GitLab, BitBucket, etc. So when we checked the hallucinated packages\nwe received in GO, we found that many of them were pointed to repositories\nthat don't exist but the username in path does or pointed to domains that were\nalready taken. In reality it means the attacker wouldn\u2019t be able to upload\nmalicious packages to this path.\n\n2\\. In .Net, there is a centralized package repository but there are some\nreserved prefixes. So when we checked the hallucinated packages we received\nmany hallucinations that start with reserved prefixes(Microsoft, AWSSDK,\nGoogle, etc.) which means an attacker that finds these hallucinations will not\nbe able to upload packages to these paths as well.\n\n### Cross models hallucinations\n\nAfter finding all the hallucinated packages in all the models above, we\nchecked the results and compared them between the models. We looked for\nintersections of hallucinated packages that were received in more than one\nmodel.\n\nI have also tested the intersection across all of the models together and\nfound 215 hallucinated packages.\n\nIt\u2019s interesting to note that the highest number of cross-hallucinated\npackages can be found between Gemini and GPT-3.5. The lowest amount is between\nCohere and GPT-4. We think this data and these types of researches can shed\nnew light about the phenomena of hallucinations in general\n\n## Heads up: Hallucinated packages in the wild?\n\nDuring our research encountered an interesting python hallucinated package\ncalled \u201chuggingface-cli\u201d.\n\n### Hallucinated Package Case Study\n\nI have decided to upload an empty package by the same name and see what\nhappens. In order to verify the number of real downloads I have uploaded a\ndummy package called: \u201cblabladsa123\u201d in order to verify how many of the\ndownloads are scanners.\n\nThe results are astonishing. In three months the fake and empty package got\nmore than 30k authentic downloads! (and still counting).\n\n### Our Hallucinated Package Adoption\n\nIn addition, we conducted a search on GitHub to determine whether this package\nwas utilized within other companies repositories. Our findings revealed that\nseveral large companies either use or recommend this package in their\nrepositories. For instance, instructions for installing this package can be\nfound in the README of a repository dedicated to research conducted by\nAlibaba.\n\n## Recommendations (What\u2019s Next)\n\nI'll divide my recommendation into two main points:\n\n\ud83d\udca1 First, exercise caution when relying on Large Language Models (LLMs). If\nyou're presented with an answer from an LLM and you're not entirely certain of\nits accuracy\u2014particularly concerning software packages\u2014make sure to conduct\nthorough cross-verification to ensure the information is accurate and\nreliable.\n\n\ud83d\udca1 Secondly, adopt a cautious approach to using Open Source Software (OSS).\nShould you encounter a package you're unfamiliar with, visit the package's\nrepository to gather essential information about it. Evaluate the size of its\ncommunity, its maintenance record, any known vulnerabilities, and the overall\nengagement it receives, indicated by stars and commits. Also, consider the\ndate it was published and be on the lookout for anything that appears\nsuspicious. Before integrating the package into a production environment, it's\nprudent to perform a comprehensive security scan.\n\n## The latest from our blog\n\nBlogs\n\n### Woo Hoo! Lasso Security is Now on Okta Marketplace\n\nBlogs\n\n### Achieving Compliance with AI TRiSM: the EU AI Act and US Executive Order\non AI\n\nBlogs\n\n### AI Code Assistants and Cybersecurity Risk: 3 Recent Findings\n\nPartnersBlogResourcesEvents & webinars\n\nAboutCareersBook a demoContact us\n\nTerms of usePrivacy policy\n\nSubscribe to our newsletter\n\nSubscribe\n\nIn Process\n\nIn Process\n\n@ 2024 Lasso.security All rights reserved\n\nFollow us on\n\nThis site uses cookies to improve your browsing experience, provide\npersonalized content, and analytics. By using our site, you consent to our use\nof cookies.\n\nLearn moreAccept\n\n", "frontpage": false}
