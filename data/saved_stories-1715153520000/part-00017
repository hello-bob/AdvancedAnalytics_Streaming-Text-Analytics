{"aid": "40291392", "title": "OpenUI let's you describe UI using your imagination, then see it rendered live", "url": "https://github.com/wandb/openui", "domain": "github.com/wandb", "votes": 2, "user": "adif_sgaid", "posted_at": "2024-05-07 20:58:35", "comments": 0, "source_title": "GitHub - wandb/openui: OpenUI let's you describe UI using your imagination, then see it rendered live.", "source_text": "GitHub - wandb/openui: OpenUI let's you describe UI using your imagination,\nthen see it rendered live.\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nwandb / openui Public\n\n  * Notifications\n  * Fork 845\n  * Star 10.4k\n\nOpenUI let's you describe UI using your imagination, then see it rendered\nlive.\n\n### License\n\nApache-2.0 license\n\n10.4k stars 845 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# wandb/openui\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n3 Branches\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nvanpeltMerge pull request #69 from\nwandb/dependabot/npm_and_yarn/frontend/ej...May 5, 202414d74f0 \u00b7 May 5,\n2024May 5, 2024\n\n## History\n\n71 Commits  \n  \n### .devcontainer\n\n|\n\n### .devcontainer\n\n| Update readme, gitignore, fix devcontainer extension| Apr 3, 2024  \n  \n### assets\n\n|\n\n### assets\n\n| Update readme, gitignore, fix devcontainer extension| Apr 3, 2024  \n  \n### backend\n\n|\n\n### backend\n\n| Build frontend with new iframe logic| Apr 30, 2024  \n  \n### frontend\n\n|\n\n### frontend\n\n| Merge pull request #69 from wandb/dependabot/npm_and_yarn/frontend/ej...|\nMay 5, 2024  \n  \n### .gitattributes\n\n|\n\n### .gitattributes\n\n| Better loading or error page, ignore dist files in diffs| Mar 31, 2024  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| Update readme, gitignore, fix devcontainer extension| Apr 3, 2024  \n  \n### .python-version\n\n|\n\n### .python-version\n\n| Skip pre-commit config for now| Apr 1, 2024  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Mega initial commit of OpenUI goodness| Mar 17, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| Link to llava| Apr 5, 2024  \n  \n### docker-compose.yaml\n\n|\n\n### docker-compose.yaml\n\n| Fixed wandb with image uploads, added docker-compose, made Ollama wai...|\nApr 5, 2024  \n  \n### docs\n\n|\n\n### docs\n\n| Fix emoji, setup github pages| Mar 18, 2024  \n  \n### openui.code-workspace\n\n|\n\n### openui.code-workspace\n\n| Mega initial commit of OpenUI goodness| Mar 17, 2024  \n  \n## Repository files navigation\n\n# OpenUI\n\nBuilding UI components can be a slog. OpenUI aims to make the process fun,\nfast, and flexible. It's also a tool we're using at W&B to test and prototype\nour next generation tooling for building powerful applications on top of\nLLM's.\n\n## Overview\n\nOpenUI let's you describe UI using your imagination, then see it rendered\nlive. You can ask for changes and convert HTML to React, Svelte, Web\nComponents, etc. It's like v0 but open source and not as polished \ud83d\ude1d.\n\n## Live Demo\n\nTry the demo\n\n## Running Locally\n\nYou can also run OpenUI locally and use models available to Ollama. Install\nOllama and pull a model like CodeLlama, then assuming you have git and python\ninstalled:\n\n    \n    \n    git clone https://github.com/wandb/openui cd openui/backend # You probably want to do this from a virtual environment pip install . # This must be set to use OpenAI models, find your api key here: https://platform.openai.com/api-keys export OPENAI_API_KEY=xxx python -m openui\n\n### Docker Compose\n\n> DISCLAIMER: This is likely going to be very slow. If you have a GPU you may\n> need to change the tag of the ollama container to one that supports it. If\n> you're running on a Mac, follow the instructions above and run Ollama\n> natively to take advantage of the M1/M2.\n\nFrom the root directory you can run:\n\n    \n    \n    docker-compose up -d docker exec -it openui-ollama-1 ollama pull llava\n\nIf you have your OPENAI_API_KEY set in the environment already, just remove\n=xxx from the OPENAI_API_KEY line. You can also replace llava in the command\nabove with your open source model of choice (llava is one of the only Ollama\nmodels that support images currently). You should now be able to access OpenUI\nat http://localhost:7878.\n\nIf you make changes to the frontend or backend, you'll need to run docker-\ncompose build to have them reflected in the service.\n\n### Docker\n\nYou can build and run the docker file manually from the /backend directory:\n\n    \n    \n    docker build . -t wandb/openui --load docker run -p 7878:7878 -e OPENAI_API_KEY wandb/openui\n\nNow you can goto http://localhost:7878\n\n## Development\n\nA dev container is configured in this repository which is the quickest way to\nget started.\n\n### Codespace\n\nChoose more options when creating a Codespace, then select New with\noptions.... Select the US West region if you want a really fast boot time.\nYou'll also want to configure your OPENAI_API_KEY secret or just set it to xxx\nif you want to try Ollama (you'll want at least 16GB of Ram).\n\nOnce inside the code space you can run the server in one terminal: python -m\nopenui --dev. Then in a new terminal:\n\n    \n    \n    cd /workspaces/openui/frontend npm run dev\n\nThis should open another service on port 5173, that's the service you'll want\nto visit. All changes to both the frontend and backend will automatically be\nreloaded and reflected in your browser.\n\n### Ollama\n\nThe codespace installs ollama automaticaly and downloads the llava model. You\ncan verify Ollama is running with ollama list if that fails, open a new\nterminal and run ollama serve. In Codespaces we pull llava on boot so you\nshould see it in the list. You can select Ollama models from the settings gear\nicon in the upper left corner of the application. Any models you pull i.e.\nollama pull llama will show up in the settings modal.\n\n### Resources\n\nSee the readmes in the frontend and backend directories.\n\n## About\n\nOpenUI let's you describe UI using your imagination, then see it rendered\nlive.\n\n### Resources\n\nReadme\n\n### License\n\nApache-2.0 license\n\nActivity\n\nCustom properties\n\n### Stars\n\n10.4k stars\n\n### Watchers\n\n71 watching\n\n### Forks\n\n845 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Contributors 8\n\n## Languages\n\n  * TypeScript 59.1%\n  * Python 33.4%\n  * HTML 5.2%\n  * CSS 0.9%\n  * JavaScript 0.8%\n  * Dockerfile 0.3%\n  * Shell 0.3%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
