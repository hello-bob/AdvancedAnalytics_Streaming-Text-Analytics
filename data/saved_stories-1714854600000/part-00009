{"aid": "40256017", "title": "Are recruiters better than a coin flip at judging resumes? Here's the data", "url": "https://interviewing.io/blog/are-recruiters-better-than-a-coin-flip-at-judging-resumes", "domain": "interviewing.io", "votes": 13, "user": "taco-hands", "posted_at": "2024-05-04 08:49:20", "comments": 16, "source_title": "Are recruiters better than a coin flip at judging resumes? Here's the data.", "source_text": "Are recruiters better than a coin flip at judging resumes? Here's the data.\n\ninterviewing.io\n\n  * For employers\n  * Gift mock interviews\n  * Blog\n  * FAQ\n  * Log in\n\n# Are recruiters better than a coin flip at judging resumes? Here's the data.\n\nBy Aline Lerner and Peter Bergman | Published: May 1, 2024; Last updated: May 2, 2024\n\nThis post is a very exciting first for interviewing.io because it\u2019s about a\nproper experiment run by a real, live academic research lab. If you\u2019ve been\nreading my work for the past decade, you know that I\u2019ve always been something\nof an armchair researcher. I ran some experiments before starting\ninterviewing.io, and since then, my team and I have kept it up.\n\nOne of the experiments I ran before I founded interviewing.io was an attempt\nto figure out how good recruiters were at judging candidate quality based on\nresumes. I ran it 10 years ago and discovered that not only was everyone bad\nat judging resumes (about as accurate as flipping a coin), they all disagreed\nwith each other about what a good candidate looked like.\n\nEven though these results were shocking at the time, the study had some\nserious limitations. First, I had no objective measures for which candidates\nwere actually good. I was working as a recruiter at the time, so I knew whom I\nhad been able to place, but that\u2019s obviously not the be-all and end-all of\nengineering ability. Second, I had a non-representative sample of software\nengineers. Due to my brand, I had managed to attract a lot of excellent, non-\ntraditional candidates \u2014 engineers who were actually very good but didn\u2019t look\ngood on paper. These types of resumes are the hardest for recruiters to judge,\nand the data was full of them. Finally, my sample size wasn\u2019t that big: I\nended up with 716 data points in total, only about half of which came from\nrecruiters (the rest came from engineers and hiring managers \u2014 my original\nhypothesis was that they might be better at the task, but I was wrong...\neveryone was bad at judging resumes).\n\nSo, now that I\u2019m CEO of interviewing.io, with access to a lot more data,\nresources, and a team of excellent academics at Learning Collider, we decided\nto run this study again, but with a more rigorous treatment and better\nconditions, to see if we could replicate the results. This time, we focused\njust on recruiters, given that they\u2019re most often the gatekeepers who decide\nwhich candidates get an interview.\n\nBelow are all the details, but here\u2019s the TL;DR: we reproduced my results from\n10 years ago! Our new study showed that recruiters were only a bit better than\na coin flip at making value judgments, and they still all disagreed with each\nother about what a good candidate looks like.\n\nIn this piece, we also talk about:\n\n  * How far off recruiters were in their predictions and how much they disagreed with each other\n  * What recruiters say they look for vs. what the data shows they actually look for\n  * Why recruiters taking more time to parse resumes would lead to better outcomes (median parse time is just 31 seconds)\n  * Whether AI can do a better job at judging resumes (spoiler: yes, it can)\n\nThe rest of this piece is co-authored by Peter Bergman, Tushar Kundu, and\nKadeem Noray of Learning Collider.\n\n## The setup\n\nIn the real world, resumes (or LinkedIn profiles) are evaluated by recruiters\nin minutes \u2014 even seconds \u2014 and these evaluations are THE thing that\ndetermines who gets an interview.\n\nBut what do these word walls tell recruiters? How predictive are their\nevaluations of actual interview success? Ultimately, how good are recruiters\nat judging resumes?\n\nTo answer these questions, we designed a study approximating technical\nrecruiters\u2019 decisions in the real world. We asked^1 76 technical recruiters\n(both agency and in-house) to review and make judgments about 30 engineers\u2019\nresumes each, just as they would in their current roles.\n\nThey answered two questions per resume:\n\n  * Would you interview this candidate?^2 (Yes or No)\n  * What is the likelihood this candidate will pass the technical interview (as a percentage)?\n\nWe ended up with nearly 2,200 evaluations of over 1,000 resumes.\n\nThe resumes in this study belonged to interviewing.io users (with their\nconsent) \u2014 actual engineers currently on the job market.\n\nCollaborating on this study with interviewing.io is an ideal scenario,\nprecisely because outcome data were available for comparison purposes. Each\nengineer in this study has completed multiple mock interviews on the platform.\nPerformance in these interviews is quite predictive of performance in real\ninterviews: top performers (roughly the top 5% of users) on interviewing.io\nare 3X more likely to pass technical interviews at top-tier companies than\ncandidates from other sources. Even passing a single interview on\ninterviewing.io is a strong predictor of outcomes; it's associated with a 32%\nincrease in the chance of working at a FAANG company post-interview.\n\nOnce we had recruiters\u2019 evaluations of the resumes, we compared them to how\nthose engineers actually performed on interviewing.io: skills scores, feedback\nfrom interviewers, and ultimately, whether they passed or failed their mock\ninterviews.\n\n## Recruiters\u2019 resume judgments are just slightly better than a coin flip\n\n### Question #1: Would you interview this candidate?\n\nIn aggregate, recruiters in the study recommended 62% of candidates for an\ninterview. But how did recruiter evaluations stack up against candidates\u2019\nperformance on the platform?\n\nWe calculated recruiter accuracy by treating each candidate\u2019s first interview\n(pass/fail) as the truth, and recruiters\u2019 decision to interview as a\nprediction. It turns out that recruiters chose correctly 55% of the time,\nwhich is just slightly better than a coin flip.\n\n### Question #2: What is the likelihood this candidate will pass the technical\ninterview?\n\nRecruiters predicted the likelihood that each candidate would pass the\ntechnical interview. In most hiring processes, the technical interview follows\nthe recruiter call and determines whether candidates proceed to the onsite.\nBeing able to accurately predict which candidates will succeed at this stage\nis important and should inform the decision about whether to interview the\ncandidate or not.\n\nWhat we found most surprising is how far their predictions were from the\ntruth:\n\n  * When recruiters predicted the lowest probability of passing (0-5%), those candidates actually passed the technical interview with a 47% probability.\n  * When recruiters predicted the highest probability of passing (95-100%), those candidates actually passed with a 64% probability.\n\nBelow is a graph that shows recruiter predictions vs. actual performance. The\nx-axis is the bucketed recruiter rating. In other words, the first point is\nall the candidates that recruiters assigned a 0-5% likelihood of passing. The\ny-axis is the average interviewing.io pass rate for those candidates. The red\ndotted line represents 100% accuracy \u2013 in an ideal world, the higher a\nrecruiter's ranking of a candidate, the higher their actual performance would\nbe. The orange line represents reality \u2013 as you can see, there isn\u2019t much\ncorrespondence between how recruiters predicted candidates would perform and\ntheir actual performance.\n\nRecruiters\u2019 predictions below 40% underestimate these candidates by an average\nof 23 percentage points. Above 60%, they\u2019re overestimating by an average of 20\npercentage points. If this was predicting student performance, recruiters\nwould be off by two full letter grades.\n\n## Recruiters can\u2019t agree on what a good candidate looks like\n\nClearly, there is lots of noise in resume evaluations. Were recruiters\u2019 noisy\njudgments at least consistent when reviewing the same resumes?\n\nNearly 500 resumes were evaluated by more than one recruiter. Based on a\nrandom selection of two evaluations per resume, the overall likelihood of two\nrecruiters agreeing to either interview or not interview a given candidate was\n64%.\n\nSince recruiters also guess the probability a candidate will pass the\ntechnical interview, we can compare how different these guesses are for a\ngiven candidate. The average differential between two randomly selected\nrecruiters\u2019 evaluations of the same resume was 41 percentage points. So, let\u2019s\nsay one recruiter predicts a 30% probability the candidate would pass; another\nrecruiter evaluating the same resume would predict, on average, a 71%\nprobability of passing.\n\nTo further understand just how prevalent the disagreement is, we looked at the\nstandard deviations for across-candidate evaluations and same-candidate\nevaluations:\n\n  * 0.34 across different candidates\n  * 0.32 across the same candidates\n\nSo, when two recruiters are asked to judge the same candidate, their level of\ndisagreement is nearly the same as if they evaluated two completely different\ncandidates.\n\n## The most sought-after resume attributes\n\nDespite the noise and variability in the study\u2019s resume evaluations, there\nwere some characteristics that recruiters consistently favored: experience at\na top-tier tech^3 company (FAANG or FAANG-adjacent) and URM (underrepresented\nminority) status (in tech, this means being Black or Hispanic).\n\nMost predictive for Question #1 (whether a recruiter would want to interview\nthat candidate) was experience at a top company \u2014 these candidates were 35%\nmore likely to be picked. Black or Hispanic candidates are also associated\nwith an increased likelihood a recruiter would interview a candidate \u2014 by\n21%.^4\n\nWith Question #2 (how likely the candidate was to pass a technical interview),\nhaving a top company on your resume is associated with a 21% increase in the\nlikelihood that recruiters believe the candidate will pass the interview.\nCompared to the actual pass rates, recruiters\u2019 predictions of FAANG candidates\nare generally accurate (average 4 percentage point overestimate).^5\n\n## How do recruiters\u2019 stated reasons for rejecting candidates line up with\nactual rejection reasons?\n\nSo, we know what recruiters tend to favor, whether they\u2019d admit to it or not:\n1) FAANG/FAANG-adjacent experience and 2) URM status. But what\u2019s even more\ninteresting than why a recruiter would say yes is why they would say no.\n\nWhen we asked recruiters to judge a resume, we also asked them WHY they made\nthat decision.^6 Below are recruiters\u2019 stated reasons for rejecting\ncandidates. As you can see, \u201cmissing skill\u201d is the main reason by far, with\n\u201cno top firm\u201d a distant third.\n\nSo, then, we wondered... How do recruiters\u2019 stated reasons for rejecting\ncandidates line up with reality? To figure that out, we analyzed the resumes\nthat ended up in the rejected pile and looked at common traits.\n\nBelow is a graph of actual rejection reasons, based on our analysis. The main\nrejection reason isn\u2019t \u201cmissing skill\u201d \u2014 it\u2019s \u201cno top firm.\u201d This is followed,\nsomewhat surprisingly, but much less reliably (note the huge error bars), by\nhaving an MBA. \u201cNo top school\u201d and having a Master\u2019s degree come in at third\nand fourth. Note that these top four rejection reasons are all based on a\ncandidate\u2019s background, NOT their skill set.\n\nThe y-axis is the coefficient from regressing rejection on that variable. So,\na coefficient of Y for a given trait means that trait is associated with a\nY*100% percentage point increase in the likelihood of being rejected.\n\n## Slowing down is associated with better decisions\n\nAnother key piece of this study is time. In hiring settings, recruiters make\ndecisions quickly. Moving stacks of candidates through the funnel gives little\nroom to second-guess or even wait before determining whether or not to give a\ncandidate the opportunity to interview.\n\nIn our study, the median time spent on resume evaluations was just 31 seconds.\nBroken down further by Question #1 \u2014 whether or not the recruiter would\ninterview them \u2014 the median time spent was:\n\n  * 25 seconds for those advanced to a technical interview\n  * 44 seconds for those placed in the reject pile\n\nGiven the weight placed on single variables (e.g., experience at a top firm),\nhow quickly recruiters make judgments isn\u2019t surprising. But might they be more\naccurate if they slowed down? It turns out that spending more time on resume\nevaluations, notably >45 seconds, is associated with more accurate predictions\n\u2014 just spending 15 more seconds appears to increase accuracy by 34%.^7 It\ncould be that encouraging recruiters to slow down might result in more\naccurate resume screening.\n\n## Can AI do better?\n\nAs a gaggle of technologists and data geeks, we tested whether algorithms\ncould quiet the noise and inconsistencies in recruiters\u2019 predictions.\n\nWe trained two local, off-the-rack machine-learning models.^8\n\nJust like human recruiters, the models were trained to predict which\ncandidates would pass technical interviews. The training dataset was drawn\nfrom interviewing.io and included anonymized resume data, candidates\u2019 race and\ngender, and interview outcomes.^9\n\nWhen presented with out-of-sample candidate profiles, both models made\npredictions more accurately than human recruiters.\n\nRandom Forest was somewhat more accurate than recruiters when predicting lower\nperforming candidates. XGBoost, however, was more accurate across the board\nthan both the Random Forest model AND recruiters.\n\n## Where does this leave us?\n\nIn this section, when we say \u201cwe,\u201d we are speaking as interviewing.io, not as\nthe researchers involved in this study. Just FYI.\n\n### Advice for candidates\n\nAt interviewing.io, we routinely get requests from our users to add resume\nreview to our list of offerings. So far, we have declined to build it. Why?\nBecause we suspected that recruiters, regardless of what they say publicly,\nprimarily hunt for name brands on your resume. Therefore, highlighting your\nskills or acquiring new skills is unlikely to make a big difference in your\noutcomes.\n\nWe are sad to see the numbers back up our intuition that it mostly is about\nbrands.^10 As such, here\u2019s an actionable piece of advice: maintain a healthy\nskepticism when recruiters advise you to grow your skill set. Acquiring new\nskills will very likely make you a better engineer. But it will very likely\nNOT increase your marketability.\n\nIf enhancing your skill set won\u2019t help, what can you do to get in front of\ncompanies? We\u2019re in the midst of a brutal market, the likes of which we\nhaven\u2019t seen since the dot-com crash in 2000. According to anecdotes shared in\nour Discord community, even engineering managers from FAANGs are getting\nsomething like a 10% response rate when they apply to companies online. If\nthat\u2019s true, what chance do the rest of us have?\n\nWe strongly encourage anyone looking for work in this market, especially if\nyou come from a non-traditional background, to stop spending energy on\napplying online, full stop. Instead, reach out to hiring managers. The numbers\nwill be on your side there, as relatively few candidates are targeting hiring\nmanagers directly. We plan to write a full blog post on how to do this kind of\noutreach well, but this CliffsNotes version will get you started:\n\n  * Get a LinkedIn Sales Navigator account\n  * Make a target list of hiring managers at the companies you\u2019re interested in\n  * Figure out their emails (you can use a tool like RocketReach), and send them something short and personalized. Do not use LinkedIn. The same way that you don\u2019t live in LinkedIn, eng managers don\u2019t either. Talk about the most impressive thing you\u2019ve built. Ask them about their work, if you can find a blog post they\u2019ve written or a project they\u2019ve worked on publicly. Tie those two things together, and you\u2019ll see a much higher response rate. Writing these personalized emails takes time, of course, but in this market, it\u2019s what you need to do to stand out.\n\n### Advice for recruiters\n\nWe know that recruiting is a tough job, especially in the current climate,\nwhere there are more applicants than ever and fewer recruiters to parse\nthrough them. So, it rationally makes sense to us that a recruiter would spend\nno more than 30 seconds per resume and focus primarily on looking for top\nbrands.\n\nWe hope, though, that this piece may have given a measure of pause about your\napproach, and we\u2019d like to leave you with two actionable pieces of advice.\nFirst, if you do nothing else, please slow down. As you saw above, taking just\n15 extra seconds to read a resume could improve your accuracy by 34%.^11\n\nOur second piece of advice is this. Freada Kapor Klein from Kapor Capital\ncoined the term \u201cdistance traveled\u201d more than two decades ago. It refers to\nwhat someone accomplished, in the context of where they started. For instance,\nKapor Klein recommends that, in their admissions processes, universities\nshould consider not just the number of AP tests a candidate has passed but the\nnumber of AP tests divided by the total number offered at their high school.\nFor example, if an applicant took 5 AP tests and their school offered 27, that\npaints a very different picture from another applicant who also took 5 AP\ntests when that\u2019s the total number offered at their school. Kapor Capital uses\ndistance traveled as one of their metrics for determining which entrepreneurs\nto fund. One can easily apply this concept to hiring as well.\n\nTake a look at the resume below. \"John\" (name has been changed; scrubbed\nresume shared with permission) studied chemical engineering and worked his way\ninto software engineering by starting as a service engineer focused on pen\ntesting. In the meantime, he completed a bootcamp, attended the Bradfield\nSchool of Computer Science (a school dedicated to teaching computer science at\na depth beyond what many university programs, and certainly most bootcamps,\noffer), and ended up with a senior title in just three years.\n\nJohn was consistently rated poorly by recruiters but is one of the top\nperformers on interviewing.io.\n\nIt takes just a bit more time, so please spend a little longer reading\nresumes, and evaluate candidates\u2019 achievements in the context of where they\ncame from. Think about the denominator. But don\u2019t think for a moment that we\nrecommend that you lower the bar \u2014 absolutely not. On interviewing.io, we\nregularly see candidates like John objectively outperforming their FAANG\ncounterparts.\n\n### What this means for our industry\n\nThe last time I did this research, I wrote about how being bad at judging\nresumes isn\u2019t anything to be ashamed about and that comes down to the resume\nitself being a low-signal and not-very-useful document.\n\nI held that same opinion for the last decade (and even wrote a recent post\nabout how AI can\u2019t do recruiting)... right up until we ran this study and\nsuccessfully built two ML models that outperformed recruiters.\n\nSo, I stand corrected.\n\nThat said, in retrospect, it kind of makes sense. Resumes do carry some\nsignal, and you can uncover it if you carefully read what people write about\ntheir jobs and themselves and also analyze how they write it. Unfortunately,\nthis takes more time and effort to uncover than most human recruiters are able\nto devote. And, in retrospect, that\u2019s a good task for AI.\n\nAs I said in the AI piece I linked above, in order for AI to do useful\nrecruiting work, rather than just perpetuating the biases that human\nrecruiters hold, it needs a data set that contains some objective measure of\nperformance. Most recruiting AI models today do one of three things: glorified\nkeyword matching, training on what recruiters prefer (the outcome is whether a\nrecruiter would want to talk to the candidate, NOT whether the candidate is\ngood), or live on top of existing tools like ChatGPT. These three approaches\njust result in the wrong thing being done, faster.\n\nI hope that, in the not too distant future, we can use AI to make less-biased\ndecisions, using meaningful performance data. And I hope that this type of AI\nsolution can get adoption among the recruiting community.\n\nFootnotes:\n\n## Footnotes\n\n  1. Participating technical recruiters were paid a base rate and then received additional $$ for each accurate prediction. \u21a9\n\n  2. Different roles have different requirements. To correct for that, we asked each candidate to specify which eng role they were applying for: Software Engineer (back-end or full-stack), Mobile Engineer, Front-end Engineer, ML Engineer, Data Engineer, or Engineering Manager. Then we prompted recruiters to evaluate them specifically for that role. If no role was specified by the candidate, the default role to evaluate for was Software Engineer (back-end or full-stack). \u21a9\n\n  3. Top firms = Airbnb, Amazon, Anthropic, AWS, Apple, Asana, Atlassian, Bloomberg LP, Checkr, Coinbase, Coursera, Cruise, Dropbox, Etsy, Facebook, Flexport, GitHub, Google, Gusto, HashiCorp, Instacart, Instagram, Jane Street, Jump Trading, Khan Academy, LinkedIn, Lyft, Medium, Microsoft, Mozilla, Netflix, Oculus, OpenAI, Palantir, Peloton, Pinterest, Postmates, Quora, Reddit, Robinhood, Roblox, Salesforce, Segment, Slack, Snap, Snowflake, SpaceX, Spotify, Square, Stripe, Tesla, Thumbtack, TikTok, Twilio, Twitch, Twitter, Two Sigma, Uber, Udemy, Waymo, Whatsapp, Yelp, and Zoom. \u21a9\n\n  4. We corrected by FAANG & FAANG-adjacent experience (and all of our other variables) before making this statement, i.e., the effect existed for engineers from underrepresented backgrounds who did not have FAANG/FAANG-adjacent companies on their resumes. We expect that recruiters favor underrepresented minority candidates because of guidelines from their employers to focus on sourcing these types of candidates, as part of DEI initiatives. Discussion about the magnitude of this effect and its implications is out of scope of this piece. \u21a9\n\n  5. Interestingly, recruiters might penalize, for example, alternative education. Candidates with only alternative education pathways post-high school \u2014 coding bootcamps or digital certifications \u2014 appeared to be penalized by recruiters in this study. However, with limited observations (n=11), it\u2019s inconclusive without further study. \u21a9\n\n  6. That field was optional, so most of the reasons recruiters provided were in cases when they said no \u2014 presumably because the reasons for saying yes may have seemed self-evident. \u21a9\n\n  7. It\u2019s not that recruiters who generally take their time make more accurate judgements. Any recruiter slowing down might make them better at judging resumes! \u21a9\n\n  8. It\u2019s important to stress that neither algorithm was custom-built. The models, one using a Random Forest algorithm and the other an XGBoost algorithm, are distinct but interrelated approaches akin to Decision Tree algorithms. Decision trees sort data into groups based on features. Random forest algorithms combine multiple decision trees to improve predictions. XGBoost builds multiple decision trees one after another, with each new tree focusing on prediction errors from the previous trees. \u21a9\n\n  9. Training data excluded data in this study. We take user privacy very seriously, and we want to stress that all models were local and anonymized and that no data in this study was shared with cloud LLMs. \u21a9\n\n  10. To see a particularly egregious example of recruiters favoring brands over substance, take a close look at this fake resume that got a bunch of recruiter responses. And this one too. \u21a9\n\n  11. We haven\u2019t proven causality here, but when we just scoped our analysis to the same person, it appeared that taking more time did help (in other words, it\u2019s not just that recruiters who spend more time usually are more accurate; it\u2019s the added time). Still, this is something that merits more work, and we'll try to investigate it causally in the future. \u21a9\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\nGet instant access to anonymous mock interviews, salary negotiation, and the\nworld's largest library of interview replays.\n\n#### Related posts\n\nWe analyzed 100K technical interviews to see where the best performers work.\nHere are the results.\n\ninterviewing.io is finally out of beta. Anonymous technical interview practice\nfor all!\n\nResumes suck. Here's the data.\n\nThe 3 things that diversity hiring initiatives get wrong\n\n#### Stuff we write about\n\nRecessionSalary negotiationCompany NewsData Deep DivesDiversityGuest\nPostsHiring is brokenInterview tipsFor employers, how to hire better\n\n### Have interviews coming up? Study up on common questions and topics.\n\nMEDIUM\n\nData Structures and Algorithms\n\n### Binary Tree Upside Down\n\nGiven a binary tree where every node has either 0 or 2 children and every\nright node is a leaf node, flip it upside down turning it into a binary tree\nwhere all left nodes are leaf nodes.\n\nMEDIUM\n\nData Structures and Algorithms\n\n### Longest Substring with At Most K Distinct Characters\n\nGiven a string, find the length of the longest substring in it with no more\nthan K distinct characters.\n\nMEDIUM\n\nData Structures and Algorithms\n\n### Partition to K Equal Sum Subsets\n\nGiven an integer array nums and an integer k, return true if it is possible to\ndivide this array into k non-empty subsets whose sums are all equal.\n\nRecursion\n\nQuestions & tips\n\nStrings\n\nQuestions & tips\n\nUnion Find\n\nQuestions & tips\n\nTries\n\nQuestions & tips\n\nGraphs\n\nQuestions & tips\n\nInorder Traversal\n\nQuestions & tips\n\n## We know exactly what to do and say to get the company, title, and salary\nyou want.\n\nInterview prep and job hunting are chaos and pain. We can help. Really.\n\ninterviewing.io\n\nInterview Replays\n\nSystem design mock interviewGoogle mock interviewJava mock interviewPython\nmock interviewMicrosoft mock interview\n\nInterview Questions by Language/Company\n\nJava interview questionsPython interview questionsJavaScript interview\nquestionsAmazon interview questionsGoogle interview questionsMeta interview\nquestionsApple interview questionsNetflix interview questionsMicrosoft\ninterview questions\n\nPopular Interview Questions\n\nReverse stringLongest substring without repeating charactersLongest common\nsubsequenceContainer with most waterReverse linked listK closest points to\noriginKth smallest elementReverse words in a string\n\nGuides\n\nAmazon Leadership PrinciplesSystem Design Interview GuideFAANG Hiring Process\nGuide\n\nCompany\n\nFor engineersFor employersBlogPressFAQLog in\n\n\u00a92024 Interviewing.io Inc. Made with <3 in San Francisco.\n\nPrivacy PolicyTerms of Service\n\n", "frontpage": true}
