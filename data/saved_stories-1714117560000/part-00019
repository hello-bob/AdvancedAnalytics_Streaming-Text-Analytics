{"aid": "40164309", "title": "Snowflake Launches Text-Embedding Model for Retrieval Use Cases", "url": "https://www.snowflake.com/blog/introducing-snowflake-arctic-embed-snowflakes-state-of-the-art-text-embedding-family-of-models/", "domain": "snowflake.com", "votes": 14, "user": "xkgt", "posted_at": "2024-04-25 23:27:49", "comments": 0, "source_title": "Snowflake Launches Practical Text-Embedding Model for Retrieval use Cases", "source_text": "Snowflake Launches Practical Text-Embedding Model for Retrieval use Cases\n\nSkip to content\n\n### Subscribe to our blog!\n\n### Thank you for your submission.\n\nAuthor\n\nSnowflake AI Research\n\nShare\n\nSubscribe\n\nApr 16, 2024\n\n# Snowflake Launches the World\u2019s Best Practical Text-Embedding Model for\nRetrieval Use Cases\n\n  * Product and Technology\n\n    * AI & ML\n\nToday Snowflake is launching and open-sourcing with an Apache 2.0 license the\nSnowflake Arctic embed family of models. Based on the Massive Text Embedding\nBenchmark (MTEB) Retrieval Leaderboard, the largest Arctic embed model with\nonly 334 million parameters is the only one to surpass average retrieval\nperformance of 55.9, a feat only less practical to deploy models with over 1\nbillion parameters are able to achieve. The family of five models are\navailable on Hugging Face for immediate use and in Snowflake Cortex embed\nfunction (in private preview), available soon. The models, which deliver\nleading retrieval performance, give organizations a new edge when combining\nproprietary datasets with LLMs as part of a Retrieval Augmented Generation\n(RAG) or semantic search service. These impressive embedding models directly\nimplement the technical expertise, proprietary search knowledge, and research\nand development that Snowflake acquired last May via Neeva.\n\nHighlights from this family of embedding models include:\n\n  * A suite of models is available in five sizes ranging from x-small (xs) to large (l), which deliver state-of-the-art retrieval performance on the Massive Text Embedding Benchmark (MTEB) retrieval benchmark.\n  * The large (l) model, which stands at 334 million parameters, can beat the performance of closed-source models estimated to be roughly 4x in size, such as those available via OpenAI and Cohere text embedding APIs.\n  * The medium (m) sized model includes a long-context version that supports long document retrieval with extended context support of up to 8192 tokens.\n  * Compared to embedding models of comparable retrieval quality, the generally smaller size of each embedding model gives organizations an edge in reducing latency and lowering the total cost of ownership (TCO).\n\n### Scalable, accurate and efficient enterprise search\n\nEmbedding models are a crucial component of most modern AI workloads. From\npowering search to empowering RAG agents with proprietary information, the\nability to find the most relevant content is a foundation of AI systems. As\npart of Snowflake\u2019s commitment to AI excellence, we sought to deeply\nunderstand text embedding models to deliver the best experience for customer\nproducts leveraging Snowflake for their search needs. Leveraging our rich\nexpertise in search and the state-of-the-art research in this space, we set\nout to create the best open-source text embedding models from the ground up.\n\nStarting with our understanding of what is needed to make search great and\ncombining it with state-of-the-art research led us to rebuild text embedding\nfrom the ground up. As we alluded to in our recent discussion on how to train\na state-of-the-art embedding model and how to use Snowflake to process\ntraining data, we analyzed text embedding models from first principles and\nthen went ahead and trained these models end to end in Snowflake. This tooling\nand our deep understanding of search allowed us to create this suite of\nmodels, which outperform previous state-of-the-art models across all our\nembedding variants. Simply stated, our models are unmatched for their quality\nand TCO for enterprises of any size seeking to power their embedding\nworkflows.\n\n### Evaluating model quality\n\nEvaluating the quality of retrieval systems in a nonproprietary way can be\ndifficult; building on decades of academic research, the Massive Text\nEmbedding Benchmark (MTEB) has emerged as a standard benchmark. It is a\ncollection of tasks that measures the performance of retrieval systems across\nseven tasks: classification, clustering, pair classification, re-ranking,\nretrieval, STS (semantic textual similarity), and summarization. It includes\n56 datasets from various domains and with various text lengths. The Snowflake\nArctic embed models firmly focus on empowering real-world retrieval workloads,\nand as a result, we focus on the retrieval portion of MTEB.\n\nAs of April 2024, each of our models is ranked first among embedding models of\nsimilar size, and our largest model is only outperformed by open-source models\nwith over 20 times (and four times for closed models) the number of parameters\nor closed-source models that do not disclose any form of model\ncharacteristics. We understand that testing against a single benchmark can\nboth understate and overstate the impact of variation of embedding models on\ncustomer workloads, which is what Snowflake ultimately cares about. This is\nwhy we are working on new benchmarks focusing on real-world use cases with\nreal-world datasets in the next benchmarking phase. We will update the broader\ncommunity when we have a large enough sample.\n\n### The models\n\nSnowflake\u2019s arctic-embed models are a family of 5 text embedding models that\nrange in context window and size (number of parameters). The model sizes range\nfrom 23 to 334 million parameters, and one of the models has an extended\ncontext window to give enterprises a full range of options that best match\ntheir latency, cost, and retrieval performance requirements. In the table\nbelow, we cover model sizes and their relative performance improvement\ncompared to the prior best-performing model with a similar size. The metric\nfor quality is NDCG@10 on the MTEB Retrieval leaderboard, and we compare each\nmodel\u2019s performance with associated open-source models with similar parameter\ncount.\n\nAs of April 16, 2024, snowflake-arctic-embed-l is the most capable open-source\nmodel that can be used in production based on its performance-to-size ratio.\nModels that outperform snowflake-arctic-embed-l, such as SFR-Embedding-\nMistral, widely available among embedding model providers, have a vector\ndimensionality four times larger (1024 vs. 4096) and have over 20x more\nparameters (334 million vs. 7.1 billion). With the Apache 2 licensed Snowflake\nArctic embed family of models, organizations now have one more open\nalternative to black-box API providers such as Cohere, OpenAI, or Google.\n\nAs shown in the table below, snowflake-arctic-embed-l outperforms retrieval\nperformance, having one-quarter of the estimated parameters and one-third of\nthe dimensions compared to Open AI.\n\n### Using snowflake-arctic-embed\n\nOur models are incredibly easy to integrate with your existing search stack.\nAvailable directly from Hugging Face with an Apache 2 license, you can use\nthis model for retrieval with five simple lines of Python.\n\n    \n    \n    import torch from transformers import AutoModel, AutoTokenizer tokenizer = AutoTokenizer.from_pretrained('Snowflake/snowflake-arctic-embed-l') model = AutoModel.from_pretrained('Snowflake/snowflake-arctic-embed-l', add_pooling_layer=False) documents = ['Snowflake is the Data Cloud!', 'Arctic-embed is a state of the art text embedding model.'] embeddings = model(**tokenizer(documents, padding=True, truncation=True, return_tensors='pt', max_length=512))[0][:, 0]\n\n### Secret sauce\n\nNow, for the part, everyone is likely curious about: how are these models so\ngood? The answer is simple: effective techniques from web searching are\nequally applicable to training text embedding models. While we will discuss\nall our findings in an upcoming in-depth technical report, we outline many of\nour findings on how to train a state-of-the-art embedding model and how to use\nSnowflake to process training data. At a high level, we found that improved\nsampling strategies and competence-aware hard-negative mining could lead to\nmassive improvements in quality. We would also be remiss to say that we did\nnot build on the shoulders of giants, and in our training, we leverage\ninitialized models such as (bert-base-uncased, nomic-embed-\ntext-v1-unsupervised, e5-large-unsupervised, sentence-transformers/all-\nMiniLM-L6-v2).\n\nWhen our findings were combined with web search data and a quick iteration\nloop to gradually improve our model until the performance was something we\nwere excited to share with the broader community. Notably, none of our\nsignificant improvements came from a massive expansion of the computing\nbudget. All of our experiments used 8 H100 GPUs!\n\n### Looking ahead\n\nThis release is the first of many steps in our commitment to provide our\ncustomers with the best models for use in common enterprise use cases such as\nRAG and search. Using our deep expertise in search derived from the Neeva\nacquisition, combined with the incredible data processing power of Snowflake\u2019s\nData Cloud, we have shared with the community a set of efficient models that\nprovide the retrieval quality our customers require. We are rapidly expanding\nthe types of models we train and the targeted workloads. Not only are we\nworking on models, but we are also developing novel benchmarks that we will\nuse to guide the development of the next generation of models. If you are\ninterested in improving our models, have any suggestions, or want to join us\nin building the future, please reach out.\n\n#### Get started and learn more:\n\n  * Access any of the embedding models in Hugging Face\n  * Try snowflake-arctic-embed-m as part of Snowflake Cortex embed function\n  * Come to our meetup in San Francisco on Wednesday, April 17, to learn more about the Snowflake Arctic embed models and the people who made this release possible.\n\n### Acknowledgments\n\nWe thank our modeling engineers, Danmei Xu, Luke Merrick, Gaurav Nuti, and\nDaniel Campos, for making these great models possible. We thank our\nleadership, Himabindu Pucha, Kelvin So, Vivek Raghunathan, and Sridhar\nRamaswamy, for supporting this work. We also thank the open-source community\nfor producing the great models we could build on top of and making these\nreleases possible. Finally, we thank the researchers who created BEIR and MTEB\nbenchmarks. It is largely thanks to their tireless work to define what\n\u201cbetter\u201d looks like that we could improve model performance.\n\nShare\n\n## Related content\n\n  * Product and Technology\n\nNov 01, 2023\n\n### Fast, Easy and Secure LLM App Development With Snowflake Cortex\n\nGenerative AI (GenAI) and large language models (LLMs) are disrupting the way\nwe work at a global scale. Snowflake is excited to announce an innovative\nproduct lineup that brings our...\n\nFind Out More\n\nRead More\n\n  * Product and Technology\n\n    * AI & ML\n\nMar 05, 2024\n\n### Easy and Secure LLM Inference and Retrieval Augmented Generation (RAG)\nUsing Snowflake Cortex\n\nBecause human-machine interaction using natural language is now possible with\nlarge language models (LLMs), more...\n\nSee how\n\nRead More\n\n### The Essential Guide to Gen AI\n\nDownload now\n\n### Build Your Code in Snowflake Using Snowpark and Your Favorite...\n\nTo develop and deploy code with Snowpark, developers have always had the\nchance to work from their favorite IDE or notebook....\n\nMore to follow\n\nRead More\n\n### What is a Data Marketplace?\n\nA data marketplace, also known as a data exchange, is an online transactional\nlocation or store that facilitates the buying,...\n\nDiscover\n\nRead More\n\n### Inside the Data Cloud | Snowflake Blog\n\nFrom technical articles about AI and application strategies to real-world\nsuccess stories, the Snowflake Blog explores...\n\nLearn More\n\nRead More\n\n### The Data Cloud Explained\n\nLearn more about the Data Cloud, a global network that connects organizations\nto the data and apps most critical to their business.\n\nMore to follow\n\nRead More\n\n  * Platform\n\n    * Cloud Data Platform\n    * Pricing\n    * Marketplace\n    * Security & Trust\n  * Solutions\n\n    * Snowflake for Financial Services\n    * Snowflake for Advertising, Media, & Entertainment\n    * Snowflake for Retail & CPG\n    * Healthcare & Life Sciences Data Cloud\n    * Snowflake for Marketing Analytics\n  * Resources\n\n    * Resource Library\n    * Webinars\n    * Documentation\n    * Community\n    * Procurement\n    * Legal\n  * Explore\n\n    * News\n    * Blog\n    * Trending\n    * Guides\n    * Developers\n  * About\n\n    * About Snowflake\n    * Investor Relations\n    * Leadership & Board\n    * Snowflake Ventures\n    * Careers\n    * Contact\n\nSign up for Snowflake Communications\n\nThanks for signing up!\n\n  * Privacy Notice\n  * Site Terms\n  * Cookie Settings\n  * Do Not Share My Personal Information\n\n\u00a9 2024 Snowflake Inc. All Rights Reserved | If you\u2019d rather not receive future emails from Snowflake, unsubscribe here or customize your communication preferences\n\n", "frontpage": true}
