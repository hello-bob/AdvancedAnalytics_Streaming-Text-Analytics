{"aid": "40256319", "title": "Loseless Image Compression Algorithms Comparison", "url": "https://magform.dev/other/Loseless-Image-Compression-Algorithms/", "domain": "magform.dev", "votes": 1, "user": "magform", "posted_at": "2024-05-04 09:59:04", "comments": 2, "source_title": "Loseless Image Compression Algorithms", "source_text": "Loseless Image Compression Algorithms - Magform.dev\n\n# Lossless Image Compression Algorithms ComparisonPermalink\n\nThis post presents an analysis of different lossless image compression\ntechniques applied to four images: three real-world images (baboon, house, and\nlake) and a randomly generated image. All images are PGM files, i.e.\ngrayscaled images with 256 different gray levels, resulting in a bit-per-pixel\n(bpp) value of 8 before compression. To compress an image, it would be\nsufficient to identify an error model through which the image deviates from a\npredicted image generated by predicting each pixel from the previous ones.\nThis approach is likely to reduce the number of bits used, especially in\nimages where pixels are highly dependent on each other. Our example images\nare:\n\n## Calculation processPermalink\n\nFor the data processing process, MATLAB was used, specifically I relied on the\nfollowing functions defined by me:\n\n    \n    \n    function [predictor] = advanced_predictor_calc(image) [rows, cols] = size(image); predictor = zeros(rows, cols); for n = 1:rows for m = 1:cols if n == 1 && m == 1 predictor(n,m) = 128; % image(n,m) - 128; elseif n == 1 predictor(n,m) = image(n,m-1); elseif m == 1 predictor(n,m) = image(n-1,m); elseif m == cols predictor(n,m) = median([image(n-1,m), image(n,m-1), image(n-1,m-1)]); else predictor(n,m) = median([image(n-1,m), image(n,m-1), image(n-1,m+1)]); end end end end function [predictor] = simple_predictor_calc(image) [rows, cols] = size(image); predictor = zeros(rows, cols); for n = 1:rows for m = 1:cols if m == 1 && n == 1 predictor(n,m) = 128; elseif m == 1 predictor(n,m) = image(n-1, cols); else predictor(n,m) = image(n, m-1); end end end end function [prediction_error] = advanced_prediction_error_calc(image) predictor = advanced_predictor_calc(image); prediction_error = int16(image)-int16(predictor); end function [prediction_error] = simple_prediction_error_calc(image) predictor = simple_predictor_calc(image); prediction_error = int16(image)-int16(predictor); end function [entropy] = entropy_calc(distribution) [counts, ~] = histcounts(distribution, unique(distribution)); probabilities = counts / sum(counts); entropy = sum(probabilities .* log2(1./probabilities)); end function [ratio] = compression_ratio(entropy) ratio = 8/entropy; end function [zip_bpp] = zip_compression(image, image_dimension) % Convert the matrix to a PGM file and zip it matrix = uint8(image); imwrite(matrix, 'tmp.pgm'); if ispc system('tar -czf temp.tar.gz tmp.pgm'); else system('gzip -c tmp.pgm > temp.zip'); end info = dir('temp.zip'); nBytes = info.bytes; zip_bpp = nBytes * 8 / image_dimension / image_dimension; delete('tmp.pgm'); delete('temp.zip'); end function [bits] = expGolombSigned(N) if N>0 bits = expGolombUnsigned(2*N-1); else bits = expGolombUnsigned(-2*N); end end function [bits] = expGolombUnsigned(N) if N trailBits = dec2bin(N+1,floor(log2(N+1))); headBits = dec2bin(0,numel(trailBits)-1); bits = [headBits trailBits]; else bits = '1'; end end function ExpGolomb_bpp = ExpGolomb_calc(prediction_error, image_dimension) bitCount = 0; for n = 1:image_dimension for m = 1:image_dimension symbol = double(prediction_error(n, m)); codeword_length = numel(expGolombSigned(symbol)); bitCount = bitCount + codeword_length; end end ExpGolomb_bpp = bitCount / (image_dimension * image_dimension); end\n\n## ResultsPermalink\n\n### Entropy (BPP)Permalink\n\nImage| No compression| Simple Spatial Prediction Error| Advanced Spatial\nPrediction Error  \n---|---|---|---  \nbabbon| 7.358| 6.501| 6.642  \nhouse| 7.056| 3.312| 3.420  \nlake| 7.484| 5.609| 5.416  \nrandom| 7.992| 8.722| 8.570  \n  \n### Compression Analysis (BPP)Permalink\n\nImage| No compression| Zip Compression| Simple Spatial Prediction with\nExpGolomb| Advanced Spatial Prediction with ExpGolomb  \n---|---|---|---|---  \nbabbon| 8| 7.258| 8.724| 8.831  \nhouse| 8| 3.995| 3.428| 3.550  \nlake| 8| 6.877| 7.036| 6.671  \nrandom| 8| 8.002| 13.740| 13.516  \n  \n### Comparison of Prediction Error with Different AlgorithmsPermalink\n\n## DiscussionPermalink\n\n### Entropy AnalysisPermalink\n\nReal-world images (i.e. baboon, house and lake), as expected, have lower\nentropy compared to randomly generated images, indicating less randomness in\ntheir pixel values. This lower entropy suggests that real-world images may be\nmore predictable and thus more favorable to compression. Furthermore, within\nthe category of real-world images, it\u2019s worth noting that \u201chouse\u201d demonstrates\nlower entropy than both \u201cbaboon\u201d and \u201clake,\u201d which exhibit similar levels of\nentropy. This discrepancy can be attributed to the tendency for \u201chouse\u201d to\ndisplay lower pixel variance compared to the other two images.\n\n### Zip Compression AnalysisPermalink\n\nEncoding rates achieved using \u201czip\u201d compression are lower than entropy because\ndictionary encoding leverages the repetition of strings and symbols.\nTherefore, the theoretical limit is the entropy of symbol blocks, which in a\nsignal exhibiting statistical dependencies, is lower than the entropy of\nindividual symbols (once normalized for the block size). In a signal lacking\nstatistical dependencies, such as a randomly generated image, the entropy of\nsymbol blocks closely resembles the entropy of the image itself. In fact,\ncompression becomes disadvantageous compared to uncompressed images since it\nnecessitates consideration of the space occupied by the dictionary and other\ndata. Among real-world images, as expected, we observe that \u201chouse\u201d boasts\nsignificantly better compression due to its numerous repeated symbol strings,\nparticularly evident in the sky and houses. When comparing \u201cbaboon\u201d and\n\u201clake,\u201d despite \u201cbaboon\u201d having lower entropy than \u201clake,\u201d compression via zip\nyields better results for \u201clake.\u201d This is attributed to \u201clake\u201d exhibiting\ncolor groupings that repeat more frequently, for exemple the sky, despite\nhaving more color variance.\n\n### Simple Spatial Prediction Error EntropyPermalink\n\nLet\u2019s now experiment with simple spatial prediction. For each pixel, we\u2019ll use\nthe previous value as the predictor, except for the first pixel where we\u2019ll\nuse 128 as the prediction. Once again, we observe that compressing the\nrandomly generated image proves disadvantageous (compression ratio of about\n0.92) because predicting a pixel from the previous one is impossible since the\npixels are independent of each other. Furthermore, we notice that compression\nimproves as the colors of the image exhibit linear dependencies (without\nscattered and disjointed colors), thus achieving good prediction for \u201chouse\u201d\nand fair prediction for \u201clake.\u201d However, due to the high entropy of the\nbaboon\u2019s cheeks, compression of this particular image isn\u2019t very effective,\nthough still better than nothing.\n\n### Exp Golomb applied to Simple Spatial Prediction ErrorPermalink\n\nWhen encoding the spatial prediction error obtained using our linear\nprediction algorithm with an Exp Golomb algorithm, we encounter values that\nare considerably penalized compared to the entropy value of our prediction\nalgorithm. This entropy value could be approximated (with excellent accuracy)\nusing an arithmetic encoder. However, the advantage of using an Exp Golomb\nalgorithm over an arithmetic encoder lies in its much simpler implementation.\nNevertheless, this encoder significantly penalizes the compression of images\nwhere prediction errors are less concentrated around zero, i.e. with a higher\nsignal sparsity. Specifically, randomly generated images and baboons are the\ntwo images most affected, even resulting in a disadvantage compared to\nuncompressed images. This is because their errors are more scattered, as\neasily seen from the histogram. On the other hand, \u201chouse,\u201d with its narrow\nhistogram, receives a much lower penalty, thus giving results that are more\nthan acceptable and even better than zip compression.\n\n### Advanced Spatial Prediction Error EntropyPermalink\n\nUsing an advanced spatial prediction instead in which the color of a pixel is\nestimated considering various factors:\n\n  * if the pixel is the first in the first row, 128 is used\n  * if it\u2019s on the first row, the pixel to its left is used\n  * if it\u2019s on the first column, the pixel above it is used\n  * If we\u2019re on the last column, the prediction is based on the median value between the pixel above, the one to the left, and the one above and to the left\n  * In all the other cases, the pixel is predicted by calculating the median value between the pixel above, the one to the left, and the one above and to the right.\n\nUsing this prediction, we notice that compressing randomly generated images is\ndisadvantageous, but a little less than using linear prediction. Because the\nmedian of three randomly generated numbers from 0 to k is closer to half of k,\npredicting a randomly generated number from 0 to k from this value is more\nadvantageous than from a random number from 0 to k. With this prediction,\ncompression is better if the colors of the image are \u201cpatch-like.\u201d We thus see\nan improvement in images like \u201clake\u201d compared to all previously used\ncompression systems. However, for images like \u201chouse\u201d and \u201cbaboon,\u201d the result\nis a little worse compared to linear prediction, as both photos are more\nlinearly dependent than \u201cpatch-like.\u201d\n\n### Exp Golomb applied to Advanced Spatial Prediction ErrorPermalink\n\nWhen encoding the error obtained from advanced spatial prediction using an\nexp-Golomb algorithm, we observe a significant penalty compared to using an\narithmetic encoder, as seen in the previous case with the use of exp-Golomb.\nHowever, as mentioned earlier, this encoder heavily penalizes image\ncompression when the error is not tightly clustered around zero. Consequently,\nit performs better than uncompressed images only in the cases of \u201chouse\u201d and\n\u201clake,\u201d where the error is more tightly clustered around zero. Moreover, it\noutperforms compression with linear prediction only in the case of \u201clake,\u201d\nconsidering the highlighted details mentioned earlier.\n\n## NotePermalink\n\nI\u2019m not exactly an expert in image compression or anything like that, but due\nto my studies in computer engineering, I\u2019ve found myself delving into various\ncompression systems and conducting some comparisons. Thus, I\u2019ve taken the\nopportunity to create this post.\n\n  * Follow:\n  * GitHub\n  * Feed\n\n\u00a9 2024 Nicolas Ferraresso. Powered by Jekyll & Minimal Mistakes.\n\n", "frontpage": false}
