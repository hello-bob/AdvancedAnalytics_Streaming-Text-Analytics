{"aid": "40104231", "title": "How does ChatGPT work? As explained by the ChatGPT team", "url": "https://blog.pragmaticengineer.com/how-does-chatgpt-work/", "domain": "pragmaticengineer.com", "votes": 1, "user": "gregdoesit", "posted_at": "2024-04-21 09:10:07", "comments": 0, "source_title": "How does ChatGPT work? As explained by the ChatGPT team.", "source_text": "How does ChatGPT work? As explained by the ChatGPT team. - The Pragmatic\nEngineer\n\n### Menu\n\nClose\n\n  * Home\n  * Newsletter\n  * My Books\n  * Jobs Board\n  * Talent Collective\n  * Reading List\n  * Popular Articles\n  * Ethics statement\n  * Sponsoring\n  * Investing\n  * Now\n  * Contact me\n  * About\n  * The Tech Resume Book\n  * Mobile Apps at Scale Book\n  * RSS Feed\n  * twitter\n  * youtube\n  * linkedin\n  * Interview Preparation\n  * Templates\n  * Stats\n\nSubscribe\n\n  * Home\n  * Newsletter\n  * My Books\n  * Jobs Board\n  * Talent Collective\n  * Reading List\n  * Popular Articles\n  * Ethics statement\n  * Sponsoring\n  * Investing\n  * Now\n  * Contact me\n  * About\n  * The Tech Resume Book\n  * Mobile Apps at Scale Book\n  * RSS Feed\n  * twitter\n  * youtube\n  * linkedin\n  * Interview Preparation\n  * Templates\n  * Stats\n\n# How does ChatGPT work? As explained by the ChatGPT team.\n\nSee a longer version of this article here: Scaling ChatGPT: Five Real-World\nEngineering Challenges.\n\nSometimes the best explanations of how a technology solution works come from\nthe software engineers who built it. To explain how ChatGPT (and other large\nlanguage models) operate, I turned to the ChatGPT engineering team.\n\n> \"How does ChatGPT work, under the hood?\"\n\nI asked this from Evan Morikawa at OpenAI. Evan joined OpenAI in 2020 \u2013 two\nyears before ChatGPT launched \u2013 and has led the Applied engineering team as\nChatGPT launched and scaled. His team was the one that created ChatGPT, and\nEvan has been there from the very beginning.\n\nWith this, it\u2019s over to Evan. My questions are in italic.\n\n### A refresher on OpenAI, and on Evan\n\nEvan: how did you join OpenAI, and end up heading the Applied engineering\ngroup \u2013 which also builds ChatGPT?\n\nOpenAI is the creator of ChatGPT, which is just one of the business\u2019 products.\nOther shipped things include DALL\u00b7E 3 (image generation,) GPT-4 (an advanced\nmodel,) and the OpenAI API which developers and companies use to integrate AI\ninto their processes. ChatGPT and the API each expose several classes of\nmodel: GPT-3, GPT-3.5, and GPT-4.\n\nThe engineering, product, and design organization that makes and scales these\nproducts is called \"Applied,\" and was founded in 2020 when GPT-3 was released.\nIt\u2019s broadly chartered with safely bringing OpenAI's research to the world.\nOpenAI itself was founded in 2015 and at the company\u2019s core today is still a\nresearch lab with the goal of creating a safe and aligned artificial general\nintelligence (AGI.)\n\nThe Applied group and ChatGPT within OpenAI\n\nI joined OpenAI in October 2020 when Applied was brand new. I do not have a\nPhD in Machine Learning, and was excited by the idea of building APIs and\nengineering teams. I managed our entire Applied Engineering org from its\nearliest days through the launch and scaling of ChatGPT. We cover more on\nEvan\u2019s story in Inside OpenAI: how does ChatGPT ship so quickly?\n\n### How does ChatGPT work?\n\nFor those of us who have not spent the past few years building ChatGPT from\nthe ground up, how does it work?\n\nWhen you ask ChatGPT a question, several steps happen:\n\n  1. Input. We take your text from the text input.\n  2. Tokenization. We chunk it into tokens. A token roughly maps to a couple of unicode characters. You can think of it as a word.\n  3. Create embeddings. We turn each token into a vector of numbers. These are called embeddings.\n  4. Multiply embeddings by model weights. We then multiply these embeddings by hundreds of billions of model weights.\n  5. Sample a prediction. At the end of this multiplication, the vector of numbers represents the probability of the next most likely token. That next most likely token are the next few characters that spit out of ChatGPT.\n\nLet\u2019s visualize these steps. The first two are straightforward:\n\nSteps 1 and 2 of what happens when you ask ChatGPT a question\n\nNote that tokenization doesn\u2019t necessarily mean splitting text into words;\ntokens can be subsets of words as well.\n\nEmbeddings are at the heart of large language models (LLM), and we create them\nfrom tokens in the next step:\n\nStep 3 of what happens when you ask ChatGPT a question. Embeddings represent\ntokens as vectors. The values in the above embedding are examples\n\nAn embedding is a multi-dimensional representation of a token. We explicitly\ntrain some of our models to explicitly allow the capture of semantic meanings\nand relationships between words or phrases. For example, the embedding for\n\u201cdog\u201d and \u201cpuppy\u201d are closer together in several dimensions than \u201cdog\u201d and\n\u201ccomputer\u201d are. These multi-dimensional embeddings help machines understand\nhuman language more efficiently.\n\nModel weights are used to calculate a weighted embedding matrix, which is used\nto predict the next likely token. For this step, we need to use OpenAI\u2019s\nweight matrix, which consists of hundreds of billions of weights, and multiply\nit by a matrix we construct from the embeddings. This is a compute-intensive\nmultiplication.\n\nStep 4 of what happens when you ask ChatGPT a question. The weight matrix\ncontains hundreds of billions of model weights\n\nSampling a prediction is done after we do billions of multiplications. The\nfinal vector represents the probability of the next most likely token.\nSampling is when we choose the next most likely token and send it back to the\nuser. Each word that spits out of ChatGPT is this same process repeated over\nand over again many times per second.\n\nStep 5. We end up with the probability of the next most likely token (roughly\na word). We sample the next most probable word, based on pre-trained data, the\nprompt, and the text generated so far. Image source: What is ChatGPT doing and\nwhy does it work? by Stehen Wolfram\n\n#### Pretraining and inference\n\nHow do we generate this complex set of model weights, whose values encode most\nof human knowledge? We do it through a process called pretraining. The goal is\nto build a model that can predict the next token (which you can think of as a\nword), for all words on the internet.\n\nDuring pretraining, the weights are gradually updated via gradient descent,\nwhich is is a mathematical optimization method. An analogy of gradient descent\nis a hiker stuck up a mountain, who\u2019s trying to get down. However, they don\u2019t\nhave a full view of the mountain due to heavy fog which limits their view to a\nsmall area around them. Gradient descent would mean to look at the steepness\nof the incline from the hiker\u2019s current position, and proceed in the direction\nof the steepest descent. We can assume steepness is not obvious from simple\nobservation, but luckily, this hiker has an instrument to measure steepness.\nHowever, it takes time to do a single measurement and they want to get down\nbefore sunset. So, this hiker needs to decide how frequently to stop and\nmeasure the steepness, so they still can get down before sunset.\n\nOnce we have our model we can run inference on it, which is when we prompt the\nmodel with text. For example, the prompt could be: \u201cwrite a guest post for the\nPragmatic Engineer.\u201d This prompt then asks the model to predict the next most\nlikely token (word). It makes this prediction based on past input, and it\nhappens repeatedly, token by token, word by word, until it spits out your\npost!\n\nThis is Gergely again.\n\nHow ChatGPT works isn\u2019t magic, and is worth understanding. Like most people,\nmy first reaction to trying out ChatGPT was that it felt magical. I typed in\nquestions and got answers that felt like they could have come from a human!\nChatGPT works impressively well with human language, and has access to more\ninformation than any one human could handle. It\u2019s also good at programming-\nrelated questions, and there was a point when I questioned if ChatGPT could be\nmore capable than humans, even in areas like programming, where humans have\ndone better, until now?\n\nFor a sense of ChatGPT\u2019s limitations, you need to understand how it works.\nChatGPT and other LLMs do not \u201cthink\u201d and \u201cunderstand\u201d like humans. ChatGPT\ndoes, however, generate words based on what the next most likely word should\nbe, looking at the input, and everything generated so far.\n\nIn an excellent deepdive about how ChatGPT works, Stephen Wolfram \u2013 creator of\nthe expert search engine WolframAlpha \u2013 summarizes ChatGPT:\n\n> \u201cThe basic concept of ChatGPT is at some level rather simple. Start from a\n> huge sample of human-created text from the web, books, etc. Then train a\n> neural net to generate text that\u2019s \u201clike this\u201d. And in particular, make it\n> able to start from a \u201cprompt\u201d and then continue with text that\u2019s \u201clike what\n> it\u2019s been trained with\u201d.\n>\n> As we\u2019ve seen, the actual neural net in ChatGPT is made up of very simple\n> elements\u2014though billions of them. And the basic operation of the neural net\n> is also very simple, consisting essentially of passing input derived from\n> the text it\u2019s generated so far \u201conce through its elements\u201d (without any\n> loops, etc.) for every new word (or part of a word) that it generates.\n>\n> But the remarkable\u2014and unexpected\u2014thing is that this process can produce\n> text that\u2019s successfully \u201clike\u201d what\u2019s out there on the web, in books, etc.\n> (...)\n>\n> The specific engineering of ChatGPT has made it quite compelling. But\n> ultimately (at least until it can use outside tools) ChatGPT is \u201cmerely\u201d\n> pulling out some \u201ccoherent thread of text\u201d from the \u201cstatistics of\n> conventional wisdom\u201d that it\u2019s accumulated. But it\u2019s amazing how human-like\n> the results are.\u201d\n\nThis was an expert from the article Scaling ChatGPT: Five Real-World\nEngineering Challenges. Read that article for more details on the engineering\nchallenges behind building (and scaling) ChatGPT, as explained by Evan.\n\nOther explanations on how ChatGPT and LLMs work:\n\n  * What Is ChatGPT Doing ... and Why Does It Work? by Stephen Wolfram. Probably the best easy-to-follow, and yet in-depth article.\n  * Via spreadsheets / Excel (really! It's easy to follow)\n\n### Featured Pragmatic Engineer Jobs\n\n  * Director of Engineering at Synthesia. \u00a3180-250K. Remote (EU or UK).\n  * Senior Product Engineer (Backend) at Plain. \u00a375-110K + equity. Remote (Europe).\n  * Founding Engineer (Full-Stack) at Ethos. New York City.\n  * Senior Platform Infrastructure Engineer at Jack Henry. $130-280K. Remote (US).\n  * Senior Software Engineer at Tint. $140-250K. Remote (US or EU).\n  * Senior Software Engineer at Tally Health. $140-180K + equity. New York City or Remote (US).\n  * Senior Frontend Engineer at WellTheory. $130-150K + equity. Remote (Global).\n  * Senior Frontend Developer at ePilot. \u20ac70-100K. K\u00f6ln (Germany) or Remote (Germany).\n  * Senior Full Stack Engineer at ePilot. \u20ac70-100K. Remote (Germany).\n  * Senior Product Engineer, Frontend at Attio. \u00a390-125K + equity. Remote (Europe).\n  * Engineering Manager at Safi. Remote (UK).\n  * Staff Full Stack Engineer at POSH. $170-220K + equity. New York (US).\n\nThe above jobs score at least 9/12 on The Pragmatic Engineer Test. Browse more\nsenior engineer and engineering leadership roles with great engineering\ncultures, or add your own on The Pragmatic Engineer Job board and apply to\njoin The Pragmatic Engineer Talent Collective.\n\n#### The Software Engineer's Guidebook\n\nMy latest book, The Software Engineer's Guidebook is out. Here is what Tanya\nReilly, senior principal engineer and author of The Staff Engineer's Path says\nabout it:\n\n> \"From performance reviews to P95 latency, from team dynamics to testing,\n> Gergely demystifies all aspects of a software career. This book is well\n> named: it really does feel like the missing guidebook for the whole\n> industry.\"\n\nRead more reviews and Get the book here.\n\n#### Newsletter\n\nSubscribe to my weekly newsletter for engineering managers and senior\nengineers. It's the #1 technology newsletter on Substack.\n\nInterested in more? See popular articles and books I published.\n\n#### Newsletter\n\nSubscribe to my weekly newsletter for weekly articles with observations and\ndeep-dives about the software engineering industry. It's the #1 technology\nnewsletter on Substack and this is what people say about it.\n\n#### Gergely Orosz\n\nLast updated 21 April 2024. Originally published 21 Apr 2024.\n\nWriting The Pragmatic Engineer Newsletter and advisor at mobile.dev.\nPreviously at Uber, Microsoft, Skype, Skyscanner.\n\nAmsterdam, Netherlands\n\n#### Share this post\n\nTwitter Facebook\n\nby Gergely Orosz\n\n### Menu\n\nClose\n\n  * Home\n  * Newsletter\n  * My Books\n  * Jobs Board\n  * Talent Collective\n  * Reading List\n  * Popular Articles\n  * Ethics statement\n  * Sponsoring\n  * Investing\n  * Now\n  * Contact me\n  * About\n  * The Tech Resume Book\n  * Mobile Apps at Scale Book\n  * RSS Feed\n  * twitter\n  * youtube\n  * linkedin\n  * Interview Preparation\n  * Templates\n  * Stats\n\nSubscribe\n\n  * Home\n  * Newsletter\n  * My Books\n  * Jobs Board\n  * Talent Collective\n  * Reading List\n  * Popular Articles\n  * Ethics statement\n  * Sponsoring\n  * Investing\n  * Now\n  * Contact me\n  * About\n  * The Tech Resume Book\n  * Mobile Apps at Scale Book\n  * RSS Feed\n  * twitter\n  * youtube\n  * linkedin\n  * Interview Preparation\n  * Templates\n  * Stats\n\n  * Subscribe via email\n  * Subscribe in a reader\n\nThe Pragmatic Engineer \u00a9 2024\n\n## Weekend maintenance kicks an Italian bank offline for days\n\nIt is now day five that Italian bank Sella has its apps and internetbank down,\nafter a weekend systems update went south. The problem seems to be database-\nrelated: \u201csomething, something Oracle.\u201d...\n\n", "frontpage": false}
