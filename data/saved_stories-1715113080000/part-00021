{"aid": "40284000", "title": "Pre-Training Large Generalizable Models for Atomic Property Prediction", "url": "https://nima.sh/jmp/", "domain": "nima.sh", "votes": 1, "user": "zerojames", "posted_at": "2024-05-07 10:22:47", "comments": 0, "source_title": "Joint Multi-domain Pre-training (JMP)", "source_text": "Joint Multi-domain Pre-training (JMP)\n\n# From Molecules to Materials: Pre-training Large Generalizable Models for\nAtomic Property Prediction\n\nNima Shoghi^*1, Adeesh Kolluru^2, John R. Kitchin^2, Zachary W. Ulissi^1, C.\nLawrence Zitnick^1, Brandon M. Wood^1\n\n^1Fundamental AI Research (FAIR) at Meta, ^2Carnegie Mellon University ^*Work\ndone while at FAIR ICLR 2024\n\nPaper OpenReview Code Slides Poster BibTeX\n\n## Abstract\n\nFoundation models have been transformational in machine learning fields such\nas natural language processing and computer vision. Similar success in atomic\nproperty prediction has been limited due to the challenges of training\neffective models across multiple chemical domains. To address this, we\nintroduce Joint Multi-domain Pre-training (JMP), a supervised pre-training\nstrategy that simultaneously trains on multiple datasets from different\nchemical domains, treating each dataset as a unique pre-training task within a\nmulti-task framework. Our combined training dataset consists of ~120M systems\nfrom OC20, OC22, ANI-1x, and Transition-1x. We evaluate performance and\ngeneralization by fine-tuning over a diverse set of downstream tasks and\ndatasets including: QM9, rMD17, MatBench, QMOF, SPICE, and MD22. JMP\ndemonstrates an average improvement of 59% over training from scratch, and\nmatches or sets state-of-the-art on 34 out of 40 tasks. Our work highlights\nthe potential of pre-training strategies that utilize diverse data to advance\nproperty prediction across chemical domains, especially for low-data tasks.\n\n## Joint Multi-domain Pre-training\n\n## Pre-Training\n\nJMP concurrently trains on over 120 million diverse equilibrium and non-\nequilibrium atomic structures by framing each chemical domain as a separate\npre-training task in a multi-task framework. This enables learning\ngeneralizable representations of atomic interactions.\n\n## Fine-tuning\n\nOnce pre-trained, JMP can be fine-tuned on various downstream tasks by adding\nprediction heads for the target properties.\n\n## Benchmark Results\n\n## JMP achieves state-of-the-art performance on all MatBench and QMOF tasks in\nthe materials domain.\n\n## JMP demonstrates state-of-the-art results on force predictions for the MD22\nand SPICE large molecule datasets.\n\n## JMP outperforms previous pre-training methods on the QM9 small molecule\ndataset.\n\n## JMP yields state-of-the-art performance on force predictions for the rMD17\ndataset.\n\n## JMP achieves state-of-the-art performance on all MatBench and QMOF tasks in\nthe materials domain.\n\n## JMP demonstrates state-of-the-art results on force predictions for the MD22\nand SPICE large molecule datasets.\n\n## JMP outperforms previous pre-training methods on the QM9 small molecule\ndataset.\n\n## JMP yields state-of-the-art performance on force predictions for the rMD17\ndataset.\n\n## JMP achieves state-of-the-art performance on all MatBench and QMOF tasks in\nthe materials domain.\n\n## JMP demonstrates state-of-the-art results on force predictions for the MD22\nand SPICE large molecule datasets.\n\n## JMP outperforms previous pre-training methods on the QM9 small molecule\ndataset.\n\n## JMP Improves Downstream Performance\n\nJMP-L consistently outperforms the scratch variant across all tasks, with an\naverage improvement of 59%. JMP-L also matches or sets state-of-the-art on 34\nout of 40 tasks, demonstrating the effectiveness of pre-training on diverse\ndata for atomic property prediction.\n\n## JMP Enables Larger Models\n\nWe compare the relative improvement of JMP-L over JMP-S to the relative\nimprovement of GN-OC-L (scratch large model) over GN-OC-S (scratch small\nmodel). We observe that the larger models yields 21% average performance\nimprovement in JMP, compared to an 8% reduction in performance in the scratch\nsetting. This demonstrates that JMP enables training larger models without\nsacrificing performance.\n\n## JMP Speeds Up Downstream Training\n\nWhile JMP's pre-training is computationally expensive, taking around 34,400\nGPU hours, this upfront cost is recovered by enabling over 12x faster fine-\ntuning compared to training models from scratch.\n\n## Per-Layer t-SNE Visualizations\n\nWe visualize the learned representations in JMP using t-SNE. Each layer of the\nmodel is visualized, showing how the representations evolve as the model\nprocesses the input data across each GemNet layer.\n\n## Video Presentation\n\n## BibTeX\n\n    \n    \n    @article{shoghi2023molecules, title={From molecules to materials: Pre-training large generalizable models for atomic property prediction}, author={Shoghi, Nima and Kolluru, Adeesh and Kitchin, John R and Ulissi, Zachary W and Zitnick, C Lawrence and Wood, Brandon M}, journal={arXiv preprint arXiv:2310.16802}, year={2023} }\n\nThis page was built using the Academic Project Page Template which was adopted\nfrom the Nerfies project page. You are free to borrow the of this website, we\njust ask that you link back to this page in the footer. This website is\nlicensed under a Creative Commons Attribution-ShareAlike 4.0 International\nLicense.\n\nFavicon made by Flaticon\n\n", "frontpage": false}
