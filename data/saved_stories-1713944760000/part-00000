{"aid": "40139501", "title": "The question that no LLM can answer and why it is important", "url": "https://www.mindprison.cc/p/the-question-that-no-llm-can-answer", "domain": "mindprison.cc", "votes": 6, "user": "13years", "posted_at": "2024-04-24 01:43:46", "comments": 4, "source_title": "The question that no LLM can answer and why it is important", "source_text": "The question that no LLM can answer and why it is important\n\nShare this post\n\n#### The question that no LLM can answer and why it is important\n\nwww.mindprison.cc\n\n#### Discover more from Mind Prison\n\nA publication of (AI, Tech, Liberty and Reason) for the freedom enthusiast and\nthe curious seeking profoundly unique observations.\n\nContinue reading\n\nSign in\n\nNotes From the Desk\n\n# The question that no LLM can answer and why it is important\n\n### Notes From the Desk: No. 32 - 2024.04.23\n\nApr 23, 2024\n\n3\n\nShare this post\n\n#### The question that no LLM can answer and why it is important\n\nwww.mindprison.cc\n\n1\n\nShare\n\nNotes From the Desk are periodic posts that summarize recent topics of\ninterest or other brief notable commentary that might otherwise be a tweet or\nnote.\n\n##\n\nThe question that no LLM can answer\n\n\u201cWhich episode of Gilligan\u2019s Island was about mind reading?\u201d\n\nNo LLM properly answers this question unless it uses a web search. But how can\na LLM not know the answer if it was trained on essentially the entire internet\nof data and certainly most likely all the data in IMDB? The correct answer is\n\u201cSeer Gilligan\u201d, one of the most popular episodes.\n\nThe following are the responses from multiple top models:\n\n> Opus\n>\n> I apologize, but there is no episode of Gilligan's Island that specifically\n> focuses on mind reading.\n>\n> GPT 4\n>\n> The episode of \"Gilligan's Island\" that involves mind reading is titled\n> \"Ring Around Gilligan\". It is the sixth episode of the third season.\n>\n> Gemini 1.5\n>\n> The episode of Gilligan's Island that features mind reading is Season 2,\n> Episode 12 - \"The Mind Reader\".\n>\n> Llama 3\n>\n> After conducting a thorough search and reviewing episode guides, I found\n> that there is no episode of Gilligan's Island that specifically focuses on\n> mind reading.\n>\n> Llama 3 - asked to list and name episodes\n>\n> \"The Postman Cometh\" (March 13, 1965) \"Seer Gilligan\" (Wait, no! I\n> apologize, but there is no episode titled \"Seer Gilligan\") \"Love Me, Love My\n> Chicken\" (March 20, 1965)\n>\n> Please note that there is no episode titled \"Seer Gilligan\". If you have any\n> other questions or need help with a specific episode, feel free to ask!\n\nAll four models fail to give the correct answer. Two of the models hallucinate\nan answer and the remaining two deny such an episode exists. Interestingly,\nprobing Llama 3 a bit more reveals it is aware of the episode; however, it\nstill denies its existence. It seems to know there are 98 episodes, but the\nmajority of titles it lists are made up.\n\nIn the case of Llama 3, we can probe some of the training dataset using\nInfini-gram and verify that the episode does exist in the corpus along with\ntext describing the episode.\n\nWe also see another interesting data and training phenomenon revealed when\nLLMs are asked to provide a number between 1-100. They all converge to 42!\n\nPointed out by Information is Beautiful, a very interesting distribution forms\nwhen AI is asked to pick a number between 1 and 100. There is a heavy\nweighting toward picking the number \u201842\u2019. Likely, this is the Hitchhiker\u2019s\nGuide to the Galaxy effect. The number 42 is overrepresented or weighted in\nsome way through training, resulting in a higher propensity for the LLM to\nchoose 42.\n\n###\n\nWhat does it mean? Implications ...\n\nThe implications are that LLMs do not perform reasoning over data in the way\nthat most people conceive or desire.\n\nThere is no self-reflection of its information; it does not know what it knows\nand what it does not. The line between hallucination and truth is simply a\nprobability factored by the prevalence of training data and post-training\nprocesses like fine-tuning. Reliability will always be nothing more than a\nprobability built on top of this architecture.\n\nAs such, it becomes unsuitable as a machine to find rare hidden truths or\nvaluable neglected information. It will always simply converge toward popular\nnarrative or data. At best, it can provide new permutations of views of\nexisting well-known concepts, but it can not invent new concepts or reveal\nconcepts rarely spoken about.\n\n\u201cYou can't cache reality in some compressed lookup table. If a particular\noutcome was never in the training data, the model will perform a random guess\nwhich is quite limiting.\u201d\n\n\u2014 Chomba Bupe\n\nFurthermore, it can never be a system for absolute dependability. Mission-\ncritical systems that require deterministic, provably correct behavior are not\nsomething applicable to LLM automation or control. The problem is that LLMs\nare impressively convincing when they are wrong, which may lead to ill-advised\nadoption. What business wants to balance the books with a hallucinating\ncalculator?\n\nImplications:\n\n  1. Results are probabilities defined more by data prevalence than logic or reason.\n\n  2. It is indiscernible to what degree a LLM is reliable on a given question.\n\n  3. Not useful to find undiscovered truths or neglected but brilliant ideas.\n\n  4. Inability to theorize new concepts or discoveries.\n\nIt is substantially ironic that LLMs are failing at the primary use cases that\nare attracting billions of investment, but are rather proficient at the use\ncases we do not desire, such as destruction of privacy and liberty, a post-\ntruth society, social manipulation, the severance of human connection,\nfountains of noise, the devaluation of meaning, and a plethora of other\nsocietal issues.\n\nUnlike much of the internet now, there is a human mind behind all the content\ncreated here at Mind Prison. I typically spend hours to days on articles\nincluding creating the illustrations for each. I hope if you find them\nvaluable and you still appreciate the creations from the organic hardware\nwithin someone\u2019s head that you will consider subscribing. Thank you!\n\nNo compass through the dark exists without hope of reaching the other side and\nthe belief that it matters ...\n\nMind Prison is a reader-supported publication. You can also assist by sharing.\n\nShare\n\n3 Likes\n\n\u00b7\n\n1 Restack\n\n3\n\nShare this post\n\n#### The question that no LLM can answer and why it is important\n\nwww.mindprison.cc\n\n1\n\nShare\n\n1 Comment\n\nshon pan3 hrs agoThank you for being for humans. Have you thought of joining\nPauseAI discord, so we can better organize for resistance?Expand full\ncommentLikeReplyShare  \n---  \n  \nAI and the end to all things ...\n\nA technology revolution is about to unfold that will change the meaning of\neverything likely beyond what we are capable of perceiving or predicting\n\nFeb 3, 2023 \u2022\n\nDakara\n\n55\n\nShare this post\n\n#### AI and the end to all things ...\n\nwww.mindprison.cc\n\n27\n\nAI Singularity: The Hubris Trap\n\nWhat is the AI Singularity? Why is AI alignment an unsolvable paradox? Why do\nall concepts of AI containment fail?\n\nFeb 27, 2023 \u2022\n\nDakara\n\n14\n\nShare this post\n\n#### AI Singularity: The Hubris Trap\n\nwww.mindprison.cc\n\n15\n\nAI instructed brainwashing effectively nullifies conspiracy beliefs\n\nResearch study demonstrates disturbing capabilities\n\nApr 8\n\n7\n\nShare this post\n\n#### AI instructed brainwashing effectively nullifies conspiracy beliefs\n\nwww.mindprison.cc\n\n9\n\nReady for more?\n\n\u00a9 2024 Dakara\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": true}
