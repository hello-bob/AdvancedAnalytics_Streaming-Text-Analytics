{"aid": "40055836", "title": "The Feature Store for Machine Learning", "url": "https://www.hopsworks.ai/dictionary/feature-store", "domain": "hopsworks.ai", "votes": 1, "user": "jamesblonde", "posted_at": "2024-04-16 18:56:00", "comments": 0, "source_title": "What is a Feature Store: The Definitive Guide - Hopsworks", "source_text": "What is a Feature Store: The Definitive Guide - Hopsworks\n\nLoginContact\n\nDownload Now\n\nO'Reilly's Book \"Building ML Systems\" First Chapter Available!\n\nBack to the Index\n\n# Feature Store: The Definitive Guide\n\n### Table of Contents:\n\n1 - What is a feature store?  \n---  \n2 - What is a feature and why do I need a specialized store for them?  \n3 - How does the feature store relate to MLOps and ML systems?  \n4 - What problems does a feature store solve?  \na. Collaborative Development  \nb. Incremental Datasets  \nc. Backfill feature data and Training data  \nd. Point-in-Time Correct Training Data  \ne. History and Context for Online Models  \nf. Feature Reuse  \ng. Multiple Feature Computation Models  \nh. Validate feature data and monitor for drift  \ni. Taxonomy of Data Transformations  \nj. Query Engine for Point-in-Time Consistent Feature Data for Training  \nk. Query Engine for Low Latency Feature Data for Online Inference  \nl. Query Engine to find similar Feature Data using Embeddings  \n5 - Do I need a feature store?  \n6 - What is the difference between a feature store and a vector database?  \n7 - Is there an integrated feature store and vector database?  \n  \n## What is a feature store?\n\nA feature store is a data platform that supports the development and operation\nof machine learning systems by managing the storage and efficient querying of\nfeature data. Machine learning systems can be real-time, batch or stream\nprocessing systems, and the feature store is a general purpose data platform\nthat supports a multitude of write and read workloads, including batch and\nstreaming writes, to batch and point read queries, and even approximate\nnearest neighbour search. Feature stores also provide compute support to ML\npipelines that create and use features, including ensuring the consistent\ncomputation of features in different (offline and online) ML pipelines.\n\n## What is a feature and why do I need a specialized store for them?\n\nA feature is a measure property of some entity that has predictive power for a\nmachine learning model. Feature data is used to train ML models, and make\npredictions in batch ML systems and online ML systems. Features can be\ncomputed either when they are needed or in advance and used later for training\nand inference. Some of the advantages of storing features is that they can be\neasily discovered and reused in different models, reducing the cost and time\nrequired to build new machine learning systems. For real-time ML systems, the\nfeature store provides history and context to (stateless) online models.\nOnline models tend to have no local state, but the feature store can enrich\nthe set of features available to the model by providing, for example,\nhistorical feature data about users (retrieved with the user\u2019s ID) as well as\ncontextual data, such as what\u2019s trending. The feature store also reduces the\ntime required to make online predictions, as these features do not need to be\ncomputed on-demand, - they are precomputed.\n\n## How does the feature store relate to MLOps and ML systems?\n\nIn a MLOps platform, the feature store is the glue that ties together\ndifferent ML pipelines to make a complete ML system:\n\n  * feature pipelines compute features and then write those features (and labels/targets) to it;\n  * training pipelines read features (and labels/targets) from it;\n  * Inference pipelines can read precomputed features from it.\n\nThe main goals of MLOps are to decrease model iteration time, improve model\nperformance, ensure governance of ML assets (feature, models), and improve\ncollaboration. By decomposing your ML system into separate feature, training,\nand inference (FTI) pipelines, your system will be more modular with 3\npipelines that can be independently developed, tested, and operated. This\narchitecture will scale from one developer to teams that take responsibility\nfor the different ML pipelines: data engineers and data scientists typically\nbuild and operate feature pipelines; data scientists build and operate\ntraining pipelines, while ML engineers build and operate inference pipelines.\nThe feature store enables the FTI pipeline architecture, enabling improved\ncommunication within and between data, ML, and operations teams.\n\n## What problems does a feature store solve?\n\nThe feature store solves many of the challenges that you typically face when\nyou (1) deploy models to production, and (2) scale the number of models you\ndeploy to production, and (3) scale the size of your ML teams, including:\n\n  1. Support for collaborative development of ML systems based on centralized, governed access to feature data, along with a new unified architecture for ML systems as feature, training and inference pipelines;\n  2. Manage incremental datasets of feature data. You should be able to easily add new, update existing, and delete feature data using DataFrames. Feature data should be transparently and consistently replicated between the offline and online stores;\n  3. Backfill feature data from data sources using a feature pipeline and backfill training data using a training pipeline;\n  4. Provides history and context to stateless interactive (online) ML applications;\n  5. Feature reuse is made easy by enabling developers to select existing features and reuse them for training and inference in a ML model;\n  6. Support for diverse feature computation frameworks - including batch, streaming, and request-time computation. This enables ML systems to be built based on their feature freshness requirements;\n  7. Validate feature data written and monitor new feature data for drift;\n  8. A taxonomy for data transformations for machine learning based on the type of feature computed (a) reusable features are computed by model-independent transformations, (b) features specific to one model are computed by model-dependent transformations, and (c) features computed with request-time data are on-demand transformations. The feature store provide abstractions to prevent skew between data transformations performed in more than one ML pipeline.\n  9. A point-in-time consistent query engine to create training data from historical time-series feature data, potentially spread over many tables, without future data leakage;\n  10. A query engine to retrieve and join precomputed features at low latency for online inference using an entity key;\n  11. A query engine to find similar feature values using embedding vectors.\n\nThe table below shows you how the feature store can help you with common ML\ndeployment scenarios.\n\nFor just putting ML in production, the feature store helps with managing\nincremental datasets, feature validation and monitoring, where to perform data\ntransformations, and how to create point-in-time consistent training data.\nReal-Time ML extends the production ML scenario with the need for history and\ncontext information for stateless online models, low latency retrieval of\nprecomputed features, online similarity search, and the need for either stream\nprocessing or on-demand feature computation. For the ML at large scale, there\nis also the challenge of enabling collaboration between teams of data\nengineers, data scientists, and ML engineers, as well as the reuse of features\nin many models.\n\n### Collaborative Development\n\nFeature stores are the key data layer in a MLOps platform. The main goals of\nMLOps are to decrease model iteration time, improve model performance, ensure\ngovernance of ML assets (feature, models), and improve collaboration. The\nfeature store enables different teams to take responsibility for the different\nML pipelines: data engineers and data scientists typically build and operate\nfeature pipelines; data scientists build and operate training pipelines, while\nML engineers build and operate inference pipelines.\n\nThey enable the sharing of ML assets and improved communication within and\nbetween teams. Whether teams are building batch machine learning systems or\nreal-time machine learning systems, they can use shared language around\nfeature, training, and inference pipelines to describe their responsibilities\nand interfaces.\n\nA more detailed Feature Store Architecture is shown in the figure below.\n\nIts historical feature data is stored in an offline store (typically a\ncolumnar data store), its most recent feature data that is used by online\nmodels in an online store (typically a row-oriented database or key-value\nstore), and if indexed embeddings are supported, they are stored in a vector\ndatabase. Some feature stores provide the storage layer as part of the\nplatform, some have partial or full pluggable storage layers.\n\nThe machine learning pipelines (feature pipelines, training pipelines, and\ninference pipelines) read and write features/labels from/to the feature store,\nand prediction logs are typically also stored there to support feature/model\nmonitoring and debugging. Different data transformations (model-independent,\nmodel-dependent, and on-demand) are performed in the different ML pipelines,\nsee the Taxonomy of Data Transformations for more details.\n\n### Incremental Datasets\n\nFeature pipelines keep producing feature data as long as your ML system is\nrunning. Without a feature store, it is non-trivial to manage the mutable\ndatasets updated by feature pipelines - as the datasets are stored in the\ndifferent offline/online/vector-db stores. Each store has its own drivers,\nauthentication and authorization support, and the synchronization of updates\nacross all stores is challenging.\n\nFeature stores make the management of mutable datasets of features, called\nfeature groups, easy by providing CRUD (create/read/update/delete) APIs. The\nfollowing code snippet shows how to append, update & delete feature data in a\nfeature group using a Pandas DataFrame in Hopsworks. The updates are\ntransparently synchronized across all of the underlying stores - the\noffline/online/vector-db stores.\n\n    \n    \n    df = # read from data source, then perform feature engineering fg = fs.get_or_create_feature_group(name=\"query_terms_yearly\", version=1, description=\"Count of search term by year\", primary_key=['year', 'search_term'], partition_key=['year'], online_enabled=True ) fg.insert(df) # insert or update fg.commit_delete_record(df) # delete\n\nWe can also update the same feature group using a stream processing client\n(streaming feature pipeline). The following code snippet uses PySpark\nstreaming to update a feature group in Hopsworks. It computes the average\namount of money spent on a credit card, for all transactions on the credit\ncard, every 10 minutes. It reads its input data as events from a Kafka\ncluster.\n\n    \n    \n    df_read = spark.readStream.format(\"kafka\")...option(\"subscribe\", KAFKA_TOPIC_NAME).load() # Deserialize data from Kafka and create streaming query df_deser = df_read.selectExpr(....).select(...) # 10 minute window windowed10mSignalDF = df_deser \\ .selectExpr(...)\\ .withWatermark(...) \\ .groupBy(window(\"datetime\", \"10 minutes\"), \"cc_num\").agg(avg(\"amount\")) \\ .select(...) card_transactions_10m_agg =fs.get_feature_group(\"card_transactions_10m_agg\", version=1) query_10m = card_transactions_10m_agg.insert_stream(windowed10mSignalDF)\n\nSome feature stores also support defining columns as embeddings that are\nindexed for similarity search. The following code snippet writes a DataFrame\nto a feature group in Hopsworks, and indexes the \u201cembedding_body\u201d column in\nthe vector database. You need to create the vector embedding using a model,\nadd it as a column to the DataFrame, and then write the DataFrame to\nHopsworks.\n\n    \n    \n    from hsfs import embedding from sentence_transformers import SentenceTransformer model = SentenceTransformer('all-MiniLM-L6-v2') df = # read from data source, then perform feature engineering embeddings_body = model.encode(df[\"Article\"]) df[\"embedding_body\"] = pd.Series(embeddings_body.tolist()) emb = embedding.EmbeddingIndex() emb.add_embedding(\"embedding_body\", len(df[\"embedding_body\"][0])) news_fg = fs.get_or_create_feature_group( name=\"news_fg\", embedding_index=emb, primary_key=[\"id\"], version=1, online_enabled=True ) news_fg.insert(df)\n\n### Backfill feature data and Training Data\n\nBackfilling is the process of recomputing datasets from raw, historical data.\nWhen you backfill feature data, backfilling involves running a feature\npipeline with historical data to populate the feature store. This requires\nusers to provide a start_time and an end_time for the range of data that is to\nbe backfilled, and the data source needs to support timestamps, e.g., Type 2\nslowly changing dimensions in a data warehouse table.\n\nThe same feature pipeline used to backfill features should also process \u201clive\u201d\ndata. You just point the feature pipeline at the data source and the range of\ndata to backfill (e.g., backfill the daily partitions with all users for the\nlast 180 days). Both batch and streaming feature pipelines should be able to\nbackfill features. Backfilling features is important because you may have\nexisting historical data that can be leveraged to create training data for a\nmodel. If you couldn\u2019t backfill features, you could start logging features in\nyour production system and wait until sufficient data has been collected\nbefore you start training your model.\n\n### Point-in-Time Correct Training Data\n\nIf you want to create training data from time-series feature data without any\nfuture data leakage, you will need to perform a temporal join, sometimes\ncalled a point-in-time correct join.\n\nFor example, in the figure below, we can see that for the (red) label value,\nthe correct feature values for Feature A and Feature B are 4 and 6,\nrespectively. Data leakage would occur if we included feature values that are\neither the pink (future data leakage) or orange values (stale feature data).\nIf you do not create point-in-time correct training data, your model may\nperform poorly and it will be very difficult to discover the root cause of the\npoor performance.\n\nIf your offline store supports AsOf Joins, feature retrieval involves joining\nFeature A and Feature B from their respective tables AsOf the timestamp value\nfor each row in the Label table. The SQL query to create training data is an\n\u201cAS OF LEFT JOIN\u201d, as this query enforces the invariant that for every row in\nyour Label table, there should be a row in your training dataset, and if there\nare missing feature values for a join, we should include NULL values (we can\nlater impute missing values in model-dependent transformations). If your\noffline store does not support AsOf Joins, you can write alternative windowing\ncode using state tables.\n\nAs both AsOf Left joins and window tables result in complex SQL queries, many\nfeature stores provide domain-specific language (DSL) support for executing\nthe temporal query. The following code snippet, in Hopsworks, creates point-\nin-time-consistent training data by first creating a feature view. The code\nstarts by (1) selecting the columns to use as features and label(s) to use for\nthe model, then (2) creates a feature view with the selected columns, defining\nthe label column(s), and (3) uses the feature view object to create a point-\nin-time correct snapshot of training data.\n\n    \n    \n    fg_loans = fs.get_feature_group(name=\"loans\", version=1) fg_applicants = fs.get_feature_group(name=\"applicants\", version=1) select= fg_loans.select_except([\"issue_d\", \"id\"]).join(\\ fg_applicants.select_except([\"earliest_cr_line\", \"id\"])) fv = fs.create_feature_view(name=\"loans_approvals\", version=1, description=\"Loan applicant data\", labels=[\"loan_status\"], query=select ) X_train, X_test, y_train, y_test = fv.train_test_split(test_size=0.2) #.... model.fit(X_train, y_train)\n\nThe following code snippet, in Hopsworks, uses the feature view we just\ndefined to create point-in-time consistent batch inference data. The model\nmakes predictions using the DataFrame df containing the batch inference data.\n\n    \n    \n    fv = fs.get_feature_view(name=\"loans_approvals\", version=fv_version) df = fv.get_batch_data(start_time=\u201d2023-12-23 00:00\u201d, end_time=NOW) predictions_df = model.predict(df)\n\n### History and Context for Online Models\n\nOnline models are often hosted in model-serving infrastructure or stateless\n(AI-enabled) applications. In many user-facing applications, the actions taken\nby users are \u201cinformation poor\u201d, but we would still like to use a trained\nmodel to make an intelligent decision. For example, in Tiktok, a user click\ncontains a limited amount of information - you could not build the world\u2019s\nbest real-time recommendation system using just a single user click as an\ninput feature.\n\nThe solution is to use the user\u2019s ID to retrieve precomputed features from the\nonline store containing the user's personal history as well as context\nfeatures (such as what videos or searches are trending). The precomputed\nfeatures returned enrich any features that can be computed from the user input\nto build a rich feature vector that can be used to train complex ML models.\nFor example, in Tiktok, you can retrieve precomputed features about the 10\nmost recent videos you looked at - their category, how long you engaged for,\nwhat\u2019s trending, what your friends are looking at, and so on. In many examples\nof online models, the entity is a simple user or product or booking. However,\noften you will need more complex data models, and it is beneficial if your\nonline store supports multi-part primary keys (see Uber talk).\n\n### Feature Reuse\n\nA common problem faced by organizations when they build their first ML models\nis that there is a lot of bespoke tooling, extracting data from existing\nbackend systems so that it can be used to train a ML model. Then, when it\ncomes to productionizing the ML model, more data pipelines are needed to\ncontinually extract new data and compute features so that the model can make\ncontinual predictions on the new feature data.\n\nHowever, after the first set of pipelines have been written for the first\nmodel, organizations soon notice that one or more features used in an earlier\nmodel are needed in a new model. Meta reported that in their feature store\n\u201cmost features are used by many models\u201d, and that the most popular 100\nfeatures are reused in over 100 different models. However, for expediency,\ndevelopers typically rewrite the data pipelines for the new model. Now you\nhave different models re-computing the same feature(s) with different\npipelines. This leads to waste, and a less maintainable (non-DRY) code base.\n\nThe benefits of feature reuse with a feature store include higher quality\nfeatures through increased usage and scrutiny, reduced storage costs - and\nless feature pipelines. In fact, the feature store decouples the number of\nmodels you run in production from the number of feature pipelines you have to\nmaintain. Without a feature store, you typically write at least one feature\npipeline per model. With a (large enough) feature store, you may not need to\nwrite any feature pipeline for your model if the features you need are already\navailable there.\n\n### Multiple Feature Computation Models\n\nThe feature pipeline typically does not need GPUs, may be a batch program or\nstreaming program, and may process small amounts of data with Pandas or Polars\nor large amounts of data with a framework such as Spark or DBT/SQL. Streaming\nfeature pipelines can be implemented in Python (Bytewax) or more commonly in\ndistributed frameworks such as PySpark, with its micro-batch computation\nmodel, or Flink/Beam with their lower latency per-event computation model.\n\nThe training pipeline is typically a Python program, as most ML frameworks are\nwritten in Python. It reads features and labels as input, trains a model and\noutputs the trained model (typically to a model registry).\n\nAn inference pipeline then downloads a trained model and reads features as\ninput (some may be computed from the user\u2019s request, but most will be read as\nprecomputed features from the feature store). Finally, it uses the features as\ninput to the model to make predictions that are either returned to the client\nwho requested them or stored in some data store (often called an inference\nstore) for later retrieval.\n\n### Validate Feature Data and Monitor for Drift\n\nGarbage-in, garbage out is a well known adage in the data world. Feature\nstores can provide support for validating feature data in feature pipelines.\nThe following code snippet uses the Great Expectations library to define a\ndata validation rule that is applied when feature data is written to a feature\ngroup in Hopsworks.\n\n    \n    \n    df = # read from data source, then perform feature engineering # define data validation rules in Great Expectations ge_suite = ge.core.ExpectationSuite( expectation_suite_name=\"expectation_suite_101\" ) ge_suite.add_expectation( ExpectationConfiguration( expectation_type=\"expect_column_values_to_not_be_null\", kwargs={\"column\":\"'search_term'\"} ) ) fg = fs.get_or_create_feature_group(name=\"query_terms_yearly\", version=1, description=\"Count of search term by year\", primary_key=['year', 'search_term'], partition_key=['year'], online_enabled=True, expectation_suite=ge_suite ) fg.insert(df) # data validation rules executed in client before insertion\n\nThe data validation results can then be viewed in the feature store, as shown\nbelow. In Hopsworks, you can trigger alerts if data validation fails, and you\ncan decide whether to allow the insertion or fail the insertion of data, if\ndata validation fails.\n\nFeature monitoring is another useful capability provided by many feature\nstores. Whether you build a batch ML system or an online ML system, you should\nbe able to monitor inference data for the system\u2019s model to see if it is\nstatistically significantly different from the model\u2019s training data (data\ndrift). If it is, you should alert users and ideally kick-off the retraining\nof the model using more recent training data.\n\nHere is an example code snippet from Hopsworks for defining a feature\nmonitoring rule for the feature \u201camount\u201d in the model\u2019s prediction log\n(available for both batch and online ML systems). A job is run once per day to\ncompare inference data for the last week for the amount feature, and if its\nmean value deviates more than 50% from the mean observed in the model\u2019s\ntraining data, data drift is flagged and alerts are triggered.\n\n    \n    \n    # Compute statistics on a prediction log as a detection window fg_mon = pred_log.create_feature_monitoring(\"name\", feature_name = \"amount\", job_frequency = \"DAILY\") .with_detection_window(row_percentage=0.8, time_offset =\"1w\") # Compare feature statistics with a reference window - e.g., training data fg_mon.with_reference_training_dataset(version=1).compare_on( metric = \"mean\", threshold=50)\n\n### Taxonomy of Data Transformations\n\nWhen data scientists and data engineers talk about data transformations, they\nare not talking about the same thing. This can cause problems in\ncommunication, but also in the bigger problem of feature reuse in feature\nstores. There are 3 different types of data transformations, and they belong\nin different ML pipelines.\n\nData transformations, as understood by data engineers, is a catch-all term\nthat covers data cleansing, aggregations, and any changes to your data to make\nit consumable by BI or ML. These data transformations are called model-\nindependent transformations as they produce features that are reusable by many\nmodels.\n\nIn data science, data transformations are a more specific term that refers to\nencoding a variable (categorical or numerical) into a numerical format,\nscaling a numerical variable, or imputing a value for a variable, with the\ngoal of improving the performance of your ML model training. These data\ntransformations are called model-dependent transformations and they are\nspecific to one model.\n\nFinally, there are data transformations that can only be performed at runtime\nfor online models as they require parameters only available in the prediction\nrequest. These data transformations are called on-demand transformations, but\nthey may also be needed in feature pipelines if you want to backfill feature\ndata from historical data.\n\nThe feature store architecture diagram from earlier shows that model-\nindependent transformations are only performed in feature pipelines (whether\nbatch or streaming pipelines). However, model-dependent transformations are\nperformed in both training and inference pipelines, and on-demand\ntransformations can be applied in both feature and online inference pipelines.\nYou need to ensure that equivalent transformations are performed in both\npipelines - if there is skew between the transformations, you will have model\nperformance bugs that will be very hard to identify and debug. Feature stores\nhelp prevent this problem of online-offline skew. For example, model-dependent\ntransformations can be performed in scikit-learn pipelines or in feature views\nin Hopsworks, ensuring consistent transformations in both training and\ninference pipelines. Similarly, on-demand transformations are version-\ncontrolled Python or Pandas user-defined functions (UDFs) in Hopsworks that\nare applied in both feature and online inference pipelines.\n\n### Query Engine for Point-in-Time Consistent Feature Data for Training\n\nFeature stores can use existing columnar data stores and data processing\nengines, such as Spark, to create point-in-time correct training data.\nHowever, as of December 2023, Spark, BigQuery, Snowflake, and Redshift do not\nsupport the ASOF LEFT JOIN query that is used to create training data from\nfeature groups. Instead, they have to implement stateful windowed approaches.\n\nThe other main performance bottleneck with many current data warehouses is\nthat they provide query interfaces to Python with either a JDBC or ODBC API.\nThese are row-oriented protocols, and data from the offline store needs to be\npivoted from columnar format to row-oriented, and then back to column-oriented\nin Pandas. Arrow is now the backing data format for Pandas 2.+.\n\nIn open-source, reproducible benchmarks by KTH, Karolinska, and Hopsworks,\nthey showed the throughput improvements over a specialist DuckDB/ArrowFlight\nfeature query engine that returns Pandas DataFrames to Python clients in\ntraining and batch inference pipelines. We can see from the table below that\nthroughput improvements of 10-45X JDBC/ODBC-based query engines can be\nachieved.\n\n### Query Engine for Low Latency Feature Data for Online Inference\n\nThe online feature store is typically built on existing low latency row-\noriented data stores. These could be key-value stores such as Redis or Dynamo\nor a key-value store with a SQL API, such as RonDB for Hopsworks.\n\nThe process of building the feature vector for an online model also involves\nmore than just retrieving precomputed features from the online feature store\nusing an entity ID. Some features may be passed as request parameters directly\nand some features may be computed on-demand - using either request parameters\nor data from some 3rd party API, only available at runtime. These on-demand\ntransformations may even need historical feature values, inference helper\ncolumns, to be computed.\n\nIn the code snippet below, we can see how an online inference pipeline takes\nrequest parameters in the predict method, computes an on-demand feature,\nretrieves precomputed features using the request supplied id, and builds the\nfinal feature vector used to make the prediction with the model.\n\n    \n    \n    def loc_diff(event_ts, cur_loc) : return grid_loc(event_ts, cur_loc) def predict(id, event_ts, cur_loc, amount) : f1 = loc_diff(event_ts, cur_loc) df = feature_view.get_feature_vector( entry = {\"id\":id}, passed_features ={\"f1\" : f1, \"amount\" : amount} ) return model.predict(df)\n\nIn the figure below, we can see important system properties for online feature\nstores. If you are building your online AI application on top of an online\nfeature store, it should have LATS properties (low Latency, high Availability,\nhigh Throughput, and scalable Storage), and it should also support fresh\nfeatures (through streaming feature pipelines).\n\nSome other important technical and performance considerations here for the\nonline store are:\n\n  * Projection pushdown can massively reduce network traffic and latency. When you have popular features in feature groups with lots of columns, your model may only require a few features. Projection pushdown only returns the features you need. Without projection pushdown (e.g., most key-value stores), the entire row is returned and the filtering is performed in the client. For rows of 10s of KB, this could mean 100s of times more data is transferred than needed, negatively impacting latency and throughput (and potentially also cost).\n  * Your feature store should support a normalized data model, not just a star schema. For example, if your user provides a booking reference number that is used as the entity ID, can your online store also return features for the user and products referenced in the booking, or does either the user or application have to provide the user ID and product ID? For high performance, your online store should support pushdown LEFT JOINs to reduce the number of database round trips for building features from multiple feature groups.\n\n### Query Engine to find similar Feature Data using Embeddings\n\nReal-time ML systems often use similarity search as a core functionality. For\nexample, personalized recommendation engines typically use similarity search\nto generate candidates for recommendation, and then use a feature store to\nretrieve features for the candidates, before a ranking model personalizes the\ncandidates for the user.\n\nThe example code snippet below is from Hopsworks, and shows how you can search\nfor similar rows in a feature group with the text \u201cHappy news for today\u201d in\nthe embedding_body column.\n\n    \n    \n    news_desc = \"Happy news for today\" df = news_fg.find_neighbors(model.encode(news_desc), k=3) # df now contains rows with 'news_desc' values that are most similar to 'news_desc'\n\n## Do I need a feature store?\n\nFeature stores have historically been part of big data ML platforms, such as\nUber\u2019s Michelangelo, that manage the entire ML workflow, from specifying\nfeature logic, to creating and operating feature pipelines, training\npipelines, and inference pipelines.\n\nMore recent open-source feature stores provide open APIs enabling easy\nintegration with existing ML pipelines written in Python, Spark, Flink, or\nSQL. Serverless feature stores further reduce the barriers of adoption for\nsmaller teams. The key features needed by most teams include APIs for\nconsistent reading/writing of point-in-time correct feature data, monitoring\nof features, feature discovery and reuse, and the versioning and tracking of\nfeature data over time. Basically, feature stores are needed for MLOps and\ngovernance. Do you need Github to manage your source code? No, but it helps.\nSimilarly, do you need a feature store to manage your features for ML? No, but\nit helps.\n\n## What is the difference between a feature store and a vector database?\n\nBoth feature stores and vector databases are data platforms used by machine\nlearning systems. The feature store stores feature data and provides query\nAPIs for efficient reading of large volumes feature data (for model training\nand batch inference) and low latency retrieval of feature vectors (for online\ninference). In contrast, a vector database provides a query API to find\nsimilar vectors using approximate nearest neighbour (ANN) search.\n\nThe indexing and data models used by feature stores and vector databases are\nvery different. The feature store has two data stores - an offline store,\ntypically a data warehouse/lakehouse, that is a columnar database with indexes\nto help improve query performance such as (file) partitioning based on a\npartition column, skip indexes (skip files when reading data using file\nstatistics), and bloom filters (which files to skip when looking for a row).\nThe online store is row-oriented database with indexes to help improve query\nperformance such as a hash index to lookup a row, a tree index (such as a\nb-tree) for efficient range queries and row lookups, and a log-structured\nmerge-tree (for improved write performance). In contrast, the vector database\nstores its data in a vector index that supports ANN search, such as FAISS\n(Facebook AI Similarity Search) or ScaNN by Google.\n\n## Is there an Integrated Feature Store and Vector Database?\n\nHopsworks is a feature store with an integrated vector database. You store\ntables of feature data in feature groups, and you can index a column that\ncontains embeddings in a built-in vector database. This means you can search\nfor rows of similar features using embeddings and ANN search. Hopsworks also\nsupports filtering, so you can search for similar rows, but provide conditions\non what type of data to return (e.g., only users whose age>18).\n\n### Interested for more?\n\n  * \ud83e\udd16 Register for free on Hopsworks Serverless\n  * \ud83d\udcda Get your early copy: O'Reilly's 'Building Machine Learning Systems' book\n  * \ud83d\udc0d Learn all about the Python-Centric Feature Store\n  * \ud83d\udee0\ufe0f Explore all Hopsworks Integrations\n  * \ud83e\udde9 Get started with codes and examples\n  * \u2696\ufe0f Compare other Feature Stores with Hopsworks\n\nDoes this content look outdated? If you are interested in helping us maintain\nthis, feel free to contact us.\n\nF\n\nAuto-regressive Models\n\nAutoML\n\nF\n\nBackfill features\n\nBackfill training data\n\nBackpressure for feature stores\n\nBatch Inference Pipeline\n\nF\n\nCI/CD for MLOps\n\nCompound AI Systems\n\nContext Window for LLMs\n\nF\n\nDAG Processing Model\n\nData Compatibility\n\nData Contract\n\nData Lakehouse\n\nData Leakage\n\nData Modeling\n\nData Partitioning\n\nData Pipelines\n\nData Quality\n\nData Transformation\n\nData Type (for features)\n\nData Validation (for features)\n\nData-Centric ML\n\nDimensional Modeling and Feature Stores\n\nDownstream\n\nF\n\nELT\n\nETL\n\nEmbedding\n\nEncoding (for Features)\n\nEntity\n\nF\n\nFeature\n\nFeature Data\n\nFeature Engineering\n\nFeature Freshness\n\nFeature Function\n\nFeature Groups\n\nFeature Logic\n\nFeature Monitoring\n\nFeature Pipeline\n\nFeature Platform\n\nFeature Reuse\n\nFeature Selection\n\nFeature Service\n\nFeature Type\n\nFeature Value\n\nFeature Vector\n\nFeature View\n\nFiltering\n\nFine-Tuning LLMs\n\nFlash Attention\n\nF\n\nGenerative AI\n\nGradient Accumulation\n\nF\n\nHallucinations in LLMs\n\nHyperparameter\n\nHyperparameter Tuning\n\nF\n\nIdempotent Machine Learning Pipelines\n\nIn Context Learning (ICL)\n\nInference Data\n\nInference Logs\n\nInference Pipeline\n\nInstruction Datasets for Fine-Tuning LLMs\n\nF\n\nLLM Code Interpreter\n\nLLMOps\n\nLLMs - Large Language Models\n\nLagged features\n\nLangChain\n\nLatent Space\n\nF\n\nML\n\nML Artifacts (ML Assets)\n\nMLOps\n\nMVPS\n\nMachine Learning Observability\n\nMachine Learning Pipeline\n\nMachine Learning Systems\n\nModel Architecture\n\nModel Bias\n\nModel Deployment\n\nModel Development\n\nModel Evaluation (Model Validation)\n\nModel Governance\n\nModel Inference\n\nModel Interpretability\n\nModel Monitoring\n\nModel Performance\n\nModel Quantization\n\nModel Registry\n\nModel Serving\n\nF\n\nNatural Language Processing (NLP)\n\nF\n\nOffline Store\n\nOn-Demand Features\n\nOn-Demand Transformation\n\nOnline Inference Pipeline\n\nOnline Store\n\nOnline-Offline Feature Skew\n\nOnline-Offline Feature Store Consistency\n\nOrchestration\n\nF\n\nKServe\n\nPandas UDF\n\nParameter-Efficient Fine-Tuning (PEFT) of LLMs\n\nPoint-in-Time Correct Joins\n\nPrecomputed Features\n\nPrompt Engineering\n\nPrompt Store\n\nPrompt Tuning\n\nPython UDF\n\nF\n\nRLHF - Reinforcement Learning from Human Feedback\n\nReal-Time Machine Learning\n\nRepresentation Learning\n\nRetrieval Augmented Generation (RAG) for LLMs\n\nRoPE Scaling\n\nF\n\nSQL UDF in Python\n\nSample Packing\n\nSchema\n\nSimilarity Search\n\nSkew\n\nSplitting Training Data\n\nStreaming Feature Pipeline\n\nStreaming Inference Pipeline\n\nF\n\nTest Set\n\nTheory-of-Mind Tasks\n\nTime travel (for features)\n\nTrain (Training) Set\n\nTraining Data\n\nTraining Pipeline\n\nTraining-Inference Skew\n\nTransformation\n\nTwo-Tower Embedding Model\n\nTypes of Machine Learning\n\nF\n\nUpstream\n\nF\n\nValidation Set\n\nVector Database\n\nVersioning (of ML Artifacts)\n\nPRODUCT\n\nThe Feature StoreProduct CapabilitiesOpen SourceCustomersIntegrationsApp\nStatus\n\nRESOURCES\n\nThe MLOps DictionaryEU AI Act GuideExamplesUse-\nCasesBlogEventsDocumentationFeature Store ComparisonCommunityFAQ\n\nCOMPANY\n\nAbout UsContact Us\n\nSlack\n\nGithub\n\nTwitter\n\nLinkedin\n\nYoutube\n\nJOIN OUR MAILING LIST\n\nSubscribe to our newsletter and receive the latest product updates, upcoming\nevents, and industry news.\n\n\u00a9 Hopsworks 2024. All rights reserved. Various trademarks held by their\nrespective owners.\n\n# Notice\n\nWe and selected third parties use cookies or similar technologies for\ntechnical purposes and, with your consent, for other purposes as specified in\nthe cookie policy.\n\nUse the \u201cAccept\u201d button to consent. Use the \u201cReject\u201d button to continue\nwithout accepting.\n\nPress again to continue 0/2\n\n", "frontpage": false}
