{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afc85e8f-7045-4239-85d3-43791acc9601",
   "metadata": {},
   "source": [
    "### Possibly interesting features based on https://news.ycombinator.com/item?id=36590226\n",
    "- Time of day [Done]\n",
    "- Analysis on title: TF-IDF? Remove stopwords? Any other importance measures, and onehot encode the impt words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac021992-c9ee-470c-a5be-0fa5109aa117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql import SQLContext, Window, SparkSession\n",
    "from pyspark.sql.functions import col,sum,desc,when,udf, percent_rank\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC, RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder, Tokenizer, StopWordsRemover, CountVectorizer, IDF, PCA, HashingTF\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2600f0b0-d93a-461b-8468-28d22a26c36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To deal with running out of memory\n",
    "spark = SparkSession.builder.master('local[2]').config(\"spark.driver.memory\", \"8g\").appName('PySparkShell').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e7dd8-6ff2-4d8b-94fe-bc30d3d73903",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06811479-467a-427d-8baa-f7912ddd001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change file path\n",
    "wd = \"/Users/hydraze/Library/CloudStorage/GoogleDrive-tohziyu2@gmail.com/My Drive/Studies/KU Leuven/Courses/Classes/Y1S2/Advanced Analytics in Business/Project/3/AdvancedAnalytics_Streaming-Text-Analytics\"\n",
    "os.chdir(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ad3cc9-187c-4567-8fb6-3452049b52f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = spark.read.json(wd + \"/data/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fdf1be-e519-418a-b3ec-f8085e78e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try to speed things up by coalescing the partitions\n",
    "# \"\"\"\n",
    "# Source: https://stackoverflow.com/questions/35800795/number-of-partitions-in-rdd-and-performance-in-spark\n",
    "# Too few partitions You will not utilize all of the cores available in the cluster.\n",
    "# Too many partitions There will be excessive overhead in managing many small tasks.\n",
    "# Between the two the first one is far more impactful on performance. Scheduling too many smalls tasks is a relatively small impact at this point for partition counts below 1000. If you have on the order of tens of thousands of partitions then spark gets very slow.\n",
    "# \"\"\"\n",
    "# df = df.coalesce(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb21f739-aeb2-496f-80e4-0375f2c3d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Numebr of rows:\n",
    "# df.count() # All data: 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f3b5e5-eedd-4ed7-adc3-f5bb7e236af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df = df.dropDuplicates(subset=[\"aid\"])\n",
    "df.count() # 10035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94eeb6c-b1bc-401a-977d-f78bd68323b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Max and min posted-at date\n",
    "# min_date = df.agg({\"posted_at\": \"min\"}).collect()[0][0]\n",
    "# max_date = df.agg({\"posted_at\": \"max\"}).collect()[0][0]\n",
    "# print(f\"min posted at date: {min_date}\")\n",
    "# print(f\"min posted at date: {max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8608a763-b684-4945-a3a9-c2f2430bae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert frontpage to numeric\n",
    "df = df.withColumn('frontpage', when(df.frontpage==True, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141b86a2-31fa-47ff-b805-3539d62bf3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile cleaning steps which cannot be fit into a pipeline. These steps will not cause data leakage\n",
    "# Will have to be implemented on the script for doing streaming predictions\n",
    "\n",
    "# Extracting type of post: Show HN\n",
    "df = df.withColumn('isShowHN', when(df.title.contains(\"Show HN\"), 1).otherwise(0))\n",
    "\n",
    "# Extracting time of day\n",
    "extract_time_of_day_udf = udf(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S').strftime('%H'))\n",
    "\n",
    "\n",
    "df = df.withColumn('time_of_day', extract_time_of_day_udf(df.posted_at))\n",
    "\n",
    "# Extracting day of week\n",
    "weekDay =  udf(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S').strftime('%w'))\n",
    "\n",
    "df = df.withColumn('day_of_week', weekDay(df.posted_at))\n",
    "\n",
    "# Fill null values\n",
    "df = df.na.fill({\"title\": \"\", \"source_title\": \"\", \"source_text\": \"\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e4f2bf-18b5-462f-a75e-dc7372156d76",
   "metadata": {},
   "source": [
    "### Modelling time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae63ae6-c3ed-4a7f-93c3-0c71887c0d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split based on time: Have to prevent data leakage. Sort dataframe by posted_at, and give a percentile rank allowing us to \n",
    "# split the dataset in to two parts based on time\n",
    "df = df.withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy(\"posted_at\"))).collect() # Computationally heavy step. to collect here first\n",
    "df = spark.createDataFrame(df)\n",
    "\n",
    "train = df.where(\"rank <= .8\").drop(\"rank\")\n",
    "test = df.where(\"rank > .8\").drop(\"rank\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fef920-5857-4b95-a897-b811d3836f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other cleaning stages which can be fit into a pipeline. Will automatically apply all steps to test set\n",
    "NUM_COL = ['votes', 'comments']\n",
    "\n",
    "# Dealing with numerical variables\n",
    "numerical_vector_assembler = VectorAssembler(inputCols=NUM_COL, outputCol='num_col_vector')\n",
    "std_scaler = StandardScaler(inputCol='num_col_vector',\n",
    "                            outputCol='scaled_num_col_vector',\n",
    "                            withStd=True, withMean=True)\n",
    "\n",
    "# Dealing with categorical variables\n",
    "isShowHN_indexer = StringIndexer(inputCol='isShowHN',\n",
    "                            outputCol='isShowHN_index',handleInvalid = \"keep\")\n",
    "isShowHN_ohe = OneHotEncoder(inputCol='isShowHN_index',\n",
    "                                outputCol='isShowHN_OHE',handleInvalid = \"keep\")\n",
    "\n",
    "time_of_day_indexer = StringIndexer(inputCol='time_of_day',\n",
    "                            outputCol='time_of_day_index',handleInvalid = \"keep\")\n",
    "time_of_day_ohe = OneHotEncoder(inputCol='time_of_day_index',\n",
    "                                outputCol='time_of_day_OHE',handleInvalid = \"keep\")\n",
    "\n",
    "day_of_week_indexer = StringIndexer(inputCol='day_of_week',\n",
    "                            outputCol='day_of_week_index',handleInvalid = \"keep\")\n",
    "day_of_week_ohe = OneHotEncoder(inputCol='day_of_week_index',\n",
    "                                outputCol='day_of_week_OHE',handleInvalid = \"keep\")\n",
    "\n",
    "# Text processing steps\n",
    "# 1. Tokenization\n",
    "tokenizer_title = Tokenizer(inputCol=\"title\", outputCol=\"title_tokens\")\n",
    "tokenizer_source = Tokenizer(inputCol=\"source_title\", outputCol=\"source_title_tokens\")\n",
    "\n",
    "# 2. Stop Word Removal\n",
    "remover_title = StopWordsRemover(inputCol=\"title_tokens\", outputCol=\"title_filtered\")\n",
    "remover_source = StopWordsRemover(inputCol=\"source_title_tokens\", outputCol=\"source_title_filtered\")\n",
    "\n",
    "# 3. Hashing\n",
    "hashingTF_title = HashingTF(inputCol=\"title_filtered\", outputCol=\"title_rawFeatures\", numFeatures=20)\n",
    "hashingTF_source = HashingTF(inputCol=\"source_title_filtered\", outputCol=\"source_rawFeatures\", numFeatures=20)\n",
    "\n",
    "# 4. TF-IDF\n",
    "idf_title = IDF(inputCol=\"title_rawFeatures\", outputCol=\"title_features\")\n",
    "idf_source = IDF(inputCol=\"source_rawFeatures\", outputCol=\"source_features\")\n",
    "\n",
    "# 5. PCA - Cut hash feastures by half\n",
    "pca_title = PCA(k=10, inputCol=\"title_features\", outputCol=\"title_pcaFeatures\")\n",
    "pca_source = PCA(k=10, inputCol=\"source_features\", outputCol=\"source_pcaFeatures\")\n",
    "\n",
    "# To put all numerical vectors and onehotencoded categorical variables into the same final_feature_vector vector\n",
    "overall_assembler = VectorAssembler(inputCols=['scaled_num_col_vector',\n",
    "                                               'isShowHN_OHE',\n",
    "                                               'time_of_day_OHE',\n",
    "                                               'day_of_week_OHE',\n",
    "                                              'title_pcaFeatures',\n",
    "                                              'source_pcaFeatures'],\n",
    "                                    outputCol='final_feature_vector')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de73d5-2055-41f6-97d0-c1785a8b5791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate model(s) and params\n",
    "lr = LogisticRegression(maxIter=100, family=\"binomial\",\n",
    "                        featuresCol='final_feature_vector', labelCol='frontpage',\n",
    "                       # weightCol=\"frontpage\"\n",
    "                       )\n",
    "\n",
    "param_grid_lr = (ParamGridBuilder()\n",
    "                .addGrid(lr.regParam, [0.2, 0.6, 0.8, 1]) \n",
    "                .addGrid(lr.elasticNetParam, [0.0, 0.2, 0.8, 1.0]) \n",
    "                .build())\n",
    "\n",
    "rfc = RandomForestClassifier(maxDepth=30, seed=42, \n",
    "                             #weightCol=\"frontpage\", \n",
    "                             labelCol='frontpage', featuresCol='final_feature_vector')\n",
    "\n",
    "param_grid_rfc = (ParamGridBuilder()\n",
    "                 .addGrid(rfc.numTrees, [10, 20, 160, 640]) \n",
    "                 .addGrid(rfc.impurity, ['gini', 'entropy']) \n",
    "                 .build())\n",
    "\n",
    "svc = LinearSVC(labelCol='frontpage', featuresCol='final_feature_vector',\n",
    "                #weightCol=\"frontpage\" \n",
    "               )\n",
    "\n",
    "param_grid_svc = (ParamGridBuilder()\n",
    "                .addGrid(svc.regParam, [0.001, 0.1, 1, 10, 1000]) \n",
    "                .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3653e67-7a23-4582-84c1-2f0283756294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile final pipelines\n",
    "cleaning_stages = [numerical_vector_assembler, std_scaler, \n",
    "                   isShowHN_indexer, isShowHN_ohe, time_of_day_indexer, time_of_day_ohe, day_of_week_indexer, day_of_week_ohe, \n",
    "                   tokenizer_title, tokenizer_source, remover_title, remover_source, hashingTF_title, hashingTF_source,\n",
    "                   idf_title, idf_sourcepca_title, pca_source,\n",
    "                   overall_assembler]\n",
    "\n",
    "pipeline_lr = Pipeline(stages=cleaning_stages + [lr])\n",
    "pipeline_rfc = Pipeline(stages=cleaning_stages + [rfc])\n",
    "pipeline_svc = Pipeline(stages=cleaning_stages + [svc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c63a64c-0c8e-4ece-8055-7c6ad7855945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining evaluator and crossvalidation object\n",
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"frontpage\")\n",
    "\n",
    "kfolds = 2\n",
    "seed = 42\n",
    "n_cores = 4\n",
    "\n",
    "# Join everything together using a CrossValidator object for each model\n",
    "crossval_lr = CrossValidator(estimator=pipeline_lr, estimatorParamMaps=param_grid_lr, evaluator=evaluator, \n",
    "    numFolds=kfolds, parallelism=n_cores, seed=seed\n",
    ")\n",
    "\n",
    "crossval_rfc = CrossValidator(estimator=pipeline_rfc, estimatorParamMaps=param_grid_rfc, evaluator=evaluator, \n",
    "    numFolds=kfolds, parallelism=n_cores, seed=seed\n",
    ")\n",
    "\n",
    "crossval_svc = CrossValidator(estimator=pipeline_svc, estimatorParamMaps=param_grid_svc, evaluator=evaluator, \n",
    "    numFolds=kfolds, parallelism=n_cores, seed=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71470b2-6e43-4816-8ea4-8af73127c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluating LR\n",
    "cvModel_lr = crossval_lr.fit(train)\n",
    "\n",
    "best_model_lr = cvModel_lr.bestModel\n",
    "best_score_lr = cvModel_lr.avgMetrics[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba1ff21-a543-40f4-8b87-263a0969f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best LR model: \", best_model_lr)\n",
    "print(\"Best LR score: \", best_score_lr)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "best_lr_params = best_model_lr.stages[-1].extractParamMap()\n",
    "for parameter, value in best_lr_params.items():\n",
    "    print(f\"{str(parameter):50s}, {value}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Test scores\n",
    "test_pred_lr = best_model_lr.transform(test)\n",
    "print(\"Test scores for LR\", evaluator.evaluate(test_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4250afd-fdc6-46be-8458-d2ce4ba32e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluating RFC\n",
    "cvModel_rfc = crossval_rfc.fit(train)\n",
    "\n",
    "\n",
    "best_model_rfc = cvModel_rfc.bestModel\n",
    "best_score_rfc = cvModel_rfc.avgMetrics[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f147e6-32b0-4c10-881f-e771f87e4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best RFC model: \", best_model_rfc)\n",
    "print(\"Best RFC score: \", best_score_rfc)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "best_rfc_params = best_model_rfc.stages[-1].extractParamMap()\n",
    "for parameter, value in best_rfc_params.items():\n",
    "    print(f\"{str(parameter):50s}, {value}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Test scores\n",
    "test_pred_rfc = best_model_rfc.transform(test)\n",
    "print(\"Test scores for RFC\", evaluator.evaluate(test_pred_rfc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c0848-c9e0-43d4-bdfc-d6f7c9449b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluating SVC\n",
    "cvModel_svc = crossval_svc.fit(train)\n",
    "\n",
    "best_model_svc = cvModel_svc.bestModel\n",
    "best_score_svc = cvModel_svc.avgMetrics[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e5960d-19eb-450c-ac2a-1ad6f93fcc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best SVC model: \", best_model_svc)\n",
    "print(\"Best SVC score: \", best_score_svc)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "best_svc_params = best_model_svc.stages[-1].extractParamMap()\n",
    "for parameter, value in best_svc_params.items():\n",
    "    print(f\"{str(parameter):50s}, {value}\")\n",
    "    \n",
    "print(\"\\n\")\n",
    "\n",
    "# Test scores\n",
    "test_pred_svc = best_model_svc.transform(test)\n",
    "print(\"Test scores for SVC\", evaluator.evaluate(test_pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f3144-be3e-482a-ba45-5ef97ed99ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save best model based on test scores: One from [best_model_lr, best_model_rfc, best_model_svc]\n",
    "# The beest one trained on all training data is: SVC\n",
    "mPath =  wd+\"/models/best_model\"\n",
    "best_model_svc.write().overwrite().save(mPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747db9d2-6015-46d3-a100-da5c21ebee80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
