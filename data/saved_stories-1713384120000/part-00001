{"aid": "40064736", "title": "Mixtral 8x22B", "url": "https://mistral.ai/news/mixtral-8x22b/", "domain": "mistral.ai", "votes": 26, "user": "meetpateltech", "posted_at": "2024-04-17 14:00:37", "comments": 0, "source_title": "Cheaper, Better, Faster, Stronger", "source_text": "Cheaper, Better, Faster, Stronger | Mistral AI | Frontier AI in your hands\n\nLe Chat Log In\n\n# Cheaper, Better, Faster, Stronger\n\nContinuing to push the frontier of AI and making it accessible to all.\n\n  * April 17, 2024\n  * Mistral AI team\n\nMixtral 8x22B is our latest open model. It sets a new standard for performance\nand efficiency within the AI community. It is a sparse Mixture-of-Experts\n(SMoE) model that uses only 39B active parameters out of 141B, offering\nunparalleled cost efficiency for its size.\n\nMixtral 8x22B comes with the following strengths:\n\n  * It is fluent in English, French, Italian, German, and Spanish\n  * It has strong maths and coding capabilities\n  * It is natively capable of function calling; along with the constrained output mode implemented on la Plateforme, this enables application development and tech stack modernisation at scale\n  * Its 64K tokens context window allows precise information recall from large documents\n\n### Truly open\n\nWe believe in the power of openness and broad distribution to promote\ninnovation and collaboration in AI.\n\nWe are, therefore, releasing Mixtral 8x22B under Apache 2.0, the most\npermissive open-source licence, allowing anyone to use the model anywhere\nwithout restrictions.\n\n### Efficiency at its finest\n\nWe build models that offer unmatched cost efficiency for their respective\nsizes, delivering the best performance-to-cost ratio within models provided by\nthe community.\n\nMixtral 8x22B is a natural continuation of our open model family. Its sparse\nactivation patterns make it faster than any dense 70B model, while being more\ncapable than any other open-weight model (distributed under permissive or\nrestrictive licenses). The base model\u2019s availability makes it an excellent\nbasis for fine-tuning use cases.\n\nFigure 1: Measure of the performance (MMLU) versus inference budget tradeoff\n(number of active parameters). Mistral 7B, Mixtral 8x7B and Mixtral 8x22B all\nbelong to a family of highly efficient models compared to the other open\nmodels.\n\n### Unmatched open performance\n\nThe following is a comparison of open models on standard industry benchmarks.\n\n#### Reasoning and knowledge\n\nMixtral 8x22B is optimized for reasoning.\n\nFigure 2: Performance on widespread common sense, reasoning and knowledge\nbenchmarks of the top-leading LLM open models: MMLU (Measuring massive\nmultitask language in understanding), HellaSwag (10-shot), Wino Grande\n(5-shot), Arc Challenge (5-shot), Arc Challenge (25-shot), TriviaQA (5-shot)\nand NaturalQS (5-shot).\n\n#### Multilingual capabilities\n\nMixtral 8x22B has native multilingual capabilities. It strongly outperforms\nLLaMA 2 70B on HellaSwag, Arc Challenge and MMLU benchmarks in French, German,\nSpanish and Italian.\n\nFigure 3: Comparison of Mistral open source models and LLaMA 2 70B on\nHellaSwag, Arc Challenge and MMLU in French, German, Spanish and Italian.\n\n#### Maths & Coding\n\nMixtral 8x22B performs best in coding and maths tasks compared to the other\nopen models.\n\nFigure 4: Performance on popular coding and maths benchmarks of the leading\nopen models: HumanEval pass@1, MBPP pass@1, GSM8K maj@1 (5 shot), GSM8K maj@8\n(8-shot) and Math maj@4.\n\nThe instructed version of the Mixtral 8x22B released today shows even better\nmath performance, with a score of 90.8% on GSM8K maj@8 and a Math maj@4 score\nof 44.6%.\n\nExplore Mixtral 8x22B now on La Plateforme and join the Mistral community of\ndevelopers as we define the AI frontier together.\n\n##### Links\n\n  * Developers\n  * Technology\n  * Business\n  * About Us\n  * News\n\n##### About\n\n  * Contact Us\n  * Careers\n  * Terms of Use\n  * Privacy Policy\n  * Data Processing Agreement\n\n\u00a9 2024 Mistral AI, All rights reserved - Legal notice\n\n", "frontpage": true}
