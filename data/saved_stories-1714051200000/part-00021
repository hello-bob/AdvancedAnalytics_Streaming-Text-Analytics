{"aid": "40152658", "title": "RAG Is Dead. Long Live RAG", "url": "https://qdrant.tech/articles/rag-is-dead/", "domain": "qdrant.tech", "votes": 1, "user": "mehulashah", "posted_at": "2024-04-25 02:29:10", "comments": 0, "source_title": "RAG is Dead. Long Live RAG! - Qdrant", "source_text": "RAG is Dead. Long Live RAG! - Qdrant\n\n  * Home\n  * /\n  * Articles\n  * /\n  * RAG is Dead. Long Live RAG!\n\n# RAG is Dead. Long Live RAG!\n\nWhen Anthropic came out with a context window of 100K tokens, they said:\n\u201cVector search is dead. LLMs are getting more accurate and won\u2019t need RAG\nanymore.\u201d\n\nGoogle\u2019s Gemini 1.5 now offers a context window of 10 million tokens. Their\nsupporting paper claims victory over accuracy issues, even when applying Greg\nKamradt\u2019s NIAH methodology.\n\nIt\u2019s over. RAG must be completely obsolete now. Right?\n\nNo.\n\nLarger context windows are never the solution. Let me repeat. Never. They\nrequire more computational resources and lead to slower processing times.\n\nThe community is already stress testing Gemini 1.5:\n\nThis is not surprising. LLMs require massive amounts of compute and memory to\nrun. To cite Grant, running such a model by itself \u201cwould deplete a small coal\nmine to generate each completion\u201d. Also, who is waiting 30 seconds for a\nresponse?\n\n## Context stuffing is not the solution\n\n> Relying on context is expensive, and it doesn\u2019t improve response quality in\n> real-world applications. Retrieval based on vector search offers much higher\n> precision.\n\nIf you solely rely on an LLM to perfect retrieval and precision, you are doing\nit wrong.\n\nA large context window makes it harder to focus on relevant information. This\nincreases the risk of errors or hallucinations in its responses.\n\nGoogle found Gemini 1.5 significantly more accurate than GPT-4 at shorter\ncontext lengths and \u201ca very small decrease in recall towards 1M tokens\u201d. The\nrecall is still below 0.8.\n\nWe don\u2019t think 60-80% is good enough. The LLM might retrieve enough relevant\nfacts in its context window, but it still loses up to 40% of the available\ninformation.\n\n> The whole point of vector search is to circumvent this process by\n> efficiently picking the information your app needs to generate the best\n> response. A vector database keeps the compute load low and the query\n> response fast. You don\u2019t need to wait for the LLM at all.\n\nQdrant\u2019s benchmark results are strongly in favor of accuracy and efficiency.\nWe recommend that you consider them before deciding that an LLM is enough.\nTake a look at our open-source benchmark reports and try out the tests\nyourself.\n\n## Vector search in compound systems\n\nThe future of AI lies in careful system engineering. As per Zaharia et al.,\nresults from Databricks find that \u201c60% of LLM applications use some form of\nRAG, while 30% use multi-step chains.\u201d\n\nEven Gemini 1.5 demonstrates the need for a complex strategy. When looking at\nGoogle\u2019s MMLU Benchmark, the model was called 32 times to reach a score of\n90.0% accuracy. This shows us that even a basic compound arrangement is\nsuperior to monolithic models.\n\nAs a retrieval system, a vector database perfectly fits the need for compound\nsystems. Introducing them into your design opens the possibilities for\nsuperior applications of LLMs. It is superior because it\u2019s faster, more\naccurate, and much cheaper to run.\n\n> The key advantage of RAG is that it allows an LLM to pull in real-time\n> information from up-to-date internal and external knowledge sources, making\n> it more dynamic and adaptable to new information. - Oliver Molander, CEO of\n> IMAGINAI\n\n## Qdrant scales to enterprise RAG scenarios\n\nPeople still don\u2019t understand the economic benefit of vector databases. Why\nwould a large corporate AI system need a stand-alone vector db like Qdrant? In\nour minds, this is the most important question. Let\u2019s pretend that LLMs cease\nstruggling with context thresholds altogether.\n\nHow much would all of this cost?\n\nIf you are running a RAG solution in an enterprise environment with petabytes\nof private data, your compute bill will be unimaginable. Let\u2019s assume 1 cent\nper 1K input tokens (which is the current GPT-4 Turbo pricing). Whatever you\nare doing, every time you go 100 thousand tokens deep, it will cost you $1.\n\nThat\u2019s a buck a question.\n\n> According to our estimations, vector search queries are at least 100 million\n> times cheaper than queries made by LLMs.\n\nConversely, the only up-front investment with vector databases is the indexing\n(which requires more compute). After this step, everything else is a breeze.\nOnce setup, Qdrant easily scales via features like Multitenancy and Sharding.\nThis lets you scale up your reliance on the vector retrieval process and\nminimize your use of the compute-heavy LLMs. As an optimization measure,\nQdrant is irreplaceable.\n\nJulien Simon from HuggingFace says it best:\n\n> RAG is not a workaround for limited context size. For mission-critical\n> enterprise use cases, RAG is a way to leverage high-value, proprietary\n> company knowledge that will never be found in public datasets used for LLM\n> training. At the moment, the best place to index and query this knowledge is\n> some sort of vector index. In addition, RAG downgrades the LLM to a writing\n> assistant. Since built-in knowledge becomes much less important, a nice\n> small 7B open-source model usually does the trick at a fraction of the cost\n> of a huge generic model.\n\n## Long Live RAG\n\nAs LLMs continue to require enormous computing power, users will need to\nleverage vector search and RAG.\n\nOur customers remind us of this fact every day. As a product, our vector\ndatabase is highly scalable and business-friendly. We develop our features\nstrategically to follow our company\u2019s Unix philosophy.\n\nWe want to keep Qdrant compact, efficient and with a focused purpose. This\npurpose is to empower our customers to use it however they see fit.\n\nWhen large enterprises release their generative AI into production, they need\nto keep costs under control, while retaining the best possible quality of\nresponses. Qdrant has the tools to do just that. Whether through RAG, Semantic\nSearch, Dissimilarity Search, Recommendations or Multimodality - Qdrant will\ncontinue to journey on.\n\nDavid Myriel\n\nFebruary 27, 2024 |\n\n  * Share this post:\n\n##### What is RAG: Understanding Retrieval-Augmented Generation\n\nExplore how RAG enables LLMs to retrieve and utilize relevant external data\nwhen generating responses, rather than being limited to their original\ntraining data alone.\n\nMarch 19, 2024 | Sabrina Aquino\n\n##### Qdrant 1.8.0 - Major Performance Enhancements\n\nMuch faster sparse vectors, optimized indexation of text fields and optional\nCPU resource management configuration.\n\nMarch 6, 2024 | David Myriel, Mike Jang\n\n##### RAG is Dead. Long Live RAG!\n\nWhy are vector databases needed for RAG? We debunk claims of increased LLM\naccuracy and look into drawbacks of large context windows.\n\nFebruary 27, 2024 | David Myriel\n\n## Get Updates from Qdrant\n\nWe will update you on new features and news regarding Qdrant and Vector\nSimilarity Search\n\n#### Product\n\n  * Use cases\n  * Solutions\n  * Benchmarks\n  * Demos\n  * Pricing\n\n#### Community\n\n  * Github\n  * Discord\n  * Twitter\n  * Newsletter\n  * Contact us\n\n#### Company\n\n  * Jobs\n  * Privacy Policy\n  * Terms\n  * Impressum\n  * Credits\n\n#### Latest Publications\n\n#### What is RAG?\n\n#### Faster sparse vectors.Optimized indexation. Optional CPU resource\nmanagement.\n\n#### Why are vector databases needed for RAG? We debunk claims of increased\nLLM accuracy and look into drawbacks of large context windows.\n\n\u00a9 2024 Qdrant. All Rights Reserved\n\n##### Thanks for using Qdrant!\n\nSubscribe to our e-mail newsletter if you want to be updated on new features\nand news regarding Qdrant.\n\nLike what we are doing? Consider giving us a \u2b50 on Github.\n\nWe use cookies to learn more about you. At any time you can delete or block\ncookies through your browser settings.\n\nLearn moreI accept\n\n", "frontpage": false}
