{"aid": "40215859", "title": "RAG Purity Test", "url": "https://www.pongo.ai/rag-purity", "domain": "pongo.ai", "votes": 1, "user": "calebrjohn", "posted_at": "2024-04-30 20:26:00", "comments": 0, "source_title": "RAG Purity Test", "source_text": "RAG Purity Test\n\nHow good is your RAG pipeline? take the test below to find out.\n\nCaution: This is not a checklist. Completion of all items on this test will\nlikely result in customer churn.\n\n  1. I blindly chunk by character limit, instead of looking for stop tokens.\n  2. I use more than 80% of the context window for each generation.\n  3. I vectorize full documents, without creating chunks.\n  4. I only use vector search for retrieving documents.\n  5. I pack the context window with more than 10 results for each generation.\n  6. I think I'm too cool to use plaintext search in my RAG pipeline\n  7. I still use text-ada-002 embeddings model from OpenAI.\n  8. I do not use a semantic filter or post-processing step.\n  9. I blindly process user query without protecting for prompt injection.\n  10. I blindly dump user documents into my vector index without any pre-processing or reformatting\n  11. I don't inject context to my document chunks before vectorizing them\n  12. I still use the standard RAG prompt from the Langchain tutorial.\n  13. I evaulate my RAG pipeline performance using vibes.\n  14. I don't use LlamaParse for processing PDFs.\n  15. I don't use overlapping chunks to prevent information loss.\n  16. I rely on cosine similarity alone to rank results.\n  17. I utilize a minimum cosine similarity threshold to filter out irrelevant results.\n  18. I do not use query expansion.\n  19. I have not experimented with chunk sizes.\n  20. I have not updated my pipeline in the last 6 months.\n\n", "frontpage": false}
