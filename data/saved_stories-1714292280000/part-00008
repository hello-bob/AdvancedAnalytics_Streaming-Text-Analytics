{"aid": "40185118", "title": "Machine learning is neither good or evil", "url": "http://viznut.fi/texts-en/machine_learning_rant.html", "domain": "viznut.fi", "votes": 2, "user": "pabs3", "posted_at": "2024-04-28 01:15:12", "comments": 0, "source_title": "Machine learning is neither good or evil | viznut", "source_text": "Machine learning is neither good or evil | viznut\n\n## Machine learning is neither good or evil\n\nI've been wanting to write this for a long time. Even before large generative\nmachine-learning models became a valid subject of conversation among non-\ntechnical people.\n\nMy relationship with \"artificial intelligence\" has been quite complicated.\nI've had a strong scientific and hacker-like curiosity towards machine-\nlearning models for many years, and I feel there's a lot of positive potential\nto them. However, I dislike how they're being adopted by the civilization.\n\nThere's already a lot of political polarization around this topic. I often\nside with the skeptics and laugh at the na\u00efve advocates, but I still feel both\nsides are misguided. Similar polarization has happened with many other\ntechnologies before, so it's nothing new.\n\nIt is much easier for me to hate, say, \"mining\"-based cryptocurrencies such as\nBitcoin. They're all environmentally destructive scam, period. But AI forms a\nmind-boggingly huge territory of possibilities, and we've barely scratched the\nsurface of it. Simplistic judgement is therefore impossible.\n\n### Obsession vs anxiety\n\nI often get obsessed by systems that do something they shouldn't be able to.\nSuch as computer programs that do \"impossible\" things \u2013 how can a short\nformula like t*5&(t<<7)|t*3&(t*4<<10) sound like music? When I found\npotentially useful undocumented features in an old 8-bit computer, I could\nspend hundreds of hours studying it in order to show it at a demoscene event.\n\nIn 2015, Google revealed its Deepdream technique. How neural networks designed\nfor image recognition could be used backwards to produce psychedelic imagery\non top of existing images. I got obsessed by the technique, dissecting several\nneural nets in order to find out how they work and to find new visually\ninteresting things to do with them.\n\nIn 2019, I became similarly obsessed by OpenAI's GPT-2. I had seen neural\nlanguage models before, but they had looked just as bad as the Markov chains\nused in silly chatbots since 1990s. But GPT-2 was different. It remained\ncoherent over several paragraphs and even showed something that looked like\ncreativity, so I wanted to know exactly how it worked and where its limits\nwere. I implemented my own version in C in order to better understand it. I\nalso trained it to speak my native language. Some even paid me for helping\nthem out with GPT.\n\nExploring neural nets was fascinating, but at the same time they were far out\nof my technological comfort zone. I generally like systems that are relatively\nsimple on the surface but have a lot of emergent complexity. Elegant\nalgorithms, simple formulas, 8-bit microchips. Big neural models are always\nsomewhat like black boxes, alien artifacts very difficult for human minds to\nreverse-engineer. So, I guess I understand pretty well the anxiety that\noldschool hackers feel when dealing with machine learning.\n\n### AI vs IA\n\nAI is just as old as computers. The terms \"artificial intelligence\" and\n\"machine learning\" were coined in the 1950s, and even the first artificial\nneural nets were made in that decade. Even earlier, there was the field of\ncybernetics, focusing on control processes in machines and living beings\nalike. Cybernetics was maybe a bit too open and wide-ranging for the U.S.\nacademics, however, so they split it into more straightforward subfields.\n\nAnother 1950s concept was \"intelligence amplification\", IA. Not many have\nheard about IA, but these days it's pretty much ingrained in all human-\ncomputer interaction. Graphical user interfaces, hypertext and WYSIWYG all\noriginated in IA research, even though the original idea of IA has somewhat\nfaded out over the decades.\n\nAI researchers wanted to create machines that would simulate human\nintelligence well enough to replace it. In their vision, humans and machines\nwould compete against each other by the same rules and metrics \u2013 pretty much\nlike in the good old industrial capitalism. On the other hand, the IA side\nwanted to create machines that would assist and \"amplify\" people's natural\nintellects \u2013 so, instead of competition there would be co-operation combining\nthe strengths of humans and computers.\n\nNow that machine learning models have been framed as \"AI\", they have also\nbrought back the old separation of AI and IA. It almost looks like we forgot\nall the decades of research in human-computer interaction and returned to the\n\"Star Trek future\" of the 1960s. The computer is a mysterious black box that\nacts like a person. You give it a question in natural language, and after some\nprocessing, a final and definitive answer will come out.\n\nThis \"prompt'n'pray\" kind of interface is particularly alienating to those who\nwant to affect what's going on and to have control over the result. But not as\nalienating to the boss types who are more into giving orders than actually\ndoing things. Just look at who are the most vocal advocates of \"prompt'n'pray\"\nservices on social media.\n\nWhenever I've played around with ML models, I've wanted to observe and\ninterfere more than the default interfaces let me. With GPT, I wanted to watch\nthe actual candidate tokens and their probabilities, sometimes select the next\ntoken by myself or hack the selection logic. With image generators, I\npreferred to feed them pre-existing images and to gradually refine the imagery\nby latent blending, inpainting and manual editing. Plain \"prompt engineering\"\nfrustrated me very quickly.\n\nBut am I not just being frustrated by the clumsiness of the early stages of a\nnew technology? No, I don't believe that corporate generative ML will get any\ncloser to IA ideals. Intelligent and knowledgeable users are bad for business,\nthat's why. Even software developers need to remain ignorant: \"Open\"AI locks\nits API to mere \"text in, text out\" in order to protect its corporate\ninterests.\n\nAnd IA is currently unfashionable even when not contrasted with AI. Mainstream\n\"user experience design\", especially in social media applications, represents\nthe diagonal opposite of IA, amplifying stupidity instead of intelligence.\n\nCorporations will probably continue on this road \u2013 dumbing people down in\norder to boost the growth of AI \u2013 unless there's an adequate non-corporate\nintervention.\n\n### Shallow pools\n\nIn the production design of the latest Dune movies, the team avoided getting\ninspired by anything found on the Internet. The reason was that the images\neasily found on-line form a \"very shallow pool\". Many TV/movie productions\nlook similar because they're ultimately based on the same imagery.\n\nThis is a common discrepancy on today's Internet. In principle, there's a huge\namount of endlessly diverse material, but the search and discovery algorithms\nhide the \"long tail\" of it. Additionally, \"good content\" is defined by gut-\nbased \"like\" reactions, which narrows down the variety even more. This\nnarrowing-down even applies to the algorithms themselves: when the various\nplatform companies plagiarize a few market leaders, algorithmic diversity\nsuffers.\n\nGenerative ML models are based on statistical probabilities and therefore\nshare the same problem. Even if there are terabytes of training material\nbehind an oversized model, it seldom spits out anything from the \"long tail\".\nIf you ask ChatGPT to say something weird, it will say something that it\nconsiders a popular choice for \"weird\". The \"prompt'n'pray\" formula is a slave\nto the probability estimates and rarely chooses anything unpopular.\n\nSome generative models even boost their shallowness with additional Internet-\nstyle shallowness. Stable Diffusion XL, for example, has been trained to\ngenerate images that maximize a one-dimensional \"aesthetic score\" which is\nbased on gut reactions of large groups of people. Something similar happens\nalso when models are designed to be competitive at one-dimensional benchmarks.\n\nSo, if you want something unique out of a generative model, the seeds of\nuniqueness need to come from somewhere else than the probability-and-\npopularity engine. Just like on the algorithmic Internet: in order to keep\nyour recommendations interesting, you need to look for marginal stuff with\nobscure search keywords from time to time.\n\nGenerative models already make it very cheap to produce shallow \"content\" such\nas marketing bullshit and something that looks like stock photos, but its\nreign will not end there. I believe the cheapness will gradually extend to all\nkinds of \"content\".\n\nWhen the models get more capable, humans will need to look for spaces that are\nstill unreachable by \"AI\". To invent new genres, just like photography led to\nthe invention of new artistic styles. The shallow pools will get deeper, which\nwill be refreshing for a while. But all this hide-and-seek game against the\nmachine may be futile in the end. The culture will need to grow beyond\n\"content\" altogether.\n\n\"Content\" is a concept that is industrial-capitalist to the core. It\nprioritizes the corporate platform, be it a social media service, a commercial\nradio station or a book publisher, and devalues the creator. In \"content\ntalk\", there are no authors who need publishers for the books they write, but\na publishing industry that needs \"content producers\" for its books.\n\nOnce all \"content\" is as cheap as tap water, the present hiearchies and games\nof \"content production\" will have been demolished. What remains valuable will\nbe the kind of culture that cannot be reduced to consumable \"content\". I don't\nknow what it will be, but I hope it will be better than what today's\ncommercial Internet offers.\n\n### The third way\n\nThis may be difficult to believe in today's world, but the most essential\ncharacteristic of computers is their flexibility. Anything can become anything\nwith a different software. Unfortunately, this also means adaptability to\noppressive ideologies, especially to those of the big owners. This is why we\nhave destructively short hardware lifespans, ridiculous usage limitations,\nnon-programmability, ultra-addictive applications and overall one-\ndimensionality and shallowness. None of these are built-in characteristics of\ncomputing.\n\nThe same is true for many subfields of computing, including machine learning.\nThe most money and hype revolves around the aspects that can make the rich\neven richer. Build big centralized AI brains to replace human workers, and\nforget about the small, decentralized and IA.\n\nPower dynamics of technology often create polarization, and polarization\nweakens the ability to see properly. Philosophy of technology has suffered\nfrom this from the beginning. Even Heidegger, despite all his insight,\nbasically attacked a strawman called \"Technik\" without properly separating the\nsocio-economical from the technical.\n\nPolarization also often destroys the third option. Arts&Crafts started as a\nforward-looking and experimental movement that could have presented an\nalternative technological vision for the early 1900s: ditch dehumanizing\nfactory industry but adopt the kind of machines that support quality and\nartistic vision. A bit like IA against AI. But instead, the polarizing forces\nreframed A&C as a \"tradition\", situating it on the \"anti-progress\" side and\nthus eliminating its transformative potential.\n\nThe personal computer started as a third option. The big computers of the\n1970s were feared and opposed: they'd take our jobs, they'd destroy humanity.\nMicrocomputers were advocated as an unusual, countercultural way of fighting\nback \u2013 learn the secrets of the establishment and use its tools against it \u2013\nbut then they became middle-class consumer products and then the new tools of\noppression. The third option became one of the two.\n\nNow we're in the middle of an AI bubble that looks much like the dot-com\nbubble of the late 1990s. Fraudsters who want to get rich quickly are\naggressively pushing \"AI\" even for purposes it is ridiculously unsuitable for.\nAll the hype makes the world look either bleak or utopian, depending on the\nside you choose in the ever louder polarization circus. But in the midst of\nthis, it is important to work on creating a third option that can gain\ntraction once the bubble bursts.\n\nIt is easy to fight a strawman, but difficult to envision alternatives. That\nrequires understanding of history, society and technology, as well as\nimagination that goes beyond the \"shallow pools\". I consider myself lucky in\nthis regard because I've been involved with computers and programming from an\nearly age. Over a few decades, I've participated in underground computer\nsubcultures whose outlooks I've been able to contrast with the mainstream\nones. For someone with a background like mine, it would be simply stupid to\ndeclare \"AI\" as a taboo that right-minded fighters cannot touch.\n\nOf course there's a lot of oppose. The energy consumption of machine learning\nis huge, and it is expected to blow up globally with the demand of mass-market\napplications. Then there's the whole bunch of social and economic issues. And,\nas with all automation, it is also important to maintain the corresponding\nmanual practices that don't use automation. But a categorical opposition of an\nentire macro-category of algorithms is misguided.\n\nDespite my occasional obsession towards big mainstream applications of ML, I'm\nmostly interested in those kinds of ML and AI that stay away from the hype.\nClassical hand-coded heuristics. Small neural nets that do small things, often\nmaking huge and wasteful models ridiculous in comparison. And, of course, the\nvast unexplored gaps between AI and IA.\n\nI sometimes encounter interesting things but seldom anything practically\nuseful. But I don't think practicality matters all that much anyway. Hobbyist\nmicrocomputers weren't very useful either in their early years, but they still\ncreated a cultural alternative to how the establishment was using the\ntechnology.\n\n", "frontpage": false}
