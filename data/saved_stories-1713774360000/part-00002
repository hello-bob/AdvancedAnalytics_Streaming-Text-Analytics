{"aid": "40109045", "title": "LLVM Is Smarter Than Me", "url": "https://blog.sulami.xyz/posts/llvm-is-smarter-than-me/", "domain": "sulami.xyz", "votes": 3, "user": "nopipeline", "posted_at": "2024-04-21 20:41:11", "comments": 0, "source_title": "LLVM is Smarter Than Me - sulami's blog", "source_text": "LLVM is Smarter Than Me - sulami's blog\n\nLoading [MathJax]/jax/output/HTML-CSS/jax.js\n\nWeak Opinions, Strongly Held\n\nFeed \u2022 Uses \u2022 About\n\n# LLVM is Smarter Than Me\n\nPublished on 2024-04-19\n\nTags: rust, compilers, espresso\n\nI was reading Algorithms for Modern Hardware recently, specifically the\nchapter on SIMD, and was impressed by auto-vectorization. The basic idea is\nthat modern CPUs have so-called SIMDSingle Instruction Multiple Data\ninstructions that allow applying an operation to several values at once, which\nis much faster than performing the equivalent scalar instructions one at a\ntime. Modern compilers can detect certain programming patterns where an\noperation is performed repeatedly on scalar values and group the operations to\nmake use of the faster instructions.There are some details around having to\neither pad the last group so it lines up with the group size or run some\nscalar instructions at the end, it's really quite clever.\n\nThe book uses primarily C++ for its examples, but I was curious if the same\npattern would work in Rust as well, without having to manually annotate\nanything. So I opened up the compiler explorer and typed in my first test,\nwhich looked like this:\n\n    \n    \n    #[no_mangle] fn sum() -> u32 { (0..1000).sum() }\n\nTo make sure the compiler would feel safe to use SIMD instructions, I used -C\nopt-level=3 -C target-cpu=skylake, telling it that the target CPU supports\nthem. But instead of SIMD instructions I got this assembly:\n\n    \n    \n    sum: mov eax, 499500 ret\n\nTurns out I was outsmarted by the compiler, which used constant folding to\ndirectly return the final value it computed at compile time. To avoid this\noptimization from happening we can pass an argument into the function, so that\nthe compiler cannot know the final result:\n\n    \n    \n    #[no_mangle] fn sum(n: u32) -> u32 { (0..n).sum() }\n\nWhich generates this assembly:\n\n    \n    \n    sum: test edi, edi je .LBB0_1 lea eax, [rdi - 1] lea ecx, [rdi - 2] imul rcx, rax shr rcx lea eax, [rdi + rcx] dec eax ret .LBB0_1: xor eax, eax ret\n\nStill there are no SIMD instructions in here, which would usually start with\nthe letter v, such as vpaddd which adds integers in parallel. To compare, I\nwrote the equivalent program in C++:\n\n    \n    \n    unsigned int sum(unsigned int n) { unsigned int rv = 0; for (unsigned int i = 0; i < n; i++) { rv += i; } return rv; }\n\nCompiler explorer defaults to GCC to compile C++. I used the equivalent -O3\n-march=skylake, and sure enough, I got back 106 lines of assembly (omitted for\nbrevity), including the desired SIMD instructions. I then switched to Clang,\nthe LLVM-based C/C++ compiler that is also used by Rust, and it produced the\nexact same assembly as it did for the Rust version. This tells us the\ndifference is not one of languages, but one of compilers.Interestingly enough,\nthe LLVM IR generated is quite different between the two languages.\n\nA quick comparative benchmark revealed the LLVM version to be significantly\nfaster than GCC's vectorized loop, especially for large values of n, which is\nunsurprising when looking at the instructions, about 10 scalar instructions\nwill beat hundreds of loops over vector instructions.\n\nThe key here is that the sum of consecutive integers from zero to n has a\nclosed form solution, which I only realized after looking more closely at the\nassembly:\n\nn\u2211i=0i=n(n\u22121)2\n\nLLVM apparently detects that that is exactly what we are trying to do, and as\na result it can do away with the looping altogether and directly calculate the\nresult in one step, changing sum from O(n) to O(1). Colour me impressed.\n\nFinally, replicating the book's example more closely, I managed to get LLVM to\nautomatically vectorize a loop for me, using the following code:\n\n    \n    \n    #[no_mangle] fn sum(arr: &[u32]) -> u32 { arr.iter().sum() }\n\nThis works because LLVM has no idea about the contents of the array, so it\ncannot deduce a more efficient way of calculating their sum than to actually\nadd them all up.\n\nMy takeaway here is that LLVM is even better at generating optimal code than I\nwould have expected, at least for trivial examples. I am happy that I can\ncontinue writing idiomatic code without feeling like I am trading off\nperformance for readability.\n\nBuilt 2024-04-21 11:59\n\n", "frontpage": false}
