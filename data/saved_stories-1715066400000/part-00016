{"aid": "40280875", "title": "Eli Bendersky: Faster XML Stream Processing in Go", "url": "https://eli.thegreenplace.net/2019/faster-xml-stream-processing-in-go/", "domain": "thegreenplace.net", "votes": 12, "user": "PaulHoule", "posted_at": "2024-05-07 00:01:44", "comments": 3, "source_title": "Faster XML stream processing in Go", "source_text": "Faster XML stream processing in Go - Eli Bendersky's website\n\nEli Bendersky's website\n\n  * About\n  * Projects\n  * Archives\n\n# Faster XML stream processing in Go\n\nJuly 22, 2019 at 05:37 Tags Go , C & C++ , Python\n\nXML processing was all the rage 15 years ago; while it's less prominent these\ndays, it's still an important task in some application domains. In this post\nI'm going to compare the speed of stream-processing huge XML files in Go,\nPython and C and finish up with a new, minimal module that uses C to\naccelerate this task for Go. All the code shown throughout this post is\navailable in this GitHub repository the new Go module is here.\n\n## What does XML stream processing mean?\n\nFirst, let's define the problem at hand in more detail. Roughly speaking,\nthere are two ways we can process data from a file:\n\n  1. Read the whole file into memory at once, and then proces the data in memory.\n  2. Read the file in chunks, process each chuck, without having the whole data in memory at any given time.\n\nIn many ways, (1) is more convenient because we can easily go back to any part\nof the file. However, in some situations (2) is essential; specifically, when\nthe file is very large. This is where stream processing comes in. If our input\nfile is 500 GiB, we're unlikely to be able to read it into memory and have to\nprocess it in parts. Even for smaller files that would theoretically fit into\nRAM, it's not always a good idea to read them wholly; this dramatically\nincreases the active heap size and can cause performance issues in garbage-\ncollected languages.\n\n## The task\n\nFor this benchmark, I'm using xmlgen to create a 230 MiB XML file [1]. A tiny\nfragment of the file may look like this:\n\n    \n    \n    <?xml version=\"1.0\" standalone=\"yes\"?> <site> <regions> <asia> <item id=\"item0\"> <location>United States</location> <quantity>1</quantity> <name>duteous nine eighteen </name> <payment>Creditcard</payment> ... </item> </asia> </regions> </site>\n\nThe task is to find how many times \"Africa\" appears in the data of the\n<location> tag throughout the document.\n\n## Baseline - using the Go standard library\n\nLet's start with a baseline implementation - using the standard library's\nencoding/xml package. While the package's Unmarshal mode will parse the whole\nfile in one go, it can also be used to process XML token by token and\nselectively parse interesting elements. Here is the code:\n\n    \n    \n    package main import ( \"encoding/xml\" \"fmt\" \"io\" \"log\" \"os\" \"strings\" ) type location struct { Data string `xml:\",chardata\"` } func main() { f, err := os.Open(os.Args[1]) if err != nil { log.Fatal(err) } defer f.Close() d := xml.NewDecoder(f) count := 0 for { tok, err := d.Token() if tok == nil || err == io.EOF { // EOF means we're done. break } else if err != nil { log.Fatalf(\"Error decoding token: %s\", err) } switch ty := tok.(type) { case xml.StartElement: if ty.Name.Local == \"location\" { // If this is a start element named \"location\", parse this element // fully. var loc location if err = d.DecodeElement(&loc, &ty); err != nil { log.Fatalf(\"Error decoding item: %s\", err) } if strings.Contains(loc.Data, \"Africa\") { count++ } } default: } } fmt.Println(\"count =\", count) }\n\nI made sure to double check that the memory usage of this program stays\nbounded and low while processing a large file - the maximum RSS was under 7\nMiB while processing our 230 MiB input file. I'm verifying this for all the\nprograms presented in this post using /usr/bin/time -v on Linux.\n\nThis program takes 6.24 seconds to process the whole file and print out the\nresult.\n\n## Python implementation\n\nThe first Python implementation uses the xml.etree.ElementTree module from the\nstandard library:\n\n    \n    \n    import sys import xml.etree.ElementTree as ET count = 0 for event, elem in ET.iterparse(sys.argv[1], events=(\"end\",)): if event == \"end\": if elem.tag == 'location' and elem.text and 'Africa' in elem.text: count += 1 elem.clear() print('count =', count)\n\nThe key here is the elem.clear() call. It ensures that each element gets\ndiscarded afer parsing it fully, so the memory usage won't grow linearly with\nthe size of the file (unless the file is pathological). This program takes 3.7\nseconds to process the whole file - much faster than our Go program. Why is\nthat?\n\nWhile the Go program uses 100% Go code for the task (encoding/xml is\nimplemented entirely in Go), the Python program is using a C extension (most\nof ElementTree is written in C) wrapping a fast XML parser in C - libexpat.\nThe bulk of the work here is done in C, which is faster than Go. The\nperformance of encoding/xml is further discussed in this issue, though it's an\nold one and the performance has been somewhat optimized since then.\n\nAn alternative XML parsing library for Python is lxml, which uses libxml\nunderneath. Here's a Python version using lxml:\n\n    \n    \n    import sys from lxml import etree count = 0 for event, elem in etree.iterparse(sys.argv[1], events=(\"end\",)): if event == \"end\": if elem.tag == 'location' and elem.text and 'Africa' in elem.text: count += 1 elem.clear() print('count =', count)\n\nThis looks very similar to the previous version, and that's on purpose. lxml\nhas an etree-compatible API to make transition from the standard library\nsmoother. This version also takes around 3.7 seconds for our 230 MiB file.\n\nThe reason I'm including lxml here is that it will run faster than\nxml.etree.ElementTree when slurping the whole file, for our particular file\nsize. I want to highlight that this is outside of the scope for my experiment,\nbecause I only care about streaming processing. The only way (that I'm aware\nof!) to successfully process a 500 GiB file with lxml would be by using\niterparse.\n\n## How fast can it run?\n\nBased on the measurements presented here, Go is about 68% slower than Python\nfor parsing a large XML file in a streaming fashion. While Go usually compiles\nto a much faster code than pure Python, the Python implementations have the\nbacking of efficient C libraries with which it's difficult to compete. I was\ncurious to know how fast it could be, in theory [2].\n\nTo answer this question, I implemented the same program using pure C with\nlibxml, which has a SAX API. I won't paste it wholly here because it's longer,\nbut you can find the full source code on GitHub. It takes just 0.56 seconds to\nprocess our 230 MiB input file, which is very impressive given the other\nresults, but also not very surprising. This is C, after all.\n\nYou may wonder - if lxml uses libxml underneath, why is it so much slower than\nthe pure C version? The answer is Python call overhead. The lxml version calls\nback into Python for every parsed element, which incurs a significant cost\n[3]. Another reason is that my C implementation doesn't actually parse an\nelement - it's just a simple event-based state machine, so there's less extra\nwork being done.\n\n## Using libxml from Go\n\nTo recap where we are so far:\n\n  1. Python libraries based on underlying C implementations are faster than pure Go.\n  2. Pure C is much faster still.\n\nWe have two options: we can either try to optimize Go's encoding/xml package,\nor we can try to wrap a fast C library with Go. While the former is a worthy\ngoal, it involves a large effort and should be a topic for a separate post.\nHere, I'll go for the latter.\n\nSeaching around the web, I found a few wrappers around libxml. Two that seemed\nmoderately popular and maintained are https://github.com/lestrrat-go/libxml2\nand https://github.com/moovweb/gokogiri. Unfortunately, neither of these (or\nthe other bindings I found) are exposing the SAX API of libxml; instead, they\nfocus on the DOM API, where the whole document is parsed by the underlying\nlibrary and a tree is returned. As mentioned above, we need the SAX interface\nto process huge files.\n\n## gosax\n\nIt's time to roll our own :-) I wrote the gosax module, which uses Cgo to call\ninto libxml and exposes a SAX interface [4]. Implementing it was an\ninteresting exercise in Cgo, because it requires some non-trivial concepts\nlike registering Go callbacks with C.\n\nHere's a version of our program using gosax:\n\n    \n    \n    package main import ( \"fmt\" \"os\" \"strings\" \"github.com/eliben/gosax\" ) func main() { counter := 0 inLocation := false scb := gosax.SaxCallbacks{ StartElement: func(name string, attrs []string) { if name == \"location\" { inLocation = true } else { inLocation = false } }, EndElement: func(name string) { inLocation = false }, Characters: func(contents string) { if inLocation && strings.Contains(contents, \"Africa\") { counter++ } }, } err := gosax.ParseFile(os.Args[1], scb) if err != nil { panic(err) } fmt.Println(\"counter =\", counter) }\n\nAs you can see, it implements a state machine that remembers being inside a\nlocation element, where the character data is checked. This program takes 4.03\nseconds to process our input file. Not bad! But we can do a bit better, and\nwith a couple of optimizations I managed to bring it down to 3.68 seconds -\nabout the same speed as the Python implementations!\n\nIMHO the roughly similar run times here are a coincidence, because the Python\nprograms are different from my approach in that they expose a higher-level API\nthan pure SAX. Recall that iterparse returns a parsed element, and we can\naccess its text attribute, etc. In gosax, we have to do this much more\nmanually. Since the the cost of calls between Cgo and Go is rather high, there\nis an optimization opportunity here for gosax. We could do more work in C -\nparsing a full element, and returning it wholly to Go. This would move work\nfrom the Go side to the C side, as well as reduce the number of cross-language\ncalls. But this is a task for another day.\n\n## Conclusion\n\nWell, this was fun :-) There are 5 different implementations of the same\nsimple task described here, in 3 different programming languages. Here is a\nsummary of the speed measurements we got:\n\nPython's performance story has always been - \"it's probably fast enough, and\nin the rare cases where it isn't, use a C extension\". In Go the narrative is\nsomewhat different: in most cases, the Go compiler produces fairly fast code.\nPure Go code is significantly faster than Python and often faster than Java.\nEven so, every once in a while it may be useful to dip into C or C++ for\nperformance, and in these cases Cgo is a good approach.\n\nIt's obvious that encoding/xml needs some work w.r.t. performance, but until\nthat happens - there are good alternatives! Leveraging the speed of libxml has\nbeen possible for the DOM API, and now is possible for the SAX API as well. In\nthe long run, I believe that serious performance work on encoding/xml can make\nit go faster than the libxml wrappers because it would elimitate the high cost\nof C-to-Go calls.\n\n[1]| This size will easily fit in RAM, but it's good enough to provide a\nmeaningful benchmarking duration.  \n---|---  \n[2]| When working on optimizations, it's often useful to know \"the speed of\nlight\" of some computation. Say we want to optimize some function in our\nprogram. It's worth asking - how much faster will the program be if this\nfunction takes 0 time? If the overall change is tiny, the function is not\nworth optimizing, most likely. This is just a practical application of\nAmdahl's law.  \n---|---  \n[3]| We can test this hypothesis by timing how long it takes the non-streaming\nAPI in lxml to parse the same file. Since it parses the whole XML file in C\nbefore returning the parsed structure to Python, we expect the Python call\noverhead to be much smaller. Indeed, for files that fit into memory this is\nfaster. But once again, in this post we return our attention to streaming APIs\n- assuming this is our only choice for gigantic files.  \n---|---  \n[4]| gosax is very minimal, only providing the most common SAX callbacks. The\ndecision to create a new module was just for convenience and speed; the more\ncorrect thing would have likely been to contribute to one of the existing\nlibxml wrappers. I don't see gosax as production-quality at this stage - I\njust hacked it together to be able to experiment for this post.  \n---|---  \n  \nFor comments, please send me an email.\n\n\u00a9 2003-2024 Eli Bendersky\n\nBack to top\n\n", "frontpage": true}
