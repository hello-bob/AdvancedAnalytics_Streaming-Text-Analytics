{"aid": "40131590", "title": "OpenAI's commitment to child safety: adopting safety by design principles", "url": "https://openai.com/blog/child-safety-adopting-sbd-principles", "domain": "openai.com", "votes": 2, "user": "todsacerdoti", "posted_at": "2024-04-23 13:13:55", "comments": 0, "source_title": "OpenAI\u2019s commitment to child safety: adopting safety by design principles", "source_text": "OpenAI\u2019s commitment to child safety: adopting safety by design principles\n\nSkip to main content\n\n# Mobile Navigation\n\nBlog\n\n# OpenAI\u2019s commitment to child safety: adopting safety by design principles\n\nWe\u2019re joining Thorn, All Tech Is Human, and other leading companies in an\neffort to prevent the misuse of generative AI to perpetrate, proliferate, and\nfurther sexual harms against children.\n\nApril 23, 2024\n\n### Authors\n\n  * OpenAI\n\nAnnouncements, Safety & Alignment\n\nOpenAI, alongside industry leaders including Amazon, Anthropic, Civitai,\nGoogle, Meta, Metaphysic, Microsoft, Mistral AI, and Stability AI, has\ncommitted to implementing robust child safety measures in the development,\ndeployment, and maintenance of generative AI technologies as articulated in\nthe Safety by Design principles. This initiative, led by Thorn, a nonprofit\ndedicated to defending children from sexual abuse, and All Tech Is Human, an\norganization dedicated to tackling tech and society's complex problems, aims\nto mitigate the risks generative AI poses to children. By adopting\ncomprehensive Safety by Design principles, OpenAI and our peers are ensuring\nthat child safety is prioritized at every stage in the development of AI. To\ndate, we have made significant effort to minimize the potential for our models\nto generate content that harms children, set age restrictions for ChatGPT, and\nactively engage with the National Center for Missing and Exploited Children\n(NCMEC), Tech Coalition, and other government and industry stakeholders on\nchild protection issues and enhancements to reporting mechanisms.\n\nAs part of this Safety by Design effort, we commit to:\n\n  1. Develop: Develop, build, and train generative AI models that proactively address child safety risks.\n\n     * Responsibly source our training datasets, detect and remove child sexual abuse material (CSAM) and child sexual exploitation material (CSEM) from training data, and report any confirmed CSAM to the relevant authorities.\n     * Incorporate feedback loops and iterative stress-testing strategies in our development process.\n     * Deploy solutions to address adversarial misuse.\n  2. Deploy: Release and distribute generative AI models after they have been trained and evaluated for child safety, providing protections throughout the process.\n\n     * Combat and respond to abusive content and conduct, and incorporate prevention efforts.\n     * Encourage developer ownership in safety by design.\n  3. Maintain: Maintain model and platform safety by continuing to actively understand and respond to child safety risks.\n\n     * Committed to removing new AIG-CSAM generated by bad actors from our platform.\n     * Invest in research and future technology solutions.\n     * Fight CSAM, AIG-CSAM and CSEM on our platforms.\n\nThis commitment marks an important step in preventing the misuse of AI\ntechnologies to create or spread child sexual abuse material (AIG-CSAM) and\nother forms of sexual harm against children. As part of the working group, we\nhave also agreed to release progress updates every year.\n\n> We care deeply about the safety and responsible use of our tools, which is\n> why we\u2019ve built strong guardrails and safety measures into ChatGPT and\n> DALL-E. We are committed to working alongside Thorn, All Tech is Human and\n> the broader tech community to uphold the Safety by Design principles and\n> continue our work in mitigating potential harms to children.\n\nChelsea Carlson, Child Safety TPM\n\nThis collective action underscores our shared approach to child safety,\ndemonstrating a shared commitment to ethical innovation and the well-being of\nthe most vulnerable members of society. Thorn has published the principles at\nhttps://teamthorn.co/gen-ai\n\n### Authors\n\n  * #### OpenAI\n\nView all articles\n\n## Related research\n\nView all research\n\n  * ### Building an early warning system for LLM-aided biological threat creation\n\nJan 31, 2024January 31, 2024\n\n  * ### Weak-to-strong generalization\n\nDec 14, 2023December 14, 2023\n\n  * ### Practices for Governing Agentic AI Systems\n\nDec 14, 2023December 14, 2023\n\n  * ### DALL\u00b7E 3 system card\n\nOct 3, 2023October 3, 2023\n\n#### Research\n\n  * Overview\n  * Index\n  * GPT-4\n  * DALL\u00b7E 3\n  * Sora\n\n#### API\n\n  * Overview\n  * Pricing\n  * Docs\n\n#### ChatGPT\n\n  * Overview\n  * Team\n  * Enterprise\n  * Pricing\n  * Try ChatGPT\n\n#### Company\n\n  * About\n  * Blog\n  * Careers\n  * Charter\n  * Security\n  * Customer stories\n  * Safety\n\nOpenAI \u00a9 2015 \u2013 2024Terms & policiesPrivacy policyBrand guidelines\n\n#### Social\n\n  * Twitter\n  * YouTube\n  * GitHub\n  * SoundCloud\n  * LinkedIn\n\n", "frontpage": false}
