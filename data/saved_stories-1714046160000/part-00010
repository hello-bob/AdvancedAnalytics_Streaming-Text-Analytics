{"aid": "40152103", "title": "Large Scale Transformer Model Training with Tensor Parallel", "url": "https://pytorch.org/tutorials/intermediate/TP_tutorial.html", "domain": "pytorch.org", "votes": 1, "user": "pama", "posted_at": "2024-04-25 01:17:55", "comments": 0, "source_title": "Large Scale Transformer model training with Tensor Parallel (TP) \u2014 PyTorch Tutorials 2.3.0+cu121 documentation", "source_text": "Large Scale Transformer model training with Tensor Parallel (TP) \u2014 PyTorch\nTutorials 2.3.0+cu121 documentation\n\n  * Get Started\n  * Ecosystem\n  * Edge\n\nAbout PyTorch Edge ExecuTorch\n\n  * Blog\n  * Tutorials\n  * Docs\n\nPyTorch\n\ntorchaudio\n\ntorchtext\n\ntorchvision\n\ntorcharrow\n\nTorchData\n\nTorchRec\n\nTorchServe\n\nTorchX\n\nPyTorch on XLA Devices\n\n  * Resources\n\nAbout\n\nLearn about PyTorch\u2019s features and capabilities\n\nPyTorch Foundation\n\nLearn about the PyTorch foundation\n\nCommunity\n\nJoin the PyTorch developer community to contribute, learn, and get your\nquestions answered.\n\nCommunity Stories\n\nLearn how our community solves real, everyday machine learning problems with\nPyTorch.\n\nDeveloper Resources\n\nFind resources and get questions answered\n\nEvents\n\nFind events, webinars, and podcasts\n\nForums\n\nA place to discuss PyTorch code, issues, install, research\n\nModels (Beta)\n\nDiscover, publish, and reuse pre-trained models\n\n  * GitHub\n\nTable of Contents\n\n  * Tutorials >\n  * Large Scale Transformer model training with Tensor Parallel (TP)\n\nShortcuts\n\nintermediate/TP_tutorial\n\nRun in Google Colab\n\nColab\n\nDownload Notebook\n\nNotebook\n\nView on GitHub\n\nGitHub\n\n# Large Scale Transformer model training with Tensor Parallel (TP)\n\nAuthor: Wanchao Liang, Tianyu Liu\n\nNote\n\nView and edit this tutorial in github.\n\nThis tutorial demonstrates how to train a large Transformer-like model across\nhundreds to thousands of GPUs using Tensor Parallel and Fully Sharded Data\nParallel.\n\nPrerequisites:\n\n  * PyTorch 2.3.0 or later installed with CUDA/Linux\n\n  * Tensor Parallel APIs\n\n  * Getting Started with DeviceMesh\n\n  * Getting Started with Fully Sharded Data Parallel\n\n## How Tensor Parallel works?\n\nTensor Parallel (TP) was originally proposed in the Megatron-LM paper, and it\nis an efficient model parallelism technique to train large scale Transformer\nmodels. Sequence Parallel (SP) we mention in this tutorial is a variant of\nTensor Parallel that shards on the sequence dimension for nn.LayerNorm or\nRMSNorm to further save activation memory during training. As the model\nbecomes larger, the activation memory becomes the bottleneck, so in Tensor\nParallel training it usually applies Sequence Parallel to LayerNorm or RMSNorm\nlayers.\n\nFigure 1. represents the sharding in Tensor Parallel style on a Transformer\nmodel\u2019s MLP and Self-Attention layer, where the matrix multiplications in both\nattention/MLP happens through sharded computations (image source)\n\nAt a high level, PyTorch Tensor Parallel works as follows:\n\nSharding initialization\n\n  * Determine which ParallelStyle to apply to each layer and shard the initialized module by calling parallelize_module.\n\n  * The parallelized modules would have their model parameters be swapped to DTensors, and DTensor would be responsible to run the parallelized module using sharded computation.\n\nRuntime foward/backward\n\n  * Depending on the input/outputs DTensor layouts user specified for each ParallelStyle, it would run proper communication operation to transform the DTensor layouts for inputs/outputs (such as allreduce, allgather and reduce_scatter).\n\n  * Run sharded computation for the parallelized layers to save compute/memory (for example, nn.Linear, nn.Embedding).\n\n## When and Why you should apply Tensor Parallel\n\nThe PyTorch Fully Sharded Data Parallel (FSDP) already has the capability to\nscale model training to a specific number of GPUs. However, when it comes to\nfurther scale the model training in terms of model size and GPU quantity, many\nadditional challenges arise that may require combining Tensor Parallel with\nFSDP.:\n\n  1. As the world size (number of GPUs) is becoming excessively large (exceeding 128/256 GPUs), the FSDP collectives (such as allgather) are being dominated by ring latency. By implementing TP/SP on top of FSDP, the FSDP world size could be reduced by 8 by applying FSDP to be inter-host only, consequently decreasing the latency costs by the same amount.\n\n  2. Hit data parallelism limit where you can not raise the global batch size to be above the number of GPUs due to both convergence and GPU memory limitations, Tensor/Sequence Parallel is the only known way to \u201cballpark\u201d the global batch size and continue scaling with more GPUs. This means both model size and number of GPUs could continue to scale.\n\n  3. For certain types of models, when local batch size becomes smaller, TP/SP can yield matrix multiplication shapes that are more optimized for floating point operations (FLOPS).\n\nSo, when pre-training, how easy is it to hit those limits? As of now, pre-\ntraining a Large Language Model (LLM) with billions or trillions of tokens\ncould take months, even when using thousands of GPUs.\n\n  * It will always hit limitation 1 when training LLM on a large scale. For example, Llama 2 70B trained with 2k GPUs for 35 days, multi-dimensional parallelisms are needed at 2k scale.\n\n  * When the Transformer model becomes larger (such as Llama2 70B), it will also quickly hit the limitation 2. One could not use FSDP alone with even local batch_size=1 due to memory and convergence constraints. For example, Llama 2 global batch size is 1K, so data parallelism alone can not be used at 2K GPUs.\n\n## How to apply Tensor Parallel\n\nPyTorch Tensor Parallel APIs offers a set of module level primitives\n(ParallelStyle) to configure the sharding for each individual layers of the\nmodel, including:\n\n  * ColwiseParallel and RowwiseParallel: Shard the nn.Linear and nn.Embedding in the column or row fashion.\n\n  * SequenceParallel: Perform sharded computations on nn.LayerNorm, nn.Dropout, RMSNormPython, etc.\n\n  * PrepareModuleInput and PrepareModuleOutput: Configure the module inputs/outputs sharding layouts with proper communication operations.\n\nTo demonstrate how to use the PyTorch native Tensor Parallel APIs, let us look\nat a common Transformer model. In this tutorial, we use the most recent Llama2\nmodel as a reference Transformer model implementation, as it is also widely\nused in the community.\n\nSince Tensor Parallel shard individual tensors over a set of devices, we would\nneed to set up the distributed environment (such as NCCL communicators) first.\nTensor Parallelism is a Single-Program Multiple-Data (SPMD) sharding algorithm\nsimilar to PyTorch DDP/FSDP, and it under the hood leverages the PyTorch\nDTensor to perform sharding. It also utilizes the DeviceMesh abstraction\n(which under the hood manages ProcessGroups) for device management and\nsharding. To see how to utilize DeviceMesh to set up multi-dimensional\nparallelisms, please refer to this tutorial. Tensor Parallel usually works\nwithin each host, so let us first initialize a DeviceMesh that connects 8 GPUs\nwithin a host.\n\n    \n    \n    # run this via torchrun: torchrun --standalone --nproc_per_node=8 ./tp_tutorial.py from torch.distributed.device_mesh import init_device_mesh tp_mesh = init_device_mesh(\"cuda\", (8,))\n\nNow that we have initialized DeviceMesh, let us take a detailed look at the\nLlama 2 model architecture and see how we should perform the Tensor Parallel\nsharding. Here we focus on the core TransformerBlock, where the Transformer\nmodel stacks the identical TransformerBlock s to scale up the model.\n\nThe core TransformerBlock consists of an Attention layer and a FeedForward\nlayer. Let us first look at the simpler FeedForward layer. For the FeedForward\nLayer it consists of three Linear layers, where it performs a SwiGLU style\nMLP, looking at its forward function:\n\n    \n    \n    # forward in the FeedForward layer def forward(self, x): return self.w2(F.silu(self.w1(x)) * self.w3(x))\n\nIt performs w1 and w3 matmuls concurrently and followed by a w2 matmul with\nthe result of the combined w1/w3 linear projection results. This means we\ncould use the idea from the Tensor Parallelism paper to shard the w1/w3 Linear\nlayers in the colwise fashion and shard the w2 Linear layer in the rowwise\nfashion, so that there is only one allreduce communication happening at the\nend of all the three layers. With the PyTorch native Tensor Parallel, we can\nsimply create a parallelize_plan for the FeedForward layer like below:\n\n    \n    \n    from torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel, parallelize_module layer_tp_plan = { # by default ColwiseParallel input layouts is replicated # and RowwiseParallel output layouts is replicated \"feed_foward.w1\": ColwiseParallel(), \"feed_forward.w2\": RowwiseParallel(), \"feed_forward.w3\": ColwiseParallel(), }\n\nThat\u2019s simply how we configure the shardings for the FeedForward layer using\nthe PyTorch Tensor Parallel APIs. Note that users would only need to specify\nhow to shard the individual layers and the communications (for example,\nallreduce) will happen under the hood.\n\nMoving on to the Attention Layer. It consists of wq, wk, wv Linear layers to\nproject input to q/ k / v, and then it performs attention and output\nprojection with the wo Linear layer. Tensor Parallelism here intends to\nperform column-wise sharding for the q/k/v projection and row-wise sharding\nfor the wo linear projection. So we can add the Attention plan to the tp_plan\nthat we just drafted up:\n\n    \n    \n    layer_tp_plan = { # by default ColwiseParallel input layouts is replicated # and RowwiseParallel output layouts is replicated \"attention.wq\": ColwiseParallel(), \"attention.wk\": ColwiseParallel(), \"attention.wv\": ColwiseParallel(), \"attention.wo\": RowwiseParallel(), \"feed_forward.w1\": ColwiseParallel(), \"feed_forward.w2\": RowwiseParallel(), \"feed_forward.w3\": ColwiseParallel(), }\n\nThis is almost the layer_tp_plan we need to apply Tensor Parallelism to the\nTransformerBlock. However, one thing we should be aware is that when sharding\nthe linear layer column-wise, the output of the linear layers would become\nsharded on the last tensor dimension, and the row-wise sharding linear layer\ndirectly accepts an input that shards on the last dimension. If there are any\nmore tensor operations (such as view operations) between the column-wise\nlinear and the row-wise linear, we would need to adjust the relevant shape\nrelated ops to sharded shape.\n\nFor the Llama model, in the attention layer there are couple of view\noperations that are shape related. In particular, column-wise parallel for wq/\nwk/ wv linear layers, the activation tensor is sharded on the num_heads\ndimension, so we would need to adjust the num_heads to local num_heads.\n\nFinally, we need to call parallelize_module API to make the plan for each\nTransformerBlock effective. Under the hood, it distributes the model\nparameters inside Attention and FeedForward layers to DTensors, and registers\ncommunication hooks for model inputs and outputs (before and after each module\nrespectively), if necessary:\n\n    \n    \n    for layer_id, transformer_block in enumerate(model.layers): layer_tp_plan = {...} # i.e. the plan we just generated # Adjust attention module to use the local number of heads attn_layer = transformer_block.attention attn_layer.n_heads = attn_layer.n_heads // tp_mesh.size() attn_layer.n_kv_heads = attn_layer.n_kv_heads // tp_mesh.size() parallelize_module( module=transformer_block, device_mesh=tp_mesh, parallelize_plan=layer_tp_plan, )\n\nNow that we have elaborated the sharding plan for each TransformerBlock, there\nis usually a nn.Embedding in the first layer and a final nn.Linear projection\nlayer, where user could choose row-wise or column-wise sharding to the first\nnn.Embedding and column-wise sharding to the last nn.Linear projection layer\nwith proper input and output layouts specified.\n\nNote\n\nIf the model to be partitioned is too large to fit into CPU memory, one could\neither use meta device initialization (for example, initialize the model on\nmeta device first, shard the layers, and the materialize the model), or\nparallelize the TransformerBlock layer by layer during the Transformer model\ninitialization.\n\n## Apply Sequence Parallel to LayerNorm/RMSNorm layers\n\nSequence Parallel works on top of the Tensor Parallel illustrated above.\nCompared with basic Tensor Parallel, which only shards tensors within the\nAttention modules and FeedForward modules and keep their module inputs and\noutputs (namely activations in the forward pass and gradients in the backward\npass) replicated, Sequence Parallel keeps them sharded on the sequence\ndimension.\n\nIn a typical TransformerBlock, the forward function combines norm layers\n(LayerNorm or RMSNorm), an attention layer, a feed forward layer, and residual\nconnections. For example:\n\n    \n    \n    # forward in a TransformerBlock def forward(self, x): h = x + self.attention(self.attention_norm(x)) out = h + self.feed_forward(self.ffn_norm(h)) return out\n\nIn most use cases, the activations (and gradients) are of the shape [batch\nsize, sequence length, hidden dimension] outside the Attention and FeedForward\nmodules. In the DTensor\u2019s language, Sequence Parallel performs activation\ncomputation using the Shard(1) layout for both forward/backward of the module.\nFollowing the code example earlier, the code below demonstrates how we apply\nSequence Parallel to the norm layers within a TransformerBlock:\n\nFirst let\u2019s import the required dependencies for Sequence Parallel:\n\n    \n    \n    from torch.distributed.tensor.parallel import ( PrepareModuleInput, SequenceParallel, )\n\nNext let\u2019s adjust the layer_tp_plan to enable sequence parallel on the RMSNorm\nlayers:\n\n    \n    \n    layer_tp_plan = { # Now the input and output of SequenceParallel has Shard(1) layouts, # to represent the input/output tensors sharded on the sequence dimension \"attention\": PrepareModuleInput( input_layouts=(Shard(1),), desired_input_layouts=(Replicate(),), ), \"attention.wq\": ColwiseParallel(), \"attention.wk\": ColwiseParallel(), \"attention.wv\": ColwiseParallel(), \"attention.wo\": RowwiseParallel(output_layouts=Shard(1)), \"attention_norm\": SequenceParallel(), \"feed_forward\": PrepareModuleInput( input_layouts=(Shard(1),), desired_input_layouts=(Replicate(),), ), \"feed_forward.w1\": ColwiseParallel(), \"feed_forward.w2\": RowwiseParallel(output_layouts=Shard(1)), \"feed_forward.w3\": ColwiseParallel(), \"ffn_norm\": SequenceParallel(), }\n\nOne can see we now use PrepareModuleInput to modify the module input layouts\nto the Attention and FeedForward layers from Shard(1) to Replicate(), and mark\ntheir output layouts as Shard(1). Just like what happens to Tensor\nParallelism, one only needs to specify the tensor sharding layouts of the\ninputs and outputs, and the communication between layers will happen\nautomatically.\n\nNote that with Sequence Parallel, we assume the inputs and outputs of a\nTransformerBlock are always sharded on the sequence dimension, so that\nmultiple TransformerBlocks can be concatenated seamlessly. The only exception\nis that the input to the first TransformerBlock is replicated from the data\nloaders, so it has to be converted explicitly:\n\n    \n    \n    model = parallelize_module( model, tp_mesh, \"layers.0\": PrepareModuleInput( input_layouts=(Replicate(),), desired_input_layouts=(Shard(1),), ), )\n\n## Apply Loss Parallel\n\nLoss Parallel is a related technique to save memory and communication when the\nloss function is computed, as model outputs are usually very large. In Loss\nParallel, when the model outputs are sharded on the (often huge) vocabulary\ndimension, the cross-entropy loss can be computed efficiently, without\ngathering all the model outputs to every single GPU. This not only\nsignificantly reduces the memory consumption, but also improves training speed\nby reducing communication overhead and doing sharded computation in parallel.\nThe picture below briefly illustrates how Loss Parallel avoids gathering all\nmodel outputs to every GPU by doing sharded computation.\n\nFigure 2. Cross-entropy loss forward computation with loss parallel on one\nGPU. Blue represents sharded tensors; green represents replicated tensors;\nyellow represents tensors with partial values (to be all-reduced). Black\narrows are local computations; red arrows are functional collectives among\nGPUs.\n\nIn the PyTorch Tensor Parallel API, Loss Parallel can be enabled via a context\nmanager loss_parallel, with which one can directly use\ntorch.nn.functional.cross_entropy or torch.nn.CrossEntropyLoss without\nmodifying other parts of their code.\n\nTo apply Loss Parallel, the model predictions, usually of the shape [batch\nsize, sequence length, vocabulary size], should be sharded on the vocabulary\ndimension. This can be easily done via marking the output layouts of the last\nlinear projection layer output:\n\n    \n    \n    model = parallelize_module( model, tp_mesh, { \"output\": ColwiseParallel( input_layouts=Shard(1), # use DTensor as the output use_local_output=False, ), \"norm\": SequenceParallel(), \"layers.0\": PrepareModuleInput( input_layouts=(Replicate(),), desired_input_layouts=(Shard(1),), ), }, )\n\nIn the code above, we also apply Sequence Parallel to the norm layer before\noutput. We apply use_local_output=False to let the output stay as a DTensor,\nto work with the loss_parallel context manager. After that, one can simply\ncall the cross_entropy loss function as is shown below. Note that the backward\ncomputation also needs to happen within the context.\n\n    \n    \n    import torch.nn.functional as F from torch.distributed.tensor.parallel import loss_parallel pred = model(input_ids) with loss_parallel(): # assuming pred and labels are of the shape [batch, seq, vocab] loss = F.cross_entropy(pred.flatten(0, 1), labels.flatten(0, 1)) loss.backward()\n\n## Combine Tensor Parallel with Fully Sharded Data Parallel together\n\nNow that we have shown how to apply Tensor/Sequence Parallel to the model, let\nus also take a look at how Tensor Parallel and Fully Sharded Data Parallel\ncould work together. Since Tensor Parallelism incurs communications that block\nthe computation, we want to make sure it runs within a fast communication\nchannel, such as NVLink. In practice, we usually apply Tensor Parallel within\neach host, and apply Fully Sharded Data Parallel across the hosts.\n\nFigure 3. FSDP and TP work on separate device dimensions, FSDP communication\nhappens inter-host and TP communication happens intra-host.\n\nThis 2-D parallelism pattern can be easily expressed via a 2-D DeviceMesh, and\nwe just need pass each \u201csub\u201d DeviceMesh to each individual parallelism APIs:\n\n    \n    \n    from torch.distributed.device_mesh import init_device_mesh from torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel, parallelize_module from torch.distributed.fsdp import FullyShardedDataParallel as FSDP # i.e. 2-D mesh is [dp, tp], training on 64 GPUs that performs 8 way DP and 8 way TP mesh_2d = init_device_mesh(\"cuda\", (8, 8)) tp_mesh = mesh_2d[\"tp\"] # a submesh that connects intra-host devices dp_mesh = mesh_2d[\"dp\"] # a submesh that connects inter-host devices model = Model(...) tp_plan = {...} # apply Tensor Parallel intra-host on tp_mesh model_tp = parallelize_module(model, tp_mesh, tp_plan) # apply FSDP inter-host on dp_mesh model_2d = FSDP(model_tp, device_mesh=dp_mesh, use_orig_params=True, ...)\n\nThis would allow us to easily apply Tensor Parallel within each host (intra-\nhost) and apply FSDP across hosts (inter-hosts), with 0-code changes to the\nLlama model. The Tensor(Model) Parallel and Data Parallel techniques combined\ntogether provides the ability to continue increasing model size and training\nefficiently using a large number of GPUs.\n\n## Conclusion\n\nThis tutorial demonstrates how to train a large Transformer-like model across\nhundreds to thousands of GPUs using Tensor Parallel in combination with Fully\nSharded Data Parallel. It explains how to apply Tensor Parallel to different\nparts of the model, with no code changes to the model itself. Tensor Parallel\nis a efficient model parallelism technique for large scale training.\n\nTo see the complete end to end code example explained in this tutorial, please\nrefer to the Tensor Parallel examples in the pytorch/examples repository.\n\nNext Previous\n\nRate this Tutorial\n\n\u00a9 Copyright 2024, PyTorch.\n\nBuilt with Sphinx using a theme provided by Read the Docs.\n\n  * Large Scale Transformer model training with Tensor Parallel (TP)\n\n    * How Tensor Parallel works?\n    * When and Why you should apply Tensor Parallel\n    * How to apply Tensor Parallel\n    * Apply Sequence Parallel to LayerNorm/RMSNorm layers\n    * Apply Loss Parallel\n    * Combine Tensor Parallel with Fully Sharded Data Parallel together\n    * Conclusion\n\n## Docs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\n\n## Tutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\n\n## Resources\n\nFind development resources and get your questions answered\n\nView Resources\n\n  * PyTorch\n  * Get Started\n  * Features\n  * Ecosystem\n  * Blog\n  * Contributing\n\n  * Resources\n  * Tutorials\n  * Docs\n  * Discuss\n  * Github Issues\n  * Brand Guidelines\n\n  * Stay up to date\n  * Facebook\n  * Twitter\n  * YouTube\n  * LinkedIn\n\n  * PyTorch Podcasts\n  * Spotify\n  * Apple\n  * Google\n  * Amazon\n\n  * Terms\n  * |\n  * Privacy\n\n\u00a9 Copyright The Linux Foundation. The PyTorch Foundation is a project of The\nLinux Foundation. For web site terms of use, trademark policy and other\npolicies applicable to The PyTorch Foundation please see\nwww.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch\nopen source project, which has been established as PyTorch Project a Series of\nLF Projects, LLC. For policies applicable to the PyTorch Project a Series of\nLF Projects, LLC, please see www.lfprojects.org/policies/.\n\nTo analyze traffic and optimize your experience, we serve cookies on this\nsite. By clicking or navigating, you agree to allow our usage of cookies. As\nthe current maintainers of this site, Facebook\u2019s Cookies Policy applies. Learn\nmore, including about available controls: Cookies Policy.\n\n  * Get Started\n  * Ecosystem\n  * Mobile\n  * Blog\n  * Tutorials\n  * Docs\n    * PyTorch\n    * torchaudio\n    * torchtext\n    * torchvision\n    * torcharrow\n    * TorchData\n    * TorchRec\n    * TorchServe\n    * TorchX\n    * PyTorch on XLA Devices\n  * Resources\n    * About\n    * PyTorch Foundation\n    * Community\n    * Community Stories\n    * Developer Resources\n    * Events\n    * Forums\n    * Models (Beta)\n  * Github\n\n", "frontpage": false}
