{"aid": "40252639", "title": "Equally distributing uploaded files with MD5 hashing of timestamp", "url": "https://hereket.com/posts/hash-naming-uploaded-files/", "domain": "hereket.com", "votes": 1, "user": "libcheet", "posted_at": "2024-05-03 21:34:04", "comments": 0, "source_title": "Equally distributing uploaded files with md5 hashing of timestamp | Hereket", "source_text": "Equally distributing uploaded files with md5 hashing of timestamp | Hereket\n\nHereket\n\n# Equally distributing uploaded files with md5 hashing of timestamp\n\n2024-05-01\n\nLately I have been digging into django source code to find out how to get\naccess to actual file field for renaming during upload in general case. This\nturned out to be impossible because this information is lost as django passes\nwhole instance to upload_to instead of file field itself. There was a need to\ndo some magic during file upload based on the actual file content hash. But\nduring the exploration I found that for a long time I have been using two\nlevel depth folder structure to limit amount of files stored per folder. It\nlooked a bit reduntant for my usecases and led me to exploration of how should\nI split the files into folders.\n\nSo let\u2019s say we have some model which stores an ImageField in it with\nupload_to set to some function that generates path.\n\n    \n    \n    class Item(models.Model): name = models.CharField(max_length=256) description = models.TextField() image = models.ImageField(upload_to=item_image_path_gen, null=True)\n\nIn the item_image_path_gen function we the Item instance and name of the file\nthat was passed. The name can be anything and I prefer to normalize it\nsomething. Without trying to be fancy I usually just use an md5 on the file\ncontents. It is not required but there were some projects where using this\napproach helped finding \u2018missing\u2019 files faster.\n\nSo let\u2019s say that user passed us file \u2018Photo 000323(3) .jpg\u2019. Of course we\ndon\u2019t whant to store it directly with this name in our system. First of all it\ncould be a privacy issue as the name can give out too much information and\nsecond we don\u2019t want to have any space or some random characters which might\nor might not be supported by current file system.\n\nLet\u2019s assume that this photo is md5 hashed into\nffbafafd06f9245e76b5dce27479d504 we append whatever is the file extension to\nget ffbafafd06f9245e76b5dce27479d504.png. For a two level depth folder\nstructure we take first [:a] and [a:b] characters and put the image/file into\nrespective folders.\n\nWhy not put everything into one folder? Well there are variaty of reasons\nstarting from different file systems not liking it and all the way to slow\nopening and listing in some other cases. Depending on the circumstances I\ncould try to keep it under 1k but even 4k seems not that bad.\n\nSo once I stumbled upon 3 chars per folder split in one of my projects I\nthought that it should be that it could be too much splitting into folders but\nis it? Let\u2019s try to calculate. The plan is for the system to be handle several\nmillion files with upper limit of 100 million files (with average of 10kb per\nfile).\n\nIf you used only decimals in our file name with folder depth of 2 then we\nwould have 100_000_000 files / (999999 folders) = 100.2 files per folder. If\nwe use hex numbers (as in example md5) for the file name then 3 chars make it\n161616 = 4094 folders per level or total of 40964096=16777216 total possible\nfolders to store files or just about 6 files per folder. Even if we had a had\njust 1k files per folder we are still very very under budgeted.\n\nBut had an issue of django not passing file field and instead giving us just\ninstance. The issue with this is that we totally lose any information about\nwhat we are working on. Very strange decision by django team as if they just\npassed the field itself, if we needed instance, we could have gotten it out of\nthe field with just on lookup. Anyways we are not trying to solve parameter\npassing issue but rather solve our current problem. The problem is that I have\na lot of models with different file and image fields in it and wanted to have\nsingle function handling all the naming work.\n\nPossible solution is to use file name as hash parameter. It is not suitable as\npassed files could have the same name which will will skew our distribution\nand creates uncertainty which we tried reduce. We could pass the field name to\nthe function and get out file contents with that. But it makes our code\nfragile and prone to human input error which is also undesirable.\n\nThe approach I wanted to try is to use time as hash function input and\ncalculated file names out of that. This sound as a good idea but how good is\nmd5 output distribution. I am not a professional matematician or cryptographer\nand could not trust my theoretical reasoning about it but I could write small\nsimulations and get approximate result emperically. This is good enough for my\nusecase. So I did it.\n\nFor this I used regular python datetime module\u2019s now() method and then get\nhash value of stringified timestamp:\n\n    \n    \n    current_time = datetime.datetime.now().timestamp() calculated_hash = hashlib.md5(str(current_time).encode('utf-8')).hexdigest()\n\nThen I just calculated hash value 100 million times successively for each\nreported timestamp. All timestamp values will be very close and should show\nanomalies if there is any.\n\nHere is folder distribution:\n\nFirst let\u2019s see how the the number of folders got distributed in our second\nlevel as our first level got completely saturated. So on the first level we\nfully used all our 4k space (for 3 char hex). One the second level we also\ncame close to fully filling all our available folder space. The full amount of\nsecond depth folder count per first level folder is 4096 and in the graph we\nthe that all values are balancing around max value.\n\nLet\u2019s look at file distribution:\n\nStrange. From th first glance on the graph it looks likes each folder has more\nthat 16 files. But in our initial calculations we got about 5 files per\nfolder. So either md5 is not distributing our files evenly or we\nmiscalculated. If we look on the X axis we see that there are more than 16\nmillion folders or 16_733_900 to be exact. If we have 16-17 files per folder\nthen there are around 300 million files and that is not correct. The issue is\nthat on X axis we have 16 million values and drawing that many lines just\ndon\u2019t show correct information about true distribution.\n\nDiffent slicing of the data (looks like skewed normal distribution):\n\nIn this graph on the X axis we show number of files per folder and on Y axis\nwe show number of folder with that many files in it. Here we can see that\namount of folder that have about 5 files is the largest group. Issue with the\nprevious graph was that there are 14000 folder containing 15 files, 5000\nfolders containing 16 fiels, ... , 40 folders containing 20 files. Even though\non this graph this numbers are almost zero an looks like do not contribute to\nthe overal distribution, on previous graph they had lines attributed to them\nand with that many lines drawn we can\u2019t see the picture.\n\nSo it looks md5 is good enough for the usecase of file distribution. But store\non average 5 files per folder seems very wasteful. To store 100 million files\nwe created 16 million folders. So let\u2019s try the same thing but with different\nprefix lenth for folder names.\n\nDistribution for 2x2 prefixes:\n\nFirst thing is the dramatic decrease in depth 2 folder count. It went down\nfrom 16 million to just about 60 thousand. Number of files is between 1450 and\n1600. If our goal was to have 1000 files we woulb be over budget. In theory we\nshould have 100_000_000 / (256 * 256) = 1525. It looks like experiment is\nlines up pretty good with theory that md5 distriutes evenly.\n\nDistribution for 1x1 prefixes:\n\nThis graph start reminding something from chaos theory but let\u2019s not search\nfor meaning in shapes. Now we get only on 256 folders in depth 2 and about\nabout 390_000 files per folder. This is totally not suitable for the usecase\nof not having more than 4k files per folder but it is good as a datapoint to\ncompare with. This might not be usable in the form of 1x1 distribution but\ncould be usable in 1x3. Which would give about 1500 files per folder and yet\nyou would get 16 distinct blocks to distribute over 16 computers/disks.\n\nIt looks like for my usecase a 2x2 distribution pattern seems like more\nappropriate compared to other methods. Here is a list of some ways to\ndistribute files and them amount of files per folder as a quick lookup. The\nformat is like AxB where A is how much characters we take from start of the\nhash for names of folders in depth 1 and B is subsequent number of characters\nto take from hash for name of folders in depth 2.\n\n    \n    \n    0x0: 100000000 0x1: 6250000 0x2: 390625 0x3: 24414 1x0: 6250000 1x1: 390625 1x2: 24414 1x3: 1526 2x0: 390625 2x1: 24414 2x2: 1526 2x3: 95 3x0: 24414 3x1: 1526 3x2: 95 3x3: 6\n\nIt was a bit crazy and at the same time interesting dive into ways of how to\ndistribute files. Now I got solid numbers to use for different usecases that I\nhave and got a few scripts to play around for quick calculations. Initial my\nidea was to use different hashing algorithms as I though that md5 was too bad\nand didstribution of files should be imporoved. But experiments showed that it\nis totally fine to use for this task and there is no need to look for better\nalternatives.\n\nThe way to divide files described here is too simplistic but good enough for\nmy projects. To get more fine grained access you could switch from using hex\ncharacter to decimals or even go crazier and take first characters and convert\nthem to real numbers. This will give access for a very tuned access to\nsplitting files in more smarter ways.\n\n\u00a9 2024, Hereket\n\n", "frontpage": false}
