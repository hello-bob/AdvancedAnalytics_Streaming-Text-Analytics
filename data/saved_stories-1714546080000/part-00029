{"aid": "40216363", "title": "llama.cpp bfloat16 support", "url": "https://github.com/ggerganov/llama.cpp/pull/6412", "domain": "github.com/ggerganov", "votes": 2, "user": "indigodaddy", "posted_at": "2024-04-30 21:07:56", "comments": 0, "source_title": "Introduce bfloat16 support by jart \u00b7 Pull Request #6412 \u00b7 ggerganov/llama.cpp", "source_text": "Introduce bfloat16 support by jart \u00b7 Pull Request #6412 \u00b7 ggerganov/llama.cpp\n\u00b7 GitHub\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nggerganov / llama.cpp Public\n\n  * Notifications\n  * Fork 8.1k\n  * Star 57.2k\n\nJump to bottom\n\n# Introduce bfloat16 support #6412\n\nOpen\n\njart wants to merge 8 commits into ggerganov:master\n\nfrom jart:bf16\n\nOpen\n\n# Introduce bfloat16 support #6412\n\njart wants to merge 8 commits into ggerganov:master from jart:bf16\n\n+1,228 \u2212102\n\n## Conversation\n\nContributor\n\n###\n\njart commented Mar 31, 2024\n\nMany models on Hugging Face (e.g. Mistral, TinyLLaMA) use bfloat16 as their\ncanonical floating point format.\n\n    \n    \n    \u250csign \u2502 \u2502 \u250cexponent \u2502 \u2502 \u2502 \u2502 \u250cmantissa \u2502 \u2502 \u2502 \u2502\u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510\u250c\u2500\u2534\u2500\u2500\u2500\u2510 0b0000000000000000 brain16\n\nThis encoding has the same number of exponent bits as float32. That makes\nconversion relatively straightforward, even in the absence of hardware\nsupport. For example, converting brain16 to binary32 means simply shifting 16\nbits to the left.\n\n    \n    \n    \u250csign \u2502 \u2502 \u250cexponent \u2502 \u2502 \u2502 \u2502 \u250cmantissa \u2502 \u2502 \u2502 \u2502\u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510\u250c\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 0b00000000000000000000000000000000 IEEE binary32\n\nThe issue is that converting weights from bf16 to fp16 will cause 3 bits of\nknowledge to be lost. There is currently no way to evaluate models like\nMistral at full fidelity, without f32, using llama.cpp.\n\n    \n    \n    \u250csign \u2502 \u2502 \u250cexponent \u2502 \u2502 \u2502 \u2502 \u250cmantissa \u2502 \u2502 \u2502 \u2502\u250c\u2500\u2534\u2500\u2510\u250c\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510 0b0000000000000000 IEEE binary16\n\nThis change fixes that, by adding a bf16 data type to GGML. Support for CPU\ninference has been implemented along with optimizations for the AVX2, AVX512F,\nand AVX512BF16 ISAs. Perplexity on Mistral 7b 0.2 improves somewhere around\n-0.0024 to -0.0046 compared to using fp16\n\njart force-pushed the bf16 branch 3 times, most recently from 436956a to\ne52d5e5 Compare March 31, 2024 15:09\n\nContributor\n\n###\n\ngithub-actions bot commented Mar 31, 2024 \u2022\n\n\ud83d\udcc8 llama.cpp server for bench-server-baseline on Standard_NC4as_T4_v3: 523\niterations \ud83d\ude80\n\n  * Concurrent users: 8, duration: 10m\n  * HTTP request : avg=8964.0ms p(90)=25761.73ms fails=0, finish reason: stop=523 truncated=0\n  * Prompt processing (pp): avg=237.3tk/s p(90)=697.6tk/s total=203.61tk/s\n  * Token generation (tg): avg=101.36tk/s p(90)=283.09tk/s total=132.93tk/s\n  * ggml-org/models/phi-2/ggml-model-q4_0.gguf parallel=8 ctx-size=16384 ngl=33 batch-size=2048 ubatch-size=256 pp=1024 pp+tg=2048 branch=bf16 commit=44d5c7070f3b33714c3d92b6e3c757e00877b4e1\n\n  \n---  \n  \nCollaborator\n\n###\n\nJohannesGaessler commented Mar 31, 2024\n\n> The issue is that converting weights from bf16 to fp16 will cause 3 bits of\n> knowledge to be lost. There is currently no way to evaluate models like\n> Mistral at full fidelity, without f32, using llama.cpp.\n\nIEEE 754 half precision floats can store values in the range to . For all\nvalues within this range there is no precision loss whatsoever when converting\nfrom BF16. And I would be very surprised if even a single model weight were to\nbe outside this range since these would also be leading to vanishing/exploding\ngradients.\n\n> Perplexity on Mistral 7b 0.2 improves somewhere around -0.0024 to -0.0046\n> compared to using fp16\n\nI think this is not due to any change in the weights but rather due to a\ndifference in rounding error in the accumulator. I expect this improvement to\nnot be consistent across models/text corpuses and I also expect there to be no\nstatistically significant improvement at all for a large enough sample size.  \n---  \n  \n###\n\nsorasoras commented Mar 31, 2024\n\nThere are some different between quant from BF16-FP32 to BF16-FP16. It's not\nthe same model when compare PPL between FP16 and FP32, and it behave\ndifferently. It would be interest to inference BF16 directly.  \n---  \n  \nContributor Author\n\n###\n\njart commented Mar 31, 2024\n\n@JohannesGaessler Only 13% of bf16 numbers can be represented accurately by a\nbf16 -> fp16 conversion. https://justine.lol/tmp/bf16-to-fp16.txt Yes, the\nvast majority of weights cluster within that 13%. By my calculation, only\n0.29101% of Mistral 7b's numbers are broken. I want those numbers. I also\ndon't want to accept limits on what's possible based on what's normal. Someone\nmight find those broken intervals useful. But if that doesn't persuade you,\nconsider this. I recently bought a Threadripper and it offers hardware\nacceleration for bf16 but not fp16. So this change is not just good for\naccuracy, it can be good for performance too.  \n---  \n  \nCollaborator\n\n###\n\nJohannesGaessler commented Mar 31, 2024\n\n> By my calculation, only 0.29101% of Mistral 7b's numbers are broken.\n\nBroken in what sense? Numbers being flushed to zero is not an issue because\nthe difference between 0 and almost 0 is negligible for matrix multiplication.\n\n> I recently bought a Threadripper and it offers hardware acceleration for\n> bf16 but not fp16. So this change is not just good for accuracy, it can be\n> good for performance too.\n\nThe performance point is valid.In terms of numerical precision, this is the\nbottom line for me: I very much expect the difference between IEEE 754 half\nprecision and bfloat to be completely negligible. I'm not telling you this out\nof malice but because I want contributors to spend their time in a way that is\nuseful. If it turns out I'm wrong I will happily accept it.  \n---  \n  \nContributor Author\n\n###\n\njart commented Apr 1, 2024\n\nYou might find the differences negligible, but it's important to me. I want\nllamafile to be able to deliver, to the best of its ability, whatever number\nof bits are claimed, even if those extra bits are only good for audiophiles.\nIn my day-to-day work as a developer, I feel more comfortable being able to\ncompare my tradeoffs with the master copies. Furthermore, I need this data\ntype in order to be able to exploit the full capabilities of my hardware.Am I\ncorrect in understanding you won't merge this? That surprises me. This project\nrecently accepted nine novel \"IQ\" quantization formats, which I know very\nlittle about. So I was under the impression there was a certain level of\ninclusiveness. Why would you not support the data type that companies like\nMistral and Google widely use?  \n---  \n  \nCollaborator\n\n###\n\nJohannesGaessler commented Apr 1, 2024\n\n> Am I correct in understanding you won't merge this? That surprises me. This\n> project recently accepted nine novel \"IQ\" quantization formats, which I know\n> very little about. So I was under the impression there was a certain level\n> of inclusiveness. Why would you not support the data type that companies\n> like Mistral and Google widely use?\n\nThe ultimate decision of what gets merged is not up to me. And I am not at all\nopposed to adding bfloat support. I only want to stress that I do not expect\nthe gains from this feature to be in any way proportional to the amount of\neffort it will take. As such I personally will not invest time into bfloat\nsupport by e.g. modifying the CUDA code. If other devs want to do it that is\ntheir decision.  \n---  \n  \nContributor Author\n\n###\n\njart commented Apr 1, 2024\n\nI don't hold any demands on your time. In terms of resources, Mozilla is\nsponsoring me to help llama.cpp so you've got a lot more resources than\nbefore. At the moment, I only need this to work on CPU however I'll likely get\npersonal enjoyment at some point in getting this to work on CUDA and Metal\ntoo. Particularly Metal, since I've been looking for a good reason to learn it\nfor some time.  \n---  \n  \n###\n\nsorasoras commented Apr 1, 2024\n\n> I don't hold any demands on your time. In terms of resources, Mozilla is\n> sponsoring me to help llama.cpp so you've got a lot more resources than\n> before. At the moment, I only need this to work on CPU however I'll likely\n> get personal enjoyment at some point in getting this to work on CUDA and\n> Metal too. Particularly Metal, since I've been looking for a good reason to\n> learn it for some time.\n\nI would imagine older cuda hardware wouldn't support it due to bf16 unsupport\non Pascal. What's solution to that?  \n---  \n  \nContributor Author\n\n###\n\njart commented Apr 1, 2024\n\nHere's the decoding process for bfloat16:\n\n    \n    \n    typedef struct { uint16_t x; } ggml_bf16_t; /** * Converts brain16 to float32. */ static inline float ggml_bf16_to_fp32(ggml_bf16_t h) { union { float f; uint32_t i; } u; u.i = (uint32_t)h.x << 16; return u.f; }\n\nSo the only thing old CUDA needs to do, is left shift the bf16 number by 16\nbits, and then it becomes a float.  \n---  \n  \nphymbert mentioned this pull request Apr 1, 2024\n\nserver: bench: continuous performance testing #6233\n\nOpen\n\n16 tasks\n\nCollaborator\n\n###\n\nArtefact2 commented Apr 1, 2024\n\nI think bf16 support is nice to have in GGUF, if only because it makes\nquantizing a lot of models much less I/O intensive. Consider changing\nconvert.py to make use of it.  \n---  \n  \nCollaborator\n\n###\n\nJohannesGaessler commented Apr 1, 2024\n\nRelevant for discussion: Mozilla-Ocho/llamafile@ef0307eIt seems there seem to\nbe at least some values above the maximum value representable by IEEE 754 half\nprecision floats. @jart do you know in which specific matrices these weights\nshow up? Depending on where they are relative to softmax this could be an\nissue.  \n---  \n  \nContributor\n\n###\n\ncpumaxx commented Apr 1, 2024\n\nIs there anything special needed to see performance gains? I\ncloned/built/tested this PR branch and am seeing no change in performance on\nCPU (CUDA support flags disabled at compile time)  \n---  \n  \n###\n\nsorasoras commented Apr 1, 2024\n\n> Is there anything special needed to see performance gains? I\n> cloned/built/tested this PR branch and am seeing no change in performance on\n> CPU (CUDA support flags disabled at compile time)\n\nFor CPU, I think you need something that support bf16 acceleration like\nAVX512VNNI? also, you need conversion script that just copy BF16 weight from\npy to GGUF to get any benefit.  \n---  \n  \nContributor\n\n###\n\ncpumaxx commented Apr 1, 2024\n\n> For CPU, I think you need something that support bf16 acceleration like\n> AVX512VNNI? also, you need conversion script that just copy BF16 weight from\n> py to GGUF to get any benefit.\n\nsystem_info: n_threads = 55 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |Hardware-wise I think I have what's needed. Is the conversion script already available? I don't see it in any obvious place in this PR  \n---  \n  \n###\n\nsorasoras commented Apr 2, 2024\n\n> > For CPU, I think you need something that support bf16 acceleration like\n> AVX512VNNI? also, you need conversion script that just copy BF16 weight from\n> py to GGUF to get any benefit.\n>\n> system_info: n_threads = 55 / 128 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 |Hardware-wise I think I have what's needed. Is the conversion script already available? I don't see it in any obvious place in this PR\n\nhttps://justine.lol/matmul/ I think the full implementation is in llamafile\nside. LLM Performance on AMD Ryzen Threadripper PRO 7995WX w/ 96 cores\n($10,000)  \n---  \n  \nContributor\n\n###\n\ncpumaxx commented Apr 2, 2024 \u2022\n\n> I think the full implementation is in llamafile side.\n\nWhat should be expected in llama.cpp from this patch specifically? I'm seeing\nabout 6% speed increase on prompt processing and inference and I've pulled and\nbuilt the master, avx512vnni, sgemm and bf16 branches. Each of them perform\nalmost identically on a Q8 70b. I'm on EPYC Genoa, so if anything I'd expect\nbetter results than that threadripper system.  \n---  \n  \njart force-pushed the bf16 branch from e52d5e5 to 07cebab Compare April 2,\n2024 12:38\n\nContributor Author\n\n###\n\njart commented Apr 2, 2024\n\n@Artefact2 I've updated gguf-py/gguf/constants.py so that BF16 is listed. I\nhave no idea how to make the Python script generate BF16 GGML files. What I've\nbeen doing is running convert.py --outtype f32 and then running the ./quantize\nggml-model-f32.gguf ggml-model-bf16.gguf bf16 program. Please take a look.  \n---  \n  \nContributor Author\n\n###\n\njart commented Apr 2, 2024\n\n@cpumaxx This change only adds support for bf16. Once #6414 the next thing\nI'll do is upstream the llamafile bfloat16 kernels. Here's what one of them\nlooks like:I'm working on ARM64 bfloat16 kernels tonight.  \n---  \n  \nContributor\n\n###\n\ncpumaxx commented Apr 2, 2024\n\n> the next thing I'll do is upstream the llamafile bfloat16 kernels\n\nNice. I'll keep an eye out for them. Is there a relevant branch on your\nllama.cpp fork I can test prior to a PR, or do you still need to merge changes\nalready in llamafile?  \n---  \n  \nContributor Author\n\n###\n\njart commented Apr 2, 2024\n\n@cpumaxx Could you download\nhttps://huggingface.co/jartine/Mistral-7B-Instruct-v0.2-llamafile/blob/main/mistral-7b-instruct-v0.2.BF16.gguf\nand then build the code in the branch I just created\nhttps://github.com/jart/llama.cpp/tree/unified which unifies #6412 and #6414?\nThanks!  \n---  \n  \nContributor Author\n\n###\n\njart commented Apr 2, 2024\n\nHere's an example of what you should expect to see with that branch.\n\n    \n    \n    wget https://huggingface.co/jartine/Mistral-7B-Instruct-v0.2-llamafile/resolve/main/mistral-7b-instruct-v0.2.BF16.gguf wget https://justine.lol/tmp/getty.txt make -j32 main && ./main -m /disk/mistral/mistral-7b-instruct-v0.2.BF16.gguf -f ~/getty.txt -n 22 --temp 0 [...] It is for us, the living, rather to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. llama_print_timings: load time = 773.90 ms llama_print_timings: sample time = 0.46 ms / 22 runs ( 0.02 ms per token, 48034.93 tokens per second) llama_print_timings: prompt eval time = 407.51 ms / 215 tokens ( 1.90 ms per token, 527.59 tokens per second) llama_print_timings: eval time = 1230.05 ms / 21 runs ( 58.57 ms per token, 17.07 tokens per second) llama_print_timings: total time = 1643.99 ms / 236 tokens Log end\n\nEPYC is for servers so I've heard they generally run at much lower clock rates\nthan Threadripper Pro. So if you get a lower number than 530 tok/sec then try\ncomparing it to llama.cpp at HEAD using the Mistral 7b f16 weights.  \n---  \n  \nContributor\n\n###\n\ncpumaxx commented Apr 2, 2024\n\n> Here's an example of what you should expect to see with that branch.\n>  \n>  \n>     llama_print_timings: load time = 773.90 ms llama_print_timings: sample\n> time = 0.46 ms / 22 runs ( 0.02 ms per token, 48034.93 tokens per second)\n> llama_print_timings: prompt eval time = 407.51 ms / 215 tokens ( 1.90 ms per\n> token, 527.59 tokens per second) llama_print_timings: eval time = 1230.05 ms\n> / 21 runs ( 58.57 ms per token, 17.07 tokens per second)\n> llama_print_timings: total time = 1643.99 ms / 236 tokens Log end\n>\n> EPYC is for servers so I've heard they generally run at much lower clock\n> rates than Threadripper Pro. So if you get a lower number than 530 tok/sec\n> then try comparing it to llama.cpp at HEAD using the Mistral 7b f16 weights.\n\nMy system is a dual 64 core 9334 running with a 3.9ghz boost clock I've got\nNPS set at 4 (so 8 numa nodes) for development reasons, which may be effecting\nresults. I tested your unified branch vs ggerganov master, and I'm seeing a\nsevere speed regression:\n\n> /usr/src/llama.cpp.jart# ./main -m\n> /media/models/bf16/mistral-7b-instruct-v0.2.BF16.gguf -n 22 --temp 0\n\n> system_info: n_threads = 64 / 128 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 |\n\n> Question: Let i = 11 + -11. Let g = 1.1 + - llama_print_timings: sample time\n> = 0.53 ms / 22 runs ( 0.02 ms per token, 41666.67 tokens per second)\n> llama_print_timings: prompt eval time = 0.00 ms / 1 tokens ( 0.00 ms per\n> token, inf tokens per second) llama_print_timings: eval time = 4618.55 ms /\n> 22 runs ( 209.93 ms per token, 4.76 tokens per second) llama_print_timings:\n> total time = 4624.48 ms / 23 tokens\n\nvs\n\n> /usr/src/llama.cpp.master.clean# ./main -m /media/models/bf16/ggml-\n> model-f16.gguf -n 22 --temp 0\n\n> llama_print_timings: sample time = 0.61 ms / 22 runs ( 0.03 ms per token,\n> 36184.21 tokens per second) llama_print_timings: prompt eval time = 0.00 ms\n> / 1 tokens ( 0.00 ms per token, inf tokens per second) llama_print_timings:\n> eval time = 1363.28 ms / 22 runs ( 61.97 ms per token, 16.14 tokens per\n> second) llama_print_timings: total time = 1369.01 ms / 23 tokens\n\nThis was with identical build flags and after dropping all caches for a level\nplaying field.Anything else I should be trying in order to see the speedup?  \n---  \n  \nContributor Author\n\n###\n\njart commented Apr 2, 2024\n\nCould you pass the flag -f getty.txt please after you've downloaded that file\nfrom the link above? Then re-post your results.  \n---  \n  \nContributor\n\n###\n\ncpumaxx commented Apr 2, 2024 \u2022\n\n> Four score and seven years ago our fathers brought forth, upon this\n> continent, a new nation, conceived in Liberty, and dedicated to the\n> proposition that all men are created equal.Now we are engaged in a great\n> civil war, testing whether that nation, or any nation so conceived, and so\n> dedicated, can long endure. We are met here on a great battlefield of that\n> war. We have come to dedicate a portion of it, as a final resting place for\n> those who here gave their lives that that nation might live. It is\n> altogether fitting and proper that we should do this.But in a larger sense,\n> we can not dedicate we can not consecrate we can not hallow this ground. The\n> brave men, living and dead, who struggled here, have consecrated it far\n> above our poor power to add or detract. The world will little note,nor long\n> remember, what we say here, but can never forget what they did here.It is\n> for us, the living, rather to be dedicated here to the unfinished work which\n> they who fought here have thus far so nobly advanced.\n\n> llama_print_timings: load time = 1045.42 ms llama_print_timings: sample time\n> = 0.48 ms / 22 runs ( 0.02 ms per token, 45738.05 tokens per second)\n> llama_print_timings: prompt eval time = 728.93 ms / 215 tokens ( 3.39 ms per\n> token, 294.95 tokens per second) llama_print_timings: eval time = 4442.57 ms\n> / 21 runs ( 211.55 ms per token, 4.73 tokens per second)\n> llama_print_timings: total time = 5178.83 ms / 236 tokens  \n  \n---  \n  \nContributor\n\n###\n\ncpumaxx commented Apr 2, 2024\n\nUpdate: I was suspicious of the large delta between the unified branch and\nmaster, so I manually downloaded the official mistral 7b and converted to FP16\ngguf manually. Doing so has made the difference essentially disappear. I'm now\nseeing 4.73t/s on bf16 and 4.81t/s on fp16. CPU prompt processing is about 3x\nfaster on your branch. I'll continue to test different setting to see if I can\npin down why and update here if I find anything.  \n---  \n  \nContributor\n\n###\n\ncpumaxx commented Apr 2, 2024\n\nI've re-run the tests with \"-t 16 --numa isolate --no-mmap\" flags in order to\neliminate any confounding memory locality issues, and there is still the same\n0.10t/s gap with FP16 being marginally faster than BF16. Prompt processing\nspeedup remains constant at 3x faster on the PR branch.  \n---  \n  \njart force-pushed the bf16 branch 2 times, most recently from 3b8a0e3 to\n44d5c70 Compare April 4, 2024 03:07\n\nggerganov mentioned this pull request Apr 8, 2024\n\nadd loongarch lsx and lasx optimize code #6454\n\nOpen\n\nggerganov approved these changes Apr 9, 2024\n\nView reviewed changes\n\nOwner\n\n###\n\nggerganov left a comment \u2022\n\nThere was a problem hiding this comment.\n\n### Choose a reason for hiding this comment\n\nThe reason will be displayed to describe this comment to others. Learn more.\n\nI think this is OK to merge. I'm looking for ways to reduce some code that is\nnot really needed atm, so I would recommend to remove the following overloads\nunless you consider some of those important:\n\n  * ggml_compute_forward_alibi_bf16 - the ggml_alibi operator will be soon deprecated in favor of the more general ggml_soft_max_ext\n  * ggml_compute_forward_flash_attn_bf16 - will be obsoleted by ggml : add Flash Attention #5021\n  * ggml_compute_forward_flash_ff_bf16 - the ggml_flash_ff operator was an experiment from the early days of whisper.cpp. It never showed any benefit and we should simply remove it at some point\n  * ggml_compute_forward_get_rows_back_f32_bf16 - the backward implementation in ggml in general requires a lot more work and refactoring to become useful, so I think there is no need to add partial BF16 support here. My impression is that the finetune example is not heavily used - unless my estimate is grossly wrong, we don't have to put effort to support BF16 in it for now\n\nIn the future, we should look into refactoring the\nggml_compute_forward_dup_xxx functions in order to simplify and reduce the\ncode duplication. For now we can accept as proposed.\n\nI haven't done any tests with BF16 models. What is the easiest way to generate\none and run some inference? (edit: nvm, saw the comments from earlier)\n\njart force-pushed the bf16 branch from 44d5c70 to b7a9144 Compare April 21,\n2024 20:52\n\nContributor Author\n\n###\n\njart commented Apr 21, 2024\n\n@ggerganov I've resolved all your suggestions (some with comments). PTAL. Your\nsuggestions were very helpful in improving this PR. For example, both LUTs\nwere able to be removed, which reduces GGML's memory footprint by 256kb. On my\nThreadripper, this change makes prompt eval speed for Mistral 7b 67% faster\n(and that's without my sgemm code).  \n---  \n  \nContributor Author\n\n###\n\njart commented Apr 21, 2024\n\nThank you, by the way, for not merging any conflicting changes. I'm very happy\nthat I was able to rebase without conflicts.  \n---  \n  \nContributor Author\n\n###\n\njart commented Apr 21, 2024\n\n> In the future, we should look into refactoring the\n> ggml_compute_forward_dup_xxx functions in order to simplify and reduce the\n> code duplication. For now we can accept as proposed.\n\nHave you considered renaming ggml.c to ggml.cpp? That's what I would do.\nPlease keep in mind that this recommendation is coming from the owner of a C\nlibrary. If I were you, I would just use C++ and then have CI tests which\nenforce a style guide that bans STL and exotic language features from the GGML\ncore library.  \n---  \n  \nsorasoras mentioned this pull request Apr 22, 2024\n\nbf16 support #6830\n\nClosed\n\n4 tasks\n\n###\n\nryao commented Apr 22, 2024\n\nJust to add something that has not been said already in favor of native\nbfloat16 support, some hardware might run things faster when given BF16\ninstead of FP16. For example, the A100 reportedly can run BF16 at 311.84\nTFLOPS while it can only run FP16 at 77.97\nTFLOPS:https://www.techpowerup.com/gpu-specs/a100-pcie-40-gb.c3623I realize\nthat almost nobody running llama.cpp has that GPU and that inferencing is\nsupposed to be memory bandwidth bound. However, given that TensorRT-LLM\nreportedly made a difference in inferencing performance benchmarks, despite\ninferencing being memory bound, I imagine there is a non-zero possibility that\nusing BF16 natively will make a difference in performance on hardware where\nBF16 runs faster.  \n---  \n  \nCollaborator\n\n###\n\nJohannesGaessler commented Apr 22, 2024\n\nDon't use TechPowerUp for looking up GPU stats, they're unreliable. If you\nlook at the official datasheet you'll find that it lists the same performance\nfor FP16 and BF16.  \n---  \n  \nggerganov approved these changes Apr 25, 2024\n\nView reviewed changes\n\nOwner\n\n###\n\nggerganov left a comment\n\nThere was a problem hiding this comment.\n\n### Choose a reason for hiding this comment\n\nThe reason will be displayed to describe this comment to others. Learn more.\n\nHere is a patch to make test-backend-ops run successfully:\n\n    \n    \n    diff --git a/ggml-backend.c b/ggml-backend.c index 402d86ef..189b5c14 100644 --- a/ggml-backend.c +++ b/ggml-backend.c @@ -822,7 +822,11 @@ GGML_CALL static enum ggml_status ggml_backend_cpu_graph_compute(ggml_backend_t GGML_CALL static bool ggml_backend_cpu_supports_op(ggml_backend_t backend, const struct ggml_tensor * op) { switch (op->op) { case GGML_OP_CPY: - return op->type != GGML_TYPE_IQ2_XXS && op->type != GGML_TYPE_IQ2_XS && op->type != GGML_TYPE_IQ1_S; // missing type_traits.from_float + return + op->type != GGML_TYPE_IQ2_XXS && + op->type != GGML_TYPE_IQ2_XS && + op->type != GGML_TYPE_IQ1_S && + op->type != GGML_TYPE_IQ1_M; // missing type_traits.from_float case GGML_OP_MUL_MAT: return op->src[1]->type == GGML_TYPE_F32 || op->src[1]->type == ggml_internal_get_type_traits(op->src[0]->type).vec_dot_type; default: diff --git a/ggml-metal.m b/ggml-metal.m index 9cb42198..56deb1f1 100644 --- a/ggml-metal.m +++ b/ggml-metal.m @@ -782,7 +782,7 @@ static bool ggml_metal_supports_op(const struct ggml_metal_context * ctx, const case GGML_OP_DIAG_MASK_INF: case GGML_OP_GET_ROWS: { - return op->ne[3] == 1; + return op->src[0]->type != GGML_TYPE_BF16 && op->ne[3] == 1; } default: return false; diff --git a/ggml.c b/ggml.c index 6a6ca7c6..0dc57786 100644 --- a/ggml.c +++ b/ggml.c @@ -19440,7 +19440,10 @@ struct ggml_cplan ggml_graph_plan(const struct ggml_cgraph * cgraph, int n_threa case GGML_OP_CPY: case GGML_OP_DUP: { - if (ggml_is_quantized(node->type)) { + if (ggml_is_quantized(node->type) || + // F16 -> BF16 and BF16 -> F16 copies go through intermediate F32 + (node->src[0]->type == GGML_TYPE_F16 && node->src[1] && node->src[1]->type == GGML_TYPE_BF16) || + (node->src[0]->type == GGML_TYPE_BF16 && node->src[1] && node->src[1]->type == GGML_TYPE_F16)) { cur = ggml_type_size(GGML_TYPE_F32) * node->ne[0] * n_tasks; } } break; diff --git a/tests/test-backend-ops.cpp b/tests/test-backend-ops.cpp index 02daad24..4545d962 100644 --- a/tests/test-backend-ops.cpp +++ b/tests/test-backend-ops.cpp @@ -50,7 +50,7 @@ static void init_tensor_uniform(ggml_tensor * tensor, float min = -1.0f, float m if (tensor->type == GGML_TYPE_F32 || tensor->type == GGML_TYPE_I32) { ggml_backend_tensor_set(tensor, data.data(), 0, size * sizeof(float)); - } else if (ggml_is_quantized(tensor->type) || tensor->type == GGML_TYPE_F16) { + } else if (ggml_is_quantized(tensor->type) || tensor->type == GGML_TYPE_F16 || tensor->type == GGML_TYPE_BF16) { GGML_ASSERT(size % ggml_blck_size(tensor->type) == 0); std::vector<uint8_t> dataq(ggml_row_size(tensor->type, size)); std::vector<float> imatrix(tensor->ne[0], 1.0f); // dummy importance matrix @@ -92,6 +92,8 @@ static std::vector<float> tensor_to_float(const ggml_tensor * t) { size_t i = i3*t->nb[3] + i2*t->nb[2] + i1*t->nb[1] + i0/bs*t->nb[0]; if (t->type == GGML_TYPE_F16) { tv.push_back(ggml_fp16_to_fp32(*(ggml_fp16_t*)&buf[i])); + } else if (t->type == GGML_TYPE_BF16) { + tv.push_back(ggml_bf16_to_fp32(*(ggml_bf16_t*)&buf[i])); } else if (t->type == GGML_TYPE_F32) { tv.push_back(*(float *) &buf[i]); } else if (t->type == GGML_TYPE_I32) { @@ -1864,7 +1866,7 @@ static bool test_backend(ggml_backend_t backend, test_mode mode, const char * op std::default_random_engine rng(0); const ggml_type all_types[] = { - GGML_TYPE_F32, GGML_TYPE_F16, + GGML_TYPE_F32, GGML_TYPE_F16, GGML_TYPE_BF16, GGML_TYPE_Q4_0, GGML_TYPE_Q4_1, GGML_TYPE_Q5_0, GGML_TYPE_Q5_1, GGML_TYPE_Q8_0,\n\nIt's always good to make sure it does not produce errors - it would give high\nconfidence that there hasn't been any regression in the operations across all\nbackends:\n\n    \n    \n    # CPU-only run make -j tests && ./tests/test-backend-ops -b CPU # CUDA run - verifies that the CPU and GPU results match within eps LLAMA_CUDA=1 make -j tests && ./tests/test-backend-ops\n\n> Have you considered renaming ggml.c to ggml.cpp?\n\nIt's something to think about. Templates can indeed save a lot of headache\n(can also cause a lot of headache if overused :D)\n\n###\n\nddh0 commented Apr 25, 2024\n\nBFloat16 support would be a welcome addition, being the native data type of\nthe Llama 3 family of models :)  \n---  \n  \njart force-pushed the bf16 branch from c2cc0dd to 9282eb4 Compare April 26,\n2024 09:41\n\nContributor Author\n\n###\n\njart commented Apr 26, 2024\n\n@ggerganov Thanks for showing me how to do that. I'll be sure to run those\ntests on future changes. All your comments have been addressed. PTAL.  \n---  \n  \nContributor\n\n###\n\ngithub-actions bot commented Apr 26, 2024 \u2022\n\n\ud83d\udcc8 llama.cpp server for bench-server-baseline on Standard_NC4as_T4_v3 for\nphi-2-q4_0: 444 iterations \ud83d\ude80  \n---  \n  \njart mentioned this pull request Apr 26, 2024\n\nQuestion Mozilla-Ocho/llamafile#363\n\nClosed\n\njart added 7 commits April 28, 2024 09:36\n\nIntroduce bfloat16 support\n\nd1eb702\n\n    \n    \n    Many models on Hugging Face (e.g. Mistral, TinyLLaMA) use bfloat16 as their canonical floating point format. \u250csign \u2502 \u2502 \u250cexponent \u2502 \u2502 \u2502 \u2502 \u250cmantissa \u2502 \u2502 \u2502 \u2502\u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510\u250c\u2500\u2534\u2500\u2500\u2500\u2510 0b0000000000000000 brain16 This encoding has the same number of exponent bits as float32. That makes conversion relatively straightforward, even in the absence of hardware support. For example, converting brain16 to binary32 means simply shifting 16 bits to the left. \u250csign \u2502 \u2502 \u250cexponent \u2502 \u2502 \u2502 \u2502 \u250cmantissa \u2502 \u2502 \u2502 \u2502\u250c\u2500\u2500\u2534\u2500\u2500\u2500\u2510\u250c\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 0b00000000000000000000000000000000 IEEE binary32 The issue is that converting bf16 to fp16 can result in information loss. Only 13% of bf16 numbers can be precisely represented in fp16 which in practice ends up being 99.71% of Mistral 7b v0.2's weights however there is currently no way other than fp32 to get the others \u250csign \u2502 \u2502 \u250cexponent \u2502 \u2502 \u2502 \u2502 \u250cmantissa \u2502 \u2502 \u2502 \u2502\u250c\u2500\u2534\u2500\u2510\u250c\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510 0b0000000000000000 IEEE binary16 This change fixes that, by adding a bf16 data type to GGML. Support for CPU inference has been implemented along with optimizations for the AVX2, AVX512, and AVX512BF16 ISAs. Perplexity on Mistral 7b 0.2 improves somewhere around -0.0024 to -0.0046 compared to using fp16\n\nRemove GGML code that's not needed\n\n205ad06\n\nMinimize the GGML API surface area for BF16\n\n700db7d\n\nRemove bf16 luts\n\n49100a8\n\nMake the GGML header look nicer\n\ned1b1d0\n\nFix documentation\n\n88b97b8\n\nApply ggerganov's fixes for test-backend-ops\n\n68614ce\n\njart force-pushed the bf16 branch from 9282eb4 to 68614ce Compare April 29,\n2024 06:43\n\nAdd BF16 code for new ggml_validate_row_data() function\n\ned0f47b\n\nSign up for free to join this conversation on GitHub. Already have an account?\nSign in to comment\n\nLabels\n\nNone yet\n\n9 participants\n\nAdd this suggestion to a batch that can be applied as a single commit. This\nsuggestion is invalid because no changes were made to the code. Suggestions\ncannot be applied while the pull request is closed. Suggestions cannot be\napplied while viewing a subset of changes. Only one suggestion per line can be\napplied in a batch. Add this suggestion to a batch that can be applied as a\nsingle commit. Applying suggestions on deleted lines is not supported. You\nmust change the existing code in this line in order to create a valid\nsuggestion. Outdated suggestions cannot be applied. This suggestion has been\napplied or marked resolved. Suggestions cannot be applied from pending\nreviews. Suggestions cannot be applied on multi-line comments. Suggestions\ncannot be applied while the pull request is queued to merge. Suggestion cannot\nbe applied right now. Please check back later.\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
