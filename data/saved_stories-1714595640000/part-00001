{"aid": "40223805", "title": "Keeping the Data in the Cache", "url": "https://johnnysswlab.com/latency-sensitive-applications-and-the-memory-subsystem-keeping-the-data-in-the-cache/", "domain": "johnnysswlab.com", "votes": 1, "user": "davikr", "posted_at": "2024-05-01 14:32:46", "comments": 0, "source_title": "Latency-Sensitive Applications and the Memory Subsystem: Keeping the Data in the Cache", "source_text": "Latency-Sensitive Applications and the Memory Subsystem: Keeping the Data in\nthe Cache - Johnny's Software Lab\n\nJohnny's Software Lab\n\nWe help you deliver fast software\n\n# Latency-Sensitive Applications and the Memory Subsystem: Keeping the Data in\nthe Cache\n\nPosted on April 30, 2024May 1, 2024Author Ivica Bogosavljevi\u0107Posted in Low\nLevel Performance, Memory Subsystem Performance, PerformanceLeave a Reply\n\nWe at Johnny\u2019s Software Lab LLC are experts in performance. If performance is\nin any way concern in your software project, feel free to contact us.\n\nThis is the 17th post in the series about memory subsystem optimizations. You\ncan also explore other posts in the same series here.\n\n## Quick Introduction to Latency Sensitive Applications\n\nMost performance techniques we covered until now were related to increasing\nthroughput \u2013 the amount of data the system can process in a unit of time. So,\na system that processes 5 GB/s of data is faster than a system that processes\n3 GB/s. And for most applications whose performance we want to improve, what\nwe actually want to do is to increase the data throughput.\n\nBut there is a smaller number of applications that are latency sensitive \u2013 the\nmost important consideration is how much time passes between the request being\nmade to the system and the system response. In these systems, although the\nthroughput is still important, latency is nevertheless more important.\n\nThere are generally two types of latency sensitive applications:\n\n  * Real-time applications, where the response to an input must be in a specified time frame. Many automotive systems, industrial automation systems, medical systems, etc. are like this.\n  * Low-latency applications, where the response to an input must be as fast as possible. Prime examples of such systems are high-frequency trading systems or audio/video communication systems, where the response needs to be provided as soon as possible.\n\nWhen it comes to memory performance in latency-sensitive environments, two\nthings are important:\n\n  * The relevant data should be in the fastest level of cache as possible. This will ensure fast access to it when it is needed.\n  * No unnecessary hardware or software mechanisms related to memory management should kick in during the access to the data. Things like swapping, TLB cache misses or physical memory allocation introduce additional latency and slow down the response.\n\nIn today\u2019s post we talk about mechanisms used to keep the relevant data in the\nfastest level of the data cache. Next post will talk about hardware or\nsoftware mechanisms related to memory management that slow down access to\ndata.\n\nLike what you are reading? Follow us on LinkedIn , Twitter or Mastodon and get\nnotified as soon as new content becomes available. Need help with software\nperformance? Contact us!\n\n## Techniques\n\n### Software Cache Warming\n\nThe main problem of latency critical systems is that our critical data gets\nevicted from the cache because it is not accessed often enough. Consider the\nfollowing of a code that receives a packet, parses it, and sends a response by\nperforming a lookup in a hash map:\n\nPlain text\n\nCopy to clipboard\n\nOpen code in new window\n\nEnlighterJS 3 Syntax Highlighter\n\nstd::unordered_map<int32_t, order> my_orders;\n\n...\n\npacket_t* p;\n\nwhile(!exit) {\n\np = get_packet();\n\n// If packet arrived\n\nif (p) {\n\n// Check if the identifier is known to us\n\nauto it = my_orders.find(p->id);\n\nif (it != my_orders.end()) {\n\nsend_answer(p->origin, it->second);\n\n}\n\n}\n\n}\n\nstd::unordered_map<int32_t, order> my_orders; ... packet_t* p; while(!exit) {\np = get_packet(); // If packet arrived if (p) { // Check if the identifier is\nknown to us auto it = my_orders.find(p->id); if (it != my_orders.end()) {\nsend_answer(p->origin, it->second); } } }\n\n    \n    \n    std::unordered_map<int32_t, order> my_orders; ... packet_t* p; while(!exit) { p = get_packet(); // If packet arrived if (p) { // Check if the identifier is known to us auto it = my_orders.find(p->id); if (it != my_orders.end()) { send_answer(p->origin, it->second); } } }\n\nThe above while loop is a busy loop. When the packet arrives, it will lookup\nthe id in the hash map my_orders and send the answer. The response time is\nlatency critical, therefore there are no sleeps in the loop.\n\nIf packets don\u2019t come too often, this loop doesn\u2019t do anything useful most of\nthe time. In this case the data of our hash map my_orders will probably get\nevicted from the cache, since it is not used.\n\nThe solution to this problem is to perform bogus requests to the hash map\ninstead of idle looping. This will make sure the data is not evicted from the\ndata cache and requests can be served faster. Here is an example on how to\nkeep the data cache warm:\n\nPlain text\n\nCopy to clipboard\n\nOpen code in new window\n\nEnlighterJS 3 Syntax Highlighter\n\nstd::unordered_map<int32_t, order> my_orders;\n\n...\n\npacket_t* p;\n\nint64_t total_random_found = 0;\n\nwhile(!exit) {\n\n// If the packet header is available in the\n\n// packet buffer, we stop cache warming because\n\n// we want to respond as fast as possible.\n\nif (packet_header_arrived()) {\n\np = get_packet();\n\n// If packet arrived\n\nif (p) {\n\n// Check if the identifier is known to us\n\nauto it = my_orders.find(p->id);\n\nif (it != my_orders.end()) {\n\nsend_answer(p->origin, it->second);\n\n}\n\n}\n\n} else {\n\n// Cache warming instead of idle looping\n\nauto random_id = get_random_id();\n\nauto it = my_orders.find(random_id);\n\n// We need to do something with the result, otherwise\n\n// the compiler may optimize the access away\n\ntotal_random_found += (it != my_orders.end());\n\n}\n\n}\n\nstd::cout << \"Total random found \" << total_random_found << \"\\n\";\n\nstd::unordered_map<int32_t, order> my_orders; ... packet_t* p; int64_t\ntotal_random_found = 0; while(!exit) { // If the packet header is available in\nthe // packet buffer, we stop cache warming because // we want to respond as\nfast as possible. if (packet_header_arrived()) { p = get_packet(); // If\npacket arrived if (p) { // Check if the identifier is known to us auto it =\nmy_orders.find(p->id); if (it != my_orders.end()) { send_answer(p->origin,\nit->second); } } } else { // Cache warming instead of idle looping auto\nrandom_id = get_random_id(); auto it = my_orders.find(random_id); // We need\nto do something with the result, otherwise // the compiler may optimize the\naccess away total_random_found += (it != my_orders.end()); } } std::cout <<\n\"Total random found \" << total_random_found << \"\\n\";\n\n    \n    \n    std::unordered_map<int32_t, order> my_orders; ... packet_t* p; int64_t total_random_found = 0; while(!exit) { // If the packet header is available in the // packet buffer, we stop cache warming because // we want to respond as fast as possible. if (packet_header_arrived()) { p = get_packet(); // If packet arrived if (p) { // Check if the identifier is known to us auto it = my_orders.find(p->id); if (it != my_orders.end()) { send_answer(p->origin, it->second); } } } else { // Cache warming instead of idle looping auto random_id = get_random_id(); auto it = my_orders.find(random_id); // We need to do something with the result, otherwise // the compiler may optimize the access away total_random_found += (it != my_orders.end()); } } std::cout << \"Total random found \" << total_random_found << \"\\n\";\n\nIn the above example, instead of idle looping, the code will look up random\nidentifiers in the hash map to prevent data eviction.\n\nYou need to be careful with this cache warming, because the data that you are\nreceiving is not necessarily random. Imagine that packets come in bursts where\nmost packets have the same identifier. In this case, cache warming as we\npresented in the above code snippet would bring useless data to the cache\nthereby evicting useful data. Therefore, instead of keeping the cache warm\nusing random values, you would need to keep the cache warm using some values\nthat came in the past.\n\nLike what you are reading? Follow us on LinkedIn , Twitter or Mastodon and get\nnotified as soon as new content becomes available. Need help with software\nperformance? Contact us!\n\n### Hardware Cache Warming\n\nApart from software cache warming, where we are accessing our data without a\nreal need just to prevent it from being evicted from the cache, some CPUs\noffer hardware mechanisms that can do that. With hardware mechanisms, the\napplication just needs to use some kind of API to let the CPU know which part\nof the memory is important, and the hardware will take care of the rest.\n\nOn Intel, the technology is called Cache Pseudo Locking and it is implemented\nas a part of Intel Resource Director technology. Setting it up is not simple.\nThe user first needs to configure a part of the cache that will be reserved\nfor holding critical data, then this part of the cache is exposed as a\ncharacter device and can be accessed using mmap system call. More info about\nit is available here.\n\nAMD has a similar technology called L3 Cache Range Reservation. It allows the\nuser to reserve a physical address range that will be kept in L3 cache.\nUnfortunately, AMD\u2019s user manual only mentions this feature and doesn\u2019t give\nexamples on how to use it from user space.\n\n## Experiments\n\nAll experiments were executed on Intel(R) Core(TM) i5-10210U. We disabled\nturbo boost for more precise measurements.\n\n### Software Cache Warming\n\nFor the experiment, we use an ordinary std::unordered_set<int> hash map whose\naccess time is very important to us and we want to minimize it. We create an\nartificial scenario, where this hash map is accessed rarely \u2013 once every\nmillion cycles. The source code is available here.\n\nWe perform the experiment with three different setups:\n\n  * Regular hash map \u2013 the program performs the access to the hash map only when requested. Apart from that, it is doing the busy wait.\n  * Reload zero \u2013 when the program is not doing critical access, it is continuously accessing the same data with value 0.\n  * Reload random \u2013 when the program is no doing critical access, it is accessing random values in the hash map.\n\nWe perform the experiment on hash maps of various sizes. Here we report the\naverage access time and standard deviation for 10.000 accesses to the hash\nmap. We measure the latency on X86-64 using rdtsc instruction, which is a low\ncost instruction used to get the clock counter.\n\nWe measure latency, i.e. the number of cycles between the time when the\nrequest is issued and the result is returned. The hash map consists of\nintegers; we vary the number of entries in the hash map. Here are the average\nlatency and the latency standard deviation in cycles depending on hash map\nsize:\n\nNumber of Entries| Regular| Reload zero| Reload random  \n---|---|---|---  \n1 K| 226.1 (219.0)| 213.3 (205.1)| 132.5 (67.3)  \n4 K| 324.7 (296.3)| 350.7 (331.3)| 140.1 (95.4)  \n16 K| 396.8 (341.1)| 389.1 (354.5)| 208.7 (134.5)  \n64 K| 425.5 (376.1)| 416.0 (360.6)| 232.1 (152.6)  \n256 K| 514.2 (451.5)| 473.3 (480.6)| 338.8 (317.6)  \n1 M| 599.8 (550.2)| 615.1 (573.6)| 466.3 (429.8)  \n4 M| 702.1 (647.0)| 619.7 (649.2)| 531.3 (508.3)  \n16 M| 756.7 (677.6)| 668.8 (707.4)| 543.2 (499.9)  \n64 M| 769.1 (702.3)| 735.9 (734.2)| 641.0 (774.4)  \n  \nFrom the above table we can draw several different conclusions:\n\n  * Both the average latency and the latency deviation is smaller for the reload random version compared to the other two versions. This means that thy system will respond more quickly both on average, but also the worse response time will be better.\n  * The advantage of reload random version are the biggest for the small to medium number of entries in the hash map. Very large hash maps don\u2019t profit too much from reloading random elements in busy loop.\n\nAlthough the table might suggest that the latency follows normal distribution,\nthis is not the case. Minimal latency for all accesses is around 40, and\nmaximum latency varies greatly from 2.000 cycles to 100.000 cycles. Smaller\nnumbers are probably related to data cache misses and larger numbers are\nrelated to the cases where the program was interrupted by the operating system\nor something similar. This highlights the importance of setting up the system\nproperly, including pinning the program to the core (which we didn\u2019t do) and\nincreasing the priority of the program (which we also didn\u2019t do).\nNevertheless, the largest latencies probably didn\u2019t happen too often otherwise\nthe averages would be much larger.\n\nLike what you are reading? Follow us on LinkedIn , Twitter or Mastodon and get\nnotified as soon as new content becomes available. Need help with software\nperformance? Contact us!\n\n## Summary\n\nLatency-sensitive applications are applications where latency between request\nand response is critical, and throughput (measured as GB of data processed per\nsecond) is only secondary. For such applications, it is important that the\ncritical data is kept in fastest level of cache. To achieve this, we described\ntwo techniques:\n\n  * Software data warming, where we access the critical data when the program is idle to prevent it from being evicted from the data cache.\n  * Hardware data warming, where we configure the hardware not to evict critical data from the data cache.\n\nSoftware data warming is easier to implement and doesn\u2019t require special\npermissions or hardware features, but it will consume more power and memory\nbandwidth. On the opposite part of the spectrum, hardware data warming is more\nresource-saving but is more complicated to set up, requires special access to\nthe system and you cannot have it keep critical data in the fastest level of\nthe data cache (works only for L2 or L3 cache).\n\nLike what you are reading? Follow us on LinkedIn , Twitter or Mastodon and get\nnotified as soon as new content becomes available. Need help with software\nperformance? Contact us!\n\nTagged: cache warminglatencylatency sensitivelow latencyreal time\n\n### Leave a Reply Cancel reply\n\n## Like what you\u2019re reading? Follow us!\n\nLinkedInTwitter\n\n  * Latency-Sensitive Applications and the Memory Subsystem: Keeping the Data in the Cache\n  * The pros and cons of explicit software prefetching\n  * A story of a very large loop with a long instruction dependency chain\n  * On Avoiding Register Spills in Vectorized Code with Many Constants\n  * Unexpected Ways Memory Subsystem Interacts with Branch Prediction\n\n## Recent Posts\n\n  * Latency-Sensitive Applications and the Memory Subsystem: Keeping the Data in the Cache\n  * The pros and cons of explicit software prefetching\n  * A story of a very large loop with a long instruction dependency chain\n  * On Avoiding Register Spills in Vectorized Code with Many Constants\n  * Unexpected Ways Memory Subsystem Interacts with Branch Prediction\n\n## Recent Comments\n\n  * Ivica Bogosavljevi\u0107 on How branches influence the performance of your code and what can you do about it?\n  * Hai on How branches influence the performance of your code and what can you do about it?\n  * Michael Dunlavey on Making your program run faster: the key concepts of software performance\n  * Michael R. Dunlavey on Making your program run faster: the key concepts of software performance\n  * tlanyan on Link Time Optimizations: New Way to Do Compiler Optimizations\n\n## Archives\n\n  * April 2024\n  * March 2024\n  * February 2024\n  * January 2024\n  * December 2023\n  * November 2023\n  * October 2023\n  * September 2023\n  * August 2023\n  * July 2023\n  * June 2023\n  * May 2023\n  * April 2023\n  * March 2023\n  * February 2023\n  * January 2023\n  * December 2022\n  * November 2022\n  * October 2022\n  * September 2022\n  * August 2022\n  * July 2022\n  * June 2022\n  * May 2022\n  * April 2022\n  * March 2022\n  * February 2022\n  * January 2022\n  * December 2021\n  * November 2021\n  * October 2021\n  * September 2021\n  * August 2021\n  * July 2021\n  * June 2021\n  * May 2021\n  * April 2021\n  * March 2021\n  * February 2021\n  * January 2021\n  * December 2020\n  * November 2020\n  * October 2020\n  * September 2020\n  * August 2020\n  * July 2020\n  * June 2020\n  * May 2020\n\n## Categories\n\n  * 2 Minute Reads\n  * Algorithms and Performance\n  * C++ Performance\n  * Computational Performance\n  * Data Structure Performance\n  * Debugging\n  * Developer Tools\n  * Help the Compiler\n  * Kernel Space and Performance\n  * Low Level Performance\n  * Memory Footprint\n  * Memory Subsystem Performance\n  * Multithreaded Performance\n  * Parallelization\n  * Performance\n  * Performance Analysis Tools\n  * Performance Contest\n  * Reliability\n  * Standard Library and Performance\n  * System Design\n  * Toolchain and Performance\n  * Vectorization\n\n## Meta\n\n  * Log in\n  * Entries feed\n  * Comments feed\n  * WordPress.org\n\n\u00a92024 Johnny's Software Lab | WordPress Theme by Superb WordPress Themes\n\nLinkedInTwitterEmailShare\n\n\u2713\n\nThanks for sharing!\n\nAddToAny\n\nMore...\n\nAddToAny\n\nMore...\n\n", "frontpage": false}
