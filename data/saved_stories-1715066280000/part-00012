{"aid": "40280524", "title": "How do large language models think?", "url": "https://thomasvoice.substack.com/p/how-do-large-language-models-think", "domain": "thomasvoice.substack.com", "votes": 1, "user": "waywardfractal", "posted_at": "2024-05-06 23:09:57", "comments": 0, "source_title": "How do Large Language Models \u201cthink\u201d?", "source_text": "How do Large Language Models \u201cthink\u201d? - by Thomas Voice\n\n# The Irregular Voice\n\nShare this post\n\n#### How do Large Language Models \u201cthink\u201d?\n\nthomasvoice.substack.com\n\n#### Discover more from The Irregular Voice\n\nMy personal Substack, unlikely to obey a schedule\n\nContinue reading\n\nSign in\n\n# How do Large Language Models \u201cthink\u201d?\n\nThomas Voice\n\nApr 25, 2024\n\nShare this post\n\n#### How do Large Language Models \u201cthink\u201d?\n\nthomasvoice.substack.com\n\nShare\n\nRecently I\u2019ve been experimenting with large language models (LLMs), looking\ninto their mathematical reasoning capability. Moderns LLMs like ChatGPT\nsometimes show remarkable abilities to solve maths problems. Of all the\nimpressive things LLMs can do, mathematics stands out as being especially\nabstract and complex. However, there\u2019s been some skepticism about whether LLMs\ncan answer problems in general or if instead they just memorise specific\nanswers. The reality is complicated, and there seems to be a blurry line\nbetween these two possibilities.\n\n####\n\nTransformer Architecture\n\nTo get some idea of how these models think, we can look at the architecture.\nModern large language models predominantly use transformer decoder neural\nnetworks. The input is some text, broken up into tokens (usually a word or\nsymbol each), which the model adds to by repeatedly predicting what the next\ntoken should be.\n\nThe input goes through several layers of neural networks. At each layer, per-\ntoken information is transformed in parallel and the results are cross-\nreferenced against each other, before being further processed. This means,\nwhen interpreting a word, the model can detect relevant words nearby - e.g.\n\u201cshare\u201d, \u201cagreement\u201d or \u201ccarnival\u201d near the word \u201cfair\u201d. Similar words have\nsimilar reference information, so the model can look for general patterns of\nwords. Information about detected patterns is further cross-referenced at the\nnext layer, letting the model detect patterns of patterns, and so on, into\nmore abstraction. The model uses this information when predicting the next\ntoken, and must attempt to continue the existing patterns in a consistent way.\nWhen LLMs generate text, they can take into account highly subtle information,\nsuch as meanings of sentences, topics of paragraphs, writing style, mood,\ntone, emotion, character motivations - and mathematical reasoning.\n\n####\n\nChain of Thought Reasoning\n\nWhen giving short answers, LLM calculations have limited generalisation.\nRecent research shows that LLMs do better at reasoning related tasks if they\nare asked to explain their answer first (called chain of thought prompting).\nTransformers don\u2019t have any internal memory, but their output tokens get added\nto their input every step. The explanations have partial solutions, so the\nproblem gets broken into multiple easier steps for the model. Here\u2019s an\nexample:\n\n####\n\nSubstitutions\n\nThe act of breaking down a problem into smaller parts seems to demonstrate\nimpressive reasoning skills. However, this behaviour is not entirely related\nto problem solving. LLMs do this when trained to generate natural looking\ntext, and are just trying to copy humans. People write out text to help them\ndo maths, there are patterns in this behaviour that the LLM can learn, and\nthis allows for generalisation. For example, I found that a GPT-3.5 instance\nwas often able to correctly prove irrationality for a wide range of square\nroots:\n\nThese could have been memorised, but that seems less likely for large numbers\nlike:\n\nThese proofs are very similar to well known results, such as that for \u221a2 or \u221ap\nfor general prime p, that are likely to be in the model training data. When\nasked to prove that \u221a5 is irrational, the model included \u201cAn integer is even\nif and only if its square is even\u201d, which is not relevant at all, but would be\nfor \u221a2. It seems likely that when answering these questions, the LLM is\nsubstituting the requested value into a learned pattern.\n\nIt\u2019s not a fixed template though, as there\u2019s variation in the words used and\nsome additional explanations can be given or left out. Also, it\u2019s not just\npure substitution, some transformations are required. Above, 144 was used for\n12 squared and 841 for 29 squared. These transformed substitutions are\nintermediary calculations. This kind of behaviour allows the LLM to break a\nproblem into steps, essentially by following a script.\n\nWhen asked silly questions, the model can (when it doesn\u2019t complain) apply a\nproof-like pattern to nonsense:\n\nA similar thing happened with other nonsense questions, e.g.:\n\nIn the nonsense examples, some responses contained nonsensical\ntransformations. For the monkey question, one response talked of a \u201cgorilla\nequation\u201d. Another question asked \u201cuse Jensen\u2019s inequality to prove that the\nmeasure of a human is more than the sum of their parts\u201d, and difference\nanswers interpreted \u201cmeasure\u201d to imply things like height and weight. One\nlengthy answer discussed Jensens inequality before diverting to talk about\nfood processing, livestock and \u201csheep inequality\u201d, \u201cfish inequality\u201d and\n\u201cjellyfish inequality\u201d.\n\nThe ability to follow a pattern, using transformations to fill in blanks, can\nbe seen in LLMs in other areas, such as poetry:\n\nHere, the transformations are about finding parallels and similar\nassociations. So, the way LLMs solve maths problems actually seems similar to\nmetaphorical thinking. It\u2019s also similar to how LLMs hallucinate, filling in a\nfamiliar pattern of text with a new concept, doing transformations and\nsubstitutions to come up with something misleadingly plausible.\n\n####\n\nPatterns of Patterns\n\nWhile exploring LLM math reasoning abilities, I fine tuned GPT-3.5-turbo on\ntwo different kinds of data, one of which was solutions from a popular maths\ndataset (MATH), the other was a set of much more long-winded answers that I\nwrote myself. Fine tuning on MATH improved the test results, but fine tuning\non my data, in some cases, actually made the model worse. I wondered if\nadopting an unfamiliar style had made it harder for the model to apply the\npatterns it knew. To find out, I tried fine-tuning the model on a totally\nalien style of data - MATH solutions with the word order reversed in every\nsentence. The resulting model adopted the reverse-order style for its output.\nWhen tested, it was ~10-15% worse at maths problems than before fine-tuning.\nThis suggests LLMs \u201creasoning\u201d is sensitive to being pushed out of familiar\nterritory.\n\nHowever, this doesn\u2019t mean that LLMs can only solve problems by copying almost\nidentical answers from their training data. For example:\n\nHere the model makes a mistake early on, by looking at divisibility by 3, it\ndoesn\u2019t find a contradiction (since 18 is divisible by 3 squared). However, it\nrecovers by repeating the same reasoning for divisibility by 2, which allows\nit to progress. Repeating that part of the proof pattern lead to a correct\nresult.\n\nI tried asking the model to solve some questions that were made of two\ndifferent problems combined, so that the answer to one became part of the\nother. The model was able to solve about a third of these, by chaining\ntogether solutions, substituting in the answer from the first into the second.\n\nThe model even showed some evidence of combining multiple smaller patterns to\nfollow a larger pattern, as, for example, in this response:\n\nThis describes its own overarching structure, explicitly calling out the\nbeginning, substitution step and eventual end result.\n\nMathematical work is usually made up of a series of familiar steps, sometimes\ncombined in a novel way to get new results. So, being able to mix and match\npatterns of reasoning, and follow higher level patterns, is potentially very\npowerful.\n\n####\n\nWhat\u2019s Missing?\n\nWhile we can see LLMs solve problems in ways that can generalise, that doesn\u2019t\nresolve the question of whether they are \u201cmemorising\u201d or \u201creasoning\u201d. An LLM\ncan prove many square roots are irrational, but if it\u2019s not doing anything\nmuch more complicated than search\u2192 replace, is that \u201creasoning\u201d? It depends on\nthe definition, and like \u201cintelligence\u201d itself, there\u2019s no agreement on what\n\u201creasoning\u201d means.\n\nLLM \u201creasoning\u201d abilities are reminiscent of intuitive (or \u201cfast\u201d) thinking in\nhumans. When I solve maths problems, familiarity with the problem type helps,\nand often it subjectively feels like I start with a hazy pattern in mind for\nthe solution, which I then flesh out. It only looks like a logical progression\nfrom question to answer when it is finished. (The same probably applies to\neveryday life, where many of the things we think and say follow common\npatterns, but seem rational in retrospect.)\n\nNot all problems can be solved with this kind of thinking. Some have to be\nbroken down into discrete options and explored methodically. That is the only\nway to find new patterns of reasoning. LLMs are not well suited for this. For\nthe LLM, the only persistent state is the output text, so any exploration\nprocess would have to be written down. The LLM would have to be trained on\ndata that included descriptions of trying wrong approaches, identifying dead-\nends, verifying results and fixing mistakes. This does not seem promising,\ngiven how fine-tuning on long explanations gave such poor results. Also, even\nif an LLM could be trained to explore, it wouldn\u2019t be able learn from doing so\nwithout external help.\n\nAn interesting potential solution is to integrate LLMs with other AI systems\nthat can take care of exploration and new learning. This was the approach in\nVoyager, an AI agent that plays Minecraft. Voyager uses LLMs to generate goals\nand then generate programs which play the game to achieve them. If successful,\nthe code is stored in a memory bank of skills, which can be drawn on to help\nachieve future goals. DeepMind have been applying similar ideas in specific\nareas of maths with AlphaGeometry and FunSearch, where they combine\nexploration systems with deep learning or large language models. Both of those\nsystems have tools to verify the output of the models and guide the\nexploration. AlphaGeometry uses a specialised geometry theorem checker, and\nFunSearch uses function evaluation. More general theorem checking systems\nexist, like LEAN, which can verify all kinds of mathematics. LLMs can convert\nmaths into LEAN, so DeepMind\u2019s Alpha could plausibly use formal verification\nto explore general mathematics results. This would be an exciting combination,\nif it could be made to work, without getting lost in the vast span of possible\ndirections to explore.\n\nInstead of relying on external tools for verification, it is possible that\nmodels could be used to check their own outputs. A recent paper from Tencent\ndescribes the AlphaLLM system, which uses Alpha style exploration, with LLM\nsteps for defining new problems, generating sub-task solutions and evaluating\nresults. This kind of approach could be very general and extremely powerful.\nHowever, it\u2019s worth being cautious about this kind of approach. Systems that\ntrain themselves can end up with compounding errors that they can\u2019t detect.\n\nAn LLM based general maths solver could have applications well outside\nmathematics. There\u2019s already huge potential in the ability of large language\nmodels to convert between structured and unstructured data. This opens up the\npossibility of connecting and automating many systems that currently rely on\nhuman effort. However, if LLMs could convert between structured and\nunstructured reasoning, the applications would be even bigger. A big example\nwould be software engineering. When coding, software engineers implicitly\ncreate a mental model for their programs. This would cover constraints on\ninputs and outputs of functions, flows of information and possible program\nstates, and is at heart simple mathematics and logical reasoning. If an AI\nsystem could convert documentation, comments and software requirements into\nmathematics, it could build large applications that it could verify\nautomatically. It would be a big step forward in AI, and maybe a significant\ndisruption to the software industry.\n\nThanks for reading my Substack! Subscribe for free to receive new posts and\nsupport my work.\n\nShare this post\n\n#### How do Large Language Models \u201cthink\u201d?\n\nthomasvoice.substack.com\n\nShare\n\nComments\n\nDo Large Language Models have a \"Reasoning Gap\"?\n\n\u201cTrue reasoning\u201d vs \u201cmemorisation\u201d The recent release of the functional MATH()\ndataset came with a paper and a twitter thread that headlined with an...\n\nApr 1 \u2022\n\nThomas Voice\n\n2\n\nShare this post\n\n#### Do Large Language Models have a \"Reasoning Gap\"?\n\nthomasvoice.substack.com\n\nCan a Large Language Model be a Calculator?\n\nI\u2019ve recently been exploring the math capabilities of large language models\n(LLMs). In particular, I\u2019ve been interested in whether they solve maths...\n\nApr 25 \u2022\n\nThomas Voice\n\n1\n\nShare this post\n\n#### Can a Large Language Model be a Calculator?\n\nthomasvoice.substack.com\n\nIssues with MATH()\n\nI have recently been looking at with the MATH() functional dataset, which is a\nsystem for synthetically generating math problems and their solutions, to...\n\nApr 1 \u2022\n\nThomas Voice\n\nShare this post\n\n#### Issues with MATH()\n\nthomasvoice.substack.com\n\nReady for more?\n\n\u00a9 2024 Thomas Voice\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
