{"aid": "40109616", "title": "Show HN: Researching 1201 Indigenous dialects \u2013 quickly", "url": "https://land.org.au/blog/researching-1201-indigenous-dialects-in-36-minutes", "domain": "land.org.au", "votes": 1, "user": "devon_c", "posted_at": "2024-04-21 22:02:08", "comments": 0, "source_title": "Researching 1201 Indigenous dialects in 36 minutes", "source_text": "Researching 1201 Indigenous dialects in 36 minutes\n\n# Ourland\n\n## ALPHA\n\nMapAIORAPIContact\n\nProducts\n\nMapAIORAPI\n\nAbout\n\nCharterBlogApplyContact\n\n\u2190 Back to blog\n\n# Researching 1201 Indigenous dialects in 36 minutes\n\nby Devon Crebbin\n\nNaarm\n\n21 April 2024\n\n### Introducing AIOR\n\n#### An AI Orchestrated Research tool\n\nSkip to the dialect research\n\nSkip to the technical part\n\n### Backstory\n\nWarning: The following raises issues of the Stolen Generation & Colonization:\nAboriginal & Torres Strait Islander people may find this upsetting\n\nAccording to AIATSIS (Australian Institute of Aboriginal and Torres Strait\nIslander Studies):\n\nThere are over 300 Australian Indigenous languages and over 1000 different\ndialects that encompasses all of these dialects.\n\nAn overwhelming majority of these are critically endangered, or practically\nextinct.\n\nThis is a monumental culture and identity loss to the Indigenous community and\nhas been a constant issue since the first invaders set foot in Australia.\n\nOn top of the Stolen Generation taking Indigenous kids away from their\nfamily's and relinquishing them of their cultural heritage by forcing them to\nadopt the Western language and to hold a negative view of their own culture;\nit comes as to no surprise why all of dialects have become or are nearing\nextinction.\n\nAn anecdotal example.\n\nMy heritage is Gangalidda, Garawa & Waanyi.\n\nMy great grandma was born on these lands but was then shipped off to the\nMornington Island mission to learn the religion and language.\n\nWhen she had my grandma, she was then shipped off to Palm Island and\nexperienced the same colonization tactic.\n\nThis had led to no one in my family knowing the language of our ancestors\napart from the practice of being assigned \"skin names\" at birth which are\nderived from the Lardil language of Mornington Island - due to my Grandma\npossessing a Lardil dictionary and her mums (forced) roots on Mornington.\n\nMy skin name is \"Ngawarr\" which translates to \"light at first dawn\". This name\nis hugely important to me as it reminds me of where my cultural heritage lies,\nwhich can be very easy to lose due to my skin pigmentation and British-\nAustralian accent.\n\nLanguage is a foundational path to one's culture and revitalization of culture\ncan have profound impacts on your friends, family and sense of self.\n\nThat's why preventing the extinction of these cultures is now at the forefront\nof my mind and one of the main reasons for creating Ourland & AIOR.\n\n### Researching 1201 Indigenous dialects in 36 minutes\n\n#### AIOR\n\nAIOR in AI orchestrated research platform that enables a user to specify an\nexpected data model alongside user parameters to then research & generate\ndatasets based off of those requirements.\n\nHere is AIOR researching Indigenous languages\n\n##### Start the Research\n\nGiven a dataset of 1201 Indigenous dialects that were retrieved from AIATSIS's\naustlang dataset - I split the dialects into batches of 100-ish, resulting in\n12 batches.\n\nI did this in order to reduce the risk of any catastrophic failures with my\nsanitization and JSON parsing code (I also haven't implemented much UI\noptimization so it'd be super laggy) I then processed each batch using a\nparallel concurrent batching process that I explain\n\nEach batch of 100 tasks was completed between 2:35 - 3:20 minutes (I didn't\nadd functionality to log these times, silly me) Taking out the delay of\nimporting a new batch and starting the process again:\n\nResearching 1201 Indigenous dialects took around 36 minutes!\n\nResulting in a combined 2.6MB JSON file with 39,000 lines!\n\n#### Breakdown\n\n  * Dialects: 1201\n  * Quickest Task: 2.84 seconds\n  * Longest Task: 20.2 seconds\n  * Average Task: 8.5 seconds\n  * Least Tokens Used: 1234\n  * Most Tokens Used: 3128\n  * Average Tokens Used: 1706\n\nCheckout the full dataset!\n\n### Some cool graphs (from a batch of 101 dialects):\n\n#### Time Taken\n\n#### LLM Tokens Used\n\n#### Dialect Prevalence\n\n### Tech\n\nThis is how the initial MVP of AIOR works from a high level.\n\nIt's pretty much similar to how AutoGPT or BabyAGI work\n\n  * Frontend: Allows the user to configure the research topic & data model\n  * Search Engine API enables an initial search for this data\n  * A sanitization step removes unnessary information from the previous request to save tokens & speed up the request\n  * An LLM processes that data and then assess if the goal is complete\n  * If the target goal is met - the data goes back to the frontend, if not: the process is repeated\n\n### Connecting an LLM to the Web\n\nGithub Repository (via Node)\n\n#### 1) Search (via Bing)\n\nHere an example request we can make to Bing to start the research process.\n\nMore information on the Bing API can be found here.\n\n    \n    \n    const researchTask = \"Indigenous Languages\"; const researchValue = \"Dictionary\"; const researchType = \"Url\"; const researchName = \"Lardil\"; async function search() { // Bing search API is used here but can be replaced with any other search engine API const searchEndpoint = \"https://api.bing.microsoft.com/v7.0/search?q=\"; const headers = { \"Ocp-Apim-Subscription-Key\": process.env.BING_API_KEY, }; const searchJson = await fetch(`${searchEndpoint}${researchTask}:${researchValue}+'${researchName}'`, { headers: headers as any, }); const searchData = await searchJson.json().catch((err) => { console.error(\"Error: \", err); return err.message; }); const transformedData = searchData.webPages.value.map((data: any) => { return { name: data.name, url: data.url, information: data.snippet, deepLinks: data.deepLinks, }; }); const llmData = await llm(transformedData); return parseData(llmData); }\n\n#### 2) LLM (via OpenAI)\n\nNOTE: this is sub-optimal approach and there are newer ways to achieve better\nresults\n\nOnce we retrieve the bing request we can then put it through an LLM to\nrearrange this data and either bring it back to the user, or continue the\nresearch process and attain more information.\n\nThe likelihood of any hallucinations should be a lot lower as we're providing\nit with specific context via the search response on top of some \"prompt\nengineering\".\n\n    \n    \n    async function llm(searchResponse: any) { // OAI is used here but can be replaced with any other language model API const completionsEndpoint = \"https://api.openai.com/v1/chat/completions\"; const model = \"gpt-3.5-turbo\"; // This is some basic initial prompt engineering, but can be expanded and improved to your needs const prompt = `Ensures that any given response is formatted as a valid JSON array. Output the top 4 results that are closest to the term: ${researchTask}:${researchName}+${researchValue}:${researchType}. The value must be of type ${researchType}. Returns: {name:\"\",value:\"\", information:\"\"}`; const mostRelevantInformation = await fetch(completionsEndpoint, { method: \"POST\", body: JSON.stringify({ model: model, messages: [ { role: \"system\", content: prompt, }, { role: \"user\", content: JSON.stringify(searchResponse) }, ], }), headers: { Authorization: \"Bearer \" + process.env.OPEN_AI_API_KEY, \"Content-Type\": \"application/json\", }, }); const mostRelevantData = await mostRelevantInformation.json(); return mostRelevantData.choices[0].message.content; }\n\n### Research Agent Organization\n\nThe above implementation can put you in good stead to start to explore how AI\nresearch agents can speed up any research tasks that you need todo but in\norder to orders of magnitude efficiecy improvements (without getting your self\nrate limited into oblivion) - we need to approach it differently.\n\nHere is how I'm currently implement agent organization in the AIOR MVP\n\n  1. Sequential Research (Linear)\n\nThis is the simplest and most inefficient process you can have.\n\nThere's 1 research process:\n\n  * it does a task\n  * completes that take\n  * takes a task from the pile\n  * repeats until it's complete\n\n  2. Batch Research (Concurrent)\n\nIf we split all our tasks into batches of a reasonable size (depending on API\nlimits) we can have multiple agents running at the same.\n\nThis allows for a speed that is equal to the number of research agents we can\nhave on the go at the same time.\n\nHuge improvement\n\n  3. Parallel Batch Research (Concurrent)\n\nThink about this research type like a really good project manager that knows\nas soon as you've completed task 1 in your batch that you can INSTANTLY pickup\nthe slack of your colleague from their batch.\n\nThis allows for an even greater efficiency improvement as Task 1-2-3 from\nBatches A, B, C won't always take the same amount of time.\n\nThis leads to a constant number of active tasks running until the goal is\ncompleted!\n\nThanks for reading!\n\nWant to know more? Sign up to the waitlist @ aior.app\n\nAIOR by land.org.au will also be submitting an application to Y Combinator's\nSummer 2024 as a non-profit, wish me luck!\n\nOurland\n\nProducts\n\nMapAIORAPI\n\nAbout\n\nCharterBlogApplyContact\n\nOurland Indigenous Corporation\u00a9 2024 | Non-profit status (in-progress)\n\n", "frontpage": false}
