{"aid": "40110222", "title": "Case Study \u2013 Optimizing Linear Layer", "url": "https://uchenml.tech/optimizing-linear/", "domain": "uchenml.tech", "votes": 1, "user": "euos", "posted_at": "2024-04-21 23:47:10", "comments": 0, "source_title": "Case Study - Optimizing Linear Layer", "source_text": "Case Study - Optimizing Linear Layer \u2014 UchenML\n\nUchenML\n\nApr 20, 2024 \u00b7 Eugene Ostroukhov \u00b7 Case Studies \u00b7 7 min read\n\n# Case Study - Optimizing Linear Layer\n\nThis case study details the process of optimizing linear layer performance\nwithout breaking the core Uchen ML requirements.\n\n## Overview\n\nAs Uchen.ml is heading towards the public announcement and first demos, some\nlow-hanging fruit needs to be picked in terms of optimizations. The most often\nused piece of any ML library is the linear layer as it is the most basic\nbuilding block for any neural net. This post details the process of optimizing\nthe code.\n\n## Requirements\n\nUchen is designed for implementing ML solutions that can be easily integrated\ninto existing systems, with specific goals on Web Assembly, embedded and video\ngames.\n\nTo maintain velocity and to avoid overcomplicating build and validation\nprocess, following constraints are in place:\n\n  * Only the C++20 standard library is used. ABSL dependency is there (logging, asserts and some utilities) but it is under consideration if its inclusion will remain mandatory.\n  * No compiler-specific optimizations, including pragmas, conditional compilations or intrinsics.\n  * No CPU architecture-specific optimizations. Particularly, no optimizations for one architecture that may be detrimental for others. Apple M2 and Intel Core CPUs are used to inform and direct the optimization efforts.\n  * Uchen is and will remain a CPU-only ML framework. There are no plans at this point to implement GPU or other acceleration support.\n\nThis constraints will be lifted as the deployment targets and actual\nrequirements are better understood.\n\n## Benchmark code\n\nThe benchmark runs inference through the linear layers of different\nconfigurations. Inputs are initialized to 0, parameters are initialized to\nrandom values outside the benchmark loop. Range of parameter values is between\n-1 and 1. Output values are not checked. float datatype is used for inputs and\noutputs\n\n    \n    \n    template <size_t Is, size_t Os, typename D = float> static void BM_Linear(benchmark::State& state) { // Linear layer with Is inputs and Os outputs uchen::Linear<uchen::Vector<D, Is>, Os> layer; // zero-initialized input vector. This operation is O(n) to the number // of the inputs and may have a negligible impact on benchmark. uchen::Vector<D, Is> input; // Parameters are using the store filled with random values outside the loop. // This operation is O(1) to the number of parameters and has no impact // on benchmark. uchen::parameters_t<decltype(layer)> parameters(params->store()); for (auto _ : state) { benchmark::DoNotOptimize(layer(input, parameters)); } }\n\n## Hardware\n\nRegular PCs are used. Note that the numbers can not be compared across the\narchitectures and this paper is only concerned with the relative gains and not\nthe absolute values.\n\n    \n    \n    Apple M2 Pro 10 Cores L1 Data 64 KiB L1 Instruction 128 KiB L2 Unified 4096 KiB (x10)\n    \n    \n    Intel(R) Core(TM) i7-10700KF CPU @ 3.80GHz 3.79 GHz L1 Data 32 KiB (x8) L1 Instruction 32 KiB (x8) L2 Unified 256 KiB (x8) L3 Unified 16384 KiB (x1)\n\n## Naive version\n\nLinear layer runs the following operation to produce the output:\n\nWhich translated into the following C++ code:\n\n    \n    \n    output_t operator()(const input_t& inputs, const Parameters<parameter_count>& parameters) const { output_t outputs; constexpr size_t Is = input_t::elements; for (size_t output = 0; output < Outputs; ++output) { outputs[output] = parameters[output * (Is + 1) + Is]; for (size_t input = 0; input < Is; ++input) { outputs[output] += inputs[input] * parameters[output * (Is + 1) + input]; } } return outputs; }\n\nGiven n inputs and m outputs, parameters layout is: Parameters are a flat\narray in the following format (n is a number of inputs, m is the number of\noutputs):\n\n### Benchmark Results:\n\n<inputs, outputs>| Parameter Count| i7-10700KF| Apple M2 Pro  \n---|---|---|---  \n<100, 200>| 20,200| 13,645 ns| 6882 ns  \n<2500, 8>| 20,008| 17,086 ns| 16,307 ns  \n<8, 2500>| 22,500| 5032 ns| 2177 ns  \n  \nNote that the number of operations (memory reads, stores and arithmetic) is\ndirectly correlated to the number of parameters so all the models were set up\nto have roughly the same number of them.\n\nIntel architecture shows approximately 3.4x spread between the best case\nscenario (number outputs drastically exceeds number of inputs) and worst case\nscenario (number of inputs is much greater then the number of outputs). Apple\nARM implementation showed 7.5x spread.\n\n# Transposed iteration order\n\nThe first optimization is to change the iteration order. Instead of iterating\nover the outputs and then over the inputs, we iterate over the inputs and then\nover the outputs. This change allows for better cache utilization and reduces\nthe number of cache misses.\n\n    \n    \n    output_t operator()( const input_t& inputs, const Parameters<(Input::elements + 1) * Outputs>& parameters) const { auto it = parameters.begin(); output_t outputs; for (size_t i = 0; i < outputs.size(); i++) { outputs[i] = *it++; } for (auto input : inputs) { for (auto& output : outputs) { output += (*it++) * input; } } return outputs; }\n\nNote that the code uses a linear iteration over the parameters, making the\nmemory access pattern more predictable and cache-friendly.\n\nParameters layout is as follows (n is a number of inputs, m is the number of\noutputs):\n\n### Benchmark Results:\n\n<inputs, outputs>| i7-10700KF| Apple M2 Pro  \n---|---|---  \n100, 200| 1880 ns| 1326 ns  \n2500, 8| 5611ns| 11,124 ns  \n8, 2500| 2015 ns| 1354 ns  \n  \nNumber of inputs has a significant negative impact on the performance as it\nadds an extra memory load.\n\n## Compile for the specific CPU\n\nBy default, the compiler generates the code that works on most CPUs. This\nprecludes usage of some newer instructions that have a significant impact on\nthe performance. To test this, I pass -march=native to the compiler which\nmakes it target my current CPU. I am using Bazel, so the invocation looked\nlike this (-g is for including debugging symbols as I use gdb to look at the\nassembly code):\n\n    \n    \n    bazel run -c opt --cxxopt=\"-g\" --cxxopt=\"-march=native\" //benchmark:linear\n\n### Benchmark Results (Intel only):\n\n<inputs, outputs>| i7-10700KF  \n---|---  \n100, 200| 1203 ns  \n2500, 8| 4871 ns  \n8, 2500| 1080 ns  \n  \nThis \u201coptimization\u201d shows pretty solid across the board. Ultimately, it will\nbe up to embedders to decide the CPU target to compile for.\n\n## A bigger layer\n\nWith the numbers in single digit microseconds, it seems reasonable to increase\nthe size of the linear layer to see what impact it has on the performance.\n\n### Benchmark Results (Intel only):\n\n<inputs, outputs>| i7-10700KF  \n---|---  \n100, 200| 1180 ns  \n2500, 8| 5056 ns  \n8, 2500| 1148 ns  \n4000, 2000| 1496652 ns  \n1000000, 8| 2986400 ns  \n8, 1000000| 2185253 ns  \n  \nThe worst case is \u201conly\u201d 2x slower than the best case.\n\n## Using SIMD intrinsics directly\n\nAs mentioned above, SIMD intrinsics are not considered (i.e. Web Assembly\nstill has no support for them). However, it is still interesting to see what\nthe performance gains could be if we tried them. I did not use AVX as data\nalignment is not supported yet, though this will change soon.\n\n    \n    \n    output_t operator()( const input_t& inputs, const Parameters<(Input::elements + 1) * Outputs>& parameters) const { auto it = parameters.begin(); output_t outputs; for (size_t i = 0; i < outputs.size(); i++) { outputs[i] = (*it++); } for (size_t i = 0; i < Input::elements; i += 4) { __m128 input = _mm_loadu_ps(&(*inputs.begin()) + i); for (size_t j = 0; j < Outputs; ++j) { // Parameters are accessed in the wrong order - this is a bug! // This code is for benchmark only. __m128 parameters = _mm_load_ps(&(*it)); __m128 product = _mm_dp_ps(input, parameters, 0xf1); it += 4; outputs[j] += _mm_cvtss_f32(product); } } return outputs; } };\n\n### Benchmark Results (Intel only):\n\n<inputs, outputs>| i7-10700KF  \n---|---  \n100, 200| 5765 ns  \n2500, 8| 5955 ns  \n8, 2500| 5761 ns  \n4000, 2000| 2783286 ns  \n1000000, 8| 2964343 ns  \n8, 1000000| 3214760 ns  \n  \n\u201dGood\u201d cases worsened, which shows that the updated code is ran. Shape of the\ndata is known at compile time when using Uchen so the compilers make informed\ndecisions about vectorization and other optimizations and it looks like\ncompeting with them is an unnecessary exercise.\n\nManually unrolling the loop yields similar results:\n\n    \n    \n    output_t operator()( const input_t& inputs, const Parameters<(Input::elements + 1) * Outputs>& parameters) const { auto it = parameters.begin(); output_t outputs; for (size_t i = 0; i < outputs.size(); i++) { outputs[i] = (*it++); } for (auto input : inputs) { for (size_t i = 0; i < Outputs; i++) { outputs[i++] += (*it++) * input; outputs[i++] += (*it++) * input; outputs[i++] += (*it++) * input; outputs[i] += (*it++) * input; } } return outputs; }\n\n<inputs, outputs>| i7-10700KF  \n---|---  \n100, 200| 6523 ns  \n2500, 8| 5088 ns  \n8, 2500| 6763 ns  \n4000, 2000| 3226969 ns  \n1000000, 8| 2918274 ns  \n8, 1000000| 3734533 ns  \n  \n## Conclusion\n\nAt this point it looks like the performance on this granularity is pretty\nclose to the practical limit for a single core. Next article will detail\nmulti-threading and the memory alignment (particularly, dealing with the\nnumber of parameters not divisible by 8).\n\nThe backpropagation optimizations and Uchen\u2019s approach to the memory\nmanagement will also be detailed in the future articles.\n\n  * c++\n  * uchen ml\n  * optimization\n\nShare:\n\nBack to Blog\n\n## Related Posts\n\nView All Posts \u00bb\n\nUchenML\n\nCompany\n\n  * Blog\n\n", "frontpage": false}
