{"aid": "40165064", "title": "What is the ideal growth rate for a dynamically allocated array?", "url": "https://stackoverflow.com/questions/1100311/what-is-the-ideal-growth-rate-for-a-dynamically-allocated-array", "domain": "stackoverflow.com", "votes": 1, "user": "phewlink", "posted_at": "2024-04-26 01:42:09", "comments": 0, "source_title": "What is the ideal growth rate for a dynamically allocated array?", "source_text": "math - What is the ideal growth rate for a dynamically allocated array? -\nStack Overflow\n\n#\n\nJoin Stack Overflow\n\nBy clicking \u201cSign up\u201d, you agree to our terms of service and acknowledge you\nhave read our privacy policy.\n\n# OR\n\nAlready have an account? Log in\n\nSkip to main content\n\nStack Overflow\n\n  1. About\n  2. Products\n  3. For Teams\n\n  1. Stack Overflow Public questions & answers\n  2. Stack Overflow for Teams Where developers & technologists share private knowledge with coworkers\n  3. Talent Build your employer brand\n  4. Advertising Reach developers & technologists worldwide\n  5. Labs The future of collective knowledge sharing\n  6. About the company\n\n##### CollectivesTM on Stack Overflow\n\nFind centralized, trusted content and collaborate around the technologies you\nuse most.\n\nLearn more about Collectives\n\nTeams\n\nQ&A for work\n\nConnect and share knowledge within a single location that is structured and\neasy to search.\n\nLearn more about Teams\n\nGet early access and see previews of new features.\n\nLearn more about Labs\n\n# What is the ideal growth rate for a dynamically allocated array?\n\nAsk Question\n\nAsked 14 years, 9 months ago\n\nModified 1 year, 1 month ago\n\nViewed 30k times\n\nThis question shows research effort; it is useful and clear\n\n112\n\nSave this question.\n\nShow activity on this post.\n\nC++ has std::vector and Java has ArrayList, and many other languages have\ntheir own form of dynamically allocated array. When a dynamic array runs out\nof space, it gets reallocated into a larger area and the old values are copied\ninto the new array. A question central to the performance of such an array is\nhow fast the array grows in size. If you always only grow it large enough to\nfit the current push, you'll end up reallocating every time. So it makes sense\nto double the array size, or multiply it by say 1.5.\n\nIs there an ideal growth factor? 2x? 1.5x? By ideal I mean mathematically\njustified, best balancing performance and wasted memory. I realize that\ntheoretically, given that your application could have any potential\ndistribution of pushes that this is somewhat application dependent. But I'm\ncurious to know if there's a value that's \"usually\" best, or is considered\nbest within some rigorous constraint.\n\nI've heard there's a paper on this somewhere, but I've been unable to find it.\n\n  * arrays\n  * math\n  * vector\n  * arraylist\n  * dynamic-arrays\n\nShare\n\nCC BY-SA 4.0\n\nImprove this question\n\nFollow this question to receive notifications\n\nedited Mar 27, 2023 at 15:54\n\nMehdi Charife\n\n92833 gold badges88 silver badges2424 bronze badges\n\nasked Jul 8, 2009 at 20:15\n\nJoseph GarvinJoseph Garvin\n\n21.3k1919 gold badges100100 silver badges169169 bronze badges\n\n0\n\nAdd a comment |\n\n## 12 Answers 12\n\nReset to default\n\nThis answer is useful\n\n128\n\nSave this answer.\n\nShow activity on this post.\n\nI remember reading many years ago why 1.5 is preferred over two, at least as\napplied to C++ (this probably doesn't apply to managed languages, where the\nruntime system can relocate objects at will).\n\nThe reasoning is this:\n\n  1. Say you start with a 16-byte allocation.\n  2. When you need more, you allocate 32 bytes, then free up 16 bytes. This leaves a 16-byte hole in memory.\n  3. When you need more, you allocate 64 bytes, freeing up the 32 bytes. This leaves a 48-byte hole (if the 16 and 32 were adjacent).\n  4. When you need more, you allocate 128 bytes, freeing up the 64 bytes. This leaves a 112-byte hole (assuming all previous allocations are adjacent).\n  5. And so and and so forth.\n\nThe idea is that, with a 2x expansion, there is no point in time that the\nresulting hole is ever going to be large enough to reuse for the next\nallocation. Using a 1.5x allocation, we have this instead:\n\n  1. Start with 16 bytes.\n  2. When you need more, allocate 24 bytes, then free up the 16, leaving a 16-byte hole.\n  3. When you need more, allocate 36 bytes, then free up the 24, leaving a 40-byte hole.\n  4. When you need more, allocate 54 bytes, then free up the 36, leaving a 76-byte hole.\n  5. When you need more, allocate 81 bytes, then free up the 54, leaving a 130-byte hole.\n  6. When you need more, use 122 bytes (rounding up) from the 130-byte hole.\n\nShare\n\nCC BY-SA 3.0\n\nImprove this answer\n\nFollow this answer to receive notifications\n\nedited Jun 17, 2015 at 9:18\n\nEmil Laine\n\n42.1k1111 gold badges105105 silver badges158158 bronze badges\n\nanswered Jul 8, 2009 at 20:36\n\nC. K. YoungC. K. Young\n\n221k4646 gold badges388388 silver badges441441 bronze badges\n\n13\n\n  * 5\n\nA random forum post I found (objectmix.com/c/...) reasons similarly. A poster\nclaims that (1+sqrt(5))/2 is the upper limit for reuse.\n\n\u2013 Naaff\n\nJul 8, 2009 at 21:05\n\n  * 20\n\nIf that claim is correct, then phi (== (1 + sqrt(5)) / 2) is indeed the\noptimal number to use.\n\n\u2013 C. K. Young\n\nJul 8, 2009 at 21:07\n\n  * 2\n\nI like this answer because it reveals the rationale of 1.5x versus 2x, but\nJon's is technically most correct for the way I stated it. I should have just\nasked why 1.5 has been recommended in the past :p\n\n\u2013 Joseph Garvin\n\nApr 15, 2010 at 15:00\n\n  * 10\n\nFacebook uses 1.5 in it's FBVector implementation, article here explains why\n1.5 is optimal for FBVector.\n\n\u2013 csharpfolk\n\nNov 4, 2014 at 18:13\n\n  * 3\n\n@jackmott Right, exactly as my answer noted: \"this probably doesn't apply to\nmanaged languages, where the runtime system can relocate objects at will\".\n\n\u2013 C. K. Young\n\nAug 16, 2016 at 13:12\n\n| Show 8 more comments\n\nThis answer is useful\n\n61\n\nSave this answer.\n\nShow activity on this post.\n\nIn the limit as n \u2192 \u221e, it would be the golden ratio: \u03c6 = 1.618...\n\nFor finite n, you want something close, like 1.5.\n\nThe reason is that you want to be able to reuse older memory blocks, to take\nadvantage of caching and avoid constantly making the OS give you more memory\npages. The equation you'd solve to ensure that a subsequent allocation can re-\nuse all prior blocks reduces to x^n \u2212 1 \u2212 1 = x^n + 1 \u2212 x^n, whose solution\napproaches x = \u03c6 for large n. In practice n is finite and you'll want to be\nable to reusing the last few blocks every few allocations, and so 1.5 is great\nfor ensuring that. (See the link for a more detailed explanation.)\n\nShare\n\nCC BY-SA 4.0\n\nImprove this answer\n\nFollow this answer to receive notifications\n\nedited Jan 22, 2023 at 15:16\n\nanswered Dec 9, 2013 at 21:31\n\nuser541686user541686\n\n208k132132 gold badges545545 silver badges909909 bronze badges\n\n2\n\n  * 1\n\n(Not sure why you deleted all of both of our comments, but I'd like to add\nsome neutral clarifications for anyone who encounters this.) To clarify, n in\nthis answer is not the size of the array, it's the minimum number of\nreallocations before you'll be able to reuse memory. So n \u2192 \u221e doesn't mean \"as\nthe array grows to infinity\", it means that the higher your tolerance for\nwasted memory is, the closer to the golden ratio you'd want your growth factor\nto be. Note this calculation only makes practical sense for small n and growth\nrates further from \u03c6, because\n\n\u2013 Han Seoul-Oh\n\nMay 24, 2021 at 18:45\n\n  * 1\n\nlarge but finite n, with growth rates approaching \u03c6, would mean you would only\nbe able to reuse older memory blocks after many many reallocations; if your\nuse case is so insensitive to wasted memory, a growth rate of 2x would perform\nbetter than a rate near \u03c6.\n\n\u2013 Han Seoul-Oh\n\nMay 24, 2021 at 18:46\n\nAdd a comment |\n\nThis answer is useful\n\n54\n\nSave this answer.\n\nShow activity on this post.\n\nIt will entirely depend on the use case. Do you care more about the time\nwasted copying data around (and reallocating arrays) or the extra memory? How\nlong is the array going to last? If it's not going to be around for long,\nusing a bigger buffer may well be a good idea - the penalty is short-lived. If\nit's going to hang around (e.g. in Java, going into older and older\ngenerations) that's obviously more of a penalty.\n\nThere's no such thing as an \"ideal growth factor.\" It's not just theoretically\napplication dependent, it's definitely application dependent.\n\n2 is a pretty common growth factor - I'm pretty sure that's what ArrayList and\nList<T> in .NET uses. ArrayList<T> in Java uses 1.5.\n\nEDIT: As Erich points out, Dictionary<,> in .NET uses \"double the size then\nincrease to the next prime number\" so that hash values can be distributed\nreasonably between buckets. (I'm sure I've recently seen documentation\nsuggesting that primes aren't actually that great for distributing hash\nbuckets, but that's an argument for another answer.)\n\nShare\n\nCC BY-SA 2.5\n\nImprove this answer\n\nFollow this answer to receive notifications\n\nedited Jul 8, 2009 at 21:07\n\nanswered Jul 8, 2009 at 20:25\n\nJon SkeetJon Skeet\n\n1.5m878878 gold badges9.2k9.2k silver badges9.3k9.3k bronze badges\n\n0\n\nAdd a comment |\n\nThis answer is useful\n\n17\n\nSave this answer.\n\nShow activity on this post.\n\nOne approach when answering questions like this is to just \"cheat\" and look at\nwhat popular libraries do, under the assumption that a widely used library is,\nat the very least, not doing something horrible.\n\nSo just checking very quickly, Ruby (1.9.1-p129) appears to use 1.5x when\nappending to an array, and Python (2.6.2) uses 1.125x plus a constant (in\nObjects/listobject.c):\n\n    \n    \n    /* This over-allocates proportional to the list size, making room * for additional growth. The over-allocation is mild, but is * enough to give linear-time amortized behavior over a long * sequence of appends() in the presence of a poorly-performing * system realloc(). * The growth pattern is: 0, 4, 8, 16, 25, 35, 46, 58, 72, 88, ... */ new_allocated = (newsize >> 3) + (newsize < 9 ? 3 : 6); /* check for integer overflow */ if (new_allocated > PY_SIZE_MAX - newsize) { PyErr_NoMemory(); return -1; } else { new_allocated += newsize; }\n\nnewsize above is the number of elements in the array. Note well that newsize\nis added to new_allocated, so the expression with the bitshifts and ternary\noperator is really just calculating the over-allocation.\n\nShare\n\nCC BY-SA 3.0\n\nImprove this answer\n\nFollow this answer to receive notifications\n\nedited Mar 12, 2018 at 14:49\n\nCristian Ciupitu\n\n20.7k77 gold badges5252 silver badges7878 bronze badges\n\nanswered Jul 8, 2009 at 20:41\n\nJason CJason C\n\n21.7k1010 gold badges4040 silver badges3333 bronze badges\n\n2\n\n  * So it grows the array from n to n + (n/8 + (n<9?3:6)), which means the growth factor, in the question's terminology, is 1.25x (plus a constant).\n\n\u2013 ShreevatsaR\n\nJul 8, 2009 at 20:58\n\n  * Wouldn't it be 1.125x plus a constant?\n\n\u2013 Jason C\n\nJul 8, 2009 at 21:15\n\nAdd a comment |\n\nThis answer is useful\n\n16\n\nSave this answer.\n\nShow activity on this post.\n\nLet's say you grow the array size by x. So assume you start with size T. The\nnext time you grow the array its size will be T*x. Then it will be T*x^2 and\nso on.\n\nIf your goal is to be able to reuse the memory that has been created before,\nthen you want to make sure the new memory you allocate is less than the sum of\nprevious memory you deallocated. Therefore, we have this inequality:\n\n    \n    \n    T*x^n <= T + T*x + T*x^2 + ... + T*x^(n-2)\n\nWe can remove T from both sides. So we get this:\n\n    \n    \n    x^n <= 1 + x + x^2 + ... + x^(n-2)\n\nInformally, what we say is that at nth allocation, we want our all previously\ndeallocated memory to be greater than or equal to the memory need at the nth\nallocation so that we can reuse the previously deallocated memory.\n\nFor instance, if we want to be able to do this at the 3rd step (i.e., n=3),\nthen we have\n\n    \n    \n    x^3 <= 1 + x\n\nThis equation is true for all x such that 0 < x <= 1.3 (roughly)\n\nSee what x we get for different n's below:\n\n    \n    \n    n maximum-x (roughly) 3 1.3 4 1.4 5 1.53 6 1.57 7 1.59 22 1.61\n\nNote that the growing factor has to be less than 2 since x^n > x^(n-2) + ... +\nx^2 + x + 1 for all x>=2.\n\nShare\n\nCC BY-SA 3.0\n\nImprove this answer\n\nFollow this answer to receive notifications\n\nedited Oct 22, 2016 at 4:47\n\nanswered Dec 4, 2012 at 5:15\n\nCEGRDCEGRD\n\n7,88555 gold badges2626 silver badges3535 bronze badges\n\n3\n\n  * You seem to claim that you can already reuse the previously deallocated memory at the 2nd allocation with a factor of 1.5. This is not true (see above). Let me know if I misunderstood you.\n\n\u2013 awx\n\nFeb 22, 2013 at 12:54\n\n  * At 2nd allocation you are allocating 1.5*1.5*T = 2.25*T while total deallocation you will be doing until then is T + 1.5*T = 2.5*T. So 2.5 is greater than 2.25.\n\n\u2013 CEGRD\n\nFeb 23, 2013 at 16:22\n\n  * 2\n\nAh, I should read more carefully; all you say is that the total deallocated\nmemory will be more than the allocated memory at the nth allocation, not that\nyou can reuse it at the nth allocation.\n\n\u2013 awx\n\nFeb 28, 2013 at 14:11\n\nAdd a comment |\n\nThis answer is useful\n\n7\n\nSave this answer.\n\nShow activity on this post.\n\nAnother two cents\n\n  * Most computers have virtual memory! In the physical memory you can have random pages everywhere which are displayed as a single contiguous space in your program's virtual memory. The resolving of the indirection is done by the hardware. Virtual memory exhaustion was a problem on 32 bit systems, but it is really not a problem anymore. So filling the hole is not a concern anymore (except special environments). Since Windows 7 even Microsoft supports 64 bit without extra effort. @ 2011\n  * O(1) is reached with any r > 1 factor. Same mathematical proof works not only for 2 as parameter.\n  * r = 1.5 can be calculated with old*3/2 so there is no need for floating point operations. (I say /2 because compilers will replace it with bit shifting in the generated assembly code if they see fit.)\n  * MSVC went for r = 1.5, so there is at least one major compiler that does not use 2 as ratio.\n\nAs mentioned by someone 2 feels better than 8. And also 2 feels better than\n1.1.\n\nMy feeling is that 1.5 is a good default. Other than that it depends on the\nspecific case.\n\nShare\n\nCC BY-SA 3.0\n\nImprove this answer\n\nFollow this answer to receive notifications\n\nanswered Jan 3, 2017 at 16:42\n\nNotinlistNotinlist\n\n16.4k1111 gold badges5757 silver badges101101 bronze badges\n\n3\n\n  * 5\n\nIt would be better to use n + n/2 to delay overflow. Using n*3/2 cuts your\npossible capacity by half.\n\n\u2013 owacoder\n\nMay 4, 2019 at 12:27\n\n  * @owacoder True. But when n*3 does not fit but n*1.5 fits we are talking about a lot of memory. If n is 32 bit unsigend then n*3 overflows when n is 4G/3, that is 1.333G approx. That is a huge number. That is lot's of memory to have in a single allocation. Evern more if elements are not 1 byte but for example 4 bytes each. Wondering about the use case...\n\n\u2013 Notinlist\n\nMay 4, 2019 at 14:40\n\n  * 5\n\nIt's true that it may be an edge case, but edge cases are what usually bite.\nGetting in the habit of looking for possible overflow or other behaviors that\nmay hint at a better design is never a bad idea, even if it may seem\nfarfetched in the present. Take 32-bit addresses as an example. Now we need\n64...\n\n\u2013 owacoder\n\nMay 4, 2019 at 19:05\n\nAdd a comment |\n\nThis answer is useful\n\n5\n\nSave this answer.\n\nShow activity on this post.\n\nThe top-voted and the accepted answer are both good, but neither answer the\npart of the question asking for a \"mathematically justified\" \"ideal growth\nrate\", \"best balancing performance and wasted memory\". (The second-top-voted\nanswer does try to answer this part of the question, but its reasoning is\nconfused.)\n\nThe question perfectly identifies the 2 considerations that have to be\nbalanced, performance and wasted memory. If you choose a growth rate too low,\nperformance suffers because you'll run out of extra space too quickly and have\nto reallocate too frequently. If you choose a growth rate too high, like 2x,\nyou'll waste memory because you'll never be able to reuse old memory blocks.\n\nIn particular, if you do the math^1 you'll find that the upper limit on the\ngrowth rate is the golden ratio \u03c6 = 1.618... . Growth rate larger than \u03c6 (like\n2x) mean that you'll never be able to reuse old memory blocks. Growth rates\nonly slightly less than \u03c6 mean you won't be able to reuse old memory blocks\nuntil after many many reallocations, during which time you'll be wasting\nmemory. So you want to be as far below \u03c6 as you can get without sacrificing\ntoo much performance.\n\nTherefore I'd suggest these candidates for \"mathematically justified\" \"ideal\ngrowth rate\", \"best balancing performance and wasted memory\":\n\n  * \u22481.466x (the solution to x^4=1+x+x^2) allows memory reuse after just 3 reallocations, one sooner than 1.5x allows, while reallocating only slightly more frequently\n  * \u22481.534x (the solution to x^5=1+x+x^2+x^3) allows memory reuse after 4 reallocations, same as 1.5x, while reallocating slightly less frequently for improved performance\n  * \u22481.570x (the solution to x^6=1+x+x^2+x^3+x^4) only allows memory reuse after 5 reallocations, but will reallocate even less infrequently for even further improved performance (barely)\n\nClearly there's some diminishing returns there, so I think the global optimum\nis probably among those. Also, note that 1.5x is a great approximation to\nwhatever the global optimum actually is, and has the advantage being extremely\nsimple.\n\n^1 Credits to @user541686 for this excellent source.\n\nShare\n\nCC BY-SA 4.0\n\nImprove this answer\n\nFollow this answer to receive notifications\n\nedited May 24, 2021 at 19:00\n\nanswered May 22, 2021 at 10:50\n\nHan Seoul-OhHan Seoul-Oh\n\n1,23611 gold badge1313 silver badges2020 bronze badges\n\n0\n\nAdd a comment |\n\nThis answer is useful\n\n5\n\nSave this answer.\n\nShow activity on this post.\n\nI recently was fascinated by the experimental data I've got on the wasted\nmemory aspect of things. The chart below is showing the \"overhead factor\"\ncalculated as the amount of overhead space divided by the useful space, the\nx-axis shows a growth factor. I'm yet to find a good explanation/model of what\nit reveals.\n\nSimulation snippet:\nhttps://gist.github.com/gubenkoved/7cd3f0cb36da56c219ff049e4518a4bd.\n\nNeither shape nor the absolute values that simulation reveals are something\nI've expected.\n\nHigher-resolution chart showing dependency on the max useful data size is\nhere: https://i.stack.imgur.com/Ld2yJ.png.\n\nUPDATE. After pondering this more, I've finally come up with the correct model\nto explain the simulation data, and hopefully, it matches experimental data\nnicely. The formula is quite easy to infer simply by looking at the size of\nthe array that we would need to have for a given amount of elements we need to\ncontain.\n\nReferenced earlier GitHub gist was updated to include calculations using\nscipy.integrate for numerical integration that allows creating the plot below\nwhich verifies the experimental data pretty nicely.\n\nUPDATE 2. One should however keep in mind that what we model/emulate there\nmostly has to do with the Virtual Memory, meaning the over-allocation\noverheads can be left entirely on the Virtual Memory territory as physical\nmemory footprint is only incurred when we first access a page of Virtual\nMemory, so it's possible to malloc a big chunk of memory, but until we first\naccess the pages all we do is reserving virtual address space. I've updated\nthe GitHub gist with CPP program that has a very basic dynamic array\nimplementation that allows changing the growth factor and the Python snippet\nthat runs it multiple times to gather the \"real\" data. Please see the final\ngraph below.\n\nThe conclusion there could be that for x64 environments where virtual address\nspace is not a limiting factor there could be really little to no difference\nin terms of the Physical Memory footprint between different growth factors.\nAdditionally, as far as Virtual Memory is concerned the model above seems to\nmake pretty good predictions!\n\nSimulation snippet was built with g++.exe simulator.cpp -o simulator.exe on\nWindows 10 (build 19043), g++ version is below.\n\n    \n    \n    g++.exe (x86_64-posix-seh-rev0, Built by MinGW-W64 project) 8.1.0\n\nPS. Note that the end result is implementation-specific. Depending on\nimplementation details dynamic array might or might not access the memory\noutside the \"useful\" boundaries. Some implementations would use memset to\nzero-initialize POD elements for whole capacity -- this will cause virtual\nmemory page translated into physical. However, std::vector implementation on a\nreferenced above compiler does not seem to do that and so behaves as per mock\ndynamic array in the snippet -- meaning overhead is incurred on the Virtual\nMemory side, and negligible on the Physical Memory.\n\nShare\n\nCC BY-SA 4.0\n\nImprove this answer\n\nFollow this answer to receive notifications\n\nedited Nov 22, 2021 at 13:56\n\nanswered Nov 4, 2021 at 11:13\n\nEugene D. GubenkovEugene D. Gubenkov\n\n5,22766 gold badges4141 silver badges7373 bronze badges\n\n2\n\n  * Could you elaborate on how you derived the formula? Do it\u2019s input and output correspond directly to the x and y axes?\n\n\u2013 Joseph Garvin\n\nNov 11, 2021 at 20:36\n\n  * 1\n\nThe formula is derived as follows -- the central piece there is\nalpha^ceil(log(n, alpha)) -- this is the dynamic array capacity required to\ncontain n items with a given growth rate (alpha). Then it's trivial to get an\noverhead factor (beta) as a ratio of overhead to the useful size (n), so it\ngives us alpha^ceil(log(n, alpha)) - n / n. The last step is to find an\naverage case (math expectancy) for that we integrate over the n on a range\n[min, max] dividing by width such interval. Input/output (which is alpha/beta\nor growth rate/overhead factor) do correspond to x and y axes.\n\n\u2013 Eugene D. Gubenkov\n\nNov 12, 2021 at 5:49\n\nAdd a comment |\n\nThis answer is useful\n\n3\n\nSave this answer.\n\nShow activity on this post.\n\nIt really depends. Some people analyze common usage cases to find the optimal\nnumber.\n\nI've seen 1.5x 2.0x phi x, and power of 2 used before.\n\nShare\n\nCC BY-SA 2.5\n\nImprove this answer\n\nFollow this answer to receive notifications\n\nanswered Jul 8, 2009 at 20:38\n\nUnknownUnknown\n\n46.3k2727 gold badges141141 silver badges182182 bronze badges\n\n5\n\n  * Phi! That's a nice number to use. I should start using it from now on. Thanks! +1\n\n\u2013 C. K. Young\n\nJul 8, 2009 at 20:40\n\n  * I don't understand...why phi? What properties does it have that makes it suitable for this?\n\n\u2013 Jason C\n\nJul 8, 2009 at 20:55\n\n  * 4\n\n@Jason: phi makes for a Fibonacci sequence, so the next allocation size is the\nsum of the current size and the previous size. This allows for moderate rate\nof growth, faster than 1.5 but not 2 (see my post as to why >= 2 is not a good\nidea, at least for unmanaged languages).\n\n\u2013 C. K. Young\n\nJul 8, 2009 at 21:03\n\n  * 2\n\n@Jason: Also, according to a commenter to my post, any number > phi is in fact\na bad idea. I haven't done the math myself to confirm this, so take it with a\ngrain of salt.\n\n\u2013 C. K. Young\n\nJul 8, 2009 at 21:09\n\n  * @ChrisJester-Young To be clear, any growth rate even close to phi (\u2248 1.618) is bad if your goal is reuse memory. Any growth rate \u2265 phi, including 2x, will never be able to reuse memory, and growth rates only slightly less than phi will waste a lot of memory before being able to reuse any. You want to be much less than phi in order to reuse memory sooner and waste less, but that has to be balanced against more frequent reallocations and copies: stackoverflow.com/a/67648650/362030\n\n\u2013 Han Seoul-Oh\n\nMay 24, 2021 at 19:03\n\nAdd a comment |\n\nThis answer is useful\n\n3\n\nSave this answer.\n\nShow activity on this post.\n\nIf you have a distribution over array lengths, and you have a utility function\nthat says how much you like wasting space vs. wasting time, then you can\ndefinitely choose an optimal resizing (and initial sizing) strategy.\n\nThe reason the simple constant multiple is used, is obviously so that each\nappend has amortized constant time. But that doesn't mean you can't use a\ndifferent (larger) ratio for small sizes.\n\nIn Scala, you can override loadFactor for the standard library hash tables\nwith a function that looks at the current size. Oddly, the resizable arrays\njust double, which is what most people do in practice.\n\nI don't know of any doubling (or 1.5*ing) arrays that actually catch out of\nmemory errors and grow less in that case. It seems that if you had a huge\nsingle array, you'd want to do that.\n\nI'd further add that if you're keeping the resizable arrays around long\nenough, and you favor space over time, it might make sense to dramatically\noverallocate (for most cases) initially and then reallocate to exactly the\nright size when you're done.\n\nShare\n\nCC BY-SA 2.5\n\nImprove this answer\n\nFollow this answer to receive notifications\n\nanswered Jul 8, 2009 at 21:00\n\nJonathan GraehlJonathan Graehl\n\n9,2223636 silver badges4040 bronze badges\n\nAdd a comment |\n\nThis answer is useful\n\n1\n\nSave this answer.\n\nShow activity on this post.\n\nI know it is an old question, but there are several things that everyone seems\nto be missing.\n\nFirst, this is multiplication by 2: size << 1\\. This is multiplication by\nanything between 1 and 2: int(float(size) * x), where x is the number, the *\nis floating point math, and the processor has to run additional instructions\nfor casting between float and int. In other words, at the machine level,\ndoubling takes a single, very fast instruction to find the new size.\nMultiplying by something between 1 and 2 requires at least one instruction to\ncast size to a float, one instruction to multiply (which is float\nmultiplication, so it probably takes at least twice as many cycles, if not 4\nor even 8 times as many), and one instruction to cast back to int, and that\nassumes that your platform can perform float math on the general purpose\nregisters, instead of requiring the use of special registers. In short, you\nshould expect the math for each allocation to take at least 10 times as long\nas a simple left shift. If you are copying a lot of data during the\nreallocation though, this might not make much of a difference.\n\nSecond, and probably the big kicker: Everyone seems to assume that the memory\nthat is being freed is both contiguous with itself, as well as contiguous with\nthe newly allocated memory. Unless you are pre-allocating all of the memory\nyourself and then using it as a pool, this is almost certainly not the case.\nThe OS might occasionally end up doing this, but most of the time, there is\ngoing to be enough free space fragmentation that any half decent memory\nmanagement system will be able to find a small hole where your memory will\njust fit. Once you get to really bit chunks, you are more likely to end up\nwith contiguous pieces, but by then, your allocations are big enough that you\nare not doing them frequently enough for it to matter anymore. In short, it is\nfun to imagine that using some ideal number will allow the most efficient use\nof free memory space, but in reality, it is not going to happen unless your\nprogram is running on bare metal (as in, there is no OS underneath it making\nall of the decisions).\n\nMy answer to the question? Nope, there is no ideal number. It is so\napplication specific that no one really even tries. If your goal is ideal\nmemory usage, you are pretty much out of luck. For performance, less frequent\nallocations are better, but if we went just with that, we could multiply by 4\nor even 8! Of course, when Firefox jumps from using 1GB to 8GB in one shot,\npeople are going to complain, so that does not even make sense. Here are some\nrules of thumb I would go by though:\n\nIf you cannot optimize memory usage, at least don't waste processor cycles.\nMultiplying by 2 is at least an order of magnitude faster than doing floating\npoint math. It might not make a huge difference, but it will make some\ndifference at least (especially early on, during the more frequent and smaller\nallocations).\n\nDon't overthink it. If you just spent 4 hours trying to figure out how to do\nsomething that has already been done, you just wasted your time. Totally\nhonestly, if there was a better option than *2, it would have been done in the\nC++ vector class (and many other places) decades ago.\n\nLastly, if you really want to optimize, don't sweat the small stuff. Now days,\nno one cares about 4KB of memory being wasted, unless they are working on\nembedded systems. When you get to 1GB of objects that are between 1MB and 10MB\neach, doubling is probably way too much (I mean, that is between 100 and 1,000\nobjects). If you can estimate expected expansion rate, you can level it out to\na linear growth rate at a certain point. If you expect around 10 objects per\nminute, then growing at 5 to 10 object sizes per step (once every 30 seconds\nto a minute) is probably fine.\n\nWhat it all comes down to is, don't over think it, optimize what you can, and\ncustomize to your application (and platform) if you must.\n\nShare\n\nCC BY-SA 3.0\n\nImprove this answer\n\nFollow this answer to receive notifications\n\nedited May 21, 2016 at 4:55\n\nanswered May 21, 2016 at 4:44\n\nRybec ArethdarRybec Arethdar\n\n5144 bronze badges\n\n8\n\n  * 13\n\nOf course n + n >> 1 is the same as 1.5 * n. It is fairly easy to come up with\nsimilar tricks for every practical growth factor you can think of.\n\n\u2013 Gaslight Deceive Subvert\n\nOct 13, 2016 at 1:11\n\n  * This is a good point. Note, however, that outside of ARM, this at least doubles the number of instructions. (Many ARM instructions, including the add instruction, can do an optional shift on one of the arguments, allowing your example to work in a single instruction. Most architectures can't do this though.) No, in most cases, doubling the number of instructions from one to two is not a significant issue, but for more complex growth factors where the math is more complex, it could make a performance difference for a sensitive program.\n\n\u2013 Rybec Arethdar\n\nOct 23, 2018 at 3:40\n\n  * 1\n\n@Rybec - While there may be some programs that are sensitive to timing\nvariations by one or two instructions, it's very unlikely that any program\nthat uses dynamic reallocations will ever be concerned that. If it needs to\ncontrol the timing that finely, it will probably be using statically allocated\nstorage instead.\n\n\u2013 owacoder\n\nMay 4, 2019 at 19:09\n\n  * I do games, where one or two instructions can make a significant performance difference in the wrong place. That said, if memory allocation is handled well, it shouldn't happen frequently enough for a few instructions to make a difference.\n\n\u2013 Rybec Arethdar\n\nMay 5, 2019 at 21:29\n\n  * 1\n\nI think talking about the performance of integer arithmetic vs. floating point\nis largely irrelevant in context of dynamic arrays as this single calculation\nper reallocation is totally negligible comparing to other processes that need\nto take place.\n\n\u2013 Eugene D. Gubenkov\n\nNov 7, 2021 at 7:46\n\n| Show 3 more comments\n\nThis answer is useful\n\n0\n\nSave this answer.\n\nShow activity on this post.\n\nI agree with Jon Skeet, even my theorycrafter friend insists that this can be\nproven to be O(1) when setting the factor to 2x.\n\nThe ratio between cpu time and memory is different on each machine, and so the\nfactor will vary just as much. If you have a machine with gigabytes of ram,\nand a slow CPU, copying the elements to a new array is a lot more expensive\nthan on a fast machine, which might in turn have less memory. It's a question\nthat can be answered in theory, for a uniform computer, which in real\nscenarios doesnt help you at all.\n\nShare\n\nCC BY-SA 2.5\n\nImprove this answer\n\nFollow this answer to receive notifications\n\nanswered Jul 8, 2009 at 20:35\n\nTomTom\n\n3,16366 gold badges3333 silver badges3838 bronze badges\n\n3\n\n  * 3\n\nTo elaborate, doubling the array size means that you get amotized O(1)\ninserts. The idea is that every time you insert an element, you copy an\nelement from the old array as well. Lets say you have an array of size m, with\nm elements in it. When adding element m+1, there is no space, so you allocate\na new array of size 2m. Instead of copying all the first m elements, you copy\none every time you insert a new element. This minimize the variance (save for\nthe allocation of the memory), and once you have inserted 2m elements, you\nwill have copied all elements from the old array.\n\n\u2013 hvidgaard\n\nNov 4, 2014 at 10:06\n\n  * @hvidgaard how does that work with random access exactly...? I don't see a way how to do that without branching, seems like copying would be faster overall, that's assuming you need to copy at all.\n\n\u2013 Kaihaku\n\nSep 20, 2021 at 10:55\n\n  * @hvidgaard Any growth factor, r, gives an amortized complexity of O(1) for insertions. The mathematical proof imagines that each time an element is inserted, a \"debt\" of copying 1/(r-1) elements is paid. Since this number is constant, the amortized complexity is O(1). However, when reallocation occurs (and that cannot happen in-place), there is no lazy copying. All existing elements are copied right away, so the worst case complexity of insertion is O(n). Otherwise, there would be no point in talking about amortized complexity.\n\n\u2013 nielsen\n\nFeb 10, 2023 at 8:11\n\nAdd a comment |\n\n##\n\nNot the answer you're looking for? Browse other questions tagged\n\n  * arrays\n  * math\n  * vector\n  * arraylist\n  * dynamic-arrays\n\nor ask your own question.\n\n  * The Overflow Blog\n  * Is GenAI the next dot-com bubble?\n\n  * What language should beginning programmers choose?\n\n  * Featured on Meta\n  * New Focus Styles & Updated Styling for Button Groups\n\n  * Upcoming initiatives on Stack Overflow and across the Stack Exchange network\n\n  * Pausing the 1-rep voting experiment on Stack Overflow: reflecting on the...\n\n  * Temporary policy: Generative AI (e.g., ChatGPT) is banned\n\n#### Linked\n\n0\n\nWhy is dynamicaly allocated array's size upon insertion 2*size of the initial\narray, rather than size+1?\n\n0\n\nWhy does the capacity of a vector increase by a factor of 1.5?\n\n0\n\nWhat is the better memory allocation method in C when the input data exceed\nthe original allocated space?\n\n178\n\nC dynamically growing array\n\n20\n\nAmortized analysis of std::vector insertion\n\n3\n\nLists double their space in c# when they need more room. At some point does it\nbecome less efficient to double say 1024 to 2048?\n\n3\n\nwhy so many copying while transforming/copying vector\n\n2\n\nWhy does push_back in this implementation reserve 2 * capacity + 1 instead of\n2 * capacity?\n\n4\n\nTime complexity of append operation in simple array\n\n4\n\nWhy Java grows ArrayList by 3/2?\n\nSee more linked questions\n\n#### Related\n\n16\n\nsize of dynamically allocated array\n\n0\n\nDynamically Growing an Array in C++\n\n2\n\nC Dynamic allocation speed question\n\n2\n\nImplementing incremental array in C++\n\n6\n\nEfficiency of growing a dynamic array by a fixed constant each time?\n\n1\n\nTime complexity and growth strategy of dynamic arrays\n\n3\n\nIs there a mathematical way to calculate the optimal growth factor for a\ndynamic array?\n\n0\n\nSize of dynamic array vs static array\n\n1\n\nAmortized complexity for an dynamic array with linear progression?\n\n2\n\nWhy do dynamic arrays specifically double in size when running out of space?\n\n#### Hot Network Questions\n\n  * Prepositional Phrases as Arguments\n\n  * Can I get a new key cut without it being reprogrammed?\n\n  * Why did the original \u2018d\u2019 in the word \u2018weather\u2019 (< Middle English \u2018weder, wedir\u2019) change to \u2018th\u2019?\n\n  * How do I programmatically check if a Flatpak package is installed in a shell script?\n\n  * Is this job a scam or legit?\n\n  * What would be the grounds for the US Chamber of Commerce to sue the FTC over its new rule concerning noncompetes?\n\n  * Is a Shimano GRX FC-RX600 crankset compatible with Shimano Claris groupset?\n\n  * Is union of orthonormal bases orthonormal?\n\n  * Why do GCC and Clang pop on both branches instead of only once? (Factoring parts of the epilogue out of tail-duplication)\n\n  * Basic python rock papers scissors game (first code)\n\n  * My PhD supervisor is indicating that I should leave my PhD. What do you think?\n\n  * Is it a cartesian product?\n\n  * Where can I get an earth-centric map of space?\n\n  * How much latency is acceptable in an amplification system for singing?\n\n  * Book where the female main character cuts off her boyfriend mid-sentence to prove her point about the perceptions created by one's choice of words\n\n  * Why don't they use guns or lasers on Arrakis?\n\n  * What does it mean for a space to be a differentiable stack?\n\n  * Analogies for quantum properties\n\n  * \"as\" and the subject\n\n  * 74HC595 chip with 7 segment display constantly displays either all 1's or all 0's\n\n  * Is my simpler proof correct?\n\n  * Why does the BRK instruction set the B flag?\n\n  * Once a congressional bill has become law, how is it noticed by and overseen within the executive branch?\n\n  * Why do journals refuse to see substantially revised manuscripts after rejection?\n\nQuestion feed\n\n# Subscribe to RSS\n\n  * Blog\n  * Facebook\n  * Twitter\n  * LinkedIn\n  * Instagram\n\nSite design / logo \u00a9 2024 Stack Exchange Inc; user contributions licensed\nunder CC BY-SA. rev 2024.4.25.8269\n\n## We Care About Your Privacy\n\nWe and our 4 partners store and/or access information on a device, such as\nunique IDs in cookies to process personal data. You may accept or manage your\nchoices by clicking below, including your right to object where legitimate\ninterest is used, or at any time in the privacy policy page. These choices\nwill be signaled to our partners and will not affect browsing data.\n\nCookie Policy.\n\n### We and our partners perform the following based on your settings:\n\nUse precise geolocation data. Actively scan device characteristics for\nidentification. Understand audiences through statistics or combinations of\ndata from different sources. Store and/or access information on a device.\nDevelop and improve services. Create profiles to personalise content. Measure\ncontent performance. Use limited data to select content. Measure advertising\nperformance. Use limited data to select advertising. Create profiles for\npersonalised advertising. Use profiles to select personalised advertising. Use\nprofiles to select personalised content.\n\n", "frontpage": false}
