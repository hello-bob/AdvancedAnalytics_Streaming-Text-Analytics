{"aid": "40203294", "title": "StarCoder2-Instruct", "url": "https://huggingface.co/blog/sc2-instruct", "domain": "huggingface.co", "votes": 1, "user": "tosh", "posted_at": "2024-04-29 19:53:00", "comments": 0, "source_title": "StarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment for Code Generation", "source_text": "StarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment for Code\nGeneration\n\nHugging Face\n\nBack to Articles\n\n# StarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment for\nCode Generation\n\nPublished April 29, 2024\n\nUpdate on GitHub\n\nUpvote\n\n16\n\nyuxiang630 Yuxiang Wei\n\nguest\n\ncassanof Federico Cassano\n\nguest\n\nganler Jiawei Liu\n\nguest\n\nYifengDing Yifeng Ding\n\nguest\n\nStringChaos Naman Jain\n\nguest\n\nharmdevries Harm de Vries\n\nguest\n\nlvwerra Leandro von Werra\n\narjunguha Arjun Guha\n\nguest\n\nlingming Lingming Zhang\n\nguest\n\nInstruction tuning is an approach of fine-tuning that gives large language\nmodels (LLMs) the capability to follow natural and human-written instructions.\nHowever, for programming tasks, most models are tuned on either human-written\ninstructions (which are very expensive) or instructions generated by huge and\nproprietary LLMs (which may not be permitted). We introduce\nStarCoder2-15B-Instruct-v0.1, the very first entirely self-aligned code LLM\ntrained with a fully permissive and transparent pipeline. Our open-source\npipeline uses StarCoder2-15B to generate thousands of instruction-response\npairs, which are then used to fine-tune StarCoder-15B itself without any human\nannotations or distilled data from huge and proprietary LLMs.\n\nStarCoder2-15B-Instruct achieves a 72.6 HumanEval score, even surpassing the\n72.0 score of CodeLlama-70B-Instruct! Further evaluation on LiveCodeBench\nshows that the self-aligned model is even better than the same model trained\non data distilled from GPT-4, implying that an LLM could learn more\neffectively from data within its own distribution than a shifted distribution\nfrom a teacher LLM.\n\n## Method\n\nOur data generation pipeline mainly consists of three steps:\n\n  1. Extract high-quality and diverse seed functions from The Stack v1, a huge corpus of permissively licensed source code.\n  2. Create diverse and realistic code instructions that incorporate different code concepts present in the seed functions (e.g., data deserialization, list concatenation, and recursion).\n  3. For each instruction, generate a high-quality response through execution-guided self-validation.\n\nIn the following sections, we will explore each of these aspects in detail.\n\n### Collecting seed code snippets\n\nTo fully unlock the instruction-following capabilities of a code model, it\nshould be exposed to a diverse set of instructions encompassing a wide range\nof programming principles and practices. Motivated by OSS-Instruct, we further\npromote such diversity by mining code concepts from open-source code snippets\nthat are, specifically, well-formed seed Python functions from The Stack V1.\n\nFor our seed dataset, we carefully extract all Python functions with\ndocstrings in The Stack V1, infer dependencies required using autoimport, and\napply the following filtering rules on all functions:\n\n  1. Type checking: We apply the Pyright heuristic type-checker to remove all functions that produce static errors, signaling a possibly incorrect item.\n  2. Decontamination: We detect and remove all benchmark items on which we evaluate. We use exact string match on both the solutions and prompts.\n  3. Docstring Quality Filtering: We utilize StarCoder2-15B as a judge to remove functions with poor documentation. We prompt the base model with 7 few-shot examples, requiring it to respond with either \"Yes\" or \"No\" for retaining the item.\n  4. Near-Deduplication: We utilize MinHash and locality-sensitive hashing with a Jaccard similarity threshold of 0.5 to filter duplicate seed functions in our dataset. This is the same process applied to StarCoder\u2019s training data.\n\nThis filtering pipeline results in a dataset of 250k Python functions filtered\nfrom 5M functions with docstrings. This process is highly inspired by the data\ncollection pipeline used in MultiPL-T.\n\n### Self-OSS-Instruct\n\nAfter collecting the seed functions, we use Self-OSS-Instruct to generate\ndiverse instructions. In detail, we employ in-context learning to let the base\nStarCoder2-15B self-generate instructions from the given seed code snippets.\nThis process utilizes 16 carefully designed few-shot examples, each formatted\nas (snippet, concepts, instruction). The instruction generation procedure is\ndivided into two steps:\n\n  1. Concepts extraction: For each seed function, StarCoder2-15B is prompted to produce a list of code concepts present within the function. Code concepts refer to the foundational principles and techniques used in programming, such as pattern matching and data type conversion, which are crucial for developers to master.\n  2. Instruction generation: StarCoder2-15B is then prompted to self-generate a coding task that incorporates the identified code concepts.\n\nEventually, 238k instructions are generated from this process.\n\n### Response self-validation\n\nGiven the instructions generated from Self-OSS-Instruct, our next step is to\nmatch each instruction with a high-quality response. Prior practices commonly\nrely on distilling responses from stronger teacher models, such as GPT-4,\nwhich hopefully exhibit higher quality. However, distilling proprietary models\nleads to non-permissive licensing and a stronger teacher model might not\nalways be available. More importantly, teacher models can be wrong as well,\nand the distribution gap between teacher and student can be detrimental.\n\nWe propose to self-align StarCoder2-15B by explicitly instructing the model to\ngenerate tests for self-validation after it produces a response interleaved\nwith natural language. This process is similar to how developers test their\ncode implementations. Specifically, for each instruction, StarCoder2-15B\ngenerates 10 samples of the format (NL Response, Test) and we filter out those\nfalsified by the test execution under a sandbox environment. We then randomly\nselect one passing response per instruction to the final SFT dataset. In\ntotal, we generated 2.4M (10 x 238k) responses for the 238k instructions with\ntemperature 0.7, where 500k passed the execution test. After deduplication, we\nare left with 50k instructions, each paired with a random passing response,\nwhich we finally use as our SFT dataset.\n\n## Evaluation\n\nOn the popular and rigorous EvalPlus benchmark, StarCoder2-15B-Instruct stands\nout as the top-performing permissive LLM at its scale, outperforming the much\nlarger Grok-1 Command-R+, DBRX, while closely matching Snowflake Arctic 480B\nand Mixtral-8x22B-Instruct. To our knowledge, StarCoder2-15B-Instruct is the\nfirst code LLM with a fully transparent and permissive pipeline reaching a 70+\nHumanEval score. It drastically outperforms OctoCoder, which is the previous\nstate-of-the-art permissive code LLM with a transparent pipeline.\n\nEven compared to powerful LLMs with restrictive licenses,\nStarCoder2-15B-Instruct remains competitive, surpassing Gemini Pro and Mistral\nLarge and comparable to CodeLlama-70B-Instruct. Additionally,\nStarCoder2-15B-Instruct, trained purely on self-generated data, closely rivals\nOpenCodeInterpreter-SC2-15B, which finetunes StarCoder2-15B on distilled data\nfrom GPT-3.5/4.\n\nBesides EvalPlus, we also evaluated state-of-the-art open-source models with\nsimilar or smaller sizes on LiveCodeBench, which includes fresh coding\nproblems created after 2023-09-01, as well as DS-1000 that targets data\nscience programs. On LiveCodeBench, StarCoder2-15B-Instruct achieves the best\nresults among the models evaluated and consistently outperforms\nOpenCodeInterpreter-SC2-15B which distills GPT-4 data. On DS-1000, the\nStarCoder2-15B-Instruct is still competitive despite being trained on very\nlimited data science problems.\n\n## Conclusion\n\nStarCoder2-15B-Instruct-v0.1 showcases for the first time that we can create\npowerful instruction-tuned code models without relying on stronger teacher\nmodels like GPT-4. This model demonstrates that self-alignment, where a model\nuses its own generated content to learn, is also effective for code. It is\nfully transparent and allows for distillation, setting it apart from other\nlarger permissive but non-transparent models such as Snowflake-Arctic, Grok-1,\nMixtral-8x22B, DBRX, and CommandR+. We have made our datasets and the entire\npipeline, including data curation and training, fully open-source. We hope\nthis seminal work can inspire more future research and development in this\nfield.\n\n### Resources\n\n  * StarCoder2-15B-Instruct-v0.1: the instruction-tuned model\n  * starcoder2-self-align: the self-alignment pipeline\n  * StarCoder2-Self-OSS-Instruct: the self-generated, instruction-tuning dataset\n\nMore Articles from our Blog\n\n## Welcome Llama 3 - Meta's new open LLM\n\nBy April 18, 2024 \u2022 224\n\n## Introducing Idefics2: A Powerful 8B Vision-Language Model for the community\n\nBy April 15, 2024 \u2022 88\n\nUpvote\n\n16\n\n", "frontpage": false}
