{"aid": "40065255", "title": "Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs", "url": "https://huggingface.co/papers/2404.05719", "domain": "huggingface.co", "votes": 1, "user": "bookofjoe", "posted_at": "2024-04-17 14:35:42", "comments": 0, "source_title": "Paper page - Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs", "source_text": "Paper page - Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs\n\nHugging Face\n\nPapers\n\narxiv:2404.05719\n\n# Ferret-UI: Grounded Mobile UI Understanding with Multimodal LLMs\n\nPublished on Apr 8\n\n\u00b7 Featured in Daily Papers on Apr 9\n\nUpvote\n\n52\n\nAuthors:\n\n,\n\nHaotian Zhang ,\n\n,\n\nFloris Weers ,\n\n,\n\n,\n\n,\n\n## Abstract\n\nRecent advancements in multimodal large language models (MLLMs) have been\nnoteworthy, yet, these general-domain MLLMs often fall short in their ability\nto comprehend and interact effectively with user interface (UI) screens. In\nthis paper, we present Ferret-UI, a new MLLM tailored for enhanced\nunderstanding of mobile UI screens, equipped with referring, grounding, and\nreasoning capabilities. Given that UI screens typically exhibit a more\nelongated aspect ratio and contain smaller objects of interest (e.g., icons,\ntexts) than natural images, we incorporate \"any resolution\" on top of Ferret\nto magnify details and leverage enhanced visual features. Specifically, each\nscreen is divided into 2 sub-images based on the original aspect ratio (i.e.,\nhorizontal division for portrait screens and vertical division for landscape\nscreens). Both sub-images are encoded separately before being sent to LLMs. We\nmeticulously gather training samples from an extensive range of elementary UI\ntasks, such as icon recognition, find text, and widget listing. These samples\nare formatted for instruction-following with region annotations to facilitate\nprecise referring and grounding. To augment the model's reasoning ability, we\nfurther compile a dataset for advanced tasks, including detailed description,\nperception/interaction conversations, and function inference. After training\non the curated datasets, Ferret-UI exhibits outstanding comprehension of UI\nscreens and the capability to execute open-ended instructions. For model\nevaluation, we establish a comprehensive benchmark encompassing all the\naforementioned tasks. Ferret-UI excels not only beyond most open-source UI\nMLLMs, but also surpasses GPT-4V on all the elementary UI tasks.\n\nView arXiv page View PDF Add to collection\n\n### Community\n\nJustfly50\n\n8 days ago\n\nThis comment has been hidden\n\nJustfly50\n\n8 days ago\n\nThis comment has been hidden\n\n\u00b7 Sign up or log in to comment\n\nUpvote\n\n52\n\n## Models citing this paper 0\n\nNo model linking this paper\n\nCite arxiv.org/abs/2404.05719 in a model README.md to link it from this page.\n\n## Datasets citing this paper 0\n\nNo dataset linking this paper\n\nCite arxiv.org/abs/2404.05719 in a dataset README.md to link it from this\npage.\n\n### Spaces citing this paper 0\n\nNo Space linking this paper\n\nCite arxiv.org/abs/2404.05719 in a Space README.md to link it from this page.\n\n## Collections including this paper 16\n\n", "frontpage": false}
