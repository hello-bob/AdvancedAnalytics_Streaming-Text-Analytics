{"aid": "40071843", "title": "Implementing Natural Conversational Agents with Elixir", "url": "https://seanmoriarity.com/2024/02/25/implementing-natural-conversational-agents-with-elixir/", "domain": "seanmoriarity.com", "votes": 3, "user": "ac_alejos", "posted_at": "2024-04-18 00:53:05", "comments": 0, "source_title": "Implementing Natural Conversational Agents with Elixir", "source_text": "Implementing Natural Conversational Agents with Elixir \u2013 Sean Moriarity\n\nSkip to content\n\nSean Moriarity\n\nAI, Math, Programming\n\n# Implementing Natural Conversational Agents with Elixir\n\nseanmoriarity Uncategorized February 25, 2024 24 Minutes\n\nIn my last post, I discussed some work I had done building Nero, the assistant\nof the future that I\u2019ve always wanted. I ended up creating an end-to-end\nexample which used Nx, OpenAI APIs, and ElevenLabs to create an in-browser\nhome automation assistant. For a first product, it\u2019s decent. Nero is a neat\nlittle party trick that I can use to impress my non-tech friends. I am,\nhowever, not in this business to impress my friends. I want Nero to actually\nhelp me and actually feel like an assistant. My previous version is not that.\n\nOne missing piece is the ability to converse naturally without browser\ninteraction. The first implementation of Nero\u2019s \u201cconversational\u201d abilities\nrelied on user interaction with the screen every time we wanted to initiate a\nresponse or action. Nero also did not retain any conversational history. In\nshort, Nero was not a great conversational assistant. It was one of the things\nI wanted to fix; however, I was motivated to do it sooner rather than later\nafter watching an impressive demo from Retell.\n\nThe Retell demo implements a conversational agent backed by their WebSocket\nAPI in a browser. The demonstration has:\n\n  * \u201cAlways on\u201d recording\n  * Low latency\n  * Support for interrupts\n  * Impressive filtering (e.g. snapping and other non-voice activity doesn\u2019t seem to throw off the agent)\n\nTheir documentation suggests they also have support for backchanneling and\nintelligent end of turn detection\u2014two things that are essential to natural\nconversational feel but which are very difficult to express programmatically.\n\nI had previously convinced myself that I could implement a passable\nconversational agent experience in a short amount of time. So that is what I\nset out to do.\n\n## Always On Recording\n\nThe first thing that needed to change about Nero\u2019s design was the speech to\ntext pipeline. My original demonstration relied on an example from Bumblebee\nwhich implemented a speech to text pipeline using Whisper. The pipeline uses\nmouse events in a Phoenix LiveView Hook to start and stop recordings before\nsending them to the server to initiate transcription. If you\u2019re not familiar,\nPhoenix LiveView is a server-side rendering framework built on top of Elixir.\nLiveView has support for client-side JavaScript hooks which support\nbidirectional communication between client and server.\n\nThe original speech to text implementation used a hook with an event listener\nattached to mousedown and mouseup on a button to start and stop recording.\nAfter recording stops, the hook decodes the recorded buffer into a PCM buffer,\nconverts the endianness, and then pushes the buffer to the server with an\nupload. The original hook implements most of the functionality we want;\nhowever, we need to make some minor tweaks. Rather than trigger recordings to\nstop and start on mouse events, we want to trigger recordings to start and\nstop exactly when a person starts and stops speaking. Simple, right?\n\nMy first idea in implementing what I called \u201calways on recording\u201d was to\nmonitor the microphone\u2019s volume, and trigger a recording when the volume\nreached a certain threshold. The recording would stop when the volume dipped\nbelow that threshold. At this point, I learned about getUserMedia.\ngetUserMedia prompts the user for permission to access media devices such as a\nmicrophone and/or webcam, and then produces a MediaStream. A MediaStream is a\nstream of media content containing information about audio and video tracks in\nthe stream. We can use data from the MediaStream to determine speaker activity\nand thus trigger recordings.\n\nTo determine the volume for a given sample, we can use an AnalyserNode. Per\nthe documentation AnyalyserNode is designed for processing generated audio\ndata for visualization purposes, but we can use it to determine spikes in\naudio:\n\n123456789101112131415161718192021222324252627282930313233|\nnavigator.mediaDevices.getUserMedia({ audio: true }).then((stream) =>\n{this.stream = stream;const analyser =\nthis.audioContext.createAnalyser();const microphone =\nthis.audioContext.createMediaStreamSource(this.stream);microphone.connect(analyser);analyser.fftSize\n= 512;const bufferLength = analyser.frequencyBinCount;const dataArray = new\nUint8Array(bufferLength);const checkAudio = () =>\n{analyser.getByteTimeDomainData(dataArray);let sum = 0;for (let i = 0; i <\nbufferLength; i++) {sum += (dataArray[i] - 128) * (dataArray[i] - 128);}const\nvolume = Math.sqrt(sum / bufferLength);if (volume > VOLUME_THRESHOLD &&\n!this.isRecording) {this.startRecording();} else if (this.isRecording())\n{this.stopRecording();}if (this.isMonitoring)\n{requestAnimationFrame(checkAudio);}}checkAudio();})  \n---|---  \n  \nThis uses an analyser and repeatedly checks if the volume of the microphone at\na given frame exceeds the given VOLUME_THRESHOLD. If it does, it checks to see\nif we are recording and if not, starts the recording.\n\nAfter testing a bit, I realized this implementation sucked. Of the many issues\nwith this approach, the biggest is that there are many natural dips in a\nperson\u2019s volume. Checking a single frame doesn\u2019t account for these natural\ndips. To fix this, I thought it would be a good idea to introduce a timeout\nwhich only stopped recording after the volume was below a threshold for a\ncertain amount of time:\n\n12345678910111213141516171819202122232425262728293031323334353637383940414243|\nnavigator.mediaDevices.getUserMedia({ audio: true }).then((stream) =>\n{this.stream = stream;const analyser =\nthis.audioContext.createAnalyser();const microphone =\nthis.audioContext.createMediaStreamSource(this.stream);microphone.connect(analyser);analyser.fftSize\n= 512;const bufferLength = analyser.frequencyBinCount;const dataArray = new\nUint8Array(bufferLength);this.silenceTimeout = null;const checkAudio = () =>\n{analyser.getByteTimeDomainData(dataArray);let sum = 0;for (let i = 0; i <\nbufferLength; i++) {sum += (dataArray[i] - 128) * (dataArray[i] - 128);}const\nvolume = Math.sqrt(sum / bufferLength);if (volume > VOLUME_THRESHOLD) {if\n(!this.isRecording())\n{this.startRecording();}clearTimeout(this.silenceTimeout);this.silenceTimeout\n= setTimeout(() => {if (this.isRecording()) {this.stopRecording();}},\nSILENCE_TIMEOUT);}if (this.isMonitoring)\n{requestAnimationFrame(checkAudio);}}checkAudio();})  \n---|---  \n  \nThis actually ended up working decent, but required tuning hyperparameters for\nboth VOLUME_THRESHOLD and SILENCE_TIMEOUT. The challenge here is that higher\nSILENCE_TIMEOUT introduces additionally latency in transition time between a\nspeaker and Nero; however, lower timeouts might be too sensitive to speakers\nwith slower and quieter speaking rhythms. Additionally, a static\nVOLUME_THRESHOLD does not account for ambient noise. Now, despite these\nshortcomings, I found I was able to passably detect a single speaker in a\nquiet room.\n\nAfter hooking this up to my existing LiveView and trying some end-to-end\nconversations, I realized something was significantly off. The transcriptions\nI was getting were off. I soon realized that they were always off at the\nbeginning of a transcription. Shorter audio sequences were especially\naffected. It turns out that the detection algorithm always resulted in some\namount of truncation at the beginning of an audio clip. When a speaker starts\ntalking, their volume ramps up \u2013 it\u2019s not an instantaneous spike. To account\nfor this, I introduced a pre-recording buffer which always tracked the\nprevious 150ms of audio. After recording started, I would stop the pre-\nrecording buffer and start the actual recording, and then eventually splice\nthese 2 together to send to the server for transcription.\n\nOverall, this actually worked okay. While there are some obvious failure\nmodes, it worked well enough to get a passable demonstration. If you can\u2019t\ntell by now, I am not an audio engineer. I learned later that this is a very\nnaive attempt at voice activity detection. Later on in this post, I\u2019ll run\nthrough some of the improvements I made based on my research into the field of\nVAD.\n\n## End-to-End Implementation\n\nThe demonstration I built for Nero in my first post already contained the\nscaffolding for an end-to-end transcription -> response -> speech pipeline. I\nonly needed to make some slight modifications to get the phone call demo to\nwork. The end-to-end the pipeline looks like this:\n\nWhen our algorithm detects that speech has stopped, it invokes the\nstopRecording method. stopRecording takes the recorded audio, does some\nclient-side pre-processing, and uploads it to the server. The server consumes\nthe uploaded entry as a part of LiveView\u2019s normal uploads lifecycle and then\ninvokes an async task to start transcription:\n\n123456789101112131415| defp handle_progress(:audio, entry, socket) when\nentry.done? dobinary =consume_uploaded_entry(socket, entry, fn %{path: path}\n->{:ok, File.read!(path)}end)audio = Nx.from_binary(binary, :f32)socket\n=start_async(socket, :transcription, fn\n->Nero.SpeechToText.transcribe(audio)end){:noreply, socket}end  \n---|---  \n  \nNote that because we did most of the pre-processing client-side, we can just\nconsume the audio binary as an Nx.Tensor, without any additional work. The\nSpeechToText module implements transcription using Nx.Serving:\n\n12345678910111213141516171819202122| defmodule Nero.SpeechToText do@repo\n\"distil-whisper/distil-medium.en\"def serving() do{:ok, model_info} =\nBumblebee.load_model({:hf, @repo}){:ok, featurizer} =\nBumblebee.load_featurizer({:hf, @repo}){:ok, tokenizer} =\nBumblebee.load_tokenizer({:hf, @repo}){:ok, generation_config} =\nBumblebee.load_generation_config({:hf,\n@repo})Bumblebee.Audio.speech_to_text_whisper(model_info, featurizer,\ntokenizer, generation_config,task: nil,compile: [batch_size: 1],defn_options:\n[compiler: EXLA])enddef transcribe(audio) dooutput =\nNx.Serving.batched_run(__MODULE__, audio)output.chunks |> Enum.map_join(&\n&1.text) |> String.trim()endend  \n---|---  \n  \nNx.Serving is an abstraction in the Elixir Nx ecosystem for serving machine\nlearning models directly in an Elixir application. It implements dynamic\nbatching, encapsulates pre-processing, inference, and post-processing,\nsupports distribution and load-balancing between multiple GPUs natively, and\nin general is an extremely easy way to serve machine learning models.\n\nAfter transcription completes, we get an async result we can handle to\ninitiate a response:\n\n123456789| def handle_async(:transcription, {:ok, transcription}, socket)\ndochat = socket.assigns.chat ++ [%{role: \"user\", content:\ntranscription}]response = Nero.Agent.respond(chat){:noreply,socket|>\nassign(chat: chat)|> speak(response)}end  \n---|---  \n  \nHere Nero.Agent.respond/1 returns an Elixir Stream of text. For my original\ndemonstration I just used the Elixir OpenAI library to produce a stream from a\nGPT-3.5 response:\n\n123456789101112131415| def respond(chat) doprompt = Nero.Prompts.response()response_stream =OpenAI.chat_completion(model: \"gpt-3.5-turbo\",messages: [%{role: \"system\", content: prompt} | chat],max_tokens: 400,stream: true)response_stream|> Stream.map(&get_in(&1, [\"choices\", Access.at(0), \"delta\", \"content\"]))|> Stream.reject(&is_nil/1)end  \n---|---  \n  \nThe response stream is consumed by speak/2. speak/2 implements the text to\nspeech pipeline:\n\n12345| defp speak(socket, text) dostart_async(socket, :speak, fn\n->Nero.TextToSpeech.stream(text)end)end  \n---|---  \n  \nWhere Nero.TextToSpeech.stream/1 uses the ElevenLabs WebSocket API to stream\ntext in and speech out. You can read a bit more about the implementation in my\nprevious post.\n\nNero.TextToSpeech.stream/1 returns the consumed response as text so we can\nappend that to the chat history after the :speak task finishes:\n\n1234| def handle_async(:speak, {:ok, response}, socket) dochat =\nsocket.assigns.chat ++ [%{role: \"assistant\", content: response}]{:noreply,\nassign(socket, :chat, chat)}end  \n---|---  \n  \nThis is basically all of the scaffolding needed for an end-to-end demo, but I\nwanted to add a few more features. First, I wanted to support \u201cintelligent\u201d\nhang-ups. Basically, I wanted to be able to detect when a conversation was\nfinished, and stop the recording. To do that, I used Instructor:\n\n1234567891011121314151617| def hang_up?(chat) do{:ok, %{hang_up: hang_up}}\n=Instructor.chat_completion(model: \"gpt-3.5-turbo\",messages: [%{role:\n\"system\",content:\"Decide whether or not to hang up the phone given this\ntranscript. You should hang up after the user says goodbye or that there's\nnothing else you can help them with. DO NOT HANG UP ON THE USER UNLESS THEY\nSAY GOODBYE.\"}| chat],response_model: %{hang_up: :boolean})hang_upend  \n---|---  \n  \nPlease ignore my wonderfully engineered prompt. This uses GPT-3.5 to determine\nwhether or not a given conversation has ended. After every one of Nero\u2019s\nturns, we check the transcript to possibly end the call:\n\n123456789101112| def handle_async(:speak, {:ok, response}, socket) dochat =\nsocket.assigns.chat ++ [%{role: \"assistant\", content: response}]socket =if\nNero.Agent.hang_up?(chat) dopush_event(socket, \"hang_up\",\n%{})elsesocketend{:noreply, assign(socket, :chat, chat)}end  \n---|---  \n  \nThis pushes a hang_up event to the socket:\n\n123456789101112| this.handleEvent('hang_up', () =>\n{hook.pushEvent(\"toggle_conversation\");if (this.audioContext)\n{this.audioContext.close();this.audioContext = null;}if (this.isMonitoring)\n{this.stopMonitoring();}});  \n---|---  \n  \nWhich stops the recording, and then pushes an event to toggle_conversation\nback to the server. toggle_conversation implements the start/stop logic from\nthe server:\n\n12345678910| def handle_event(\"toggle_conversation\", _params, socket) dosocket\n=if socket.assigns.conversing\ndostop_conversation(socket)elsestart_conversation(socket)end{:noreply,\nsocket}end  \n---|---  \n  \nFinally, I wanted to implement information extraction from the transcript.\nAgain, I used instructor and defined an extraction schema:\n\n123456789101112| defmodule Appointment douse Ecto.Schemaembedded_schema\ndofield :booked, :booleanfield :date, :stringfield :time, :stringfield :name,\n:stringfield :phone_number, :stringfield :reason_for_visit, :stringendend  \n---|---  \n  \nAnd used GPT-3.5 with a rough prompt to get the necessary information from the\ntranscript:\n\n1234567891011121314| def extract_appointment(chat)\ndoInstructor.chat_completion(model: \"gpt-3.5-turbo\",messages: [%{role:\n\"system\",content:\"Extract appointemnt information from the transcript. If info\nis missing, leave it blank. If it seems like no appointment was booked, mark\nbooked as false and leave everything else blank. An appointment is not booked\nif there's no established date.\"}| chat],response_model: Appointment)end  \n---|---  \n  \nAnd then anytime a conversation ends, we attempt to retrieve appointment\ninformation:\n\n12345678910111213141516| defp stop_conversation(socket) docase\nNero.Agent.extract_appointment(socket.assigns.chat) do{:ok, %{booked: true} =\nappointment} ->assign(socket,message: \"You made an appointment!\nDetails:\",appointment: appointment,conversing: false)_\n->assign(socket,message: \"Looks like you didn't actually book an appointment.\nTry again\",conversing: false)endend  \n---|---  \n  \nNow this is essentially the exact implementation that produced this\ndemonstration. End-to-end this amounted to a couple of hours of work; however,\nI already had most of the basic scaffold implemented from my previous work on\nNero. In my biased opinion, I think my demo is pretty good, but as others have\npointed out Retell\u2019s demo kicks my ass in:\n\n  * Latency\n  * Reliability\n  * Natural sounding voice\n\nAnd so, I set out to improve my implementation \u2013 starting with latency.\n\n## Reducing Latency\n\nHuman conversations have extremely tight \u201ctime-to-turn.\u201d In-person\nconversations are especially rapid because we rely on visual as well as audio\nsignals to determine when it\u2019s our time to participate in a conversation. The\n\u201caverage\u201d time-to-turn in a conversation can be as quick as 200ms. That means\nfor a conversational agent to feel realistic, it needs an extremely quick turn\naround time for \u201ctime to first spoken word.\u201d\n\nAfter posting my original demonstration, I already knew there were some very\neasy optimizations I could make, so I set out to improve the average latency\nof my implementation as much as possible in a short amount of time. First, I\nneeded at least some method for determining whether an optimization worked. My\nrudimentary approach was to use JavaScript Performance Timers and logging.\nBasically, I computed a startTime from the exact moment an audio recording\nstopped and an endTime from the exact moment an audio output started, and then\nI logged that time to the console.\n\nThis is a very unscientific way of doing business. In the future, I\u2019d like to\nimplement a much more involved profiling and benchmarking methodology. For\nthis process though, it worked well enough.\n\nNext, I considered all of the areas that could introduce latency into the\npipeline. From the moment a recording stops, these are all of the steps we\ntake:\n\n  1. Pre-process recording by converting to PCM buffer, and then converting endianness to match server (if necessary)\n  2. Upload buffer to server\n  3. Perform speech to text on buffer to produce text\n  4. Send text to LLM\n  5. Send streamed text to ElevenLabs\n  6. Receive streamed audio from ElevenLabs\n  7. Broadcast audio to client\n  8. Decode audio on client and play\n\nThat\u2019s a lot of steps that can introduce latency, including potentially 3 (in\nour case 2 because we own the STT pipeline) network calls.\n\nNext, I wanted to esablish a \u201cbaseline\u201d of performance. To demonstrate this\niterative process, I did a baseline example on my M3 Mac CPU. Note that this\nis going to be slow relative to my previous demo because the previous demo\nruns on a GPU. The baseline performance I got from the original demo running\non my mac was 4537 ms. 4.5 seconds turn around time. Yikes. Lots of work to\ndo.\n\nTo start, I knew that the SILENCE_TIMEOUT used to wait for speech to end was\nrather long. For the original demo, I used 1000 ms, which basically means a\nspeaker has to stop talking for a full second before we\u2019ll even start the long\nresponse process. After some trial and error, I figured 500 ms was a\n\u201cpassable\u201d hyperparameter. After adjusting this down, the latency change was\nalmost exactly correlated to the dip: 4079 ms.\n\nI had a hunch that my text to speech pipeline was not efficient. Fortunately,\nElevenLabs gives us a nice Latency Guide. The first suggestion is to use their\nturbo model by specifying eleven_turbo_v2. I set that and we got a slight\nperformance boost: 4014 ms.\n\nNext, they suggest adding optimize_streaming_latency. I set the value to 3 and\nwe get: 3791 ms. Their next suggestion is to use a pre-made voice. I actually\ndidn\u2019t realize until much later that I was not using a pre-made voice so I\ndon\u2019t have a comparison for how that change impacted latency.\n\nNow it says to limit closing WebSocket connections. my current implementation\nopens a connection everytime it speaks \u2013 which is not good. Basically every\n\u201cturn\u201d has to establish a new websocket connection. Additionally, ElevenLabs\nhas a timeout of 20s from when you connect. So you need to send a message at\nleast every 20s. I considered 2 options at this point:\n\n  1. Open a global WebSocket connection, or maybe even a pool of connections, and try to keep the connection alive. But that seems really wasteful, and I don\u2019t think is the intended use of their API\n  2. Open a WebSocket connection when convo starts. We don\u2019t have to worry about 20s pauses\n\nI decided to go with option 2, but I still think there are some drawbacks and\nconsiderations for a production system. The implementation I used opens a\nwebsocket connection on first \u201cspeak\u201d and stores the connection PID as an\nassign in the LiveView socket. If you have a system with potentially many\nconcurrent users speaking, you run the risk of creating a potentially\nunbounded number of connections. A more robust solution would probably use\nconnection pools; however, I\u2019m not really worried about traffic or scaling\nhere.\n\nWhile adding this optimization, I struggled a bit because ElevenLabs would\nsend the first frame back, then cut off. Then I realized that it was waiting\nto generate becuase it thought I was going to send more frames. So I needed to\n\u201cflush\u201d the generation after I finished sending my tokens. This also seemed to\nfix unnatural audio problems I was having. After applying this optimization,\nour time to first spoken word was slightly lower in the 3700 ms range.\n\nAfter perusing their docs a bit more, I learned that ElevenLabs will send PCM\nbuffers instead of MP3. Web Browser\u2019s have to decode MP3 to PCM, which\npotentially introduces some overhead. One drawback is that you need to be on\nthe independent creator tier to receive PCM instead of MP3. Now, if you\u2019re\nwondering if I spent $99 to save some milliseconds for a meaningless demo, the\nanswer is absolutely yes I did.\n\nAt this point, I believe I\u2019ve exhausted a lot of the \u201ceasy\u201d optimizations for\nTTS latency. One thing that does bother me about the ElevenLabs Websocket API\nis that there\u2019s no way to receive binary payloads instead of JSON payloads.\nThis is probably because they send alignment data, but I\u2019m not using the\nalignment data here. When handling an incoming frame from their API we have to\nfirst decode the JSON, and then decode the Base64 encoded audio buffer. I\u2019m\nnot sure what the latency impact is, but I\u2019m sure we could shave some time by\navoiding both of these conversions. I also think the Base64 representation\nresults in slightly larger buffers which could impact network latency.\n\nThe next area I looked to improve was the speech-to-text pipeline. I am using\nNx.Serving specifically for Speech-to-Text. The benefit of this approach is\nthat we can avoid an additional network call just for transcription. Of\ncourse, that assumes our transcription pipeline can run fast enough on our own\nhardware. XLA is notoriously slow on CPUs (it\u2019s getting better). The first\n\u201coptimization\u201d I did was to switch to my GPU: 2050 ms\n\nAnd that right there is a bitter lesson, because it\u2019s the largest performance\nboost we\u2019re going to get.\n\nNext, I realized the model isn\u2019t using F16, which can introduce some solid\nspeed-ups: 1800 ms. Now, there are probably some additional optimizations we\ncould add to Nx and EXLA specifically. For example, we don\u2019t have a flash\nattention implementation. Of course, XLA does a great job of applying similar\noptimizations as a baseline, so I\u2019m not sure how much it would help. There\u2019s\nalso fast JAX implementations of Whisper that claim up to 70x speed ups. One\nissue with a lof of these claimed speed-ups; however, is that they are almost\nalways for long audio sequences. GPUs and TPUs do well with large batch sizes\nand sequence lengths, but not for batch size 1 and short sequence lengths like\nwe care about in this implementation. One day I may go down the performance\nhole of fast batch size 1 transcription, but today is not that day.\n\nAt this point, I had moved on to improving some of the failure modes of my\ndemo. While doing so, I learned much more about audio than I had previously\nknown, and realized that the configuration I used to record audio can\nsignificantly improve whisper performance as well. Turns out there\u2019s a nice\nguide of somebody discussing parameters that work. Specifically, you should\nuse 16 kHz sample rate for transcriptions. Reducing the sample rate also\nshould reduce network overhead because we have less data, but it could reduce\nquality of the transcription. Oh well. Additionally, I realized I wasn\u2019t using\na pre-made ElevenLabs voice. After introducing both of these optimizations, I\nwas able to achieve 1520 ms turn time.\n\nFinally, I realized I was doing all of my benchmarks on a development server.\nI switched my phoenix environment from dev to prod and got: 1375 ms. So, with\nall of these optimizations we\u2019re sitting at about 1.3s turn around time in a\nconversation. When conversing, it starts to feel somewhat close to natural. I\nshould also point out that this is also running over Tailscale, so there is\nabout 100 ms ping between my Mac and the server running on my GPU. When I run\nthis locally on my GPU, I can consistently get about 1000 ms and sometimes 900\nms turn around time. Still, unfortunately, this does not match Retell\u2019s\nlatency. According to them, they are able to achieve 800 ms consistently. I\nhave some musings at the end about how this is possible.\n\nI believe the biggest area I could improve the implementation is to use a\nbetter VAD implementation that relies on small rolling windows of activity\nrather than frames. We could probably get away with using 20-30 ms windows,\nwhich could theoretically offer a 480 ms latency improvement. I would like to\neventually explore this.\n\nIn all honesty though, I think that is a significant improvement, and I could\nprobably stop right here and be done with it.\n\nIf I were to keep going, I would explore using a local LLM with Nx and\nBumblebee. Nx and Bumblebee support LLMs like Mistral and Llama out-of-the\nbox. And our text generation servings support streaming. That means we can\npossibly eliminate any network latency to OpenAI, and instead run 2 of the 3\nmodels locally. One issue with this is that Nx currently does not have any\nquantized inference support (it\u2019s coming I promise), so my single 4090 is not\nsufficient to deploy both Whisper and Mistral. Fortunately, the folks at\nFly.io were kind enough to give me access to some 80GB A100s. I will post a\ndemo when I get one deployed\n\nMaybe one day I will implement StyleTTS2 and see how efficient we can get with\nan entirely local inference pipeline.\n\n## Improving the Conversational Experience\n\nSome people pointed out that my original demo did not have the same\nconversational experience as Retell\u2019s, and they are absolutely right. Aside\nfrom latency, mine was prone to failure, picks up system sounds, picks up\nrandom noises like keyboard and mouse clicks, and doesn\u2019t do well with ambient\nnoise. They also have support for backchanneling, fillers and interruptions\nwhich introduces some element of \u201crealness\u201d when interacting with their agent.\n\nNow I didn\u2019t get around to adding backchannels or fillers, but I was able to\nmake some slight improvements to the VAD algorithm I used, and I added support\nfor interruptions.\n\n### Fixing Some Failure Modes with VAD\n\nThe first failure mode that seems to happen is echo from the system sounds.\nNero always records and will start transcribing after audio spikes over a\ncertain threshold. After some digging into the getUserMedia API, I found\noptions for echoCancellation, noiseSuppression, and autoGainControl. This is\nthe same point I realized that I could specify the microphone sample rate for\nthe optimization I could added from the last section. Most of these options\nare on by default depending on your browser, but I added them explicitly\nanyway:\n\n1234567891011| const audioOptions = {sampleRate:\nSAMPLING_RATE,echoCancellation: true,noiseSuppression: true,autoGainControl:\ntrue,channelCount: 1,};navigator.mediaDevices.getUserMedia({ audio:\naudioOptions }).then((stream) => {...}  \n---|---  \n  \nNow that somewhat helped, but Nero still picks up it\u2019s own audio. This\nprobably requires a more sophisticated solution, but I moved on to the next\nproblem.\n\nThe second obvious failure mode is the fact that it picks up keyboard clicks,\nand the silence timeout is hard to tune. My first attempt to fix this was to\n\u201cignore\u201d large spikes in audio by \u201csmoothing\u201d the volume at each frame:\n\n12345678| let sum = 0;for (let i = 0; i < bufferLength; i++) {sum +=\n(dataArray[i] - 128) * (dataArray[i] - 128);}const volume = Math.sqrt(sum /\nbufferLength);const smoothedVolume = SMOOTHING_ALPHA * volume + (1 -\nSMOOTHING_ALPHA) * lastSmoothedVolume;lastSmoothedVolume = smoothedVolume;  \n---|---  \n  \nThen, with some advice from Paulo Valente, I implemented a biquad filter to\nwith a low and high-pass in order to filter audio to the range of human\nspeech:\n\n123456789101112131415| this.stream = stream;const analyser =\nthis.audioContext.createAnalyser();const microphone =\nthis.audioContext.createMediaStreamSource(this.stream);var highPassFilter =\nthis.audioContext.createBiquadFilter();highPassFilter.type =\n'highpass';highPassFilter.frequency.value = FILTER_LOWER_BOUND;var\nlowPassFilter = this.audioContext.createBiquadFilter();lowPassFilter.type =\n'lowpass';lowPassFilter.frequency.value =\nFILTER_UPPER_BOUND;microphone.connect(highPassFilter);highPassFilter.connect(lowPassFilter);lowPassFilter.connect(analyser);  \n---|---  \n  \nIn practice, both of these solutions actually seemed to work decent, but they\ncould absolutely be better. I know it\u2019s possible to improve the client-side\nfiltering using a rolling-window that looks energy of the speaking frequences\nrelative to energy of an entire sample. But, there are also machine learning\nmodels that perform VAD, and have 1ms inference times. I realized that it\u2019s\nprobably quicker to just send all of the data over the websocket in chunks,\nand perform VAD on the server. I\u2019ll discuss that implementation a little\nlater.\n\n### Supporting Interruptions\n\nNext I wanted to add support for interruptions. In the Retell example, the\nspeaker will cut off mid-speech if it detects that you are speaking. To\nimplement this feature in Nero, I added a pushEvent to the Microphone hook\nwhich would push an interrupt event to the server anytime speech is\ndetectected:\n\n123456789101112| if (smoothedVolume > VOLUME_THRESHOLD) {if\n(!this.isRecording()) {// To handle interrupts, push an event to the LV which\nwill// then push an event to the TTS channel. Not sure how much// these round\ntrips will lag. Alternatively we could create// a global audio context and\nstop that, but we would need// a different way to push alignment info to the\nserverthis.pushEvent(\"interrupt\");this.startRecording();}...}  \n---|---  \n  \nThe server handles this event and broadcasts an event to the TTS channel to\nstop speaking:\n\n12345678910| def handle_event(\"interrupt\", _params, socket)\ndoNeroWeb.Endpoint.broadcast_from(self(),socket.assigns.audio_channel,\"phx:audio-\nstop\",%{}){:noreply, socket}end  \n---|---  \n  \nAnd the channel handles the event by clearing out the output audio stream and\nqueue:\n\n12345678910111213| this.channel.on(\"phx:audio-stop\", () => {if\n(hook.audioContext.state === 'running') {hook.audioContext.suspend().then(()\n=> {if (hook.source) {hook.source.onended =\nnull;hook.source.stop();hook.source = null;}hook.isPlaying =\nfalse;hook.audioQueue = [];});}});  \n---|---  \n  \nUnfortunately, this does create a race condition. There\u2019s a potential\nsituation where a speaker interrupts and the speaking queue gets cleared on\nthe client, but ElevenLabs is still streaming audio back to the server. The\nserver is always going to just broadcast this info to the client, and as is\nthe client will process it. This potentially creates a situation with weird\ncontinutations in the audio. To get around this, I refactored the TTS\nimplementation so that each audio broadcast appends a 6 digit token to the\npayload. Then, all we need to do is keep the token in sync with the client and\nserver. On the client, when processing the audio queue, it simply checks\nwhether or not the token at the beginning of the payload matches, and if it\ndoesn\u2019t it ignores that sample.\n\nThe limitation with this implementation is it does not update the chat\ntranscript. It\u2019s entirely possible because we have access to the alignment\ninformation from ElevenLabs, but I just didn\u2019t implement it at this time.\n\n### Time-based Hang Ups\n\nAnother thing the Retell demo has support for is cues and hang ups after a\nduration of silence. If you are silent for too long, you\u2019ll get a cue from the\nAI speaker asking you if you\u2019re still there. After another duration of\nsilence, it will hang up. This is something that\u2019s pretty easy to do with\nLiveView and Process.send_after/4:\n\n1234| defp nudge(socket) donudge_pid = Process.send_after(self(), :nudge,\n@nudge_ms)assign(socket, :nudge_pid, nudge_pid)end  \n---|---  \n  \nAnd then we can cancel the timer anytime we receive a transcription, and\nrestart it after every turn speaking. Note that we can\u2019t depend on the Phoenix\nspeak async task ending as the trigger to send nudges. Instead, we need to\npush an event from the speaker hook that the audio has ended. This avoids a\ncase where the speaker initiates a really long speech, which overlaps with the\nnudge_ms duration. Now, we can control the number of nudges with an assign. In\nmy case, I just used a boolean:\n\n123456789101112| def handle_info(:nudge, socket) dosocket =if\nsocket.assigns.nudged? dostop_conversation(socket)elsesocket|> speak([\"Are \",\n\"you \", \"still \", \"there? \"])|> assign(nudged?: true)end{:noreply, socket}end  \n---|---  \n  \n## Re-doing the Entire Thing\n\nSomewhere along the line I realized that my attempts at engineering solid VAD\nclient-side were never going to deliver the experience that I wanted. I\ndiscussed with Andres Alejos a bit, and he found a Silero VAD model which is\ncapable of performing VAD in 1ms on a single CPU thread. They also had an ONNX\nmodel\u2014and we have a library in the Elixir ecosystem called Ortex which allows\nus to execute ONNX models.\n\nTo accomodate for the new VAD model, I ended up re-implementing the original\nLiveView I had as a WebSocket. This actually works out well because the\nWebSocket server is generic, and can be consumed by any language with a\nWebSocket client. The implementation is also relatively simple, and easily\nexpanded to accomodate for other LLMs, TTS, and STT models. The WebSocket\nimplementation has low latency (when running on a GPU), and supports\ninterrupts.\n\nYou can find the project on my GitHub as well as an example using the server.\n\n## Musings\n\nThe final implementation I ended up with still does not match the quality of\nthe Retell demo. That said, I think it\u2019s a solid start for future work. I\nbelieve I acted with some hubris when first posting about this project, and I\nwould like to say that Retell\u2019s work should not be understated. I can\nappreciate the attention to detail that goes into making an effective\nconversational agent, and Retell\u2019s demo shows they paid a lot of attention to\nthe details. Kudos to them and their team.\n\nI will also admit that my demo is playing to one benchmark. I\u2019m optimizing the\nhell out of latency to support a single user\u2014me. I think this solution would\nchange if it needed to accomodate for multiple concurrent users.\n\nRetell\u2019s website claims they have a conversation orchestration model under the\nhood to manage the complexities of conversation. I had my doubts about that\ngoing into this, but I believe it now. Whether or not this model is actually a\nsingle model or a series of models for VAD, adding backchannels, etc. I\u2019m not\nsure. I think eventually it will be a single model, but I\u2019m not sure if it is\nnow, which leads me to my next point.\n\n### Another Bitter Lesson\n\nWhile doing all of these optimizations, I could not help but think that it\nwill eventually be all for naught. Not because I don\u2019t think people will find\nit useful, but because large models trained on lots of data simply seem to\nalways beat engineering effort. I believe the future of this area of work is\nin joint models. I think the only way to achieve real-time conversations is to\nmerge parts of the stack. I predict in less than a year we will see an\nincredibly capable joint speech/text model. I recently saw a large audio model\ncalled Qwen-Audio that I believe is similar to what I envision.\n\nSpecifically, if somebody were kind enough to give me some money to throw at\nthis problem, here is exactly what I would do:\n\n  1. Generate an Alpaca-style and/or LLaVA-style dataset of synthetic speech. Note that it would require a bit of pre-processing to change Alpaca inputs to mirror a style compatible with spoken-word. I would use ElevenLabs to generate the dataset in mulitple voices. Of course this dataset would be a bit too \u201cclean,\u201d so we\u2019d need to apply some augmentations which add ambient noise, change speaking pitch and speed, etc. Bonus points: adding samples of \u201cnoise\u201d which require no response to merge the VAD part of the pipeline in as well. You can even throw in text prompts that dictate when and when not to respond to support things like wake word detection without needing to train a separate model.\n  2. Create a LLaVA-style model with a Whisper or equivalent base, an LLM, and a projection layer.\n  3. Secure H100s, train model, and \u201cturn H100s into $100s\u201d (thank you @thmsmlr)\n\nIf you want to give me some $$$, my e-mail is smoriarity.5@gmail.com\n\nI believe we are also close to just having full-on speech-to-speech models. A\nspecific challenge I can see when creating these models is coming up with a\nhigh-quality dataset. I think if you make a deliberate attempt at \u201crecording\nconversations\u201d for the purposes of training, you will actually probably end up\nwith a lower-quality dataset. People tend to change their behavior under\nobservation. Additionally, conversations from movies and TV shows aren\u2019t\nactually very natural. Even some podcasts have an unnatural converastional\nrhythm.\n\nWhile watching Love is Blind with my fianc\u00e9, I realized you could probably get\na decent amount of quality data from reality tv shows. The conversations in\nreality TV are overly dramatic and chaotic, but are (I think) closer to\nrealistic than anything else.\n\n### Conversational Knowledge Base?\n\nI do wonder what a solid RAG implementation looks like on top of a\nconversational agent. RAG and complex CoT pipelines will introduce latency\nwhich could deteriorate the conversational experience. However, there are\nclever ways you can hide this. In conversations that require \u201csearch\u201d between\nhumans, e.g. like scheduling an appointment, you\u2019ll often have one party\nsaying \u201cone moment please\u201d before performing a system search. Building\nsomething like that in is entirely possible. Additionally, if your agent\nrequires information up front about an individual, it\u2019s possible to include\nthat in the initial prompt.\n\n### You Should Use Elixir\n\nI was very excited for this problem in particular because it\u2019s literally the\nperfect application of Elixir and Phoenix. If you are building conversational\nagents, you should seriously consider giving Elixir a try. A large part of how\nquick this demo was to put together is because of how productive Elixir is.\n\n## Conclusion\n\nThis was a fun technical challenge. I am pleased with the performance of the\nfinal demonstration. I\u2019m also happy I was able to OSS a small library for\nothers to build off of. If you are interested in conversational agents, I\nencourage you to check it out, give feedback, and contribute! I know it\u2019s very\nrough right now, but it will get better with time.\n\nAdditionally, I plan to periodically build out the rest of the Nero project,\nso please follow me on Twitter if you\u2019d like to stay up to date.\n\n### Share this:\n\n  * Twitter\n  * Facebook\n\nLike Loading...\n\n### Related\n\nNero Part 1: Home AutomationsFebruary 25, 2024In \"elixir\"\n\nNx Tip of the Week #6 \u2013 Compiler or Backend?March 25, 2021In \"nx totw\"\n\nJIT/GPU accelerated deep learning for Elixir with Axon v0.1June 16, 2022In\n\"axon\"\n\n  * Tagged\n  * accessibility\n  * ai\n  * conversational-ai\n  * elixir\n  * technology\n\nPublished February 25, 2024\n\n### Leave a comment Cancel reply\n\nBlog at WordPress.com.\n\n  * Comment\n  * Reblog\n  * Subscribe Subscribed\n\n    * Sean Moriarity\n    * Already have a WordPress.com account? Log in now.\n\n  * Privacy\n  *     * Sean Moriarity\n    * Customize\n    * Subscribe Subscribed\n    * Sign up\n    * Log in\n    * Copy shortlink\n    * Report this content\n    * View post in Reader\n    * Manage subscriptions\n    * Collapse this bar\n\n%d\n\n", "frontpage": true}
