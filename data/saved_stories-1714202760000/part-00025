{"aid": "40173473", "title": "On Limited Government and AI", "url": "https://1a3orn.com/sub/essays-limited-government-ai.html", "domain": "1a3orn.com", "votes": 1, "user": "1a3orn", "posted_at": "2024-04-26 19:55:02", "comments": 0, "source_title": "On Limited Government and AI", "source_text": "On Limited Government and AI\n\n# 1 A 3 O R N\n\n# 1 A 3 O R N\n\n# On Limited Government and AI\n\nCreated: 2024-04-25\n\nWordcount: 2k\n\nTags:essaysmindkill\n\n> William Roper: \u201cSo, now you give the Devil the benefit of law!\u201d\n>\n> Sir Thomas More: \u201cYes! What would you do? Cut a great road through the law\n> to get after the Devil?\u201d\n>\n> William Roper: \u201cYes, I'd cut down every law in England to do that!\u201d\n>\n> Sir Thomas More: \u201cOh? And when the last law was down, and the Devil turned\n> 'round on you, where would you hide, Roper, the laws all being flat? This\n> country is planted thick with laws, from coast to coast, Man's laws, not\n> God's! And if you cut them down, and you're just the man to do it, do you\n> really think you could stand upright in the winds that would blow then? Yes,\n> I'd give the Devil benefit of law, for my own safety's sake!\u201d\n\n### TLDR\n\nThe recently-proposed model legislation from the \"Center for AI Policy\" (CAIP)\ncreates a federal administration with arbitrary power to ban or regulate any\nmachine-learning training process -- i.e., power to regulate any ML training\nof any size.\n\nThat is, the sometimes-mentioned limit for training runs of 10^24 FLOPs,\nbeneath which the new administration is sometimes described as having no new\nauthority, is just incorrect -- it doesn't consider the deliberately built-in\ncapacity of the new administration to modify its own standards, with no\nsubstantial limits placed on this modification.\n\nI will walk through the legislation and explain why this is so, before\nconcluding with some reflections the AI safety movement's predilection for\ndespotism and Platonic top-down solutions.\n\n## Unconstrained Power\n\nThe \"Responsible Advanced Artificial Intelligence Act\" from the Center for AI\nPolicy proposes a new \"Frontier Artificial Intelligence Systems\nAdministration\" (FAISA). The purpose of the administration is to \"oversee and\nregulate advanced general-purpose artificial intelligence systems.\"\n\nConcretely, it (pg. 4) proposes four tiers of models, with different levels of\nregulation applied to each. The initial definitions of these tiers and the\nbroad regulations imposed on them are as follows:\n\n  * Low concern:\n\n    * Trained with less than 10^24 FLOP during training run.\n    * No regulation required.\n  * Medium concern:\n\n    * Trained with more than 10^24 FLOP and less than 10^26 FLOP during its training run, and doesn't exceed certain measures on completely-unspecified-by-the-act benchmarks.\n    * Regulations require notifying FAISA before training; at the conclusion of training; and requires testing to ensure that the model doesn't exceed aforementioned completely-unspecified benchmarks.\n  * High concern:\n\n    * Trained with 10^26 FLOP, or one of the five most capable AI systems in the world, based on the again-unspecified benchmarks.\n    * Requires separate permits for training the model, possessing weights for the model, and deploying the model. The permits are scored based on lists of desiderata that would themselves need to be further specified, providing yet another free variable to this whole mess of free variables\n\nOpen-weights AI projects would score 0 on a large number of these desiderata\n-- i.e., you cannot monitor them; you cannot prevent finetuning; you cannot\nrecall the deployment -- so I take \"high concern\" to be the level that pretty\nmuch forbids open weights, although the document is rather reluctant to say\nthis outright.\n\nIn general, the administration would have powers to ban the training,\npossession, or use of models at this level if they did not meet various\nrequirements.\n\n  * Extremely high concern:\n\n    * This is quite vaguely defined. A machine-learning system is defined (again, initially and tentatively) to be high-concern if it can \"significantly assist with the development.... of biological, chemical, radiological, or nuclear weapons\" or \"has or could easily develop the ability to autonomously spread\" or could \"accelerate scientific research to such a degree as to... destabilize the global balance of power.\"\n\nI could spend more time on the vagueness of the fourth tier, but it's not\nreally necessary to make my point.\n\nThe vital point is that these thresholds are the initial thresholds, rather\nthan final thresholds.\n\nAnd these tiers can be modified arbitrarily by the Administrator of FAISA. The\ntext (pg. 4) states clearly: \"The Administrator may modify any or all of these\nthresholds as set forth in Sections 4 through 6.\"\n\nSection 6 speaks to this directly, and establishes that although the\nAdministrator may make any these thresholds more constraining without any\nparticular standard of evidence at all, he may not make them less constraining\nin the same way:\n\n> Except as otherwise modified by this section, the Administrator shall have\n> full power to promulgate rules to carry out this Act in accordance with\n> section 553 of title 5, United States Code. This includes the power to\n> update or modify any of the technical definitions in this Act (including the\n> definition of \u201cfrontier AI\u201d and the definitions of \u201ctiers of concern\u201d) to\n> ensure that these definitions will continue to adequately protect against\n> major security risks despite changes in the technical landscape such as\n> improvements in algorithmic efficiency. However, neither these definitions\n> nor any rule promulgated under this Act may be altered by the Administrator\n> so as to be more permissive of frontier AI development unless the\n> Administrator first makes findings supported by clear and convincing\n> evidence that such alterations will not significantly increase major\n> security risks.\n\nYou'll note that this specifically anticipates tightening thresholds due, for\ninstance, to improvements in algorithmic efficiency.\n\n### Extra: Ability to Ignore Congress\n\nUnless I'm misunderstanding the law, after FAISA \"updated a threshold,\"\nCongress could issue a disapproval of the rule altering the threshold, and\nprevent it from going into effect, beneath 5 U.S.C. \u00a7 802, during a 60-day\nwindow after the submission of the rule.\n\nCongress has apparently issued such a disapproval only a total 20 times since\n1996, when 5 U.S.C. \u00a7 802 came into existence, so it disapproves rules at the\nrate of about 3/4th of a rule per year. Literally 19 of these disapprovals\nwere Democratic / Republican administrations disapproving rules that were\npassed during the last 60 days of of the prior Republican / Democratic\nadministration. So -- it's not a particularly likely restraint on a rule-\nmaking body by any means at all.\n\nHowever, even this great unlikelihood is too heavy a limit upon the\nAdministrator's power for the drafters of this law.\n\nNormally, regulatory agencies cannot simply re-pass a nearly identical rule\nafter Congress disapproves if it is in \"substantially the same form.\" This\nobviously makes sense -- given how unlikely Congress is to specifically\ndisapprove a rule even once, if you could just re-submit a rephrased version\nof the rule then Congress would be even more nullified.\n\nBut the model legislation establishing FAISA has a special provision that\nspecifically allows it to pass similar rules to those previously disallowed by\nCongress!\n\n> Because of the rapidly changing and highly sensitive technical landscape, a\n> rule that appears superficially similar to a rule that has been disapproved\n> by Congress may nevertheless be a substantially different rule. Therefore, a\n> rule issued under this section that varies at least one material threshold\n> or material consequence by at least 20% from a previously disapproved rule\n> is not \u201csubstantially the same\u201d under 5 U.S.C. \u00a7 802(b)(2).\n\nI want to note that even without this provision, the freedom of regulatory\npower granted to FAISA seems absurdly broad. This provision brings it to\nactual parody.\n\n### And Continued\n\nAgain the document states that the Administrator has a power and obligation to\ntighten thresholds -- but fails to mention a corresponding obligation to\nloosen them, should they be excessive.\n\n> No later than September 1 st of each year, the Administrator shall review\n> each of the thresholds in section 8(a), together with the relevant\n> definitions in section 3, and determine whether each threshold and each\n> definition remains adequate to defend against major security risks. If any\n> threshold or definition has become inadequate, then the Administrator shall\n> promptly promulgate rules to appropriately strengthen or tighten the\n> threshold or definition.\n\nImagine the following scenario:\n\nThe new Administration could lower the threshold for \"low concern AI\" from\n10^24 FLOPs to 10^21 FLOPs, and of high-concern AI from 10^26 to 10^23. This\nwould effectively outlaw making an open-weights model like the original Llama\nmodel from Meta -- not even Llama 2 or 3, but Llama 1.\n\n(This is, notably, the kind of thing that highly-concerned AI-safetyists have\nspecifically said they want to do. The StopAI people, for instance, want to\noutright ban AI training runs of more than 10^23 and require official\ngovernment permission for AI runs to an literally unspecified degree below\nthat.)\n\nCongress might, in an uncharacteristic spasm of concern for limited\ngovernment, decide this was a little too much and nullify the rule.\n\nAnd then FAISA could change the threshold for low-concern AI from 10^21 FLOPs\nto 1.2 * 10^21, which is a 20% change, and reinstate the rule anyhow.\n\nThis isn't an accidental ability built into the act; this is extremely\ndeliberate.\n\n# On Indifference to Second Order Effects\n\nI've seen a number of people who care about AI safety say that this act seems\nlike a good starting point. I've seen people praise it as a serious and worthy\nattempt. I've seen others even claim that the bill is too weak.\n\nLook.\n\nWhen the archetypical man-in-the-state-of-nature finds that something has gone\nwrong -- \"Someone is stealing our sheep!\" -- he gathers with his fellows, and\ninstitutes government, by giving Some Dude the Authority to stop the wrong\nthing -- \"You, find who is stealing our sheep and figure out how to stop it.\"\n\nShortly afterwards, in this fake mythological origination of government, a\nsecond discovery is made -- \"Fuck, we should have set forth clear rules for\nwhat Some Dude could not do without talking to us, rather than just giving him\nauthority to do whatever he wants.\" Thus the origin of due process, democratic\ninstitutions, and proceduralism in general.\n\nAnd shortly afterwards, the third discovery gets made -- \"Huh, maybe there\nshould be limits to what Some Dude can do -- even if he does talk do us?\" Thus\nthe origin of rights, specifically limited government, freedom of association,\nfreedom of the press, and liberalism in general.\n\nIf you think AI is just going to kill everyone... I mean, I get it. You might\njust not think limited powers should apply to anything to do with machine\nlearning at all. You think that we need arbitrary power to guard ourselves\nfrom the Devil.\n\nIn general, people like to destroy the features of limited government whenever\nthey think they're dealing with something sufficiently bad. The Catholic\nfundamentalist says \"Lo, freedom of speech is good, but heresy sends souls to\nhell so we must establish an Index of Forbidden Books.\" The NSA agent says\n\"Lo, freedom of association is good, but non-visible-to-me online discussions\nallow terrorists to talk about things with each other, so we must ban civilian\ncryptography.\" And the AI safety guy from MIRI says, \"Lo, limited government\npower is nice, but a superintelligent AI could sprout forth in mere days from\nany computer at all, so someone ideologically identical to me should have\nunlimited power over all ML training.\"\n\nBut -- in general -- civilization depends on taking a look at people who think\nthat they've found something Sufficiently Bad that we should destroy these\nfeatures of government and politely saying, \"No. We will not.\"\n\nI want to be clear -- I think even without the self-modifications allowing\narbitrary powers, this would be a really awful act.\n\nSomething like 10^24 is just too low. I think implementing this act would\nprematurely cripple the mechanisms by which much human knowledge is generated,\nin the name of safety. (Seriously, take a look at the number of groups that\nhave tried to ban Llama 2, and then look on ArXiV for the number of safety and\ninterpretability papers that have depended on it. It's increasingly likely\nMeta has assisted safety research than most and potentially any AI safety\norg.) And I think that the likely corruptions of this kind of act could make\nit bad for AI safety like the National Environmental Protection Act is bad\neven just for the environment. But it is the provisions to let the new\nAdministration extend its power however it wants to that make me think, \"Yeah,\nI'm done. Whoever wrote this should just... not be writing laws, period, full\nstop.\"\n\nIf people who advocate for extreme AI regulation do want to make a Tsar of AI\nbecause they think the situation is just that desperate -- I hope they can say\nso more clearly than the discussion about this law says it. Like that would be\na big step, let's at least be clear that is the step we would be taking. If we\nwant to say, \"Nah, government should have arbitrary authority over all ML\ntraining everywhere,\" I'd at least hope we could say that to ourselves before\nwe take that step, rather than try to sneak it in like an intelligence agency\nsneaking in provisions for universal surveillance.\n\nBut mostly, I hope that people concerned with AI safety can come to the\nconclusion that a Tsar of AI is neither the best way to deal with AI risk nor\na good feature to add to our government in general.\n\nIf you want, you can help me spend more time on things like this.\n\n", "frontpage": false}
