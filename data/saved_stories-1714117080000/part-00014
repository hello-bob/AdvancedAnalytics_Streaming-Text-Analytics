{"aid": "40162632", "title": "2.5x better performance: Rama vs. MongoDB and Cassandra", "url": "https://blog.redplanetlabs.com/2024/04/25/better-performance-rama-vs-mongodb-and-cassandra/", "domain": "redplanetlabs.com", "votes": 1, "user": "refset", "posted_at": "2024-04-25 20:32:21", "comments": 0, "source_title": "2.5x better performance: Rama vs. MongoDB and Cassandra", "source_text": "2.5x better performance: Rama vs. MongoDB and Cassandra \u2013 Blog\n\nSkip to content\n\nBlog\n\n# 2.5x better performance: Rama vs. MongoDB and Cassandra\n\nApril 25, 2024 ~ Nathan Marz\n\nWe ran a number of benchmarks comparing Rama against the latest stable\nversions of MongoDB and Cassandra. The code for these benchmarks is available\non Github. Rama\u2019s indexes (called PStates) can reproduce any database\u2019s data\nmodel since each PState is an arbitrary combination of durable data structures\nof any size. We chose to do our initial benchmarks against MongoDB and\nCassandra because they\u2019re widely used and like Rama, they\u2019re horizontally\nscalable. In the future we\u2019ll also benchmark against other databases of\ndifferent data models.\n\nThere are some critical differences between these systems that are important\nto keep in mind when looking at these benchmarks. In particular, Cassandra by\ndefault does not guarantee writes are durable when giving acknowledgement of\nwrite success. It has a config commitlog_sync that specifies its strategy to\nsync its commit log to disk. The default setting \u201cperiodic\u201d does the sync\nevery 10 seconds. This means Cassandra can lose up to 10 seconds of\nacknowledged writes and regress reads on those keys (we disagree strongly with\nthis setting being the default, but that\u2019s a post for another day).\n\nRama has extremely strong ACID properties. An acknowledged write is guaranteed\nto be durable on the leader and all in-sync followers. This is an enormous\ndifference with Cassandra\u2019s default settings. As you\u2019ll see, Rama beats or\ncomes close to Cassandra in every benchmark. You\u2019ll also see we benchmarked\nCassandra with a commitlog_sync setting that does guarantee durability, but\nthat causes its performance to plummet far below Rama.\n\nMongoDB, at least in the latest version, also provides a durability guarantee\nby default. We benchmarked MongoDB with this default setting. Rama\nsignificantly outperforms MongoDB in every benchmark.\n\nAnother huge difference between Rama and MongoDB/Cassandra (and pretty much\nevery database) comes from Rama being a much more general purpose system. Rama\nexplicitly distinguishes data from indexes and stores them separately. Data is\nstored in durable, partitioned logs called \u201cdepots\u201d. Depots are a distinct\nconcept from \u201ccommit logs\u201d, which is a separate mechanism that MongoDB,\nCassandra, and Rama also have as part of their implementations. When using\nRama, you code \u201ctopologies\u201d that materialize any number of indexes of any\nshape from depots. You can use depots to recompute indexes if you made a\nmistake, or you can use depots to materialize entirely new indexes in the\nfuture to support new features. Depots can be consumed by multiple topologies\nmaterializing multiple indexes of different shapes. So not only is Rama in\nthese benchmarks materializing equivalent indexes as MongoDB / Cassandra with\ngreat comparable performance, it\u2019s also materializing a durable log. This is a\nnon-trivial amount of additional work Rama is doing, and we weren\u2019t expecting\nRama to perform so strongly compared to databases that aren\u2019t doing this\nadditional work.\n\n## Benchmark setup\n\nAll benchmarks were done on a single m6gd.large instance on AWS. We used this\ninstance type rather than m6g.large so we could use a local SSD to avoid\ncomplications with IOPS limits when using EBS.\n\nWe\u2019re just testing single node performance in this benchmark. We may repeat\nthese tests with clusters of varying sizes in the future, including with\nreplication. However, all three systems have already demonstrated linear\nscalability so we\u2019re most interested in raw single-node performance for this\nset of benchmarks.\n\nFor all three systems we only tested with the primary index, and we did not\ninclude secondary indexes in these tests. We tried configuring Cassandra to\nhave the same heap size of Rama\u2019s worker (4GB) instead of the default 2GB that\nit was choosing, but that actually made its read performance drastically\nworse. So we left it to choose its own memory settings.\n\nThe table definition used for Cassandra was:\n\n1 2 3 4 5 6| CREATE TABLE IF NOT EXISTS test.test ( pk text, ck text, value\ntext, PRIMARY KEY (pk, ck) );  \n---|---  \n  \nThis is representative of the kind of indexing that Cassandra can handle\nefficiently, like performing range queries on a clustering key.\n\nAll Cassandra reads/writes were done with the prepared statements \"SELECT\nvalue FROM test.test WHERE pk = ? AND ck = ?;\" and \"INSERT INTO test.test (pk,\nck, value) VALUES (?, ?, ?);\" .\n\nCassandra was tested with both the \u201cperiodic\u201d commitlog_sync config, which\ndoes not guarantee durability of writes, and the \u201cbatch\u201d commitlog_sync\nconfig, which does guarantee durability of writes. We played with different\nvalues of commitlog_sync_batch_window_in_ms , but that had no effect on\nperformance. We also tried the \u201cgroup\u201d commitlog_sync config, but we couldn\u2019t\nget its throughput to be higher than \u201cbatch\u201d mode. We tried many permutations\nof the configs commitlog_sync_group_window (e.g. 1ms, 10ms, 20ms, 100ms) and\nconcurrent_writes (e.g. 32, 64, 128, 256), but the highest we could get the\nthroughput was about 90% that of batch mode. The other suggestions on the\nCassandra mailing list didn\u2019t help.\n\nThe Rama PState equivalent to this Cassandra table had this data structure\nschema:\n\n1| {[String, String] -> String}  \n---|---  \n  \nThe module definition was:\n\n1 2 3 4 5 6 7 8 9 10 11| (defmodule CassandraModule [setup topologies]\n(declare-depot setup *insert-depot :random)(let [s (stream-topology topologies\n\"cassandra\")] (declare-pstate s $$primary {java.util.List String}) (<<sources\ns (source> *insert-depot :> *data) (ops/explode *data :> [*pk *ck *val])\n(|hash *pk) (local-transform> [(keypath [*pk *ck]) (termval *val)] $$primary)\n)))  \n---|---  \n  \nThis receives triples of partitioning key, clustering key, and value and\nwrites it into the PState, ensuring the data is partitioned by the\npartitioning key.\n\nCassandra and Rama both index using LSM trees, which sorts on disk by key.\nDefining the key as a pair like this is equivalent to Cassandra\u2019s\n\u201cpartitioning key\u201d and \u201cclustering key\u201d definition, as it\u2019s first sorted by\nthe first element and then by the second element. This means the same kinds of\nefficient point queries or range queries can be done.\n\nThe Rama PState equivalent to MongoDB\u2019s index had this data structure schema:\n\n1| {String -> Map}  \n---|---  \n  \nThe module definition was:\n\n1 2 3 4 5 6 7 8 9 10 11| (defmodule MongoModule [setup topologies] (declare-\ndepot setup *insert-depot :random)(let [s (stream-topology topologies\n\"mongo\")] (declare-pstate s $$primary {String java.util.Map}) (<<sources s\n(source> *insert-depot :> *data) (ops/explode *data :> {:keys [*_id] :as *m})\n(|hash *_id) (local-transform> [(keypath *_id) (termval *m)] $$primary) )))  \n---|---  \n  \nThis receives maps containing an :_id field and writes each map to the\n$$primary index under that ID, keeping the data partitioned based on the ID.\n\nWe used strings for the IDs given to MongoDB, so we used strings in the Rama\ndefinition as well. MongoDB\u2019s documents are just maps, so they\u2019re stored that\nway in the Rama equivalent.\n\nWriting these modules using Rama\u2019s Java API is pretty much the same amount of\ncode. There\u2019s no difference in performance between Rama\u2019s Clojure and Java\nAPIs as they both end up as the same bytecode.\n\n## Max write throughput benchmark\n\nFor the max write throughput benchmark, we wrote to each respective system as\nfast as possible from a single client colocated on the same node. Each request\ncontained a batch of 100 writes, and the client used a semaphore and the\nsystem\u2019s async API to only allow 1000 writes to be in-flight at a time. As\nrequests got acknowledged, more requests were sent out.\n\nAs described above, we built one Rama module that mimics how MongoDB works and\nanother module that mimics how Cassandra works. We then did head to head\nbenchmarks against each database with tests writing identical data.\n\nFor the MongoDB tests, we wrote documents solely containing an \u201c_id\u201d key set\nto a UUID. Here\u2019s MongoDB vs. Rama:\n\nRama\u2019s throughput stabilized after 50 minutes, and MongoDB\u2019s throughput\ncontinued to decrease all the way to the end of the three hour test. By the\nend, Rama\u2019s throughput was 9x higher.\n\nFor the Cassandra tests, each write contained a separate UUID for the fields\n\u201cpk\u201d, \u201cck\u201d, and \u201cvalue\u201d. We benchmarked Cassandra both with the default\n\u201cperiodic\u201d commit mode, which does not guarantee durability on write\nacknowledgement, and with the \u201cbatch\u201d commit mode, which does guarantee\ndurability. As mentioned earlier, we couldn\u2019t get Cassandra\u2019s \u201cgroup\u201d commit\nmode to match the performance of \u201cbatch\u201d mode, so we focused our benchmarks on\nthe other two modes. Here\u2019s a chart with benchmarks of each of these modes\nalong with Rama:\n\nSince Rama guarantees durable writes, the equivalent comparison is against\nCassandra\u2019s batch commit mode. As you can see, Rama\u2019s throughput is 2.5x\nhigher. Rama\u2019s throughput is only a little bit below Cassandra when Cassandra\nis run without the durability guarantee.\n\n## Mixed read/write throughput benchmark\n\nFor the mixed read/write benchmark, we first wrote a fixed amount of data into\neach system. We wanted to see the performance after each system had a\nsignificant amount of data in it, as we didn\u2019t want read performance skewed by\nthe dataset being small enough to fit entirely in memory.\n\nFor the MongoDB tests, we wrote documents solely containing an \u201c_id\u201d field\nwith a stringified number that incremented by two for each write (\u201c0\u201d, \u201c2\u201d,\n\u201c4\u201d, \u201c6\u201d, etc.). We wrote 250M of those documents (max ID was \u201c500000000\u201d).\nThen for the mixed reads/writes test, we did 50% reads and 50% writes. 1000\npairs of read/writes were in-flight at a time. Each write was a single\ndocument (as opposed to batch write test above which did 100 at a time), and\neach read was randomly chosen from the keyspace from \u201c0\u201d to the max ID. Since\nonly half the numbers were written, this means each read had a 50% chance of\nbeing a hit and a 50% chance of being a miss.\n\nHere\u2019s the result of the benchmark for MongoDB vs. Rama:\n\nWe also ran another test of MongoDB with half the initial data:\n\nMongoDB\u2019s performance is unaffected by the change in data volume, and Rama\noutperforms MongoDB in this benchmark by 2.5x.\n\nFor the Cassandra tests, we followed a similar strategy. For every write, we\nincremented the ID by two and wrote that number stringifed for the \u201cpk\u201d, \u201cck\u201d,\nand \u201cvalue\u201d fields (e.g. \"INSERT INTO test.test (pk, ck, value) VALUES ('2',\n'2', '2');\" ). Reads were similarly chosen randomly from the keyspace from \u201c0\u201d\nto the max ID, with each read fetching the value for a \u201cpk\u201d and \u201cck\u201d pair.\nJust like the MongoDB tests, each read had a 50% chance of being a hit and a\n50% chance of being a miss.\n\nAfter writing 250M rows to each system, here\u2019s the result of the benchmark for\nCassandra vs. Rama:\n\nRama performs more than 2.5x better in this benchmark whether Cassandra is\nguaranteeing durability of writes or not. Since Cassandra\u2019s write performance\nin this non-durable mode was a little higher than Rama in our batch write\nthroughput test, this test indicates its read performance is substantially\nworse.\n\nCassandra\u2019s non-durable commit mode being slightly worse than its durable\ncommit mode in this benchmark, along with Cassandra\u2019s reputation as a high\nperformance database, made us wonder if we misconfigured something. As\ndescribed earlier, we tried increasing the memory allocated to the Cassandra\nprocess to match Rama (4GB), but that actually made its performance much\nworse. We made sure Cassandra was configured to use the local SSD for\neverything (data dir, commit log, and saved caches dir). Nothing else in the\ncassandra.yaml or cassandra-env.sh files seemed misconfigured. There are a\nvariety of configs relating to compaction and caching that could be relevant,\nbut Rama has similar configs that we also didn\u2019t tune for these tests. So we\nleft those at the defaults for both systems. After double-checking all the\nconfigs we reran this benchmark for Cassandra for both commit modes and got\nthe same results.\n\nOne suspicious data point was the amount of disk space used by each system.\nSince we wrote a fixed amount of identical data to each system before this\ntest, we could compare this directly. Cassandra used 11GB for its \u201cdata dir\u201d,\nwhich doesn\u2019t include the commit log. Rama used 4GB for the equivalent. If you\nadd up the raw amount of bytes used by 250M rows with identical \u201cpk\u201d, \u201cck\u201d,\nand \u201cvalue\u201d fields that are stringified numbers incrementing by two, you end\nup with 6.1GB. Both Cassandra and Rama compress data on disk, and since there\nare so many identical values compression should be effective. We don\u2019t know\nenough about the implementation of Cassandra to say why its disk usage is so\nhigh relative to the amount of data being put into it.\n\nWe ran the test again for Cassandra with half the data (125M rows), and these\nwere the results:\n\nCassandra\u2019s numbers are much better here, though the numbers were degrading\ntowards the end. Cassandra\u2019s read performance seems to suffer as the dataset\ngets larger.\n\n## Conclusion\n\nWe were surprised by how well Rama performed relative to Cassandra and MongoDB\ngiven that it also materializes a durable log. When compared to modes of\noperation that guarantee durability, Rama performed at least 2.5x better in\nevery benchmark.\n\nBenchmarks should always be taken with a grain of salt. We only tested on one\nkind of hardware, with contrived data, with specific access patterns, and with\ndefault configs. It\u2019s possible MongoDB and Cassandra perform much better on\ndifferent kinds of data sets or on different hardware.\n\nRama\u2019s performance is reflective of the amount of work we put into its design\nand implementation. One of the key techniques we use all over the place in\nRama\u2019s implementation is what we call a \u201ctrailing flush\u201d. This technique\nallows all disk and network operations to be batched even though they\u2019re\ninvoked one at a time. This is important because disk syncs and network\nflushes are expensive. For example, when an append is done to a depot (durable\nlog), we don\u2019t apply that immediately. Instead the appends gets put into an\nin-memory buffer, and an event is enqueued that will flush that buffer if no\nsuch event is already enqueued. When that event comes to the front of the\nprocessing queue, it flushes whatever has accumulated on the buffer. If the\nrate of appends is low, it may do a disk operation for a single append. As the\nrate of appends gets higher, the number of appends that gets performed\ntogether increases. This technique greatly increases throughput while also\nminimizing latency. We use this technique for sending appends from a client,\nfor flushing network messages in Netty (called \u201cflush consolidation\u201d), for\nwriting to indexes, for sending replication messages to followers, and more.\n\nThe only performance numbers we shared previously were for our Twitter-scale\nMastodon instance, so we felt it was important to publish some more numbers\nagainst tools many are already familiar with. If there are any flaws in how we\nbenchmarked MongoDB or Cassandra, please share with us and we\u2019ll be happy to\nrepeat the benchmarks.\n\nSince Rama encompasses so much more than data indexing, in the future we will\nbe doing more benchmarks against different kinds of tooling, like queues and\nprocessing systems. Additionally, since Rama is an integrated system we expect\nits most impressive performance numbers to be when benchmarked against\ncombinations of tooling (e.g. Kafka + Storm + Cassandra + ElasticSearch). Rama\neliminates the overhead inherent when using combinations of tooling like that.\n\nFinally, since Rama is currently in private beta you have to join the beta to\nget access to a full release in order to be able to reproduce these\nbenchmarks. As mentioned at the start of this post, the code we used for the\nbenchmarks is on our Github. Benchmarks are of course better when they can be\nindependently reproduced. Eventually Rama will be generally available, but in\nthe meantime we felt publishing the numbers was still important even with this\nlimitation.\n\n### Leave a ReplyCancel reply\n\n# Follow Us\n\n  * Twitter\n  * GitHub\n\n# RSS\n\n# Recent Posts\n\n  * 2.5x better performance: Rama vs. MongoDB and Cassandra\n  * Everything wrong with databases and why their complexity is now unnecessary\n  * How Rama is tested: a primer on testing distributed systems\n  * Introducing Rama\u2019s Clojure API: build end-to-end scalable backends in 100x less code\n\nContents\n\n1\\. Benchmark setup\n\n2\\. Max write throughput benchmark\n\n3\\. Mixed read/write throughput benchmark\n\n4\\. Conclusion\n\n## Discover more from Blog\n\nSubscribe now to keep reading and get access to the full archive.\n\nContinue reading\n\n", "frontpage": false}
