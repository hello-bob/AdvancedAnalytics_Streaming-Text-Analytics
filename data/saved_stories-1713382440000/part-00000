{"aid": "40064316", "title": "Implementation for Mini-Gemini", "url": "https://github.com/dvlab-research/MiniGemini", "domain": "github.com/dvlab-research", "votes": 3, "user": "adif_sgaid", "posted_at": "2024-04-17 13:30:33", "comments": 0, "source_title": "GitHub - dvlab-research/MiniGemini: Official implementation for Mini-Gemini", "source_text": "GitHub - dvlab-research/MiniGemini: Official implementation for Mini-Gemini\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\ndvlab-research / MiniGemini Public\n\n  * Notifications\n  * Fork 103\n  * Star 1.6k\n\nOfficial implementation for Mini-Gemini\n\nmini-gemini.github.io/\n\n### License\n\nApache-2.0 license\n\n1.6k stars 103 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# dvlab-research/MiniGemini\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nyanwei-liMerge branch 'main' of https://github.com/dvlab-\nresearch/MiniGeminiApr 17, 2024b326497 \u00b7 Apr 17, 2024Apr 17, 2024\n\n## History\n\n29 Commits  \n  \n### images\n\n|\n\n### images\n\n| transparent images| Mar 28, 2024  \n  \n### minigemini\n\n|\n\n### minigemini\n\n| Merge branch 'main' of https://github.com/dvlab-research/MiniGemini| Apr 17,\n2024  \n  \n### scripts\n\n|\n\n### scripts\n\n| Fix drop path of HR encoder and other bugs| Apr 17, 2024  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| first commit| Mar 27, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update README.md| Apr 16, 2024  \n  \n### cog.yaml\n\n|\n\n### cog.yaml\n\n| first commit| Mar 27, 2024  \n  \n### hostfile\n\n|\n\n### hostfile\n\n| first commit| Mar 27, 2024  \n  \n### hostfile_4\n\n|\n\n### hostfile_4\n\n| first commit| Mar 27, 2024  \n  \n### predict.py\n\n|\n\n### predict.py\n\n| first commit| Mar 27, 2024  \n  \n### pyproject.toml\n\n|\n\n### pyproject.toml\n\n| first commit| Mar 27, 2024  \n  \n## Repository files navigation\n\n# Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models\n\nMini-Gemini supports a series of dense and MoE Large Language Models (LLMs)\nfrom 2B to 34B with image understanding, reasoning, and generation\nsimultaneously. We build this repo based on LLaVA.\n\n## Release\n\n  * [04/15] \ud83d\udd25 The Hugging Face demo is available. It's a 13B-HD version, welcome to watch and try.\n  * [03/28] \ud83d\udd25 Mini-Gemini is coming! We release the paper, demo, code, models, and data for Mini-Gemini!\n\n## Contents\n\n  * Demo\n  * Install\n  * Model\n  * Preparation\n  * Train\n  * Evaluation\n  * Examples\n  * Citation\n  * Acknowledgement\n  * License\n\n## Demo\n\nWe provide some selected examples in this section. More examples can be found\nin our project page. Feel free to try our online demo!\n\n## Install\n\nPlease follow the instructions below to install the required packages.\n\nNOTE: If you want to use Mini-Gemini-2B, please ensure to install the latest\nversion Transformers (>=4.38.0).\n\n  1. Clone this repository\n\n    \n    \n    git clone https://github.com/dvlab-research/MiniGemini.git\n\n  2. Install Package\n\n    \n    \n    conda create -n minigemini python=3.10 -y conda activate minigemini cd MiniGemini pip install --upgrade pip # enable PEP 660 support pip install -e .\n\n  3. Install additional packages for training cases\n\n    \n    \n    pip install ninja pip install flash-attn --no-build-isolation\n\n## Model\n\nThe framework of Mini-Gemini is conceptually simple: dual vision encoders are\nutilized to provide low-resolution visual embedding and high-resolution\ncandidates; patch info mining is proposed to conduct patch-level mining\nbetween high-resolution regions and low-resolution visual queries; LLM is\nutilized to marry text with images for both comprehension and generation at\nthe same time.\n\nWe provide all our fully finetuned models on Stage 1 and 2 data for Mini-\nGemini:\n\nModel| LR| HR| Base LLM| Vision Encoder| Finetuning Data| Finetuning schedule|\nDownload  \n---|---|---|---|---|---|---|---  \nMini-Gemini-2B| 336| 768| Gemma-2B| CLIP-L| MiniGemini-Instruct| full_ft-1e|\nckpt  \nMini-Gemini-7B| 336| 768| Vicuna-7B-v1.5| CLIP-L| MiniGemini-Instruct|\nfull_ft-1e| ckpt  \nMini-Gemini-13B| 336| 768| Vicuna-13B-v1.5| CLIP-L| MiniGemini-Instruct|\nfull_ft-1e| ckpt  \nMini-Gemini-8x7B| 336| 768| Mixtral-8x7B-Instruct-v0.1| CLIP-L| MiniGemini-\nInstruct| full_ft-1e| ckpt  \nMini-Gemini-34B| 336| 768| Nous-Hermes-2-Yi-34B| CLIP-L| MiniGemini-Instruct|\nfull_ft-1e| ckpt  \nMini-Gemini-7B-HD| 672| 1536| Vicuna-7B-v1.5| CLIP-L| MiniGemini-Instruct|\nfull_ft-1e| ckpt  \nMini-Gemini-13B-HD| 672| 1536| Vicuna-13B-v1.5| CLIP-L| MiniGemini-Instruct|\nfull_ft-1e| ckpt  \nMini-Gemini-8x7B-HD| 672| 1536| Mixtral-8x7B-Instruct-v0.1| CLIP-L|\nMiniGemini-Instruct| full_ft-1e| ckpt  \nMini-Gemini-34B-HD| 672| 1536| Nous-Hermes-2-Yi-34B| CLIP-L| MiniGemini-\nInstruct| full_ft-1e| ckpt  \n  \nHere are the pretrained weights on Stage 1 data only:\n\nModel| LR| HR| Base LLM| Vision Encoder| Pretrain Data| Finetuning schedule|\nDownload  \n---|---|---|---|---|---|---|---  \nMini-Gemini-2B| 336| 768| Gemma-2B| CLIP-L| MiniGemini-Pretrain| 1e| ckpt  \nMini-Gemini-7B| 336| 768| Vicuna-7B-v1.5| CLIP-L| MiniGemini-Pretrain| 1e|\nckpt  \nMini-Gemini-13B| 336| 768| Vicuna-13B-v1.5| CLIP-L| MiniGemini-Pretrain| 1e|\nckpt  \nMini-Gemini-8x7B| 336| 768| Mixtral-8x7B-Instruct-v0.1| CLIP-L| MiniGemini-\nPretrain| 1e| ckpt  \nMini-Gemini-34B| 336| 768| Nous-Hermes-2-Yi-34B| CLIP-L| MiniGemini-Pretrain|\n1e| ckpt  \n  \n## Preparation\n\n### Dataset\n\nWe provide the processed data for Mini-Gemini training. For model pretraining,\nplease download the following the training image-based data and organize them\nas:\n\n-> means put the data in the local folder.\n\n  * LLaVA Images -> data/MiniGemini-Pretrain/images, data/MiniGemini-Finetune/llava/LLaVA-Pretrain/images\n  * ALLaVA Caption -> data/MiniGemini-Pretrain/ALLaVA-4V\n\nFor model finetuning, please download the following the instruction data and\norganize them as:\n\n-> means put the data in the local folder.\n\n  * COCO train2017 -> data/MiniGemini-Finetune/coco\n  * GQA -> data/MiniGemini-Finetune/gqa\n  * OCR-VQA (we save all files as .jpg) -> data/MiniGemini-Finetune/ocr_vqa\n  * TextVQA (not included for training) -> data/MiniGemini-Finetune/textvqa\n  * VisualGenome part1, VisualGenome part2 -> data/MiniGemini-Finetune/vg\n  * ShareGPT4V-100K -> data/MiniGemini-Finetune/sam, share_textvqa, wikiart, web-celebrity, web-landmark\n  * LAION GPT4V -> data/MiniGemini-Finetune/gpt4v-dataset\n  * ALLaVA Instruction -> data/MiniGemini-Pretrain/ALLaVA-4V\n  * DocVQA -> data/MiniGemini-Finetune/docvqa\n  * ChartQA -> data/MiniGemini-Finetune/chartqa\n  * DVQA -> data/MiniGemini-Finetune/dvqa\n  * AI2D -> data/MiniGemini-Finetune/ai2d\n\nFor model evaluation, please follow this link for preparation. We use some\nextra benchmarks for evaluation. please download the following the training\nimage-based data and organize them as:\n\n-> means put the data in the local folder.\n\n  * MMMU -> data/MiniGemini-Eval/MMMU\n  * MMB -> data/MiniGemini-Eval/MMB\n  * MathVista -> data/MiniGemini-Eval/MathVista\n\nPlease put the pretrained data, finetuned data, and eval data in MiniGemini-\nPretrain, MiniGemini-Finetune, and MiniGemini-Eval subset following Structure.\n\nFor meta info, please download the following files and organize them as in\nStructure.\n\nData file name| Size  \n---|---  \nminigemini_pretrain.json| 1.68 G  \nminigemini_instruction.json| 1.79 G  \nminigemini_generation_pure_text.json| 0.04 G  \n  \nIMPORTANT: minigemini_generation_pure_text.json is a generation-related\nsubset. DO NOT merge it with minigemini_instruction.json as it is already\nincluded in it. You may merge this file with your customized LLM/VLM SFT\ndataset to enable the reasoning generation ability.\n\n### Pretrained Weights\n\nWe recommend users to download the pretrained weights from the following link\nCLIP-Vit-L-336, OpenCLIP-ConvNeXt-L, Gemma-2b-it, Vicuna-7b-v1.5,\nVicuna-13b-v1.5, Mixtral-8x7B-Instruct-v0.1, and Nous-Hermes-2-Yi-34B , and\nput them in model_zoo following Structure.\n\n### Structure\n\nThe folder structure should be organized as follows before training.\n\n    \n    \n    MiniGemini \u251c\u2500\u2500 minigemini \u251c\u2500\u2500 scripts \u251c\u2500\u2500 work_dirs \u2502 \u251c\u2500\u2500 Mini-Gemini \u2502 \u2502 \u251c\u2500\u2500 Mini-Gemini-2B \u2502 \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 model_zoo \u2502 \u251c\u2500\u2500 LLM \u2502 \u2502 \u251c\u2500\u2500 gemma \u2502 \u2502 \u2502 \u251c\u2500\u2500 gemma-2b-it \u2502 \u2502 \u251c\u2500\u2500 vicuna \u2502 \u2502 \u2502 \u251c\u2500\u2500 7B-V1.5 \u2502 \u2502 \u2502 \u251c\u2500\u2500 13B-V1.5 \u2502 \u2502 \u251c\u2500\u2500 mixtral \u2502 \u2502 \u2502 \u251c\u2500\u2500 Mixtral-8x7B-Instruct-v0.1 \u2502 \u2502 \u251c\u2500\u2500 Nous-Hermes-2-Yi-34B \u2502 \u251c\u2500\u2500 OpenAI \u2502 \u2502 \u251c\u2500\u2500 clip-vit-large-patch14-336 \u2502 \u2502 \u251c\u2500\u2500 openclip-convnext-large-d-320-laion2B-s29B-b131K-ft-soup \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 MiniGemini-Pretrain \u2502 \u2502 \u251c\u2500\u2500 minigemini_pretrain.json \u2502 \u2502 \u251c\u2500\u2500 images \u2502 \u2502 \u251c\u2500\u2500 ALLaVA-4V \u2502 \u251c\u2500\u2500 MiniGemini-Finetune \u2502 \u2502 \u251c\u2500\u2500 minigemini_instruction.json \u2502 \u2502 \u251c\u2500\u2500 llava \u2502 \u2502 \u251c\u2500\u2500 coco \u2502 \u2502 \u251c\u2500\u2500 gqa \u2502 \u2502 \u251c\u2500\u2500 ocr_vqa \u2502 \u2502 \u251c\u2500\u2500 textvqa \u2502 \u2502 \u251c\u2500\u2500 vg \u2502 \u2502 \u251c\u2500\u2500 gpt4v-dataset \u2502 \u2502 \u251c\u2500\u2500 sam \u2502 \u2502 \u251c\u2500\u2500 share_textvqa \u2502 \u2502 \u251c\u2500\u2500 wikiart \u2502 \u2502 \u251c\u2500\u2500 web-celebrity \u2502 \u2502 \u251c\u2500\u2500 web-landmark \u2502 \u2502 \u251c\u2500\u2500 ALLaVA-4V \u2502 \u2502 \u251c\u2500\u2500 docvqa \u2502 \u2502 \u251c\u2500\u2500 chartqa \u2502 \u2502 \u251c\u2500\u2500 dvqa \u2502 \u2502 \u251c\u2500\u2500 ai2d \u2502 \u251c\u2500\u2500 MiniGemini-Eval \u2502 \u2502 \u251c\u2500\u2500 MMMU \u2502 \u2502 \u251c\u2500\u2500 MMB \u2502 \u2502 \u251c\u2500\u2500 MathVista \u2502 \u2502 \u251c\u2500\u2500 ...\n\n## Train\n\nMini-Gemini training consists of two stages: (1) feature alignment stage:\nbridge the vision and language tokens; (2) instruction tuning stage: teach the\nmodel to follow multimodal instructions.\n\nMini-Gemini is trained on 8 A100 GPUs with 80GB memory. To train on fewer\nGPUs, you can reduce the per_device_train_batch_size and increase the\ngradient_accumulation_steps accordingly. Always keep the global batch size the\nsame: per_device_train_batch_size x gradient_accumulation_steps x num_gpus.\n\nPlease make sure you download and organize the data following Preparation\nbefore training.\n\nNOTE: Please set hostfile for 2 machine training and hostfile_4 for 4 machine\ntraining.\n\nIf you want to train and finetune Mini-Gemini, please run the following\ncommand for Mini-Gemini-7B with image size 336:\n\n    \n    \n    bash scripts/llama/train/stage_1_2_full_v7b_336_hr_768.sh\n\nor for Mini-Gemini-13B with image size 336:\n\n    \n    \n    bash scripts/llama/train/stage_1_2_full_v13b_336_hr_768.sh\n\nBecause we reuse the pre-trained projecter weights from the Mini-Gemini-7B,\nyou can directly use the Mini-Gemini-7B-HD with image size 672 for stage-2\ninstruction tuning:\n\n    \n    \n    bash scripts/llama/train/stage_2_full_v7b_672_hr_1536.sh\n\nPlease find more training scripts of gemma, llama, mixtral, and yi in\nscripts/.\n\n## Evaluation\n\nWe perform evaluation on several image-based benchmarks. Please download the\nevaluation data following Preparation and organize them as in Structure.\n\nModel| LLM| Res.| Link| TextVQA| MMB| MME| MM-Vet| MMMU_val| MMMU_test|\nMathVista  \n---|---|---|---|---|---|---|---|---|---|---  \nMini-Gemini-2B| Gemma-2B| 336| ckpt| 56.2| 59.8| 1341/312| 31.1| 31.7| 29.1|\n29.4  \nMini-Gemini-7B| Vicuna-7B-v1.5| 336| ckpt| 65.2| 69.3| 1523/316| 40.8| 36.1|\n32.8| 31.4  \nMini-Gemini-13B| Vicuna-13B-v1.5| 336| ckpt| 65.9| 68.5| 1565/322| 46.0| 38.1|\n33.5| 37.0  \nMini-Gemini-8x7B| Mixtral-8x7B-Instruct-v0.1| 336| ckpt| 69.2| 75.6| 1639/379|\n45.8| 41.8| 37.1| 41.8  \nMini-Gemini-34B| Nous-Hermes-2-Yi-34B| 336| ckpt| 70.1| 79.6| 1666/439| 53.0|\n48.7| 43.6| 38.9  \nMini-Gemini-7B-HD| Vicuna-7B-v1.5| 672| ckpt| 68.4| 65.8| 1546/319| 41.3|\n36.8| 32.9| 32.2  \nMini-Gemini-13B-HD| Vicuna-13B-v1.5| 672| ckpt| 70.2| 68.6| 1597/320| 50.5|\n37.3| 35.1| 37.0  \nMini-Gemini-8x7B-HD| Mixtral-8x7B-Instruct-v0.1| 672| ckpt| 71.9| 74.7|\n1633/356| 53.5| 40.0| 37.0| 43.1  \nMini-Gemini-34B-HD| Nous-Hermes-2-Yi-34B| 672| ckpt| 74.1| 80.6| 1659/482|\n59.3| 48.0| 44.9| 43.3  \n  \nIf you want to evaluate the model on image-based benchmarks, please use the\nscripts in scripts/MODEL_PATH/eval. For example, run the following command for\nTextVQA evaluation with Mini-Gemini-7B-HD:\n\n    \n    \n    bash scripts/llama/eval/textvqa.sh\n\nPlease find more evaluation scripts in scripts/MODEL_PATH.\n\n### CLI Inference\n\nChat with images using Mini-Gemini without the need of Gradio interface. It\nalso supports multiple GPUs, 4-bit and 8-bit quantized inference. With 4-bit\nquantization. Please make sure you have installed diffusers and PaddleOCR\n(only for better experience with OCR), and try this for image and generation\ninference:\n\n    \n    \n    python -m minigemini.serve.cli \\ --model-path work_dirs/Mini-Gemini/Mini-Gemini-13B-HD \\ --image-file <path to your image>\n\nor try this better experience with OCR (make sure you have installed\nPaddleOCR):\n\n    \n    \n    python -m minigemini.serve.cli \\ --model-path work_dirs/Mini-Gemini/Mini-Gemini-13B-HD \\ --image-file <path to your image> \\ --ocr\n\nor try this for inference with generation (make sure you have installed\ndiffusers):\n\n    \n    \n    python -m minigemini.serve.cli \\ --model-path work_dirs/Mini-Gemini/Mini-Gemini-13B-HD \\ --image-file <path to your image> \\ --gen\n\nYou can also try 8bit or even 4bit for efficient inference\n\n    \n    \n    python -m minigemini.serve.cli \\ --model-path work_dirs/Mini-Gemini/Mini-Gemini-13B-HD \\ --image-file <path to your image> \\ --gen --load-8bit\n\n### Gradio Web UI\n\nHere, we adopt the Gradio UI similar to that in LLaVA to provide a user-\nfriendly interface for Mini-Gemini. To launch a Gradio demo locally, please\nrun the following commands one by one. If you plan to launch multiple model\nworkers to compare between different checkpoints, you only need to launch the\ncontroller and the web server ONCE.\n\n#### Launch a controller\n\n    \n    \n    python -m minigemini.serve.controller --host 0.0.0.0 --port 10000\n\n#### Launch a gradio web server.\n\n    \n    \n    python -m minigemini.serve.gradio_web_server --controller http://localhost:10000 --model-list-mode reload\n\nYou just launched the Gradio web interface. Now, you can open the web\ninterface with the URL printed on the screen. You may notice that there is no\nmodel in the model list. Do not worry, as we have not launched any model\nworker yet. It will be automatically updated when you launch a model worker.\n\n#### Launch a model worker\n\nThis is the actual worker that performs the inference on the GPU. Each worker\nis responsible for a single model specified in --model-path.\n\n    \n    \n    python -m minigemini.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path work_dirs/Mini-Gemini/Mini-Gemini-13B-HD\n\nWait until the process finishes loading the model and you see \"Uvicorn running\non ...\". Now, refresh your Gradio web UI, and you will see the model you just\nlaunched in the model list.\n\nYou can launch as many workers as you want, and compare between different\nmodels in the same Gradio interface. Please keep the --controller the same,\nand modify the --port and --worker to a different port number for each worker.\n\n    \n    \n    python -m minigemini.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port <different from 40000, say 40001> --worker http://localhost:<change accordingly, i.e. 40001> --model-path work_dirs/Mini-Gemini/Mini-Gemini-34B-HD\n\nIf you are using an Apple device with an M1 or M2 chip, you can specify the\nmps device by using the --device flag: --device mps.\n\n#### Launch a model worker (Multiple GPUs, when GPU VRAM <= 24GB)\n\nIf the VRAM of your GPU is less than 24GB (e.g., RTX 3090, RTX 4090, etc.),\nyou may try running it with multiple GPUs. Our latest code base will\nautomatically try to use multiple GPUs if you have more than one GPU. You can\nspecify which GPUs to use with CUDA_VISIBLE_DEVICES. Below is an example of\nrunning with the first two GPUs.\n\n    \n    \n    CUDA_VISIBLE_DEVICES=0,1 python -m minigemini.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path work_dirs/Mini-Gemini/Mini-Gemini-13B-HD\n\n#### Launch a model worker (4-bit, 8-bit inference, quantized)\n\nYou can launch the model worker with quantized bits (4-bit, 8-bit), which\nallows you to run the inference with reduced GPU memory footprint. Note that\ninference with quantized bits may not be as accurate as the full-precision\nmodel. Simply append --load-4bit or --load-8bit to the model worker command\nthat you are executing. Below is an example of running with 4-bit\nquantization.\n\n    \n    \n    python -m minigemini.serve.model_worker --host 0.0.0.0 --controller http://localhost:10000 --port 40000 --worker http://localhost:40000 --model-path work_dirs/Mini-Gemini/Mini-Gemini-13B-HD --load-4bit\n\n## Examples\n\nWe provide some examples in this section. More examples can be found in our\nproject page.\n\n### Hi-Resolution Understanding\n\n### Generation with Reasoning\n\n## Citation\n\nIf you find this repo useful for your research, please consider citing the\npaper\n\n    \n    \n    @article{li2024minigemini, title={Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models}, author={Li, Yanwei and Zhang, Yuechen and Wang, Chengyao and Zhong, Zhisheng and Chen, Yixin and Chu, Ruihang and Liu, Shaoteng and Jia, Jiaya}, journal={arXiv:2403.18814}, year={2023} }\n\n## Acknowledgement\n\nWe would like to thank the following repos for their great work:\n\n  * This work is built upon the LLaVA.\n  * This work utilizes LLMs from Gemma, Vicuna, Mixtral, and Nous-Hermes.\n\n## License\n\nThe data and checkpoint is intended and licensed for research use only. They\nare also restricted to uses that follow the license agreement of LLaVA, LLaMA,\nVicuna and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial\nuse) and models trained using the dataset should not be used outside of\nresearch purposes.\n\n## About\n\nOfficial implementation for Mini-Gemini\n\nmini-gemini.github.io/\n\n### Topics\n\ngeneration large-language-models vision-language-model\n\n### Resources\n\nReadme\n\n### License\n\nApache-2.0 license\n\nActivity\n\nCustom properties\n\n### Stars\n\n1.6k stars\n\n### Watchers\n\n18 watching\n\n### Forks\n\n103 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Contributors 5\n\n## Languages\n\n  * Python 86.5%\n  * Shell 10.0%\n  * JavaScript 1.8%\n  * HTML 1.4%\n  * CSS 0.3%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
