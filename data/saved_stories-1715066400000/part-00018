{"aid": "40280930", "title": "DeepSeek-V2: A Strong, Economical, and Efficient Moe Language Model", "url": "https://github.com/deepseek-ai/DeepSeek-V2", "domain": "github.com/deepseek-ai", "votes": 7, "user": "jasondavies", "posted_at": "2024-05-07 00:10:39", "comments": 0, "source_title": "GitHub - deepseek-ai/DeepSeek-V2", "source_text": "GitHub - deepseek-ai/DeepSeek-V2\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\ndeepseek-ai / DeepSeek-V2 Public\n\n  * Notifications\n  * Fork 3\n  * Star 140\n\n### License\n\n140 stars 3 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# deepseek-ai/DeepSeek-V2\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nluofuliUpdate README.mdMay 6, 2024e23eeb5 \u00b7 May 6, 2024May 6, 2024\n\n## History\n\n5 Commits  \n  \n### figures\n\n|\n\n### figures\n\n| Update figure| May 6, 2024  \n  \n### LICENSE-CODE\n\n|\n\n### LICENSE-CODE\n\n| chore: rebase commits| May 6, 2024  \n  \n### LICENSE-MODEL\n\n|\n\n### LICENSE-MODEL\n\n| chore: rebase commits| May 6, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update README.md| May 6, 2024  \n  \n### deepseek-v2-tech-report.pdf\n\n|\n\n### deepseek-v2-tech-report.pdf\n\n| chore: rebase commits| May 6, 2024  \n  \n## Repository files navigation\n\nModel Download | Evaluation Results | Model Architecture | API Platform | License | Citation\n\nPaper Link\ud83d\udc41\ufe0f\n\n# DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language\nModel\n\n## 1\\. Introduction\n\nToday, we\u2019re introducing DeepSeek-V2, a strong Mixture-of-Experts (MoE)\nlanguage model characterized by economical training and efficient inference.\nIt comprises 236B total parameters, of which 21B are activated for each token.\nCompared with DeepSeek 67B, DeepSeek-V2 achieves stronger performance, and\nmeanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and\nboosts the maximum generation throughput to 5.76 times.\n\nWe pretrained DeepSeek-V2 on a diverse and high-quality corpus comprising 8.1\ntrillion tokens. This comprehensive pretraining was followed by a process of\nSupervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unleash\nthe model's capabilities. The evaluation results validate the effectiveness of\nour approach as DeepSeek-V2 achieves remarkable performance on both standard\nbenchmarks and open-ended generation evaluation.\n\n## 2\\. Model Downloads\n\nModel| Context Length| Download  \n---|---|---  \nDeepSeek-V2| 128k| \ud83e\udd17 HuggingFace  \nDeepSeek-V2-Chat(RL)| 128k| \ud83e\udd17 HuggingFace  \n  \nDue to the constraints of HuggingFace, the open-source code currently\nexperiences slower performance than our internal codebase when running on GPUs\nwith Huggingface. To facilitate the efficient execution of our model, we offer\na dedicated vllm solution that optimizes performance for running our model\neffectively.\n\n## 3\\. Evaluation Results\n\n### Base Model\n\n#### Standard Benchmark\n\nBenchmark| Domain| LLaMA3 70B| Mixtral 8x22B| DeepSeek V1 (Dense-67B)|\nDeepSeek V2 (MoE-236B)  \n---|---|---|---|---|---  \nMMLU| English| 78.9| 77.6| 71.3| 78.5  \nBBH| English| 81.0| 78.9| 68.7| 78.9  \nC-Eval| Chinese| 67.5| 58.6| 66.1| 81.7  \nCMMLU| Chinese| 69.3| 60.0| 70.8| 84.0  \nHumanEval| Code| 52.4| 39.0| 42.7| 40.9  \nMBPP| Code| 68.6| 64.2| 57.4| 66.6  \nGSM8K| Math| 83.0| 80.3| 63.4| 79.2  \nMath| Math| 42.2| 42.5| 18.7| 43.6  \n  \nFor more evaluation details, such as few-shot settings and prompts, please\ncheck our paper.\n\n#### Context Window\n\nEvaluation results on the Needle In A Haystack (NIAH) tests. DeepSeek-V2\nperforms well across all context window lengths up to 128K.\n\n### Chat Model\n\n#### Standard Benchmark\n\nBenchmark| Domain| QWen1.5 72B Chat| Mixtral 8x22B| LLaMA3 70B Instruct|\nDeepSeek V1 Chat (SFT)| DeepSeek V2 Chat(SFT)| DeepSeek V2 Chat(RL)  \n---|---|---|---|---|---|---|---  \nMMLU| English| 76.2| 77.8| 80.3| 71.1| 78.4| 77.8  \nBBH| English| 65.9| 78.4| 80.1| 71.7| 81.3| 79.7  \nC-Eval| Chinese| 82.2| 60.0| 67.9| 65.2| 80.9| 78.0  \nCMMLU| Chinese| 82.9| 61.0| 70.7| 67.8| 82.4| 81.6  \nHumanEval| Code| 68.9| 75.0| 76.2| 73.8| 76.8| 81.1  \nMBPP| Code| 52.2| 64.4| 69.8| 61.4| 70.4| 72.0  \nLiveCodeBench (0901-0401)| Code| 18.8| 25.0| 30.5| 18.3| 28.7| 32.5  \nGSM8K| Math| 81.9| 87.9| 93.2| 84.1| 90.8| 92.2  \nMath| Math| 40.6| 49.8| 48.5| 32.6| 52.7| 53.9  \n  \n#### English Open Ended Generation Evaluation\n\nWe evaluate our model on AlpacaEval 2.0 and MTBench, showing the competitive\nperformance of DeepSeek-V2-Chat-RL on English conversation generation.\n\n#### Chinese Open Ended Generation Evaluation\n\nAlignbench (https://arxiv.org/abs/2311.18743)\n\n\u6a21\u578b| \u5f00\u6e90/\u95ed\u6e90| \u603b\u5206| \u4e2d\u6587\u63a8\u7406| \u4e2d\u6587\u8bed\u8a00  \n---|---|---|---|---  \ngpt-4-1106-preview| \u95ed\u6e90| 8.01| 7.73| 8.29  \nDeepSeek-V2 Chat(RL)| \u5f00\u6e90| 7.91| 7.45| 8.36  \nerniebot-4.0-202404(\u6587\u5fc3\u4e00\u8a00)| \u95ed\u6e90| 7.89| 7.61| 8.17  \nDeepSeek-V2 Chat(SFT)| \u5f00\u6e90| 7.74| 7.30| 8.17  \ngpt-4-0613| \u95ed\u6e90| 7.53| 7.47| 7.59  \nerniebot-4.0-202312(\u6587\u5fc3\u4e00\u8a00)| \u95ed\u6e90| 7.36| 6.84| 7.88  \nmoonshot-v1-32k-202404(\u6708\u4e4b\u6697\u9762)| \u95ed\u6e90| 7.22| 6.42| 8.02  \nQwen1.5-72B-Chat(\u901a\u4e49\u5343\u95ee)| \u5f00\u6e90| 7.19| 6.45| 7.93  \nDeepSeek-67B-Chat| \u5f00\u6e90| 6.43| 5.75| 7.11  \nYi-34B-Chat(\u96f6\u4e00\u4e07\u7269)| \u5f00\u6e90| 6.12| 4.86| 7.38  \ngpt-3.5-turbo-0613| \u95ed\u6e90| 6.08| 5.35| 6.71  \n  \n#### Coding Benchmarks\n\nWe evaluate our model on LiveCodeBench (0901-0401), a benchmark designed for\nlive coding challenges. As illustrated, DeepSeek-V2 demonstrates considerable\nproficiency in LiveCodeBench, achieving a Pass@1 score that surpasses several\nother sophisticated models. This performance highlights the model's\neffectiveness in tackling live coding tasks.\n\n## 4\\. Model Architecture\n\nDeepSeek-V2 adopts innovative architectures to guarantee economical training\nand efficient inference:\n\n  * For attention, we design MLA (Multi-head Latent Attention), which utilizes low-rank key-value union compression to eliminate the bottleneck of inference-time key-value cache, thus supporting efficient inference.\n  * For Feed-Forward Networks (FFNs), we adopt DeepSeekMoE architecture, a high-performance MoE architecture that enables training stronger models at lower costs.\n\n## 5\\. Chat Website\n\nYou can chat with the DeepSeek-V2 on DeepSeek's official website:\nchat.deepseek.com\n\n## 6\\. API Platform\n\nWe also provide OpenAI-Compatible API at DeepSeek Platform:\nplatform.deepseek.com. Sign up for over millions of free tokens. And you can\nalso pay-as-you-go at an unbeatable price.\n\n## 7\\. How to run locally\n\nTo utilize DeepSeek-V2 in BF16 format for inference, 80GB*8 GPUs are required.\n\n### Inference with Huggingface's Transformers\n\nYou can directly employ Huggingface's Transformers for model inference.\n\n### Text Completion\n\n    \n    \n    import torch from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig model_name = \"deepseek-ai/DeepSeek-V2\" tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) # `max_memory` should be set based on your devices max_memory = {i: \"75GB\" for i in range(8)} model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, device_map=\"auto\", torch_dtype=torch.bfloat16, max_memory=max_memory) model.generation_config = GenerationConfig.from_pretrained(model_name) model.generation_config.pad_token_id = model.generation_config.eos_token_id text = \"An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is\" inputs = tokenizer(text, return_tensors=\"pt\") outputs = model.generate(**inputs.to(model.device), max_new_tokens=100) result = tokenizer.decode(outputs[0], skip_special_tokens=True) print(result)\n\n### Chat Completion\n\n    \n    \n    import torch from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig model_name = \"deepseek-ai/DeepSeek-V2-Chat\" tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True) # `max_memory` should be set based on your devices max_memory = {i: \"75GB\" for i in range(8)} model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True, device_map=\"auto\", torch_dtype=torch.bfloat16, max_memory=max_memory) model.generation_config = GenerationConfig.from_pretrained(model_name) model.generation_config.pad_token_id = model.generation_config.eos_token_id messages = [ {\"role\": \"user\", \"content\": \"Write a piece of quicksort code in C++\"} ] input_tensor = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\") outputs = model.generate(input_tensor.to(model.device), max_new_tokens=100) result = tokenizer.decode(outputs[0][input_tensor.shape[1]:], skip_special_tokens=True) print(result)\n\nThe complete chat template can be found within tokenizer_config.json located\nin the huggingface model repository.\n\nAn example of chat template is as belows:\n\n    \n    \n    <|begin\u2581of\u2581sentence|>User: {user_message_1} Assistant: {assistant_message_1}<|end\u2581of\u2581sentence|>User: {user_message_2} Assistant:\n\nYou can also add an optional system message:\n\n    \n    \n    <|begin\u2581of\u2581sentence|>{system_message} User: {user_message_1} Assistant: {assistant_message_1}<|end\u2581of\u2581sentence|>User: {user_message_2} Assistant:\n\n## 8\\. License\n\nThis code repository is licensed under the MIT License. The use of DeepSeek-V2\nBase/Chat models is subject to the Model License. DeepSeek-V2 series\n(including Base and Chat) supports commercial use.\n\n## 9\\. Citation\n\n    \n    \n    @misc{deepseek-v2, author = {DeepSeek-AI}, title = {DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model}, year = {2024}, note = {GitHub repository}, url = {https://github.com/deepseek-ai/deepseek-v2} }\n\n## 10\\. Contact\n\nIf you have any questions, please raise an issue or contact us at\nservice@deepseek.com.\n\n## About\n\nNo description, website, or topics provided.\n\n### Resources\n\nReadme\n\n### License\n\nActivity\n\nCustom properties\n\n### Stars\n\n140 stars\n\n### Watchers\n\n3 watching\n\n### Forks\n\n3 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Contributors 2\n\n  * luofuli Fuli Luo\n  * pkuzqh ZHU QIHAO\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": true}
