{"aid": "40256566", "title": "System and method for parallelizing convolutional neural networks", "url": "https://patents.google.com/patent/US11928577B2/en", "domain": "patents.google.com", "votes": 1, "user": "ofou", "posted_at": "2024-05-04 10:47:04", "comments": 0, "source_title": "US11928577B2 - System and method for parallelizing convolutional neural networks - Google Patents", "source_text": "US11928577B2 - System and method for parallelizing convolutional neural\nnetworks - Google Patents\n\n#\n\nPatents\n\nInclude patents\n\nInclude non-patent literature\n\nSearch within\n\nSearch within the title, abstract, claims, or full patent document: You can\nrestrict your search to a specific field using field names.\n\nUse TI= to search in the title, AB= for the abstract, CL= for the claims, or\nTAC= for all three. For example, TI=(safety belt).\n\nSearch by Cooperative Patent Classifications (CPCs): These are commonly used\nto represent ideas in place of keywords, and can also be entered in a search\nterm box. If you're searching forseat belts, you could also search for\nB60R22/00 to retrieve documents that mention safety belts or body harnesses.\nCPC=B60R22 will match documents with exactly this CPC, CPC=B60R22/low matches\ndocuments with this CPC or a child classification of this CPC.\n\nLearn More\n\nTitle Abstract Claims Full Document\n\nFind patents\n\nKeywords and boolean syntax (USPTO or EPO format): seat belt searches these\ntwo words, or their plurals and close synonyms. \"seat belt\" searches this\nexact phrase, in order. -seat -belt searches for documents not containing\neither word.\n\nFor searches using boolean logic, the default operator is AND with left\nassociativity. Note: this means safety OR seat belt is searched as (safety OR\nseat) AND belt. Each word automatically includes plurals and close synonyms.\nAdjacent words that are implicitly ANDed together, such as (safety belt), are\ntreated as a phrase when generating synonyms.\n\nLearn More\n\nSearch by\n\nChemistry searches match terms (trade names, IUPAC names, etc. extracted from\nthe entire document, and processed from .MOL files.)\n\nSubstructure (use SSS=) and similarity (use ~) searches are limited to one per\nsearch at the top-level AND condition. Exact searches can be used multiple\ntimes throughout the search query.\n\nSearching by SMILES or InChi key requires no special syntax. To search by\nSMARTS, use SMARTS=.\n\nTo search for multiple molecules, select \"Batch\" in the \"Type\" menu. Enter\nmultiple molecules separated by whitespace or by comma.\n\nLearn More\n\nPatent numbers\n\nSearch specific patents by importing a CSV or list of patent publication or\napplication numbers.\n\nDisplay advanced search options\n\nSorry, we couldn't find this patent number.\n\nof 0\n\nPrevious result\n\nNext result\n\nSearch tools Text Classification Chemistry Measure Numbers Full documents\nTitle Abstract Claims All Any Exact Not Add AND condition These CPCs and their\nchildren These exact CPCs Add AND condition\n\nExact Exact Batch Similar Substructure Substructure (SMARTS) Full documents\nClaims only Add AND condition Add AND condition\n\nApplication Numbers Publication Numbers Either Add AND condition\n\n# System and method for parallelizing convolutional neural networks\n\n###\n\nAbstract\n\ntranslated from\n\nA parallel convolutional neural network is provided. The CNN is implemented by\na plurality of convolutional neural networks each on a respective processing\nnode. Each CNN has a plurality of layers. A subset of the layers are\ninterconnected between processing nodes such that activations are fed forward\nacross nodes. The remaining subset is not so interconnected.\n\n### Images (5)\n\n###\n\nClassifications\n\nmachine-classified\n\ncpc-machine-classified\n\nfterm-machine-classified\n\nfterm-family-classified\n\nThe classifications are assigned by a computer and are not a legal conclusion.\nGoogle has not performed a legal analysis and makes no representation as to\nthe accuracy of the classifications listed.\n\nThe CPC classifications are assigned by a computer and are not a legal\nconclusion. Google has not performed a legal analysis and makes no\nrepresentation as to the accuracy of the classifications listed.\n\nThe F-term classifications are assigned based on a patent family member\ncontaining these classification codes.\n\nThe F-term classifications are assigned by a computer and are not a legal\nconclusion. Google has not performed a legal analysis and makes no\nrepresentation as to the accuracy of the classifications listed.\n\nG PHYSICS\n\nG06 COMPUTING; CALCULATING OR COUNTING\n\nG06N COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS\n\nG06N3/00 Computing arrangements based on biological models\n\nG06N3/02 Neural networks\n\nG06N3/06 Physical realisation, i.e. hardware implementation of neural\nnetworks, neurons or parts of neurons\n\nG06N3/063 Physical realisation, i.e. hardware implementation of neural\nnetworks, neurons or parts of neurons using electronic means\n\nG PHYSICS\n\nG06 COMPUTING; CALCULATING OR COUNTING\n\nG06F ELECTRIC DIGITAL DATA PROCESSING\n\nG06F18/00 Pattern recognition\n\nG06F18/20 Analysing\n\nG06F18/21 Design or setup of recognition systems or techniques; Extraction of\nfeatures in feature space; Blind source separation\n\nG06F18/214 Generating training patterns; Bootstrap methods, e.g. bagging or\nboosting\n\nG PHYSICS\n\nG06 COMPUTING; CALCULATING OR COUNTING\n\nG06N COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS\n\nG06N3/00 Computing arrangements based on biological models\n\nG06N3/02 Neural networks\n\nG06N3/04 Architecture, e.g. interconnection topology\n\nG PHYSICS\n\nG06 COMPUTING; CALCULATING OR COUNTING\n\nG06N COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS\n\nG06N3/00 Computing arrangements based on biological models\n\nG06N3/02 Neural networks\n\nG06N3/04 Architecture, e.g. interconnection topology\n\nG06N3/045 Combinations of networks\n\nG PHYSICS\n\nG06 COMPUTING; CALCULATING OR COUNTING\n\nG06N COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS\n\nG06N3/00 Computing arrangements based on biological models\n\nG06N3/02 Neural networks\n\nG06N3/08 Learning methods\n\nG PHYSICS\n\nG06 COMPUTING; CALCULATING OR COUNTING\n\nG06T IMAGE DATA PROCESSING OR GENERATION, IN GENERAL\n\nG06T1/00 General purpose image data processing\n\nG06T1/20 Processor architectures; Processor configuration, e.g. pipelining\n\nG PHYSICS\n\nG06 COMPUTING; CALCULATING OR COUNTING\n\nG06V IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING\n\nG06V10/00 Arrangements for image or video recognition or understanding\n\nG06V10/40 Extraction of image or video features\n\nG06V10/44 Local feature extraction by analysis of parts of the pattern, e.g.\nby detecting edges, contours, loops, corners, strokes or intersections;\nConnectivity analysis, e.g. of connected components\n\nG06V10/443 Local feature extraction by analysis of parts of the pattern, e.g.\nby detecting edges, contours, loops, corners, strokes or intersections;\nConnectivity analysis, e.g. of connected components by matching or filtering\n\nG06V10/449 Biologically inspired filters, e.g. difference of Gaussians [DoG]\nor Gabor filters\n\nG06V10/451 Biologically inspired filters, e.g. difference of Gaussians [DoG]\nor Gabor filters with interaction between the filter responses, e.g. cortical\ncomplex cells\n\nG06V10/454 Integrating the filters into a hierarchical structure, e.g.\nconvolutional neural networks [CNN]\n\nView 6 more classifications\n\nHide more classifications\n\n## US11928577B2\n\nUnited States\n\nPatent ( having previously published pre-grant publication)\n\nDownload PDF Find Prior Art Similar\n\nInventor\n\n    Alexander Krizhevsky\n    Ilya Sutskever\n    Geoffrey E. Hinton\n\nCurrent Assignee\n\nThe listed assignees may be inaccurate. Google has not performed a legal\nanalysis and makes no representation or warranty as to the accuracy of the\nlist.\n\n    DNNresearch Inc\n    Google LLC\n\nWorldwide applications\n\n2013 US\n\nApplication number: US14/030,938\n\nFiling date: 2013-09-18\n\nLegal status: Active\n\nTW\n\nApplication number: TW102147804A\n\nFiling date: 2013-12-23\n\nLegal status:\n\nAU\n\nApplication number: AU2013370514A\n\nFiling date: 2013-12-23\n\nLegal status: Abandoned\n\nWO\n\nApplication number: PCT/US2013/077606\n\nFiling date: 2013-12-23\n\nLegal status: Application Filing\n\n2015 US\n\nApplication number: US14/817,492\n\nFiling date: 2015-08-04\n\nLegal status: Active\n\n2017 US\n\nApplication number: US15/414,514\n\nFiling date: 2017-01-24\n\nLegal status: Active\n\n2020 US\n\nApplication number: US16/859,815\n\nFiling date: 2020-04-27\n\nLegal status: Active\n\nApplication US16/859,815 events\n\nA timeline of key events for this patent application, including priority\nclaims, publications, legal status, reassignments, and litigation.\n\nGoogle has not performed a legal analysis and makes no representation as to\nthe accuracy or completeness of the events listed.\n\n2020-04-27\n\nApplication filed by Google LLC\n\n2020-04-27\n\nPriority to US16/859,815\n\n2020-10-15\n\nPublication of US20200327391A1\n\n2024-03-12\n\nApplication granted\n\n2024-03-12\n\nPublication of US11928577B2\n\nStatus\n\nActive\n\n2036-04-03\n\nAdjusted expiration\n\nShow all events\n\nInfo\n\n    Patent citations (13)\n    Non-patent citations (6)\n    Cited by (117)\n    Legal events\n    Similar documents\n    Priority and Related Applications\nExternal links\n\n    USPTO\n    USPTO PatentCenter\n    USPTO Assignment\n    Espacenet\n    Global Dossier\n    Discuss\n\n###\n\nDescription\n\ntranslated from\n\nCROSS-REFERENCE TO RELATED APPLICATIONS\n\nThis is a continuation of U.S. application Ser. No. 15/414,514, filed on Jan.\n24, 2017, which is a continuation of U.S. application Ser. No. 14/817,492,\nfiled on Aug. 4, 2015 (now U.S. Pat. No. 9,563,840), which is a continuation\nof U.S. application Ser. No. 14/030,938, filed on Sep. 18, 2013 (now U.S. Pat.\nNo. 9,811,775), which claims the benefit of priority under 35 U.S.C. \u00a7 119(e)\nof U.S. Provisional Application No. 61/745,717, filed on Dec. 24, 2012. The\ndisclosures of the prior applications are considered part of and are\nincorporated by reference in the disclosure of this application.\n\nTECHNICAL FIELD\n\nThe following relates generally to convolutional neural networks and more\nspecifically to a parallel convolutional neural network.\n\nBACKGROUND\n\nConvolutional neural networks (CNNs) are powerful models that can be\nconfigured to be well suited for computer vision problems. CNNs typically\nperform best when they are large (i.e., more complex), meaning that they have\nmore, deeper and highly-interconnected layers. A primary drawback of these\nCNNs is computational cost. Thus, large CNNs are typically impractically slow.\nIn many applications, a large CNN demands more computation than is currently\navailable on a serial computer.\n\nComplex CNNs can, therefore, be implemented by parallelizing the network\nacross multiple processors. For example, for image processing or\nclassification tasks, a CNN could be implemented on several graphics\nprocessing units (GPUs).\n\nThere have been various proposals to increase the practicality of CNNs by\nmeans of parallelizing the CNN across several processors. Such approaches\npartition the network into parallel subnetworks in such a way as to minimize\ncommunication cost.\n\nA first approach naively partitions the network into parallel subnetworks and\ncommunicates the state of every subnetwork's layer to all other subnetworks.\nThis approach may be applied to both CNNs and fully connected networks.\n\nIn this approach, the network is partitioned into some number of parallel\nsubnetworks in some manner. The subnetworks communicate their activations to\nall other subnetworks at every layer, which results in a parallel\nimplementation of the feedforward neural network.\n\nIn certain implementations, however, this approach is inefficient with CNNs.\nIts efficiency is best for fully connected weight matrices because the amount\nof computation required by such matrices causes the communication-to-\ncomputation ratio to be small.\n\nIn contrast, CNN weight matrices are much sparser, so their communication-to-\ncomputation ratio is much larger. As a result, when applying this approach to\nCNNs, a large fraction of time is spent on communication, which makes the\nparallelization less useful.\n\nA second approach partitions the network into slices that communicate with\ntheir neighbors, and is commonly only applied to convolutional or to locally\nconnected networks. However, current implementations of this approach\ntypically handle pooling inefficiently. Pooling is a technique for making the\nnetwork's activations more invariant to small translations. While pooling\nincreases the accuracy of the CNN, it changes the dimensions of the activation\ntensor in such a way that typically allows for less parallelism and requires\nincreased communication for the second approach.\n\nFor example, one particular implementation of the second approach parallelizes\nCNNs into slices that communicate only with their neighbours. The approach\npartitions the input tensor (of size N\u00d7N\u00d7u) into m subtensors of size\n(N/m)\u00d7N\u00d7u and allocates a computing node to each of the m subtensors. This is\nefficient only when N is large and u is small, because a large N allows m, and\nthus the number of computing nodes, to be large, and a small u allows the\nneighbouring slices to not communicate much. However, when pooling is used, N\nis necessarily small and u is necessarily large. Since m cannot exceed N, a\nsmall N restricts the number of computing nodes which limits the attainable\nacceleration, while a large u requires more communication between neighbouring\nslices which increases the communicaiton cost.\n\nIt is an object of the following to obviate or mitigate at least one of the\nforegoing issues.\n\nSUMMARY\n\nIn one aspect, a parallel convolutional neural network is provided, the\nparallel convolutional neural network comprising a plurality of convolutional\nneural networks each implemented on a respective processing node and each\ncomprising a plurality of layers, a subset of said layers being interconnected\nbetween said processing nodes to feed forward respective activations and a\nremaining subset not being so interconnected.\n\nIn another aspect, a method for establishing a parallel convolutional neural\nnetwork is provided.\n\nDESRIPTION OF THE DRAWINGS\n\nThe features of the invention will become more apparent in the following\ndetailed description in which reference is made to the appended drawings\nwherein:\n\nFIG. 1 is a four-layer feedforward neural network;\n\nFIG. 2 is a parallel convolutional neural network;\n\nFIG. 3 is a flowchart depicting a method of establishing a parallel\nconvolutional neural network; and\n\nFIG. 4 is a neural network comprising a parallel convolutional neural network.\n\nDETAILED DESCRIPTION\n\nEmbodiments will now be described with reference to the figures. It will be\nappreciated that for simplicity and clarity of illustration, where considered\nappropriate, reference numerals may be repeated among the figures to indicate\ncorresponding or analogous elements. In addition, numerous specific details\nare set forth in order to provide a thorough understanding of the embodiments\ndescribed herein. However, it will be understood by those of ordinary skill in\nthe art that the embodiments described herein may be practiced without these\nspecific details. In other instances, well-known methods, procedures and\ncomponents have not been described in detail so as not to obscure the\nembodiments described herein. Also, the description is not to be considered as\nlimiting the scope of the embodiments described herein.\n\nIt will also be appreciated that any module, unit, component, server,\ncomputer, terminal or device exemplified herein that executes instructions may\ninclude or otherwise have access to computer readable media such as storage\nmedia, computer storage media, or data storage devices (removable and/or non-\nremovable) such as, for example, magnetic disks, optical disks, or tape.\nComputer storage media may include volatile and non-volatile, removable and\nnon-removable media implemented in any method or technology for storage of\ninformation, such as computer readable instructions, data structures, program\nmodules, or other data. Examples of computer storage media include RAM, ROM,\nEEPROM, flash memory or other memory technology, CD-ROM, digital versatile\ndisks (DVD) or other optical storage, magnetic cassettes, magnetic tape,\nmagnetic disk storage or other magnetic storage devices, or any other medium\nwhich can be used to store the desired information and which can be accessed\nby an application, module, or both. Any such computer storage media may be\npart of the device or accessible or connectable thereto. Any application or\nmodule herein described may be implemented using computer readable/executable\ninstructions that may be stored or otherwise held by such computer readable\nmedia.\n\nThe following describes a system and method for parallelizing a neural network\nin which a parallel neural network comprises a plurality of neural networks\nthat communicate to other ones of the neural networks a subset of their state\nconditions. Thus, communications cost is relatively low compared to a fully\nconnected parallel neural network, while maintaining suitable ability to\nbenefit from parallel computation. Pooling is also enabled under the described\nsystem and method.\n\nThe following describes the parallelization of a CNN, however it is to be\nunderstood that the following principles apply to neural networks generally.\n\nReferring first to FIG. 1 , a feedforward neural network (FNN) comprises a\nplurality of layers, each of which comprises a pluralirty of cells. In various\nimplementations, the network, the layers and/or the cells are each implemented\nby a processor.\n\nAn FNN is a family of functions that map\n\n^n to\n\n^m which is parameterized with a sequence of weight matrices (W_1, . . . ,\n\n) and a sequence of bias vectors (b_1, . . . ,\n\n). Given an input x\u2208\n\n^n, the network's output\n\ncan be computed by x_i\u2190f_i(W_ix_i\u22121+b_i) by iterating i from 1 to\n\n, where x_0\u2190x, x_i is the activation vector (or state) of the i-th layer, and\nf_i(\u22c5) is an easy to compute nonlinearity, such as the element-wise tanh or\nthe element-wise rectification max(0,x).\n\nEvery configuration of weights and biases (W_1, . . . ,\n\n) and (b_1, . . . ,\n\n) defines a different x_0\u2192\n\nmapping. Training the network comprises applying nonlinear optimization\nmethods to find a parameter setting whose mapping x_0\u2192\n\nproduces correct labels on the training set. An FNN is said to be fully-\nconnected when each of the W_i matrices is dense.\n\nNeural networks can, in principle, be applied to vision tasks if the input\nimage is encoded as a vector. However, even small images are extremely high-\ndimensional objects. For example, a 256\u00d7256 color image has 196608 dimensions.\nThis high dimensionality typically renders neural networks infeasible to apply\nin practice, because their weight matrices tend to be impractically large. For\ninstance, a dense square matrix of the above dimensions has almost 5\u00b710^10\nparameters, which requires more computation than is typically available on a\nsingle CPU. It also typically requires an infeasible amount of data for\nlearning.\n\nA CNN, however, typically requires much less computation and training data\nthan a fully-connected neural network with similarly-sized layers, while being\ncapable of achieving similar performance on tasks such as vision (provided\nthere are no limits on the amount of data and computation). The CNN has a\nrestricted connectivity, so each neuron is only connected to a small patch of\nthe input image as opposed to the entire image, which subsequently reduces the\nnumber of connections. This restriction does not hurt performance relative to\na fully-connected network, because weighted sums of spatially-separated pixels\nare not informative. In addition, the CNN uses weight sharing, which means\nthat it processes every image patch with the same connections. This results in\nan additional reduction in the number of parameters, which greatly reduces the\namount of necessary training data. This weight sharing does not hurt\nperformance relative to networks that do not use such sharing, because image\nstatistics are homogeneous, so images can be reasonably processed in the same\nmanner in every locations.\n\nStated more formally, the input 1 to a CNN weight matrix may be a stack of u\nimages of size N\u00d7N each, which is a tensor of size N\u00d7N\u00d7u. A single CNN layer\nmay apply u\u00d7v filters, each of size (2m+1)\u00d7(2m+1), to produce an output tensor\n(activation tensor) 0 of size (N\u22122m)\u00d7(N\u22122m)\u00d7v. The output tensor may be\ncomputed with the following formula for each i, j, and v\u2032:\n\nwhere i\u2032=i+m+1 and j\u2032=j+m+1.\n\nThus, while a fully connected network with identically-sized layers has\nN^2u\u00d7(N\u22122m)^2v connections, a convolutional neural networks has\n(N\u22122m)^2u\u00d7(2m+1)^2v connections and only u\u00d7v\u00d7(2m+1)^2 parameters, so the\nnumber of connections is reduced by a factor of about N^2/4m^2 and the number\nof parameters is reduced by a factor of almost N^4/4m^2.\n\nHowever, despite the relative efficiency of a reasonably-sized convolutional\nneural network, in practice it may be impracticably slow for classification of\nlarge images.\n\nReferring now to FIG. 2 , in one aspect, a system for parallelizing a CNN is\nprovided. The system comprises a plurality of CNNs instantiated on a plurality\nof computing nodes. Each computing node is a processor such as a CPUs or GPUs.\nIt will be appreciated that a set of nodes may comprise combinations of CPUs\nand GPUs as well as other processors. It will also be appreciated that the\ndescribed CNN need not be applied only to image processing, but can be applied\nto other suitable tasks.\n\nIn one aspect, the system comprises interconnections initiated at a\npredetermined subset of layers for which activations will be communicated to\nother CNNs. The activations may be communicated to the subsequent adjacent\nlayer of the other CNNs. For example, activations of nodes at layer i are\ncommunicated to cells of layer i+1 in other nodes. In the example shown in\nFIG. 2 , for example, activations of layer 2 and 4 in each node are\ncommunicated to layer 3 and 5, respectively, of the other nodes.\n\nThe layers selected for interconnection are a subset of all layers. In an\nexample, which is to be understood as non-limiting, activations may be\ncommunicated across all nodes of particular pair of adjacent layers at\npredetermined intervals (i.e., nodes of layer xi+k are communicated to nodes\nof layer xi+k+1, where x is an integer and k is an offset constant to define\nthe first such interconnected layer). In a specific example, the selected\nlayers are every third or fourth layer (i.e., x=3 or 4). In another example,\nthe interval of such layers is irregular, such that the layers whose\nactivations are to be communicated are selected arbitrarily, or selected based\non additional considerations.\n\nIn another aspect, activations of a particular node may be communicated to a\nsubset of the other nodes. For example, when the number of computing nodes is\nlarge, such as being greater than 10 for example, the cost of communicating\nthe activation of every CNN at the predetermined layers to each other CNN at\nthe respective subsequent layers may be impractically or prohibitively\nexpensive. In such a case, the activations may be communicated to a\npredetermined subset (that may be selected randomly or in some other way prior\nto training) of the other CNNs. In an example, activations for node 1 layer 1\nmay be interconnected to node 2 layer 2 but not node 3 layer 2.\n\nIn yet another aspect, activations of nodes of a particular layer may be\ninterconnected with the subsequent layers of other CNNs while nodes in the\ncorresponding layer of the other CNNs are not interconnected with the\nsubsequent layers of other CNNs. In an example, node 1 layer 1 may be\ninterconnected to node 2 layer 2 and node 3 layer 2, while node 2 layer 1 may\nonly be connected to node 2 layer 2 and not interconnected to node 1 layer 2\nor node 3 layer 2. In another example, layer 1 of both nodes 1 and 2 may be\ninterconnected with layer 2 of each node, while layer 1 of node 3 may only be\nconnected to node 3 layer 2.\n\nCombinations of the foregoing are also possible. For example, node 1 layer 1\nmay be interconnected to node 2 layer 2 but not node 3 layer 2; while node 2\nlayer 1 may not be interconnected to either node 1 layer 2 or node 3 layer 2;\nand node 3 layer 1 may be interconnected to both node 1 layer 2 and node 2\nlayer 2.\n\nSuch examples may be beneficial where, for example, one or more node (one or\nmore of the GPUs) is slower than others. In this example, it may be beneficial\nto reduce the size of each layer at the slower nodes relative to that layer at\nthe other nodes to enable all the GPUs to process each layer at roughly the\nsame speed. In turn, the smaller layer size of said GPU may make it cheap to\ncommunicate at every layer, without greatly increasing communication cost. It\nmay thus be cost-effective to communicate the activations of the slower GPU at\nevery layer, while communicating the activations of the faster GPUs at only\nsome of the layers.\n\nReferring to FIG. 3 , a method of establishing a parallel CNN may comprise\nestablishing a plurality of parallel CNNs each instantiated on a computing\nnode (300), selecting one or more layers for each CNN for which to communicate\ntheir activations to other CNNs (302), selecting, for each such layer in each\nsuch CNN, at least a subset of the other CNNs to which to communicate the\nactivations (304), and interconnecting each of the selected layers with the\nrespective subsequent layers of such subset (308).\n\nReferring to FIG. 4 , a neural network comprising another example of a\nparallel CNN implemented on two nodes is shown. In this example, the neural\nnetwork comprises eight layers where the first five are convolutional and the\nremaining three are fully-connected. The second, fourth and fifth\nconvolutional layers are connected to the previous layer only within the same\nprocessor, while those of the remaining layers are interconnected across the\ntwo nodes.\n\nAlthough the invention has been described with reference to certain specific\nembodiments, various modifications thereof will be apparent to those skilled\nin the art without departing from the spirit and scope of the invention as\noutlined in the claims appended hereto. The entire disclosures of all\nreferences recited above are incorporated herein by reference.\n\n###\n\nClaims (20)\n\nHide Dependent\n\ntranslated from\n\nWhat is claimed is:\n\n1\\. A system comprising one or more computers and one or more storage devices\nstoring instructions that when executed by the one or more computers cause the\none or more computers to implement a convolutional neural network, the\nconvolutional neural network comprising:\n\na first convolutional layer configured to receive an input image and to\nprocess the input image to generate a first convolved output;\n\na first max-pooling operation immediately after the first convolutional layer\nin the convolutional neural network and configured to pool the first convolved\noutput to generate a first pooled output;\n\na sequence having only a plurality of second convolutional layers that is\nafter the max-pooling layer in the convolutional neural network and that is\nconfigured to receive a first input derived from the first pooled output and\nto process the first input to generate a second convolved output;\n\none or more fully-connected layers after the sequence of second convolutional\nlayers in the convolutional neural network and configured to receive an output\nderived from the second convolved output and to collectively process the\noutput derived from the second convolved output to generate an initial output\nfor the input image; and\n\nan output layer configured to classify the input image based at least in part\non the initial output.\n\n2\\. The system of claim 1, wherein the one or more fully-connected layers are\nnot immediately after the sequence of second convolutional layers in the\nconvolutional neural network.\n\n3\\. The system of claim 1, wherein the sequence of second convolutional layers\nis not immediately followed by a max-pooling layer in the convolutional neural\nnetwork.\n\n4\\. The system of claim 1, wherein the convolutional neural network comprises\none or more other layers between the sequence of second convolutional layers\nand the plurality of fully-connected layers.\n\n5\\. The system of claim 1, wherein the convolutional neural network comprises\none or more sets of two or more convolutional neural network layers that each\nreceive a respective same input.\n\n6\\. The system of claim 1, the convolutional neural network further\ncomprising:\n\none or more additional sequences of neural network layers.\n\n7\\. The system of claim 6, wherein each additional sequence of neural network\nlayers is configured to collectively process an input derived from the input\nimage to generate a respective initial output for the input image.\n\n8\\. One or more non-transitory computer-readable storage media storing\ninstructions that when executed by one or more computers cause the one or more\ncomputers to implement a convolutional neural network, the convolutional\nneural network comprising:\n\na first convolutional layer configured to receive an input image and to\nprocess the input image to generate a first convolved output;\n\na first max-pooling operation immediately after the first convolutional layer\nin the convolutional neural network and configured to pool the first convolved\noutput to generate a first pooled output;\n\na sequence having only a plurality of second convolutional layers that is\nafter the max-pooling layer in the convolutional neural network and that is\nconfigured to receive a first input derived from the first pooled output and\nto process the first input to generate a second convolved output;\n\none or more fully-connected layers after the sequence of second convolutional\nlayers in the convolutional neural network and configured to receive an output\nderived from the second convolved output and to collectively process the\noutput derived from the second convolved output to generate an initial output\nfor the input image; and\n\nan output layer configured to classify the input image based at least in part\non the initial output.\n\n9\\. The computer-readable storage media of claim 8, wherein the one or more\nfully-connected layers are not immediately after the sequence of second\nconvolutional layers in the convolutional neural network.\n\n10\\. The computer-readable storage media of claim 8, wherein the sequence of\nsecond convolutional layers is not immediately followed by a max-pooling layer\nin the convolutional neural network.\n\n11\\. The computer-readable storage media of claim 8, wherein the convolutional\nneural network comprises one or more other layers between the sequence of\nsecond convolutional layers and the plurality of fully-connected layers.\n\n12\\. The computer-readable storage media of claim 8, wherein the convolutional\nneural network comprises one or more sets of two or more convolutional neural\nnetwork layers that each receive a respective same input.\n\n13\\. The computer-readable storage media of claim 8, the convolutional neural\nnetwork further comprising:\n\none or more additional sequences of neural network layers, wherein each\nadditional sequence of neural network layers is configured to collectively\nprocess an input derived from the input image to generate a respective initial\noutput for the input image.\n\n14\\. A method performed by one or more computers for processing an input image\nusing a convolutional neural network to generate a classification output for\nthe input image, the method comprising:\n\nprocessing the input image using a first convolutional layer configured to\nreceive the input image and to process the input image to generate a first\nconvolved output;\n\nprocessing the first convolved output through a first max-pooling operation\nimmediately after the first convolutional layer in the convolutional neural\nnetwork and configured to pool the first convolved output to generate a first\npooled output;\n\nprocessing a first input derived from the first pooled output using a sequence\nhaving only a plurality of second convolutional layers that are after the max-\npooling layer in the convolutional neural network and configured to receive a\nfirst input derived from the first pooled output and to process the first\ninput to generate a second convolved output;\n\nprocessing an output derived from the second convolved output using one or\nmore fully-connected layers that are after the sequence of second\nconvolutional layers in the convolutional neural network and configured to\nreceive the output derived from the second convolved output and to\ncollectively process the output derived from the second convolved output to\ngenerate an initial output for the input image, and\n\nprocessing at least the initial output using an output layer configured to\nclassify the input image based at least in part on the initial output.\n\n15\\. The method of claim 14, wherein the one or more fully-connected layers\nare not immediately after the sequence of second convolutional layers in the\nconvolutional neural network.\n\n16\\. The method of claim 14, wherein the sequence of second convolutional\nlayers is not immediately followed by a max-pooling layer in the convolutional\nneural network.\n\n17\\. The method of claim 14, wherein the convolutional neural network\ncomprises one or more other layers between the sequence of second\nconvolutional layers and the plurality of fully-connected layers.\n\n18\\. The method of claim 14, wherein the convolutional neural network\ncomprises one or more sets of two or more convolutional neural network layers\nthat each receive a respective same input.\n\n19\\. The method of claim 14, the convolutional neural network further\ncomprising:\n\none or more additional sequences of neural network layers.\n\n20\\. The method of claim 19, wherein each additional sequence of neural\nnetwork layers is configured to collectively process an input derived from the\ninput image to generate a respective initial output for the input image.\n\n### Patent Citations (13)\n\nPublication number Priority date Publication date Assignee Title\n\nTW226454B 1993-11-02 1994-07-11 Ind Tech Res Inst Pattern recognition method\nfor car plate\n\nTW235935B 1992-06-26 1994-12-11 Yung Y Lee\n\nUS5740326A 1994-07-28 1998-04-14 International Business Machines Corporation\nCircuit for searching/sorting data in neural networks\n\nUS20030236760A1 2002-06-05 2003-12-25 Alex Nugent Multi-layer training in a\nphysical neural network formed utilizing nanotechnology\n\nUS6820070B2 2000-06-07 2004-11-16 Insyst Ltd. Method and tool for data mining\nin automatic decision making systems\n\nUS20110255741A1 * 2010-02-05 2011-10-20 Sang-Hack Jung Method and apparatus\nfor real-time pedestrian detection for urban driving\n\nUS20120203932A1 * 2011-02-08 2012-08-09 Microsoft Corporation Multi-master\nmedia metadata synchronization\n\nUS20120275690A1 * 2011-04-26 2012-11-01 Nec Laboratories America, Inc.\nDistributed artificial intelligence services on a cell phone\n\nUS20140180989A1 2012-12-24 2014-06-26 Google Inc. System and method for\nparallelizing convolutional neural networks\n\nFamily To Family Citations\n\nEP1262907B1 * 2001-05-28 2007-10-03 Honda Research Institute Europe GmbH\nPattern recognition with hierarchical networks\n\nJP4532915B2 * 2004-01-29 2010-08-25 \u30ad\u30e4\u30ce\u30f3\u682a\u5f0f\u4f1a\u793e Pattern recognition learning\nmethod, pattern recognition learning device, image input device, computer\nprogram, and computer-readable recording medium\n\nUS8345984B2 * 2010-01-28 2013-01-01 Nec Laboratories America, Inc. 3D\nconvolutional neural networks for automatic human action recognition\n\nUS10078620B2 * 2011-05-27 2018-09-18 New York University Runtime\nreconfigurable dataflow processor with multi-port memory access module\n\n* Cited by examiner, \u2020 Cited by third party\n\n### Non-Patent Citations (6)\n\nTitle\n\nChen et al., \"Pipelined back-propagation for context-dependent deep neural\nnetworks,\" Proceedings of the 13th Annual Conference of the International\nSpeech Communication Association (Interspeech'2012), Sep. 9, 2012, 26-29.\n\nCiresan et al., \"Multi-column deep neural networks for image classification,\"\nProceedings 0f the 2012 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR'12). Jun. 16, 2012, 3642-3649.\n\nDean et al., \"Large Scale Distributed Deep Networks,\" Proceedings of the 26th\nannual conference on Neural Information Processing Systems (NIPS'2012), Dec.\n3, 2012, Retrieved from the Internet:\nURL:http://books.nips.cc/papers/files/nips25/NIPS2012_0598.pdf [retrieved on\nApr. 7, 2012].\n\nInternational Search Report and Written Opinion in International Application\nNo. PCT/US2013/077606, dated Apr. 22, 2014, 11 pages.\n\nKrizhevsky et al., \"ImageNet classification with deep Convolutional neural\nnetworks,\" Proceedings of the 26th annual conference on Neural Information\nProcessing Systems (NIPS'2012), Dec. 3, 2012, Retrieved from the Internet:\nURL:http://books.nips.cc/papers/files/nips25/NIPS2012_0534.pdf, [retrieved\non;Apr. 7, 2014].\n\nOffice Action issued in Taiwanese Application No. 102147804, dated Feb. 13,\n2017, 10 pages ( with English translation).\n\n* Cited by examiner, \u2020 Cited by third party\n\n### Cited By (117)\n\nPublication number Priority date Publication date Assignee Title\n\nUS20220262356A1 * 2019-08-08 2022-08-18 Nippon Telegraph And Telephone\nCorporation Determination device, training device, determination method, and\ndetermination program\n\nFamily To Family Citations\n\nUS9811775B2 2012-12-24 2017-11-07 Google Inc. Parallelizing neural networks\nduring training\n\nUS9542626B2 2013-09-06 2017-01-10 Toyota Jidosha Kabushiki Kaisha Augmenting\nlayer-based object detection with deep convolutional neural networks\n\nUS9679258B2 * 2013-10-08 2017-06-13 Google Inc. Methods and apparatus for\nreinforcement learning\n\nUS10331997B2 * 2014-05-07 2019-06-25 Seagate Technology Llc Adaptive\nconfiguration of a neural network device\n\nUS10346726B2 * 2014-12-15 2019-07-09 Samsung Electronics Co., Ltd. Image\nrecognition method and apparatus, image verification method and apparatus,\nlearning method and apparatus to recognize image, and learning method and\napparatus to verify image\n\nKR102486699B1 2014-12-15 2023-01-11 \uc0bc\uc131\uc804\uc790\uc8fc\uc2dd\ud68c\uc0ac Method and apparatus for\nrecognizing and verifying image, and method and apparatus for learning image\nrecognizing and verifying\n\nUS10580401B2 2015-01-27 2020-03-03 Google Llc Sub-matrix input for neural\nnetwork layers\n\nJP2016146174A * 2015-02-06 2016-08-12 \u30d1\u30ca\u30bd\u30cb\u30c3\u30afIp\u30de\u30cd\u30b8\u30e1\u30f3\u30c8\u682a\u5f0f\u4f1a\u793e Determination method\nand program\n\nWO2016134183A1 2015-02-19 2016-08-25 Digital Reasoning Systems, Inc. Systems\nand methods for neural language modeling\n\nUS10762894B2 2015-03-27 2020-09-01 Google Llc Convolutional neural networks\n\nUS9436895B1 * 2015-04-03 2016-09-06 Mitsubishi Electric Research Laboratories,\nInc. Method for determining similarity of objects represented in images\n\nUS10606651B2 2015-04-17 2020-03-31 Microsoft Technology Licensing, Llc Free\nform expression accelerator with thread length-based thread assignment to\nclustered soft processor cores that share a functional circuit\n\nUS9805303B2 2015-05-21 2017-10-31 Google Inc. Rotating data for neural network\ncomputations\n\nUS9747546B2 2015-05-21 2017-08-29 Google Inc. Neural network processor\n\nUS10049322B2 2015-05-21 2018-08-14 Google Llc Prefetching weights for use in a\nneural network processor\n\nUS10438117B1 * 2015-05-21 2019-10-08 Google Llc Computing convolutions using a\nneural network processor\n\nUS10192162B2 2015-05-21 2019-01-29 Google Llc Vector computation unit in a\nneural network processor\n\nUS10083395B2 2015-05-21 2018-09-25 Google Llc Batch processing in a neural\nnetwork processor\n\nUS10417555B2 * 2015-05-29 2019-09-17 Samsung Electronics Co., Ltd. Data-\noptimized neural network traversal\n\nCN106203619B * 2015-05-29 2022-09-13 \u4e09\u661f\u7535\u5b50\u682a\u5f0f\u4f1a\u793e Data optimized neural network\ntraversal\n\nUS10769531B2 2015-06-05 2020-09-08 Cisco Technology, Inc. Methods and systems\nfor counting people\n\nUS10540588B2 2015-06-29 2020-01-21 Microsoft Technology Licensing, Llc Deep\nneural network processing on hardware accelerators with stacked memory\n\nUS10452995B2 2015-06-29 2019-10-22 Microsoft Technology Licensing, Llc Machine\nlearning classification on hardware accelerators with stacked memory\n\nUS10452971B2 2015-06-29 2019-10-22 Microsoft Technology Licensing, Llc Deep\nneural network partitioning on servers\n\nUS10002402B2 2015-07-23 2018-06-19 Sony Corporation Learning convolution\nneural networks on heterogeneous CPU-GPU platform\n\nWO2017015887A1 * 2015-07-29 2017-02-02 Nokia Technologies Oy Object detection\nwith neural network\n\nAU2016315938B2 2015-08-31 2022-02-24 Cape Analytics, Inc. Systems and methods\nfor analyzing remote sensing imagery\n\nUS11295506B2 2015-09-16 2022-04-05 Tmrw Foundation Ip S. \u00c0 R.L. Chip with game\nengine and ray trace engine\n\nUS10078794B2 2015-11-30 2018-09-18 Pilot Ai Labs, Inc. System and method for\nimproved general object detection using neural networks\n\nUS10648924B2 * 2016-01-04 2020-05-12 Kla-Tencor Corp. Generating high\nresolution images from low resolution images for semiconductor applications\n\nJP6610278B2 2016-01-18 2019-11-27 \u5bcc\u58eb\u901a\u682a\u5f0f\u4f1a\u793e Machine learning apparatus, machine\nlearning method, and machine learning program\n\nUS10366451B2 2016-01-27 2019-07-30 Huawei Technologies Co., Ltd. System and\nmethod for prediction using synthetic features and gradient boosted decision\ntree\n\nUS9858340B1 2016-04-11 2018-01-02 Digital Reasoning Systems, Inc. Systems and\nmethods for queryable graph representations of videos\n\nCN107704267B * 2016-04-29 2020-05-08 \u4e2d\u79d1\u5bd2\u6b66\u7eaa\u79d1\u6280\u80a1\u4efd\u6709\u9650\u516c\u53f8 Convolution neural network\noperation instruction and method thereof\n\nUS10338931B2 2016-04-29 2019-07-02 International Business Machines Corporation\nApproximate synchronization for parallel deep learning\n\nGB201607713D0 * 2016-05-03 2016-06-15 Imagination Tech Ltd Convolutional\nneural network\n\nUS10706348B2 2016-07-13 2020-07-07 Google Llc Superpixel methods for\nconvolutional neural networks\n\nUS20190265955A1 * 2016-07-21 2019-08-29 Ramot At Tel-Aviv University Ltd.\nMethod and system for comparing sequences\n\nUS11238337B2 * 2016-08-22 2022-02-01 Applied Brain Research Inc. Methods and\nsystems for implementing dynamic neural networks\n\nUS11556779B2 2016-09-26 2023-01-17 Arizona Board Of Regents On Behalf Of\nArizona State University Cascaded computing for convolutional neural networks\n\nUS10733505B2 2016-11-10 2020-08-04 Google Llc Performing kernel striding in\nhardware\n\nDE112017006136T5 * 2016-12-05 2019-08-22 Avigilon Corporation System and\nmethod for CNN layer sharing\n\nCN108256544B * 2016-12-29 2019-07-23 \u676d\u5dde\u5149\u542f\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u9662 Picture classification method\nand device, robot\n\nWO2018126073A1 * 2016-12-30 2018-07-05 Lau Horace H Deep learning hardware\n\nCN108229325A * 2017-03-16 2018-06-29 \u5317\u4eac\u5e02\u5546\u6c64\u79d1\u6280\u5f00\u53d1\u6709\u9650\u516c\u53f8 Method for detecting human\nface and system, electronic equipment, program and medium\n\nCN110582748A 2017-04-07 2019-12-17 \u82f1\u7279\u5c14\u516c\u53f8 Method and system for boosting deep\nneural networks for deep learning\n\nUS11164071B2 * 2017-04-18 2021-11-02 Samsung Electronics Co., Ltd. Method and\napparatus for reducing computational complexity of convolutional neural\nnetworks\n\nWO2018217829A1 * 2017-05-23 2018-11-29 Intel Corporation Methods and apparatus\nfor enhancing a neural network using binary tensor and scale factor pairs\n\nCN107301456B * 2017-05-26 2020-05-12 \u4e2d\u56fd\u4eba\u6c11\u89e3\u653e\u519b\u56fd\u9632\u79d1\u5b66\u6280\u672f\u5927\u5b66 Deep neural network\nmulti-core acceleration implementation method based on vector processor\n\nWO2018226527A1 * 2017-06-08 2018-12-13 D5Ai Llc Data splitting by gradient\ndirection for neural networks\n\nJP7146372B2 2017-06-21 2022-10-04 \u30ad\u30e4\u30ce\u30f3\u682a\u5f0f\u4f1a\u793e Image processing device, imaging\ndevice, image processing method, program, and storage medium\n\nCN107341127B * 2017-07-05 2020-04-14 \u897f\u5b89\u7535\u5b50\u79d1\u6280\u5927\u5b66 Convolutional neural network\nacceleration method based on OpenCL standard\n\nUS10671349B2 2017-07-24 2020-06-02 Tesla, Inc. Accelerated mathematical engine\n\nUS11409692B2 2017-07-24 2022-08-09 Tesla, Inc. Vector computational unit\n\nUS11157287B2 2017-07-24 2021-10-26 Tesla, Inc. Computational array\nmicroprocessor system with variable latency memory access\n\nUS11893393B2 2017-07-24 2024-02-06 Tesla, Inc. Computational array\nmicroprocessor system with hardware arbiter managing memory requests\n\nUS11157441B2 2017-07-24 2021-10-26 Tesla, Inc. Computational array\nmicroprocessor system using non-consecutive data formatting\n\nCN107247949B * 2017-08-02 2020-06-19 \u667a\u6167\u773c\u79d1\u6280\u80a1\u4efd\u6709\u9650\u516c\u53f8 Face recognition method and\ndevice based on deep learning and electronic equipment\n\nCN107301864B * 2017-08-16 2020-12-22 \u91cd\u5e86\u90ae\u7535\u5927\u5b66 Deep bidirectional LSTM acoustic\nmodel based on Maxout neuron\n\nCN107481209B * 2017-08-21 2020-04-21 \u5317\u4eac\u822a\u7a7a\u822a\u5929\u5927\u5b66 Image or video quality\nenhancement method based on convolutional neural network\n\nUS11741354B2 2017-08-25 2023-08-29 Ford Global Technologies, Llc Shared\nprocessing with deep neural networks\n\nRU2656990C1 * 2017-09-11 2018-06-07 \u0421\u0430\u043c\u0441\u0443\u043d\u0433 \u042d\u043b\u0435\u043a\u0442\u0440\u043e\u043d\u0438\u043a\u0441 \u041a\u043e., \u041b\u0442\u0434. System and\nmethod for artificial neural network invariant to transferring\n\nEP3457324A1 2017-09-15 2019-03-20 Axis AB Method for locating one or more\ncandidate digital images being likely candidates for depicting an object\n\nCN107609645B * 2017-09-21 2024-04-02 \u767e\u5ea6\u5728\u7ebf\u7f51\u7edc\u6280\u672f(\u5317\u4eac)\u6709\u9650\u516c\u53f8 Method and apparatus for\ntraining convolutional neural network\n\nKR102586173B1 2017-10-31 2023-10-10 \uc0bc\uc131\uc804\uc790\uc8fc\uc2dd\ud68c\uc0ac Processor and control methods\nthererof\n\nWO2019090325A1 2017-11-06 2019-05-09 Neuralmagic, Inc. Methods and systems for\nimproved transforms in convolutional neural networks\n\nUS10346720B2 * 2017-11-08 2019-07-09 Bae Systems Information And Electronic\nSystems Integration Inc. Rotation variant object detection in Deep Learning\n\nKR102561261B1 2017-11-14 2023-07-28 \uc0bc\uc131\uc804\uc790\uc8fc\uc2dd\ud68c\uc0ac Apparatus and method for\nprocessing convolution operation using kernel\n\nCN108229650B * 2017-11-15 2021-04-09 \u5317\u4eac\u5e02\u5546\u6c64\u79d1\u6280\u5f00\u53d1\u6709\u9650\u516c\u53f8 Convolution processing\nmethod and device and electronic equipment\n\nUS20190156214A1 2017-11-18 2019-05-23 Neuralmagic Inc. Systems and methods for\nexchange of data in distributed training of machine learning algorithms\n\nCN108052975B * 2017-12-12 2020-12-11 \u6d59\u6c5f\u5927\u5b66\u5b81\u6ce2\u7406\u5de5\u5b66\u9662 Vehicle operation real-time\nworking condition prediction method based on kernel principal component and\nneural network\n\nCN108038815B * 2017-12-20 2019-12-17 \u6df1\u5733\u4e91\u5929\u52b1\u98de\u6280\u672f\u6709\u9650\u516c\u53f8 integrated circuit with a\nplurality of transistors\n\nWO2019134987A1 * 2018-01-05 2019-07-11 Deepmind Technologies Limited Parallel\nvideo processing systems\n\nUS11561791B2 2018-02-01 2023-01-24 Tesla, Inc. Vector computational unit\nreceiving data elements in parallel from a last row of a computational array\n\nCN108416270B * 2018-02-06 2021-07-06 \u5357\u4eac\u4fe1\u606f\u5de5\u7a0b\u5927\u5b66 Traffic sign identification\nmethod based on multi-attribute combined characteristics\n\nUS11119915B2 2018-02-08 2021-09-14 Samsung Electronics Co., Ltd. Dynamic\nmemory mapping for neural networks\n\nCN108364061B * 2018-02-13 2020-05-05 \u5317\u4eac\u65f7\u89c6\u79d1\u6280\u6709\u9650\u516c\u53f8 Arithmetic device, arithmetic\nexecution apparatus, and arithmetic execution method\n\nUS10601960B2 2018-02-14 2020-03-24 Eingot Llc Zero-knowledge environment based\nnetworking engine\n\nUS10719613B1 * 2018-02-23 2020-07-21 Facebook, Inc. Systems and methods for\nprotecting neural network weights\n\nUS10699190B1 * 2018-03-04 2020-06-30 Facebook, Inc. Systems and methods for\nefficiently updating neural networks\n\nUS11301951B2 2018-03-15 2022-04-12 The Calany Holding S. \u00c0 R.L. Game engine\nand artificial intelligence engine on a chip\n\nCN108694386B * 2018-05-15 2021-08-10 \u534e\u5357\u7406\u5de5\u5927\u5b66 Lane line detection method based\non parallel convolution neural network\n\nCN108734211B * 2018-05-17 2019-12-24 \u817e\u8baf\u79d1\u6280(\u6df1\u5733)\u6709\u9650\u516c\u53f8 Image processing method and\ndevice\n\nUS10963787B2 2018-05-31 2021-03-30 Neuralmagic Inc. Systems and methods for\ngeneration of sparse code for convolutional neural networks\n\nUS10832133B2 2018-05-31 2020-11-10 Neuralmagic Inc. System and method of\nexecuting neural networks\n\nUS11449363B2 2018-05-31 2022-09-20 Neuralmagic Inc. Systems and methods for\nimproved neural network execution\n\nUS11216732B2 2018-05-31 2022-01-04 Neuralmagic Inc. Systems and methods for\ngeneration of sparse code for convolutional neural networks\n\nWO2021061172A1 * 2019-09-27 2021-04-01 Neuralmagic Inc. System and method of\nexecuting neural networks\n\nUS10417558B1 2018-09-28 2019-09-17 Deep Insight Solutions, Inc. Methods and\nsystems for artificial neural network optimistic event processing\n\nCN108830377B * 2018-06-21 2020-12-15 \u745e\u82af\u5fae\u7535\u5b50\u80a1\u4efd\u6709\u9650\u516c\u53f8 Neural network circuit and\nself-circulation multi-stage iteration method thereof\n\nUS20210390397A1 * 2018-09-29 2021-12-16 VII Philip Alvelda Method, machine-\nreadable medium and system to parameterize semantic concepts in a multi-\ndimensional vector space and to perform classification, predictive, and other\nmachine learning and ai algorithms thereon\n\nUS11636343B2 2018-10-01 2023-04-25 Neuralmagic Inc. Systems and methods for\nneural network pruning with accuracy preservation\n\nCN109631848B * 2018-12-14 2021-04-16 \u5c71\u4e1c\u9c81\u80fd\u8f6f\u4ef6\u6280\u672f\u6709\u9650\u516c\u53f8 Transmission line foreign\nmatter intrusion detection system and detection method\n\nCN109754073B * 2018-12-29 2020-03-10 \u4e2d\u79d1\u5bd2\u6b66\u7eaa\u79d1\u6280\u80a1\u4efd\u6709\u9650\u516c\u53f8 Data processing method and\ndevice, electronic equipment and readable storage medium\n\nKR102209917B1 * 2018-12-31 2021-01-29 \uc544\uc8fc\ub300\ud559\uad50\uc0b0\ud559\ud611\ub825\ub2e8 Data processing apparatus and\nmethod for deep reinforcement learning\n\nUS11557107B2 2019-01-02 2023-01-17 Bank Of America Corporation Intelligent\nrecognition and extraction of numerical data from non-numerical graphical\nrepresentations\n\nUS11544559B2 2019-01-08 2023-01-03 Neuralmagic Inc. System and method for\nexecuting convolution in a neural network\n\nUS11313950B2 2019-01-15 2022-04-26 Image Sensing Systems, Inc. Machine\nlearning based highway radar vehicle classification across multiple lanes and\nspeeds\n\nCN110110845B * 2019-04-24 2020-09-22 \u957f\u5b89\u5927\u5b66 Learning method based on parallel\nmulti-level width neural network\n\nUS11625884B2 2019-06-18 2023-04-11 The Calany Holding S. \u00c0 R.L. Systems,\nmethods and apparatus for implementing tracked data communications on a chip\n\nJP2022541899A * 2019-07-19 2022-09-28 \u30b7\u30f3\u30cf,\u30d1\u30fc\u30f4\u30a7\u30eb A configurable processor for\nimplementing convolutional neural networks\n\nUS11195095B2 2019-08-08 2021-12-07 Neuralmagic Inc. System and method of\naccelerating execution of a neural network\n\nUS11210474B2 * 2019-08-15 2021-12-28 Advanced New Technologies Co., Ltd.\nLanguage processing using a neural network\n\nUS20220366223A1 * 2019-10-09 2022-11-17 The Uab Research Foundation A method\nfor uncertainty estimation in deep neural networks\n\nCN110807519B * 2019-11-07 2023-01-17 \u6e05\u534e\u5927\u5b66 Parallel acceleration method of\nneural network based on memristor, processor and device\n\nEP4094194A1 2020-01-23 2022-11-30 Umnai Limited An explainable neural net\narchitecture for multidimensional data\n\nUS20220076100A1 * 2020-09-10 2022-03-10 Mitsubishi Electric Research\nLaboratories, Inc. Multi-Dimensional Deep Neural Network\n\nUS11367265B2 2020-10-15 2022-06-21 Cape Analytics, Inc. Method and system for\nautomated debris detection\n\nUS11875261B2 2020-10-16 2024-01-16 Ford Global Technologies, Llc Automated\ncross-node communication in distributed directed acyclic graph\n\nUS11556757B1 2020-12-10 2023-01-17 Neuralmagic Ltd. System and method of\nexecuting deep tensor columns in neural networks\n\nUS11883746B2 * 2021-02-23 2024-01-30 Electronic Arts Inc. Adversarial\nreinforcement learning for procedural content generation and improved\ngeneralization\n\nWO2023283231A1 2021-07-06 2023-01-12 Cape Analytics, Inc. System and method\nfor property condition analysis\n\nUS11954467B2 2021-08-05 2024-04-09 Aarish Technologies Convolutional neural\nnetwork compiler for programmable functional array (PFA) processors\n\nUS11960982B1 2021-10-21 2024-04-16 Neuralmagic, Inc. System and method of\ndetermining and executing deep tensor columns in neural networks\n\nUS20230230250A1 2022-01-19 2023-07-20 Cape Analytics, Inc. System and method\nfor property detection and analysis\n\nWO2023219898A1 * 2022-05-09 2023-11-16 MemComputing, Inc. Deep neural network\nwith multiple layers formed of multi\u2010terminal logic gates\n\n* Cited by examiner, \u2020 Cited by third party, \u2021 Family to family citation\n\n### Similar Documents\n\nPublication Publication Date Title\n\nUS11928577B2 2024-03-12 System and method for parallelizing convolutional\nneural networks\n\nUS11568258B2 2023-01-31 Operation method\n\nCN107622302B 2021-03-19 Superpixel method for convolutional neural network\n\nUS11263526B2 2022-03-01 Gradient-based training engine for quaternion-based\nmachine-learning systems\n\nUS20180260709A1 2018-09-13 Calculating device and method for a sparsely\nconnected artificial neural network\n\nHinton et al. 2012 A better way to pretrain deep boltzmann machines\n\nDE112020003127T5 2022-05-05 Extension of dynamic processing element array\n\nUS11580376B2 2023-02-14 Electronic apparatus and method for optimizing trained\nmodel\n\nJP2019032808A 2019-02-28 Mechanical learning method and device\n\nUS20200151573A1 2020-05-14 Dynamic precision scaling at epoch granularity in\nneural networks\n\nUS20180129930A1 2018-05-10 Learning method based on deep learning model having\nnon-consecutive stochastic neuron and knowledge transfer, and system thereof\n\nUS11775832B2 2023-10-03 Device and method for artificial neural network\noperation\n\nJP2015197702A 2015-11-09 Information processor and information processing\nmethod\n\nCN111882053A 2020-11-03 Neural network model compression method based on\nsplicing convolution\n\nAppuswamy et al. 2016 Structured convolution matrices for energy-efficient\ndeep learning\n\nCN112966729A 2021-06-15 Data processing method and device, computer equipment\nand storage medium\n\nEP4009240A1 2022-06-08 Method and apparatus for performing deep learning\noperations\n\nBazerque et al. 2013 Inference of Poisson count processes using low-rank\ntensor data\n\nEP3660742B1 2022-07-20 Method and system for generating image data\n\nZhang et al. 2017 Crescendonet: A simple deep convolutional neural network\nwith ensemble behavior\n\nEP4141646B1 2024-03-13 Method and apparatus with calculation\n\nUS20230334289A1 2023-10-19 Deep neural network accelerator with memory having\ntwo-level topology\n\nUS11875557B2 2024-01-16 Polynomial convolutional neural network with early\nfan-out\n\nAl-Allaf 2014 Parfor and co-distributor parallel approaches for implementing\nfractal image compression based genetic algorithm\n\nUS20220343162A1 2022-10-27 Method for structure learning and model compression\nfor deep neural network\n\n### Priority And Related Applications\n\n### Parent Applications (1)\n\nApplication Priority date Filing date Relation Title\n\nUS15/414,514 2012-12-24 2017-01-24 Continuation System and method for\nparallelizing convolutional neural networks\n\n### Priority Applications (1)\n\nApplication Priority date Filing date Title\n\nUS16/859,815 2012-12-24 2020-04-27 System and method for parallelizing\nconvolutional neural networks\n\n### Applications Claiming Priority (5)\n\nApplication Filing date Title\n\nUS201261745717P 2012-12-24\n\nUS14/030,938 2013-09-18 Parallelizing neural networks during training\n\nUS14/817,492 2015-08-04 System and method for parallelizing convolutional\nneural networks\n\nUS15/414,514 2017-01-24 System and method for parallelizing convolutional\nneural networks\n\nUS16/859,815 2020-04-27 System and method for parallelizing convolutional\nneural networks\n\n### Legal Events\n\nDate Code Title Description\n\n2020-04-27 FEPP Fee payment procedure\n\nFree format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE:\nBIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY\n\n2020-07-08 STPP Information on status: patent application and granting\nprocedure in general\n\nFree format text: APPLICATION DISPATCHED FROM PREEXAM, NOT YET DOCKETED\n\n2021-08-20 STPP Information on status: patent application and granting\nprocedure in general\n\nFree format text: DOCKETED NEW CASE - READY FOR EXAMINATION\n\n2023-05-18 STPP Information on status: patent application and granting\nprocedure in general\n\nFree format text: NON FINAL ACTION MAILED\n\n2023-10-26 STPP Information on status: patent application and granting\nprocedure in general\n\nFree format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO\nEXAMINER\n\n2023-11-06 STPP Information on status: patent application and granting\nprocedure in general\n\nFree format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE\nOF PUBLICATIONS\n\n2023-11-17 AS Assignment\n\nOwner name: GOOGLE INC., CALIFORNIA\n\nFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:KRIZHEVSKY,\nALEXANDER;SUTSKEVER, ILYA;HINTON, GEOFFREY E.;REEL/FRAME:065607/0805\n\nEffective date: 20130311\n\nOwner name: GOOGLE INC., CALIFORNIA\n\nFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:DNNRESEARCH\nINC.;REEL/FRAME:065607/0801\n\nEffective date: 20130311\n\nOwner name: DNNRESEARCH INC., CANADA\n\nFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:THE GOVERNING\nCOUNCIL OF THE UNIVERSITY OF TORONTO;REEL/FRAME:065607/0792\n\nEffective date: 20130311\n\nOwner name: GOOGLE LLC, CALIFORNIA\n\nFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:065626/0590\n\nEffective date: 20170929\n\n2024-02-12 STPP Information on status: patent application and granting\nprocedure in general\n\nFree format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED\n\n2024-02-21 STCF Information on status: patent grant\n\nFree format text: PATENTED CASE\n\n###\n\nConcepts\n\nmachine-extracted\n\nThe concepts are extracted by a computer and may be incomplete or incorrect.\n\nDownload Filter table \u00b7\n\nAnatomy\n\nEffects\n\nMethods\n\nSubstances\n\nName Image Sections Count Query match\n\nconvolutional neural network title,claims,abstract,description 88 0.000\n\nmethod title,claims,description 33 0.000\n\nprocessing claims,abstract,description 13 0.000\n\nartificial neural network claims,description 21 0.000\n\npooling claims,description 14 0.000\n\nprocess claims,description 14 0.000\n\nactivation abstract,description 22 0.000\n\nactivation abstract,description 22 0.000\n\nShow all concepts from the description section\n\nData provided by IFI CLAIMS Patent Services\n\nLearn more about data coverage, search syntax and other features.\n\nAbout\n\nSend feedback about technical issues or feature requests for Google Patents.\n\nSend Feedback Public Datasets Terms Privacy Policy Help\n\n", "frontpage": false}
