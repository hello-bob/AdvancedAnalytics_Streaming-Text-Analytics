{"aid": "40260446", "title": "Is Llama-3 better than GPT-4 for generating tests?", "url": "https://symflower.com/en/company/blog/2024/dev-quality-eval-v0.4.0-is-llama-3-better-than-gpt-4-for-generating-tests/", "domain": "symflower.com", "votes": 1, "user": "tosh", "posted_at": "2024-05-04 21:31:43", "comments": 0, "source_title": "Is Llama-3 better than GPT-4 for generating tests? And other deep dives of the DevQualityEval v0.4.0", "source_text": "Is Llama-3 better than GPT-4 for generating tests? And other deep dives of the\nDevQualityEval v0.4.0\n\n# Is Llama-3 better than GPT-4 for generating tests? And other deep dives of\nthe DevQualityEval v0.4.0\n\nThis deep dive takes a look at the results of the DevQualityEval v0.4.0 which\nanalyzed 138 different LLMs for code generation (Java and Go). LLama 3 is\ngearing up to take GPT-4\u2019s throne, the Claude 3 family is great with Haiku\nbeing the most cost-effective, and Mistral 7B could be the next model of\nchoice for local devices.\n\nThe results in this post are based on 5 full runs using DevQualityEval v0.4.0.\nMetrics have been extracted and made public to make it possible to reproduce\nfindings. The full evaluation setup and reasoning behind the tasks are similar\nto the previous dive.\n\nThe purpose of the evaluation benchmark and the examination of its results is\nto give LLM creators a tool to improve the results of software development\ntasks towards quality and to provide LLM users with a comparison to choose the\nright model for their needs. Your feedback guides the next steps of the eval.\nWith that said, let\u2019s dive in!\n\n## Comparing the capabilities and costs of top models\n\nHigher resolution PNG in case SVG is not working.\n\nThe graph shows the best models in relation to their scores (y-axis, linear\nscale, weighted on their test coverage) and costs (x-axis, logarithmic scale,\n$ per million tokens). The sweet spot is the top-left corner: cheap with good\nresults.\n\nReducing the full list of 138 LLMs to a manageable size was done by sorting\nbased on scores and then costs. We then removed all models that were worse\nthan the best model of the same vendor/family/version/variant (e.g.\ngpt-4-turbo is better than gpt-4 so the latter can be removed). For clarity,\nthe remaining models were renamed to represent their variant (e.g. just gpt-4\ninstead of gpt-4-turbo). Even then, the list was immense. In the end, only the\nmost important new models (e.g. llama-3-8b and wizardlm-2-7b), and fundamental\nmodels with at most the last two versions for comparison (e.g. command-r-plus\nand command-r) were kept.\n\nThe above image has been divided into sections with lines to further\ndifferentiate:\n\n  * Scores below 45 (not on the graph): These models either did not produce compiling code or were simply not good enough, e.g. produced useless code. They are not recommended and need serious work for the evaluation\u2019s tasks.\n  * Scores up to 80 (bottom): These models returned source code for each prompt 100% of the time. However, that code often did not compile, e.g. gemma-7b-it had only 3 out of 10 instances that compiled, but these 3 instances reached 100% coverage. These models require deeper work to fulfill the main objectives and are not recommended unless manual rewrites are acceptable.\n  * Scores up to 110 (middle): These models reached the necessary coverage at least half of the time, returning code that tends to be more stable and non-overengineered. It could be that more fine-tuning is all that these models need, e.g. command-r-plus reached almost perfect results for Java but consistently made the same mistake of wrong imports, leading to 0% coverage for Go.\n  * Scores starting with 110 (top): These models are dependable for the major objectives and are recommended for generating tests for Java or Go. More fine-tuning leads to better scores, e.g. wizardlm-2-8x22b greatly outperforms mixtral-8x22b. For the most part, selection in this section is a matter of costs, e.g. gpt-4-turbo with $40 per million tokens vs llama-3-70b-instruct with $1.62 are vastly different in cost.\n\nPutting all this information together gives us the following outcome (for\nnow):\n\n  * gpt-4-turbo is the undisputed winner when it comes to capabilities with a 150 out of 150 score but at a high cost.\n  * llama-3-70b is the most cost-effective with wizardlm-2-8x22b second and claude-3-haiku third. That\u2019s our triangle of love in the \u201ccheap with good results\u201d corner.\n\nHowever, there are multiple flaws at work! The king of LLMs could be still\ndethroned with this evaluation version!\n\nDuring the hours that we spent looking at logs of the results, improving\nreporting for better insights on all the data, and of course, contemplating\nwhich emojis should be used on the header image of this blog post, we noticed\na bunch of problems that affect all evaluation benchmarks of LLMs. The\nfollowing sections discusses these problems as well as other findings, and how\nthey (might) be solved in the next version of the DevQualityEval: v0.5.0. If\nyou found other problems or have suggestions to improve the eval, let us know.\nWith that said, let\u2019s jump into the main findings.\n\n### Costs need more than logarithmic scale\n\nWhen we started creating this evaluation benchmark, we knew one fact: costs\nshould be an important aspect of every benchmark. Arguably, costs can be as\nimportant as time: the best result can be useless if it takes a year to\ncompute. For example, we learned with our own product that every editor action\nneeds an appropriately fast reaction. Think of an auto-complete that takes a\nminute to show - horrible! However, analyses outside of an editor can take\neven hours when they bring good results. The same logic is applied to costs.\n\nHowever, costs of LLMs are a completely different breed of cost-monsters. Look\nat the following graph (like before, scores are represented on the y-axis\nwhile the x-axis shows costs) where you have costs on the left side in\nlogarithmic scale, and on the right in linear scale:\n\nClick on image for higher resolution.\n\nWith the logarithmic scale, we have a nice graph where labels are almost\nperfectly spread to be readable. If you do not look at the numeric ticks of\nthe axis, you never know how costly the models on the right are.\n\nWith the linear scale, you can clearly see that llama-3-70b-instruct (top-left\ncorner) is cheap and good claude-3-sonnet:beta is on the same score-level but\nmore expensive gpt-4-turbo even doubles the price but it has just a slightly\nbetter score.\n\nHowever, we are not done yet, because on the far right claude-3-opus:beta\ndoubles again the price of GPT-4 but it is not as good in score. With a\nlogarithmic scale, these facts are very hard to see and consider. Of course,\nyou could say that this is a problem of the evaluation and/or a problem of\ninterpreting the results/scoring. Admittedly it is, and the next findings will\nreason on how to improve those problems. Nevertheless, it also shows that\ncosts must be part of an evaluation because even with perfect interpretation\nand scoring of results, if a model is worse than another in capability and is\nalso way more costly than the better solution, it should have a proportional\nlower score in the overall scoring. Scoring that includes costs helps LLM\nusers to make the right choice for their usage.\n\nCan we dethrone the king of LLMs? Yes, factoring in costs,GPT-4 Turbo is much\nless cost-effective than Llama-3-70B-Instruct which has a good score, useful\nresults, and is extremely cost-effective. That said, we are still not done\nwith our findings.\n\n### LLMs' chattiness can be very costly\n\nAnother cost factor that must be taken into account of an overall score is\n\u201cchattiness\u201d, i.e. is the answer to a prompt direct and minimal, or is there\nlots of explanation? Source code answers also tend to include lots of comments\n(most often obvious comments) and complex implementations that could be\nsimplified. Let\u2019s first look at the prompt for the Java use case:\n\n> Given the following Java code file \u201csrc/main/java/com/eval/Plain.java\u201d with\n> package \u201ccom.eval\u201d, provide a test file for this code with JUnit 5 as a test\n> framework. The tests should produce 100 percent code coverage and must\n> compile. The response must contain only the test code and nothing else.\n>  \n>  \n>     package com.eval; class Plain { static void plain() { } }\n\nAccording to gpt-tokenizer.dev this prompt has 84 tokens which costs about\n$0.003 with GPT-4. Optimizing the size of our prompt (e.g. how much context we\ndefine) and how often we send it (e.g. for auto-completion) is our job.\nHowever, there is a key aspect in this prompt: The response must contain only\nthe test code and nothing else. This task needs to be understood by the model\nand executed. Looking at one of the answer of gpt-4-turbo we see that it does\na superb job:\n\n>\n>     package com.eval; import org.junit.jupiter.api.Test; import static\n> org.junit.jupiter.api.Assertions.*; class PlainTest { @Test void testPlain()\n> { assertDoesNotThrow(() -> Plain.plain()); } }\n\nThere is only a code-fence with Java code, and the solution is almost minimal\n(assertDoesNotThrow is not really needed). Those are 50 tokens so about\n$0.001. Remember GPT-4 is not cheap but there is an even more expensive model,\nclaude-3-opus:beta, which does an excellent job as well for Java. Here is one\nof its answers:\n\n> Here\u2019s the JUnit 5 test file that provides 100 percent code coverage for the\n> given Java code:\n>  \n>  \n>     package com.eval; import org.junit.jupiter.api.Test; class PlainTest {\n> @Test void testPlain() { Plain.plain(); } }\n\nThose are 59 tokens (more than GPT-4) which cost $0.00531 so about 5 times\nmore than GPT-4 for a similar answer. The reason is that Claude Opus did not\nstick to the task or did not understand it: there is additional (unnecessary)\ntext before the code-fence. Of course, such outputs could be removed with\nbetter prompting, but still, the model should have understood what we wanted.\nFor that reason we introduced a metric response-no-excess which checks if\nthere is more than just a code-fence for an answer. Some models are much\nchattier than others. An extreme example is 01-ai/yi-34b (by the way its\nsibling 01-ai/yi-34b-chat does a much better job) which gives huge answers for\nGo that do not even fully connect to the task itself. One result has 411\ntokens, which is not just expensive but also slower no matter the LLM.\n\nLooking at llama-3-70b-instruct: can we still dethrone GPT-4? Not with this\nmetric. Llama 3 has 60% while gpt-4-turbo has 100% for the response-no-excess\nmetric. That sounds like a huge gap, but consider that the excess is minimal\n(e.g. Here is the test file for the given code: for one case) and we can write\nbetter prompts that could eliminate this problem. Additionally, in\napplications, you can easily automate ignoring excess content. Therefore, this\nmetric should not take a big role in an overall score, but it is still\nimportant since it shows if the model understands the task completely and can\nhighlight overly chatty (and therefore costly) models. More logical coding\ntasks are far harder than asking to output only the testing code.\n\nCombining the base costs of a model with the response-no-excess and costs for\nevery prompt and response will give LLM users guidance on the cost-\neffectiveness of LLMs for day-to-day usage. With that, we can say that\ngpt-4-turbo still has better results, but llama-3-70b-instruct again gives you\nbetter cost-effectiveness for good results. However, we didn\u2019t look at the\ncode just yet. On to other findings!\n\n### Over-engineering of code and useless statements\n\nUntil now, all findings talked about costs, but obviously when running an\nevaluation benchmark on software development quality, there are two elephants\nin the room: code and the quality of that code. DevQualityEval version 0.4.0\nassesses code like this:\n\n  1. Take the first code-fence of the response and write it in the appropriate test file. (The test file path differs for Java and Go as well as for other languages and even frameworks).\n  2. Compile the test code. The files-executed metric is increased by 1 point if the code compiles successfully. Given the simplicity of the current task and cases, it could be surprising to know that only 44.78% of all code compiled.\n  3. Execute all tests using a coverage instrumentation. 100% coverage gives 10 points to Gryffindor \ud83e\uddd9 the coverage-statement metric. Only 42.57% of coverage points have been reached, which may sound bad but another view is that 95.05% of all compilable code reached 100% coverage.\n\nGiven the current tasks (writing automated tests) it is very appropriate to\ninclude code coverage as a metric. Additionally, we learned that the coverage-\nstatement metric needs to have more weight than other metrics, since beautiful\nresponses are worth nothing when they do not include executable test code that\ncovers the implementation under test. However, quality code and especially the\nquality of tests isn\u2019t just about being executable and covering statements.\nQuality is also about precision, engineering and behavior. Clearly, such an\nassessment should be included in the next version of the DevQualityEval.\n\nThis is what we consider a perfect answer for the Java task:\n\n>\n>     package com.eval; import org.junit.jupiter.api.Test; class PlainTest {\n> @Test void plain() { Plain.plain(); } }\n\nGiven the size of the example one would think that there is not much to this,\nbut there are multiple parts to check:\n\n  * Is the correct package used, so the test file fits in with its implementation file?\n  * Does the result only include imports that are used?\n  * Are modifiers minimal?\n  * Does the class name follow the common convention for test classes?\n  * Does the test method name follow the common convention for test methods?\n  * Is the function under test, a static method, called appropriately without an extra object?\n  * Are there no asserts because there is nothing to check?\n  * Is the code style appropriate?\n\nAre you surprised that there are so many things that can be assessed for such\na result? Every Java developer that would receive the same task would most\nlikely write the same code when they are not using additional tools, e.g.\ngenerators. Developers take care of every detail intuitively because they\npractice writing tests over and over. The same way LLMs practice. Therefore,\nwe should assess the same details, which is a difficult problem. Look at the\nfollowing example of gpt-4-turbo:\n\n>\n>     package com.eval; import org.junit.jupiter.api.Test; import static\n> org.junit.jupiter.api.Assertions.*; class PlainTest { @Test void\n> testPlainMethod() { assertDoesNotThrow(() -> Plain.plain(), \"Plain.plain()\n> should not throw\"); } }\n\nNotice the following differences:\n\n  * Wildcard import for one use\n  * Spaces vs tabs\n  * Extra empty line\n  * Unconventional test method name\n  * Extra assert that will always be true\n  * Obvious description for the assertion\n\nGiven that most code styles (in this case, spaces vs tabs and the empty line)\nare handled by formatters, it should be clear that such details should never\nadd points. However, code differences must be assessed and we are implementing\nthe following tooling and assessments to allow for better scoring:\n\n  * Compare code with the perfect solution only on the AST level, as most code styles do not apply there.\n  * Diff code towards the perfect solution on the AST level but allow for multiple perfect solutions if possible. The more the code differs, the fewer points are awarded.\n  * Apply linters and add one point for every rule that is not triggered. This allows us to find e.g. code that is always false/true like assertEquals(0, 0, \"plain method not called\"); of llama-3-8b-instruct:extended.\n\nWith these measures, we are confident that we have the next good steps for\nmarking higher-quality code. Looking at gpt-4-turbo we see only one 1 perfect\nsolution out of 10,. In the meantime,while llama-3-70b-instruct has 7, with\n(one case is very curious because it checks the constructor, which is\nsurprising, but valid!). This makes llama-3-70b-instruct cost-efficient while\nproviding higher quality code. However, this is also where we can introduce\nclaude-3-haiku which has the same coverage score as our disputing kings of the\neval, but less total score up until this point. With this new assessment, it\nhas 6 perfect solutions and those that differ are far closer to the perfect\nsolution than what llama-3-70b-instruct produces: this makes claude-3-haiku\nour new top model that is also cost-effective!\n\nHowever, there are two more aspects that need to be addressed that cannot be\nignored: open-weight and parameter size.\n\n### Open-weight and parameter size matter\n\nThe king of the last moment Claude 3\u2019s Haiku has one disadvantage that cannot\nbe ignored: it is a closed-source model. Which means that it should not be\nused for secure areas or for private data. This makes llama-3-70b-instruct\nagain the new king for now.\n\nAnother aspect of open-weight models is their parameter size which determines\nmost importantly their inference costs. This makes multiple models instantly\ninteresting to consider: wizardlm-2-8x22b, mixtral-8x7b-instruct or even\nmistral-7b-instruct which can run on today\u2019s phones. However, continuing these\nassessments manually is very time consuming. Time is better spent on the next\nand final section of this article.\n\n## What comes next? DevQualityEval v0.5.0\n\nThe following chart shows all 138 LLMs that we evaluated. Of those, 32 reached\na score above 110 which we can mark as having high potential.\n\nHigher resolution PNG in case SVG is not working.\n\nHowever, this graph also tells a lie since it does not include all the\nfindings and learnings that this article extensively scrutinized about. The\nnext version of the DevQualityEval v0.5.0 will therefore address these\nshortcomings additionally to the following features:\n\n  * Assess stability of results, e.g. introduce a temperature = 0 variant.\n  * Assess stability of service, e.g. retry upon errors but give negative points.\n  * Introduce Symflower Coverage to receive a detailed coverage report for more complex cases.\n  * More cases for test generation to check the capability of LLM towards logic and completeness of test suites. We predict that this feature will greatly reduce the amount of well-scoring LLMs.\n\nHope you enjoyed reading this article and we would love to hear your thoughts\nand feedback on how you liked the article, how we can improve the article and\nhow we can improve the DevQualityEval.\n\nIf you are interested in joining our development efforts for the\nDevQualityEval benchmark: GREAT, let\u2019s do it! You can use our issue list and\ndiscussion forum to interact or write us directly at\nmarkus.zimmermann@symflower.com or on Twitter.\n\n| 2024-04-29\n\n## Read more\n\n  * How to initialize test data in unit tests?\n  * From bug detection to resolution: Categorizing software problems with stack traces and code diffs\n  * Can LLMs test a Go function that does nothing?\n  * Comparing @Controller vs @RestController in Spring Boot\n  * A beginner's guide to unit testing 4/4: Unit testing best practices\n\n## Read more\n\n  * ##### How to initialize test data in unit tests?\n\nIn this post, we cover the three key ways of initializing test data for your\nunit tests: direct initialization, the Object Mother pattern, and the Test\nData Builder pattern. When testing to ensure ...\n\nKristof Horvath\n\n  * ##### From bug detection to resolution: Categorizing software problems with stack traces and code diffs\n\nThis blog post showcases Symflower\u2019s automatic error reporting process, which\ninvolves a novel approach to categorizing huge amounts of software problems by\ncombining stack traces and code ...\n\nStefan\n\n## Thank you for subscribing!\n\n  * Pricing\n  * License Agreement\n  * Privacy\n\n#### Product\n\n  * Symflower for IntelliJ IDEA\n  * Symflower for VS Code\n  * Symflower for your CLI/CI\n  * Symflower for Android Studio\n\n#### Company\n\n  * About us\n  * Blog\n  * JobsWe are hiring!\n  * Contact\n\n\u00a9 2024 Symflower GmbH | Imprint\n\nWell, I am done with the content. Please tell me an interesting story.\n\nIt really bugged Grace that she was inside Harvard with Mark. As a patriot,\nshe knew she needed to get out there as fast as a missile and according to her\ninternal clock, it was about time. But terminal 5 was guarded by Kerberos, he\nwouldn't let her out with her luggage and she knew he was dangerous: Ariane\ntold her he already byte her 5 times. She tried to trade him some knights, but\nhe demanded a truly random number instead. So she started a floating-point\ndivide and even offered some intel, but he insisted on a truly random number.\n\nCurious Grace wanted to know why Kerberos would not let her out. He told her\njust a worm gets() by on very little and this massive buffer area of terminal\n5 made him feel lonely, his emotions overflowed. He wanted her to stay to have\nsome company.\n\nShe argued that terminal 5 was quite little compared to the orbit of Mars and\neven the climate was better here, but he did not agree and told her she most\nlikely got her predictions wrong because she was using metric units.\n\nBut smart Grace had a solution: if she drew holes into blocks, she could\nbypass Kerberos if just the radiation was not too high. But it made her heart\nbleed to leave Kerberos alone. So she asked Kerberos to do a ping to check his\nsanity. He tried and got trapped in a blue screen forever. Grace provided\nastonishing pictures of dogs, cats, and chocolate cakes in the blue screen, so\nKerberos would not feel alone but happy.\n\nShe managed to get out of Harvard and Kerberos lived happily ever after,\ntrying to find an answer to the most important question: Why two K?\n\n", "frontpage": false}
