{"aid": "40209160", "title": "GGML Flash Attention support merged into llama.cpp", "url": "https://github.com/ggerganov/llama.cpp/pull/5021", "domain": "github.com/ggerganov", "votes": 2, "user": "smcleod", "posted_at": "2024-04-30 10:05:50", "comments": 0, "source_title": "ggml : add Flash Attention by ggerganov \u00b7 Pull Request #5021 \u00b7 ggerganov/llama.cpp", "source_text": "ggml : add Flash Attention by ggerganov \u00b7 Pull Request #5021 \u00b7\nggerganov/llama.cpp \u00b7 GitHub\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nggerganov / llama.cpp Public\n\n  * Notifications\n  * Fork 8.1k\n  * Star 57.2k\n\nJump to bottom\n\n# ggml : add Flash Attention #5021\n\nMerged\n\nggerganov merged 145 commits into master from gg/flash-attn Apr 30, 2024\n\nMerged\n\n# ggml : add Flash Attention #5021\n\nggerganov merged 145 commits into master from gg/flash-attn Apr 30, 2024\n\n+2,917 \u2212453\n\n## Conversation\n\nOwner\n\n###\n\nggerganov commented Jan 18, 2024 \u2022\n\nref #3365\n\nSetting up what's needed for Flash Attention support in ggml and llama.cpp\n\nThe proposed operator performs:\n\n    \n    \n    // new res = ggml_flash_attn(ctx, q, k, v, kq_mask, kq_scale); // fused scale + mask + soft_max (old) kq = ggml_mul_mat (ctx, k, q); kq = ggml_soft_max_ext(ctx, kq, kq_mask, kq_scale); kqv = ggml_mul_mat (ctx, v, kq); kqv = ggml_permute (ctx, kqv, 0, 2, 1, 3); res = ggml_cont_2d (ctx, kqv, n_embd_head_k*n_head, n_tokens); // unfused (old) kq = ggml_mul_mat (ctx, k, q); kq = ggml_scale (ctx, kq, kq_scale); kq = ggml_add (ctx, kq, kq_mask); kq = ggml_soft_max(ctx, kq); kqv = ggml_mul_mat (ctx, v, kq); kqv = ggml_permute (ctx, kqv, 0, 2, 1, 3); res = ggml_cont_2d (ctx, kqv, n_embd_head_k*n_head, n_tokens);\n\nSuggestions and comments for the API are welcome. Looking for help in\nimplementing efficient GPU kernels - please open PR to this branch if you have\nproposals\n\n  * ggml API: ggml_flash_attn_ext()\n  * llama.cpp use in llm_build_kqv()\n  * add test-backend-ops test\n  * CPU implementation (slow, just for testing)\n  * CUDA implementation (CUDA: Faster FlashAttention kernel #6374)\n  * Metal implementation\n  * GGML_PREC_F32 support (CUDA) (CUDA: faster FlashAttention for batch sizes > 1 #6646)\n  * GGML_PREC_F32 support (Metal)\n\n## Changes to ggml/llama\n\n  * Add new op GGML_OP_FLASH_ATTN_EXT and ggml_flash_attn_ext() call (before merging we can consider reusing the old GGML_OP_FLASH_ATTN and removing the legacy code)\n  * Change mask type to F16 for ggml_soft_max_ext() and require that it is padded to GGML_KQ_MASK_PAD 32\n  * The n_kv denoting the number of computed tokens from the KV cache is now padded to 128 (from 32) to support larger FA blocks without making out-of-bounds access\n  * The minimum llama_context_params.n_batch that can be used is GGML_KQ_MASK_PAD 32 to avoid out-of-bounds access in the FA kernels for small batch size\n  * The V tensor is no longer transposed when storing it in the KV cache\n  * The input buffer is cleared with zeros to avoid NaNs in the padded tensors\n\n## Things to consider\n\n  * Pass KQ list with/instead of KQ mask\n  * Pass block-wise KQ mask\n  * Support Alibi\n  * Finally transform Alibi as ggml_add()? (low-prio)\n  * No longer store transposed V-cache (gg/flash-attn-online)\n\n## Testing\n\n    \n    \n    ./tests/test-backend-ops -o FLASH_ATTN_EXT\n\n  * main, server: add -fa\n  * llama-bench: add -fa 1\n\n## Benchmark\n\nBaseline:\n\n    \n    \n    # CUDA LLAMA_CUBLAS=1 make -j tests && ./tests/test-backend-ops -o ATTN -b CUDA0 perf # Metal LLAMA_CUBLAS=1 make -j tests && ./tests/test-backend-ops -o ATTN -b Metal perf\n\nFA kernel:\n\n    \n    \n    # CUDA LLAMA_CUBLAS=1 make -j tests && ./tests/test-backend-ops -o FLASH_ATTN_EXT -b CUDA0 perf # Metal LLAMA_CUBLAS=1 make -j tests && ./tests/test-backend-ops -o FLASH_ATTN_EXT -b Metal perf\n\nText-generation after long prompt:\n\n    \n    \n    # without flash attention ./batched-bench models/mistral-instruct-7b-v0.2/ggml-model-f16.gguf 10000 2048 512 0 1 99 8192 256 1 # with flash attention ./batched-bench models/mistral-instruct-7b-v0.2/ggml-model-f16.gguf 10000 2048 512 1 1 99 8192 256 1\n\n## References\n\n  * https://arxiv.org/pdf/1805.02867.pdf Online softmax\n  * https://arxiv.org/pdf/2112.05682.pdf O(n) memory self-attention\n  * https://arxiv.org/pdf/2307.08691.pdf Flash-attention 2\n\nggml : add ggml_flash_attn_ext API\n\na1c004e\n\nggerganov added help wanted Extra attention is needed performance Speed\nrelated topics labels Jan 18, 2024\n\nggerganov closed this Jan 18, 2024\n\nggerganov reopened this Jan 18, 2024\n\nggerganov marked this pull request as draft January 18, 2024 17:09\n\nCollaborator\n\n###\n\nslaren commented Jan 18, 2024 \u2022\n\nSince we are doing this from scratch, wouldn't it be better to remove the\ncustom attention mask entirely and pass a list of KV cells used in each\nsequence? Considering our implementation of batching, I think we should be\nlooking at implementing something closer to paged attention rather than flash\nattention. I suppose it is possible to convert the mask to a list of sequences\nin the kernels, but it would be less efficient.  \n---  \n  \nOwner Author\n\n###\n\nggerganov commented Jan 18, 2024 \u2022\n\nYes, we can pass list instead of mask. I am not sure of the format though - if\neach list has different length I feel it will hinder the GPU performance.Edit:\nI just got an idea - we can pass both the kq_mask as it is, plus a second\nboolean tensor that tells each token to which KV blocks it should attend. For\nexample, we split the KV cache in blocks of 128 (or some other round number)\nand a token (i.e. row in q) attends to a block if atleast one of the cells in\nit belongs to the token's sequence. This way, we can skip entire blocks of the\nKV cache that do not belong to the current sequence and keep the problem\nparallel-friendly. Thoughts?  \n---  \n  \nCollaborator\n\n###\n\nslaren commented Jan 18, 2024\n\nWe could use a vector with dimension [num_seqs] that contains the length of\nthe sequences, and a 2D tensor with dimensions [max_seq_len, num_seqs] that\ncontains the KV cells in each sequence, padded to the length of the longest\nsequence.  \n---  \n  \nCollaborator\n\n###\n\nslaren commented Jan 18, 2024\n\nIt seems that vLLM has added a new version of paged attention since it looked\ninto the implementation (vllm-project/vllm#1348). I am not sure what are the\nchanges, but I think it is worth looking into what they are doing. The kernel\nis in https://github.com/vllm-\nproject/vllm/blob/main/csrc/attention/attention_kernels.cu  \n---  \n  \nCollaborator\n\n###\n\nslaren commented Jan 18, 2024\n\nAlibi could also be done in this kernel.  \n---  \n  \nOwner Author\n\n###\n\nggerganov commented Jan 18, 2024\n\nRegarding the Alibi, I feel reinterpreting it as a KQ_mask via ggml_add() is a\nmore general solution - we will avoid having a ggml_alibi() operator and\nexplicit support in the kernels that we write (like in vLLM).It remains to be\nseen though if the KQ_mask will be a bottleneck - my feeling is that just\navoiding the extra read/write of KQ will bring us close to the optimal\nperformance, even with the existing \"cross-KV compute\" drawback.Will take a\nlook at the vLLM code and I've updated the description with some of the things\nfrom this discussion  \n---  \n  \n###\n\ncalvintwr commented Jan 19, 2024\n\n@ggerganov @slaren Together with @JohannesGaessler and @FSSRepo we are working\non the same thing over at Pints-App#1 which we intend to do a pull to llamacpp\nonce work is done.However, I think we will converge into this one. Given the\namount of work here, @ggerganov @slaren how do you want to organise this? The\n3 of us are in a temporary discord group actually to work this out, perhaps we\ncan use that?What are your thoughts?  \n---  \n  \nOwner Author\n\n###\n\nggerganov commented Jan 19, 2024 \u2022\n\nDiscord is not an option for me - I prefer to communicate over Github issues /\ndiscussions / e-mail.Happy to see you have started work on the CUDA\nimplementation. Please take into account the proposed API here - note that it\nis still a WIP and can change. I can review the implementation that you have\nwhen you think it is in a good state. Would prefer PR's that are compatible\nwith this branch so we can verify correctness using test-backend-ops and\nsupport for all backends.  \n---  \n  \nggml : fix GQA support in ggml_flash_attn_ext\n\nfa7ebcc\n\n###\n\ncalvintwr commented Jan 20, 2024\n\n@ggerganov Got it. Let us work on a plan to converge with this PR.  \n---  \n  \nggerganov added 2 commits January 20, 2024 10:12\n\nMerge branch 'master' into gg/flash-attn\n\nc3cdfff\n\nggml : online attention (CPU)\n\na9681fe\n\nCollaborator\n\n###\n\ncebtenzzre commented Jan 20, 2024 \u2022\n\ntest-backend-ops -o FLASH_ATTN_EXT fails for Metal on my M2 Pro, is this\nknown? edit: I see, not implemented yet.  \n---  \n  \n###\n\nJianbangZ commented Jan 20, 2024\n\nAny performance numbers?  \n---  \n  \nggerganov added 3 commits January 21, 2024 10:15\n\nmetal : initial implementation\n\n1173f49\n\nmetal : f16 precision\n\n528da75\n\nmetal : reduce branches\n\n52ae085\n\nggerganov force-pushed the gg/flash-attn branch from e0ba0da to 52ae085\nCompare January 21, 2024 09:59\n\nggerganov added 6 commits January 21, 2024 12:01\n\nmetal : specialize for head size\n\nb973258\n\nwip : 8 rows per simd group\n\n8cde449\n\nwip : 4 rows per simd group\n\nf31955f\n\nwip : template for rows per warp\n\na4b6341\n\nmetal : parallelize across KV size\n\n77d08f3\n\nmetal : parallel reduce across heads\n\n17720fa\n\nCollaborator\n\n###\n\nslaren commented Apr 25, 2024\n\n./save-load-state -fa fails. I think that the KV save/load code needs to be\nupdated to support the non-transposed V.  \n---  \n  \nllama : support save/load state with FA enabled\n\n1fd5bc3\n\n    \n    \n    ggml-ci\n\nCollaborator\n\n###\n\nslaren commented Apr 25, 2024\n\nsave-load-state still fails with CUDA with full offload, but that's due to a\ndifferent issue. I'll open a PR soon.  \n---  \n  \nOwner Author\n\n###\n\nggerganov commented Apr 25, 2024\n\nThere is one more failure case where one can save the state for example\nwithout FA, and then try to load it with FA. We can handle this with an extra\ninfo in the state that indicates if the V cache is transposed  \n---  \n  \nslaren reviewed Apr 25, 2024\n\nView reviewed changes\n\nggerganov added 3 commits April 25, 2024 19:01\n\nMerge branch 'master' into gg/flash-attn\n\n09d0381\n\nci : add CUDA save-load-state tests\n\nac1c6d9\n\n    \n    \n    ggml-ci\n\nllama : llama_kv_cache_clear zeroes data + fix save-load seq\n\nc225609\n\n    \n    \n    ggml-ci\n\nOwner Author\n\n###\n\nggerganov commented Apr 25, 2024\n\nI've updated llama_kv_cache_clear to now also zero-out the KV tensor data.\nBefore this change, the save-load-state test was incorrectly successful in the\nlast case where we save and restore the state of a single sequence in the case\nof !v_trans. The reason is that the cache maintained it's data regardless in\nwhat order we get and set it.The v_trans and !v_trans handling is definitely\nnot great - need to figure out ways in the future to simplify this  \n---  \n  \nggerganov added 2 commits April 25, 2024 19:45\n\nllama : fix copy-paste errors, add TODO\n\nbab346b\n\nllama : disallow incompatible states\n\n0fc5c5e\n\nslaren reviewed Apr 25, 2024\n\nView reviewed changes\n\nggerganov added 3 commits April 25, 2024 20:06\n\nllama : update llama_state_get_size after v_trans field\n\n1e590ac\n\nmetal : remove tmp log\n\n4f4c024\n\nllama : add static reminder for llama_state_get_size\n\n9e38760\n\nslaren approved these changes Apr 26, 2024\n\nView reviewed changes\n\nhenk717 mentioned this pull request Apr 27, 2024\n\nImplement 4-bit quantized KV Cache for faster performance and to enable longer\ncontext #6863\n\nOpen\n\n3 tasks\n\nFSSRepo approved these changes Apr 28, 2024\n\nView reviewed changes\n\nCollaborator\n\n###\n\nFSSRepo left a comment\n\nThere was a problem hiding this comment.\n\n### Choose a reason for hiding this comment\n\nThe reason will be displayed to describe this comment to others. Learn more.\n\nIn most cases, the FA kernel represents no more than 15% of the computation\ngraph's execution time, so the performance improvement is very slight, but at\nleast we gave this approach a chance.\n\n###\n\nJamshedQurbonboev commented Apr 28, 2024\n\nIt has been months since start of this PR. Someone interested in how much this\nPR would increase his performance(or reduce memory consumption), would be\nforced to gather information from different timeframes and different PRs.\nCould someone gather such information in handy table? Or better yet, test\nperformance under master branch and this branch on various backends and report\nit there  \n---  \n  \nggerganov added 2 commits April 29, 2024 17:19\n\nMerge branch 'master' into gg/flash-attn\n\na1616e9\n\n    \n    \n    ggml-ci\n\nMerge branch 'master' into gg/flash-attn\n\nca0275c\n\n    \n    \n    ggml-ci\n\nCollaborator\n\n###\n\nochafik commented Apr 29, 2024\n\n-fa seems to be causing the following command to crash (GGML_ASSERT: ggml-metal.m:2652: smem <= ctx->device.maxThreadgroupMemoryLength) on an M3 Pro 36GB RAM:\n    \n    \n    ./main -fa \\ -p \"$( python -c 'print(\"abc\" * 1000)' )\" \\ -n 10 \\ -mu https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf \\ -m models/Phi-3-mini-4k-instruct-q4.gguf \\ --seed 123 \\ -c 2000 \\ --no-display-prompt\n\n(works w/o -fa, w/ -n 1 - seems to crash upon the 2nd token - and w/ smaller\nprompts)  \n---  \n  \nOwner Author\n\n###\n\nggerganov commented Apr 29, 2024\n\nWhat is the value of ctx->device.maxThreadgroupMemoryLength on your device?  \n---  \n  \nCollaborator\n\n###\n\nochafik commented Apr 29, 2024 \u2022\n\n> What is the value of ctx->device.maxThreadgroupMemoryLength on your device?\n\nIt's 32kB, uncommented the line before the assert and it prints smem: 42496,\nmax: 32768edit: also FWIW ne00: 96, ne01: 1, ne11: 1024, nsg: 32,\npipeline.maxTotalThreadsPerThreadgroup: 1024, device.maxThreadsPerThreadgroup:\n6093726360  \n---  \n  \nggerganov added 2 commits April 30, 2024 11:04\n\nmetal : fix max nsg\n\ne180fcd\n\n    \n    \n    ggml-ci\n\nci : fix arg order\n\nc240ae2\n\n    \n    \n    ggml-ci\n\nggerganov merged commit 9c67c27 into master Apr 30, 2024\n\n69 checks passed\n\nThis was referenced Apr 30, 2024\n\nFused attention kernel for small batch sizes #6178\n\nClosed\n\nFlashAttention: pragma unroll, use_mask template parameter #6165\n\nClosed\n\nCollaborator\n\n###\n\nochafik commented Apr 30, 2024 \u2022\n\nSo excited to see this land!! \ud83c\udf89(edit: can confirm the crash is fixed)Did quick\nllama-bench runs on Metal and I\u2019m seeing ~5% increase of pp t/s & ~2.5%\nincrease of tg t/s on an M3 Pro (18gpu), and 8-9% increase of pp t/s & ~6%\nincrease of tg t/s on an M1 Ultra (64gpu).I also ran a custom benchmark which\nimplies the speed up on Metal increases both with prompt length and generation\nlength (plateauing at 9.5% faster), but I might have got my script wrongMeta-\nLlama-3-8B-Instruct-Q4_K_M.ggufM1 Ultra 128gb 64gpu| prompt \\ n| 10| 100| 1000  \n---|---|---|---  \n10| -0.68%| -1.43%| -5.33%  \n100| -0.70%| -3.88%| -5.57%  \n1000| -1.41%| -4.81%| -9.26%  \n  \nM3 Pro 36gb 18gpu\n\nprompt \\ n| 10| 100| 1000  \n---|---|---|---  \n10| -2.26%| -2.39%| -9.45%  \n100| -0.85%| -2.41%| -7.69%  \n1000| -5.23%| -8.13%| -9.61%  \n  \nSign up for free to join this conversation on GitHub. Already have an account?\nSign in to comment\n\nLabels\n\nneed feedback Testing and feedback with results are needed performance Speed\nrelated topics\n\n16 participants\n\nAdd this suggestion to a batch that can be applied as a single commit. This\nsuggestion is invalid because no changes were made to the code. Suggestions\ncannot be applied while the pull request is closed. Suggestions cannot be\napplied while viewing a subset of changes. Only one suggestion per line can be\napplied in a batch. Add this suggestion to a batch that can be applied as a\nsingle commit. Applying suggestions on deleted lines is not supported. You\nmust change the existing code in this line in order to create a valid\nsuggestion. Outdated suggestions cannot be applied. This suggestion has been\napplied or marked resolved. Suggestions cannot be applied from pending\nreviews. Suggestions cannot be applied on multi-line comments. Suggestions\ncannot be applied while the pull request is queued to merge. Suggestion cannot\nbe applied right now. Please check back later.\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
