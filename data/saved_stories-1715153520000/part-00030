{"aid": "40291549", "title": "Ninety-Five Theses on AI", "url": "https://www.secondbest.ca/p/ninety-five-theses-on-ai", "domain": "secondbest.ca", "votes": 2, "user": "kvee", "posted_at": "2024-05-07 21:11:38", "comments": 0, "source_title": "Ninety-five theses on AI", "source_text": "Ninety-five theses on AI - by Samuel Hammond - Second Best\n\nShare this post\n\n#### Ninety-five theses on AI\n\nwww.secondbest.ca\n\n#### Discover more from Second Best\n\nImperfect takes with uncertain payoff\n\nOver 4,000 subscribers\n\nContinue reading\n\nSign in\n\n# Ninety-five theses on AI\n\n### In no particular order\n\nSamuel Hammond\n\nMay 07, 2024\n\n6\n\nShare this post\n\n#### Ninety-five theses on AI\n\nwww.secondbest.ca\n\n2\n\nShare\n\n####\n\nOversight of AGI labs is prudent\n\n  1. It is in the U.S. national interest to closely monitor frontier model capabilities.\n\n  2. You can be ambivalent about the usefulness of most forms of AI regulation and still favor oversight of the frontier labs.\n\n  3. As a temporary measure, using compute thresholds to pick out the AGI labs for safety-testing and disclosures is as light-touch and well-targeted as it gets.\n\n  4. The dogma that we should only regulate technologies based on \u201cuse\u201d or \u201crisk\u201d may sound more market-friendly, but often results in a far broader regulatory scope than technology-specific approaches (see: the EU AI Act).\n\n  5. Training compute is an imperfect but robust proxy for model capability, and has the immense virtue of simplicity.\n\n  6. The use of the Defense Production Act to require disclosures from frontier labs is appropriate given the unique affordances available to the Department of Defense, and the bona fide national security risks associated with sufficiently advanced forms of AI.\n\n  7. You can question the nearness of AGI / superintelligence / other \u201cdual use\u201d capabilities and still see the invocation of the DPA as prudent for the option value it provides under conditions of fundamental uncertainty.\n\n  8. Requiring safety testing and disclosures for the outputs of $100 million-plus training runs is not an example of regulatory capture nor a meaningful barrier to entry relative to the cost of compute.\n\n####\n\nMost proposed \u201cAI regulations\u201d are ill-conceived or premature\n\n  1. There is a substantial premium on discretion and autonomy in government policymaking whenever events are fast moving and uncertain, as with AI.\n\n  2. It is unwise to craft comprehensive statutory regulation at a technological inflection point, as the basic ontology of what is being regulated is in flux.\n\n  3. The optimal policy response to AI likely combines targeted regulation with comprehensive deregulation across most sectors.\n\n  4. Regulations codify rules, standards and processes fit for a particular mode of production and industry structure, and are liable to obsolesce in periods of rapid technological change.\n\n  5. The benefits of deregulation come less from static efficiency gains than from the greater capacity of markets and governments to adapt to innovation.\n\n  6. The main regulatory barriers to the commercial adoption of AI are within legacy laws and regulations, mostly not prospective AI-specific laws.\n\n  7. The shorter the timeline to AGI, the sooner policymaker and organizations should switch focus to \u201cbracing for impact.\u201d\n\n  8. The most robust forms of AI governance will involve the infrastructure and hardware layers.\n\n  9. Existing laws and regulations are calibrated with the expectation of imperfect enforcement.\n\n  10. To the extent AI greatly reduces monitoring and enforcement costs, the de facto stringency of all existing laws and regulations will greatly increase absent a broader liberalization.\n\n  11. States should focus on public sector modernization and regulatory sandboxes and avoid creating an incompatible patchwork of AI safety regulations.\n\n####\n\nAI progress is accelerating, not plateauing\n\n  1. The last 12 months of AI progress were the slowest they\u2019ll be for the foreseeable future.\n\n  2. Scaling LLMs still has a long way to go, but will not result in superintelligence on its own, as minimizing cross-entropy loss over human-generated data converges to human-level intelligence.\n\n  3. Exceeding human-level reasoning will require training methods beyond next token prediction, such as reinforcement learning and self-play, that (once working) will reap immediate benefits from scale.\n\n  4. RL-based threat models have been discounted prematurely.\n\n  5. Future AI breakthroughs could be fairly discontinuous, particularly with respect to agents.\n\n  6. AGI may cause a speed-up in R&D and quickly go superhuman, but is unlikely to \u201cfoom\u201d into a god-like ASI given compute bottlenecks and the irreducibility of high dimensional vector spaces, i.e. Ray Kurzweil is underrated.\n\n  7. Recursive self-improvement and meta-learning may nonetheless give rise to dangerously powerful AI systems within the bounds of existing hardware.\n\n  8. Slow take-offs eventually become hard.\n\n####\n\nOpen source is mostly a red-herring\n\n  1. The delta between proprietary AI models and open source will grow overtime, even as smaller, open models become much more capable.\n\n  2. Within the next two years, frontier models will cross capability thresholds that notable open source advocates will agree are dangerous to open source ex ante.\n\n  3. No major open source AI model has been dangerous to date, while the benefits from open sourcing models like Llama3 and AlphaFold are immense.\n\n  4. True \u201copen source\u201d means open sourcing training data and code, not just model weights, which is essential for avoiding the spread of models with Sleeper Agents or contaminated data.\n\n  5. The most dangerous AI models will be expensive to train and only feasible for large companies, suggesting our focus should be on monitoring frontier capabilities.\n\n  6. The open vs. closed source debate is mainly a debate about Meta, not deeper philosophical ideals.\n\n  7. It is not in Meta\u2019s shareholders\u2019 interest to unleash an unfriendly AI into the world.\n\n  8. Companies governed by nonprofit boards and CEOs who don\u2019t take compensation face lower-powered incentives against AI x-risk than your typical publicly traded company.\n\n  9. Lower-tier AI risks, like from the proliferation of deepfakes, are collective action problems that will be primarily mitigated through defensive technologies and institutional adaptation.\n\n  10. Restrictions on open source risk undermining adaptation by incidentally restricting the diffusion of defensive forms of AI.\n\n  11. Trying to restrict access to capabilities that are widely available and / or cheap to train from scratch is pointless in a free society, and likely to do more harm than good.\n\n  12. Nonetheless, releasing an exotic animal into the wild is a felony.\n\n####\n\nAccelerate vs. decelerate is a false dichotomy\n\n  1. Decisions made in the next decade are more highly levered to shape the future of humanity than at any point in human history.\n\n  2. You can love technology and be an \u201caccelerationist\u201d across virtually every domain \u2014 housing, transportation, healthcare, space commercialization, etc. \u2014 and still be concerned about future AI risks.\n\n  3. \u201cAccelerate vs. decelerate\u201d imagines technology as a linear process when technological innovation is more like a search down branching paths.\n\n  4. If the AI transition is a civilizational bottleneck (a \u201cGreat Filter\u201d), survival likely depends more on which paths we are going down than at what speed, except insofar as speed collapses our window to shift paths.\n\n  5. Building an AGI carries singular risks that merit being treated as a scientific endeavor, pursued with seriousness and trepidation.\n\n  6. Tribal mood affiliations undermine epistemic rationality.\n\n  7. e/acc and EA are two sides of the same rationalist coin: EA is rooted in Christian humanism; e/acc in Nietzschean atheism.\n\n  8. The de facto lobby for \u201caccelerationism\u201d in Washington, D.C., vastly outstrips the lobby for AI safety.\n\n  9. It genuinely isn\u2019t obvious whether Trump or Biden is better for AI x-risk.\n\n  10. EAs have more relationships on the Democratic side, but can work in either administration and are a tiny contingent all things considered.\n\n  11. Libertarians, e/accs, and Christian conservatives \u2014 whatever their faults \u2014 have a far more realistic conception of AI and government than your average progressive.\n\n  12. The more one thinks AI goes badly by default, the more one should favor a second Trump term precisely because he is so much higher variance.\n\n  13. Steve Bannon believes the singularity is near and a serious existential risk; Janet Haven thinks AI is Web3 all over again.\n\n####\n\nThe AI wave is inevitable, superintelligence isn\u2019t\n\n  1. Building a unified superintelligence is an ideological goal, not a fait accompli.\n\n  2. The race to build a superintelligence is driven by two or three U.S. companies with significant degrees of freedom over near-term developments, as distinguished from the inevitability of the AI transition more generally.\n\n  3. Creating a superintelligence is inherently dangerous and destabilizing, independent of the hardness of alignment.\n\n  4. We can use advanced AI to accelerate science, cure diseases, solve fusion, etc., without ever building a unified superintelligence.\n\n  5. Creating an ASI is a direct threat to the sovereign.\n\n  6. AGI labs led by childless Buddhists with alt accounts are probably more risk tolerant than is optimal.\n\n  7. Sam Altman and Sam Bankman-Fried are more the same than different.\n\n  8. High functioning psychopaths demonstrate anti-social behaviors in their youth but learn to compensate in adulthood, becoming adept social manipulators with grandiose visions and a drive to \u201cwin\u201d at all cost.\n\n  9. Corporate malfeasance is mostly driven by bad incentives and \u201ctechniques of neutralization\u201d \u2014 convenient excuses for over-riding normative constraints, such as \u201cIf I didn\u2019t, someone else would.\u201d\n\n####\n\nTechnological transitions cause regime changes\n\n  1. Even under best case scenarios, an intelligence explosion is likely to induce state collapse / regime change and other severe collective action problems that will be hard to adapt to in real time.\n\n  2. Government bureaucracies are themselves highly exposed to disruption by AI, and will need \u201cfirmware-level\u201d reforms to adapt and keep-up, i.e. reforms to civil service, procurement, administrative procedure, and agency structure.\n\n  3. Congress will need to have a degree of legislative productivity not seen since FDR.\n\n  4. Inhibiting the diffusion of AI in the public sector through additional layers of process and oversight (such as through Biden\u2019s OMB directive) tangibly raises the risk of systemic government failure.\n\n  5. The rapid diffusion of AI agents with approximately human-level reasoning and planning abilities is likely sufficient to destabilize most existing U.S. institutions.\n\n  6. The reference class of prior technological transitions (agricultural revolution, printing press, industrialization) all feature regime changes to varying degrees.\n\n  7. Seemingly minor technological developments can affect large scale social dynamics in equilibrium (see: Social media and the Arab Spring or the Stirrup Thesis).\n\n####\n\nInstitutional regime changes are packaged deals\n\n  1. Governments and markets are both kinds of spontaneous orders, making the 19th and 20th century conception of liberal democratic capitalism a technologically-contingent equilibrium.\n\n  2. Technological transitions are packaged deals, e.g. free markets and the industrial revolution went hand-in-hand with the rise of \u201cbig government\u201d (see Tyler Cowen on The Paradox of Libertarianism).\n\n  3. The AI-native institutions created in the wake of an intelligence explosion are unlikely to have much continuity with liberal democracy as we now know it.\n\n  4. In steady state, maximally democratized AI could paradoxically hasten the rise of an AI Leviathan by generating irreversible negative externalities that spur demand for ubiquitous surveillance and social control.\n\n  5. Periods of rapid technological change tend to shuffle existing public choice / political economy constraints, making politics more chaotic and less predictable.\n\n  6. Periods of rapid technological change tend to disrupt global power balances and make hot wars more likely.\n\n  7. Periods of rapid technological change tend to be accompanied by utopian political and religious movements that usually end badly.\n\n  8. Explosive growth scenarios imply massive property rights violations.\n\n  9. A significant increase in productivity growth will exacerbate Baumol\u2019s Cost Disease and drive mass adoption of AI policing, teachers, nurses, etc.\n\n  10. Technological unemployment is only possible in the limit where market capitalism collapses, say into a forager-style gift economy.\n\n####\n\nDismissing AGI risks as \u201csci-fi\u201d is a failure of imagination\n\n  1. If one\u2019s forecast of 2050 doesn\u2019t resemble science fiction, it\u2019s implausible.\n\n  2. There is a massive difference between something sounding \u201csci-fi\u201d and being physically unrealizable.\n\n  3. Terminator analogies are underrated.\n\n  4. Consciousness evolved because it serves a functional purpose and will be an inevitable feature of certain AI systems.\n\n  5. Human consciousness is scale-dependent and not guaranteed to exist in minds that are vastly larger or less computationally bounded.\n\n  6. Joscha Bach\u2019s Cyber Animism is the best candidate for a post-AI metaphysics.\n\n  7. The creation of artificial minds is more likely to lead to the demotion of humans\u2019 moral status than to the promotion of artificial minds into moral persons.\n\n  8. Thermodynamics may favor futures where our civilization grows and expands, but that doesn\u2019t preclude futures dominated by unconscious replicators.\n\n  9. Finite-time singularities are indicators of a phase-transition, not a bona fide singularity.\n\n  10. It is an open question whether the AI phase-transition will be more like the printing press or photosynthesis.\n\n####\n\nBiology is an information technology\n\n  1. The complexity of biology arises from processes resembling gradient descent and diffusion guided by comparatively simple reward signals and hyperparameters.\n\n  2. Full volitional control over biology is achievable, enabling the creation of arbitrary organisms that wouldn\u2019t normally be \u201cevolvable.\u201d\n\n  3. Superintelligent humans with IQs on the order of 1,000 are possible through genetic engineering.\n\n  4. Indefinite life extension is a tragedy of the anticommons.\n\n  5. There are more ways for a post-human transition to go poorly than to go well.\n\n  6. Natural constraints are often better than man-made ones because there\u2019s no one to hold responsible.\n\n  7. We live in base reality, and in nature there is no such thing as plot armor.\n\nThanks for reading Second Best! Subscribe for free to receive new posts and\nsupport my work.\n\n8 Likes\n\n\u00b7\n\n3 Restacks\n\n6\n\nShare this post\n\n#### Ninety-five theses on AI\n\nwww.secondbest.ca\n\n2\n\nShare\n\n2 Comments\n\nKevin2 hrs agoUnfortunately there's no indication that regulations based on\ncompute would only be a \"temporary measure\". It's a bad idea to make \"time\nbomb\" regulations that start doing something unintended and bad unless they\nare regularly fixed by competent future legislators.Why not regulate based on\nmoney spent? Restrict regulations to AI models that cost over $100m to train,\nin 2024 dollars, indexed for inflation.Expand full commentLikeReplyShare  \n---  \n  \nDave FriedmanBuy the Rumor; Sell the News2 hrs agoIn general I think these 95\ntheses are useful and ought to be carefully considered. I maintain that\ninstitutionalists will fare poorly as AI technology becomes more powerful.\n\"Institutionalists\" is a term that ought to be broadly construed: it includes\nlawyers, bureaucrats, politicians, regulators, government agencies,\naccountants, educators (across the spectrum from childhood through PhD\nprograms), management consultants, certain financiers...Expand full\ncommentLikeReplyShare  \n---  \n  \nBefore the flood\n\nRuminations on the future of AI\n\nDec 6, 2022 \u2022\n\nSamuel Hammond\n\n101\n\nShare this post\n\n#### Before the flood\n\nwww.secondbest.ca\n\n21\n\nWhy AGI is closer than you think\n\nLet's talk timelines\n\nSep 22, 2023 \u2022\n\nSamuel Hammond\n\n49\n\nShare this post\n\n#### Why AGI is closer than you think\n\nwww.secondbest.ca\n\n13\n\nAI and Leviathan: Part II\n\nPreparing for regime change\n\nAug 28, 2023 \u2022\n\nSamuel Hammond\n\n44\n\nShare this post\n\n#### AI and Leviathan: Part II\n\nwww.secondbest.ca\n\n5\n\nReady for more?\n\n\u00a9 2024 Samuel Hammond\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
