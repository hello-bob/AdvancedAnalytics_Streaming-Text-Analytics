{"aid": "40275722", "title": "Democratization of Text-to-Video Models Will Launch Us into Hyperreal Cyberspace", "url": "https://hackernoon.com/democratization-of-text-to-video-models-will-launch-us-into-a-hyperreal-cyberspace-are-we-ready", "domain": "hackernoon.com", "votes": 1, "user": "smooke", "posted_at": "2024-05-06 15:21:49", "comments": 0, "source_title": "Democratization of Text-to-video Models Will Launch Us Into a Hyperreal Cyberspace. Are We Ready? | HackerNoon", "source_text": "Democratization of Text-to-video Models Will Launch Us Into a Hyperreal Cyberspace. Are We Ready? | HackerNoon\n\nLoginReadWrite\n\nNotifications\n\nsee more\n\nLOGIN / SIGNUP\n\nREAD\n\nTOP STORIES\n\nWRITE\n\nLEARN\n\nWATCH WEB 2.5\n\nCOMPANIES\n\nABOUT\n\nHELP\n\nSTARTUPS\n\nJOBS\n\nCRYPTO\n\nSHOP\n\nADVERTISE\n\n  * READ\n\n  * TOP STORIES\n\n  * WRITE\n\n  * LEARN\n\n  * WATCH WEB 2.5\n\n  * COMPANIES\n\n  * ABOUT\n\n  * HELP\n\n  * STARTUPS\n\n  * JOBS\n\n  * CRYPTO\n\n  * SHOP\n\n  * ADVERTISE\n\nApply now and work #LikeABosch\n\nDemocratization of Text-to-video Models Will Launch Us Into a Hyperreal\nCyberspace. Are We Ready? by@privs\n\n273 reads\n\n# Democratization of Text-to-video Models Will Launch Us Into a Hyperreal\nCyberspace. Are We Ready?\n\nby Priyavrat S\n\n## Priyavrat S\n\n### @privs\n\nRunner, reader, and writer split between many worlds \u2013 tech,...\n\n8mFebruary 28th, 2024\n\nRead this story in a terminal\n\nRead this story w/o Javascript\n\n## Too Long; Didn't Read\n\nDemocratization of access to GenAI will lead to the explosion of AI-generated\ncontent on the internet. Everything we see on content platforms will show\n(probably superior visuals of) people, places, and events that never existed\nor happened. Will this disorient our idea or reality, or trigger an upward\ncomparison that renders our world as dry and lacking? Whatever the case, when\nAI generates content for us, we will be compelled to exercise our higher\ncognitive abilities to create more sophisticated content, just faster and\nbetter.\n\n1x\n\nRead by Dr. One\n\nAudio Presented by\n\n## Priyavrat S\n\n@privs\n\nRunner, reader, and writer split between many worlds \u2013 tech, culture,\ninternet, fitness, and business, to name a few.\n\n## STORY\u2019S CREDIBILITY\n\n## Opinion piece / Thought Leadership\n\nThe is an opinion piece based on the author\u2019s POV and does not necessarily\nreflect the views of HackerNoon.\n\nI heard about AI-generated art for the first time in a Wall Street Journal\npodcast in 2021. In the podcast, the host was playing AI-generated pieces of\nclassical music. These pieces emulated the style of classical composers, and\nthe precision with which they did that was rather impressive. A listener\nuninitiated to classical music would probably have a hard time differentiating\nthem from the actual works of those composers.\n\nThis was also the first time I took generative AI (GenAI) seriously. Listening\nto those pieces made me wonder \u2013 what if AI gets so good at making music that\nthose renowned composers eventually get buried under piles of AI-generated\nclassical music? I have always believed in judging a work of art for what it\nis instead of finding its aesthetic validity in the legacy of a musician or\nhow critics received a piece of music. If AI-generated music is going to get\nbetter, I will most likely say, Sorry, Chopin.\n\nLast week, a colleague showed me OpenAI Sora, a text-to-video model that\ngenerates remarkably detailed, realistic videos from text prompts. The first\nvideo they showcased on the website featured a woman (with five fingers, not\nfour) walking through the streets of Tokyo. The texture of her skin, the\nblemishes and spots on her face, the wrinkles, and her hair \u2013 everything about\nthis woman in focus is more realistic than (or at least, as much as) subjects\nin those videos that were once \u2018really shot\u2019 by professional videographers.\n\n## Are we entering the era of the hyperreal?\n\nIn his book Simulacra and Simulation, Jean Baudrillard first defined the\nconcept of hyperreal as the blurring of the lines that separate reality from\nrepresentation. This blurring is caused by the creation of representations\n(signs) that do not refer to anything that exists in reality. Hyperreality\nresults in our inability to distinguish what is simulated from what is real.\n\nIn his text, Baudrillard was alluding to the process by which value gets\ncreated in consumerist paradigms. But with GenAI, his idea is being stretched\nto its literal limits. On the OpenAI Sora page are over 30 video clips, each\ncontaining chunks of time that never happened, all in great detail, ready for\nus to witness with our eyes. What do we see when we see \u2018historical footage of\nCalifornia during the gold rush\u2019 generated by Sora?\n\nIn truth, there is nothing historical about this footage, and neither is it\nfootage in the true sense. The historical effect is rendered by employing\nvisual aesthetics that elicit a sense of historicity in films and\ndocumentaries \u2013 warm tones, vintage overlay, grainy effects, and a lower frame\nrate. Yet, the prompt results in a very believable \"footage.\" I am no\nhistorian, so I cannot say if the clip gets historical details right in how it\nrenders the look of the buildings and the attire of the people on horses. But\nthe geographical features look rather compelling.\n\nBut this is not the crux of the issue here. Such visual effects have been\nachieved by studios using CGI for decades now. But these endeavors were\napplied to a specific end and within particular contexts \u2013 like creating a\nrealistic game, a movie sequence, or making a documentary more tangible, if\nyou will. The content was vetted by experts who could verify its correctness\nand authenticity. But now that this technology (text-to-video models) will\ndemocratize access to such capabilities, the consequences will be very\ndifferent. Content will be affected by this democratization of GenAI long\nbefore it starts weaving symphonies that draw tears from our eyes or write\nNobel-winning novels.\n\n## A new distinction between content and art\n\nContent and art have coexisted ever since the advent of media \u2013 writing and\npainting, followed by print and photography, then audio and video. Content was\nusually short-lived and created for a specific function, such as entertaining,\ndelivering news, capturing memories, recording events, or even selling things.\nArt served a higher purpose in the human being\u2019s socio-cultural experience,\nand artworks survived well beyond their time. However, both content and art\nhad to be created before GenAI entered the picture.\n\nUntil now, art \u2013 or at least, what is deemed art in \u2018high culture\u2019 has\nremained largely untouched by GenAI capabilities. While GenAI may be able to\nemulate Mozart\u2019s musical style, the aesthetic and cultural value of that style\nwas rooted in the time and context in which Mozart was creating music. The\nsame goes for literary works, which, in effect, created a paradigm shift in\nthe ways of thinking about the world, human beings, society, and the small and\nbig questions underpinning our existence during a period in history.\n\nThe story of the content is somewhat different. Until some point in the 2000s,\ncontent creation required specialized apparatus \u2013 sophisticated video cameras\nor DSLRs and teams of people were needed to create audio-visual content, for\nexample. But with social media and the proliferation of smartphones, content\ncreation took a new turn. Everyone was now a creator, snipping pictures (or\nvideos) of their dogs and cats doing funny stuff, creating video memes, or\nmaking comic faces with Instagram filters. Compilations of these snippets of\nhuman existence made their way to YouTube in droves, and content flooded\ndepthless Instagram, Facebook, and TikTok feeds. Yet, few pieces of content\nwarranted the need to question the reality of the subject of these videos.\n\nWith the democratization of GenAI, we will find ourselves asking this question\nmore often. In fact, while scrolling through the comments section of the Apple\nAirpods Pro 2 launch live stream in 2023, I found a user wondering whether the\nwoman on screen, who was presenting the features of the new Airpods, was a\nreal person. However, the industry is attempting to solve this problem with\nstandards like the C2PA, which will embed details like the creator and mode of\ncreation within media files. This will make it possible to differentiate AI-\ngenerated content from \u2018real\u2019 content.\n\nNonetheless, AI-generated content will coexist with the content that is\ncreated by people. This will add another level of distinction between content\nand art \u2013 which is that art must be created, but content can be generated.\n\nThe ability to generate content will undoubtedly have far-reaching economic\nconsequences and leave a profound impact on many professions. But it will also\nalter our relationship with content and how it informs our worldview.\n\n## The desert of the real?\n\nWhat if generated content is more compelling, more convincing, more\nentertaining, and more appealing to the viewer? Given how most societies today\nare inclined to promote value-creating systems, generated content will likely\novertake content created by people in such a scenario \u2013 which means that\ncontent platforms will be saturated with AI-generated content that will show\nplaces that never existed, people who never walked on the planet, events that\nnever occurred, or cute dog stuff that no real dog ever did.\n\nThis is quite likely to happen \u2013 AI thought leader Nina Schick says 90% of\nonline content could be AI-generated. But the proliferation of AI-generated\ncontent could go two ways.\n\nAccording to neuroscientists, knowing an artwork as authentic activates reward\npathways in the brain of viewers. Forged or fake art, on the other hand, does\nnot lead to the same effect. If this result is extrapolated to AI-generated\ncontent, one would be led to conclude that AI-generated content will not be as\nfulfilling as that which is created by real people recording real things \u2013\ngiven that content platforms enforce rules that enable viewers to identify AI-\ngenerated content.\n\nWill AI-generated dog videos be as effective in reducing our stress levels as\nthe footage of real dogs doing real things? If not, then the adoption of text-\nto-video models will be limited to certain areas, like making clips for\npurchasable stock video collections. Or generating elements of videos, like\nbackgrounds or animated overlays.\n\nIn this scenario, we will see the return of content that is created rather\nthan generated \u2013 and social media platforms would be the first sites where\nthis shift will unfold.\n\nBut there is another factor that must be taken into account here. Research on\nInstagram usage patterns has shown that upward comparison (comparing oneself\nto those who people perceive as superior to them) through content consumption\nhas detrimental effects on users' mental health. If AI-generated content\nbreaks through the barriers of creative possibilities, it will present a\nversion of the world that is superior to its real counterpart. If this\nhappens, will AI-generated content immerse us in a hyperreal world, framing\nour existing one into a dull and dry reality?\n\nProbably not \u2013 at least not right away. Current text-to-video models still\nhave limitations. For instance, the wolf pups in Sora-generated clips\nsometimes emerge out of nothing, and bodies sometimes magically pass through\nother bodies. In another clip, a plastic chair sometimes behaves like a sheet\nof paper and changes shape through the clip. OpenAI claims that giving the\nmodel foresight into multiple frames enables them to retain the visual\nidentity of a subject even when it goes out of view for a few moments. But\nthis doesn't make the model infallible, as is evident in the videos on\nOpenAI\u2019s Sora page. Moreover, the generated clips are still only 60 seconds\nlong, and it will probably be a while before we see full-length feature films\nand documentaries generated by such models.\n\nHowever, these limitations will likely be eliminated in upcoming generations\nof multimodal models \u2013 and soon enough, considering that the technology has\nbeen in development for a little over half a decade. When that time comes, the\nexperience of consuming content or simply navigating through cyberspace will\ntake a new turn. For the better or worse? Only time will tell.\n\n## Looking towards a brighter future\n\nGeneration and creation remain very distinct ideas, even in the era of GenAI.\nThis distinction is rooted in the etymology of these words \u2013 create comes from\nthe Latin creare, which means to form out of nothing. On the other hand,\ngenerate comes from the Latin gener\u014d (meaning to produce), which in turn comes\nfrom genus (meaning a type or a family). In both mathematics and linguistics,\ngeneration pertains to a process of applying a set of rules to an input to\nproduce something. This is precisely what Generative Pre-Trained Transformers\n(GPTs) do.\n\nAs far-fetched as the meaning of creation may seem in the era of generated\ncontent, GenAI is compelling us to exercise our higher cognitive abilities as\nit abstracts the heavy lifting of content generation from the creators.\nCreation, at its heart, remains a process of making something out of nothing.\nIt is this idea of creation that has ushered era-defining epochs in the course\nof our cultural, social, and technological history.\n\nWith GenAI solutions undertaking the less cognitive aspects of content\ncreation, each of us will be empowered to create more sophisticated things,\njust better and faster. But where this paradigm of GenAI-augmented creation\ntakes us will be determined collectively by content-powered platforms,\nregulators, content provenance standards and their endorsement, and our\ninevitable neurological responses to new forms of content.\n\nL O A D I N G . . . comments & more!\n\n### About Author\n\nPriyavrat S@privs\n\nRunner, reader, and writer split between many worlds \u2013 tech, culture,\ninternet, fitness, and business, to name a few.\n\nRead my stories\n\n#### TOPICS\n\nmachine-learning #generative-ai #society #ai-generated-art #content #future-\nof-the-internet #ai-generated-video #openai-sora #ai-ethics\n\n#### THIS ARTICLE WAS FEATURED IN...\n\nPermanent on Arweave\n\nRead this story in a terminal\n\nTerminal\n\nRead this story w/o Javascript\n\nLite\n\nYcombinator\n\n#### RELATED STORIES\n\nCan you relate?\n\nvisit Bosch\n\n#Sponsored\n\nA 3-Pronged Framework for Achieving Fair, Ethical, and Bias-Free Results From\nYour AI Strategy\n\nby privs\n\nMar 02, 2024\n\n#ai-bias\n\nWhose Game Are You Playing?\n\nby rimaeneva\n\nMay 05, 2024\n\n#personal-development\n\nPursuit of Trustworthiness: Increasing User Trust in Generative AI Products\n\nby iswaryam\n\nMay 03, 2024\n\n#generative-ai\n\nGenerative Artificial Intelligence for Software Engineering: Background\n\nby textmodels\n\nMay 03, 2024\n\n#generative-ai\n\nGenerative Artificial Intelligence for Software Engineering: Research Approach\n\nby textmodels\n\nMay 03, 2024\n\n#generative-ai\n\nJoin HackerNoon\n\nLatest technology trends. Customized Experience. Curated Stories. Publish Your\nIdeas\n\n## about\n\n  * Careers\n  * Contact\n  * Cookies\n  * Emails\n  * Help\n  * Privacy\n  * Sitemap\n  * Shareholders\n  * Startups 2023\n  * Testimonials\n  * Terms\n  * Updates\n\n## read\n\n  * Archive\n  * Categories\n  * Image Gallery\n  * Leaderboard\n  * Learn Repo\n  * Noonification\n  * Signup\n  * Tech Beat\n  * Tech Brief\n  * Tech Tags\n  * Terminal Reader\n  * Top Stories\n\n## write\n\n  * Distribution\n  * Editing Protocol\n  * Editor Tips\n  * Guidelines\n  * Help\n  * New Story\n  * Perks\n  * Process\n  * Subscribers\n  * Story Templates\n  * Testimonials\n  * Why Write\n\n## Business\n\n  * Billboard\n  * Book Demo Meeting\n  * Business Blogging\n  * Case Studies\n  * Company Directory\n  * Crypto Directory\n  * Live Business Posts\n  * Newsletters\n  * Niche Targetting\n  * Partnerships\n  * Startup Package\n  * Writing Contests\n\n\u00a9 2023 HackerNoon. All rights reserved - PO Box 2206, Edwards, Colorado 81632,\nUSA\n\n## HACKERNOON\n\n# Notice\n\nWe and selected third parties use cookies or similar technologies for\ntechnical purposes and, with your consent, for other purposes as specified in\nthe cookie policy.\n\nUse the \u201cAccept\u201d button or close this notice to consent.\n\nPress again to continue 0/2\n\n", "frontpage": false}
