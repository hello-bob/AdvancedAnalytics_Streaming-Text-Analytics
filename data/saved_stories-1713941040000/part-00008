{"aid": "40137970", "title": "Enhancing Inference Efficiency of LLMs: Optimization and Architectures", "url": "https://arxiv.org/abs/2404.05741", "domain": "arxiv.org", "votes": 1, "user": "PaulHoule", "posted_at": "2024-04-23 22:12:01", "comments": 0, "source_title": "Enhancing Inference Efficiency of Large Language Models: Investigating Optimization Strategies and Architectural Innovations", "source_text": "[2404.05741] Enhancing Inference Efficiency of Large Language Models:\nInvestigating Optimization Strategies and Architectural Innovations\n\nSkip to main content\n\nWe gratefully acknowledge support from the Simons Foundation, member\ninstitutions, and all contributors. Donate\n\n> cs > arXiv:2404.05741\n\n# Computer Science > Machine Learning\n\narXiv:2404.05741 (cs)\n\n[Submitted on 2 Apr 2024]\n\n# Title:Enhancing Inference Efficiency of Large Language Models: Investigating\nOptimization Strategies and Architectural Innovations\n\nAuthors:Georgy Tyukin\n\nView a PDF of the paper titled Enhancing Inference Efficiency of Large\nLanguage Models: Investigating Optimization Strategies and Architectural\nInnovations, by Georgy Tyukin\n\nView PDF HTML (experimental)\n\n> Abstract:Large Language Models are growing in size, and we expect them to\n> continue to do so, as larger models train quicker. However, this increase in\n> size will severely impact inference costs. Therefore model compression is\n> important, to retain the performance of larger models, but with a reduced\n> cost of running them. In this thesis we explore the methods of model\n> compression, and we empirically demonstrate that the simple method of\n> skipping latter attention sublayers in Transformer LLMs is an effective\n> method of model compression, as these layers prove to be redundant, whilst\n> also being incredibly computationally expensive. We observed a 21% speed\n> increase in one-token generation for Llama 2 7B, whilst surprisingly and\n> unexpectedly improving performance over several common benchmarks.\n\nSubjects:| Machine Learning (cs.LG); Artificial Intelligence (cs.AI);\nComputation and Language (cs.CL); Performance (cs.PF)  \n---|---  \nCite as:| arXiv:2404.05741 [cs.LG]  \n(or arXiv:2404.05741v1 [cs.LG] for this version)  \nhttps://doi.org/10.48550/arXiv.2404.05741arXiv-issued DOI via DataCite  \n  \n## Submission history\n\nFrom: Georgy Tyukin [view email] [v1] Tue, 2 Apr 2024 19:53:54 UTC (1,247 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Enhancing Inference Efficiency of Large\nLanguage Models: Investigating Optimization Strategies and Architectural\nInnovations, by Georgy Tyukin\n\n  * View PDF\n  * HTML (experimental)\n  * TeX Source\n  * Other Formats\n\nview license\n\nCurrent browse context:\n\ncs.LG\n\n< prev | next >\n\nnew | recent | 2404\n\nChange to browse by:\n\ncs cs.AI cs.CL cs.PF\n\n### References & Citations\n\n  * NASA ADS\n  * Google Scholar\n  * Semantic Scholar\n\na export BibTeX citation Loading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer (What is the Explorer?)\n\nLitmaps (What is Litmaps?)\n\nscite Smart Citations (What are Smart Citations?)\n\n# Code, Data and Media Associated with this Article\n\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\n\nDagsHub (What is DagsHub?)\n\nGotit.pub (What is GotitPub?)\n\nPapers with Code (What is Papers with Code?)\n\nScienceCast (What is ScienceCast?)\n\n# Demos\n\nReplicate (What is Replicate?)\n\nHugging Face Spaces (What is Spaces?)\n\nTXYZ.AI (What is TXYZ.AI?)\n\n# Recommenders and Search Tools\n\nInfluence Flower (What are Influence Flowers?)\n\nConnected Papers (What is Connected Papers?)\n\nCORE Recommender (What is CORE?)\n\nIArxiv Recommender (What is IArxiv?)\n\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new\narXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and\naccepted our values of openness, community, excellence, and user data privacy.\narXiv is committed to these values and only works with partners that adhere to\nthem.\n\nHave an idea for a project that will add value for arXiv's community? Learn\nmore about arXivLabs.\n\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n\n  * About\n  * Help\n\n  * Contact\n  * Subscribe\n\n  * Copyright\n  * Privacy Policy\n\n  * Web Accessibility Assistance\n  * arXiv Operational Status Get status notifications via email or slack\n\n", "frontpage": false}
