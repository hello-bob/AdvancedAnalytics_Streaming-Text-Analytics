{"aid": "40208769", "title": "Otherness and Control in the Age of AGI", "url": "https://joecarlsmith.com/2024/01/02/otherness-and-control-in-the-age-of-agi/", "domain": "joecarlsmith.com", "votes": 1, "user": "wjb3", "posted_at": "2024-04-30 09:07:24", "comments": 0, "source_title": "Otherness and control in the age of AGI - Joe Carlsmith", "source_text": "Otherness and control in the age of AGI - Joe Carlsmith\n\nJoe Carlsmith\n\n  * About\n  * Archive\n\nOtherness and control in the age of AGI / Part 1\n\nOtherness and control in the age of AGI\n\nLast updated: 03.28.2024\n\nPublished: 01.02.2024\n\nSeries\n\nOtherness and control in the age of AGI / Part 1\n\n# Otherness and control in the age of AGI\n\n> With malice towards none; with charity towards all; with firmness in the\n> right, as God gives us to see the right...\n>\n> Abraham Lincoln\n\nLincoln\u2019s Second Inaugural (image source here)\n\nI\u2019ve written a series of essays that I\u2019m calling \u201cOtherness and control in the\nage of AGI.\u201d The series examines a set of interconnected questions about how\nagents with different values should relate to one another, and about the\nethics of seeking and sharing power. They\u2019re old questions \u2013 but I think that\nwe will have to grapple with them in new ways as increasingly powerful AI\nsystems come online. And I think they\u2019re core to some parts of the discourse\nabout existential risk from misaligned AI (hereafter, \u201cAI risk\u201d).^1\n\nThe series covers a lot of ground, but I\u2019m hoping the individual essays can be\nread fairly well on their own. Here\u2019s a brief summary of the essays that have\nbeen released thus far (I\u2019ll update it as I release more):\n\n  * The first essay, \u201cGentleness and the artificial Other,\u201d discusses the possibility of \u201cgentleness\u201d towards various non-human Others \u2013 for example, animals, aliens, and AI systems. And it also highlights the possibility of \u201cgetting eaten,\u201d in the way that Timothy Treadwell gets eaten by a bear in Werner Herzog\u2019s Grizzly Man: that is, eaten in the midst of an attempt at gentleness.\n  * The second essay, \u201cDeep atheism and AI risk,\u201d discusses what I call \u201cdeep atheism\u201d \u2013 a fundamental mistrust both towards Nature, and towards \u201cbare intelligence.\u201d I take Eliezer Yudkowsky as a paradigmatic deep atheist, and I highlight the connection between his deep atheism and his concern about misaligned AI. I also connect deep atheism to the duality of \u201cyang\u201d (active, controlling) vs \u201cyin\u201d (receptive, letting-go). A lot of my concern, in the series, is about ways in which certain strands of the AI risk discourse can propel themselves, philosophically, towards ever-greater yang.\n  * The third essay, \u201cWhen \u2018yang\u2019 goes wrong,\u201d expands on this concern. In particular: it discusses the sense in which deep atheism can prompt an aspiration to exert extreme levels of control over the universe; it highlights the sense in which both humans and AIs, on Yudkowsky\u2019s narrative, are animated by this sort of aspiration; and it discusses some ways in which our civilization has built up wariness around control-seeking of this kind. I think we should be taking this sort of wariness quite seriously.\n  * Pursuant to this goal, the fourth essay, \u201cDoes AI risk \u2018other\u2019 the AIs?\u201d, examines Robin Hanson\u2019s critique of the AI risk discourse \u2013 and in particular, his accusation that this discourse \u201cothers\u201d the AIs, and seeks too much control over the values that steer the future. I find some aspects of Hanson\u2019s critique uncompelling and implausible, but I do think he\u2019s pointing at a real discomfort.\n  * The fifth essay, \u201cAn even deeper atheism,\u201d argues that this discomfort should deepen yet further when we bring some other Yudkowskian philosophical vibes into view \u2013 in particular, vibes related to the \u201cfragility of value,\u201d \u201cextremal Goodhart,\u201d and \u201cthe tails come apart.\u201d These vibes, I suggest, create a certain momentum towards deeming more and more agents \u2013 including: human agents \u2013 \u201cmisaligned\u201d in the sense of: not-to-be-trusted to optimize the universe very intensely according to their values-on-reflection. And even if we do not follow this momentum, I think it can remind us of the sense in which AI risk is substantially (though, not entirely) a generalization and intensification of the sort of \u201cbalance of power between agents with different values\u201d problem we already deal with in the purely human world \u2013 a problem about which our existing ethical and political traditions already offer lots of guidance.\n  * The sixth essay, \u201cBeing nicer than Clippy,\u201d tries to draw on this guidance. In particular, it tries to point at the distinction between a paradigmatically \u201cpaperclip-y\u201d way of being, and some broad and hazily-defined set of alternatives that I group under the label \u201cniceness/liberalism/boundaries.\u201d Too often, I think, a simplistic interpretation of the alignment discourse imagines that humans and AIs-with-different-values are both paperclippy at heart \u2013 except, only, with a different favored sort of \u201cstuff.\u201d I think this picture neglects core aspects of human ethics that are, themselves, about navigating precisely the sorts of differences-in-values that the possibility of misaligned AI forces us to grapple with. I think that attention to this part of human ethics can help us be better than the paperclippers we fear \u2013 not just in what we do with spare resources, but in how we relate to the distribution of power amongst a plurality of value systems more broadly. And I think it may have practical benefits as well, in navigating possible conflicts both between different humans, and between humans and AIs. That said, I don\u2019t think that \u201cniceness/liberalism/boundaries\u201d is enough, on its own, to ensure a good future, or to allay all concern about trying to control that future over-much.\n  * The seventh essay, \u201cOn the abolition of man,\u201d examines another version of that concern: namely, C.S. Lewis\u2019s argument (in his book The Abolition of Man) that attempts by moral anti-realists to influence the values of future people must necessarily be \u201ctyrannical.\u201d I mostly disagree with Lewis \u2014 and in particular, I think he makes a number of fairly basic philosophical mistakes related to e.g. compatibilism about freedom, to the difference between creating-Bob-instead-of-Alice vs. brainwashing-Alice-to-make-her-like-Bob, and to the sense in which moral anti-realists can retain their grip on morality. But I do think his discussion points at some difficult questions about the ethics of influencing the values of others, including AIs \u2013 questions the essay takes an initial stab at grappling with.\n  * The eight essay, \u201cOn green,\u201d examines a philosophical vibe that I (following others) call \u201cgreen,\u201d and which I think contrasts in interesting ways with \u201cdeep atheism.\u201d Green is one of the five colors on the Magic the Gathering Color Wheel, which I\u2019ve found (despite not playing Magic myself) an interesting way of classifying the sort of the energies that tend to animate people.^2 The colors, and their corresponding shticks-according-to-Joe, are: White = Morality; Blue = Knowledge; Black = Power; Red = Passion; and Green = ... I haven\u2019t found a single word that I think captures green, but associations include: environmentalism, tradition, spirituality, hippies, stereotypes of Native Americans, Yoda, humility, wholesomeness, health, and yin. The essay tries to bring the vibe that underlies these associations into clearer view, and to point at some ways that attempts by other colors to reconstruct green can miss parts of it. In particular, I focus on the way green cares about respect, in a sense that goes beyond \u201cnot trampling on the rights/interests of moral patients\u201d (what I call \u201cgreen-according-to-white\u201d); and on the way green takes joy in (certain kinds of) yin, in a sense that contrasts with merely \u201caccepting things you\u2019re too weak to change\u201d (what I call \u201cgreen-according-to-black\u201d).\n  * The ninth essay, \u201cOn attunement,\u201d continues the project of the previous essay, but with a focus on what I call \u201cgreen-according-to-blue,\u201d on which green is centrally about making sure that we act with enough knowledge. I think there\u2019s something to this, but I also suggest that green cares especially about \u201cattunement\u201d \u2013 a kind of meaning-laden receptivity to the world \u2013 as opposed to more paradigmatically blue-like types of knowledge. What\u2019s more, I think that attunement is core to certain kinds of ethical epistemology, including my own; and it plays a key role in my own vision, at least, of a \u201cwise\u201d future. And while attunement may, ultimately, be made out of red and blue, I think we should take it seriously on its own terms.\n  * (More later.)\n\nI\u2019ll also note two caveats about the series as a whole. First, the series is\ncentrally an exercise in philosophy, but it also touches on some issues\nrelevant to the technical challenge of ensuring that the AI systems we build\ndo not kill all humans, and to the empirical question of whether our efforts\nin this respect will fail. And I confess to some worry about bringing the\nphilosophical stuff too near to the technical/empirical stuff. In particular:\nmy sense is that people are often eager, in discussions about AI risk, to\nargue at the level of grand ideological abstraction rather than brass-tacks\nempirics \u2013 and I worry that these essays will feed such temptations. This\nisn\u2019t to say that philosophy is irrelevant to AI risk \u2013 to the contrary, part\nof my hope, in these essays, is to help us see more clearly the abstractions\nthat move and shift underneath certain discussions of the issue. But we should\nbe very clear about the distinction between affiliating with some\nphilosophical vibe and making concrete predictions about the future.\nUltimately, it\u2019s the concrete-prediction thing that matters most;^3 and if the\nright concrete prediction is \u201cadvanced AIs have a substantive chance of\nkilling all the humans,\u201d you don\u2019t need to do much philosophy to get upset, or\nto get to work. Indeed, particularly in AI, it\u2019s easy to argue about\nphilosophical questions over-much. Doing so can be distracting candy,\nespecially if it lets you bounce off more technical problems. And if we fail\non certain technical problems, we may well end up dead.\n\nSecond: even as the series focuses on philosophical stuff rather than\ntechnical/empirical stuff, it also focuses on a very particular strand of\nphilosophical stuff \u2013 namely, a cluster of related philosophical assumptions\nand frames that I associate most centrally with Eliezer Yudkowsky, whose\nwritings have done a lot to frame and popularize AI risk as an issue. And\nhere, too, I worry about pushing the conversation in the wrong direction. That\nis: I think that Yudkowsky\u2019s philosophical views are sufficiently influential,\ninteresting, and fleshed-out that it\u2019s worth interrogating them in depth. But\nI don\u2019t want people to confuse their takes on Yudkowsky\u2019s philosophical views\n(or his more technical/empirical views, or his vibe more broadly) for their\ntakes on the severity of existential risk from AI more generally \u2013 and I worry\nthese essays might prompt such a conflation. So please, remember: there are a\nvery wide variety of ways to care about making sure that advanced AIs don\u2019t\nkill everyone. Fundamentalist Christians can care about this; deep ecologists\ncan care about this; solipsists can care about this; people who have no\ninterest in philosophy at all can care about this. Indeed, in many respects,\nthese essays aren\u2019t centrally about AI risk in the sense of \u201clet\u2019s make sure\nthat the AIs don\u2019t kill everyone\u201d (i.e., \u201cAInotkilleveryoneism\u201d) \u2013 rather,\nthey\u2019re about a set of broader questions about otherness and control that\narise in the context of trying to ensure that the future goes well more\ngenerally. And what\u2019s more, as I note in the series in various places, much of\nmy interrogation of Yudkowsky\u2019s views has to do with the sort of philosophical\nmomentum they create in various directions, rather than with whether Yudkowsky\nin particular takes them there. In this sense, my concern is not ultimately\nwith Yudkowsky\u2019s views per se, but rather with a sort of abstracted\nexistential narrative that I think Yudkowsky\u2019s writings often channel and\nexpress \u2013 one that I think different conversations about advanced AI live\nwithin to different degrees, and which I hope to help us see more whole.\n\nThanks to Katja Grace, Ketan Ramakrishnan, Carl Shulman, Anna Salamon, Will\nMacAskill, and many others over the years for conversation about these topics;\nand thanks to Carl Shulman for written comments. Some of my thinking and\nwriting on these topics occurred in the context of my work for Open\nPhilanthropy, but I am here speaking only for myself and not for my employer.\n\nLeave a comment\n\nSubstackLessWrongEA Forum\n\nNext up\n\nRead next in series\n\n01.02.2024\n\nGentleness and the artificial Other\n\nAIs as fellow creatures. And on getting eaten.\n\nContinue reading\n\n## Further reading\n\n11.15.2023\n\nNew report: \u201cScheming AIs: Will AIs fake alignment during training in order to\nget power?\u201d\n\nMy report examining the probability of a behavior often called \u201cdeceptive\nalignment.\u201d\n\nContinue reading\n\n05.08.2023\n\nPredictable updating about AI risk\n\nHow worried about AI risk will we be when we can see advanced machine\nintelligence up close? We should worry accordingly now.\n\nContinue reading\n\n03.22.2023\n\nExistential Risk from Power-Seeking AI (shorter version)\n\nBuilding a second advanced species is playing with fire.\n\nContinue reading\n\n1\n\nThere are lots of other risks from AI, too; but I want to focus on existential\nrisk from misalignment, here, and I want the short phrase \u201cAI risk\u201d for the\nthing I\u2019m going to be referring to repeatedly.\n\n2\n\nMy relationship to the MtG Color Wheel is mostly via somewhat-reinterpreting\nDuncan Sabien\u2019s presentation here\n\n3\n\nSee here and here\n\n@ Joe Carlsmith, 2024\n\n  * Policies\n\nDesigned by And\u2013Now, with support from the Blog Prize\n\n", "frontpage": false}
