{"aid": "40272875", "title": "Pure-PyTorch Implementation of Kolmogorov-Arnold Network (Kan)", "url": "https://github.com/Blealtan/efficient-kan", "domain": "github.com/blealtan", "votes": 12, "user": "adif_sgaid", "posted_at": "2024-05-06 09:45:24", "comments": 0, "source_title": "GitHub - Blealtan/efficient-kan: An efficient pure-PyTorch implementation of Kolmogorov-Arnold Network (KAN).", "source_text": "GitHub - Blealtan/efficient-kan: An efficient pure-PyTorch implementation of\nKolmogorov-Arnold Network (KAN).\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nBlealtan / efficient-kan Public\n\n  * Notifications\n  * Fork 42\n  * Star 609\n\nAn efficient pure-PyTorch implementation of Kolmogorov-Arnold Network (KAN).\n\n609 stars 42 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# Blealtan/efficient-kan\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nBlealtanUpdate README.May 4, 2024f39e514 \u00b7 May 4, 2024May 4, 2024\n\n## History\n\n13 Commits  \n  \n### examples\n\n|\n\n### examples\n\n| Adjust initialization and simple tunes over mnist/| May 4, 2024  \n  \n### src/efficient_kan\n\n|\n\n### src/efficient_kan\n\n| Adjust initialization and simple tunes over mnist/| May 4, 2024  \n  \n### tests\n\n|\n\n### tests\n\n| Fix and test update_grid.| May 2, 2024  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| Add corresponding .gitignore entry for temporary datasets.| May 4, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update README.| May 4, 2024  \n  \n### pdm.lock\n\n|\n\n### pdm.lock\n\n| Random initialize the base weight, and fix scaled_spline_weight.| May 4,\n2024  \n  \n### pyproject.toml\n\n|\n\n### pyproject.toml\n\n| Random initialize the base weight, and fix scaled_spline_weight.| May 4,\n2024  \n  \n## Repository files navigation\n\n# An Efficient Implementation of Kolmogorov-Arnold Network\n\nThis repository contains an efficient implementation of Kolmogorov-Arnold\nNetwork (KAN). The original implementation of KAN is available here.\n\nThe performance issue of the original implementation is mostly because it\nneeds to expand all intermediate variables to perform the different activation\nfunctions. For a layer with in_features input and out_features output, the\noriginal implementation needs to expand the input to a tensor with shape\n(batch_size, out_features, in_features) to perform the activation functions.\nHowever, all activation functions are linear combination of a fixed set of\nbasis functions which are B-splines; given that, we can reformulate the\ncomputation as activate the input with different basis functions and then\ncombine them linearly. This reformulation can significantly reduce the memory\ncost and make the computation a straightforward matrix multiplication, and\nworks with both forward and backward pass naturally.\n\nThe problem is in the sparsification which is claimed to be critical to KAN's\ninterpretability. The authors proposed a L1 regularization defined on the\ninput samples, which requires non-linear operations on the (batch_size,\nout_features, in_features) tensor, and is thus not compatible with the\nreformulation. I instead replace the L1 regularization with a L1\nregularization on the weights, which is more common in neural networks and is\ncompatible with the reformulation. The author's implementation indeed include\nthis kind of regularization alongside the one described in the paper as well,\nso I think it might help. More experiments are needed to verify this; but at\nleast the original approach is infeasible if efficiency is wanted.\n\nAnother difference is that, beside the learnable activation functions\n(B-splines), the original implementation also includes a learnable scale on\neach activation function. I provided an option enable_standalone_scale_spline\nthat defaults to True to include this feature; disable it will make the model\nmore efficient, but potentially hurts results. It needs more experiments.\n\n2024-05-04 Update: @xiaol hinted that the constant initialization of\nbase_weight parameters can be a problem on MNIST. For now I've changed both\nthe base_weight and spline_scaler matrices to be initialized with\nkaiming_uniform_, following nn.Linear's initialization. It seems to work much\nmuch better on MNIST (~20% to ~97%), but I'm not sure if it's a good idea in\ngeneral.\n\n## About\n\nAn efficient pure-PyTorch implementation of Kolmogorov-Arnold Network (KAN).\n\n### Resources\n\nReadme\n\nActivity\n\n### Stars\n\n609 stars\n\n### Watchers\n\n8 watching\n\n### Forks\n\n42 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Languages\n\n  * Python 100.0%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": true}
