{"aid": "40091843", "title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars", "url": "https://arxiv.org/abs/2404.07413", "domain": "arxiv.org", "votes": 1, "user": "PaulHoule", "posted_at": "2024-04-19 21:01:11", "comments": 0, "source_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars", "source_text": "[2404.07413] JetMoE: Reaching Llama2 Performance with 0.1M Dollars\n\nSkip to main content\n\nWe gratefully acknowledge support from the Simons Foundation, member\ninstitutions, and all contributors. Donate\n\n> cs > arXiv:2404.07413\n\n# Computer Science > Computation and Language\n\narXiv:2404.07413 (cs)\n\n[Submitted on 11 Apr 2024]\n\n# Title:JetMoE: Reaching Llama2 Performance with 0.1M Dollars\n\nAuthors:Yikang Shen, Zhen Guo, Tianle Cai, Zengyi Qin\n\nView a PDF of the paper titled JetMoE: Reaching Llama2 Performance with 0.1M\nDollars, by Yikang Shen and 3 other authors\n\nView PDF HTML (experimental)\n\n> Abstract:Large Language Models (LLMs) have achieved remarkable results, but\n> their increasing resource demand has become a major obstacle to the\n> development of powerful and accessible super-human intelligence. This report\n> introduces JetMoE-8B, a new LLM trained with less than $0.1 million, using\n> 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU\n> hours. Despite its low cost, the JetMoE-8B demonstrates impressive\n> performance, with JetMoE-8B outperforming the Llama2-7B model and\n> JetMoE-8B-Chat surpassing the Llama2-13B-Chat model. These results suggest\n> that LLM training can be much more cost-effective than generally thought.\n> JetMoE-8B is based on an efficient Sparsely-gated Mixture-of-Experts (SMoE)\n> architecture, composed of attention and feedforward experts. Both layers are\n> sparsely activated, allowing JetMoE-8B to have 8B parameters while only\n> activating 2B for each input token, reducing inference computation by about\n> 70% compared to Llama2-7B. Moreover, JetMoE-8B is highly open and academia-\n> friendly, using only public datasets and training code. All training\n> parameters and data mixtures have been detailed in this report to facilitate\n> future efforts in the development of open foundation models. This\n> transparency aims to encourage collaboration and further advancements in the\n> field of accessible and efficient LLMs. The model weights are publicly\n> available at this https URL.\n\nSubjects:| Computation and Language (cs.CL); Artificial Intelligence (cs.AI)  \n---|---  \nCite as:| arXiv:2404.07413 [cs.CL]  \n(or arXiv:2404.07413v1 [cs.CL] for this version)  \nhttps://doi.org/10.48550/arXiv.2404.07413arXiv-issued DOI via DataCite  \n  \n## Submission history\n\nFrom: Zhen Guo [view email] [v1] Thu, 11 Apr 2024 00:52:39 UTC (1,388 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled JetMoE: Reaching Llama2 Performance with 0.1M\nDollars, by Yikang Shen and 3 other authors\n\n  * View PDF\n  * HTML (experimental)\n  * TeX Source\n  * Other Formats\n\nview license\n\nCurrent browse context:\n\ncs.CL\n\n< prev | next >\n\nnew | recent | 2024-04\n\nChange to browse by:\n\ncs cs.AI\n\n### References & Citations\n\n  * NASA ADS\n  * Google Scholar\n  * Semantic Scholar\n\na export BibTeX citation Loading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer (What is the Explorer?)\n\nLitmaps (What is Litmaps?)\n\nscite Smart Citations (What are Smart Citations?)\n\n# Code, Data and Media Associated with this Article\n\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\n\nDagsHub (What is DagsHub?)\n\nGotit.pub (What is GotitPub?)\n\nPapers with Code (What is Papers with Code?)\n\nScienceCast (What is ScienceCast?)\n\n# Demos\n\nReplicate (What is Replicate?)\n\nHugging Face Spaces (What is Spaces?)\n\nTXYZ.AI (What is TXYZ.AI?)\n\n# Recommenders and Search Tools\n\nInfluence Flower (What are Influence Flowers?)\n\nConnected Papers (What is Connected Papers?)\n\nCORE Recommender (What is CORE?)\n\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new\narXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and\naccepted our values of openness, community, excellence, and user data privacy.\narXiv is committed to these values and only works with partners that adhere to\nthem.\n\nHave an idea for a project that will add value for arXiv's community? Learn\nmore about arXivLabs.\n\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n\n  * About\n  * Help\n\n  * Contact\n  * Subscribe\n\n  * Copyright\n  * Privacy Policy\n\n  * Web Accessibility Assistance\n  * arXiv Operational Status Get status notifications via email or slack\n\n", "frontpage": false}
