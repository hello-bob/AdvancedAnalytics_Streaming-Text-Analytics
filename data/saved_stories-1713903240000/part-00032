{"aid": "40131627", "title": "Everything you know about loss is a lie", "url": "https://github.com/kohya-ss/sd-scripts/discussions/294", "domain": "github.com/kohya-ss", "votes": 1, "user": "reqo", "posted_at": "2024-04-23 13:16:46", "comments": 0, "source_title": "Everything you know about loss is a LIE! \u00b7 kohya-ss/sd-scripts \u00b7 Discussion #294", "source_text": "Everything you know about loss is a LIE! \u00b7 kohya-ss/sd-scripts \u00b7 Discussion\n#294 \u00b7 GitHub\n\n## Navigation Menu\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nkohya-ss / sd-scripts Public\n\n  * Notifications\n  * Fork 713\n  * Star 4.2k\n\n# Everything you know about loss is a LIE! #294\n\nAI-Casanova started this conversation in Show and tell\n\nEverything you know about loss is a LIE! #294\n\nAI-Casanova\n\nMar 14, 2023 \u00b7 12 comments \u00b7 57 replies\n\nReturn to top\n\nDiscussion options\n\n##\n\nAI-Casanova\n\nMar 14, 2023\n\nOriginal comment in English -\n\nI've been experimenting with different noising strategies, inspired in part by\nNoise Offset and Pyramid Noise.This is the standard implementation of\ntimesteps, which tells the noise scheduler how much of the noise to add to the\nlatents. timesteps = torch.randint(0,\nnoise_scheduler.config.num_train_timesteps, (b_size,), device=latents.device)\nA sampling from the Uniform distribution [0,1000)But something very\ninteresting happens when you replace those random timesteps with a constant\nvalue, your loss variability is almost none!(Deterministic training at\ntimestep intervals from [100-900], note the inverse exponential effect on\nloss)Judging by our previous expectations of loss, very little training is\nexpected to have occurred, but that is not the case.(Timesteps 500 [center]\nand 600 are closest to my subject, with 200 coming in as a surprising\nthird)I'm still running tests to see what more I can glean from this, but in\ngeneral I'm experiencing an unprecedented stability in training that I have a\nhard time explaining.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nYou must be logged in to vote\n\n## Replies: 12 comments \u00b7 57 replies\n\nComment options\n\n###\n\nswfsql\n\nMar 29, 2023\n\n-\n\nCould it be that the loss is actually \"loss per timestep\" or maybe the average\nof the loss for each timestep? I think this would be different from the loss\naccumulation for all timesteps (not sure if what I said even makes sense)  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nYou must be logged in to vote\n\n0 replies\n\nComment options\n\n###\n\nAI-Casanova\n\nMar 29, 2023\n\nAuthor\n\n-\n\n@swfsql it has to do with the signal to noise ratio, low timesteps have very\nlittle noise added into it, and when the trainer takes a sample (a single step\ndown the timestep chain IIUC) to predict noise, it gets a lot larger\nerror.There's a lot more math involved, but basically you can think of it as\n5/4 is a lot bigger that 500/499There's some more info here #308 because Min-\nSNR is designed precisely to counteract some of this high loss from low\ntimesteps.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nYou must be logged in to vote\n\n0 replies\n\nComment options\n\n###\n\ncheald\n\nSep 25, 2023\n\n-\n\nI took this idea and tinkered a bit.I set the timestep to be a fixed\n(1000-global_step), and it produced a very clean U-curve, bottoming out at\naround min_timestep 400. Observationally, setting the timestep range higher\ndoes a better job of absorbing \"broad\" details, but misses the fine details.\nOnce the timestep dropped under 400, the results got rapidly garbled and\nnonsensical; much worse than just normal overtraining.Taking this information,\nI wondered how training would be affected if the lower timesteps weren't\nselected as frequently. I reimplemented the random timestep generation to use\na standard normal distribution where -6 sigma is min_timestep and 6 sigma is\nmax_timestep, and a mean of (max-min) / 2 + min. From early tinkering, this is\nproducing both faster convergence, more stable loss, and better preservation\nof the underlying model. Much of the model's \"burnout\" seems to happen when\nnoising with low timesteps.\n\n    \n    \n    sigma = 6 timesteps = ((torch.randn((b_size,), device=latents.device).clip(-sigma, sigma) + sigma) / (2*sigma)) * (max_timestep - min_timestep) + min_timestep\n\nHere's a comparison of loss between the standard implementation and my normal\ndistribution (teal is the modified routine). In both cases,\nMinSNR=5.Observationally, knocking out the lower timesteps results in\nsignificantly faster improvements to the samples over time. Setting\nmin_timestep too high causes the model to not learn the finer details of the\nsubject, so a balance is needed. A 6-sigma standard normal distribution using\n[100..1000] as my range (which should give me an average timestep of 550)\nresults in the model learning significantly faster - in the dataset I'm\nworking on, I typically get pretty decent results after 1500-2000 steps, but\nwith this change I've been seeing it approach the same level of fidelity by\n~400-500 steps, with significantly less overtraining \"damage\" to the\nunderlying model.Here's a comparison of one of my training images, the\nstandard timestep selection routine (min/max range of [0..1000]), and my\ntimestep selection routine with bounds set at [100..1000]. The two generated\nimages are generated after 500 steps of training. All other parameters other\nthan the timestep range and random generation routine were held constant.For\nwhat it's worth, I'm using the Prodigy optimizer with a CosineAnnealingLR\nscheduler; I suppose tests should also be run with the more standard Adam8bit\nand a constant learning rate, but the results were a significant-enough\nimprovement that I felt the observation bore sharing.I have no theoretical\nbasis for any of this, I'm just kind of experimenting and found that this had\na massive impact. Intuitively, one thing that might be worth trying is some\nkind of combination of global step, learning rate and timestep range\nscheduling, to tilt the timestep ranges higher early on or when the learning\nrate is higher, and then reducing the lower bound of the range over time to\nsee if it can balance learning the finer details without overtraining all that\nextra error from the low timesteps.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nYou must be logged in to vote\n\n22 replies\n\nComment options\n\n####\n\ncheald Oct 22, 2023\n\n-\n\nI've been doing more experimentation with pre-training analysis of the\ndataset, and I've run across something interesting. When using masking, the\nuntrained loss for the dataset is notably different.Here's what the aggregate\nloss curves look like for my dataset. Each image has loss computed at timestep\n(0..1000 by 20) with 5 random noises, and the losses at each timestep for each\nsample are averaged together.This is quite what you'd expect - exponential\ndecay of the loss curve as the noising timesteps increase. Nothing unusual\nthere. But, when you run the exact same routine with masking:This quite\nsurprised me, but it's encouraging because it suggests that my goal\n(identifying timestep ranges at which certain features of an image are best\nlearned most easily) is plausible.Generally speaking, the higher the loss in\nthe earliest timesteps, the lower the loss in the latest, and visa versa. Also\ngenerally speaking, the high-early-loss images tend to be those which are\ncloser-up shots of my face, while the late-loss images tend to images with\nmore background in them. I did apply the scaling factor patch from #589, so\nloss shouldn't be affected by how much of the image is masked, but my hunch is\nthat \"percentage of the image masked\" is affecting the overall shape of an\nimage's loss curve.One thing I'm experimenting here with is precalculating\nloss curves for each item in my dataset, then using those during training to\ninterpret a sample's loss as being relative to the base loss for that sample,\nthe hunch being that this will effectively neutralize the bias imposed by\ndifferent timesteps, since the loss at any given timestep is easily\ninterpreted as how much noise prediction at that timestep has improved for\nthat particular sample. It's promising so far, but I'm not ready to call it\nright just yet.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\nAI-Casanova Oct 22, 2023\n\nAuthor\n\n-\n\nKeep up the good work! I expect a full academic paper out of you soon!  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\ncheald Oct 22, 2023\n\n-\n\nDisabling factor in get_latent_masks produces the more familiar curve, though\nwith obvious striations in magnitude, as one would expect since without any\ncompensation for how much of the image is masked, more masked images will have\nlower loss.I think the simplest conclusion here is that that particular factor\ncalculation may not be right. It's quite interesting that it amplified the\nright-hand portion of the loss curve as much as it did, though.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\nrecris Oct 23, 2023\n\n-\n\nGood stuff @cheald !I'm thinking of a couple different approaches to adjusting\nthe loss in the presence of masks\n\n  * Use textbook \"weighed MSE\" for loss: I've seen a few different formulations, not sure which one is the best\n  * Restrict MSE to non-black pixels - assuming black and white masks, no extra factors, and mask the loss instead of latents:\n\n    \n    \n    loss = torch.nn.functional.mse_loss(noise_pred.float(), target.float(), reduction=\"none\") loss = (mask * loss).sum([1, 2, 3]) / mask.sum([1, 2, 3])\n\nEverything here is untested :)  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\ncheald Oct 24, 2023\n\n-\n\nYour second proposal there is incorrect because loss is a latent tensor of\nshape (batch, 4, x, y), and mask is a tensor of (batch, x, y); mask gets\nbroadcast and overweights the numerator by a factor of loss.shape[1] (4, in\nthis case).One of these two would work:\n\n    \n    \n    loss = (mask * loss).sum([1, 2, 3]) / mask.sum([1, 2, 3]) / loss.shape[1] # or loss = (mask * loss).mean([1,2,3]) / mask.mean([1,2,3])\n\nQuick little unit test to play with the mechanisms:\n\n    \n    \n    import torch torch.set_printoptions(precision=10) mask_shape = (512, 512) shape = (4, *mask_shape) white_mask = torch.ones(mask_shape) mask = torch.rand(mask_shape) pred = torch.rand(shape) target = torch.rand(shape) l = lambda p, t: torch.nn.functional.mse_loss(p.float(), t.float(), reduction=\"none\") m1 = lambda m, l: (m * l).mean() / m.mean() m2 = lambda m, l: (m * l).sum() / m.sum() loss = l(pred, target) print(\"Mean loss\\t\", loss.mean()) print(\"Unmasked loss\\t\", m1(white_mask, loss)) print(\"Masked loss\\t\", m1(mask, loss)) print(\"Naive masking\\t\", l(pred*mask, target*mask).mean()) print(\"Sum mean\\t\", m2(mask, loss)) print(\"Sum mean/4\\t\", m2(mask, loss)/shape[0]) # Mean loss tensor(0.1664090306) # Unmasked loss tensor(0.1664090306) # Masked loss tensor(0.1664098203) # Naive masking tensor(0.0554658212) # Sum mean tensor(0.6656392813) # Sum mean/4 tensor(0.1664098203)\n\nIt's not exactly normalized back to the unweighted loss (floating point\nerror?), but it's pretty close, and in empirical testing it seems to do the\ntrick of causing images to be more or less weighted evenly regardless of\nmasking.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n###\n\ncheald\n\nOct 23, 2023\n\n-\n\nIt looks like #889 (and the linked paper) have potentially addressed this\nproblem, as well. I'll be running some tests, but if it's really that simple,\nthen all's the better!  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nYou must be logged in to vote\n\n1 reply\n\nComment options\n\n####\n\ncheald Oct 25, 2023\n\n-\n\nAfter running some tests, I think that this is indeed the right fix, at least\ninitially. The implementation is a little more complex (and inefficient) than\nnecessary, though. This suffices:\n\n    \n    \n    def prepare_scheduler_for_custom_training(noise_scheduler, device): # ... # No need to recompute this on every step noise_scheduler.all_snr_sqrt = noise_scheduler.all_snr.sqrt() def apply_debiased_estimation(loss, timesteps, noise_scheduler): return (loss / noise_scheduler.all_snr_sqrt[timesteps]).clip(max=10000)\n\nI used 10k rather than 1k as the clip, as 1/(alll_snr[997, 998]) yields\n[1124.5431, 2254.1948]. While it likely doesn't have much visible effect, it\ndoesn't make sense to arbitrarily shortchange those last two timesteps. It\nalso does mean that loss compensation for the 1000th timestep is going to\nalways be wrong; the expected loss for the 1000th timestep is always zero, and\nit doesn't make sense to interpret observed loss as a percentage of 0. Setting\nmax_timesteps to 999 avoids this issue and likely doesn't cause any real\ndamage, but it makes me wonder if loss / all_snr.sqrt() is the best debiasing\nmechanism.In validating this, I ran some experiments that yielded some\nextremely interesting results, though.Given fixed latents, zero noise, and\nconstant random text encoder states, loss exhibits a distinct curve, losing\naccuracy above the 700-ish step range, demonstrating that the network does\nbest at predicting the noise residual somewhere in the 700-timestep\nrange.(Note: This isn't the same as a black image; I skipped the VAE encode\nstep so that we're only testing the unet's ability to predict a controlled\nnoise.)Here's the general form of the code used to produce the following\ngraphs:\n\n    \n    \n    with torch.no_grad(): batch_size = 10 torch.random.manual_seed(0) latents = torch.ones((batch_size, 4, 64, 64), device=device, dtype=torch.bfloat16) noise = torch.zeros_like(latents) hidden_states = torch.randn((2, 768), device=device, dtype=torch.bfloat16).expand((batch_size, 2, 768)) timesteps = torch.arange(0, 1000, step=1, device=device, dtype=torch.long) dataloader = torch.utils.data.DataLoader(timesteps, batch_size=latents.shape[0], shuffle=False) sampled_timesteps = [] all_losses = [] for step, ts in tqdm(enumerate(dataloader), total=len(dataloader)): noised = noise_scheduler.add_noise(latents, noise, ts) noise_pred = unet(noised, ts, hidden_states).sample losses = torch.nn.functional.mse_loss(noise_pred.float(), noise.float(), reduction=\"none\") del noise_pred del noised sampled_timesteps.extend(list(ts.cpu())) all_losses.extend(list(losses.mean([1,2,3]).cpu())) plt.plot(np.array(sampled_timesteps), np.array(all_losses))\n\n(This graph is mislabeled, the TE states are held constant here)Using random\nTE states introduces some noise, but confirms a pattern and amplifies the\nevidence of a decrease in fidelity as steps increase past ~750 or so (and holy\ncow, the change in variance is smooth!):Increasing the latents to 1 shows a\ndifferent loss shape, but still clear evidence of a minimum and then\nincreasing loss past that:Further increasing the latent to all 3.0 produces a\ngentler curve, but it still blows up at the end:And this one is really wild:\nwhen the latents are drawn from the standard normal distribution, we get a\nloss curve peaking at ~390 and what looks like no variation and extremely\nsmall loss at low timesteps...\n\n    \n    \n    latents = torch.normal(0, 1, (batch_size, 4, 64, 64), device=device, dtype=torch.bfloat16)\n\n...but if we look at it on a log scale, we do in fact see that variation at\nthe upper end show up, and an overall increase in loss for the last 100\ntimesteps or so.And here's rand latents (interval [0, 1)):\n\n    \n    \n    latents = torch.rand((batch_size, 4, 64, 64), device=device, dtype=torch.bfloat16)\n\nOn a log scale, it's evident that there's still a loss of fidelity at the low\nend:I'm pondering what the implications of these various findings are, but my\nhunch is that it demonstrates that not all latent distributions are equal in\nterms of the network's ability to effectively denoise them at different\ntimesteps, and that by understanding the relationship better we might be able\nto more accurately guide the network towards accurately predicting noise\nacross across all timesteps.Update: Here's some more exploration with a real\nimage, and some thoughts.Using my random TE tensors and no noise, here's the\nloss for one of my training images. Applying the proposed debiasing scheme to\nit vastly amplifies the relative strength of the last 100 timesteps or so at\nthe upper end.I've been thinking about the variance bands demonstrated by the\nrandom TE tensors, and I think I have a theory there: the broader the band,\nthe more the text encoder's influence matters in overall loss at that\ntimestep! The variance introduced by the random TE tensor essentially produces\na visualization of how much the TE improves or harms the overall noise\nresidual at that timestep. Look at the normalized loss in that last 100\ntimesteps: the effect of loss sqrt(SNR) loss normalization is that the effect\nof the text encoder is massively magnified in that last 100 timesteps or so,\nand it drastically flattens the relative influence of the TE on the loss\nacross the entire rest of the curve.The conclusion that we can draw from that\nis that we can estimate the relative overall influence of the TE vs the unet\non overall loss at a given timestep: te_influence = band width / 2 and\nunet_influence = mean - te_influence.If I look at just the first 850 steps:The\nTE variance is pretty tight up until around timestep 150, peaks around 300,\nand then it remains relatively stable in its influence up to 850.We can see\nthat the relative influence of the text encoder may outpace the influence of\nthe unet past roughly step 850 or so - which is exactly where the debiased\ncurve blows up and takes off. This makes me wonder if we should be using two\ndebias factors: one for the unet, and one for the TE. This potentially has\nlarge implications for unet and te learning rates, as well.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n###\n\ncheald\n\nOct 26, 2023\n\n-\n\nI think I might have stumbled into something extraordinary, and want to throw\nthis out there to get other brains on it.One of my observations in my\nexperiments is that forward-noising applies noise as sample *\nsqrt(alphas_cumprod) + noise * sqrt(1 - alphas_cumprod). This looks similar to\nyour standard image blending function: img1 * alpha + img2 * (1-alpha), but\nit's not, because sqrt(alphas_cumprod) and sqrt(1 - alphas_cumprod) have a\nnonlinear relationship. The sum of them looks like:This shape has shown up in\nother experiments (you'll notice it in my post above in the loss\nexplorations), which has made me wonder if there's a fundamental bias in the\nforward noising mechanism - there will be much more total information (sample\n+ noise) in the latent at step 400 than there is at steps 10 or 800. The pixel\nvalues will always have their largest magnitude at the ~400ish step peak. In\nabstract, maybe this doesn't matter, because the unet should learn to predict\nnoise regardless of the function used to generate the noise, so long as that\nfunction is differentiable, right? But what if it's actually a source of bias\nin training?At first I thought that this wouldn't change things that much, but\nthat we might be able to debias it by dividing both noise and target by\n(noise_scheduler.alphas_cumprod.sqrt() + (1 -\nnoise_scheduler.alphas_cumprod)[timesteps]. I tried this, and I think it might\nhave marginally improved my training run. But then, out of curiosity, I tried\napplying it to just one half of the loss (noise_pred), and magic happened.\n\n    \n    \n    if args.apply_noise_compensation: noise_comp = (noise_scheduler.alphas_cumprod.sqrt() + (1 - noise_scheduler.alphas_cumprod).sqrt()).to(device=accelerator.device) noise_pred = noise_pred / noise_comp[timesteps] # .sqrt() loss = torch.nn.functional.mse_loss(noise_pred.float(), target.float(), reduction=\"none\")\n\nMy first samples were blurry. REALLY blurry. But they \"felt\" much more like my\nsubject than samples of the same timestep were with the standard routine.\nFurthermore, as the epochs progressed, the samples retained significantly more\nof my subject's ineffable quality, but they sharpened up! The longer the\ntraining ran, the clearer my samples became. Around epoch 30 or so, they were\nessentially back to full resolution, but without the mutations, distortions,\nor burnout that is characteristic of overtraining in that range prior, and the\nsamples had astonishingly preserved most of the \"shape\" from the underlying\nmodel.This smells, in principle, much like what the ip_noise_gamma setting\n(based off this paper: https://arxiv.org/abs/2301.11706 ) is supposed to do -\ncreating an intentional blind gap between the noise and target is supposed to\nhelp with regularization. I've used ip_noise_gamma to maybe some sleight\nbenefit, but using a \"nonlinear gamma\" based off the sum of the noise\nscheduler information magnitude MASSIVELY increased this effect.When I then\npulled the LoRA into SD and tried it against various models other than the one\nI trained it on (RealisticVision) and in combination with other LoRAs, it\nfeels like it generalizes FAR better than I've ever accomplished before. This\nfelt like a fundamentally superior output in terms of fidelity AND\nflexibility.Training details: AdamW8bit (modified to use the the AdaBelief\nterm) constant LR, of 8e-6, algo=full, train_norm=True, preset=attn-mlp. This\nwas done without the debiased_estimation_loss flag or SNR in play - very\nvanilla settings. I'm running another training run with\ndebiased_estimation_loss on to see what impact that has, as well.What I'm now\nwondering is a) why does this work? and b) if we can bias the model towards\n\"level of detail\" by intentionally selecting the curve by which noise_pred is\nmodified before loss calculation. What I'm envisioning is some kind of\n\"10-band EQ\" where you could drag the learning rate of certain LODs up or\ndown.Here's a video of my training run over 39 epochs.As you can see, it\nlearns the \"large outline\" of my features early on, but then improves\nincrementally on the level of details in the image WITHOUT losing those \"large\nfeatures\". In much of my previous experiments, I've found that both large and\nsmall features got learned at the same time, and that frequently I'd end up\nwith models where either there was too much \"large detail\" (and the shape of\nthe outputs got distorted and nonsensical) or there wasn't enough \"small\ndetail\" (and the fidelity of the subject didn't feel right). This feels like\nit sidestepped that issue entirely and gave me an unexpectedly flexible high-\nfidelity output.I would appreciate any insight into what might be happening\nhere, and how it might be understood to improve the ability to better control\ntraining.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nYou must be logged in to vote\n\n9 replies\n\nComment options\n\n####\n\ncheald Mar 25, 2024\n\n-\n\nI unfortunately threw away my changes at some point, but I think you've got\nthe gist of it.I ultimately concluded that while this did have some\nregularization potential, it can very easily be too extreme, too, and I wasn't\nsure that it was worth the effort over other regularization techniques. I\ncouldn't get the effect to generalize across datasets.I have been\nexperimenting with modifications to the latents during training, based on\nobservations from https://huggingface.co/blog/TimothyAlexisVass/explaining-\nthe-sdxl-latent-space and I think there might be something happening there.In\nparticular, the observation that the 4th channel is \"shape\" explains a lot\nabout both the blurriness (and the sepia tones - which are characterized by\nflat luminosity ranges AND flat color ranges). By flattening the noise\nprediction in the \"high signal\" ranges, we're essentially lying about how good\nthe unet's prediction was for those timestep ranges (telling it that it wasn't\nas good as it thought), and penalize it more the closer it is to that \"signal\npeak\" so that it learns lower values. This causes it to initially be pulled\ntowards an understated range of the appropriate noise in those time ranges.\nOptimizers being what they are, it eventually starts predicting noise values\nwhich, even after flattening, minimize loss.I'd like to do some new\nexperiments, such as:\n\n  1. Splitting the scaling value per channel in the predicted noise (ie, you could lie to it about shape, but not color, or visa versa)\n  2. Evaluating the effects of scaling the noise prediction for a given channel for a given timestep range\n  3. Evaluating the effects of imposing scaling on the full latent prior to noising\n\nOverall, any of these techniques should theoretically make it harder for the\noptimizer to do its job, but if it's getting really good at one particular\nelement (say, shape) while it's still struggling with another (say,\nluminosity) then it might be that careful selection of a regularization factor\ncould produce subjectively superior results.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\ncheald Mar 26, 2024\n\n-\n\nFWIW, this discussion inspired me to fire up another experiment: just dividing\nthe noised latent by the magnitude of sqrt(alphas_cumprod) + sqrt(1 -\nalphas_cumprod)\n\n    \n    \n    # prior to self.call_unet in train_network.py noise_comp = (noise_scheduler.alphas_cumprod.sqrt() + (1 - noise_scheduler.alphas_cumprod).sqrt()).to(device=latents.device) noisy_latents = noisy_latents / noise_comp[timesteps].reshape(-1, 1, 1, 1)\n\nInitial results are promising; it seems to be picking up quality from the\ntraining set faster than before, but doesn't have the initial blurring/loss of\nshape issues. It remains to be seen how this plays out at higher epochs in\novertraining territory, and how it generalizes, but it's something worth\ntrying out.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\nSQCU Mar 30, 2024\n\n-\n\nTried out the noisy_latents tweak. Seems to be compatible with debiased\nestimation noise, produce reasonable results in separating different artstyles\nand compositions at 60+ repeats of a dataset. I'm still not sure if I've found\na way to 'overtrain' a model with these approaches, at least with the training\ndurations and dataset sizes I'm willing to sit through twice (to test with and\nwithout debias estimation noise).  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\nkabachuha Mar 31, 2024\n\n-\n\n@cheald While pure Huber loss misses the fine details as you noted, it is\npossible to use timestep-Scheduled Huber loss, with timestep-dependent\nparameter, making it look like L1 when the image only begins to form (first\nreverse diffusion timesteps) and thus more resilient to outliers, and like L2\nwhen the image is almost ready, making it learn the fine details!In our paper\nwe did a lot of tests, with different parameters, datasets and schedules and\nconfirmed that indeed scheduled Huber outperforms both Huber and L2 by\nfar!https://arxiv.org/abs/2403.16728See also the discussion in the Diffusers\nrepo huggingface/diffusers#7488 and the linked code in the PR  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\ncheald Mar 31, 2024\n\n-\n\nAmazing! I had experimented with Huber loss but it didn't occur to me to\nschedule it. I'm very excited by your results and am going to be doing a lot\nof experimentation with it.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n###\n\nhts2008\n\nNov 29, 2023\n\n-\n\nHi @cheald ,Hope you are well.Thank you for the info about the timestep.\nCurrently, I used kohya-gui, I have a concern that what the number of\n1000_global_timestep in min/max timestep? or -6 sigma and 6 sigma?Thank you.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nYou must be logged in to vote\n\n0 replies\n\nComment options\n\n###\n\ndrhead\n\nApr 7, 2024\n\n-\n\nHi @cheald,I have also been experimenting with this problem, and I believe I\nhave the true ideal solution, which should end the need to tune timestep\nweightings.The traditional solution to this would be multi-task loss which is\nstructured in such a way that timesteps would get extra weight depending on\ntheir difficulty (e.g. more difficult timesteps are weighted higher, less\ndifficult ones are lower weight), so that all timesteps contribute the same\namount towards the image. The weights would be trained as a tensor with one\nelement for each timestep. The main problem with this, is that it is not very\nstable on smaller batch sizes that anyone who isn't training a foundation\nmodel would use -- instead of optimizing the parameters, what you'd end up\ndoing with this is effectively smacking them across the room every now and\nthen, since you're not going to touch every timestep every training step. It\nis not likely to converge properly unless using a batch size of at least 256\nor so (very conservative estimate, it probably needs more), and is inefficient\nfor learning since we would expect nearby timesteps to have similar difficulty\nand this doesn't capture that.The EDM2 paper (repo:\nhttps://github.com/NVlabs/edm2) includes a different form of this multi-task\nloss in the form of a single-layer MLP with no activation function that takes\nnoise level (sigma) as input. While this was designed to be used with a\ncontinuous noise schedule like the EDM models use, I have found that it works\nextremely well on discrete timestep schedules, and most importantly for us, it\nallows multi-task loss to work on smaller batch sizes efficiently. From my\ntesting (still ongoing), I have found that this drastically improved results\nover the debias schedule you noted that you used before, and interestingly,\nthe weightings it chose did not look too much like other weightings I had been\nrecommended before. I have also had wonderful results with scheduled pseudo\nhuber loss in combination with this.One remaining problem with the learned\ntimestep weightings, though, is that it most likely will take a longer time to\nconverge than most short training runs will use. My tests so far have been\nwith a full finetune with a virtual batch size of 64. I do get good, fast\nconvergence with this method when I use the recently released Schedule Free\noptimizer (https://github.com/facebookresearch/schedule_free/tree/main), where\nI get fairly close to the final schedule within about 500 steps with an LR of\n0.005. Regardless of whether it is viable for use on all training run\ndurations for your use cases, I am sure that you could use it on a longer\ntraining run for the purpose of discovering better timestep weight schedules.\nThe MLP is also formulated in a way that it accepts a \"baseline\" timestep\nweighting of sorts (noted in the paper's formulas as lambda(sigma)) -- I would\nrecommend doing a training run until the timestep weights seem to converge,\nthen run a regression over it and have that as the baseline for whichever\nprediction mode you're using. I would expect that discovering an appropriate\none for epsilon and an appropriate one for v-prediction would be sufficient,\nthe main other factor that would affect things is resolution and of course the\nnoise schedule's shape itself, but I would think those should still be in the\nsame ballpark and within reach of virtually any training run.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nYou must be logged in to vote\n\n8 replies\n\nComment options\n\n####\n\ndrhead Apr 7, 2024\n\n-\n\nYou should definitely be charting 1/exp(MLP outputs) (or \u03bb(\u03c3)/exp(MLP outputs)\nif you're using a baseline schedule) instead of just the raw outputs, it\nshould make it much easier to visualize what the network is trying to do (the\nnetwork isn't operating in the space it is because those numbers are useful,\nit's that way for numerical stability only essentially). I am curious what the\ncurve for epsilon ends up settling as. I also made a similar mistake on the\nsigma input -- ultimately, it won't drastically alter the outputs as long as\nthe input is still generally correlated with noise levels.The negative loss is\nalso not a cause for concern at all. It'll stop looking so alarmingly wrong\nbefore you know it, probably sooner if you add the other loss metrics I use.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\ncheald Apr 7, 2024\n\n-\n\nHere's my take on it: cheald@0afee96 - it adds sigma_uncertainty_model as a\npath to a safetensors file of weights to use (or to create, if it doesn't\nexist), and a train_sigma_uncertainty boolean indicating whether the weighting\nnetwork should be trained or not.I'm still seeing overall negative losses even\nafter changing my sigmas, so it's possible I still have something off.\nHowever, it still appears that it's improving training results, so I'm not\nworried about it. 3 iterative passes, with the MLP being continually trained,\nstarting from initially untrained. Each pass is 900 samples (30 epochs @ 10\nsteps/epoch @ batch size 3):And the curve after 3 passes. It's worth noting\nthat I used the same seed for all 3 passes, and I think there are some\ntimesteps in the 0-50 range that aren't being hit; the curve was really jagged\nthere until the third pass. Multiple passes with different seeds (and thus\nsampled timesteps) may help smooth it out.And here's samples from the\ntraining, per epoch. These are huge, but if you compare them, I think it does\na good job of illustrating the acceleration in learning.It's worth noting that\nI'm using a couple of new techniques in here, as well, which may be\nconfounding things:\n\n  1. I'm learning the multires discount per channel, rather than a static fixed configuration value (multires_discount_lr=3.5e-2).\n  2. I'm learning offsets in latent space which I use for \"latent centering\", which I've found improves both dynamic range and color neutrality in training (ie, if you have photos in your training set which have an extreme color temperature or contrast, it compensates for that pretty decently) (latent_centering_lr=5e-2)\n  3. I'm applying a corruption to the latents prior to noising them, by reducing the magnitude of x% of the latent values by a random value. This is a regularization technique which I've found vastly improves the ability to learn the subject, and has a really nice benefit of helping the model to not learn JPEG artifacts or digital camera sensor noise. (latent_corruption=0.0008)\n\nI'll run some more tests with those passed off when I get a chance, as\nwell.Pass 1:Pass 2:Pass 3:I'll add 1/exp(MLP outputs) to my charting as\nrecommended.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\ncheald Apr 7, 2024\n\n-\n\nInitial tests with that logging show I've got something off still:I've got\nother stuff to do this afternoon, but will see if I can chase it down soon.\nThat explains the continually decreasing loss, though.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\ndrhead Apr 7, 2024\n\n-\n\nah, I think I wasn't specific enough there -- I was suggesting 1/exp(MLP\noutputs) for your matplotlib chart of the timestep weightings mainly, since\nthat will tell you what the actual scaling factor is on a given timestep.As\nfar as loss metrics to record go, here's my example of how my loss metrics\nbehave:\"Objective Loss\" is equivalent to your \"loss\", and yes, it is negative\non mine. That's just part of the multi-objective loss function, and while it\ndoes look bizarre at first, I can assure you it is completely normal. As you\ncan see from mine, it isn't completely unbounded, and it'll settle on some\n(very likely negative) value eventually. That's why I recommend tracking raw\nMSE/SPH loss on the side, so you actually have something meaningful to\ntrack.The \"scaled training loss\" on mine (refer to my code example for how I\nget that value) is the most useful way to track the convergence of the loss\nweight curve -- as you can see on mine, it settles on 1. You can also see that\nonce it settles, the objective loss also settles, which helps demonstrate that\nit really is not acting as unbounded. So don't worry about the negative loss!\nYour implementation appears to be working completely fine. It's just trying to\nkeep adaptive_loss_weights low to satisfy the additive term of the loss\nobjective, while also keeping them high so that loss *\n(lambda_weights[timesteps] / torch.exp(adaptive_loss_weights)) comes out to a\nlow value, and it just happens that it gets into the negatives doing that, but\nit can't keep going forever.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\ndrhead Apr 14, 2024\n\n-\n\nI do want to follow up on this since I discovered that I made an error in my\nimplementation. The inputs are handled incorrectly. You are supposed to\nstandardize the inputs by expectation as part of the design of the magnitude-\npreserving layers of the model. The sigma.log() / 4 part of this is doing that\nfor the EDM schedule which we are not using.To correct it, I suggest that you\nchange the input from sigmas to just noise_scheduler.alphas_cumprod, and then\nnormalize it on expectation. Simply take the mean and standard deviation of\nthe whole alphas_cumprod array at the start, and store those values somewhere.\nFor handling your inputs to the model, instead of doing .flatten().log() / 4\ndo .flatten().sub_(mean).div_(std). That should make the model converge better\nand it actually fits the design pattern for EDM2.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n###\n\ncheald\n\nApr 11, 2024\n\n-\n\nI've developed a way to do posthoc analysis of a lora to see WHERE it improved\nloss on your training set.The basic idea is simple: Take a training dataset, a\nlora, and a model. Load each training sample, noise it by every 50th timestep,\ndo noise prediction, and take loss as the standard MSE(noise, noise_pred).\nThen, load the lora and do the same thing. Compare the ratio of the new noise\nand old noise; if it's below 1, then training improved loss. It loads in\nmasks, if they're present in a /masks/ subdirectory, and masks losses with\nthem if they exist.Here's the samples at the 20th epoch. Observations: Fine\ndetails, very warm overall tone, blurry and low-detail backgrounds. The color\ndepth feels a bit flat, but the textures are decent.And here's the loss ratio\nplot. What I've found is that the stock training regieme is good at reducing\nloss at higher timesteps, but has a much harder time with lower timesteps.\nForgive the lack of labels; the X axis is \"timestep / 50\" (ranging from 0-20,\nwhich expands to 0-1000), and the Y axis is the ratio of baseline loss:lora\nloss (higher means the lora reduced loss):(Edit: I realized this morning that\nI was using baseline/lora rather than lora/baseline, so that changes my\ninterpretations, which I've updated)Additionally, I can plot statistics PER\nSAMPLE to find pathological samples in my dataset which are not converging!The\nred line is a ratio of 1.0, and the box plot plots the loss reductions across\nall 20 sampled timesteps, with the typical mean, median, and 1SD. This is\nreally useful for finding samples which the optimizer has outsized trouble\nconverging.By comparison, here's a run where I experimented with using\nnoisy_latents / (alpha_cumprod.sqrt() + (1-alpha_cumprod).sqrt())[timesteps]\n(on the theory that the greater overall information in the noised latents in\nthe \"fat\" part of that curve is doing funny things.Observations from these\nsamples: Much more neutral colors, better dynamic range, but the likeness\nisn't quite as good. The teeth are better (and this has held through my\nexperiments; the teeth overtrain first, but with this technique they remain\nfine the whole time):And here's the loss plots. The loss on the high end has\ntangibly improved. If I let this training run go for 60 epochs, it does a\nGREAT job at learning structure and form, but doesn't quite get details.What's\ninteresting here is that the tail end flipped, but the loss change as a\npercentage on the low end didn't change much at all. This might be due to the\nlower absolute values on the high end, but it's interesting that the first\npart of the curve didn't change much.Here's the notebook. It should go in your\nsd-scripts directory as it uses a few utility functions from sd-scripts to\nease model loading. Right now it's just working with SD1.5 but it shouldn't be\nhard to extend it for SDXL or whatnot. My hope is that lessons learned in SD15\nland can be applied to SDXL, since SD15 is a lot faster to run experiments\nwith.loss_analyzer_cleaned.zip  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nYou must be logged in to vote\n\n0 replies\n\nComment options\n\n###\n\ncheald\n\nApr 15, 2024\n\n-\n\nI am pretty sure that I've directly identified a cause of the original\nobservation in this issue. Essentially: Given a static noise, and then forward\nnoising a latent with that noise, and then predicting the noise from that\nnoised latent, earlier timestamps consistently end up with a lower overall\nmagnitude of noise.\n\n    \n    \n    with torch.no_grad(): timesteps = torch.arange(0, 1000, 25, device=dev) latent = encode_path_sd15(image, 128) latent = latent.expand(timesteps.shape[0], *latent.shape[1:]) noise = torch.randn((1, *latent.shape[1:]), device=latent.device).expand(latent.shape) _, text_embeddings = prompt_to_cond([\"man\"], latent.unsqueeze(0)) noisy_latents = noise_scheduler.add_noise(latent, noise, timesteps) noise_pred = unet(noisy_latents, timesteps, text_embeddings.cuda()).sample fig = plt.figure( figsize=(20, 50) ) for i, timestep in enumerate(timesteps): ax = plt.subplot(10, 4, i + 1) ax.set_title(f\"Timestep {timestep}\") n = noise[i].flatten().cpu() ax.hist(n, alpha=0.5, bins=100, color=\"red\") ax.hist(noise_pred[i].flatten().cpu(), alpha=0.5, bins=100) plt.show()\n\nRed is the true noise (held constant), and blue is the predicted noise. You'll\nnotice that at t=0, the predicted noise histogram has a significantly narrower\ndistribution of noise, and the magnitude of noise increases as the timestamp\nincreases.This will plainly lead to significant differences in loss for lower\ntimesteps.If we plot noise_pred.abs().quantile(0.999) /\ntrue_noise.abs().quantile(0.999) by timestamp, we end up with something like\nthis:This is essentially the difference in the magnitude of the true noise and\npredicted noise at each timestep for a given static noise.I regressed this\ncurve to roughly 4.312e-01 * timesteps.pow(1.215e-01), which in theory would\ngive us a compensation factor to apply to either noise, noisy_latents,\nnoise_pred, or loss (though that's likely harder than just modifying one of\nthe other input factor magnitudes directly).I tried scaling noise_pred\ndirectly first, and this does something very interesting: it causes what feels\nlike a relative \"hyperfocus\" on detail, resulting in over-sharpened (one might\neven say \"overtrained\") images. This is after only 4 epochs, but the pattern\nholds.\n\n    \n    \n    noise_pred.div_(4.312e-01 * (timesteps + 1).pow(1.215e-01).view(-1, 1, 1, 1))\n\nBut, okay, if the issue is just magnitude, we can directly normalize the\nnoise_pred to the standard deviation of noise:\n\n    \n    \n    noise_pred = noise_pred / noise_pred.std() * noise.std() # noise.std() should be pretty close to 1 here, so maybe unnecessary?\n\nThis results in a significantly higher level of subject detail (and feels\nperhaps the most photoreal result I've achieved yet) but the background\nbasically entirely disappeared in all my samples. My first thought is that I\nwonder if it's related to masked training, but I wouldn't think so, since\nnoising and noise_pred normalization are both applied without respect to\nmasks.At any rate, there's a dial here to play with here. I suspect there's\nmore to the nature of noise vs noise_pred than just the difference in noise,\nbut the noise is definitely a good clue.Perplexingly, normalizing noise to\nnoise_std (which should produce similar loss values, I think?) does NOT\nproduce similar results:\n\n    \n    \n    noise_pred = unet.call(...) noise = noise / noise.std() * noise_pred.std()\n\nI don't have an explanation for this, so there's clearly something else in\nplay here that I'm missing.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nYou must be logged in to vote\n\n6 replies\n\nComment options\n\n####\n\nSQCU Apr 17, 2024\n\n-\n\nCurious about the captioning / training scheme you're applying when you're\ngetting these no-background results. does the most recent (and most\nbackground-ablating) training method have the same behavior if you train with\ndropped out captions? if there's a difference in the degree to which\nbackgrounds are preserved or lost depending on the rate of caption dropout it\nmight be easier to understand if the missing backgrounds reflect a stronger or\nweaker absolute adherence to the literal training data (image, prompt dyads).  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\ncheald Apr 18, 2024\n\n-\n\nI am using the multiple caption method recently introduced, with 3 captions\nper image, randomly sampled per sample. I'm also training with masked loss,\nwhich should, in theory, disregard the background of my image entirely. I\nhaven't played any more with that training method yet, but it's worth\nexperimenting with.I did go back and read the DDPM paper last night, and\nlearned that not only is the loss discrepancy known, it's intentional. Here's\nthe objective function as originally defined:But they intentionally dropped\nthe weighting factor:I did try a training run last night, with:\n\n    \n    \n    betas = noise_scheduler.betas.to(device=accelerator.device) alphas = noise_scheduler.alphas.to(device=accelerator.device) alphas_cumprod = noise_scheduler.alphas_cumprod.to(device=accelerator.device) weight = ((betas**2) / (2 * betas * alphas * (1 - alphas_cumprod))) loss = loss.mean(dim=(1,2,3)) * weight[timesteps]\n\nThis did, in fact, have the effect of flattening the loss change ratio across\ntimesteps:I do think the samples were worse, though, in accordance with the\npaper - they had better fine detail, but less general subject fidelity. I\nthink that can probably be chalked up to that \"elbow\" at around T=550.It's\ninteresting that this weighting factor heavily smoothed out the T<550 range,\nwhile resulting in a generally linear increase in the loss ratio T>550\\. I\nwould have expected it to preserve the loss ratio across all timesteps.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\nTopSalad3530 Apr 21, 2024\n\n-\n\n>\n>     noise_pred.div_(4.312e-01 * (timesteps + 1).pow(1.215e-01).view(-1, 1,\n> 1, 1)) noise_pred = noise_pred / noise_pred.std() * noise.std() #\n> noise.std() should be pretty close to 1 here, so maybe unnecessary?\n\nAnecdote: I'm currently trying these changes for a moderate scale (~3k images\n+ repeats for balance) anime style dataset, and so far it seemed to have been\nimmensely helpful in preventing the distortions, blur, and deep-fry effects\nthat is usually referred to as \"overfit\" around the SD community. Before this,\nI've tried all the dials and knobs I can find, but invariably the degradation\nbegins to sets in before the samples really start to resemble the input,\nusually around the mark of epoch 20. Right now though, I'm 50 epochs in, the\nmodel is still improving, and there's not a single sign of the usual artifacts\nin sight. This is amazing.I did not observe any disappearing backgrounds from\nmy samples. Perhaps it's something that's only triggered by realistic data or\nmasked loss. I'll report back with further sustained training if any adverse\neffects appear (apart from true, conventional overfit, that is). I think this\nwould still be a very valuable addition to the library, even if it only really\nworked on unmasked illustrations. It would be huge if it turned out that we\ncould actually have trained our models for far longer than we thought.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\ncheald Apr 21, 2024\n\n-\n\nThe \"deep fry\" effect from overtraining is caused by the network learning to\npredict more and more extreme noise. Extreme noise causes extreme latents,\nwhich causes extreme values from the decoded latent. Once the predicted latent\nstarts to exceed certain bounds, the output of the VAE decode exceeds the clip\nranges and you get \"fry\". You can observe this trivially by just increasing\nthe magnitude of an encoded latent then decoding it:\n\n    \n    \n    latent = encode_path_sd15(\"4zor47.jpg\", 386) plt.figure(figsize=(12, 9)) for i in range(0, 12): noisy_latent = latent * (1 + 0.1*i) plt.subplot(3, 4, i+1) plt.plot(f\"Latent magnituide {noisy_latent.std():2.2f}\") plt.imshow(decode_sd15(noisy_latent).cpu()[0])\n\nPreventing this is a matter of keeping the network from predicting extreme\nnoise values. This could be done as simply as adding a loss penalty term based\non the magnitude of noise_pred. Naively, adding some variation of\n(noise_pred.std(dim=(1,2,3)) - noise.std(dim=(1,2,3))).clip(min=0) to loss\nmight by sufficient to prevent deep-fry. It's worth trying.Dividing noise_pred\nby that curve has the effect of teaching the network to learn lower noise\nmagnitudes in those earlier steps. Because losses tend to be highest in the\nearlier timesteps, the default scheme learns to overweight the early\ntimesteps, and achieves much of its overall loss reduction by overtraining\nthose timesteps. For whatever reason, as time goes on, this tends to cause it\nto learn more and more extreme noise values - likely with wider variance,\nalthough still around a mean of 0. As the variance exceeds the bounds of the\nVAE's clip range, we get \"fry\". The manual adjustment from this curve teaches\nthe network to predict \"artificially\" lower values of noise for those early\ntimesteps, which is likely why it doesn't fry out. However, this could also be\nharming the network's ability to learn the fine details which really\ndifferentiate a subject, so I'm not convinced that it's the ideal\nsolution.I've also had some luck with weighting timestep selection, too,\nrather than just doing uniform timestep selection, doing something like this\nin get_noise_noisy_latents_and_timesteps:\n\n    \n    \n    weighting = (alphas_prod**0.5 + (1-alphas_prod)**0.5) probs = weighting[min_timestep:max_timestep] timesteps = torch.tensor( list(torch.utils.data.WeightedRandomSampler(probs, b_size, replacement=True)), device=latents.device) + min_timestep\n\nI don't think the weighting scheme is ideal, but it's very clear that\nweighting the network to more often learn timesteps in the 400ish range\nproduces perceptually-superior results. This is consistent with the\nobservations from https://arxiv.org/pdf/2404.07946.pdf - particularly their\ncurriculum learning based timestep schedule strategy.Additionally, I think the\nnaive regression of 4.312e-01 * (timesteps + 1)1^.215e-01 can be improved by\nexpressing it in terms of alphas/alpha_prod/snr. I don't have the formulation,\nbut I do recall noting that it was similar to other curves I've seen in my\nexperiments.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\ncheald Apr 21, 2024\n\n-\n\nOkay, so adding noise std/mean as loss targets is RIDICULOUSLY effective. In\ntheory, it should prevent \"deep fry\" from ever happening, too. I haven't\ntrained enough on it to validate that it prevents frying, but it should, in\ntheory, work just fine.This should at least partially fix the discrepancy\nintroduced by removing the VLB weighting factor, without the need for a\nsupplementary network ala EDM2. By just saying \"we expect you to learn to\nproduce noise of the same variance and mean as the true noise\", we can retrain\nthe network to do so organically. With the importance of matching the noise\nvariance included in the objective function, we should have less timestep-\nvariant loss attributable to difference in overall predicted variance.My\nmodification is:\n\n    \n    \n    noise_std_loss = torch.nn.functional.mse_loss(noise_pred.std(dim=(1,2,3)), noise.std(dim=(1,2,3))) noise_mean_loss = torch.nn.functional.mse_loss(noise_pred.mean(dim=(1,2,3)), noise.mean(dim=(1,2,3))) loss = loss.mean([1, 2, 3]) + noise_std_loss + noise_mean_loss\n\nThat is, we don't just care about minimizing overall noise error, we also care\nabout minimizing deviation from distribution of noise. Intuitively, standard\nMSE loss should do this, but in practice, we can observe that the DDPM noise\nschedule and unweighted loss term results in the network learning different\nnoise distributions across timesteps. By adding it as an explicit loss term,\nwe can make sure that the network explicitly cares about predicting noise that\nmatches the overall variance of the true noise.Without the std loss\nterm:With:This is a fairly remarkable result, because it's the first time I've\nseen a mechanism which brings both tail ends of the process up into that \"U\"\nshape, rather than resulting in the higher timesteps flying past an inflection\npoint. I'm undecided yet if this is a net positive or not.Noise distribution\nwith the std loss term added. The earlier timesteps are still undershooting\nstd after 20 epochs, but they're definitely much closer to true.And then check\nthis out - this is the observed:true noise ratio, plotted against my regressed\ncurve from earlier:Early t are still low, but it's clear that it's coming up\nto neutral much more aggressively than before. I'm going to keep experimenting\nwith this approach for sure. Combined with weighted timestamp sampling, this\ncould end up doing some really interesting things for high-epoch training.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n###\n\ncheald\n\nApr 22, 2024\n\n-\n\nOkay, so initial runs show extreme promise. I've added multiple additional\nloss objectives:\n\n  * mse(noise_pred.std, noise.std)\n  * mse(skew(noise_pred), skew(noise))\n  * mse(kurtosis(noise_pred), kurtosis(noise))\n  * kl_div_loss(p(noise_pred), p(noise))\n\n(Edit: I'm doing more testing, and it might be that kl_div loss alone is\nsufficient for this effect; it keeps us in the right \"neighborhood\" but allows\nmore flexibility\")They can be tested individually, or combined. Each has a\nweight, and the individual objectives are summed and added to the overall\nloss. The results are really, REALLY good.\n\n    \n    \n    # in train_utils.py def noise_stats(noise): mean = noise.mean(dim=(1,2,3)).view(-1, 1, 1, 1) std = noise.std(dim=(1,2,3)).view(-1, 1, 1, 1) skew = torch.sum((noise - mean)**3 / std**3) / (noise.numel() / noise.shape[0]) kurt = torch.sum((noise - mean)**4 / std**4) / (noise.numel() / noise.shape[0]) - 3 return skew, kurt def stat_losses(noise, noise_pred, std_loss_weight=0.5, kl_loss_weight=3e-3, skew_loss_weight=0, kurtosis_loss_weight=0): std_loss = torch.nn.functional.mse_loss( noise_pred.std(dim=(1,2,3)), noise.std(dim=(1,2,3)), reduction=\"none\") * std_loss_weight skew_pred, kurt_pred = noise_stats(noise_pred) skew_true, kurt_true = noise_stats(noise) skew_loss = torch.nn.functional.mse_loss(skew_pred, skew_true, reduction=\"none\") * skew_loss_weight kurt_loss = torch.nn.functional.mse_loss(kurt_pred, kurt_true, reduction=\"none\") * kurtosis_loss_weight p1s = [] p2s = [] for i, v in enumerate(noise_pred): n = noise[i] p1s.append(torch.histc(v.float(), bins=500, min=n.min(), max=n.max()) + 1e-6) p2s.append(torch.histc(n.float(), bins=500) + 1e-6) p1 = torch.stack(p1s) p2 = torch.stack(p2s) kl_loss = torch.nn.functional.kl_div(p1.log(), p2, reduction=\"none\").mean(dim=1) * kl_loss_weight return std_loss, skew_loss, kurt_loss, kl_loss\n    \n    \n    # in train_network.py std_loss, skew_loss, kurt_loss, kl_loss = train_util.stat_losses(noise, noise_pred) loss = loss + std_loss + kl_loss + skew_loss + kurt_loss loss = loss.mean() # \u5e73\u5747\u306a\u306e\u3067batch_size\u3067\u5272\u308b\u5fc5\u8981\u306a\u3057\n\nCharting all those metrics - std, skew, kurtosis, and kl_div shows that\ndespite the classic loss objective improving, as training continues, various\nmetrics go wonky. But, we KNOW that the desired noise target has a consistant\nstd, skew, and kurtosis. My hunch here is that models overtrain by learning\nnoise predictions which do not resemble IID gaussian, and using metrics like\nstd, skew, and kurtosis as objectives keeps them from getting too far afield.\n\n  * std starts low, around 0.9-0.95, but starts to creep up. It eventually keeps climbing, and once it's sufficiently > 1, you start to see \"fry\". The std objective keeps learned noise in an acceptable range, and prevents that kind of overtraining from manifesting.\n  * skew and kurtosis loss both tend to increase across a training run when not used as part of the objective function. I theorize that, like std, the network is learning rather odd noise shapes used to optimize just the most important timesteps, but that these odd shapes have a detrimental effect on other timesteps. Keeping the noise distribution closer to gaussian seems to substantially help.\n  * kl_divergence is all over the place when not used as part of the objective function. I'm still playing with it, but it should theoretically kind of work like a proxy for std/skew/kurtosis all together. It might be viable to use on its own - I'm still experimenting. Right now I have the bin size fixed to 500, but that should probably be a hyperparameter.\n\n(Edit: I'm using torch.histc for divergence which isn't differentiable, so\nit's not working quite correctly WRT the backwards pass. I'm reimplementing\nwith a soft histogram instead.)This DOES unfortunately add four (!) new\nhyperparameters - weights for each new loss type - but so far the values I've\nused are producing astonishingly good results. After 20 epochs, I'm getting\nremarkably good samples, with no sign of overtraining, yet.I'm still running a\ntraining run, and don't have the vram to run analytics on a trained lora while\nit's going, but I'll get some timestep error distribution graphs once this run\ncompletes. I would very much like others to give this a go and let me know\nwhat kinds of results you end up with.Edit: Here's the timestep error graph\nafter 36 epochs. It's still a little funny at the extremes, but this is a\nSIGNIFICANT improvement overall.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nYou must be logged in to vote\n\n3 replies\n\nComment options\n\n####\n\nTopSalad3530 Apr 22, 2024\n\n-\n\nkurt_loss seems to be NaN for me. I'm currently trying this with the term\nremoved, seeing that its weight defaults to zero anyway. Maybe there are cases\nwhere std**4 underflows? Perhaps it might be more stable to calculate it as\n((noise - mean)**2 / std**2) ** 2?For now I'm resuming from my previous\ncheckpoint, which has by now seen 100+ epochs of training and is still looking\npristine, if a little over-saturated, but nowhere nearly as bad as what you\ntypically would expect. I think I'll want to try re-training from scratch for\ncomparison once I'm done, which I sense might be soon, given just how\nshockingly well these methods have been performing so far.Curiously, judging\npurely by perception, I've found these changes to help with generalization as\nwell. Perhaps by removing whatever pressure (or pushing against, for that\nmatter) that was causing the models to fry, we're allowing the regularization\ntechniques to better shine through. Amazing work, in any case. I'm very\nexcited to see how this might turn out.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\ncheald Apr 22, 2024\n\n-\n\nThis is a much more straightforward formulation:\n\n    \n    \n    def noise_stats(noise): diff = noise - noise.mean(dim=(1,2,3), keepdim=True) zscores = diff / noise.std(dim=(1,2,3), keepdim=True) skews = (zscores**3).mean(dim=(1,2,3)) kurtoses = (zscores**4).mean(dim=(1,2,3)) - 3.0 return skews, kurtoses\n\nMaybe try that and see what you get? If you can, nail down if you're getting\nNaNs in the forward or backward pass. I suspect backward.I let my training run\ngo for 150 epochs last night (using std_loss_weight=1.0, kl_loss_weight=0.004,\nskew_loss_weight=0.75, kurtosis_loss_weight=0.05), and the results are pretty\nconclusive. The classic signs of overtraining are more or less fully absent.\nHere are 3 samples at epochs 50, 100, and 150:Metrics for the run:And here is\nlora error / baseline error, so anything < 1 indicates improvement in error at\nthose timesteps. The improvement minimum at the T=250 level and the T=1000\nlevel are very interesting; also interesting is the increased loss in the\nearliest timesteps. (Side note: it might be interesting to try taking a lora\ntrained to a state like this, and then continue training it restricted to the\nfirst 100 timesteps or so).  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\nTopSalad3530 Apr 23, 2024\n\n-\n\nThe new formulation does prevent NaNs from appearing.I've started logging the\nmetrics myself, and I've noticed that despite having had 0 weights assigned to\nthem for most of the training process, it looks like skew_loss and kurt_loss\nhave settled into rather reasonable ranges (compared to your results)\nseemingly by themselves:I'm logging infrom train_db.py, having switched to DB\nfine-tuning after merging my previous results into the base model, which means\nthat the numbers charted here are after multiplication with the\nhyperparameters (currently same as your latest ones), but still these numbers\ndon't seem to be bad at all even if you divide the weights out. Maybe std is\nall you need?(My previous model was a LoKR. Initially, I made a mistake by\nusing merge_lora.py instead of the appropriate version from LyCORIS. This\ncaused the model to not-really-get-merged, and I didn't notice because I\ndidn't sample_at_first. After I got the correct data though, it seemed that my\noriginal post still applied, so I've restored it.)  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n###\n\ndrhead\n\nApr 22, 2024\n\n-\n\nHey, you might want to check out this recent nvidia paper:\nhttps://research.nvidia.com/labs/toronto-ai/AlignYourSteps/It looks like it\ncould effectively be a way to handle this issue on the inference end -- or at\nthe very least, you could gain some useful insights that are relevant to your\nproblem by reading the paper and experimenting with the schedule they have\n(keeping in mind that it is dataset- and model-dependent). They unfortunately\nhaven't released \"training\" code (this isn't really training, it's a zeroth-\norder optimization), but I've collaborated with someone to replicate what I am\npretty sure is a valid and correct implementation, and I am experimenting with\nit.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nYou must be logged in to vote\n\n1 reply\n\nComment options\n\n####\n\ncheald Apr 22, 2024\n\n-\n\nI actually saw this one, but initially skipped over it because it was\ninference-focused. However, reading over it in light of what I've been doing\nlately, there might be some good stuff to learn. Thanks!  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n###\n\ndoctorpangloss\n\nApr 22, 2024\n\n-\n\n@cheald thanks for your diligent work on this avenue of research\n\n> I'm also training with masked loss, which should, in theory, disregard the\n> background of my image entirely.\n\nbased on my empirical results and my interpretation of what is being trained\nin particular by LoRA fine tunings, masked loss does not ignore backgrounds.\nIt will learn this aka for a region of noisy pixels, a function that will make\nthat region trend towards \"gray\".Another interpretation is that error is\nincreasing for the backgrounds after every iteration and simply being\nignored.My hypothesis for why masked training works well is that for many of\nthe subjects trained by the community, the number of steps needed to achieve\ndecent results does not add \"too much\" error to backgrounds. Specifically, the\nconditional Unet LoRAs for a face that already looks like a celebrity will\nhave small changes from identity (aka 0s) for good performance, and near small\nuntrained / random values, the amount of \"damage\" done to the weights\ncomputing backgrounds is relatively small. If you use masked training for many\nother subjects it tends to blow up backgrounds simply because the parameters\nvia whatever fine tuning method have to \"actually\" learn something.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nYou must be logged in to vote\n\n7 replies\n\nComment options\n\n####\n\ncheald Apr 22, 2024\n\n-\n\nWhich modifications were you using? I've proposed a whole bunch in this thread\n:)\n\n> Bad news: in some generations, parts of the image outside the subject seem\n> to have weird colors and textures, it reminds me of some VAE decoding\n> artifacts I've seen elsewhere. The effect is more pronounced when CFG is\n> higher.\n\nIf you were doing this with the statistical objective losses in play, this\nmight be due to the network learning to predict extreme values in the masked\nregions in order to satisfy the statistical loss objectives without being\npenalized by the classic loss objective due to masking.I have had another\nthought about adding an objective which punishes the network harshly if it\npredicts noise values which are outside of a particular extreme range, but I\ndidn't settle on a way to decide what that range is, or how to scale the loss.\nHowever, since we know that extreme-valued pixels in the latent translate into\nclipped images once decoded, it might be worth pursing. My hope was that the\nmore implicit methods would implicitly address it without having to go after\nthose values directly, but I haven't measured it yet. I'd suggest that\nanything beyond -5/+5 is probably suspect. Maybe just something like adding a\nterm like (relu(noise-5) + relu(-noise-5)).sum(dim=(1,2,3)) would do it, but\nthat's just a guess.\n\n> in your suggested implementation we are calculating the loss after applying\n> the mask. Would it make sense to do this loss calculation before masking?\n\nIt would be worth trying, but my suspicion was that each masked subregion of\neach sample would have a somewhat unique set of statistics associated with it,\nand that it wouldn't be productive to try to learn them, versus learning to\njust predict the overall known statistics.Trying it would be easy enough -\nsince our outputs from the statistical functions are a single number per\nchannel, we don't have to maintain shape parity and could compute those stats\non the masked subset of values from noise/noise_pred, though we'd likely need\nto do them sample-wise, since different samples would end up with different\nshapes, which would be less efficient. I need to check the math, but maybe\nsomething like using a relationship between stats(masked pixels) and\nstats(unmasked pixels) would work, too. I don't think it's valid to just\ncompute a tensor-wide stat on (noise*mask) because that'll artificially\ndepress all the statistics due to the large number of 0-valued entries.I have\ntinkered with masked image dropout, too, and didn't feel that it made a big\ndifference, but it would be worth trying in concert with these changes.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\nrecris Apr 22, 2024\n\n-\n\nI'm just using the code snippet you posted to calculate multiple loss\nobjectives, no other modifications. I also using the recently implemented\npseudo huber loss function, but I don't think it matters here?  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\ncheald Apr 22, 2024\n\n-\n\nI don't think the pseudo-huber loss should directly affect things here, but I\nhaven't tested interactions with it.I've since improved the kl_div loss to use\nsmooth histograms so it's now differentiable and can actually steer gradient\ndescent; I think the version you ran was more or less just learning a std of\n~1. That's at https://github.com/cheald/sd-scripts/tree/kl_div_lossOne other\ndifference is that I'm using a normalized mask loss, which scales up the loss\nby the percentage of the image which is masked out. That is, if you had an\nimage with only 1/4 of the image considered via mask, then the loss would be\nincreased by a factor of 4. I don't know if this is correct (or even\nproductive yet), but I'm using it to try to get a more apples-to-apples\ncomparison. Without it, samples with a larger percentage of their pixels\nmasked out will be more influenced by statistical loss than by the standard\nloss.\n\n    \n    \n    index 406e0e3..14c8d8e 100644 --- a/library/custom_train_functions.py +++ b/library/custom_train_functions.py @@ -486,7 +486,10 @@ def apply_masked_loss(loss, batch): # resize to the same size as the loss mask_image = torch.nn.functional.interpolate(mask_image, size=loss.shape[2:], mode=\"area\") mask_image = mask_image / 2 + 0.5 - loss = loss * mask_image + + pct = mask_image != 0 + pct_unmasked = pct.int().to(dtype=loss.dtype).sum(dim=(1,2,3)) / (mask_image.shape[-1] * mask_image.shape[-2]) + loss = loss * mask_image / pct_unmasked.view(-1, 1, 1, 1) return loss  \n  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\nrecris Apr 23, 2024\n\n-\n\nI've experimented with normalized mask loss before, didn't observe a\nsignificant difference.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n####\n\ncheald Apr 23, 2024\n\n-\n\nMy thought is that it might matter more with multi-objective loss, since the\nratio between losses does matter.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nSign up for free to join this conversation on GitHub. Already have an account?\nSign in to comment\n\nCategory\n\nShow and tell\n\nLabels\n\nNone yet\n\n12 participants\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
