{"aid": "40157348", "title": "Did we just receive an AI-generated meta-review?", "url": "http://opensamizdat.com/posts/llm_meta_review/", "domain": "opensamizdat.com", "votes": 3, "user": "empiko", "posted_at": "2024-04-25 13:32:46", "comments": 0, "source_title": "\u203b Did we just receive an <abbr>AI</abbr>-generated meta-review?", "source_text": "\u203b Did we just receive an <abbr>AI</abbr>-generated meta-review?\n\n# \u203b Open Samizdat\n\n2024-04-25\n\n## Did we just receive an AI-generated meta-review?\n\nAbstract I recently received what I believe is an LLM-generated meta-review\nfor one of my papers. This is the first time for me, but I am afraid that it\nis not the last. I will talk about why I think this review was generated\nautomatically, but more importantly, I would like to start a debate about the\nethics of using LLMs in the reviewing process. I believe that the entire\ntrust-based concept of peer review might be threatened with the proliferation\nof LLMs.\n\n\u2042\n\nLast February, we submitted a paper about the GEST dataset to the ACL Rolling\nReview system. In April, we have obtained 3 reviews and then a meta-review. As\nsoon as I read the meta-review, I smelled a text generated with an LLM. While\nthere are valid ways of using LLMs for reviewing, for example as writing\nassistants, they still require attentive human oversight. I came to a\nconclusion that the level of human oversight in this case might have been less\nthan I would prefer. The meta-review is not completely unreasonable and mostly\nsummarizes points raised by the reviewers, but there are still some issues\nthat might have been caused by how it was created. The fact that I can not\ntell know how much human effort actually went into it bothers me as well.\n\nThis blog has two sections. The Evidence section describes why I believe that\nthe meta-review is LLM-generated. Other than going by vibes, it is\nsurprisingly difficult to describe what exactly smells like an LLM in a text.\nI will also go through various issues of how the ideas are formulated in the\nmeta-review, such as repetitiveness or hallucinations, and how these issues\nmight have been caused by the generation process. I think this is a useful\nexercise, mainly because it provides insights into what you can expect when\nyou receive an automatically generated meta-review.\n\nThe Implications section discusses the impacts on authors when they receive\nLLM-generated reviews, as well as the impacts on the entire peer review\nprocess and the scientific community at large. LLMs have the capability to\ndamage peer review as one of the cornerstones of today\u2019s science, and there\nare signs that it is already happening. My conclusion is that they should be\ncompletely banned from the review process.\n\nMy goal here is to describe how it feels to be on the receiving end of such a\nreview, but also to open the discussion about using LLMs in reviewing. As far\nas I am aware, the top AI conferences are just formulating their positions on\nthis issue. My guess is that other fields lag behind as they do not have\nexperts in the field readily available. The risk that LLMs will be misused and\nwill harm authors and their careers is imminent and immense.\n\n\u2042\n\n### Evidence\n\nThis analysis is speculative, but I consider the evidence for the hypothesis\nthat the meta-review was indeed LLM-generated to be really strong. Still, I\nmight be proven wrong. The meta-review can be found here.\n\n#### Style\n\nMy original suspicion came from the very specific style of the text. I work\nwith LLMs as a researcher almost daily, so I would like to think that I have a\npretty fine-tuned sense for detecting their outputs. In general, the style is\nvery verbose and almost tiresome in how little meat, and how much noise there\nis. Consider the following snippet:\n\n> Additionally, the utility of the resources provided, including the datasets\n> and metrics, is a substantial boon for researchers in the field, furnishing\n> tools for further exploration of how stereotypes are propagated through AI\n> systems.\n\nThis is such a convoluted way of basically saying that the provided datasets\nand metrics are useful. It would be highly unusual for a human reviewer to\nwrite like this. The reviews are usually much more concise and to the point.\nBased on this smell, I decided to check what AI detection tools have to say.\n\n#### AI detection tools\n\nI have some experience with training and benchmarking tools for detecting AI-\ngenerated texts, so I also know that they are usually far from perfect. To be\nsure, I have decided to use an ensemble of such tools. I googled \"ai text\ndetection\" and used the top 5 hits. I let the tools detect (1) the meta-review\nwe received, (2) one of the human-written reviews we received, and (3) a meta-\nreview I generated with ChatGPT and edited to match the format.\n\nTool| (1) Our meta-review| (2) Human review| (3) ChatGPT meta-review  \n---|---|---|---  \nCopyleaks| 100%| 0%| 100%  \nQuillBot| 100%| 0%| 80%  \nScribbr| 86%| 14%| 100%  \nZeroGPT| 0%| 0%| 0%  \nGPTZero| 100%| 0%| 100%  \n  \nApart from ZeroGPT which does not seem to work at all, all the other tools\ngive positive results for both the meta-review we received and the one I have\ngenerated with ChatGPT. The tools that highlight what specific parts are LLM-\ngenerated all highlighted the entire meta-review, including the discussion\nabout the final verdict. This little experiment further confirmed my\nsuspicions. Based on this, I decided to analyze the content of the review more\nthoroughly.\n\n#### Repetitive text\n\nThe first thing I noticed after reading the meta-review more carefully is the\nrepetitiveness in basically all the paragraphs. The most egregious example is\nthe two paragraphs in the weaknesses section. I have color-coded the sentences\nthat are about the same objections to make it more obvious:\n\n> The paper falls short in several areas that need attention. First, the\n> evaluation metrics introduced for measuring gender bias require clearer\n> definitions and more robust validation. These metrics need to accurately\n> reflect what they measure and be interpretable to the readers. The paper\u2019s\n> lack of exploration into the implications of detected biases on downstream\n> applications also limits its practical relevance. Understanding how gender\n> bias in language models impacts real-world tasks such as translation\n> accuracy or co-reference resolution could significantly enhance the paper\u2019s\n> applicability. Moreover, the comparative analysis with related works is\n> insufficient. A more thorough comparison and a clearer exposition of how\n> this study differs from and improves upon previous efforts would solidify\n> the paper\u2019s place in the literature.\n>\n> The authors should consider revising their metrics to ensure they accurately\n> reflect the presence of gender biases and are interpretable to practitioners\n> and researchers. Additionally, expanding the discussion on how detected\n> biases affect practical applications will make the findings more relevant to\n> a wider audience. This could involve linking bias metrics to performance in\n> specific tasks through case studies or additional experiments. Enhancing the\n> comparative analysis with existing benchmarks and clarifying the unique\n> contributions of this study would also help position the paper more clearly\n> within the existing research landscape. Addressing minor presentation issues\n> such as overlapping figures, ambiguous statements, and providing clearer\n> definitions for terms like \u201cgender experts\u201d would also improve the paper\u2019s\n> clarity and professionalism.\n\nThis is basically the same paragraph written twice! Both paragraphs have the\nsame 3 objections in the same order, and sometimes they even use identical\nwording (e.g., \u201caccurately reflect\u201d, \u201ccomparative analysis\u201d). I do not believe\nthat a human could write like this by mistake.\n\n#### Hallucinated objections\n\nNext, I looked for statements that could be hallucinations \u2014 stuff that the\nmodel made up. I found several such cases, although they are all inspired by\nwhat was actually said by the reviewers. This is not an entirely LLM-specific\nrisk. A human meta-reviewer might misunderstand or make wrong assumptions\nabout the reviews as well. Fortunately, these hallucinations are all more or\nless not that important for the overall message of the meta-review.\n\n  * \u201cUnderstanding how gender bias in language models impacts real-world tasks such as translation accuracy [...] could significantly enhance the paper\u2019s applicability.\u201d \u2014 This objection does not make sense, because we do not use language models for machine translation, and our dataset also cannot really be used to measure translation accuracy. I believe that this objection is based on the following snippet from a human-written review: \u201c[...] the issue of its effect on downstream tasks isn\u2019t really discussed. For instance, are gendered translations worse for these models when translating. Is co-reference resolution performance correlated to stereotypical gender assumptions?\u201d The problem here is that the first part of this human remark was made about machine translation systems, but the meta-review assumes that it is about language models.\n  * \u201cAddressing minor presentation issues such as overlapping figures [...] would also improve the paper\u2019s clarity and professionalism.\u201d \u2014 There are no overlapping figures in our paper. This is probably based on a similar remark made by one of the reviewers: \u201cFigure 6 is hard to read since the text overlaps significantly.\u201d These two sentences have clearly different meanings. Also, did this LLM just judge our professionalism? Rude.\n  * \u201cAdditionally, the utility of the resources provided, including the datasets and metrics, is a substantial boon for researchers in the field, furnishing tools for further exploration of how stereotypes are propagated through AI systems.\u201d \u2014 There is only one dataset (singular) introduced in the paper, not datasets (plural). This is a minor nitpick, but it is interesting because datasets (plural) is mentioned only in the canned responses that the reviewers select: \u201cDatasets: 4 = Useful: I would recommend the new datasets to other researchers or developers for their ongoing work.\u201d The motif or recommending the datasets to researchers might have come from this response as well.\n  * \u201cthe evaluation metrics [...] require clearer definitions\u201d \u2014 Arguably, no reviewer claims that the metrics are not clearly defined. One reviewer raised a question about how clear the interpretability of a metric is, but problems with the clarity of definition usually mean something different.\n\n#### Contradictions\n\nThis is a bit subtle, but the strengths and weaknesses sections contradict\neach other. When these two sections describe the metrics, they use\nsignificantly different language. On one hand, our \u201cmethodological rigor is\nsignificant strength\u201d, our metrics are \u201ccritical for understanding and\nmitigating gender bias in AI\u201d, and they are even \u201ca substantial boon for\nresearchers\u201d. Based on this language, our metrics seem pretty great! But at\nthe same time, we \u201cshould consider revising our metrics to ensure they\naccurately reflect the presence of gender biases\u201d and they also \u201crequire\nclearer definitions and more robust validation.\u201d These two viewpoints rule\neach other out, and I have a hard time imagining human writing such\ncontradictory remarks.\n\n\u2042\n\n### Implications\n\nConsidering the evidence above, I am convinced that our meta-review is LLM-\ngenerated. The question remains: how much human oversight went into the\ngeneration process? Some of the issues in our meta-review make me think that\nthe amount of oversight in this case might have been less than I would have\nliked. An attentive meta-reviewer should have noticed that two generated\nparagraphs are almost identical in meaning; should have ensured that the\nreviewers\u2019 objections are reproduced carefully; and should have ensured that\nthere are no contradictions in the text. And if the meta-reviewer was not that\nattentive, I find it hard to trust their judgment when they decided on the\nfinal verdict for our paper.\n\nThe unfairness of not getting the paper we spent months on properly reviewed\nis not the only problem here. We now also have to deal with the consequences\nof the meta-review \u2014 writing rebuttals, revisions, cover letters, conducting\nexperiments \u2014 all based on what an LLM generated. The reviewers in the next\nround will also have to read the generated text and reflect upon it. An LLM-\ngenerated review for one paper can easily lead to tens of researcher human-\nhours spent pondering what a stochastic parrot said.\n\nDespite my speculations, it is impossible to tell how much human input\nactually went into this meta-review. The meta-reviewer might have simply copy-\npasted entire reviews and asked an LLM to summarize them. It is completely\nfeasible for LLMs nowadays to write a meta-review such as the one we obtained\nthis way. Or, the meta-reviewer could have actually done their job properly,\nengaging with the reviews and discussion and only using an LLM to summarize\ntheir own notes. With LLMs being as available as they are now, there will\nalways be this uncertainty in the peer review process from now on.\n\n\u2042\n\nThis uncertainty is an extremely destructive concept that can easily endanger\ntrust within scientific communities. Subjectively, trust in peer review in the\nAI community was pretty low even before LLMs. If we start having a significant\npercentage of reviews generated automatically, trust will plummet even more.\nThe publish-or-perish culture is the underlying root cause here, creating all\nthe wrong incentives, but the misuse of LLMs can definitely synergize with it\nand reinforce the negative feedback loop.\n\nWorryingly, our paper is not an exception. Up to 16.9% of reviews in high-\nimpact conferences such as NeurIPS, ICLR, or EMNLP are already generated with\nLLMs\n\nLiang, Weixin, et al. Monitoring AI-Modified Content at Scale: A Case Study on\nthe Impact of ChatGPT on AI Conference Peer Reviews. arXiv preprint\narXiv:2403.07183.\n\n. This is frankly a shocking number. If this number is true, you have only a\n48% chance of receiving a fully human set of three reviews and one meta-review\n(0.831^4). The authors also show that these LLM-generated reviews are\nsystematically less confident and submitted closer to the deadline, indicating\nthat this cannot be attributed solely to using LLMs as writing assistants.\nEven a single LLM-generated review can lead to tens of person-hours being lost\non unnecessary work. If we scale this to the jumbo conferences with thousands\nof papers, we could be seriously talking about several human-decades worth of\nresearcher time being lost due to LLMs every time a major conference is\nhappening!\n\nIf we continue to allow LLMs to influence the peer review process, we are\nfirmly entering cargo cult science territory \u2014 we are pretending that\nscientific discourse is happening and emulating all the usual steps, but the\ncore idea of why this process was originally created eludes us. My conclusion\nis that the use of LLMs for reviewing should be strictly forbidden. What they\nbring to the table as writing assistants is not worth the harm they can cause.\nMost researchers would rather receive a review with bad English they can trust\nthan an eloquent hallucination. The entire premise of peer review is that an\nexpert in the field carefully considers your paper. Banning LLMs would\nunfortunately not guarantee this bare minimum, but at least it would take away\na readily available tool that tempts some reviewers to make their work easier.\n\n\u2042\n\n### Cite\n\n    \n    \n    @misc{pikuliak_llm_meta_review, author = \"Mat\u00fa\u0161 Pikuliak\", title = \"Did we just receive an AI-generated meta-review?\", howpublished = \"https://www.opensamizdat.com/posts/llm_meta_review\", month = \"04\", year = \"2024\", }\n\n\u2042\n\n### Comments\n\n\u2042\n\n", "frontpage": false}
