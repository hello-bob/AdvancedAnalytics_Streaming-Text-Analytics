{"aid": "40059783", "title": "Posteriors from Normal \u2013 Approximate Bayesian inference for large models", "url": "https://blog.normalcomputing.ai/posts/introducing-posteriors/posteriors.html", "domain": "normalcomputing.ai", "votes": 1, "user": "dark_jensen", "posted_at": "2024-04-17 02:27:36", "comments": 0, "source_title": "posteriors", "source_text": "The Normal Blog - posteriors: Normal Computing\u2019s library for Uncertainty-Aware\nLLMs\n\n# posteriors: Normal Computing\u2019s library for Uncertainty-Aware LLMs\n\nScalable uncertainty quantification with PyTorch and Bayes\n\nengineering\n\nresearch\n\nAuthor\n\nSam Duffield\n\nPublished\n\nApril 16, 2024\n\nposteriorsis a new open source Python library from Normal Computing that\nprovides tools for uncertainty quantification and Bayesian computation. We use\nPyTorch and its functional API. Online learning and hallucination detection\nare understood as frontier problems in AI and with LLMs. Here we introduce the\nposteriors library and demonstrate how it can robustify predictions and avoid\ncatastrophic forgetting.\n\n# Uncertainty: computing the unknown \ud83e\udd37\n\nThere are a number of ways which the community has begun investigating trading\noff compute for reliability \u2013 including adaptively \u2013 in AI models. We will\ndiscuss a path which has been less explored given scalability and\naccessibility questions which we seek to resolve. By investing more compute\ninto the probabilistic inference required to perform uncertainty\nquantification, we can unlock LLMs that hallucinate less, understand their own\nlimits, and even reason with higher precision.\n\nRobust decision making needs to handle complex uncertainty. In the context of\ndeep learning, uncertainty quantification is particularly important because\nneural networks are often overconfident in their predictions or generations,\nsalmon jumping in a river anyone?\n\nWhat\u2019s more, traditional neural networks do not have the capacity to inform\nyou when they are met with unfamiliar data or are asked about something they\ndon\u2019t know. By quantifying uncertainty in the model parameters, we can average\npredictions over many plausible model instances given the training data. This\nprovides a compelling route to more robust predictions on unseen data.\nAccurate uncertainty characterisation also provides the ability to identify\nsituations where the model is met with data beyond that it has seen in\ntraining, thus critically improving model auditability.\n\nBayesian updating is concisely described by Bayes\u2019 theorem:\n\nThe prior distribution encodes our current beliefs and the likelihood function\nrelates the model parameters to the data. Bayes\u2019 theorem then tells us exactly\nhow to update our beliefs in the face of new data. Thus providing a cohesive\nframework for the continual learning of new information, which can also be\nused for informed online decision making.\n\n# Aren\u2019t posterior distributions intractable? \ud83e\udd2f\n\nWhilst Bayes theorem gives you a coherent way to update your beliefs in the\nface of new data, computing the posterior distribution is often intractable \u2014\nespecially when said distribution is over trillions of parameters of a neural\nnetwork. The good news is that approximate posterior distributions, when\ncomputed effectively, provide many of the benefits promised by exact Bayesian\ninference. But they can still be tricky to deal with and hard to compute\nespecially for Large Language Models \u2013 a massive hurdle we wanted to unlock.\n\nTraditional techniques for Bayesian computation have typically relied on\nmethods such as Markov chain Monte Carlo (MCMC), where the posterior\ndistribution is represented by a set of samples (generated iteratively). MCMC\nmethods are powerful but computationally expensive in the settings of very\nlarge datasets due to having to query all datapoints at every step of the\niterative algorithm. In the context of deep learning, this can be\nprohibitively slow.\n\nposteriors provides a suite of tools allowing for approximate Bayesian\ninference that is scalable to settings of many parameters and/or large\ndatasets. Many of the posteriors methods have also been carefully implemented\nto provide a seamless transition between optimization and Bayesian\ncomputation^1.\n\n# Why posteriors? \u0398\n\nposteriors is designed to be a comprehensive library for uncertainty\nquantification in deep learning models. The key features outlining the\nposteriors philosophy are:\n\n  * PyTorch: posteriors is built on top of PyTorch, this means that it can be integrated with pre-trained models such as Llama2 and Mistral via Hugging Face\u2019s transformers package. posteriors takes elements of the JAX packages fortuna and blackjax (plus more) and brings them to the PyTorch ecosystem.\n  * Functional: posteriors adopts a functional API via torch.func. The functional approach, as championed by the JAX ecosystem, makes for code that is easier to test and compose with other functions. Importantly for posteriors, functional programming is also closer to the mathematical description which is particularly useful for Bayesian modelling.\n  * Extensible: The transform framework^2 adopted by posteriors is very general and allows for the easy adoption of new algorithms. Additionally, posteriors supports arbitrary likelihoods^3 rather than being restricted to hard coded regression or classification as is common in other libraries like fortuna or laplace.\n  * Swappable: The framework also allows the user to seamlessly switch between approaches.\n  * Scalable: posteriors is minibatch first thus allowing for efficient computation in large datasets. Additionally flexible subspace methods are provided for scaling to large models.\n  * Composable: posteriors composes seamlessly with other torch libraries including transformers for pre-trained models, torch.distributions for probabilistic modeling, torchopt for functional optimization and lightning for convenient logging and training.\n\nThe python ecosystem is rich with wonderful tools for deep learning and\nuncertainty. Including fortuna, laplace, blackjax, numpyro, uncertainty-\nbaselines and more, however none meet all of the above criteria. posteriors is\ndesigned to be a one-stop shop for uncertainty quantification in deep learning\nmodels.\n\nposteriors is open-source! Come try it out, raise an issue or contribute a\nmethod! github.com/normal-computing/posteriors\n\n# Learning without forgetting \ud83d\udd04\n\nThe key difficulty in continual learning is adapting to new data without\nforgetting what has been learned before, so-called catastrophic forgetting. In\nour continual_lora example we demonstrate how a Laplace approximation using\nposteriors can be used to help Llama-2-7b retain old information whilst it is\ntrained on a series of books from the pg19 dataset.\n\nFigure 1: Continual validation loss by episode\n\nFigure 2: Average cumulative validation performance\n\nIn Figure 1, we compare the continual learning of the LLM stochastic gradient\ndescent (AdamW) against a Bayesian inspired Laplace approximation approach.\nThe dashed vertical lines represents \u201cnew episodes\u201d where the model starts\ntraining on a new book - after this point the model does not see the book\nagain. The horizontal dashed lines represent a single offline train with\naccess to all four training datasets concurrently; the network\u2019s total\nlearning capacity, although this isn\u2019t feasible in a practical online setting.\nEach row represents validation loss for a different book. For example, in the\nfirst row we can see that the SGD approach quickly forgets the information it\nhas learned from the first book as it trains on new books, whereas the\nBayesian LLM encourages the model to retain knowledge.\n\nIn Figure 2, we track the average performance^4 of the two approaches over\nbooks seen so far. We can see that SGD, when averaged across all tasks\nperforms extremely^5 poorly compared to the Bayesian LLM. The key thing here\nis that the use of the approximate Bayesian method allows you to use a single\nmodel to learn across multiple tasks whereas with traditional methods you\nwould need to train a new model for each task.\n\nThis example demonstrates how posteriors can be used to implement a continual\nlearning strategy and assist the model in learning tasks sequentially.\nHowever, the Laplace approximation represents a very simple and somewhat crude\napproximation to Bayesian updating, certainly there is room for further\nimprovement. posteriors can help with this! Via its flexible and extensible\nframework we can add and compare different and new approaches.\n\nFurther information on this example (and others) including complete code can\nbe found on GitHub.\n\n# Knowing what you don\u2019t know \ud83e\udd14\n\nBayesian methods provide the ability to break predictive uncertainty into two\ncomponents: aleatoric and epistemic uncertainty^6. Aleatoric uncertainty is\nthe uncertainty inherent in the data itself (for example, a review like \u201cThe\nfood was amazing! But the service was horrendous!\u201d would have a high amount of\naleatoric uncertainty when predicting the associated rating), whereas\nepistemic uncertainty would be reduced with more data. High epistemic\nuncertainty is an indication that the model is unsure about the data it is\nbeing asked to predict.\n\nSo in principle, we might hope to use epistemic uncertainty as a measure to\npredict hallucinations in LLMs \u2013 low confidence should correlate with\nmistakes.\n\nFigure 3: Uncertainty breakdown on English test data (in-distribution).\n\nFigure 4: Uncertainty breakdown on Spanish test data (out-of-distribution).\n\nIn the yelp example, we use posteriors to train a host of Bayesian methods on\nthe Yelp review dataset (English). In Figure 3, we show the breakdown of\nuncertainty on the in-distribution English data. We compare this to\nuncertainty on out-of-distribution Spanish data in Figure 4. The non-Bayesian\noptimisation (map) method does not provide the ability to breakdown\nuncertainty, whereas the Bayesian methods successfully identify an increase in\nepistemic uncertainty on the out-of-distribution data, allowing us to infer\nthat the model does not know the answer in this case and would like to have\nsome Spanish training data to make more accurate predictions.\n\nAs before, comprehensive code and info on GitHub!\n\n# What\u2019s next? \ud83d\udd1c\n\nposteriors is a new Python library designed to make it possible to apply\nuncertainty quantification to large-scale deep learning models. This\nrepresents a key component of Normal Computing\u2019s mission to build AI systems\nthat natively reason, so they can partner with us on our most important\nproblems. We are excited to expand posteriors and support community efforts to\nimprove the auditability and robustness of AI systems, as well as integrating\nwith thermodynamic compute that can accelerate Bayesian posteriors methods. If\nyou are as interested as we are in advancing the frontier of AI reasoning and\nreliability then reach out to us at info@normalcomputing.ai!\n\n## Footnotes\n\n  1. Typically through a temperature parameter where temperature=0 represents optimisation and temperature=1 represents Bayes. With values in between also valid.\u21a9\ufe0e\n\n  2. posteriors conforms to a very general unified API where each method is comprised of build, init and update functions.\u21a9\ufe0e\n\n  3. There is an equivalence between the negative-log-likelihood function and the loss function in the context of maximum likelihood estimation. For example, a likelihood with a conditional Gaussian distribution is the same as the mean squared error loss function for regression and conditional Categorial distribution is the same as cross entropy for classification.\u21a9\ufe0e\n\n  4. To be exact, relative to the perfomance of the model trained to convergence on each individual book\u21a9\ufe0e\n\n  5. catastrophically, perhaps?\u21a9\ufe0e\n\n  6. Further details on second-order uncertainty can be found in e.g. Wimmer et al. It should be noted that the entropy approach to breaking down uncertainty has some potentially undesirable features and, in the Bayesian setting, can be sensitive to inaccuracies in the posterior approximation.\u21a9\ufe0e\n\n## Reuse\n\nCC BY-NC 4.0\n\n", "frontpage": false}
