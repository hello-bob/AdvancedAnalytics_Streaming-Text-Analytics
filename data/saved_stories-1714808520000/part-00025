{"aid": "40252592", "title": "Show HN: CandleEmbed \u2013 A crate for generating text embeddings from HF models", "url": "https://github.com/ShelbyJenkins/candle_embed", "domain": "github.com/shelbyjenkins", "votes": 1, "user": "J_Shelby_J", "posted_at": "2024-05-03 21:29:36", "comments": 0, "source_title": "GitHub - ShelbyJenkins/candle_embed: A simple, CUDA or CPU powered, library for creating vector embeddings using Candle and models from Hugging Face", "source_text": "GitHub - ShelbyJenkins/candle_embed: A simple, CUDA or CPU powered, library\nfor creating vector embeddings using Candle and models from Hugging Face\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nShelbyJenkins / candle_embed Public\n\n  * Notifications\n  * Fork 0\n  * Star 2\n\nA simple, CUDA or CPU powered, library for creating vector embeddings using\nCandle and models from Hugging Face\n\n### License\n\nMIT license\n\n2 stars 0 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# ShelbyJenkins/candle_embed\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nShelbyJenkinsRemoves box dyn err from exampleMay 3, 20243036c6f \u00b7 May 3,\n2024May 3, 2024\n\n## History\n\n17 Commits  \n  \n### src\n\n|\n\n### src\n\n| Updates README.md, tests, and Candle to 0.5.0| May 3, 2024  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| Changes ref cel wrapped functions to immutable| Apr 29, 2024  \n  \n### Cargo.toml\n\n|\n\n### Cargo.toml\n\n| Removes box dyn err from example| May 3, 2024  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Initial commit| Apr 17, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| Removes box dyn err from example| May 3, 2024  \n  \n## Repository files navigation\n\n# CandleEmbed \ud83e\udde8\n\nText embeddings with any model on hugging face. Embedded in your app. Replace\nyour API bill with a GPU.\n\n### Features\n\n  * Enums for most popular embedding models OR specify custom models from HF (check out the leaderboard)\n\n  * GPU support with CUDA\n\n  * Builder with easy access to configuration settings\n\n### Installation\n\nAdd the following to your Cargo.toml file:\n\n    \n    \n    [dependencies] candle_embed = \"*\" [dependencies] candle_embed = { version = \"*\", features = [\"cuda\"] } // For CUDA support\n\n### Basics \ud83e\udee1\n\n    \n    \n    use candle_embed::{CandleEmbedBuilder, WithModel}; fn main() -> Result<()> { // Create a builder with default settings // let candle_embed = CandleEmbedBuilder::new().build()?; // Embed a single text // let text = \"This is a test sentence.\"; let embeddings = candle_embed.embed_one(text)?; // Embed a batch of texts // let texts = vec![ \"This is the first sentence.\", \"This is the second sentence.\", \"This is the third sentence.\", ]; let batch_embeddings = candle_embed.embed_batch(texts)?; Ok(()) }\n\n### Custom \ud83e\udd13\n\n    \n    \n    // Custom settings // builder .approximate_gelu(false) .mean_pooling(true) .normalize_embeddings(false) .truncate_text_len_overflow(true) // Set model from preset // builder .set_model_from_presets(WithModel::UaeLargeV1); // Or use a custom model and revision // builder .custom_embedding_model(\"avsolatorio/GIST-small-Embedding-v0\") .custom_model_revision(\"d6c4190\") // Will use the first available CUDA device (Default) // builder.with_device_any_cuda() // Use a specific CUDA device // builder.with_device_specific_cuda(ordinal: usize); // Use CPU (CUDA options fail over to this) // builder.with_device_cpu() // Unload the model and tokenizer, dropping them from memory // candle_embed.unload(); // --- // These are automatically loaded from the model's `config.json` after builder init // model_dimensions // This is the same as \"hidden_size\" // let dimensions = candle_embed.model_dimensions; // model_max_input // This is the same as \"max_position_embeddings\" // If `truncate_text_len_overflow == false`, and your input exceeds this a panic will result // If you don't want to worry about this, the default truncation strategy will just chop the end off the input // However, you lose accuracy by mindlessly truncating your inputs // let max_position_embeddings = candle_embed.model_max_input; // ---\n\n### Tokenization \ud83e\uddee\n\n    \n    \n    // Generate tokens using the model let texts = vec![ \"This is the first sentence.\", \"This is the second sentence.\", \"This is the third sentence.\", ]; let text = \"This is the first sentence.\"; // Get tokens from a batch of texts // let batch_tokens = candle_embed.tokenize_batch(&texts)?; assert_eq!(batch_tokens.len(), 3); // Get tokens from a single text // let tokens = candle_embed.tokenize_one(text)?; assert!(!tokens.is_empty()); // Get a count of tokens // This is important to use if you are using your own chunking strategy // For example, using a text splitter on any text string whose token count exceeds candle_embed.model_max_input // Get token counts from a batch of texts // let batch_tokens = candle_embed.token_count_batch(&texts)?; // Get token count from a single text // let tokens = candle_embed.token_count(text)?; assert!(tokens > 0);\n\n### How is this differant than text-embeddings-inference \ud83e\udd17\n\n  * TEI implements a client-server model. This requires running it as it's own external server, a docker container, or locally as a server.\n  * CandleEmbed is made to be embedded: it can be installed as a crate and runs in process.\n  * TEI is very well optimized and very scalable.\n  * CandleEmbed is fast (with a GPU), but was not created for serving at the scale, of say, HuggingFace's text embeddings API.\n\ntext-embeddings-inference is a more established project, and well respected. I\nrecommend you check it out!\n\n### How is this differant than fastembed.rs \ud83e\udd80\n\n  * Both are usable as a crate!\n  * Custom models downloaded and ran from hf-hub by entering their repo_name/model_id.\n  * CandleEmbed is designed so projects can implement their own truncation and/or chunking strategies.\n  * CandleEmbed uses CUDA. FastEmbed uses ONNX.\n  * And finaly.. CandleEmbed uses Candle.\n\nfastembed.rs is a more established project, and well respected. I recommend\nyou check it out!\n\n### Roadmap\n\n  * Multi-GPU support\n  * Benchmarking system\n  * Bindings for GO/Python\n\n### License\n\nThis project is licensed under the MIT License.\n\n### Contributing\n\nMy motivation for publishing is for someone to point out if I'm doing\nsomething wrong!\n\n## About\n\nA simple, CUDA or CPU powered, library for creating vector embeddings using\nCandle and models from Hugging Face\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\nActivity\n\n### Stars\n\n2 stars\n\n### Watchers\n\n2 watching\n\n### Forks\n\n0 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Languages\n\n  * Rust 100.0%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
