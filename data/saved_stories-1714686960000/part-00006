{"aid": "40237466", "title": "A Beginner's Guide to Vector Embeddings", "url": "https://www.timescale.com/blog/a-beginners-guide-to-vector-embeddings/", "domain": "timescale.com", "votes": 1, "user": "Anon84", "posted_at": "2024-05-02 15:38:30", "comments": 0, "source_title": "A Beginner\u2019s Guide to Vector Embeddings", "source_text": "A Beginner\u2019s Guide to Vector Embeddings\n\nOpens in a new window Opens an external website Opens an external website in a\nnew window\n\nThis website utilizes technologies such as cookies to enable essential site\nfunctionality, as well as for analytics, personalization, and targeted\nadvertising purposes. You may change your settings at any time or accept the\ndefault settings. You may close this banner to continue with only essential\ncookies. Privacy Policy\n\nContact us\n\nProducts\n\nCustomer stories\n\nDevelopers\n\nPricing\n\nContact us\n\nLogin\n\nProducts\n\nTimescale is a reliable PostgreSQL cloud optimized for your business\nworkloads.\n\nTime series and analytics\n\nPostgreSQL, but faster. Built for lightning-fast ingest and querying of time-\nbased data.\n\nEarly access\n\nVector (AI/ML)\n\nPostgreSQL engineered for fast search with high recall on millions of vector\nembeddings.\n\nEarly access\n\nDynamic PostgreSQL\n\nPostgreSQL managed services with the benefits of serverless, but none of the\nproblems.\n\nIndustries that rely on us\n\nCrypto and finance Energy and environment Transportation and logistics\n\nTimescale benchmarks\n\nvs RDS vs Amazon Timestream vs Influx vs MongoDB vs ClickHouse vs Aurora\n\nWe're in your corner even during the trial phase. Contact us to discuss your\nuse case with a Timescale technical expert.\n\nDevelopers\n\nTimescale Docs\n\nStart using and integrating Timescale for your demanding data needs.\n\nDocs\n\nAI / Vector\n\nTimescale Vector Docs\n\nTimescale Vector Docs\n\npgvector docs\n\npgvector docs\n\nLearn PostgreSQL\n\nTimescale is PostgreSQL, but faster. Learn the PostgreSQL basics and scale\nyour database performance to new heights.\n\nGetting started\n\nGetting started\n\nTime-series database basics\n\nTime-series database basics\n\nBuilding blocks\n\nBuilding blocks\n\nTimescale benchmarks\n\nTimescale benchmarks\n\nPostgres cheat sheet\n\nPostgres cheat sheet\n\nBlog\n\nBlog\n\nTutorials\n\nTutorials\n\nSupport\n\nSupport\n\nCommunity\n\nCommunity\n\nGithub\n\nGithub\n\nSlack\n\nSlack\n\nForum\n\nForum\n\nSubscribe to the Timescale Newsletter\n\nBy submitting, you acknowledge Timescale's Privacy Policy\n\nSubscribe to the Timescale Newsletter\n\nBy submitting, you acknowledge Timescale's Privacy Policy\n\nSubscribe to the Timescale Newsletter\n\nBy submitting you acknowledge Timescale's Privacy Policy.\n\n# A Beginner\u2019s Guide to Vector Embeddings\n\n01\n\nTypes of Vector Embeddings\n\n02\n\nHow Do Neural Networks Create Embeddings?\n\n03\n\nHow Do Vector Embeddings Work?\n\n04\n\nHow to Create Vector Embeddings for Your Data\n\n05\n\nHow to Store Vector Embeddings\n\n06\n\nStart Creating Vector Embeddings Today\n\nSee More\n\nVector embeddings are essential for AI applications like retrieval-augmented\ngeneration (RAG), agents, natural language processing (NLP), semantic search,\nand image search. If you\u2019ve ever used ChatGPT, language translators, or voice\nassistants, there\u2019s a high chance you\u2019ve come across systems that use\nembeddings.\n\nAn embedding is a compact representation of raw data, such as an image or\ntext, transformed into a vector comprising floating-point numbers. It\u2019s a\npowerful way of representing data according to its underlying meaning. High-\ndimensional data is mapped into a lower-dimensional space (think of it as a\nform of \u201clossy compression\u201d) that captures structural or semantic\nrelationships within data, making it possible for embeddings to preserve\nimportant information while reducing the computational burden that comes with\nprocessing large datasets. It also helps uncover patterns and relationships in\ndata that might not have been apparent in the original space.\n\nSince an embedding model represents semantically similar things close together\n(the more similar the items, the closer the embeddings for those items are\nplaced in the vector space), you can now have computers search for and\nrecommend semantically similar things and cluster things by similarity with\nimproved accuracy and efficiency.\n\nIn this article, we\u2019ll examine vector embeddings in depth, including the types\nof vector embeddings, how neural networks create them, how vector embeddings\nwork, and how you can create embeddings for your data.\n\n### Are vectors and embeddings the same thing?\n\nWhile both terms are used interchangeably and refer to numerical data\nrepresentations where data points are represented as vectors in high-\ndimensional space, they\u2019re not the same thing. Vectors are simply an array of\nnumbers where each number corresponds to a specific dimension or feature,\nwhile embeddings use vectors for representing data in a structured and\nmeaningful way in continuous space.\n\nEmbeddings can be represented as vectors, but not all vectors are embeddings.\nEmbeddings generate vectors, but there are other ways of generating them, too.\n\n\ud83d\udca1\n\nLearn the difference between vector search and semantic search.\n\n### What is the difference between indexing and embedding?\n\nEmbedding is the process of turning raw data into vectors, which can then be\nindexed and searched over. Meanwhile, indexing is the process of creating and\nmaintaining an index over vector embeddings, a data structure that allows for\nefficient search and information retrieval from a dataset of embeddings.\n\n## Types of Vector Embeddings\n\nThere are many different kinds of vector embeddings, each representing a\ndifferent kind of data:\n\n  * Word embeddings: This is the most common type of embedding and represents words in NLP. They\u2019re typically used for capturing semantic relationships between words (such as antonyms and synonyms) and contextual usage in tasks like language translation and modeling, word similarity, synonym generation, and sentiment analysis. They also help enhance the relevance of search results by understanding the meaning of queries.\n  * Sentence embeddings: These are vector representations of sentences and capture their semantic meaning and context. They\u2019re used for tasks such as information retrieval, text categorization, and sentiment analysis. Sentence embeddings are also important for chatbots, allowing them to better understand and respond to user inputs as well as machine translation services, ensuring that the translations retain the context and meaning of the original sentence.\n  * Document embeddings: Like sentence embeddings, document embeddings are vector representations of documents like reports or articles. They capture the content and general meaning of the document and are used for tasks like recommendation systems, information retrieval, clustering, and document similarity and classification.\n  * Graph embeddings: These embeddings represent edges and nodes of graphs within the vector space and are used for tasks such as node classification, community recognition, and link prediction.\n  * Image embeddings: These are representations of different aspects of visual items like video frames and images. From individual pixels to full images, image embeddings classify image features and are used for tasks like content-based recommendation systems, image and object recognition, and image search systems.\n  * Product embeddings: Product embeddings can range from embeddings for digital products like songs and movies to physical products like shampoos and phones. These are useful for product recommendation (based on semantic similarity) and classification systems, and product searches.\n  * Audio embeddings: These embeddings are a representation of the different features of audio signals, such as the rhythm, tone, and pitch, in a vector format. They\u2019re then used for various applications such as emotion detection, voice recognition, and music recommendations based on the user\u2019s listening history. They\u2019re also essential for developing smart assistants that understand voice commands.\n\n### What types of objects can be embedded?\n\nMany kinds of data types and objects can be represented as vector embeddings.\nSome of the common ones include:\n\n  * Text: Documents, paragraphs, sentences, and words can be embedded into numerical vectors using techniques like Word2Vec (for word embeddings) and Doc2Vec (for document embeddings.\n  * Images: Images can be embedded into vectors using methods like CNNs (Convolutional Neural Networks) or pre-trained image embedding models like ResNet and VGG. These are commonly used in e-commerce applications.\n  * Audio: Audio signals like music or speech can be embedded into numerical representations using techniques like RNNs (Recurrent Neural Networks) or spectrogram embeddings. They capture auditory properties, making it possible for systems to interpret audio more effectively. Some common audio embedding applications include OpenAI Whisper and Google Speech-to-Text.\n  * Graphs: Edges and nodes in a graph can be embedded using techniques like graph convolutional networks and node embeddings to capture relational and structural information. Nodes in a graph represent entities like a person, product, or web page, and each edge represents the connection or link between those entities.\n  * 3D models and time-series data: These embeddings capture temporal patterns in sequential data and are used for sensor data, financial data, and IoT applications. Their common use cases include pattern identification, anomaly detection, and time series forecasting. Meanwhile, 3D model embeddings represent different geometric aspects of 3-dimensional objects and are used for tasks like form matching, objection detection, and 3D reconstruction.\n  * Molecules: Molecule embeddings that represent chemical compounds are used for molecular property prediction, drug discovery and development, and chemical similarity searching.\n\n## How Do Neural Networks Create Embeddings?\n\nNeural networks, including large language models like GPT-4, Llama-2, and\nMistral-7B, create embeddings through a process called representation\nlearning. In this process, the network learns to map high-dimensional data\ninto lower-dimensional spaces while preserving important properties of the\ndata. They take raw input data, like images and texts, and represent them as\nnumerical vectors.\n\nDuring the training process, the neural network learns to transform these\nrepresentations into meaningful embeddings. This is usually done through\nlayers of neurons (like recurrent layers and convolutional layers) that adjust\ntheir weights and biases based on the training data.\n\nThe process looks something like this:\n\nNeural networks often include embedding layers within the network\narchitecture. These receive processed data from preceding layers and have a\nset number of neurons that define the dimensionality of the embedding space.\nInitially, the weights within the embedding layer are initialized randomly and\nthen updated through techniques like backpropagation. These weights initially\nserve as the embedding themselves, and then they gradually evolve during\ntraining to encode meaningful relationships between input data points. As the\nnetwork continues to learn, these embeddings become increasingly refined\nrepresentations of data.\n\nThrough iterative training, the neural network refines its parameters,\nincluding the weights in the embedding layer, to better represent the meaning\nof a particular input and how it relates to another piece of input (like how\none word relates to another). Backpropagation is used to adjust these weights\nalong with other weights depending on whether the overall task involves image\nclassification, language translation, or something else.\n\nThe training task is essential in shaping the learned embeddings. Optimizing\nthe network for the task at hand forces it to learn embeddings that capture\nthe underlying semantic relationships within the input data.\n\nLet\u2019s take an example to understand this better. Imagine you\u2019re building a\nneural network for text classification that determines whether a movie review\nis positive or negative. Here\u2019s how it works:\n\n  * In the beginning, each word in the vocabulary is randomly assigned an embedding vector that represents the essence of the word in numerical form. For instance, the vector for the word \u201cgood\u201d might be [0.2, 0.5, -0.1], while the vector for the word \u201cbad\u201d might be [0.4, -0.3, 0.6].\n  * The network is then trained on a dataset of labeled movie reviews. During this process, it learns to predict the sentiment of the review based on the words used in it. This involves adjusting the weights, including the embedding vectors, to minimize errors in sentiment prediction.\n  * As the network continues to learn from the data, the embedding vectors for words are adjusted to better perform sentiment classification. Words that often appear together in similar contexts, like \u201cgood\u201d and \u201cexcellent,\u201d end up with similar embeddings, while words with opposite meanings, like \u201cterrible\u201d and \u201cgreat,\u201d have embeddings that sit further apart, reflecting their semantic relationships.\n\n## How Do Vector Embeddings Work?\n\nVector embeddings work by representing features or objects as points in a\nmultidimensional vector space, where the relative positions of these points\nrepresent meaningful relationships between the features or objects. As\nmentioned, they capture semantic relationships between features or objects by\nplacing similar items closer together in the vector space.\n\nThen, distances between vectors are used to quantify relationships between\nfeatures or objects. Common distance metrics include Euclidean distance,\ncosine similarity, and Manhattan distances and measure how \u201cclose\u201d or how\n\u201cfar\u201d vectors are to each other in the multidimensional space.\n\nEuclidean distance measures the straight-line distance between points.\nMeanwhile, cosine similarity measures the cosine of the angle between two\nvectors. The latter is often used to quantify how similar two vectors are,\nregardless of their magnitudes. The higher the cosine similarity value, the\nmore similar the vectors.\n\nConsider a word embedding space where words are represented as vectors in a\ntwo-dimensional space. In this space:\n\n  * The word \u201ccat\u201d might be represented as [1.2, 0.8]\n  * The word \u201cdog\u201d might be represented as [1.0, 0.9]\n  * The word \u201ccar\u201d might be represented as [0.3, -1.5]\n\nIn this example, the Euclidean distance between the words \u201ccat\u201d and \u201cdog\u201d is\nless than the distance between the words \u201ccar\u201d and \u201ccat\u201d, indicating that\n\u201ccat\u201d is more similar to \u201cdog\u201d than to \u201ccar.\u201d Meanwhile, the cosine similarity\nbetween \u201ccat\u201d and \u201cdog\u201d is also higher than the similarity between \u201ccat\u201d and\n\u201ccar\u201d, which further indicates their semantic similarity.\n\n### What you can create with embeddings (as a developer)\n\n  * Chatbots that use retrieval augmented generation to better answer user queries, generate contextually relevant responses, and maintain coherent conversations.\n  * Semantic search engines that can retrieve multimedia content, web pages, or documents based on semantic similarity instead of keyword matching for more relevant search results.\n  * Text classification systems that categorize documents based on phrases and words.\n  * Recommendation systems that recommend content depending on the similarity of keywords and descriptions.\n\nYou can also use embeddings for data preprocessing tasks like language\ntranslation, sentiment analysis, normalization, and entity recognition. Plus,\nembeddings can allow GenAI models to generate more realistic content, whether\nthat\u2019s images, music, or text.\n\nSimilarly, you can create application experiences with retrieval-augmented\ngeneration.\n\nEmbedding-based RAG systems combine the benefits of both large language model\n(LLM) generation-based and retrieval-based approaches. For instance, in a\nsupport assistant application, you can use embeddings to retrieve relevant\ncontext related to the customer and then have an LLM generate responses based\non the retrieved context to give the customer a more personalized and useful\nsupport response.\n\n## How to Create Vector Embeddings for Your Data\n\nCreating vector embeddings of your data is also commonly called vectorization.\nHere\u2019s a general overview of the vectorization process:\n\n  * The first step is to collect the raw data that you want to process. This could be text, audio, images, time series data, or any other kind of structured or unstructured data.\n  * Then, you need to preprocess the data to clean it and make it suitable for analysis. Depending on the kind of data you have, this may involve tasks like tokenization (in the case of text data), removing noise, resizing images, normalization, scaling, and other data cleaning operations.\n  * Next, you need to break down the data into chunks. Depending on the type of data you\u2019re dealing with, you might have to split text into sentences or words (if you have text data), divide images into segments or patches (if you have image data), or partition time series into intervals or windows (if you have time series data).\n  * Once you preprocess the data and break it into suitable chunks, the next step is to convert each chunk into a vector representation, a process known as embedding.\n\nWhile different techniques are used to embed different kinds of data, you\nusually call an embedding model via an API to create vector representations:\n\n  * For text data, some popular choices are OpenAI\u2019s text-embedding-3 models, Google Gemini\u2019s textembedding-gecko models, Cohere\u2019s Embed models. Other options include SentenceTransformers, FastText, GloVe, and Word2Vec, which all create vector representations of words and sentences.\n  * For image data, CNNs like VGG and Inception or CLIP from OpenAI can be used to extract feature vectors from images.\n  * For audio, you can use Spectrogram.\n\nIf your data is already in PostgreSQL, you can simply use pgvectorizer, which\nis part of Timescale\u2019s Vector Python library and makes it simple to manage\nembeddings. In addition to creating embeddings from data, pgvectorizer keeps\nthe embedding and relational data in sync as data changes, allowing you to\nleverage hybrid and semantic search within your applications.\n\n### Example: Creating text embeddings using OpenAI\u2019s text-embedding API\n\nBefore you start, you first need to head over to the OpenAI website and create\nyour API key.\n\nThen, to create your own embeddings, make sure you install the OpenAI Python\npackage.\n\n    \n    \n    !pip install openai\n\nNext, import the OpenAI class from the module:\n\n    \n    \n    from openai import OpenAI\n\nNext, create an instance of the OpenAI class (we\u2019ll call it client) and\ninitialize it with your API key to authenticate and authorize access to the\nAPI:\n\n    \n    \n    client = OpenAI(api_key=\"YOUR_API_KEY\")\n\nThen, as defined in the API docs, we use the client.embeddings.create() method\nto generate embeddings for a given text. We also need to specify the model,\nenter the input, and choose the encoding format before sending the request:\n\n    \n    \n    # Generate an embedding for a given input sentence response = client.embeddings.create( model=\"text-embedding-3-small\", input=\"How to create vector embeddings using OpenAI\", encoding_format=\"float\" ) #Extract the embedding vector embedding = response['data'][0]['embedding']\n\nThe result you get is the vector embeddings for the provided text, and looks\nsomething like this:\n\n## How to Store Vector Embeddings\n\nTraditional databases are not enough to handle the complexity of vector data,\nmaking it difficult to analyze it and extract meaningful insights. This is\nwhere vector databases come in\u2014these are specialized databases designed to\nhandle vectors and can efficiently store and retrieve vector embeddings.\n\nA great way to store vector embeddings is using PostgreSQL since it allows you\nto combine vector data with other kinds of relational and time-series data,\nlike business data, metrics, and analytics. While PostgreSQL wasn\u2019t originally\ndesigned for vector search, extensions like pgvector make it possible to store\nvectors in a regular PostgreSQL table in a vector column, along with metadata.\nIt\u2019s a powerful extension for storing and querying vectors and enables\napplications like RAG, NLP, computer vision, semantic search, and image\nsearch.\n\n## Start Creating Vector Embeddings Today\n\nYou now know everything you need to know to get started with creating vector\nembeddings. In this article, we\u2019ve talked about what vector embeddings really\nare, their types, how they work, and how neural networks create embeddings.\n\nYou now also know that applications involving LLMs, semantic search, and GenAI\nall rely on vector embeddings to power them, making it important to learn how\nto create, store, and query vector embeddings.\n\nIf you\u2019re ready to get started, store vector embeddings on Timescale today\nwith Timescale Vector.\n\nSign up today for a free 90-day trial.\n\nIngest and query in milliseconds, even at terabyte scale.\n\nThis post was written by\n\n  * Team Timescale\n\n29 Apr 2024 11 min read\n\nAI\n\nContributors\n\n  * Team Timescale\n\nShare\n\n## Related posts\n\nPostgreSQL\n\n## How to Build LLM Applications With pgvector Vector Store in LangChain\n\n12 Jul 2023 11 min read\n\n\u201cHello World\u201d for pgvector and LangChain! Learn how to build LLM applications\nwith LangChain framework, using PostgreSQL and pgvector as a vector database\nfor embeddings data.\n\nPostgreSQL\n\n## PostgreSQL as a Vector Database: A pgvector Tutorial\n\n21 Jun 2023 18 min read\n\nVector databases add organizational intelligence to AI. Learn how to use\nPostgreSQL as a vector database for Retrieval Augmented Generation.\n\nPython\n\n## Jupyter Notebook Tutorial: Setting Up Python & Jupyter Notebooks on macOS\nfor OpenAI Exploration\n\n5 Jul 2023 2 min read\n\nUnlock the power of Python and OpenAI with our step-by-step Jupyter Notebook\ntutorial.\n\nShare this post\n\nSubscribe to the Timescale Newsletter By submitting you acknowledge\nTimescale's Privacy Policy.\n\n2024 \u00a9 Timescale, Inc. All Rights Reserved.\n\nPrivacy preferences Legal Privacy Sitemap\n\n", "frontpage": false}
