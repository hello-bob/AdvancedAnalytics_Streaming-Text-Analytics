{"aid": "40110349", "title": "A benchmark for emotional intelligence in large language models", "url": "https://github.com/EQ-bench/EQ-Bench", "domain": "github.com/eq-bench", "votes": 1, "user": "andy99", "posted_at": "2024-04-22 00:10:05", "comments": 0, "source_title": "GitHub - EQ-bench/EQ-Bench: A benchmark for emotional intelligence in large language models", "source_text": "GitHub - EQ-bench/EQ-Bench: A benchmark for emotional intelligence in large\nlanguage models\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nEQ-bench / EQ-Bench Public\n\n  * Notifications\n  * Fork 9\n  * Star 118\n\nA benchmark for emotional intelligence in large language models\n\n### License\n\nMIT license\n\n118 stars 9 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# EQ-bench/EQ-Bench\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n6 Branches\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nsqrklfixApr 19, 202428cf0a4 \u00b7 Apr 19, 2024Apr 19, 2024\n\n## History\n\n120 Commits  \n  \n### data\n\n|\n\n### data\n\n| judgemark updates| Apr 4, 2024  \n  \n### images\n\n|\n\n### images\n\n| v2| Jan 16, 2024  \n  \n### instruction-templates\n\n|\n\n### instruction-templates\n\n| judgemark updates| Apr 6, 2024  \n  \n### lib\n\n|\n\n### lib\n\n| fix| Apr 19, 2024  \n  \n### results\n\n|\n\n### results\n\n| judgemark updates| Apr 4, 2024  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| update readme| Mar 20, 2024  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Initial commit| Dec 2, 2023  \n  \n### README.md\n\n|\n\n### README.md\n\n| llama3 support| Apr 19, 2024  \n  \n### config.cfg\n\n|\n\n### config.cfg\n\n| creative-writing| Mar 13, 2024  \n  \n### eq-bench.py\n\n|\n\n### eq-bench.py\n\n| creative writing benchmark| Mar 13, 2024  \n  \n### eq_bench_prompts_v1.txt\n\n|\n\n### eq_bench_prompts_v1.txt\n\n| v2| Jan 16, 2024  \n  \n### eq_bench_prompts_v2.txt\n\n|\n\n### eq_bench_prompts_v2.txt\n\n| v2| Jan 16, 2024  \n  \n### install_reqs.sh\n\n|\n\n### install_reqs.sh\n\n| llama3 support| Apr 19, 2024  \n  \n### ooba_quick_install.sh\n\n|\n\n### ooba_quick_install.sh\n\n| fix quickstart paths| Jan 16, 2024  \n  \n## Repository files navigation\n\n# EQ-Bench\n\nEQ-Bench is a benchmark for language models designed to assess emotional\nintelligence. You can read more about it in our paper.\n\nThe latest leaderboard can be viewed at EQ-Bench Leaderboard.\n\n## News\n\n### 2024-04-19 Minor updates\n\n  * Changed behaviour when using Transformers and no chat template is specified. In this scenario, the benchmark will now apply the tokenizer's chat template if there is one.\n  * Models are now loaded in 16 bit precision if \"none\" quantisation is selected.\n  * Preliminary support for Llama3 models (adding <|eot_id|> to the tokenizer).\n\n### Version 2.3\n\nThis version includes two new benchmarks: creative-writing and judgemark.\n\n#### Creative Writing Benchmark\n\nThis is a LLM-as-a-judge benchmark using detailed criteria to assess the\nmodel's output to a set of creative writing prompts.\n\n#### Judgemark\n\nThis is the latest benchmark task in the EQ-Bench pipeline. It tests a model's\nability to judge creative writing from a set of pre-generated outputs from 20\ntest models. The writing prompts & judging process are the same as those used\nin the creative writing benchmark. Several metrics are aggregated on the\njudge's performance (correlation with other benchmarks + measures of spread),\nresulting in a \"Judgemark\" score.\n\n#### Launching the New Benchmarks\n\nTo launch individual benchmark tasks:\n\npython eq-bench.py --benchmarks eq-bench\n\npython eq-bench.py --benchmarks creative-writing\n\npython eq-bench.py --benchmarks judgemark\n\nThe creative-writing and judgemark tasks require the following parameters to\nbe configures in your config.cfg:\n\njudge_model_api = judge_model = judge_model_api_key =\n\n#### Creative Writing Benchmark Details\n\nOfficial scores for the creative-writing benchmark use claude-3-opus as judge.\nHowever you can use any openai, mistralai or anthropic model as judge. Be\naware that the results won't be directly comparable between judge models.\n\nThe creative writing benchmark involves 19 writing prompts. The model's output\nis then judged according to 36 criteria for good & bad writing, with each\ncriteria being scored 0-10.\n\nGiven the small number of questions in the test set, you may wish to run\nseveral iterations of the benchmark to reduce variance. You can set\nn_iterations in the config per benchmark run. We recommend 3+ iterations.\nBenchmarking a model over 3 iterations using Claude Opus will cost approx.\n$3.00.\n\nTemperature is set at 0.7 for the test model inference, so output will vary\nbetween iterations.\n\nFor more details, click here.\n\n## Requirements\n\n  * Linux\n  * Python 3x\n  * Working install of Oobabooga (optional)\n  * Sufficient GPU / System RAM to load the models\n  * Python libraries listed in install_reqs.sh\n\n## Installation\n\n### Quick start:\n\nIf you are installing EQ-Bench into a fresh linux install (like a runpod or\nsimilar), you can run ooba_quick_install.sh. This will install oobabooga into\nthe current user's home directory, install all EQ-Bench dependencies, then run\nthe benchmark pipeline.\n\n### Install (not using quick start)\n\nNote: Ooobabooga is optional. If you prefer to use transformers as the\ninference engine, or if you are only benchmarking through the OpenAI API, you\ncan skip installing it.\n\n  * Install the required Python dependencies by running install_reqs.sh.\n  * Optional: install the Oobabooga library and make sure it launches.\n  * Optional: Set up firebase / firestore for results upload (see instructions below).\n  * Optional: Set up Google Sheets for results upload (see instructions below).\n\n### Configure\n\n  * Set up config.cfg with your API keys and runtime settings.\n  * Add benchmark runs to config.cfg, in the format:\n\n    * run_id, instruction_template, model_path, lora_path, quantization, n_iterations, inference_engine, ooba_params, downloader_filters\n\n      * run_id: A name to identify the benchmark run\n      * instruction_template: The filename of the instruction template defining the prompt format, minus the .yaml (e.g. Alpaca)\n      * model_path: Huggingface model ID, local path, or OpenAI model name\n      * lora_path (optional): Path to local lora adapter\n      * quantization: Using bitsandbytes package (8bit, 4bit, None)\n      * n_iterations: Number of benchmark iterations (final score will be an average)\n      * inference_engine: Set this to transformers, openai, ooba or llama.cpp.\n      * ooba_params (optional): Any additional ooba params for loading this model (overrides the global setting above)\n      * downloader_filters (optional): Specify --include or --exclude patterns (using same syntax as huggingface-cli download)\n\n## Benchmark run examples\n\n# run_id, instruction_template, model_path, lora_path, quantization,\nn_iterations, inference_engine, ooba_params, downloader_args\n\nmyrun1, openai_api, gpt-4-0613, , , 1, openai, ,\n\nmyrun2, Llama-v2, meta-llama/Llama-2-7b-chat-hf, /path/to/local/lora/adapter,\n8bit, 3, transformers, , ,\n\nmyrun3, Alpaca, ~/my_local_model, , None, 1, ooba, --loader transformers\n--n_ctx 1024 --n-gpu-layers -1,\n\nmyrun4, Mistral, TheBloke/Mistral-7B-Instruct-v0.2-GGUF, , None, 1, ooba,\n--loader llama.cpp --n-gpu-layers -1 --tensor_split 1,3,5,7, --include\n[\"*Q3_K_M.gguf\", \"*.json\"]\n\nmyrun5, Mistral, mistralai/Mistral-7B-Instruct-v0.2, , None, 1, ooba, --loader\ntransformers --gpu-memory 12, --exclude \"*.bin\"\n\nmyrun6, ChatML, model_name, , None, 1, llama.cpp, None,\n\n## Running the benchmark\n\n  * Run the benchmark:\n\n    * python3 eq-bench.py\n  * Results are saved to benchmark_results.csv\n\n## Script Options\n\n  * -h: Displays help.\n  * -w: Overwrites existing results (i.e., disables the default behaviour of resuming a partially completed run).\n  * -d: Downloaded models will be deleted after each benchmark successfully completes. Does not affect previously downloaded models specified with a local path.\n  * -f: Use hftransfer for multithreaded downloading of models (faster but can be unreliable).\n  * -v: Display more verbose output.\n  * -r: Set the number of retries to attempt if a benchmark run fails. Default is 5.\n  * -l: Sets the language: en and de currently supported. Defaults to English if not specified.\n  * -v1: Runs v1 of the benchmark (legacy). If not set, the benchmark defaults to v2.\n  * -revise: Enables the revision component of the test questions (this is off by default since v2.1).\n\n## Prompt Formats / Instruction Templates\n\nEQ-Bench uses the same instruction template format as the Oobabooga library.\nYou can modify the existing ones or add your own. When you specify a prompt\nformat in config.cfg, use the filename minus the .yaml, e.g. Alpaca.\n\n  * If using transformers as the inference engine, the benchmark pipeline uses templates located in [EQ-Bench dir]/instruction-templates.\n  * If using ooba as the inference engine, the pipeline uses templates located in [ooba dir]/instruction-templates\n\nWhen using transformers, if you leave the prompt format blank in config.cfg,\ntransformers will apply the chat template in the tokenizer if there is one.\n\nWhen using ooba, if you leave the prompt format blank in config.cfg, ooba will\nmake its best guess as to what the prompt format should be.\n\n## Setting up Firebase / Firestore for Results Uploading (Optional)\n\n## Setting up Google Sheets for Results Uploading (Optional)\n\n## Cite\n\n    \n    \n    @misc{paech2023eqbench, title={EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models}, author={Samuel J. Paech}, year={2023}, eprint={2312.06281}, archivePrefix={arXiv}, primaryClass={cs.CL} }\n\n## About\n\nA benchmark for emotional intelligence in large language models\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\nActivity\n\nCustom properties\n\n### Stars\n\n118 stars\n\n### Watchers\n\n2 watching\n\n### Forks\n\n9 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Contributors 3\n\n  * sqrkl Sam Paech\n  * CrispStrobe\n  * dnhkng David\n\n## Languages\n\n  * Python 99.3%\n  * Shell 0.7%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
