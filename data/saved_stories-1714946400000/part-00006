{"aid": "40263679", "title": "MemoryDB: Speed, Durability, and Composition", "url": "https://brooker.co.za/blog/2024/04/25/memorydb.html", "domain": "brooker.co.za", "votes": 1, "user": "kiyanwang", "posted_at": "2024-05-05 10:21:53", "comments": 0, "source_title": "MemoryDB: Speed, Durability, and Composition. - Marc's Blog", "source_text": "MemoryDB: Speed, Durability, and Composition. - Marc's Blog\n\n# Marc's Blog\n\n# About Me\n\nMy name is Marc Brooker. I've been writing code, reading code, and living\nvicariously through computers for as long as I can remember. I like to build\nthings that work. I also dabble in machining, welding, cooking and skiing.\n\nI'm currently an engineer at Amazon Web Services (AWS) in Seattle, where I\nwork on databases, serverless, and serverless databases. Before that, I worked\non EC2 and EBS. All opinions are my own.\n\n# Links\n\nMy Publications and Videos @marcbrooker on Mastodon @MarcJBrooker on Twitter\n\n# MemoryDB: Speed, Durability, and Composition.\n\nBlocks are fun.\n\nEarlier this week, my colleagues Yacine Taleb, Kevin McGehee, Nan Yan, Shawn\nWang, Stefan Mueller, and Allen Samuels published Amazon MemoryDB: A fast and\ndurable memory-first cloud database^1. I\u2019m excited about this paper, both\nbecause its a very cool system, and because it gives us an opportunity to talk\nabout the power of composition in distributed systems, and about the power of\ndistributed systems in general.\n\nBut first, what is MemoryDB?\n\n> Amazon MemoryDB for Redis is a durable database with microsecond reads, low\n> single-digit millisecond writes, scalability, and enterprise security.\n> MemoryDB delivers 99.99% availability and near instantaneous recovery\n> without any data loss.\n\nor, from the paper:\n\n> We describe how, using this architecture, we are able to remain fully\n> compatible with Redis, while providing single-digit millisecond write and\n> microsecond-scale read latencies, strong consistency, and high availability.\n\nThis is remarkable: MemoryDB keeps compatibility with an existing in-memory\ndata store, adds multi-AZ (multi-datacenter) durability, adds high\navailability, and adds strong consistency on failover, while still improving\nread performance and with fairly little cost to write performance.\n\nHow does that work? As usual, there\u2019s a lot of important details, but the\nbasic idea is composing the in-memory store (Redis) with our existing fast,\nmulti-AZ transaction journal^2 service (a system we use in many places inside\nAWS).\n\nComposition\n\nWhat\u2019s particularly interesting about this architecture is that the journal\nservice doesn\u2019t only provide durability. Instead, it provides multiple\ndifferent benefits:\n\n  * durability (by synchronously replicating writes onto storage in multiple AZs),\n  * fan-out (by being the replication stream replicas can consume),\n  * leader election (by having strongly-consistent fencing APIs that make it easy to ensure there\u2019s a single leader per shard),\n  * safety during reconfiguration and resharding (using those same fencing APIs), and\n  * the ability to move bulk data tasks like snapshotting off the latency-sensitive leader boxes.\n\nMoving these concerns into the Journal greatly simplifies the job of the\nleader, and minimized the amount that the team needed to modify Redis. In\nturn, this makes keeping up with new Redis (or Valkey) developments much\neasier. From an organizational perspective, it also allows the team that owns\nJournal to really focus on performance, safety, and cost of the journal\nwithout having to worry about the complexities of offering a rich API to\ncustomers. Each investment in performance means better performance for a\nnumber of AWS services, and similarly for cost, and investments in formal\nmethods, and so on. As an engineer, and engineering leader, I\u2019m always on the\nlook out for these leverage opportunities.\n\nOf course, the idea of breaking systems down into pieces separated by\ninterfaces isn\u2019t new. It\u2019s one of the most venerable ideas in computing.\nStill, this is a great reminder of how composition can reduce overall system\ncomplexity. The journal service is a relatively (conceptually) simple system,\npresenting a simple API. But, by carefully designing that API with affordances\nlike fencing (more on that later), it can remove the need to have complex\nthings like consensus implementations inside its clients (see Section 2.2 of\nthe paper for a great discussion of some of this complexity).\n\nAs Andy Jassy says:\n\n> Primitives, done well, rapidly accelerate builders\u2019 ability to innovate.\n\nDistribution\n\nIt\u2019s well known that distributed systems can improve durability (by making\nmultiple copies of data on multiple machines), availability (by allowing\nanother machine to take over if one fails), integrity (by allowing machines\nwith potentially corrupted data to drop out), and scalability (by allowing\nmultiple machines to do work). However, it\u2019s often incorrectly assumed that\nthis value comes at the cost of complexity and performance. This paper is a\ngreat reminder that assumption is not true.\n\nLet\u2019s zoom in on one aspect of performance: consistent latency while taking\nsnapshots. MemoryDB moves snapshotting off the database nodes themselves, and\ninto a separate service dedicated to maintaining snapshots.\n\nThis snapshotting service doesn\u2019t really care about latency (at least not the\nsub-millisecond read latencies that the database nodes worry about). It\u2019s a\nthroughput-optimized operation, where we want to stream tons of data in the\nmost throughput-efficient way possible. By moving it into a different service,\nwe get to avoid having throughput-optimized and latency-optimized processes\nrunning at the same time (with all the cache and scheduling issues that come\nwith that). The system also gets to avoid some implementation complexities of\nsnapshotting in-place. From the paper, talking about the on-box BGSave\nsnapshotting mechanism:\n\n> However, there is a spike on P100 latency reaching up to 67 milliseconds for\n> request response times. This is due to the fork system call which clones the\n> entire memory page table. Based on our internal measurement, this process\n> takes about 12ms per GB of memory.\n\nand things get worse if there\u2019s not enough memory for the copy-on-write (CoW\ncopy of the data):\n\n> Once the instance exhausts all the DRAM capacity and starts to use swap to\n> page out memory pages, the latency increases and the throughput drops\n> significantly. [...] The tail latency increases over a second and throughput\n> drops close to 0...\n\nthe conclusion being that to avoid this effect database nodes need to keep\nextra RAM around (up to double) just to support snapshotting. An expensive\nproposition in an in-memory database! Moving snapshotting off-box avoids this\ncost: memory can be shared between snapshotting tasks, which significantly\nimproves utilization of that memory.\n\nThe upshot is that, in MemoryDB with off-box snapshotting, performance impact\nis entirely avoided. Distributed systems can optimize components for the kind\nof work they do, and can use multi-tenancy to reduce costs.\n\nConclusion\n\nGo check out the MemoryDB team\u2019s paper. There\u2019s a lot of great content in\nthere, including a smart way to ensure consistency between the leader and the\nlog, a description of the formal methods the team used, and operational\nconcerns around version upgrades. This is what real system building looks\nlike.\n\nBonus: Fencing\n\nAbove, I mentioned how fencing in the journal service API is something that\nmakes the service much more powerful, and a better building block for real-\nworld distributed systems. To understand what I mean, let\u2019s consider a journal\nservice (a simple ordered stream service) with the following API:\n\n    \n    \n    write(payload) -> seq read() -> (payload, seq) or none\n\nYou call write, and when the payload has been durably replicated it returns a\ntotally-ordered sequence number for your write. That\u2019s powerful enough, but in\nmost systems would require an additional leader election to ensure that the\nwrites being sent make some logical sense.\n\nWe can extend the API to avoid this case:\n\n    \n    \n    write(payload, last_seq) -> seq read() -> (payload, seq) or none\n\nIn this version, writers can ensure they are up-to-date with all reads before\ndoing a write, and make sure they\u2019re not racing with another writer. That\u2019s\nsufficient to ensure consistency, but isn\u2019t particularly efficient (multiple\nleaders could always be racing), and doesn\u2019t allow a leader to offer\nconsistent operations that don\u2019t call write (like the in-memory reads the\nMemoryDB offers). It also makes pipelining difficult (unless the leader can\nmake an assumption about the density of the sequences). An alternative design\nis to offer a lease service:\n\n    \n    \n    try_take_lease() -> (uuid, deadline) renew_lease(uuid) -> deadline write(payload) -> seq read() -> (payload, seq) or none\n\nA leader who believes they hold the lease (i.e. their current time is\ncomfortably before the deadline) can assume they\u2019re the only leader, and can\ngo back to using the original write API. If they end up taking the lease, they\npoll read until the stream is empty, and then can take over as the single\nleader. This approach offers strong consistency, but only if leaders\nabsolutely obey their contract that they don\u2019t call write unless they hold the\nlease.\n\nThat\u2019s easily said, but harder to do. For example, consider the following\ncode:\n\n    \n    \n    if current_time < deadline: <gc or scheduler pause> write(payload)\n\nThose kinds of pauses are really hard to avoid. They come from GC, from page\nfaults, from swapping, from memory pressure, from scheduling, from background\ntasks, and many many other things. And that\u2019s not even to mention the possible\ncauses of error on local_time. We can avoid this issue with a small adaptation\nto our API:\n\n    \n    \n    try_take_lease() -> (uuid, deadline) renew_lease(uuid) -> deadline write(payload, lease_holder_uuid) -> seq read() -> (payload, seq) or none\n\nIf write can enforce that the writer is the current lease holder, we can avoid\nall of these races while still allowing writers to pipeline things as deeply\nas they like. This still-simple API provides an extremely powerful building\nblock for building systems like MemoryDB.\n\nFinally, we may not need to compose our lease service with the journal\nservice, because we may want to use other leader election mechanisms. We can\navoid that by offering a relatively simple compare-and-set in the journal API:\n\n    \n    \n    set_leader_uuid(new_uuid, old_uuid) -> old_uuid write(payload, leader_uuid) -> seq read() -> (payload, seq) or none\n\nNow we have a super powerful composable primitive that can offer both safety\nto writers, and liveness if the leader election system is reasonably well\nbehaved.\n\nFootnotes\n\n  1. To appear at SIGMOD\u201924.\n  2. The paper calls it a log service, which is technically correct, but a term I tend to avoid because its easily confused with logging in the observability sense.\n\n## Other Posts\n\n\u00ab Back to the blog index\n\n  * 17 Apr 2024 \u00bb Formal Methods: Just Good Engineering Practice?\n  * 25 Mar 2024 \u00bb Finding Needles in a Haystack with Best-of-K\n  * 04 Mar 2024 \u00bb The Builder's Guide to Better Mousetraps\n\nMarc Brooker The opinions on this site are my own. They do not necessarily\nrepresent those of my employer. marcbrooker@gmail.com\n\nRSS Atom\n\nThis work is licensed under a Creative Commons Attribution 4.0 International\nLicense.\n\n", "frontpage": false}
