{"aid": "40144374", "title": "Multi-Level Summarization in Instapaper", "url": "https://bthdonohue.com/2024/04/15/multilevel-summarization-instapaper.html", "domain": "bthdonohue.com", "votes": 1, "user": "jparise", "posted_at": "2024-04-24 13:47:13", "comments": 0, "source_title": "Multi-Level Summarization in Instapaper", "source_text": "Multi-Level Summarization in Instapaper\n\nBrian Donohue\n\nMakin\u2019 that Instapaper\n\n# Multi-Level Summarization in Instapaper\n\nApr 24, 2024\n\nToday Instapaper launched Summaries, which is a feature I\u2019ve wanted to build\nfor a long time. Summaries help readers both understand an article before\nreading it, and help them recall the details of previously read articles.\n\nBuilding summarization features has become a whole lot easier with new tools\nand APIs, and this post will outline the technical details on how Instapaper\ngenerates its summaries.\n\nInstapaper Summaries are generated using a multi-level summarization\ntechnique, where the summary is generated in two steps:\n\n  1. Extractive Summarization: Most relevant sentences extracted from the text.\n  2. Abstractive Summarization: Generate an independent summary from the extracted sentences.\n\nBelow we\u2019ll dive into the technical details on each step and how it comes\ntogether in the product...\n\n## Extractive Summarization\n\nExtractive Summarization is a technique to create a summary by extracting\nportions of the provided text.\n\nFor instance, Instapaper Summaries use TextRank for Sentence Extraction, which\nis based on the premise that the most relevant sentences have the most\nsimilarity to other sentences in the document. The algorithm identifies the\nmost relevant sentences by constructing a graph where every node is a\nsentence, and sentences are connected by similarity based on overlapping\nwords.\n\nInstapaper uses an open source library to build the graph using the TextRank\nalgorithm, grabs the top 15 sentences (or fewer when fewer are returned), and\nthen sorts them in the order they appear in the text. In the application code:\n\n    \n    \n    def key_sentences(text, num_sentences=15): scored_sentences = textrank(text) # Create dictionary with score and index index_scored_sentences = [{'sentence': s[0], 'score': s[1], 'index': i} for i, s in enumerate(scored_sentences)] # Sort by score and grab top N index_scored_sentences.sort(key=lambda d: d['score'], reverse=True) top_scored_sentences = index_scored_sentences[:num_sentences] # Sort again by the original order/index and return only sentences top_scored_sentences.sort(key=lambda d: d['index']) return [d['sentence'] for d in top_scored_sentences]\n\nIn testing I often used Eugene Wei\u2019s Status-as-a-Service blog post, which is\nboth an excellent read, and an extremely long read. It clocks in at almost 20k\nwords, 550 sentences, and would take the average reader over an hour to read.\n\nUsing TextRank we\u2019ll extract the most relevant sentences from Status-as-a-\nService. The table below includes the top 15 most relevant sentences in the\narticle, and the position of each sentence in the article. If you look at each\nsentence\u2019s position you\u2019ll notice that, while not perfectly uniform, the\nhighly ranked sentences are well distributed throughout the article.\n\nSentence| Position in Article  \n---|---  \nA social network like Facebook allows me to reach lots of people I would\notherwise have a harder time tracking down, and that is useful.| 7%  \nSo, to answer an earlier question about how a new social network takes hold,\nlet\u2019s\u2019 add this: a new Status as a Service business must devise some proof of\nwork that depends on some actual skill to differentiate among users.| 19%  \nIf I were an investor or even an employee, I might have something like a\nrepresentative basket of content that I\u2019d post from various test accounts on\ndifferent social media networks just to track social capital interest rates\nand liquidity among the various services.| 23%  \nOtherwise a form of social capital inequality sets in, and in the virtual\nworld, where exit costs are much lower than in the real world, new users can\neasily leave for a new network where their work is more properly rewarded and\nwhere status mobility is higher.| 26%  \nThe same way many social networks track keystone metrics like time to X\nfollowers, they should track the ROI on posts for new users.| 27%  \nMaintenance of existing social capital stores is often a more efficient use of\ntime than fighting to earn more on a new social network given the ease of just\nearning interest on your sizeable status reserves.| 41%  \nThis bottom right quadrant is home to some businesses with over a billion\nusers, but in minimizing social capital and competing purely on utility-\nderived network effects, this tends to be a brutally competitive battleground\nwhere even the slimmest moat is fought for with blood and sweat, especially in\nthe digital world where useful features are trivial to copy.| 49%  \nIn fact, it\u2019s usually the most high status or desirable people who leave\nfirst, the evaporative cooling effect of social networks.| 59%  \nAs with aggregate follower counts and likes, the Best Friends list was a\nmechanism for people to accumulate a very specific form of social capital.|\n66%  \nStill, given the precarious nature of status, and given the existence of\nInstagram which has always been a more unabashed social capital accumulation\nservice, it\u2019s not a bad strategy for Snapchat to push out towards increased\nutility in messaging instead.| 69%  \nOne way to combat this, which the largest social networks tend to do better\nthan others, is add new forms of proof of work which effectively create a new\nreserve of potential social capital for users to chase.| 70%  \nEngagement goals may drive them towards building services that are optimized\nas social capital games, but they themselves are hardly in need of more\nstatus, except of a type they won\u2019t find on their own networks.| 79%  \nIt\u2019s strange to think that social networks like Twitter and Facebook once\nallowed users to just wholesale export their graphs to other networks since it\nallowed competing networks to jumpstart their social capital assets in a\nmassive way, but that only goes to show how even some of the largest social\nnetworks at the time underestimated the massive value of their social capital\nassets.| 88%  \nHe accumulated millions of followers on Instagram in large part by taking\nother people\u2019s jokes from Twitter and other social networks and then posting\nthem as his own on Instagram.| 89%  \nTwitter is another social network where people tend to bring interesting\ncontent in the hopes of amassing more followers and likes.| 92%  \n  \nLeveraging this technique we\u2019ve been able to generate an extractive summary\nthat reduces the article by 98% to 550 words.\n\nIf you\u2019ve read the post previously, I\u2019m sure the above sentences will jog your\nmemory. In addition to being a great tool for initially evaluating an article,\nit\u2019s a great way to recall an article for the key points and easily jump\nbetween them.\n\nExtracted Summary is shown as \u201cSummary Highlights\u201d. Readers can tap each\nhighlight to navigate to the highlight in the article. It also connects to\nInstapaper's other features like highlights and note taking.\n\n## Extractive Summarization Performance\n\nWhen building Instapaper Summaries, performance was a major consideration, and\neach component of the summarization process had performance evaluations around\ntime, cost, and quality.\n\nTextRank Sentence Extraction is a deterministic graph-based ranking algorithm,\nand its performance is directly correlated with the amount of text provided.\nBelow is a table containing a selection of articles, the number of sentences\nin each article, and the time it took TextRank to generate the extractive\nsummary.\n\nArticle| Sentences| Est. Read Time| Time  \n---|---|---|---  \nThe Mystery of White Rural Rage| 33| 5 min| .06s  \nWhy Substack is at a crossroads| 146| 15 min| .4s  \nThe Pain Hustlers| 325| 35 min| 1.5s  \nStatus as a Service| 547| 82 min| 7.3s  \n  \nGraphing a sample of 1,000 articles from Instapaper, we can see the majority\nof articles have less than 400 sentences, and those mostly complete within a\nsecond:\n\nBelow are the mean, median, and P95 for the 1,000 articles in the dataset\nevaluated:\n\nStat| Sentences| Est. Reading Time| Time  \n---|---|---|---  \nMean| 63| 8 min| .25s  \nMedian| 36| 5 min| .05s  \nP95| 234| 26 min| 1.01s  \n  \nWhile the results are effective and create the right product experience, this\ncall is expensive for longer articles.\n\nIn order to prevent server-side performance issues, I rely on a few factors:\n\n  * TextRank Sentence Extraction runs on compute-optimized machines that can withstand longer running tasks (i.e. not our API/web servers).\n  * Sentence extraction for large articles is done asynchronously, and the results are stored to prevent recomputing them.\n  * There is caching in place for all sentence extraction.\n  * Very few articles saved to Instapaper are more than 30 minute reads.\n\n## Abstractive Summarization\n\nI evaluated a few abstract summary models including pegasus-xsum, t5,\nChatGPT-3.5 Turbo, and ChatGPT-4 to generate abstract summaries of the\narticles. At this stage, the model evaluation was done mostly on the quality\nof the response and ignoring other factors like time and cost.\n\nBased on the results from a small sample of articles, it\u2019s clear that ChatGPT\nis the winner here. Given similar quality and less cost, we opted to use\nChatGPT-3.5 Turbo instead of ChatGPT-4. Notably absent from the evaluation\nwere other LLMs like Llama from Meta and Claude from Anthropic. Once I found\nthat ChatGPT generated great summaries, I stopped evaluating additional\nmodels.\n\nLong-term, I would like to bring the abstract summary closer to Instapaper\u2019s\ninfrastructure, especially given we can likely tune a model only focused on\nsummarization that can potentially be smaller and faster than more general\npurpose LLMs.\n\nIn the product, the abstract summary is shown above the summary highlights.\nLong press on the abstract summary provides the reader with some options.\n\nUsing Status-as-a-Service against ChatGPT, there\u2019s an issue with the size of\nthe text provided. Here\u2019s the response from the Completions API:\n\n> This model\u2019s maximum context length is 16385 tokens. However, your messages\n> resulted in 23607 tokens. Please reduce the length of the messages.\n\nThis seems to be a fairly common problem, and I quickly came across How to\nsend long articles for summarization? from the OpenAI community forums. There\nare two solutions recommended in the thread:\n\n  * Sub-Summaries: Chunk the text into smaller parts. Summarize each part with ChatGPT. Combine the summaries, and then provide that to ChatGPT for a final summary.\n  * Refine: Chunk the text into smaller parts. Summarize the first part with ChatGPT. Combine the first summary with the second part, then summarize with ChatGPT. Repeat for all parts.\n\nUsing Status-as-a-Service, we\u2019ll compare the above two approaches with Multi-\nLevel Summarization, which provides the Extractive Summarization sentences to\nChatGPT to generate the abstract summary.\n\n## Abstractive Summarization Performance\n\nApproach| Prompt Tokens| Cost| Time| Abstract Summary  \n---|---|---|---|---  \nSub-Summaries| 23,646| 1.21c| 5.83s| Social networks are like popularity\ncontests where people try to gain more followers and likes to increase their\nstatus. Young people are often more active on social media because they have\nmore free time and less social status than adults. While social media can be\nfun, it can also have negative effects on mental health and spread harmful\ninformation. It\u2019s important for these platforms to find a balance between\nbeing enjoyable and keeping users safe.  \nRefine| 23,718| 1.19c| 4.47s| Social networks like Facebook and Instagram are\nlike games where people try to increase their popularity and status by\naccumulating likes, followers, and other forms of social capital. Young people\noften dominate social media because they have more time and less existing\nsocial capital compared to adults. However, social networks can also have\nnegative effects on mental health and lead to feelings of anxiety and\ninadequacy.  \nMulti-Level Summarization| 643| .063c| 1.62s| Social networks like Facebook\nand Instagram are places where people can connect with others and share things\nlike jokes, photos, and ideas. These networks can be competitive because\npeople want to gain more followers and likes, which can make them feel\nimportant. Sometimes, people might leave one social network for another if\nthey think they can earn more status or attention there.  \n  \nEach approach comes with trade-offs against time, cost, and quality. Comparing\nSub-Summaries with Refine:\n\n  * Time: Sub-Summaries require an additional API call to get the final summarized output, which results in +30% time to completed response.\n  * Cost: The cost difference is negligible (+1.6% for sub-summaries) because they use a similar amount of prompt tokens and output tokens, despite the additional API call.\n  * Quality: The output is very similar in terms of structure and content. That said, the last sentence about social media & mental health is not something actually discussed in the article other than a brief mention of societal impact of social networks.\n\nNext we\u2019ll compare Refine with Multi-Level Summarization:\n\n  * Time: Multi-Level Summarization requires a single API call to ChatGPT, which is 63% faster than Refine. However, it requires the upfront time to generate the sentences. Combining the total time for Multi-Level Summarization (8.92s), it\u2019s 50% slower when the sentences also need to be extracted.\n  * Cost: Given Multi-Level Summarization uses a sample of highly ranked sentences to generate the summary, it uses significantly fewer prompt tokens (98% less), and as a result is 47% cheaper. There is some negligible compute cost to extract the sentences that are not included.\n  * Quality: Despite much less text input, ChatGPT seems to return a more on-topic and appropriate summary for multi-level summarization. The last sentence in particular more accurately captures the point of the article versus the other two approaches.\n\nWith longer articles, multi-level summaries can be conducted in less time and\ncheaper than alternative solutions. Additionally, there\u2019s evidence to suggest\nthat ChatGPT performs better when being provided with fewer, more highly\nrelevant sentences versus the entire text.\n\nFor completeness, we also evaluated ChatGPT against shorter articles which fit\nwithin one API call. We found time difference negligible between approaches,\ncost for multi-level approach was 65% cheaper, and quality to be comparable.\nSee below for abstract summaries for Why the Internet Isn\u2019t Fun Anymore (50\nsentences, 5 min read):\n\nApproach| Prompt Tokens| Cost| Time| Abstract Summary  \n---|---|---|---|---  \nEntire Text| 1,682| .087c| 1.77s| The author discusses how social media\nplatforms like Twitter, Instagram, and TikTok have changed over time. They\nexplain that these platforms have become less fun, more hierarchical, and\nfocused on self-promotion. The decline in these platforms has also affected\nthe spread of news and content from outside sources.  \nMulti-Level Summarization| 515| .031c| 1.57s| Recently, the social media\nplatform X (formerly known as Twitter) has been filled with posts that aren\u2019t\nvery meaningful and keep repeating the same few topics. Users who pay to be\nverified by Elon Musk dominate the platform and often share right-wing\nopinions and false information. Social media used to be a place for\nconversation and connecting with others, but now it feels more like a\nbroadcast station where one person talks to millions of followers.\nAdditionally, people are spending less time on social media and less likely to\nclick on links shared there.  \n  \nBased on the above, I chose Multi-Level Summarization as the best approach for\nall Instapaper summaries given the fewer API calls, lower cost, and\noptimizations made for sentence extraction in large articles.\n\n## Abstract Summary Internationalization\n\nPeople from all over the world use Instapaper to read, and I wanted to ensure\nsome basic support for internationalization in summaries. For Extractive\nSummarization, the summaries are generated using text directly in the article,\nand I did some testing to ensure that languages like Chinese and Japanese,\nwhich do not use a Latin-style period, worked reasonably well.\n\nFor the Abstract Summary powered by ChatGPT, I found that when prompted in\nEnglish with non-English text, the response would be in English. I attempted\nsome prompt engineering to ensure the responses were in the same language of\nthe text and added \u201cRespond in the language of the text.\u201d to our prompt. I\nfound that while this mostly worked, ChatGPT would occasionally revert to\nEnglish when responding.\n\nIn order to ensure ChatGPT responds in the correct language, I integrated\nlanguage detection to determine the language on the extracted sentences:\n\n    \n    \n    def language_code_for_text(text): lang_probabilities = langdetect.detect_langs(text) if not lang_probabilities: return None lang_probability = lang_probabilities[0] return lang_probability.lang if lang_probability.prob >= 0.8 else None\n\nIn testing, I found a probability over 80% was a pretty reliable indicator of\nsuccessful language detection, with most results returning over 95%.\nPerformance-wise langdetect runs pretty reliably in 20-30ms regardless of the\nlanguage or size of text.\n\nIf language detection fails, Instapaper falls back on the prompt engineering\nsolution:\n\n    \n    \n    language = nlp.language_for_text(prompt) or 'the language of the text' system_prompt = 'Respond in %s.' % language\n\n## In Summary...\n\nFor fun, I took a draft of this article, saved it to Instapaper, and generated\na summary:\n\n> Instapaper has developed a way to generate summaries of articles using a\n> two-step process. First, they create a summary by taking important parts of\n> the text. Then, they use a tool called ChatGPT to generate a more general\n> summary. This method is faster and cheaper than other methods they tried,\n> and it works well for articles in different languages.\n\nThis was my first deeper foray into natural language processing, generative\nAI, and building features based on these new tools. A big reason I\u2019ve been\nexcited about Going Full-Time on Instapaper is exploring how to improve\nInstapaper with machine learning.\n\nSummaries are a small baby step in that direction. It was a lot of fun\nexploring the different techniques for summarizing text, understanding the\npower of the new generative AI tools\u2013and its limits, and I\u2019m ultimately really\nhappy with how the feature came out. If you have a chance to try Instapaper\nSummaries, I\u2019d love to hear your feedback on Twitter\n\n^1.\n\nThanks to Jon Parise, Linus Lee, and Ying Yang for providing feedback on\ndrafts of this post. Also special thanks to Linus Lee who provided feedback on\nthe technical approach throughout the implementation\n\n  1. Still calling it Twitter. \u21a9\n\n\u25c6\n\nHome \u00b7 About \u00b7 Contact\n\n", "frontpage": false}
