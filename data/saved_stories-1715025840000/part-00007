{"aid": "40272409", "title": "Building RAG with Open-Source and Custom AI Models", "url": "https://www.bentoml.com/blog/building-rag-with-open-source-and-custom-ai-models", "domain": "bentoml.com", "votes": 4, "user": "sherlockxu", "posted_at": "2024-05-06 08:21:13", "comments": 0, "source_title": "Building RAG with Open-Source and Custom AI Models", "source_text": "Building RAG with Open-Source and Custom AI Models\n\n#### Building RAG with Open-Source and Custom AI Models\n\nMay 6, 2024 \u2022 Written By Chaoyu Yang\n\nRetrieval-Augmented Generation (RAG) is a widely used application pattern for\nLarge Language Models (LLMs). It uses information retrieval systems to give\nLLMs extra context, which aids in answering user queries not covered in the\nLLM's training data and helps to prevent hallucinations. In this blog post, we\ndraw from our experience working with BentoML customers to discuss:\n\n  * Common challenges in making a RAG system ready for production\n  * How to use open-source or custom fine-tuned models to enhance RAG performance\n  * How to build scalable AI systems comprising multiple models and components\n\nBy the end of this post, you'll learn the basics of how open-source and custom\nAI/ML models can be applied in building and improving RAG applications.\n\n> Note: This blog post is based on the video below, with additional details.\n\n#### Simple RAG system\n\nA simple RAG system consists of 5 stages:\n\n  * Chunking: RAG begins with turning your structured or unstructured dataset into text documents, and breaking down text into small pieces (chunks).\n  * Embed documents: A text embedding model steps in, turning each chunk into vectors representing their semantic meaning.\n  * VectorDB: These embeddings are then stored in a vector database, serving as the foundation for data retrieval.\n  * Retrieval: Upon receiving a user query, the vector database helps retrieve chunks relevant to the user's request.\n  * Response Generation: With context, an LLM synthesizes these pieces to generate a coherent and informative response.\n\nImplementing a simple RAG system with a text embedding model and an LLM might\ninitially only need a few lines of Python code. However, dealing with real-\nworld datasets and improving performance for the system require more than\nthat.\n\n#### Challenges in production RAG\n\nBuilding a RAG for production is no easy feat. Here are some of the common\nchallenges:\n\n#### Retrieval performance\n\n  * Recall: Not all chunks that are relevant to the user query are retrieved.\n  * Precision: Not all chunks retrieved are relevant to the user query.\n  * Data ingestion: Complex documents, semi-structured and unstructured data.\n\n#### Response synthesis\n\n  * Safeguarding: Is the user query toxic or offensive and how to handle it.\n  * Tool use: Use tools such as browsers or search engines to assist the response generation.\n  * Context accuracy: Retrieved chunks lacking necessary context or containing misaligned context.\n\n#### Response evaluation\n\n  * Synthetic dataset for evaluation: LLMs can be used to create evaluation datasets for measuring the RAG system\u2019s responses.\n  * LLMs as evaluators: LLMs also serve as evaluators themselves.\n\n#### Improving RAG pipeline with custom AI models\n\nTo build a robust RAG system, you need to take into account a set of building\nblocks or baseline components. These elements or decisions form the foundation\nupon which your RAG system's performance is built.\n\n#### Text embedding model\n\nCommon models like text-embedding-ada-002, while popular, may not be the best\nperformers across all languages and domains. Their one-size-fits-all approach\noften falls short when you have nuanced requirements for specialized fields.\n\nSource: Hugging Face Massive Text Embedding Benchmark (MTEB) Leaderboard\n\nOn this note, fine-tuning an embedding model on a domain-specific dataset\noften enhances the retrieval accuracy. This is due to the improvement of\nembedding representations for the specific context during the fine-tuning\nprocess. For instance, while a general embedding model might associate the\nword \"Bento\" closely with \"Food\" or \"Japan\", a model fine-tuned for AI\ninference would more likely connect it with terms like \"Model Serving\", \"Open\nSource Framework\", and \"AI Inference Platform\".\n\n#### Large language model\n\nWhile GPT-4 leads the pack in performance, not all applications require such\nfirepower. Sometimes, a more modest and well-optimized model can deliver the\nspeed and cost-effectiveness needed, especially when provided with the right\ncontext. In particular, consider the following questions when choosing the LLM\nfor your RAG:\n\n  * Security and privacy: What level of control do you need over your data?\n  * Latency requirement: What is your TTFT (Time to first token) and TPOT (Time per output token) requirement? Is it serving real-time chat applications or offline data processing jobs?\n  * Reliability: For mission-critical applications, dedicated deployment that you control, often provides more reliable response time and generation quality.\n  * Capabilities: What tasks do you need your LLM to perform? For simple tasks, can it be replaced by a smaller specialized models?\n  * Domain knowledge: Does an LLM trained on general web content understand your specific domain knowledge?\n\nThese questions are important no matter you are self-hosting open-source\nmodels or using commercial model endpoints. The right model should align with\nyour data policies, budget plan, and the specific demands of your RAG\napplication.\n\n#### Context-aware chunking\n\nMost simple RAG systems rely on fixed-size chunking, dividing documents into\nequal segments with some overlap to ensure continuity. This method, while\nstraightforward, can sometimes strip away the rich context embedded in the\ndata.\n\nBy contrast, context-aware chunking breaks down text data into more meaningful\npieces, considering the actual content and its structure. Instead of splitting\ntext at fixed intervals (like word count), it identifies logical breaks in the\ntext using NLP techniques. These breaks can occur at the end of sentences,\nparagraphs, or when topics shift. This ensures each chunk captures a complete\nthought or idea, and makes it possible to add additional metadata to each\nchunk, for implementing metadata filtering or Small-to-Big retrieval.\n\nAs your RAG system can understand the overall flow and ideas within a document\nwith context-aware chunking, it is capable of creating chunks that capture not\njust isolated sentences but also the broader context they belong to.\n\n#### Parsing complex documents\n\nThe real world throws complex documents at us - product reviews, emails,\nrecipes, and websites that not only contain textual content but are also\nenriched with structure, images, charts, and tables.\n\nTraditional Optical Character Recognition (OCR) tools such as EasyOCR and\nTesseract are proficient in transcribing text but often fall short when it\ncomes to understanding the layout and contextual significance of the elements\nwithin a document.\n\nFor those grappling with the complexities of modern documents, consider\nintegrating the following models and tools into your RAG systems:\n\n  * Layout analysis: LayoutLM (and v2, v3) have been pivotal in advancing document layout analysis. LayoutLMv3, in particular, integrates text and layout with image processing without relying on conventional CNNs, streamlining the architecture and leveraging masked language and image modeling, making it highly effective in understanding both text-centric and image-centric tasks.\n  * Table detection and extraction: Table Transformer (TATR) is specifically designed for detecting, extracting, and recognizing the structure of tables within documents. It operates similarly to object detection models, using a DETR-like architecture to achieve high precision in both table detection and functional analysis of table contents.\n  * Document question-answering systems: Building a Document Visual Question Answering (DocVQA) system often requires multiple models, such as models for layout analysis, OCR, entity extraction, and finally, models trained to answer queries based on the document's content and structure. Tools like Donut and the latest versions of LayoutLMv3 can be helpful in developing robust DocVQA systems.\n  * Fine-tuning: Existing open-source models are great places to start but with additional fine-tuning on your specific documents, handling its unique content or structure, can often lead to greater performance.\n\n#### Metadata filtering\n\nIncorporating these models into your RAG systems, especially when combined\nwith NLP techniques, allows for the extraction of rich metadata from\ndocuments. This includes elements like the sentiment expressed in text, the\nstructure or summarization of a document, or the data encapsulated in a table.\nMost modern vector databases supports storing metadata alongside text\nembeddings, as well as using metadata filtering during retrieval, which can\nsignificantly enhance the retrieval accuracy.\n\n#### Reranking models\n\nWhile embedding models are a powerful tool for initial retrieval in RAG\nsystems, they can sometimes return a large number of documents that might be\ngenerally relevant, but not necessarily the most precise answers to a user's\nquery. This is where reranking models come into play.\n\nImage source: Rerankers and Two-Stage Retrieval\n\nReranking models introduce a two-step retrieval process that significantly\nimproves precision:\n\n  1. Initial retrieval: An embedding model acts as a first filter, scanning the entire database and identifying a pool of potentially relevant documents. This initial retrieval is fast and efficient.\n  2. Reranking: The reranking model then takes over, examining the shortlisted documents from the first stage. It analyzes each document's content in more detail, considering its specific relevance to the user's query. Based on this analysis, the reranking model reorders the documents, placing the most relevant ones at the top (sometimes at both ends of the context window for maximum relevance).\n\nWhile reranking provides superior precision, it adds an extra step to the\nretrieval process. Many may think this can increase latency. However,\nreranking also means you don\u2019t need to send all retrieved chunks to the LLM,\nleading to faster generation time.\n\nFor more information, see this article Rerankers and Two-Stage Retrieval.\n\n#### Cross-modal retrieval\n\nWhile traditional RAG systems primarily focus on text data, research like\nImageBind: One Embedding Space To Bind Them All is opening doors to a more\nversatile approach: Cross-modal retrieval.\n\nImage source: ImageBind: One Embedding Space To Bind Them All\n\nCross-modal retrieval transcends traditional text-based limitations,\nsupporting interplay between different types of data, such as audio and visual\ncontent. For example, when a RAG system incorporates models like BLIP for\nvisual reasoning, it\u2019s able to understand the context within images, improving\nthe textual data pipeline with visual insights.\n\nWhile still in its early stages, multi-modal retrieval holds great potential\nwhat RAG systems can achieve.\n\n#### Recap: AI models in RAG systems\n\nAs we improve our RAG system for production, the complexity increases\naccordingly. Ultimately, we may find ourselves orchestrating a group of AI\nmodels, each playing its part in the workflow of data processing and response\ngeneration.\n\nAs we address these complexities, we also need to pay attention to the\ninfrastructure for deploying AI models. In the next part of this blog post,\nwe\u2019ll explore these infrastructure challenges and introduce how BentoML is\ncontributing to this space.\n\n#### Scaling RAG services with multiple custom AI models\n\n#### Serving embedding models\n\nOne of the most frequent challenges is efficiently serving the embedding\nmodel. BentoML can help improve its performance in the following ways:\n\n  * Asynchronous non-blocking invocation: BentoML allows you to convert synchronous inference methods of a model to asynchronous calls, providing non-blocking implementation and improving performance in IO-bound operations.\n  * Shared model replica across multiple API workers: BentoML supports running shared model replicas across multiple API workers, each assigned with a specific GPU. This can maximize parallel processing, increase throughput, and reduce overall inference time.\n  * Adaptive batching: Within a BentoML Service, there is a dispatcher that manages how batches should be optimized by dynamically adjusting batch sizes and wait time to suit the current load. This mechanism is called adaptive batching in BentoML. In the context of text embedding models, we often see performance improvements up to 3x in latency and 2x in throughput comparing to non-batching implementations.\n\nFor more information, see this BentoML example project to deploy an embedding\nmodel.\n\n#### Self-hosting LLMs\n\nMany developers may start with pulling a model from Hugging Face and run it\nwith frameworks like PyTorch or Transformers. This is fine for development and\nexploration, but performs poorly when serving high throughput workloads in\nproduction.\n\nThere are a variety of open-source tools like vLLM, OpenLLM, mlc-llm, and\nTensorRT-LLM available for self-hosting LLMs. Consider the following when\nchoosing such tools:\n\n  * Inference best practices: Does the tool support optimized LLM inference? Techniques like continuous batching, Paged Attention, Flash Attention, and automatic prefix caching need to be implemented for efficient performance.\n  * Customizations: LLM behavior needs customized control like advanced stop conditioning (when a model should cease generating further content), specific output formats (ensuring the results adhere to a specific structure or standard), or input validation (using a classification model to detect). If you need such customization, consider using BentoML + vLLM.\n\nIn addition to the LLM inference server, the infrastructure required for\nscaling LLM workloads also comes with unique challenges. For example:\n\n  * GPU Scaling: Unlike traditional workloads, GPU utilization metrics can be deceptive for LLMs. Even if the metrics suggest full capacity, there might still be room for more requests and more throughput. This is why solutions like BentoCloud offers concurrency-based autoscaling. Such an approach learns the semantic meanings of different requests, using dynamic batching and wise resource management strategies to scale effectively.\n\n  * Cold start and fast scaling with large container image and model files: Downloading large images and models from remote storage and loading models into GPU memory is a time-consuming process, breaking most existing cloud infrastructure\u2019s assumptions about the workload. Specialized infrastructure, like BentoCloud, helps accelerate this process via lazy image pulling, streaming model loading and in-cluster caching.\n\nFor details, refer to Scaling AI Model Deployment.\n\n#### Model composition\n\nModel composition is a strategy that combines multiple models to solve a\ncomplex problem that cannot be easily addressed by a single model. Before we\ntalk about how BentoML can help you compose multiple models for RAG, let\u2019s\ntake a look at other two typical scenarios used in RAG systems.\n\n#### Document processing pipeline\n\nA document processing pipeline consists of multiple AI/ML models, each\nspecializing in a stage of the data conversion process. In addition to OCR\nthat extract text from images, it can extend to layout analysis, table\nextraction and image understanding.\n\nThe models used in this process might have different resource requirements,\nsome requiring GPUs for model inference and others, more lightweight, running\nefficiently on CPUs. Such a setup naturally fits into a distributed system of\nmicro-services, each service serving a different AI model or function. This\narchitectural choice can drastically improve resource utilization and reduce\ncost.\n\nBentoML facilitates this process by allowing users to easily implement a\ndistributed inference graph, where each stage can be a separate BentoML\nService wrapping the capability of the corresponding model. In production,\nthey can be deployed and scaled separately (more details can be found below).\n\n#### Using small language models\n\nIn some cases, \"small\" models can be an ideal choice for their efficiency,\nparticularly for simpler, more direct tasks like summarization,\nclassification, and translation. Here's how and why they fit into a multi-\nmodel system:\n\n  * Rapid response: For example, when a user query is submitted, a small model like BERT can swiftly determine if the request is inappropriate or toxic. If so, it can reject the query directly, conserving resources by avoiding the follow-up steps.\n  * Routing: These nimble models can act as request routers. Fine-tuned BERT-like model needs no more than 10 milliseconds to identify which tools or data sources are needed for a given request. By contrast, an LLM may need a few seconds or more to complete.\n\n#### Uniting separate RAG components\n\nRunning a RAG system with a large number of custom AI models on a single GPU\nis highly inefficient, if not impossible. Although each model could be\ndeployed and hosted separately, this approach makes it challenging to iterate\nand enhance the system as a whole.\n\nBentoML is optimized for building such serving systems, streamlining both the\nworkflow from development to deployment and the serving architecture itself.\nDevelopers can encapsulate the entire RAG logic within a single Python\napplication, referencing each component (like OCR, reranker, text embedding,\nand large language models) as a straightforward Python function call. The\nframework eliminates the need to build and manage distributed services,\noptimizing resource efficiency and scalability for each component. BentoML\nalso manages the entire pipeline, packaging the necessary code and models into\na single versioned unit (a \"Bento\"). This consistency across different\napplication lifecycle stages drastically simplifies the deployment and\nevaluation process.\n\n> Note: In the next series of blog posts, we will dive into more details on\n> how developers can leverage BentoML for model composition and serving RAG\n> systems at scale. Stay tuned!\n\nTo summarize, here is how BentoML can help you build RAG systems:\n\n  * Define the entire RAG components within one Python file\n  * Compile them to one versioned unit for evaluation and deployment\n  * Adopt baked-in model serving and inference best practices like adaptive batching\n  * Assign each model inference components to different GPU shapes and scale them independently for maximum resource efficiency\n  * Monitor production performance in BentoCloud, which provides comprehensive observability like tracing and logging\n\nFor more information, refer to our RAG tutorials.\n\n#### Conclusion\n\nModern RAG systems often requires a large number of open-source and custom\nfine-tuned AI models for achieving the optimal performance. As we improve RAG\nsystems with all these additional AI models, the complexity grows quickly,\nwhich not only slows down your development iterations, but also comes with a\nhigh cost in deploying and maintaining such a system in production.\n\nBentoML is designed for building and serving compound AI systems with multiple\nmodels and components easily. It comes in handy in the orchestration of\ncomplex RAG systems, ensuring seamless scaling in the cloud.\n\n#### More on BentoML\n\nTo learn more about BentoML, check out the following resources:\n\n  * [Blog] Introducing BentoML 1.2\n  * [Blog] Deploying Solar with BentoML\n  * [Blog] Scaling AI Models Like You Mean It\n  * If you are interested in our AI inference platform BentoCloud, sign up now and get $10 in free credits!\n  * Join the BentoML Slack community to get help and contact us to schedule a call with our expert!\n\n## Freedom To Build\n\nJoin our global Community\n\nBillions of predictions per day 3000+ community members Use by 1000+\norganizations\n\nStart a free trial\n\nGet in touch\n\nSubscribe our newsletter\n\n### Products\n\nOpen Source\n\nBentoCloud\n\n### Resources\n\nTutorial\n\nDocumentation\n\nBlog\n\nAnnouncements\n\nExample Projects\n\n### Company\n\nNews\n\nContact Us\n\nCareers\n\nPrivacy Policy\n\n### Join our community\n\nSlack\n\nTwitter\n\nLinkedIn\n\nYouTube\n\nGitHub\n\n", "frontpage": false}
