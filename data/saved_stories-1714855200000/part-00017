{"aid": "40257745", "title": "Simple Complex Programming", "url": "https://habr.com/en/articles/812253/", "domain": "habr.com", "votes": 1, "user": "gulced", "posted_at": "2024-05-04 14:00:45", "comments": 0, "source_title": "Simple complex programming", "source_text": "Simple complex programming / Habr\n\n\u03b2How to become an author\n\nrsashka 8 hours ago\n\n# Simple complex programming\n\nMedium\n\n5 min\n\n124\n\nProgramming*System Analysis and Design*C++*Unreal Engine*\n\nOpinion\n\nI always pay attention to assessing the complexity of programming in a\nparticular language. Programming is indeed not an easy task and this is\nperceived as a fact and usually does not require any confirmation.\n\nBut the concept of \u201ccomplexity\u201d is akin to the term \u201cheap\u201d. For some, five\ncoconuts is not so much, but for someone who ate one and \u201cdidn\u2019t want any\nmore,\u201d this means that even one coconut will be too much for him.\n\nThe same goes for the complexity of programs. It seems that the constant\nincrease in the complexity of programs is obvious to everyone and is observed\nin all areas of application of IT technologies, and programming languages\nthemselves become more and more complex as they develop, but assessing\n\u201ccomplexity\u201d using numerical metrics is a problem. obviously a thankless task,\nbut also \u201cYou can\u2019t manage what you can\u2019t measure...\u201d\n\nTypically, talk of \u201ccomplexity\u201d only implies value judgments without any\nnumerical evaluation. And since I am personally interested in the issue of the\ncomplexity of programming languages, I decided to calculate the complexity of\nimplementing the gcc compiler on some conditional \u201cparrots\u201d. What if we could\nsee some patterns of difficulty changing over time?\n\n### Choosing \"parrots\" to measure\n\nI didn\u2019t come up with my own and calculate empirical program code metrics and,\nas a \u201cparrot,\u201d I decided to take the simplest metric SLOC (Source Lines of\nCode) \u2014 the number of lines of source code compiler, which is very easy to\ncalculate.\n\nBut it will be possible to evaluate the complexity of a language with its help\nonly under the following assumption: the complexity of the language should be\ndirectly dependent on the complexity of its implementation, if simple\nsyntactic structures require less code than more complex ones.\n\nOf course, using the \u201cnumber of lines of source code\u201d metric has its\ndrawbacks, since it strongly depends on the programming language used, the\nstyle of source code and, in general, does not allow correct comparison of\nseveral different projects.\n\nBut for numerically assessing the complexity of code within one project, the\nSLOC metric is well suited.\n\n### Methodology for calculating SLOC\n\nInitially I tried to use a simple bash script with mask search and counting\nthe number of lines in source files via wc -l. But after a while it became\nclear that I had to invent another bicycle.\n\nSo I decided to take a ready-made utility SLOCCount, which can analyze almost\nthree dozen types of sources.\n\nMoreover, it counts not just the number of lines of source code, but can\nignore comments, exclude duplicate files from the calculation (compares their\nhash sums), and also displays the estimated labor intensity, an approximate\nestimate of the cost of developing the analyzed project file and other\ncharacteristics.\n\nI was initially interested in the volume of source codes in C/C++ and,\npossibly, in assembler, if there were a lot of such files. But after I started\nworking, I was very glad that I did not reinvent the wheel, but took a ready-\nmade toolkit, because it separately calculates the statistics of the\nYacc/Bison parser source files (.y), which determines the actual complexity of\nthe parser (read the complexity of the programming language syntax).\n\nI took the old gcc sources from https://gcc.gnu.org/mirrors.html, but before\nrunning the analyzer I deleted the directories of other compilers (ada,\nfortran, java, etc.) so that they would not be included in the final\nstatistics.\n\n### Results in \"parrots\"\n\n#### Yacc/Bison parser code size\n\n#### Size of entire GCC source code (C and C++ languages only)\n\nUnfortunately, the Yacc/Bison parser was used only up to version 3, and after\nthat its use was reduced to nothing. That's why we can estimate the complexity\nof C/C++ syntax with the help of the parser's code volume only till about\n1996-98, after which it was gradually phased out, i.e. a little less than ten\nyears. But even during this period the volume of the parser's code base grew\ntwo times, which approximately corresponds to the time of implementing the C99\nstandard.\n\nBut even if we don't take into account the code of the parser, the volume of\nthe total code base also correlates with the introduction of new C++\nstandards: C99, C11 and C14.\n\nThe graph doesn't show a pronounced peak for C+17 and the next versions, but I\nsuppose that with the current size of the code base (more than 4 million lines\nof C and C++ code only), several thousand lines needed to support syntactic\nconstructions of new standards are simply unnoticeable.\n\n### The first conclusion is obvious. Increasing complexity of development\ntools\n\nIn fact, on the example of the GCC project we can see the constant and\ninevitable growth of complexity of programmers' working tools.\n\nAnd no matter how much they talk about degradation of development, about\nsystem crisis of software, which is generational in nature, but it seems to me\nthat the matter is a bit different.\n\nPersonnel renewal and as a consequence \u2014 the necessity to train new employees\nin old developments, here it is not so much about knowledge transfer as about\nthe possibility to absorb this knowledge.\n\nAnd the ability to assimilate knowledge for different generations will be\ndifferent, but not because the previous generation was smarter, and not\nbecause the new generation does not have enough sense to understand it. It's\njust that the environment itself is changing and the working tools are more\ncomplicated than those used by the previous generation.\n\n### The second conclusion is the entry threshold\n\nImagine that you need to \u201cmake your own website\u201d. Naturally, you need to\ndetermine which CMS to use for this.\n\nAnd if there are simple solutions for simple sites, then for those who are not\nlooking for easy ways, there is the CMS Drupal, which is notable for the fact\nthat it has a fantastically high entry threshold for starting to use it.\n\nWhat I'm talking about? When using any tool, such as a programming language,\nthere is a certain minimum level of comfort level.\n\nMoreover, this level is directly proportional to the size of the target\naudience for which it is intended. More precisely, the size of the possible\naudience is determined, among other things, by the requirements for the level\nof starting knowledge and qualifications of the potential user.\n\n### The final conclusion is disappointing\n\nIf we consider the increase in complexity of the software itself, then this is\none thing. Here's an example:\n\n  * September 17, 1991: Linux version 0.01 (10,239 lines of code).\n  * March 14, 1994: Linux version 1.0.0 (176,250 lines of code).\n  * March 1995: Linux version 1.2.0 (310,950 lines of code).\n  * June 9, 1996: Linux version 2.0.0 (777,956 lines of code).\n  * January 25, 1999: Linux version 2.2.0, initially rather unfinished (1,800,847 lines of code).\n  * January 4, 2001: Linux version 2.4.0 (3,377,902 lines of code).\n  * December 18, 2003: Linux version 2.6.0 (5,929,913 lines of code).\n  * March 23, 2009: Linux version 2.6.29, temporary Linux symbol \u2014 Tasmanian devil Tuz (11,010,647 lines of code).\n  * July 22, 2011: Linux 3.0 released (14.6 million lines of code).\n  * October 24, 2011: Linux 3.1 release.\n  * January 15, 2012: Linux 3.3 release surpasses 15 million lines of code.\n  * February 23, 2015: First release candidate of Linux 4.0 (more than 19 million lines of code).\n  * January 7, 2019: First release candidate of Linux 5.0 (more than 26 million lines of code) ...\n\nAnd what if the complexity of software is superimposed on the tendency of\nconstant complication of the working tools themselves? After all, the constant\ndevelopment of programming languages inevitably raises the entry threshold for\nall beginners and only exacerbates the problem of software development\ncomplexity.\n\nIn other words, no matter how well documented the code is and how completely\nit is covered with tests, after some time the tools used become obsolete, the\nlife cycles of external dependencies are completed, and most importantly, new\npeople come to replace those who have developed or managed to understand the\nsystem.\n\nAnd new people have a need to understand the system from the beginning, but\nunder different initial conditions. And because of this, the complexity of\nlearning the system for all new people will be higher simply by the fact that\nthe external conditions have changed and the working tools that new employees\nhave to use have become more complex.\n\nIt is clear that the further you go, the easier it will not be. After all, the\nIT field is the most competitive environment. And how not to remember Lewis\nCarroll, that his winged expression\n\n> You need to run as fast just to stay in place, but to get somewhere, you\n> must run at least twice as fast!\n\nThis applies not only to Alice in Wonderland, but to all information\ntechnology in general!\n\nTags:\n\n  * \u0441\u043b\u043e\u0436\u043d\u043e\u0441\u0442\u044c\n  * \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u043a\u0430\n\nHubs:\n\n  * Programming\n  * System Analysis and Design\n  * C++\n  * Unreal Engine\n\nIf this publication inspired you and you want to support the author, do not\nhesitate to click on the button\n\n0\n\n0\n\n### Editorial Digest\n\nWe email you the best articles monthly\n\n128\n\nKarma\n\n58.5\n\nRating\n\n\u0410\u043b\u0435\u043a\u0441\u0430\u043d\u0434\u0440 \u0420\u044f\u0431\u0438\u043a\u043e\u0432 @rsashka\n\n\u0421\u0438\u0441\u0442\u0435\u043c\u043d\u044b\u0439 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u043e\u0440\n\nWebsiteWebsite\n\nLeave a comment\n\n## Articles\n\nShow the best of all time\n\nVacancies\n\n  * \u0418\u043d\u0436\u0435\u043d\u0435\u0440-\u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0441\u0442 \u0410\u0421\u0423 \u0422\u041f\n\nfrom 75,000 to 111,000 \u20bd\u041f\u0410\u041e \u00ab\u0413\u0430\u0437\u043f\u0440\u043e\u043c \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044f\u00bb\u041a\u0430\u043b\u0438\u043d\u0438\u043d\u0433\u0440\u0430\u0434\n\n  * \u0420\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0447\u0438\u043a 1\u0421\n\nfrom 190,000 \u20bdCRAFTERRemote job\n\n  * \u0418\u043d\u0436\u0435\u043d\u0435\u0440-\u043f\u0440\u043e\u0435\u043a\u0442\u0438\u0440\u043e\u0432\u0449\u0438\u043a \u0410\u0421\u0423\u0422\u041f\n\nfrom 110,000 \u20bd\u0413\u041a \u00ab\u0422\u0443\u0440\u0431\u0443\u043b\u0435\u043d\u0442\u043d\u043e\u0441\u0442\u044c-\u0414\u041e\u041d\u00bb\u0420\u043e\u0441\u0442\u043e\u0432-\u043d\u0430-\u0414\u043e\u043d\u0443Remote job\n\n  * Fullstack PHP Developer\n\nfrom 40,000 to 65,000 \u20bdSmapse EducationRemote job\n\n  * \u041f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u0438\u0441\u0442 1\u0421\n\nfrom 300,000 \u20bd\u0412\u0438\u043a\u0442\u043e\u0440\u0438\u044f\u041c\u043e\u0441\u043a\u0432\u0430\n\nMore vacancies on Habr Career\n\nSupport\n\n\u00a9 2006\u20132024, Habr\n\n", "frontpage": false}
