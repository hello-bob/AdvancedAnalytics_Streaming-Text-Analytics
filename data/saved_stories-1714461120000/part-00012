{"aid": "40202980", "title": "Why Brain-Like Computers Are Hard", "url": "https://www.asianometry.com/p/why-brain-like-computers-are-hard", "domain": "asianometry.com", "votes": 2, "user": "hocaoglv", "posted_at": "2024-04-29 19:34:03", "comments": 0, "source_title": "Why Brain-like Computers Are Hard", "source_text": "Why Brain-like Computers Are Hard - by Jon Y\n\n# The Asianometry Newsletter\n\nShare this post\n\n#### Why Brain-like Computers Are Hard\n\nwww.asianometry.com\n\n#### Discover more from The Asianometry Newsletter\n\nNewsletter for the Asianometry channel. Studies on Asia - Financials,\nsemiconductors, history, demography, development and other stuff. A letter\nevery week Thursday, 1 am Taipei time\n\nOver 30,000 subscribers\n\nContinue reading\n\nSign in\n\n# Why Brain-like Computers Are Hard\n\nJon Y\n\nApr 29, 2024\n\n6\n\nShare this post\n\n#### Why Brain-like Computers Are Hard\n\nwww.asianometry.com\n\nShare\n\nComputers that run the Von Neumann architecture store their programs and data\nin the same memory bank.\n\nSince both have to travel the same road to get and from the CPU, we find that\nthe system is ultimately limited not by the CPU or GPU's computational limits\nbut by said road.\n\nThis is the famous Von Neumann bottleneck.\n\nIn a previous video I talked about in-memory computing as a way to bring a\ncomputer\u2019s memory closer to the compute.\n\nBut making a computer that thinks like the brain - a neuromorphic system as it\nis called - entails far more than just memory.\n\nFor this video, a look at these brain-inspired systems and the fundamental\ndifferences between computer and brain.\n\n##\n\nWhy the Brain\n\nFirst things, first. The brain.\n\nComputer scientists have long had the desire to replicate the brain. But why?\nWhat is so special about the brain? Aren't computers just better?\n\nComputer scientists have long admired the brain's ability to function at very\nlow energy. The brain operates at about 12 to 20 watts of power - 20% of the\nbody's metabolic rate.\n\nThe desktop computer on the other hand does about 175 watts. Leading edge AI\naccelerators like the Nvidia H100 use anything from 300 to 700 watts.\n\nThe brain also does not operate at a very fast pace - with a clock frequency\nof about 10 hertz. Though this varies depending on what the person is doing at\nthe time and their mental state.\n\nSo the brain does not use a lot of power and doesn't operate very quickly. And\nyet it is capable of so much.\n\nImagine a bee. A bee's brain has less than a million neurons and runs on less\nthan a watt of power. And yet it can fly. It can navigate to flowers and back\nhome. It can socialize and maybe even calculate things.\n\nA bee's brain is just as capable as a 18 billion transistor system on chip,\nand it can do all that with just a million neurons and virtually no power.\n\nPerhaps we should start using brains and not silicon chips for learning? Oh\nwait ...\n\nThe biological brain's powerful capabilities - achieved at low power - is the\nsingle most significant motivation for building neuromorphic hardware today.\n\n##\n\nInsane Parallelism\n\nA brain accomplishes these with parallelism.\n\nYour brain's 86 billion neurons operate without a central clock. By which I\nmean that they fire their signals - referred to as \"spikes\" - on their own\ntime, based only on the spikes they receive from their neighbor neurons.\n\nMeasured in floating-point operations per second, the brain is an exaflop-\nlevel compute device on par with the most advanced supercomputers.\n\nAn Nvidia H100 by comparison can only do something like 60 teraflops -\ndepending on the variant.\n\nThe brain\u2019s lack of synchronization is in contrast to digital circuits like a\nCPU. A CPU relies on signals from a central clock as a reference by which to\ncoordinate their calculations. It lets them crunch certain tasks very quickly,\nbut synchronicity has costs.\n\nA central clock system costs time and energy to distribute the clock signals\nacross the system. And there is waste as each system component does not\nexecute its task until it is told to do so as per the central clock signal.\n\n##\n\nChaos as a Feature\n\nNo coordination implies that neural activity is chaotic, to say the least.\n\nNo doubt about that. But brains make that chaos a feature not a bug. The\nneural environment is full of noise - spikes firing from neuron to neuron but\ngoing nowhere.\n\nWhen a neuron receives a spike from a synapse, the majority of the time it\ndoes nothing - noise. But there are so many extensive connections - with so\nmany synapses - that enough spikes get through to the right neurons to carry\non.\n\nAgain, in huge contrast to the digital computer, which works hard to make sure\nthat every signal matters. A modern 3.2 gigahertz Intel CPU sends a \"noise\"\nsignal once every 24 hours.\n\nBut as always, there are tradeoffs. You use a lot more power to achieve this\nvery low signal-to-noise ratio. Imagine the work you need to put in to\nsynchronize billions of transistors.\n\nThat is how the brain achieves its efficiency. Neurons literally throw things\nat the wall and are not afraid to make mistakes. In doing so, they find\nsomething that works. I hope that made sense.\n\nAnd by living amidst chaos, brains also become shockingly resilient and\nflexible - even in situations of massive damage. It is not hard to see the\nvalue of this for computer hardware systems.\n\n##\n\nNeuron\n\nPeople who want to copy the brain often start with its fundamental unit - the\nneuron.\n\nNeurons are cells in the brain and the rest of the nervous system. When they\nreceive spikes from their neighbors, they can - if they so choose - send their\nown spikes on to their neighbors.\n\nAll neurons share several common features - dendrites, soma, and the axon.\nSpikes enter the neuron's cell body - formally called the soma - through\ndendrites.\n\nDendrites are the neuron's input pathways. A typical neuron in the outer layer\nof the cerebrum - the largest part of the brain - has 10,000 inputs.\n\nA typical neuron in the cerebellum - the second largest part of the brain, but\ntrying hard to get to number one - has up to a quarter of a million inputs.\n\nIf sufficiently stimulated, the neuron sends a spike to its neighbors. Or a\nseries of spikes - a neural code, though how the code works remains somewhat\nof a mystery.\n\nThis spike is sent through an out-path called an axon. The axon's primary goal\nis to ensure the signal is forwarded faithfully - though it does not always do\nthis.\n\nAnd then from there, it enters the neighbor neurons' dendrites through what we\ncall \"synapses\" - an electrochemical structure for connecting two neurons.\n\nSuch a description understates their role in computation. Variations in\nsynaptic structure over long time periods - a thing referred to as \"synaptic\nplasticity\" can subtly change the neuron spikes and how they are received. We\nshould not ignore them.\n\n##\n\nDiversity\n\nSo those are the common structures, but the brain's 86 billion neurons exhibit\nincredible diversity. As you might expect with 86 billion of anything.\n\nSome neurons - due to the complicated chemistries of their axons - can send\nsignals faster than the Taiwan High Speed Rail. Other neurons, slower than a\nTaiwan sea turtle.\n\nSome motor neurons can stretch up to a meter long. The ones in a giraffe can\nbe multiple meters long! Other neurons on the other hand might just be 0.1\nmillimeters long.\n\nAnd neurons - just like us - have their own preferred stimuli. In 1981, David\nHuber and Torsten Wiesel won half of the Nobel Prize for Medicine for showing\nthat some neurons fired most rapidly when shown lines going in one direction\nover others.\n\nAs I mentioned above, the majority of the time a neuron receives a spike from\na neighbor neuron, things get left on read.\n\nSo if we are to sum it up enough to get us to the next section of the video,\nthe brain works by propagating huge, random waves of spikes throughout its\nbillions of diverse neurons. Many of these spikes end up as wasted noise, but\nmany are relevant too.\n\nNeurons and Synapses merge together the work of computation and memory - doing\na bit of both. Memories are stored in the relative strength of the synapses\nbetween neurons. But those synapses can do calculations as well.\n\nThat is why the brain does not suffer the Von Neumann bottleneck. That's their\nsecret, Cap. They aren't separate.\n\n##\n\nGoing Neuromorphic\n\nSo in order to create a proper neuromorphic computer,\n\nwe not only have to implement artificial versions of neurons and synapses -\nBut also the way they very tightly bind together the memory, compute, and\ncommunications between the two.\n\nNow, you might be thinking, \"What about artificial neural networks like those\nrunning in ChatGPT? Can we call those neuromorphic?\"\n\nIt is true that these neural networks started from our understanding of how\nthe human brain works. So many of the concepts overlap.\n\nPerceptrons for instance are a simplified mathematical model of the meat\nneuron. It approximates the neuron's behavior by taking in a weighted sum of\ninputs, applying an \"activation function\" to mimic the neuron's stimulation,\nand fires off an output to its neighbors.\n\nBut virtually all of these artificial neural networks - especially the ones\nrunning out there in the real world - run on Von Neumann hardware - which\nmeans dealing with the bottleneck.\n\nChanges can be made to the hardware in order to improve performance and power\nconsumption, of course. That is why we GPU and TPU. But the tantalizing\npossibility remains of getting game-changing benefits by running this neural\n\"software\" on actual neural \"hardware\".\n\nThe neural software - often referred to as \"spiking neural networks\" to\ndifferentiate from Von Neumann-style ANNs and Deep Neural Networks - shall be\ndiscussed some other day. Let's talk hardware.\n\n##\n\nSilicon Neurons\n\nMany industrial and academic players have demoed neuromorphic hardware created\nwith traditional CMOS transistors.\n\nSo normal semiconductor manufacturing processes like that for Von Neumann\ncomputers. Let me note a few of these \"silicon neurons\". Many are - like\ntoday's artificial neural networks - programmatic approximations of the\nneuron's behavior.\n\nWe have IBM's TrueNorth project from 2014 - the first widely distributed\nneuromorphic chip. It is capable of running inference - recognizing that a\nperson is doing something in a video or controlling a robot.\n\nTrueNorth is a special CMOS integrated circuit with 4,096 cores - each with\n256 programmable neurons. The whole chip has 256 million programmable\nsynapses.\n\nA big point that IBM made was how it uses far less power than most computer\nsystems - the chip's 5.4 billion transistors consume about 70 milliwatts.\n\nOther semiconductor companies have shipped silicon neurons of their own too.\nIntel Labs has their Loihi and Loihi 2 neuromorphic AI hardware.\n\nThe European Human Brain Project was a massive ten year science project\ninitiated in 2013, looking to do groundbreaking work in the neural sciences.\n\nOne of their projects was the BrainScaleS project, which was first released in\n2011. A second version was released in 2020 with improved local learning\ncapabilities.\n\nIt is a mixed-signal ASIC chip that uses analog electronic circuits to mimic\nthe spiking neurons of the brain. Very interesting though its analog neurons\nare not as flexible as its digital counterparts.\n\nAnd then in 2019 we have the Tianjic project, a hybrid chip created by\nscientists at China's elite Tsinghua University. It is a hybrid platform that\nattempts to run various types of neural networks - including those designed\nfor neuromorphic hardware.\n\n##\n\nCrossbar\n\nHow do we implement a neuron network in hardware? Let us briefly look at how\nIBM's TrueNorth does it.\n\nTrueNorth is fabbed on a 28 nanometer CMOS process. As I mentioned, it has\n4,096 of what they call neurosynaptic cores. It is capable of running widely\nadopted convolutional neural networks.\n\nEach basic TrueNorth core has 256 axons, 256 neurons, and synapses fully\nconnecting them in a 256x256 crossbar structure, implemented using SRAM.\n\nEach of the axons are given a synaptic weight depending on the neuron that\nthey are connected to. And each neuron has a state or \"membrane potential\" as\nwell as a particular threshold for sending on a spike.\n\nThe system works on cycles. During the cycle, spikes travel to the neurons\nthrough axons, which affects the spike's value. The neuron collects the\nincoming spikes into buffers and then evaluates them.\n\nThey then update their own membrane potentials accordingly. Once having done\nthat, they compare it against their threshold. If the membrane potential meets\nor beats the threshold, the neuron sends a spike of its own. Spikes can be\nsent to local neighbors or outside the core itself.\n\nJust like the real brain, TrueNorth does not have a global clock. The elements\nin each of the cores work asynchronously in cycles, doing things only in\nresponse to events. They also operate at low frequency - 10 hertz.\n\nAnd to implement the neuron's inherent randomness and noise, each core has a\nrandom number generator that can raise the thresholds for creating a spike. Or\nrandomly delay or even halt a spike's transmission through a synapse.\n\nIt's like that episode of It's Always Sunny in Philadelphia. Wildcard!\n\n##\n\nCMOS Shortcomings\n\nA significant benefit of building these \"silicon neuron\" chips using CMOS\nprocesses is that we get to draft in the wake of the chip giants.\n\nThey share some of the benefits of the brain. TrueNorth for instance looks to\nbe very scalable, sips relatively lower power compared to GPUs, and can run\nwidely used neural network software to get accurate results.\n\nBut there are drawbacks. A test of TrueNorth's performance found tradeoffs\nbetween energy efficiency and accuracy. If we want the model to give us\ncompetitively accurate results - which can make a big difference in a\ncommercial context - we need to use more power.\n\nThese circuits are also quite large, since each of the cores have to have\ntheir own memories. Von Neumann machines benefit from having a single, very\ndense central memory bank. Having denser, higher capacity allows us to\naccommodate the larger models that are so in vogue.\n\nAnd lastly, the brain is an analog device - digital devices are an ill fit for\nreplicating their behavior. So we need to incorporate the analog element,\nwhich limits the system's flexibility.\n\nThese disadvantages have pushed scientists to look beyond CMOS for new ways to\nimplement neuromorphic devices. The most popular such approach is the\nmemristor.\n\n##\n\nMemristors\n\nIn 1971, Leon Chua - then a professor of Electrical Engineering and Computer\nSciences at UC Berkeley - published an article proposing a new type of\ncircuit.\n\nBy looking at the relations between the three major circuit elements, he\nproposed the existence of a fourth - the \"memory resistor\" or memristor.\n\nThis paper was very difficult to read and I will freely admit that I did not\nget it. So I am not going to explain any more than that. Anyway, The idea fell\non the wayside until 2008, when scientists at Hewlett-Packard announced a\nphysical implementation of the memristor.\n\nThe original 2008 memristor was a simple metal-dielectric-metal sandwich with\ntwo terminals, or points of connection. In this case, the metal electrodes\nwere made from platinum and the dielectric was titanium oxide.\n\nIf we are to apply a voltage pulse to the memristor, the film can switch its\nelectrical resistance - flipping between an insulating and conducting state in\na non-linear fashion.\n\nWhat is it about the memristor that makes it so suitable for neuromorphic\ncomputing? Their key characteristic is that the value of that electrical\nresistance is dependent upon the history of the voltage passing through it -\nergo the name \"memory\".\n\nEven better, it can remember that history even when the power goes off. This\nmakes the memristor a form of \"non-volatile memory\" like flash memory or a\nhard drive.\n\nPeople quickly drew connections between memristor behavior and that of\nbiological synapses. Scientists envisioned them as a non-volatile memory for\nin-memory computing. For the past 15 years, such devices have been at the\ncenter of neuromorphic study.\n\nBut there are a few challenges. The first has to do with manufacturability. It\ncan be very difficult to produce enough of these memristors uniformly and at\nscale. Can they handle many cycles of resistance switches? And how long can\nthey store their data? Engineers are still working through these questions.\n\nAlso, the changes in the memristor's resistance are non-linear, which makes it\nsomewhat challenging to program for. New software paradigms in neural network\nprogramming may be needed for memristor-based neuromorphic computing to work.\nThe work goes on.\n\n##\n\nConclusion\n\nI think the biggest challenge with these neuromorphic systems though is the\ncompetition.\n\nNvidia and other adherents of \"Huang's Law\" are leading the way. According to\nHuang's Law, silicon chips powering AI more than double in power every two\nyears.\n\nIn the past 10 years, AI inference performance in the GPU has improved 1,000\ntimes. It might be hard to compete against this. But maybe we don\u2019t have to.\nConsidering each system's advantages, we might start seeing more hybrid\nsystems.\n\nPerhaps we can put neuromorphic and Von Neumann chiplets together so to give\nus the best of both worlds. Like a mullet. Business in the front. Party in the\nback.\n\n### Subscribe to The Asianometry Newsletter\n\nBy Jon Y \u00b7 Launched 4 years ago\n\nNewsletter for the Asianometry channel. Studies on Asia - Financials,\nsemiconductors, history, demography, development and other stuff. A letter\nevery week Thursday, 1 am Taipei time\n\n6 Likes\n\n6\n\nShare this post\n\n#### Why Brain-like Computers Are Hard\n\nwww.asianometry.com\n\nShare\n\nTSMC's CHIPS Act Money\n\nAt the end of my video on TSMC\u2019s experiences in Japan, I wrote: As for TSMC\nArizona, that fab will eventually make chips. Local news just reported that...\n\nApr 8 \u2022\n\nJon Y\n\n60\n\nShare this post\n\n#### TSMC's CHIPS Act Money\n\nwww.asianometry.com\n\nLooking Back at 2023\n\nAn Asianometry Recap\n\nDec 27, 2023 \u2022\n\nJon Y\n\n77\n\nShare this post\n\n#### Looking Back at 2023\n\nwww.asianometry.com\n\nIs the AI Boom Real?\n\nFor the past three weeks I have been traveling through Japan and the United\nStates. This trip has been part sight-seeing and part learning tour with\nthe...\n\nMar 7 \u2022\n\nJon Y\n\n46\n\nShare this post\n\n#### Is the AI Boom Real?\n\nwww.asianometry.com\n\nReady for more?\n\n\u00a9 2024 Jon Y\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n", "frontpage": false}
