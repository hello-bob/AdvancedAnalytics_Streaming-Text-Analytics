{"aid": "40165774", "title": "Developing Multicore Property-Based Tests for OCaml 5", "url": "https://tarides.com/blog/2024-04-24-under-the-hood-developing-multicore-property-based-tests-for-ocaml-5", "domain": "tarides.com", "votes": 1, "user": "andrewstetsenko", "posted_at": "2024-04-26 03:43:28", "comments": 0, "source_title": "Under the Hood: Developing Multicore Property-Based Tests for OCaml 5", "source_text": "Under the Hood: Developing Multicore Property-Based Tests for OCaml 5 | Tarides\n\n# Under the Hood: Developing Multicore Property-Based Tests for OCaml 5\n\nby Jan Midtgaard on Apr 24th, 2024\n\nIn 2022, Multicore OCaml became reality. Programming on multiple threads\nbrings new possibilities, but also new complexities. In order to foster\nconfidence in OCaml 5 and retain OCaml's reputation as a trustworthy and\nmemory-safe platform, Tarides has developed multicoretests: Two property-based\ntesting libraries with a test suite built on top. This effort has successfully\npinpointed a range of issues and contributed towards a stable multicore\nenvironment for the OCaml community to build on.\n\nIn this article and in the upcoming part two, I describe how we developed\nproperty-based tests for OCaml 5, the challenges we encountered, and the\nlessons we learned. Part one will focus mainly on the two open-source testing\nlibraries STM and Lin, including some of our findings along the way. It may be\nof interest to both compiler hackers and library writers who are curious about\nhow their code behaves under parallel usage.\n\n## Unit Testing vs. Property-Based Testing\n\nIn traditional unit testing, a developer asserts an expected result for a\ngiven input on a case-by-case basis. For example, when calling OCaml's floor :\nfloat -> float function with argument 0.5 we expect the result to be 0.:\n\n    \n    \n    assert (Float.equal 0. (floor 0.5));;\n\nIn general, one can imagine a range of test cases on this form:\n\n    \n    \n    assert (Float.equal 0. (floor 0.5));; assert (Float.equal 0. (floor 0.9999999));; assert (Float.equal 1. (floor 1.0000001));; assert (Float.equal 10. (floor 10.999999));; ...\n\nRather than manually writing several of these tests, property-based testing\n(PBT) (QuickCheck) advocates for expressing a general property which should\nhold true across all inputs. For example, for any input f given to the floor\nfunction, we expect the result to be less or equal to f: floor f <= f to\ncapture that the function is rounding down. Based on this presumption, we can\ntest this property on any f provided by a generator conveniently named float,\nhere phrased as a QCheck test:\n\n    \n    \n    let floor_test = Test.make float (fun f -> floor f <= f);;\n\nSuch a parameterised property-based test allows us to check the property for\neach input generated. For the above example, this corresponds to a collection\nof test cases beyond what developers like to write by hand:\n\n    \n    \n    assert (floor 1313543.66397378966 <= 1313543.66397378966);; assert (floor (-24763.5086878848342) <= -24763.5086878848342);; assert (floor 1280.58075149504566 <= 1280.58075149504566);; assert (floor (-0.00526932453845931851) <= -0.00526932453845931851);; assert (floor (-35729.1783938070657) <= -35729.1783938070657);; assert (floor (-152180.150007840159) <= -152180.150007840159);; assert (floor 0.000198774450118538313 <= 0.000198774450118538313);; ...\n\nBy default, QCheck's Test.make runs 100 such test cases, but by passing an\noptional ~count parameter, we can raise the test count to our liking with\nminimal effort. In QCheck, the input for each such test case is randomised and\nproduced with the help of a pseudo-random number generator (PRNG). By passing\nthe same seed to the PRNG, we are thus able to trigger the same test case runs\nand reproduce any issue we may encounter.\n\nTesting is still incomplete since, as captured in the immortal words of Edsger\nDijkstra:\n\n> \u201cProgram testing can be used to show the presence of bugs, but never to show\n> their absence!\u201d -- Edsger W. Dijkstra, Notes On Structured Programming, 1970\n\nProperty-based testing does not change that incompleteness. However, because\nthe amount of test cases is less tied to developer effort, PBT tends to be\n'less incomplete' than handwritten test cases and can thus reveal the presence\nof otherwise undetected bugs. Its effectiveness, however, depends on the\nstrength of the tested properties and the distribution of test inputs from the\ngenerator. For example, the property floor f <= f alone does not fully capture\nfloor's correct behaviour. Similarly, we may want to adjust the generator's\ndistribution to exercise floor on corner cases such as nan or floating point\nnumbers ending in .0 or .5.\n\n## Property-Based Testing With a State-Machine Model\n\nAbove, we saw an example of using randomised input to test a property of one\nfunction, floor, in isolation. Often, software defects only appear when\ncombining a particular sequence of function calls. A property-based test\nagainst a state-machine model allows us to test behaviour across a random\ncombination of function calls. For each call, we perform it twice: once over\nthe system under test and once over a purely functional reference model, and\nfinally compare the two results as illustrated in the below figure. This idea\ngrew out of the Clean and Erlang QuickCheck communities and has since been\nported to numerous other programming languages.\n\nSuppose we want to test a selection of the Float.Array interface across random\ncombinations using OCaml's qcheck-stm test library. To do so, we first express\na type of symbolic commands, cmd, along with a function show_cmd to render\nthem as strings:\n\n    \n    \n    type cmd = | To_list | Sort | Set of int * float let show_cmd cmd = match cmd with | To_list -> \"To_list\" | Sort -> \"Sort\" | Set (x, y) -> Printf.sprintf \"Set (%i, %F)\" x y\n\nWe will furthermore need to express the type of our 'System Under Test' (SUT),\nhow to initialise it, and how to clean up after it:\n\n    \n    \n    type sut = Float.Array.t let floatarray_size = 12 let init_sut () = Float.Array.make floatarray_size 1.0 let cleanup _ = ()\n\nThis will test a float array of size 12, initialised to contain 1.0 entries.\nFor the cleanup we will just let OCaml's garbage collector reclaim the array\nfor us.\n\nWe can now phrase an interpreter over the symbolic commands. We annotate each\nresult with combinators and wrap each result up in a Res constructor for later\ncomparison. For example, since Float.Array.to_list returns a float list it is\nannotated with the combinator list float mimicking its return type:\n\n    \n    \n    let run f fa = match f with | To_list -> Res (list float, Float.Array.to_list fa) | Sort -> Res (unit, Float.Array.sort Float.compare fa) | Set (i,f) -> Res (result unit exn, protect (Float.Array.set fa i) f)\n\nSince Float.Array.set may raise an out of bounds exception, we wrap its\ninvocation with protect which will turn the result into an OCaml Result type,\nand suitably annotate it with result unit exn to reflect that it may either\ncomplete normally or raise an exception.\n\nNow, what should we compare the Float.Array operations to? We can express a\npure model, capturing its intended meaning. The state of a float array can be\nexpressed as a simple float list. We then explain to STM how to initialise\nthis model with init_state and how each of our 3 commands change the state of\nthe model using a second interpreter:\n\n    \n    \n    type state = float list let init_state = List.init floatarray_size (fun _ -> 1.0) let next_state f s = match f with | To_list -> s | Sort -> List.sort Float.compare s | Set (i,f) -> List.mapi (fun j f' -> if i=j then f else f') s\n\nOut of the three commands, only To_list does not change the underlying array\nand hence returns our model s unmodified. The Sort case utilises List.sort to\nsort the model accordingly. Finally the Set case expresses how the list model\nis updated on the i-th entry, to reflect the array assignment of a new entry\nf.\n\nWith a model in place, we can then express as pre- and post-conditions what we\ndeem acceptable behaviour. As none of the functions have pre-conditions we\nleave precond as constantly true:\n\n    \n    \n    let precond _cmd _s = true let postcond f (s:float list) res = match f, res with | To_list, Res ((List Float,_),fs) -> List.equal Float.equal fs s | Sort, Res ((Unit,_),r) -> r = () | Set (i,_), Res ((Result (Unit,Exn),_), r) -> if i < 0 || i >= List.length s then r = Error (Invalid_argument \"index out of bounds\") else r = Ok () | _, _ -> false\n\nIn the To_list case we use List.equal to compare the actual list result to the\nmodel. Since Sort returns a unit and is executed for its side effect, there is\nnot much to verify about the result. Finally in the Set case we verify that\ncmd fails as expected when receiving invalid array indices.\n\nAs a final piece of the puzzle we write a function arb_cmd to generate\narbitrary commands using QCheck's combinators:\n\n    \n    \n    let arb_cmd s = let int_gen = Gen.(frequency [ (1,small_nat); (7,int_bound (List.length s - 1)); ]) in let float_gen = Gen.float in QCheck.make ~print:show_cmd Gen.(oneof [ return To_list; return Sort; map2 (fun i f -> Set (i,f)) int_gen float_gen; ])\n\nThe function accepts a state parameter to enable model-dependent cmd\ngeneration. Here we use it to generate an array index guaranteed to be within\nbounds in 7/8 of the cases. In the other cases we fall back on QCheck's\nsmall_nat generator to check the out-of-bounds indexing behaviour of\nFloat.Array.set. Overall, we choose uniformly between generating either a\nTo_list, a Sort, or a Set cmd.\n\nAssuming we surround the above code in a suitable OCaml module FAConf, we can\npass it to the functor STM_sequential.Make to create a runnable sequential STM\ntest:\n\n    \n    \n    module FAConf = struct [...] end module FA_STM_seq = STM_sequential.Make(FAConf) let () = QCheck_base_runner.run_tests_main [FA_STM_seq.agree_test ~count:1000 ~name:\"Sequential STM Float Array test\"]\n\nThis test quickly checks 1000 cmd lists for agreement with our model:\n\n    \n    \n    random seed: 271125846 generated error fail pass / total time test name [\u2713] 1000 0 0 1000 / 1000 0.5s Sequential STM Float Array test ================================================================================ success (ran 1 tests)\n\nThis hasn't always been so \u2013 on all platforms at least. When testing OCaml 5's\nnewly restored PowerPC backend, we first started to observe crashes on array-\nrelated tests such as the above. This was fixed by Tarides compiler engineer\nMiod Vallat by changing the PowerPC compiler backend to avoid using signals\nfor array-bounds checks. However that fix alone wasn't enough to get the STM\nfloat array test passing. In particular, it found wrong float values\nappearing, causing a disagreement between the SUT and the model on the 64-bit\nPowerPC platform.\n\nFor example, here is the output of our example model from one such failing run\nunder PowerPC:\n\n    \n    \n    random seed: 421297093 generated error fail pass / total time test name [\u2717] 27 0 1 26 / 1000 0.0s STM Float Array test sequential --- Failure -------------------------------------------------------------------- Test STM Float Array test sequential failed (0 shrink steps): To_list Sort Set (1, 9.02935000701) Set (0, 118.517154099) Set (8, -0.33441184552) To_list Set (5, -0.000114416837276) Sort To_list +++ Messages ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ Messages for test STM Float Array test sequential: Results incompatible with model To_list : [1.; 1.; 1.; 1.; 1.; 1.; 1.; 1.; 1.; 1.; 1.; 1.] Sort : () Set (1, 9.02935000701) : Ok (()) Set (0, 118.517154099) : Ok (()) Set (8, -0.33441184552) : Ok (()) To_list : [118.517154099; 9.02935000701; 1.; 1.; 1.; 1.; 1.; 1.; -0.33441184552; 1.; 1.; 1.] Set (5, -0.000114416837276) : Ok (()) Sort : () To_list : [-0.33441184552; -0.000114416837276; 1.; 1.; 1.; 1.; 1.; 1.; 1.; 118.517154099; 1.; 9.02935000701] ================================================================================ failure (1 tests failed, 0 tests errored, ran 1 tests)\n\nHere the counterexample consists of 9 cmds first printed without the returned\nresults and then with the observed result. After setting 4 entries (index 1,\n0, 8, and 5) arbitrarily the result of the final to_list appears unsorted with\nthe 118.517154099 entry strangely out of place! A similar observation from our\nlarger model-based STM test prompted us to create and share a small stand-\nalone reproducer illustrating the misbehaviour. With that at hand, OCaml's own\nXavier Leroy then quickly identified and fixed the bug, which was caused by\nPowerPC's FPR0 floating point register not being properly saved and restored\nacross function calls. Both these fixes went into ocaml/ocaml#12540 and will\nbe included in the forthcoming OCaml 5.2 release, restoring the 64-bit POWER\nbackend.\n\n## Testing Parallel Behaviour Against a Sequential STM Model\n\nSince Multicore OCaml programs can be non-deterministic \u2013 meaning that they\ncan behave in not just one way but in a number of different, equally\nacceptable, ways \u2013 it is harder to capture acceptable behaviour in a test.\nFurthermore, errors may go unnoticed or be hard to reproduce, which further\ncomplicate their testing and debugging.\n\nFortunately a sequential model can also function as an oracle for the observed\nbehaviour of the SUT under parallel usage. This idea originates from the paper\n\"Finding Race Conditions in Erlang with QuickCheck and PULSE\" by Claessen et\nal., from ICFP 2009. Rather than generate a sequential list of arbitrary cmds,\none can generate two such cmd lists to be executed in parallel. If we add a\n\"sequential prefix\" to bring the SUT to an arbitrary state before spawn-ing\ntwo parallel Domains, the result is an upside-down Y-shaped test.\n\nWithout changing anything, from the model above we can create a parallel STM\ntest by passing our specification module FAConf to the functor\nSTM_domain.Make:\n\n    \n    \n    module FA_STM_dom = STM_domain.Make(FAConf) let () = QCheck_base_runner.run_tests_main [FA_STM_dom.agree_test_par ~count:1000 ~name:\"Parallel STM Float Array test\"]\n\nThis will test that each observed parallel behaviour can be explained by some\nsequential interleaved cmd run of the model. As such, the property performs an\ninterleaving search. To counter that each random Y-shaped cmd input may yield\na non-deterministic answer, STM repeats each property 25 times and fails if\njust one of the runs cannot be explained by an interleaved model run.\n\nRunning the test quickly finds a counterexample, illustrating that float\narrays are not safe to use in parallel:\n\n    \n    \n    random seed: 224773045 generated error fail pass / total time test name [\u2717] 1 0 1 0 / 1000 1.0s Parallel STM Float Array test --- Failure -------------------------------------------------------------------- Test Parallel STM Float Array test failed (7 shrink steps): | Set (8, -327818.639845) | .------------------------. | | To_list Sort +++ Messages ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ Messages for test Parallel STM Float Array test: Results incompatible with linearized model | Set (8, -327818.639845) : Ok (()) | .-----------------------------------------------------------. | | To_list : [1.; 1.; 1.; 1.; 1.; 1.; 1.; 1.; 1.; 1.; 1.; 1.] Sort : () ================================================================================ failure (1 tests failed, 0 tests errored, ran 1 tests)\n\nThe produced counterexample shows that from a float array with all\n1.0-entries, if one sets entry 8 to, e.g., -327818.639845 and then proceeds to\nsample the array contents with a To_list call in parallel with executing a\nmutating call to Sort, we may experience unexpected behaviour: The To_list\nresult indicates no -327818.639845 entry! This illustrates and confirms that\nOCaml arrays and Float.Array in particular are not safe to use in parallel\nwithout coordinated access, e.g., with a Mutex.\n\n## Lowering Model-Requirements With Lin\n\nOne may rightfully point out that developing an STM model takes some effort.\nThis inspired us to develop a simpler library Lin, requiring substantially\nless input from its end user. Lin also tests a property by performing and\nrecording the output of a Y-shaped parallel run like STM. However, it does so\nby trying to consolidate the outcome against some sequential run of the tested\nsystem, by performing a search over all possible cmd interleavings. In effect,\nLin thus reuses the tested system as a \"sequential oracle\".\n\nHere is the complete code for a corresponding Float.Array example, now using\nLin:\n\n    \n    \n    module FAConf = struct type t = Float.Array.t let array_size = 12 let init () = Float.Array.make array_size 1.0 let cleanup _ = () open Lin let int = int_small let api = [ val_ \"Float.Array.to_list\" Float.Array.to_list (t @-> returning (list float)); val_ \"Float.Array.sort\" Float.(Array.sort compare) (t @-> returning unit); val_ \"Float.Array.set\" Float.Array.set (t @-> int @-> float @-> returning_or_exc unit); ] end\n\nJust as with STM, Lin needs to be told the type t of the system under test,\nhow to initialise it with init, and how to clean up after it with cleanup.\nFinally, we describe the type signatures of the tested system with a\ncombinator-based DSL in the style of OCaml's ctypes library. In the to_list\ncase, the input is pretty close to the signature to_list : t -> float list\nfrom the Float.Array signature. In the sort case, we test the result of\npassing the Float.compare function. Finally, in the set case, the\nreturning_or_exc combinator expresses that an out-of-bounds exception may be\nraised.\n\nBased on the above, we can now create and run our qcheck-lin test as follows:\n\n    \n    \n    module FA_Lin_dom = Lin_domain.Make(FAConf) let () = QCheck_base_runner.run_tests_main [FA_Lin_dom.neg_lin_test ~count:1000 ~name:\"Lin Float.Array test with Domain\"]\n\nThe result of the Lin_domain.Make functor offers both lin_test for a positive\ntest of sequential consistency and neg_lin_test as we use here for a negative\ntest, confirming absense of sequential consistency with a counterexample:\n\n    \n    \n    random seed: 349336243 generated error fail pass / total time test name [\u2713] 2 0 1 1 / 1000 1.1s Lin Float.Array test with Domain --- Info ----------------------------------------------------------------------- Negative test Lin Float.Array test with Domain failed as expected (27 shrink steps): | Float.Array.set t 0 111.772797434 | .----------------------------------. | | Float.Array.to_list t Float.Array.sort t +++ Messages ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ Messages for test Lin Float.Array test with Domain: Results incompatible with sequential execution | Float.Array.set t 0 111.772797434 : Ok (()) | .-----------------------------------------------------------------------------------------------. | | Float.Array.to_list t : [111.772797434; 1.; 1.; 1.; 1.; 1.; 1.; 1.; 1.; 1.; 1.; 111.772797434] Float.Array.sort t : () ================================================================================ success (ran 1 tests)\n\nNote how the above result is a passing test, which found a counterexample in\nthe second attempt: 2 generated, 1 pass, 1 fail. After shrinking, the\nresulting shape is similar to the one found by STM. The details differ from\nrun to run, reflecting the non-determinism of the tested program. The output\nthis time illustrates how the non-1.0 entry 111.772797434 can unexpectedly\nshow up twice in a result from to_list, again illustrating how Float.Array is\nunsafe to use in parallel.\n\n## Growing a Test Suite\n\nOver time we have developed a growing test suite, which includes tests of\n(parts of) Array, Atomic, Bigarray, Buffer, Bytes, Dynlink, Ephemeron,\nHashtbl, In_channel, Out_channel, Lazy, Queue, Semaphore, Stack, Sys, and Weak\nfrom OCaml's Stdlib in addition to the above-mentioned Float.Array test.\n\nNot everything fits into the STM and Lin formats. To stress-test the new\nruntime's primitives underlying the Domain and Thread modules, we have\ndeveloped separate ad-hoc property-based tests of each of these, as well as a\nproperty-based test of their combination.\n\n## Growing a CI System\n\nStarting from a single GitHub Actions workflow to run the test suite on Linux,\nwe have gradually added additional CI targets, to the point that we can now\nrun the test suite under Linux, macOS, and Windows (MinGW + Cygwin).\nFurthermore, we can run the test suite on OCaml with particular\nconfigurations, such as\n\n  * Bytecode builds\n  * 32-bit builds\n  * Enabling frame pointers\n  * The debug runtime\n\nUntil recently GitHub Actions offered only amd64-based machines for testing,\nlimiting testing to just one OCaml compiler backend. Our colleague Ben Andrew\ntherefore built multicoretests-ci \u2013 an ocurrent-based CI system that lets us\nrun the testsuite on a range of additional platforms:\n\n  * Linux ARM64\n  * macOS ARM64\n  * Linux PowerPC64\n  * Linux s390x\n  * FreeBSD amd64\n\nThe above mentioned POWER bugs were found thanks to multicoretests-ci runs.\n\n## Understanding Issues Found\n\nUp to this point, we have found 30 issues, ranging from test cases crashing\nOCaml's runtime to discovering that the Sys.readdir function may behave\ndifferently on Windows. In order to better understand the issues we have\nfound, we have divided them into categories:\n\n  * runtime \u2013 for issues requiring a change in OCaml's runtime system\n  * stdlib \u2013 for issues requiring a change in OCaml's standard library\n  * codegen \u2013 for issues requiring a change in a backend code generator\n  * flexdll \u2013 for issues requiring a change in the FlexDLL tool for Windows\n  * dune - for issues requiring a change in the dune build system tool\n  * domainslib \u2013 for issues discovered while testing the domainslib library\n  * lockfree \u2013 for issues discovered while testing the lockfree (now: saturn) library\n\nThe found issues are distributed as follows:\n\nThis distribution was a surprise to us: Half of the issues are runtime\nrelated! Initially, we had expected to use the PBT approach to test OCaml's\nexisting Stdlib for safety under parallel usage. However, as testing has\nprogressed, it has become apparent that the approach also works well to stress\ntest and detect errors in the new multicore runtime.\n\nThe above only counts fixed issues for which PRs have been merged. Without a\n'fix PR', it is harder to judge where changes are required and thus to\ncategorise each issue. In addition to the above, we are currently aware of at\nleast 3 additional outstanding issues that we need to investigate further.\n\n## Lin vs STM vs TSan vs DSCheck\n\nAt Tarides, we test multicore OCaml with a variety of tools. Lin tests are\nrelatively easy to write and useful in themselves, but they test a weaker\nproperty compared to STM. For one, they only test a parallel property.\nSecondly, they do not express anything about the intended semantics of a\ntested API, e.g. a module with functions consistently raising a\nNot_implemented_yet exception would pass a Lin test. Furthermore, if we had\nonly used a negative Lin test such as the above, the PowerPC register bug is\nlikely to have been missed or disregarded as a parallel-usage misbehaviour.\n\nOn the other hand, a Lin test was sufficient to reveal, e.g. early out-of-thin\nair values from the Weak module or reading of uninitialised bytes with\nIn_channel.seek on a channel. As such, Lin and STM present a trade-off between\nrequired user input and provided guarantees in a passing test.\n\nThreadSanitizer (TSan) for OCaml is a compiler instrumentation mode targeted\nat detecting data races in OCaml code. For comparison, TSan may detect races\neven if a thread schedule has not revealed a difference in the resulting\noutput, an ability which is beyond Lin and STM as black-box testing libraries.\nOn the other hand, Lin and STM can detect a broader class of observable\ndefects. In one case, STM even detected an issue caused by a race between\natomic reads and writes. As such TSan and PBT are very much complementary\ntools.\n\nDSCheck is a model-checking tool that exhaustively explores all thread\nschedules (up to some bound). As such, OCaml code tested with DSCheck ensures\ncorrectness even for very rarely occurring schedules, thus offering an\nadvantage over Lin and STM. On the other hand, Lin and STM excel in exploring\nrandom combinations of cmds and input parameters, rather than the thread\nschedules they may give rise to. As such, we again see DSCheck and PBT as\nsupplementary.\n\nFinally, Arthur Wendling's earlier blog post offers an example that nicely\nillustrates how Lin, TSan, and DSCheck can complement each other well when\ndeveloping and testing multicore OCaml code. Part two of our DSCheck series\nprovides additional background on how the model checker works and how we use\nit to test data structures for the Saturn library.\n\n## End of Part One\n\nWe will stop here for now and continue in the second part of this miniseries.\nThe next part will focus on the challenges and lessons learned during the\nprocess I've described. I will share some findings that surprised us and threw\nspanners in the works, as well as how we got creative to overcome them.\n\nIn the meantime, you can stay up-to-date with Tarides on X and LinkedIn, and\nsign up for our Newsletter. See you next time!\n\nCopyright Tarides 2018\u20132024\n\n", "frontpage": false}
