{"aid": "40051338", "title": "Bringing serverless GPU inference to Hugging Face users", "url": "https://huggingface.co/blog/cloudflare-workers-ai", "domain": "huggingface.co", "votes": 2, "user": "gregorymichael", "posted_at": "2024-04-16 12:59:16", "comments": 0, "source_title": "Bringing serverless GPU inference to Hugging Face users", "source_text": "Bringing serverless GPU inference to Hugging Face users\n\nHugging Face\n\nBack to Articles\n\n# Bringing serverless GPU inference to Hugging Face users\n\nPublished April 2, 2024\n\nUpdate on GitHub\n\nUpvote\n\n6\n\nphilschmid Philipp Schmid\n\njeffboudier Jeff Boudier\n\nrita3ko Rita Kozlov\n\nguest\n\nnkothariCF Nikhil Kothari\n\nguest\n\nToday, we are thrilled to announce the launch of Deploy on Cloudflare Workers\nAI, a new integration on the Hugging Face Hub. Deploy on Cloudflare Workers AI\nmakes using open models as a serverless API easy, powered by state-of-the-art\nGPUs deployed in Cloudflare edge data centers. Starting today, we are\nintegrating some of the most popular open models on Hugging Face into\nCloudflare Workers AI, powered by our production solutions, like Text\nGeneration Inference.\n\nWith Deploy on Cloudflare Workers AI, developers can build robust Generative\nAI applications without managing GPU infrastructure and servers and at a very\nlow operating cost: only pay for the compute you use, not for idle capacity.\n\n## Generative AI for Developers\n\nThis new experience expands upon the strategic partnership we announced last\nyear to simplify the access and deployment of open Generative AI models. One\nof the main problems developers and organizations face is the scarcity of GPU\navailability and the fixed costs of deploying servers to start building.\nDeploy on Cloudflare Workers AI offers an easy, low-cost solution to these\nchallenges, providing serverless access to popular Hugging Face Models with a\npay-per-request pricing model.\n\nLet's take a look at a concrete example. Imagine you develop an RAG\nApplication that gets ~1000 requests per day, with an input of 1k tokens and\nan output of 100 tokens using Meta Llama 2 7B. The LLM inference production\ncosts would amount to about $1 a day.\n\n\"We're excited to bring this integration to life so quickly. Putting the power\nof Cloudflare's global network of serverless GPUs into the hands of\ndevelopers, paired with the most popular open source models on Hugging Face,\nwill open the doors to lots of exciting innovation by our community around the\nworld,\" said John Graham-Cumming, CTO, Cloudflare\n\n## How it works\n\nUsing Hugging Face Models on Cloudflare Workers AI is super easy. Below, you\nwill find step-by-step instructions on how to use Hermes 2 Pro on Mistral 7B,\nthe newest model from Nous Research.\n\nYou can find all available models in this Cloudflare Collection.\n\nNote: You need access to a Cloudflare Account and API Token.\n\nYou can find the Deploy on Cloudflare option on all available model pages,\nincluding models like Llama, Gemma or Mistral.\n\nOpen the \u201cDeploy\u201d menu, and select \u201cCloudflare Workers AI\u201d - this will open an\ninterface that includes instructions on how to use this model and send\nrequests.\n\nNote: If the model you want to use does not have a \u201cCloudflare Workers AI\u201d\noption, it is currently not supported. We are working on extending the\navailability of models together with Cloudflare. You can reach out to us at\napi-enterprise@huggingface.co with your request.\n\nThe integration can currently be used via two options: using the Workers AI\nREST API or directly in Workers with the Cloudflare AI SDK. Select your\npreferred option and copy the code into your environment. When using the REST\nAPI, you need to make sure the ACCOUNT_ID and API_TOKEN variables are defined.\n\nThat\u2019s it! Now you can start sending requests to Hugging Face Models hosted on\nCloudflare Workers AI. Make sure to use the correct prompt & template expected\nby the model.\n\n## We\u2019re just getting started\n\nWe are excited to collaborate with Cloudflare to make AI more accessible to\ndevelopers. We will work with the Cloudflare team to make more models and\nexperiences available to you!\n\nMore Articles from our Blog\n\n## Making thousands of open LLMs bloom in the Vertex AI Model Garden\n\nBy April 10, 2024 \u2022 11\n\n## A Chatbot on your Laptop: Phi-2 on Intel Meteor Lake\n\nBy March 20, 2024 \u2022 1\n\nUpvote\n\n6\n\n", "frontpage": false}
