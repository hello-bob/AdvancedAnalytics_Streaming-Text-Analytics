{"aid": "40154854", "title": "Using squeezed wav2vec2 to automatically detect owl calls", "url": "https://www.seangoedecke.com/animal-call-audio-recognition/", "domain": "seangoedecke.com", "votes": 1, "user": "gfysfm", "posted_at": "2024-04-25 08:19:11", "comments": 0, "source_title": "From hours to seconds: AI tools to detect animal calls", "source_text": "From hours to seconds: AI tools to detect animal calls | Sean Goedecke\n\n### Sean Goedecke\n\n# From hours to seconds: AI tools to detect animal calls\n\nIf you\u2019re currently spending time listening to long audio recordings of\nbirdcalls or animal noises, it\u2019s likely that the last four years of advances\nin machine learning can save you a lot of time. I would like to help. I hope\nthis blog post convinces you of two things: first, that near-state-of-the-art\nmachine learning is easy to use even if you\u2019re not a professional programmer,\nand second, that with a day or two of effort you can set up a pipeline that\nwill reliably recognize any animal call (e.g. a birdcall), allowing you to\nrecord more data in more places. If you read this post and you want to try\nthis, feel free to email me and I\u2019ll pitch in to help.\n\nThe genesis of this idea came from my partner, who came back from a field\nnaturalist talk having been told that it was currently impossible to build an\nautomatic recogniser for the call of the Australian Powerful Owl, Ninox\nstrenua. I felt that this couldn\u2019t possibly be right in 2024, after the recent\nfrenzy of investment in AI models and tooling. Indeed, it is possible! I\nhacked together ninoxstrenua.site, which will take in an audio file (up to a\ncouple of hundred mb) and tell you if it contains powerful owl calls:\n\nI\u2019m going to try to explain how to do this, assuming an audience that is not a\nprofessional programmer but has dabbled a bit with Python, and who has access\nto a reasonable volume of animal audio (say, a couple of hours).\n\nEdit: Since writing this up, I learned about BirdNET, which is an existing\nsolution that will probably meet your needs. You\u2019re probably better off using\nthat instead of training your own model, unless you need it to be very fast or\nyou\u2019re willing to commit a lot of time to making a large enough dataset that\ncan compete with BirdNET.\n\n## Setting up your tools and accounts\n\nThis is going to be the hardest part if you don\u2019t do a lot of Python\nprogramming. Bear with it - it\u2019s all downhill from here. First, you\u2019re going\nto need to install python3 on whatever system you have:\nhttps://www.python.org/downloads/. Next, you\u2019re going to have to install a\nbunch of libraries for audio processing. In a macOS terminal, the command is\nsomething like pip install pydub librosa soundfile datasets simpleaudio numpy\nhuggingface_hub. It might not work the first time. In the worst case, you\u2019ll\nhave to look up the error message and figure it out. The general tutorial for\ninstalling Python packages is\nhttps://packaging.python.org/en/latest/tutorials/installing-packages/.\n\nAs a general note, ChatGPT is pretty good at helping out with stuff like this:\nyou might be able to paste in whatever error message you\u2019re getting and have\nit figure it out.\n\nYou\u2019ll have to sign up for a free account on https://huggingface.co/ - that\u2019s\nthe website that\u2019s going to store your dataset and your model. I also strongly\nrecommend signing up to https://cloud.lambdalabs.com/. You\u2019ll be using that\nwebsite to rent out a GPU to train your model later on, which will cost a\ncouple of bucks. If you really don\u2019t want to do that, you can rent a GPU\nsomewhere else, or rely on your own computer if it has a GPU already, but the\nsetup will be harder.\n\n## Chunking and labelling your dataset\n\nThe next step is to gather up all your audio files. It doesn\u2019t matter if you\nhave one large file or a lot of small ones, but ideally you should have a\ncouple of hundred instances of the animal call you\u2019re looking for in there.\nDon\u2019t use too much data, since you\u2019re going to be processing it manually.\nYou\u2019ll want it all in .wav format. Start in a new directory, with all your\nfiles in a /data directory:\n\n    \n    \n    workspace/ \u251c\u2500 data/ \u2502 \u251c\u2500 recording1.wav \u2502 \u251c\u2500 moredata.wav\n\nThis is going to form the training data for your model. Before you can train,\nyou\u2019ll need to do three things to your data: split it into even chunks, label\neach chunk (e.g. with \u201cowl\u201d and \u201cnot an owl\u201d), and package it into a\nHuggingFace dataset. Now, from your workspace folder, run this script. That\nwill turn all your audio files into regular five-second chunks:\n\n    \n    \n    workspace/ \u251c\u2500 segments/ \u2502 \u251c\u2500 recording1_segment_1.wav \u2502 \u251c\u2500 recording1_segment_2.wav \u2502 \u251c\u2500 recording1_segment_3.wav \u2502 \u251c\u2500 moredata_segment_1.wav \u251c\u2500 data/ \u2502 \u251c\u2500 recording1.wav \u2502 \u251c\u2500 moredata.wav\n\nNow they\u2019re ready to be labelled. You\u2019ll need to split them up into two\nfolders: dataset/owls/ and dataset/not-owls/. This is the slow part. You could\nmanually listen to all of these and move them into each folder - that would\nwork fine. To make it quicker, I used this script. That will play them\nautomatically, then prompt you to classify like this:\n\n    \n    \n    sgoedecke@Seans-MacBook-Pro new-apr-24 % python3 /Users/sgoedecke/Code/birds/manual-audio-classifier.py Processing sample 1 out of 22 Press ENTER to move to /not_owls, 'o' then ENTER for /owls, 'r' to replay:\n\nMy advice here is to not worry about whether something is a \u201cgood enough\u201d\nexample of your animal call - even if it\u2019s quiet, or cut off at the end or\nstart of the segment, classify it as a \u201cyes\u201d and let the model sort it out.\n\nI spent three hours doing this for my owl classifier. You could spend less,\nbut the more time you spend here the better your model is going to be. Your\nmodel\u2019s only as smart as the data you train it with. Eventually, you should\nhave a structure like this:\n\n    \n    \n    workspace/ \u251c\u2500 dataset/ \u251c\u2500 \u251c\u2500 owls/ \u2502 \u251c\u2500 \u251c\u2500 recording1_segment_1.wav \u2502 \u251c\u2500 \u251c\u2500 recording1_segment_3.wav \u251c\u2500 \u251c\u2500 not-owls/ \u2502 \u251c\u2500 \u251c\u2500 recording1_segment_2.wav \u2502 \u251c\u2500 \u251c\u2500 moredata_segment_1.wav \u251c\u2500 segments/ \u2502 \u251c\u2500 recording1_segment_1.wav \u2502 \u251c\u2500 recording1_segment_2.wav \u2502 \u251c\u2500 recording1_segment_3.wav \u2502 \u251c\u2500 moredata_segment_1.wav \u251c\u2500 data/ \u2502 \u251c\u2500 recording1.wav \u2502 \u251c\u2500 moredata.wav\n\nNow it\u2019s time to package this up into a dataset! This script will do it for\nyou - you should edit the name of the dataset on line 42 to be whatever animal\nyou\u2019re doing. Call it anything you like. It\u2019s not strictly necessary, but you\nshould also probably edit the label names from owl/not-owl to whatever animal\nyou\u2019re using.\n\nThe script will prompt you for a HuggingFace token at some point - go log in\nand generate one, making sure it has write permissions. If all goes well, you\nshould be able to go to your HuggingFace profile and see your brand new\ndataset. After ten minutes or so, you should even be able to listen to some of\nthe audio in it. Here\u2019s what mine looks like:\n\nThere should be at least several hundred entries in the dataset: if there\naren\u2019t, or if the audio doesn\u2019t work, or the labelling isn\u2019t right, then\nunfortunately you\u2019ve gone wrong somewhere when running that last script.\n\n## Training your model\n\nNow it\u2019s time to train the model. We\u2019re going to be using the SEW-D model,\nreleased in this September 2021 paper. It\u2019s a much smaller and faster version\nof Facebook\u2019s wav2vec2 model, which was released about a year earlier. These\ndetails shouldn\u2019t really matter to you - I mention them in case you\u2019re curious\nand because I spent a long time trying out different options before I landed\non SEW-D.\n\nTo train a model, you need GPUs. Otherwise we\u2019ll be here for days. I recommend\ngoing to https://cloud.lambdalabs.com/ and launching an instance. The cheapest\ninstance available is fine. If all goes well, you\u2019ll only be using it for\nabout ten minutes. Note that once you boot an instance, you\u2019re paying for it\nuntil you terminate it, so don\u2019t forget to terminate it when you\u2019re done!\n\nCopy the ssh command from https://cloud.lambdalabs.com/instances and run it.\nAfter typing yes to the prompt, you should be in a terminal with a prompt like\nubuntu@146-235-202-184:~$. Paste in these commands to make sure you\u2019ll have\neverything you need:\n\n    \n    \n    pip install soundfile librosa evaluate transformers pydub pip install accelerate -U\n\nNow you can train your model by running this script. I typically do it by\nopening a python repl with python3 and then just pasting the body of that file\ndirectly into the prompt. You\u2019ll need to enter your HuggingFace token again,\nand update the dataset on line 18 from my dataset\n(sgoedecke/powerful_owl_whatever to your dataset, which will begin with your\nHuggingFace username and a forward slash). If you\u2019re using different labels in\nyour dataset, make sure you update those as well. Finally, update the name\nhere to be whatever you want to call your model. Now paste in the script!\n\nThis is going to take about 10-15 minutes. You\u2019ll see some loading bars as the\nscript downloads your model, and then a bunch of training output that looks\nsomething like this:\n\n    \n    \n    >>> trainer.train() {'loss': 0.6925, 'grad_norm': 1.4354974031448364, 'learning_rate': 6.521739130434783e-06, 'epoch': 0.22} {'loss': 0.6624, 'grad_norm': 3.0547149181365967, 'learning_rate': 1.3043478260869566e-05, 'epoch': 0.43} {'loss': 0.5909, 'grad_norm': 7.781609535217285, 'learning_rate': 1.956521739130435e-05, 'epoch': 0.65} {'loss': 0.5691, 'grad_norm': 6.032699108123779, 'learning_rate': 2.608695652173913e-05, 'epoch': 0.87} {'eval_loss': 0.4933532178401947, 'eval_precision': 0.6876513317191283, 'eval_recall': 0.9562289562289562, 'eval_f1': 0.7999999999999999, 'eval_fbeta': 0.728578758337609, 'eval_runtime': 22.1506, 'eval_samples_per_second': 28.532, 'eval_steps_per_second': 0.903, 'epoch': 1.0}\n\nAs this scrolls by, you should see the loss value decrease (it might tick up\noccasionally, but overall it should go down). The more important number is\nthat eval_f1 value that you\u2019ll get at the end of each \u201cepoch\u201d, or block of\ntraining. We\u2019re doing 15 epochs. At the end, eval_f1 should be around .94\n(higher is better). This is the F1-score, which represents a nice balance\nbetween the model\u2019s \u201cprecision\u201d (how many of the animal calls it notices) and\nits \u201crecall\u201d (the rate at which it makes accurate predictions). This is a\npretty sensible way to measure how good a classification model is, so it\u2019s\nwhat we\u2019re using.\n\nWhen the training is complete, your model should automatically upload itself\nto HuggingFace. You should be able to navigate to your profile on\nhttps://huggingface.co/ and see it under your models. If you see it, then you\nshould be able to terminate your LambdaLabs instance.\n\n## Using your model\n\nYou\u2019ve done all the hard work. Now you get to actually use your model on your\nown computer! Download this script, and update the model name here to be the\nmodel you just uploaded in the last step. Now pick any of your audio files\n(ideally one you didn\u2019t use to train the model, but it will work on those\ntoo), and run:\n\n    \n    \n    python3 infer.py my-audio-file.wav\n\nAfter running for a few minutes, it should print some output like this (and\nalso save it to a file):\n\n    \n    \n    --------------------------------------------- Some of these are likely to be false-positives. Please listen to the actual audio segments to confirm. Potential owls found: File: ./data/WCS5_20231229_045900.wav Start: 00:13:20, End: 00:13:25, Chance: 0.75 Start: 00:13:25, End: 00:13:30, Chance: 1.60 Start: 00:13:30, End: 00:13:35, Chance: 3.44 Start: 00:13:35, End: 00:13:40, Chance: 3.43\n\nAnd that\u2019s it! Those represent where your model thought it recognized the\nanimal call. The chance value is how confident the model was (here the first\ntwo might be false positives, but probably not the last two).\n\nThis is a lot of work for someone who doesn\u2019t write a lot of Python for their\nday job (for what it\u2019s worth, I write very little Python for my day job, and\nrelied heavily on ChatGPT/Copilot to hack these scripts together). But once\nyou do it, it stays done, and you can process an unlimited amount of audio\nvery quickly.\n\nA big thanks to my partner, and to James Deane from the Warringal Conservation\nSociety for his advice and providing testing data. I think the potential of\nmodern machine learning for audio classification is really under-utilized, and\nI\u2019d love to see more people spend a few days setting this up instead of\nspending weeks listening to wildlife recordings. If you got stuck, and you\u2019re\nin a position where you could benefit from a model like this, I am happy to\nhelp: email me at sean.goedecke@gmail.com.\n\nSean works in Melbourne as a software engineer, currently for GitHub. Recruiters, please read this post on what I'm looking for. articles | resume | github | linkedin\n\n", "frontpage": false}
