{"aid": "40210392", "title": "Trying Out New Clothes in Stable Diffusion-Based Videos", "url": "https://blog.metaphysic.ai/trying-out-new-clothes-in-stable-diffusion-based-videos/", "domain": "metaphysic.ai", "votes": 1, "user": "Hard_Space", "posted_at": "2024-04-30 12:39:38", "comments": 0, "source_title": "Trying out New Clothes in Stable Diffusion-Based Videos", "source_text": "Trying out New Clothes in Stable Diffusion-Based Videos Trying out New Clothes\nin Stable Diffusion-Based Videos - Metaphysic.ai\n\nSkip to content\n\n## Home\n\n# Trying out New Clothes in Stable Diffusion-Based Videos\n\n  * April 30, 2024\n  * 12:38 pm\n\n### About the author\n\n#### Martin Anderson\n\nI'm Martin Anderson, a writer occupied exclusively with machine learning,\nartificial intelligence, big data, and closely-related topics, with an\nemphasis on image synthesis, computer vision, and NLP.\n\n  * Author Website\n  * Author Archive\n\n## Share This Post\n\nThough it may not be obvious to the casual observer of the image and video\nsynthesis scene, the fashion industry is funding considerable research into\nremoving the need for people to actually come into stores and try on clothing\nthat they might want to purchase.\n\nTypical output from the vast range of try-on models released into the\nliterature over the last 5-6 years, typified by static image results, and not\nvery different to Photoshop-style output. Source:\nhttps://diffuse2choose.github.io/\n\nThe \u2018try-on\u2019 strand of computer vision research is well-funded and well-\npursued; but, until lately, has been limited either to static images that show\nthe consumer what they might look like if wearing the potential purchase, or\nelse has provided only rudimentary and quite crude video synthesis systems\nthat cannot handle billowy or non-tight clothing, fails to produce consistent\nrenders, or is unable to reproduce more elaborate new designs in a realistic\ncontext.\n\nThus, our attention has been drawn this week to a new offering from China that\nuses Stable Diffusion as a backbone to \u2018project\u2019 single images from new\nproduct lines into real footage, effectively substituting the clothing from\nthe source video with an adroitness that\u2019s rather impressive:\n\nThe new system can adapt to a wide variety of clothing styles, and does not\nrequire that the clothing in question is tight-fitting to the body. This is a\nmontage of videos available at higher resolution at the project page. Source:\nhttps://mengtingchen.github.io/tunnel-try-on-page/\n\nThe new approach uses a method dubbed \u2018tunneling\u2019 by the authors \u2013 really, a\nsimple recognition of the area to be affected (i.e., the superimposition of a\nnew sweatshirt), which is then diverted into a dedicated process for\nconforming through Stable Diffusion and various ancillary methods, until the\nclothing is credibly superimposed into the new footage.\n\nExamples of some of the novel features available in the new system, which\npresent a challenge for the very small number of prior attempts to create this\nkind of functionality. This is a montage of videos available at higher\nresolution at the project page.\n\nContextual information about the environment is also handled via a separate\nmodule, so that the projected clothing maintains a natural appearance.\n\nThere are very few existing projects that have attempted this, and the\nauthors\u2019 tests report that their approach, dubbed \u2018Tunnel Try-On\u2019, notably\ndefeats the scant current state-of-the-art for this task.\n\nThey consider Tunnel Try-On to be the first attempt towards commercially\nviable clothing imposition of this nature, and further state:\n\n\u2018By integrating the focus tunnel strategy and focus tunnel enhancement, our\nmethod demonstrates the ability to effectively adapt to different types of\nhuman movements and camera variations, resulting in high-detail preservation\nand temporal consistency in the generated try-on videos.\n\n\u2018Moreover, unlike previous video try-on methods limited to fitting tight-\nfitting tops, our model can perform try-on tasks for different types of tops\nand bottoms based on the user\u2019s choices.\u2019\n\nThe new paper is titled Tunnel Try-on: Excavating Spatial-temporal Tunnels for\nHigh-quality Virtual Try-on in Videos, and comes from nine researchers across\nHuazhong University of Science and Technology and Alibaba. The initiative\ncomes with an accompanying project site, from which the videos in this article\nhave been extrapolated.\n\n## Method\n\nThe system uses a latent encoder to project an image of the new fashion item\ninto the latent space of the model. The Stable Diffusion inpainting model is\nused for the task.\n\nAt this point, Stable Diffusion itself transforms this passed-on latent\nrepresentation into Gaussian noise through a Markov process, and eventually\nwill pass the processed data through to CLIP, so that the image/text bindings\ntrained into the model will become used as an interpretive layer for apposite\noutput.\n\nSchema for the architectural workflow of Tunnel Try-On.\n\nAs mentioned, the obscure term \u2018tunneling\u2019 in this project really refers to\nthe extraction of the target area by means of image recognition, similar to\nzooming-in and cropping off a relevant area of a video.\n\nTunnel extraction is essentially extraction of the area of interest, which is\nusually a face or person in image synthesis pipelines, but in this case is a\n'garment area'.\n\nWe can see in the image above, on the right, that the tunneling pipeline\nconcludes in a masked-off area representing the part of the video that will be\ncloistered away for processing, until a later environmental module will help\nto integrate it better into the context of the clip.\n\nThe environmental feature encoding consists of a frozen CLIP image encoder and\na learnable (non-frozen) mapping layer. The output from this part of the\nprocess is subsequently fine-tuned through a learnable projection layer (i.e.,\na layer that concatenates the results obtained so far).\n\nThe baseline module (in grey in the schema earlier above) consists of two\nU-Nets \u2013 a \u2018main\u2019 U-Net module initialized with an inpainting module, and a\n\u2018ref\u2019 U-Net that utilizes the popular Reference Only component of the\nControlNet system, an ancillary control system for Stable Diffusion, which\nallows users to generate images based on existing images, instead of the\nimages being pulled from the imaginative faculties of the trained model, based\nonly on a text prompt.\n\nControlNet's 'Reference Only' module can force generations to stick to the\ncontent of a source image, while still producing variations on that image.\nSource: https://github.com/Mikubill/sd-webui-controlnet/discussions/1236\n\nWith the Reference Only module preserving the image detail from the input\ngarment, CLIP is also used to capture high-level semantic details of the\ntarget garment (i.e., a descriptive text component, which is a typical usage\nin Stable Diffusion generation, is considered additionally).\n\nThe main U-Net consists of nine channels, four of which are extracted from the\ntarget area in the source video, four of which consist of related latent\nnoise, and one of which is reserved for the generated mask in which the\nsynthesis will occur. Pose maps (estimations of figure disposition) are also\nincorporated at this stage to increase accuracy, and concatenated into the\nresulting embeddings.\n\nTemporal attention is then applied after each stage of the primary U-Net, to\nadapt the try-on model (the garment) to the processing of video. The feature\nmaps generated at this stage will ultimately result, after processing, in\noutput features consisting entirely of denoising feature maps.\n\nThe so-called \u2018focus tunnel\u2019 obtained by this point is then subject to Kalman\nfiltering, a linear quadratic estimation (LQE) technique that captures a\ndiverse series of measurements over time, accounting for statistical noise, in\norder to arrive at cleaner and more meaningful variables.\n\nAt the same time, an environment encoder captures the outlying and contextual\ninformation into which the synthesized material will ultimately be projected,\nso that the final result does not look like a crude \u2018overlay\u2019, but is instead\nintegrated well into the clip.\n\nThe criteria for cropping the segments for the tunnel come with a number of\npitfalls. The authors explain:\n\n\u2018In typical image virtual try-on datasets, the target person is typically\ncentered and occupies a large portion of the image. However, in video virtual\ntry-on, due to the movement of the person and camera panning, the person in\nvideo frames may appear at the edges or occupy a smaller portion.\n\n\u2018This can lead to a decrease in the quality of video generation results and\nreduce the model\u2019s ability to maintain clothing identity. To enhance the\nmodel\u2019s ability to preserve details and better utilize the training weights\nlearned from image try-on data, we propose the \u201dfocus tunnel\u201d [strategy.]\u2019\n\nThe aforementioned pose maps are therefore used to identify the smallest\npossible bounding box for the upper or lower body (i.e., shirts or jeans,\netc.). Using this as a base, the system then enlarges the bounding area until\nthe entirety of necessary clothing is covered in the crop, and ultimately the\nvideo frames will be cropped, padded and resized to the target training input\nresolution in the main U-Net.\n\nRegarding the tunnel embedding stage, the authors state*:\n\n\u2018The input form of the focus tunnel has increased the magnitude of the camera\nmovement. To mitigate the challenge faced by the temporal-attention module in\nsmoothing out such significant camera movements, we introduce the Tunnel\nEmbedding.\n\n\u2018Tunnel Embedding accepts a three-tuple input, comprising the original image\nsize, tunnel center coordinates, and tunnel size. Inspired by the design of\nresolution embedding in SDXL, Tunnel Embedding first encodes the three-tuple\ninto 1D absolute position encoding, and then obtains the corresponding\nembedding through linear mapping and activation functions.\n\n\u2018Subsequently, the focus tunnel embedding is added to the temporal attention\nas position encoding.\u2019\n\nThe final generated output is, at this stage, blended into the source video\nusing Gaussian blur as a means of avoiding excessive sharpness that would make\nthe new content stand out excessively.\n\n## Data and Tests\n\nThe training of the model is divided into two phases: in the first, the\nresearchers freeze the SD-native Variational Autoencoder (VAE), its decoder,\ntemporal attention, the environment encoder and the tunnel embedding\nfunctionality, and updates solely the parameters of the primary U-Net, the\nreference (i.e., ControlNet-related) U-Net, the pose estimator guidance unit.\n\nHere the model is trained on paired data relating to image try-ons. The\nauthors state:\n\n\u2018The objective of this stage is to learn the extraction and preservation of\nclothing features using larger, higher-quality, and more diverse paired image\ndata compared to the video data, aiming to achieve high-fidelity image-level\ntry-on generation results as a solid foundation.\u2019\n\nIn the second stage the model is trained on datasets featuring video-based\ntry-on material, with only temporal attention and the environment encoder\nupdated, and other facets frozen. The objective of this is to translate the\nstatic features learned in the first phase into the temporal realm, with\napposite video data.\n\nTo test the system, the authors used two video try-on datasets \u2013 their own\ncurated collection, and the FW-GAN VVT dataset.\n\nExamples from the prior dataset, VVT, used in experiments for the new paper.\nSource: \u2020\n\nThe VVT dataset contains 791 paired videos of people and clothing, at a\nresolution of 192x256px. The models in the video have an unchallenging pure\nwhite background (see example images above). The researchers of the new paper\nnote that the content is equally unchallenging, since the people in the videos\nperform \u2018simple\u2019 poses in fitted (i.e., tight) tops, which falls below the\nscope of the new system proposed in Tunnel Try-On.\n\nBecause of these limitations, the authors; own curated datasets is drawn from\nreal-life ecommerce applications, featuring 5,350 video-image pairings, split\nby the researchers into 4,280 training videos and 1,070 testing videos. This\nbespoke dataset features more complex backgrounds and more problematic (i.e.,\nnon-tight) types of clothing.\n\nThe main U-Net utilizes the aforementioned Stable Diffusion inpainting model;\nthe reference U-Net is initialized with the standard text-to-image\nfunctionality of the V1.5 release of Stable Diffusion; and the temporal\nattention module is initialized from the motion module in the Stable Diffusion\nadjunct system AnimateDiff.\n\nTwo training stages were undertaken, during each of which the disparately-\nsized source data was resized to a standard 512x512px.\n\nThe models were trained on eight A100 GPUs (the paper does not specify whether\nthese GPUs were the 40GB or 80GB VRAM models \u2013 the A100 comes in both\nconfigurations).\n\nInitially the researchers used image try-on paired images extracted from their\nvideo sources, merging them with prior equivalent data from the VITON-HD\ndataset.\n\nExamples from the VITON-HD dataset, used in the tests for Tunnel Try-On.\nSource: https://arxiv.org/pdf/2103.16874\n\nAfter this, a 24-frame clip was sampled as input for the second stage. During\ntesting, longer video clips were facilitated by the temporal aggregation\ntechnique developed for the 2022 EDGE project.\n\nEDGE was designed to produce choreography directly from music, and its video\nconcatenation technique was used in the training of Tunnel Try-On. Source:\nhttps://edge-dance.github.io/\n\nThe researchers compared Tunnel Try-On with alternative and prior methods in\nthe VVT dataset, across qualitative, quantitative and user studies.\n\nMethods examined included the GAN-based approaches FW-GAN, PBAFN, and the\ninfluential ClothFormer, as well as the diffusion-based frameworks Anydoor and\nStableVITON.\n\nFor fair comparison, the authors used the VITON-HD dataset for first-stage\ntraining, and VVT for the second stage, avoiding the use of their own dataset\n(which might have excessively and unfairly challenged the prior frameworks\ntested).\n\nQualitative comparisons over the VVT dataset. Please refer to the source paper\nfor better resolution.\n\nAbove are featured the initial qualitative tests. Of these, the authors\ncomment:\n\n\u2018[It] is evident that GAN-based methods like FW-GAN and PBAFN, which utilize\nwarping modules, struggle to adapt effectively to variations in the sizes of\nindividuals in the video. Satisfactory results are achieved only in close-up\nshots, with the warping of clothing producing acceptable outcomes.\n\n\u2018However, when the model moves farther away and becomes smaller, the warping\nmodule produces inaccurately wrapped clothing, resulting in unsatisfactory\nsingle-frame try-on results.\n\n\u2018ClothFormer can handle situations where the person\u2019s proportion is relatively\nsmall, but its generated results are blurry, with significant color\ndeviation.\u2019\n\nThe authors further observe that the diffusion-based methods, including\nStableVITON, are unable to maintain continuity of lettering on the text of a\nt-shirt (though the paper does not mention that later versions of Stable\nDiffusion than V1.5 have largely solved the typography issue).\n\nThe complex pattern, including lettering, is not maintained consistently\nacross generations under rival diffusion-based methods.\n\nAs can be imagined, when applied to videos, these and similar aberrations\ncause a notable \u2018jitter\u2019 or \u2018sizzle\u2019, as observed by the authors of the new\nwork.\n\nThe authors also conducted quantitative tests. Metrics used for static images\nwere Structural Similarity Index (SSIM), and Learned Perceptual Similarity\nMetrics (LPIPS).\n\nFor video, the metrics used were from Video Fr\u00e9chet Inception Distance (VFID,\nwhich appears to be an alternative take on Fr\u00e9chet Video Distance, or FVD),\nhere with feature extraction provided by I3D and 3D-ResNeXt101.\n\nResults from the quantitative test round.\n\nOf the quantitative tests, the authors state:\n\n\u2018[On] the VVT dataset, our Tunnel Try-on outperforms others in terms of SSIM,\nLPIPS, and VFID metrics, further confirming the superiority of our model in\nimage visual quality (similarity and diversity) and temporal continuity\ncompared to other methods.\n\n\u2018It\u2019s worth noting that we have a substantial advantage in LPIPS compared to\nother methods. Considering that LPIPS is more in line with human visual\nperception compared to SSIM, this highlights the superior visual quality of\nour approach.\u2019\n\nA user study was also conducted, wherein 10 annotators were asked to compare\n130 samples from the VVT test set, with different video generation methods\nused from the same input. Evaluation criteria were \u2018smoothness\u2019, \u2018fidelity\u2019,\nand \u2018quality\u2019.\n\nResults from the user study.\n\nThe authors emphasize, as can be seen from the results table above, that\nTunnel Try-On comfortably leads the results in the user study.\n\nReferring to the initial qualitative tests at the head of the new paper (see\nimage below), the authors conclude:\n\n\u2018By integrating the focus tunnel strategy and focus tunnel enhancement, our\nmethod demonstrates the ability to effectively adapt to different types of\nhuman movements and camera variations, resulting in high-detail preservation\nand temporal consistency in the generated try- on videos.\n\n\u2018Moreover, unlike previous video try-on methods limited to fitting tight-\nfitting tops, our model can perform try-on tasks for different types of tops\nand bottoms based on the user\u2019s choices.\u2019\n\nInitial showcase results from the new paper. Please refer to the source paper\nfor better resolution.\n\n## Conclusion\n\nAfter a long and relatively arid winter of papers that either cannot fulfill\ntheir promise, or else only produce incremental improvement over prior works\n(often at great expense of resources), it has been refreshing to see a\nbreakthrough as impressive as Tunnel Try-On, as the season begins to warm up\nin anticipation of the coming array of computer vision and AI conferences.\n\nOf particular note for the new system is its ability to handle clothing that\nis not \u2018sprayed on\u2019 to the person in the video, since fashion is a volatile\nmedium, and systems of this nature will need to be able to handle volume\nbetter than prior works have done. Here, a very encouraging start is made to\nmeet that challenge, even if there is the occasional glitch with hand\nrendering (a problem which has always plagued Stable Diffusion).\n\nHowever, the new system has not taken on the most voluminous possible types of\nclothing, such as wide-borne dresses or wedding dresses, which would likely\nrequire addressing via physic models of some kind.\n\nYet, disregarding the tendencies of fashion models to elaborately flourish\naround loose clothing that they might be modeling, such as long scarves,\nTunnel Try-On appears to have made a notable leap towards a genuinely\nversatile and photorealistic fashion video try-on system.\n\n* My substitution of hyperlinks for the authors\u2019 inline citation/s.\n\n^\u2020 https://openaccess.thecvf.com/content_ICCV_2019/papers/Dong_FW-GAN_Flow-\nNavigated_Warping_GAN_for_Video_Virtual_Try-On_ICCV_2019_paper.pdf\n\nRETURN TO METAPHYSIC BLOG HOME\n\nPrevPreviousRepairing the Nightmarish Hands Produced by Stable Diffusion\n\n## More To Explore\n\nAI ML DL\n\n### Trying out New Clothes in Stable Diffusion-Based Videos\n\nThe fashion industry has been investing in virtual try-on systems heavily over\nthe last 5-6 years, but to date has not produced a system capable of\nprojecting customers\u2019 diverse appearances into the latest fashions in an\nactual video. Now, a new system from China has used the generative power of\nStable Diffusion to make that facility a reality, in a new project titled\nTunnel Try-On, that\u2019s capable of projecting individual items of new clothing\nconvincingly into existing videos.\n\nMartin Anderson April 30, 2024\n\nAI ML DL\n\n### Repairing the Nightmarish Hands Produced by Stable Diffusion\n\nStable Diffusion has captured the imagination of the world since its release\nin 2022, but retains a notable difficulty in rendering human hands \u2013 one of\nthe most difficult anatomical challenges also for human artists. A new wave of\n\u2018hand repair\u2019 architectures is appearing in the literature of late, the most\nrecent of which is this complex but effective new post-processing framework\nfrom China.\n\nMartin Anderson April 25, 2024\n\n\u201c\n\n## It is the mark of an educated mind to be able to entertain a thought\nwithout accepting it.\n\nAristotle\n\nCopyright \u00a9 2023. All rights reserved. Privacy Policy\n\n### Quick Links\n\n  * Home\n  * Every Anyone\n  * Synthetic Futures\n\n### Connect with us\n\n  * Discord\n  * Tiktok\n  * Twitter\n  * Youtube\n  * Instagram\n  * Github\n  * Linkedin\n\n### Contact Info\n\n  * info@metaphysic.ai\n  * press@metaphysic.ai\n\n", "frontpage": false}
