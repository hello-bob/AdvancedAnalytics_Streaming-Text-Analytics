{"aid": "40285986", "title": "Instructlab AI CLI", "url": "https://github.com/instructlab/instructlab", "domain": "github.com/instructlab", "votes": 10, "user": "gudvardur", "posted_at": "2024-05-07 14:34:49", "comments": 0, "source_title": "GitHub - instructlab/instructlab: Command-line interface. Use this to chat with the model or train the model (training consumes the taxonomy data)", "source_text": "GitHub - instructlab/instructlab: Command-line interface. Use this to chat\nwith the model or train the model (training consumes the taxonomy data)\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\ninstructlab / instructlab Public\n\n  * Notifications\n  * Fork 82\n  * Star 151\n\nCommand-line interface. Use this to chat with the model or train the model\n(training consumes the taxonomy data)\n\n### License\n\nApache-2.0 license\n\n151 stars 82 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# instructlab/instructlab\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n8 Branches\n\n22 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nmergify[bot]Merge pull request #1130 from leseb/fix-1129May 7, 2024d7d1342 \u00b7\nMay 7, 2024May 7, 2024\n\n## History\n\n866 Commits  \n  \n### .github\n\n|\n\n### .github\n\n| Do not cache llama-cpp-python builds| May 7, 2024  \n  \n### CONTRIBUTING\n\n|\n\n### CONTRIBUTING\n\n| feat: run functional test on MacOS| May 2, 2024  \n  \n### containers\n\n|\n\n### containers\n\n| Reapply \"Intel Gaudi / Habana Labs HPU support\"| May 7, 2024  \n  \n### docs\n\n|\n\n### docs\n\n| Reapply \"Intel Gaudi / Habana Labs HPU support\"| May 7, 2024  \n  \n### notebooks\n\n|\n\n### notebooks\n\n| Merge pull request #1117 from RedHat-Israel/lora| May 7, 2024  \n  \n### scripts\n\n|\n\n### scripts\n\n| Fix up model name checking in the functional tests| May 6, 2024  \n  \n### src/instructlab\n\n|\n\n### src/instructlab\n\n| Reapply \"Intel Gaudi / Habana Labs HPU support\"| May 7, 2024  \n  \n### tests\n\n|\n\n### tests\n\n| Reapply \"Intel Gaudi / Habana Labs HPU support\"| May 7, 2024  \n  \n### .dockerignore\n\n|\n\n### .dockerignore\n\n| Improve ROCm container and documentation| Apr 18, 2024  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| tox: add -e docs target to generate man pages| May 3, 2024  \n  \n### .gitmodules\n\n|\n\n### .gitmodules\n\n| schema: Use schema repository via submodule| Apr 29, 2024  \n  \n### .isort.cfg\n\n|\n\n### .isort.cfg\n\n| Add formatting| Feb 28, 2024  \n  \n### .markdownlint-cli2.yaml\n\n|\n\n### .markdownlint-cli2.yaml\n\n| templates don't need to follow markdown lint rules| May 3, 2024  \n  \n### .pre-commit-config.yaml\n\n|\n\n### .pre-commit-config.yaml\n\n| Replace Black with Ruff| Apr 23, 2024  \n  \n### .pylintrc\n\n|\n\n### .pylintrc\n\n| Update notice comments to single line SPDX-License-Identifier| Apr 17, 2024  \n  \n### .spellcheck-en-custom.txt\n\n|\n\n### .spellcheck-en-custom.txt\n\n| Reapply \"Intel Gaudi / Habana Labs HPU support\"| May 7, 2024  \n  \n### .spellcheck.yml\n\n|\n\n### .spellcheck.yml\n\n| fix: do not spellcheck on venv and taxonomy| May 7, 2024  \n  \n### CODE_OF_CONDUCT.md\n\n|\n\n### CODE_OF_CONDUCT.md\n\n| Reflect new org and repo name| Apr 17, 2024  \n  \n### CONTRIBUTOR_ROLES.md\n\n|\n\n### CONTRIBUTOR_ROLES.md\n\n| Reflect new org and repo name| Apr 17, 2024  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Add LICENSE| Feb 23, 2024  \n  \n### MAINTAINERS.md\n\n|\n\n### MAINTAINERS.md\n\n| Reflect new org and repo name| Apr 17, 2024  \n  \n### Makefile\n\n|\n\n### Makefile\n\n| Reapply \"Intel Gaudi / Habana Labs HPU support\"| May 7, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| added line about ensuring model still running when creating synthetic...|\nMay 6, 2024  \n  \n### SECURITY.md\n\n|\n\n### SECURITY.md\n\n| Update SECURITY.md| May 2, 2024  \n  \n### TROUBLESHOOTING.md\n\n|\n\n### TROUBLESHOOTING.md\n\n| spell: don't be greedy when ignoring backtick fences| Apr 22, 2024  \n  \n### governance.md\n\n|\n\n### governance.md\n\n| Reflect new org and repo name| Apr 17, 2024  \n  \n### pyproject.toml\n\n|\n\n### pyproject.toml\n\n| Reapply \"Intel Gaudi / Habana Labs HPU support\"| May 7, 2024  \n  \n### requirements-dev.txt\n\n|\n\n### requirements-dev.txt\n\n| Update notice comments to single line SPDX-License-Identifier| Apr 17, 2024  \n  \n### requirements-hpu.txt\n\n|\n\n### requirements-hpu.txt\n\n| Reapply \"Intel Gaudi / Habana Labs HPU support\"| May 7, 2024  \n  \n### requirements.txt\n\n|\n\n### requirements.txt\n\n| Reapply \"Intel Gaudi / Habana Labs HPU support\"| May 7, 2024  \n  \n### tox.ini\n\n|\n\n### tox.ini\n\n| tox: add -e docs target to generate man pages| May 3, 2024  \n  \n## Repository files navigation\n\n# InstructLab \ud83d\udc36 (ilab)\n\n## \ud83d\udcd6 Contents\n\n  * \u2753What is ilab\n  * \ud83d\udccb Requirements\n  * \u2705 Getting started\n\n    * \ud83e\uddf0 Installing ilab\n    * \ud83c\udfd7\ufe0f Initialize ilab\n    * \ud83d\udce5 Download the model\n    * \ud83c\udf74 Serving the model\n    * \ud83d\udce3 Chat with the model (Optional)\n  * \ud83d\udcbb Creating new knowledge or skills and training the model\n\n    * \ud83c\udf81 Contribute knowledge or compositional skills\n    * \ud83d\udcdc List and validate your new data\n    * \ud83d\ude80 Generate a synthetic dataset\n    * \ud83d\udc69\ud83c\udfeb Train the model\n    * Test the newly trained model\n    * \ud83c\udf74 Serve the newly trained model\n    * \ud83d\udce3 Chat with the new model (not optional this time)\n  * \ud83c\udf81 Submit your new knowledge or skills\n  * \ud83d\udcec Contributing to InstructLab CLI\n\n## Welcome to the InstructLab CLI\n\nInstructLab \ud83d\udc36 uses a novel synthetic data-based alignment tuning method for\nLarge Language Models (LLMs.) The \"lab\" in InstructLab \ud83d\udc36 stands for Large-\nScale Alignment for ChatBots [1].\n\n[1] Shivchander Sudalairaj*, Abhishek Bhandwaldar*, Aldo Pareja*, Kai Xu,\nDavid D. Cox, Akash Srivastava*. \"LAB: Large-Scale Alignment for ChatBots\",\narXiv preprint arXiv: 2403.01081, 2024. (* denotes equal contributions)\n\n## \u2753 What is ilab\n\nilab is a Command-Line Interface (CLI) tool that allows you to:\n\n  1. Download a pre-trained Large Language Model (LLM).\n  2. Chat with the LLM.\n\nTo add new knowledge and skills to the pre-trained LLM you have to add new\ninformation to the companion taxonomy repository. After that is done, you can:\n\n  1. Use ilab to generate new synthetic training data based on the changes in your local taxonomy repository.\n  2. Re-train the LLM with the new training data.\n  3. Chat with the re-trained LLM to see the results.\n\nThe full process is described graphically in the workflow diagram.\n\nImportant\n\nIt is important to understand that running InstructLab on a laptop will give\nyou a low-fidelity approximation of both synthetic data generation (using the\nilab generate command) and model instruction tuning (using the ilab train\ncommand, which uses QLoRA.) The quality of the results you get using these\ntools on a laptop will not be as high-fidelity as they might be using a larger\nteacher model and a different training method. We have optimized InstructLab\nto enable community members with modest hardware to be able to use the\ntechnique. If you have more sophisticated hardware, you can configure\nInstructLab to use a larger teacher model such as Mixtral in order to achieve\nhigher-fidelity results.\n\n## \ud83d\udccb Requirements\n\n  * \ud83c\udf4e Apple M1/M2/M3 Mac or \ud83d\udc27 Linux system (tested on Fedora). We anticipate support for more operating systems in the future.\n  * C++ compiler\n  * Python 3.9+ (<3.12 for PyTorch JIT)\n  * Approximately 60GB disk space (entire process)\n\n> NOTE: PyTorch 2.2.1 does not support torch.compile with Python 3.12. On\n> Fedora 39+, install python3.11-devel and create the virtual env with\n> python3.11 if you wish to use PyTorch's JIT compiler.\n\n## \u2705 Getting started\n\n### \ud83e\uddf0 Installing ilab\n\n  1. When installing on Fedora Linux, install C++, Python 3.9+, and other necessary tools by running the following command:\n    \n        sudo dnf install g++ gcc make pip python3 python3-devel python3-GitPython\n\nOptional: If g++ is not found, try gcc-c++ by running the following command:\n\n    \n        sudo dnf install gcc-c++ gcc make pip python3 python3-devel python3-GitPython\n\nIf you are running on macOS, this installation is not necessary and you can\nbegin your process with the following step.\n\n  2. Create a new directory called instructlab to store the files the ilab CLI needs when running and cd into the directory by running the following command:\n    \n        mkdir instructlab cd instructlab\n\n> NOTE: The following steps in this document use Python venv for virtual\n> environments. However, if you use another tool such as pyenv or Conda\n> Miniforge for managing Python environments on your machine continue to use\n> that tool instead. Otherwise, you may have issues with packages that are\n> installed but not found in venv.\n\n  3. Install and activate your venv environment by running the following command:\n\n> NOTE: \u23f3 pip install may take some time, depending on your internet\n> connection. In case installation fails with error unsupported instruction\n> `vpdpbusd', append -C cmake.args=\"-DLLAMA_NATIVE=off\" to pip install\n> command.\n\nSee the GPU acceleration documentation for how to to enable hardware\nacceleration for inference and training on AMD ROCm, Apple Metal Performance\nShaders (MPS), and Nvidia CUDA.\n\n#### To install with no GPU acceleration and PyTorch without CUDA bindings\n\n    \n        python3 -m venv --upgrade-deps venv source venv/bin/activate (venv) $ pip cache remove llama_cpp_python (venv) $ pip install git+https://github.com/instructlab/instructlab.git@stable --extra-index-url=https://download.pytorch.org/whl/cpu\n\n#### To install with AMD ROCm\n\n    \n        python3 -m venv --upgrade-deps venv source venv/bin/activate (venv) $ pip cache remove llama_cpp_python (venv) $ pip install git+https://github.com/instructlab/instructlab.git@stable \\ --extra-index-url https://download.pytorch.org/whl/rocm6.0 \\ -C cmake.args=\"-DLLAMA_HIPBLAS=on\" \\ -C cmake.args=\"-DAMDGPU_TARGETS=all\" \\ -C cmake.args=\"-DCMAKE_C_COMPILER=/opt/rocm/llvm/bin/clang\" \\ -C cmake.args=\"-DCMAKE_CXX_COMPILER=/opt/rocm/llvm/bin/clang++\" \\ -C cmake.args=\"-DCMAKE_PREFIX_PATH=/opt/rocm\"\n\nOn Fedora 40+, use -DCMAKE_C_COMPILER=clang-17 and\n-DCMAKE_CXX_COMPILER=clang++-17.\n\n#### To install with Apple Metal on M1/M2/M3 Mac\n\n    \n        python3 -m venv --upgrade-deps venv source venv/bin/activate (venv) $ pip cache remove llama_cpp_python (venv) $ pip install git+https://github.com/instructlab/instructlab.git@stable -C cmake.args=\"-DLLAMA_METAL=on\"\n\n#### To install with Nvidia CUDA\n\n    \n        python3 -m venv --upgrade-deps venv source venv/bin/activate (venv) $ pip cache remove llama_cpp_python (venv) $ pip install git+https://github.com/instructlab/instructlab.git@stable -C cmake.args=\"-DLLAMA_CUBLAS=on\"\n\n  4. From your venv environment, verify ilab is installed correctly, by running the ilab command.\n    \n        ilab\n\n#### Example output\n\n    \n        (venv) $ ilab Usage: ilab [OPTIONS] COMMAND [ARGS]... CLI for interacting with InstructLab. If this is your first time running InstructLab, it's best to start with `ilab init` to create the environment. Options: --config PATH Path to a configuration file. [default: config.yaml] --version Show the version and exit. --help Show this message and exit. Commands: chat Run a chat using the modified model check (Deprecated) Check that taxonomy is valid convert Converts model to GGUF diff Lists taxonomy files that have changed since <taxonomy-base>... download Download the model(s) to train generate Generates synthetic data to enhance your example data init Initializes environment for InstructLab list (Deprecated) Lists taxonomy files that have changed since <taxonomy-base>. serve Start a local server test Runs basic test to ensure model correctness train Takes synthetic data generated locally with `ilab generate`...\n\n> IMPORTANT: every ilab command needs to be run from within your Python\n> virtual environment. To enter the Python environment, run the following\n> command:\n    \n        source venv/bin/activate\n\n### \ud83c\udfd7\ufe0f Initialize ilab\n\n  1. Initialize ilab by running the following command:\n    \n        ilab init\n\n#### Example output\n\n    \n        Welcome to InstructLab CLI. This guide will help you set up your environment. Please provide the following values to initiate the environment [press Enter for defaults]: Path to taxonomy repo [taxonomy]: <ENTER>\n\n  2. When prompted by the interface, press Enter to add a new default config.yaml file.\n\n  3. When prompted, clone the https://github.com/instructlab/taxonomy.git repository into the current directory by typing y.\n\nOptional: If you want to point to an existing local clone of the taxonomy\nrepository, you can pass the path interactively or alternatively with the\n--taxonomy-path flag.\n\n#### Example output\n\n    \n        (venv) $ ilab init Welcome to InstructLab CLI. This guide will help you set up your environment. Please provide the following values to initiate the environment [press Enter for defaults]: Path to taxonomy repo [taxonomy]: <ENTER> `taxonomy` seems to not exists or is empty. Should I clone https://github.com/instructlab/taxonomy.git for you? [y/N]: y Cloning https://github.com/instructlab/taxonomy.git... Generating `config.yaml` in the current directory... Initialization completed successfully, you're ready to start using `ilab`. Enjoy!\n\nilab will use the default configuration file unless otherwise specified. You\ncan override this behavior with the --config parameter for any ilab command.\n\nThe taxonomy repository uses submodules to incorporate the taxonomy schema.\nWhen the ilab init command clones the taxonomy repository, it automatically\nhandles the submodules. If you clone the taxonomy repository yourself, be sure\nto use the --recurse-submodules option on the git clone command and the git\npull command when pulling updates from the remote repository. For example:\n\n    \n        git clone --recurse-submodules https://github.com/instructlab/taxonomy.git git pull --recurse-submodules\n\n### \ud83d\udce5 Download the model\n\n  * Run the ilab downloadcommand.\n    \n        ilab download\n\nilab download will download a pre-trained model (~4.4G) from HuggingFace and\nstore it in a models directory:\n\n    \n        (venv) $ ilab download Downloading model from instructlab/merlinite-7b-lab-GGUF@main to models... (venv) $ ls models merlinite-7b-lab-Q4_K_M.gguf\n\n> NOTE \u23f3 This command can take few minutes or immediately depending on your\n> internet connection or model is cached. If you have issues connecting to\n> Hugging Face, refer to the Hugging Face discussion forum for more details.\n\n### \ud83c\udf74 Serving the model\n\n  * Serve the model by running the following command:\n    \n        ilab serve\n\nOnce the model is served and ready, you'll see the following output:\n\n    \n        (venv) $ ilab serve INFO 2024-03-02 02:21:11,352 lab.py:201 Using model 'models/ggml-merlinite-7b-lab-Q4_K_M.gguf' with -1 gpu-layers and 4096 max context size. Starting server process After application startup complete see http://127.0.0.1:8000/docs for API. Press CTRL+C to shut down the server.\n\n> NOTE: If multiple ilab clients try to connect to the same InstructLab server\n> at the same time, the 1st will connect to the server while the others will\n> start their own temporary server. This will require additional resources on\n> the host machine.\n\n### \ud83d\udce3 Chat with the model (Optional)\n\nBecause you're serving the model in one terminal window, you will have to\ncreate a new window and re-activate your Python virtual environment to run\nilab chat command:\n\n    \n    \n    source venv/bin/activate ilab chat\n\nBefore you start adding new skills and knowledge to your model, you can check\nits baseline performance by asking it a question such as what is the capital\nof Canada?.\n\n> NOTE: the model needs to be trained with the generated synthetic data to use\n> the new skills or knowledge\n    \n    \n    (venv) $ ilab chat \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 system \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 Welcome to InstructLab Chat w/ GGML-MERLINITE-7B-lab-Q4_K_M (type /h for help) \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f >>b> what is the capital of Canada [S][default] \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 ggml-merlinite-7b-lab-Q4_K_M \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 The capital city of Canada is Ottawa. It is located in the province of Ontario, on the southern banks of the Ottawa River in the eastern portion of southern Ontario. The city serves as the political center for Canada, as it is home to \u2502 \u2502 Parliament Hill, which houses the House of Commons, Senate, Supreme Court, and Cabinet of Canada. Ottawa has a rich history and cultural significance, making it an essential part of Canada's identity. \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 elapsed 12.008 seconds \u2500\u256f >>> [S][default]\n\n## \ud83d\udcbb Creating new knowledge or skills and training the model\n\n### \ud83c\udf81 Contribute knowledge or compositional skills\n\n  1. Contribute new knowledge or compositional skills to your local taxonomy repository.\n\nDetailed contribution instructions can be found in the taxonomy repository.\n\nImportant\n\nThere is a limit to how much content can exist in the question/answer pairs\nfor the model to process. Due to this, only add a maximum of around 2300 words\nto your question and answer seed example pairs in the qna.yaml file.\n\n### \ud83d\udcdc List and validate your new data\n\n  1. List your new data by running the following command:\n    \n        ilab diff\n\n  2. To ensure ilab is registering your new knowledge or skills, you can run the ilab diff command. The following is the expected result after adding the new compositional skill foo-lang:\n    \n        (venv) $ ilab diff compositional_skills/writing/freeform/foo-lang/foo-lang.yaml Taxonomy in /taxonomy/ is valid :)\n\n### \ud83d\ude80 Generate a synthetic dataset\n\nBefore following these instructions, ensure the existing model you are adding\nskills or knowledge to is still running.\n\n  1. To generate a synthetic dataset based on your newly added knowledge or skill set in taxonomy repository, run the following command:\n    \n        ilab generate\n\n> NOTE: \u23f3 This can take from 15 minutes to 1+ hours to complete, depending on\n> your computing resources.\n\n#### Example output\n\n    \n        (venv) $ ilab generate INFO 2024-02-29 19:09:48,804 lab.py:250 Generating model 'ggml-merlinite-7b-lab-Q4_K_M' using 10 CPUs, taxonomy: '/home/username/instructlab/taxonomy' and seed 'seed_tasks.json' 0%|##########| 0/100 Cannot find prompt.txt. Using default prompt. 98%|##########| 98/100 INFO 2024-02-29 20:49:27,582 generate_data.py:428 Generation took 5978.78s\n\nThe synthetic data set will be three files in the newly created generated\ndirectory named generated*.json, test*.jsonl, and train*.jsonl.\n\nNote\n\nIf you want to pickup from where a failed or canceled ilab generate left off,\nyou can copy the generated*.json file into a file named regen.json. regen.json\nwill be picked up at the start of lab generate when available. You should\nremove it when the process is completed.\n\n  2. Verify the files have been created by running the ls generated command.\n    \n        (venv) $ ls generated/ 'generated_ggml-merlinite-7b-lab-0226-Q4_K_M_2024-02-29T19 09 48.json' 'train_ggml-merlinite-7b-lab-0226-Q4_K_M_2024-02-29T19 09 48.jsonl' 'test_ggml-merlinite-7b-lab-0226-Q4_K_M_2024-02-29T19 09 48.jsonl'\n\nOptional: It is also possible to run the generate step against a different\nmodel via an OpenAI-compatible API. For example, the one spawned by ilab serve\nor any remote or locally hosted LLM (e.g. via ollama, LM Studio, etc.). Run\nthe following command:\n\n    \n        ilab generate --endpoint-url http://localhost:8000/v1\n\n### \ud83d\udc69\ud83c\udfeb Train the model\n\nThere are three options to train the model on your synthetic data-enhanced\ndataset.\n\n> Note: Every ilab command needs to be run from within your Python virtual\n> environment.\n\n#### Train the model locally on Linux\n\n    \n    \n    ilab train\n\n> NOTE: \u23f3 This step can potentially take several hours to complete depending\n> on your computing resources. Please stop ilab chat and ilab serve first to\n> free resources.\n\nilab train outputs a brand-new model that can be served in the models\ndirectory called ggml-model-f16.gguf.\n\n    \n    \n    (venv) $ ls models ggml-merlinite-7b-lab-Q4_K_M.gguf ggml-model-f16.gguf\n\n#### Train the model locally on an M-series Mac\n\nTo train the model locally on your M-Series Mac is as easy as running:\n\n    \n    \n    ilab train\n\n> Note: \u23f3 This process will take a little while to complete (time can vary\n> based on hardware and output of ilab generate but on the order of 5 to 15\n> minutes)\n\nilab train outputs a brand-new model that is saved in the <model_name>-mlx-q\ndirectory called adapters.npz (in Numpy compressed array format). For example:\n\n    \n    \n    (venv) $ ls instructlab-merlinite-7b-lab-mlx-q adapters-010.npz adapters-050.npz adapters-090.npz config.json tokenizer.model adapters-020.npz adapters-060.npz adapters-100.npz model.safetensors tokenizer_config.json adapters-030.npz adapters-070.npz adapters.npz special_tokens_map.json adapters-040.npz adapters-080.npz added_tokens.json tokenizer.jso\n\n#### Training the model locally with GPU acceleration\n\nTraining has experimental support for GPU acceleration with Nvidia CUDA or AMD\nROCm. Please see the GPU acceleration documentation for more details. At\npresent, hardware acceleration requires a data center GPU or high-end consumer\nGPU with at least 18 GB free memory.\n\n    \n    \n    ilab train --device=cuda\n\n#### Training the model in the cloud\n\nFollow the instructions in Training.\n\n\u23f3 Approximate amount of time taken on each platform:\n\n  * Google Colab: 5-10 minutes with a T4 GPU\n  * Kaggle: ~30 minutes with a P100 GPU.\n\nAfter that's done, you can play with your model directly in the Google Colab\nor Kaggle notebook. Model trained on the cloud will be saved on the cloud. The\nmodel can also be downloaded and served locally.\n\n### \ud83d\udcdc Test the newly trained model\n\n  * Run the following command to test the model:\n    \n        ilab test\n\n> NOTE: \ud83c\udf4e This step is only implemented for macOS with M-series chips (for\n> now)\n\nThe output from the command will consist of a series of outputs from the model\nbefore and after training.\n\n### \ud83c\udf74 Serve the newly trained model\n\n  1. Stop the server you have running by entering ctrl+c keys in the terminal running the server.\n\n> IMPORTANT:\n\n     * \ud83c\udf4e This step is only implemented for macOS with M-series chips (for now).\n\n     * Before serving the newly trained model you must convert it to work with the ilab cli. The ilab convert command converts the new model into quantized GGUF format which is required by the server to host the model in the ilab serve command.\n\n  2. Convert the newly trained model by running the following command:\n    \n        ilab convert\n\n  3. Serve the newly trained model locally via ilab serve command with the --model-path argument to specify your new model:\n    \n        ilab serve --model-path <New model name>\n\nWhich model should you select to serve? After running the ilab convert\ncommand, a few files and directories are generated. The one you will want to\nserve will end in .gguf and will exist in a directory with the suffix fused-\npt. For example: instructlab-merlinite-7b-lab-mlx-q-fused-pt/ggml-\nmodel-Q4_K_M.gguf\n\n## \ud83d\udce3 Chat with the new model (not optional this time)\n\n  * Try the fine-tuned model out live using the chat interface, and see if the results are better than the untrained version of the model with chat by running the following command:\n    \n        ilab chat -m <New model name>\n\nIf you are interested in optimizing the quality of the model's responses,\nplease see TROUBLESHOOTING.md\n\n## \ud83c\udf81 Submit your new knowledge or skills\n\nOf course, the final step is, if you've improved the model, to open a pull-\nrequest in the taxonomy repository that includes the files (e.g. qna.yaml)\nwith your improved data.\n\n## \ud83d\udcec Contributing\n\nCheck out our contributing guide to learn how to contribute.\n\n## About\n\nCommand-line interface. Use this to chat with the model or train the model\n(training consumes the taxonomy data)\n\n### Resources\n\nReadme\n\n### License\n\nApache-2.0 license\n\n### Code of conduct\n\nCode of conduct\n\n### Security policy\n\nSecurity policy\n\nActivity\n\nCustom properties\n\n### Stars\n\n151 stars\n\n### Watchers\n\n45 watching\n\n### Forks\n\n82 forks\n\nReport repository\n\n## Releases 21\n\nv0.14.1 Latest\n\nApr 30, 2024\n\n\\+ 20 releases\n\n## Packages 0\n\nNo packages published\n\n## Contributors 101\n\n\\+ 87 contributors\n\n## Languages\n\n  * Python 83.3%\n  * Jupyter Notebook 8.2%\n  * Shell 4.9%\n  * Dockerfile 2.4%\n  * Makefile 1.2%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": true}
