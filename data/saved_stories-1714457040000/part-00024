{"aid": "40202120", "title": "Vanishing Gradients in Reinforcement Finetuning of Language Models", "url": "https://machinelearning.apple.com/research/vanishing-gradients-reinforcement", "domain": "machinelearning.apple.com", "votes": 1, "user": "zerojames", "posted_at": "2024-04-29 18:32:04", "comments": 0, "source_title": "Vanishing Gradients in Reinforcement Finetuning of Language Models", "source_text": "Vanishing Gradients in Reinforcement Finetuning of Language Models - Apple\nMachine Learning Research\n\nresearch areaSpeech and Natural Language Processing | conference ICLR\n\ncontent type paper | published April 2024\n\n# Vanishing Gradients in Reinforcement Finetuning of Language Models\n\nAuthorsNoam Razin, Hattie Zhou, Preetum Nakkilan, Josh Susskind, Omid Saremi,\nArwen Bradley, Vimal Thilak, Etai Littwin\n\nView publication\n\nPretrained language models are commonly adapted to comply with human intent\nand downstream tasks via finetuning. The finetuning process involves\nsupervised finetuning (SFT), using labeled samples, and/or reinforcement\nlearning based fine-tuning (RFT) via policy gradient methods, using a\n(possibly learned) reward function. This work highlights an overlooked\noptimization hurdle in RFT: we prove that the expected gradient for an input\nsample (i.e. prompt) vanishes if its reward standard deviation under the model\nis low, regardless of whether the reward mean is near-optimal or not. We then\ndemonstrate the prevalence and detrimental effects of vanishing gradients due\nto low reward standard deviation in an RFT benchmark for language models. In\nparticular, we show that in datasets where samples with low reward standard\ndeviation under the pretrained model are more prevalent, the reward that RFT\nachieves compared to SFT is worse. Controlled experiments and a theoretical\nanalysis further establish that, even in simplified settings, vanishing\ngradients in RFT can lead to extremely slow convergence. Lastly, we explore\nways to overcome vanishing gradients in RFT of language models. We find the\ncommon practice of an initial SFT phase to be the most promising candidate,\nwhich sheds light on its importance in an RFT pipeline. Furthermore, our\nexperiments reveal that a relatively few number of optimization steps of SFT\non a small number of labeled samples suffice, implying that the initial SFT\nphase need not be expensive in terms of compute and data labeling efforts\n\n## Related readings and updates.\n\n### Symbol Guided Hindsight Priors for Reward Learning from Human Preferences\n\nThis paper was accepted at the \"Human in the Loop Learning Workshop\" at\nNeurIPS 2022. Specification of reward functions for Reinforcement Learning is\na challenging task which is bypassed by the framework of Preference Based\nLearning methods which instead learn from preference labels on trajectory\nqueries. These methods, however, still suffer from high requirements of\npreference labels and often would still achieve low reward recovery. We\npresent...\n\nSee paper details\n\n### Rewards Encoding Environment Dynamics Improves Preference-based\nReinforcement Learning\n\nThis paper was accepted at the workshop at \"Human-in-the-Loop Learning\nWorkshop\" at NeurIPS 2022. Preference-based reinforcement learning (RL)\nalgorithms help avoid the pitfalls of hand-crafted reward functions by\ndistilling them from human preference feedback, but they remain impractical\ndue to the burdensome number of labels required from the human, even for\nrelatively simple tasks. In this work, we demonstrate that encoding\nenvironment...\n\nSee paper details\n\n## Discover opportunities in Machine Learning.\n\nOur research in machine learning breaks new ground every day.\n\nWork with us\n\nPrivacy Policy Terms of Use Legal\n\nCopyright \u00a9 2024 Apple Inc. All rights reserved.\n\n", "frontpage": false}
