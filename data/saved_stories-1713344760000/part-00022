{"aid": "40059914", "title": "Learned Structures", "url": "https://nonint.com/2024/03/03/learned-structures/", "domain": "nonint.com", "votes": 2, "user": "lawrencechen", "posted_at": "2024-04-17 02:48:59", "comments": 0, "source_title": "Learned Structures", "source_text": "Learned Structures \u2013 Non_Interactive \u2013 Software & ML\n\nSkip to content\n\nNon_Interactive \u2013 Software & ML\n\nMenu\n\n# Learned Structures\n\nPosted on March 3, 2024 by jbetker\n\nFrom 2019-2021, I was fascinated with neural network architectures. I think a\nlot of researchers in the field were at the time. The transformer paper had\nbeen out for a little while and it was starting to sink in how\ntransformational it was going to be. The general question in the air was: what\nother simple tweaks can we make to greatly improve performance?\n\nAs time has passed, I\u2019ve internally converged on the understanding that there\nare only a few types of architectural tweaks that actually meaningfully impact\nperformance across model scales. These tweaks seem to fall into one of two\ncategories: modifications that improve numerical stability during training,\nand modifications that enhance the expressiveness of a model in learnable\nways.\n\nImproving numerical stability is a bit of a black art. I\u2019m not an expert but\nthose that are remind me of the RF engineers I worked with in my first job.\nThings that fit into this category would include where and how to normalize\nactivations, weight initialization and smoothed non-linearities. I\u2019d love to\ntalk more about this someday.\n\nI wanted to talk about learnable expressiveness in this post. The core idea\nhere is to build structured representations of your data, and let those\nstructures interact in learnable ways. Let\u2019s start by looking at different\nways this currently can happen:\n\nMLPs are the most basic building block of a neural network and provide the\nfoundation of interacting structures: they allow all of the elements of a\nvector to interact with each other through the weights of the neural network.\n\nAttention builds another layer: rather than considering just a single vector\ninteracting with weights, we consider a set of vectors. Through the attention\nlayer, elements from this set can interact with each other.\n\nMixture of Experts adds yet another layer: Rather than considering vectors\ninteracting with a fixed set of weights, we now dynamically select the weights\nto use for other operations based on the values within the vector (and some\nmore weights!)\n\nHopefully you\u2019re seeing the pattern here: in each of the above cases, we add\nan axis by which our activations can affect the end result of the computation\nperformed by our neural network. I have no empirical proof for this, but what\nI think is actually happening here is that as you add these nested structures\ninto the computational graph, you are adding ways for the network to learn in\nstages.\n\nWhy is important to learn in stages? Because we train our neural networks in a\nreally, really dumb way: we optimize the entire parameter space from the\nbeginning of training. This means all of the parameters fight from the very\nbeginning to optimize really simple patterns of the data distribution. 7\nBillion parameters learning that \u201cpark\u201d and \u201cfrisbee\u201d are common words to find\naround \u201cdog\u201d.\n\nThe neat thing about these learned structures is that they\u2019re practically\nuseless in the early training regime. Attention cannot be meaningfully learned\nwhile the network is still learning \u201cblack\u201d from \u201cwhite\u201d. Same with MoE:\nexpert routing amounts to random chance when the network activations are akin\nto random noise. As training progresses, these mechanisms come \u201conline\u201d,\nthough: providing meaningful value just when you need a boost in capacity to\nlearn a more complex layer of the data distribution.\n\nAnyhow, regardless of whether or not my philosophical waxing is correct,\nlearnable structures are probably the most fascinating research direction I\ncan think of in architecture right now. My hunch is that there are additional\nstructures that we can bolt onto our neural networks for another meaningful\nincrease in performance. The main thing to pay attention to is that you are\nnot just re-inventing a type of learned structure that already exists. Like\nMamba. \ud83d\ude42\n\nOne idea along this vein that I had explored before joining OpenAI:\n\nStyleGAN is an image generation model with exceptional fidelity and speed. The\ncatch is that it is an extremely narrow learning framework: It only works when\nyou heavily regularize the dataset you train it on. For example, only photos\nof center-cropped faces, or specific types of churches. If you attempt to\ntrain it on something like LAION quality drops off as you lose the ability to\nmodel the data distribution: it\u2019s just too wide to fit in the parameter space.\nBut here\u2019s the thing: you can think of most images as being made up of several\nmodal components. Maybe a persons face here, a hand there, a tree in the\nbackground. It seems to me that an optimal way to get high generation\nperformance and fidelity would be to train StyleGAN-like things separately\nfrom an image \u201ccomposer\u201d that learns to place the correct StyleGAN over the\ncorrect places in an image to decode. A \u201cmixture of StyleGANs\u201d if you will.\n\nAs a final note: I don\u2019t want to claim the above is novel or anything, just a\ngood idea. I think one of my favorite early applications of this general idea\nis using StyleGAN to fix StableDiffusion faces, like this. I want to try\nsomething like this learned end to end someday!\n\n\u00a9 2024 Non_Interactive \u2013 Software & ML | Powered by Minimalist Blog WordPress Theme\n\n", "frontpage": false}
