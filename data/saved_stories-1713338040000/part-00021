{"aid": "40056756", "title": "The Shift from Models to Compound AI Systems", "url": "https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/", "domain": "bair.berkeley.edu", "votes": 1, "user": "shenli3514", "posted_at": "2024-04-16 20:28:21", "comments": 0, "source_title": "The Shift from Models to Compound AI Systems", "source_text": "The Shift from Models to Compound AI Systems \u2013 The Berkeley Artificial\nIntelligence Research Blog\n\n# The Shift from Models to Compound AI Systems\n\nMatei Zaharia, Omar Khattab, Lingjiao Chen, Jared Quincy Davis, Heather\nMiller, Chris Potts, James Zou, Michael Carbin, Jonathan Frankle, Naveen Rao,\nAli Ghodsi Feb 18, 2024\n\nAI caught everyone\u2019s attention in 2023 with Large Language Models (LLMs) that\ncan be instructed to perform general tasks, such as translation or coding,\njust by prompting. This naturally led to an intense focus on models as the\nprimary ingredient in AI application development, with everyone wondering what\ncapabilities new LLMs will bring. As more developers begin to build using\nLLMs, however, we believe that this focus is rapidly changing: state-of-the-\nart AI results are increasingly obtained by compound systems with multiple\ncomponents, not just monolithic models.\n\nFor example, Google\u2019s AlphaCode 2 set state-of-the-art results in programming\nthrough a carefully engineered system that uses LLMs to generate up to 1\nmillion possible solutions for a task and then filter down the set.\nAlphaGeometry, likewise, combines an LLM with a traditional symbolic solver to\ntackle olympiad problems. In enterprises, our colleagues at Databricks found\nthat 60% of LLM applications use some form of retrieval-augmented generation\n(RAG), and 30% use multi-step chains. Even researchers working on traditional\nlanguage model tasks, who used to report results from a single LLM call, are\nnow reporting results from increasingly complex inference strategies:\nMicrosoft wrote about a chaining strategy that exceeded GPT-4\u2019s accuracy on\nmedical exams by 9%, and Google\u2019s Gemini launch post measured its MMLU\nbenchmark results using a new CoT@32 inference strategy that calls the model\n32 times, which raised questions about its comparison to just a single call to\nGPT-4. This shift to compound systems opens many interesting design questions,\nbut it is also exciting, because it means leading AI results can be achieved\nthrough clever engineering, not just scaling up training.\n\nIn this post, we analyze the trend toward compound AI systems and what it\nmeans for AI developers. Why are developers building compound systems? Is this\nparadigm here to stay as models improve? And what are the emerging tools for\ndeveloping and optimizing such systems\u2014an area that has received far less\nresearch than model training? We argue that compound AI systems will likely be\nthe best way to maximize AI results in the future, and might be one of the\nmost impactful trends in AI in 2024.\n\nIncreasingly many new AI results are from compound systems.\n\n# Why Use Compound AI Systems?\n\nWe define a Compound AI System as a system that tackles AI tasks using\nmultiple interacting components, including multiple calls to models,\nretrievers, or external tools. In contrast, an AI Model is simply a\nstatistical model, e.g., a Transformer that predicts the next token in text.\n\nEven though AI models are continually getting better, and there is no clear\nend in sight to their scaling, more and more state-of-the-art results are\nobtained using compound systems. Why is that? We have seen several distinct\nreasons:\n\n  1. Some tasks are easier to improve via system design. While LLMs appear to follow remarkable scaling laws that predictably yield better results with more compute, in many applications, scaling offers lower returns-vs-cost than building a compound system. For example, suppose that the current best LLM can solve coding contest problems 30% of the time, and tripling its training budget would increase this to 35%; this is still not reliable enough to win a coding contest! In contrast, engineering a system that samples from the model multiple times, tests each sample, etc. might increase performance to 80% with today\u2019s models, as shown in work like AlphaCode. Even more importantly, iterating on a system design is often much faster than waiting for training runs. We believe that in any high-value application, developers will want to use every tool available to maximize AI quality, so they will use system ideas in addition to scaling. We frequently see this with LLM users, where a good LLM creates a compelling but frustratingly unreliable first demo, and engineering teams then go on to systematically raise quality.\n  2. Systems can be dynamic. Machine learning models are inherently limited because they are trained on static datasets, so their \u201cknowledge\u201d is fixed. Therefore, developers need to combine models with other components, such as search and retrieval, to incorporate timely data. In addition, training lets a model \u201csee\u201d the whole training set, so more complex systems are needed to build AI applications with access controls (e.g., answer a user\u2019s questions based only on files the user has access to).\n  3. Improving control and trust is easier with systems. Neural network models alone are hard to control: while training will influence them, it is nearly impossible to guarantee that a model will avoid certain behaviors. Using an AI system instead of a model can help developers control behavior more tightly, e.g., by filtering model outputs. Likewise, even the best LLMs still hallucinate, but a system combining, say, LLMs with retrieval can increase user trust by providing citations or automatically verifying facts.\n  4. Performance goals vary widely. Each AI model has a fixed quality level and cost, but applications often need to vary these parameters. In some applications, such as inline code suggestions, the best AI models are too expensive, so tools like Github Copilot use carefully tuned smaller models and various search heuristics to provide results. In other applications, even the largest models, like GPT-4, are too cheap! Many users would be willing to pay a few dollars for a correct legal opinion, instead of the few cents it takes to ask GPT-4, but a developer would need to design an AI system to utilize this larger budget.\n\nThe shift to compound systems in Generative AI also matches the industry\ntrends in other AI fields, such as self-driving cars: most of the state-of-\nthe-art implementations are systems with multiple specialized components (more\ndiscussion here). For these reasons, we believe compound AI systems will\nremain a leading paradigm even as models improve.\n\n# Developing Compound AI Systems\n\nWhile compound AI systems can offer clear benefits, the art of designing,\noptimizing, and operating them is still emerging. On the surface, an AI system\nis a combination of traditional software and AI models, but there are many\ninteresting design questions. For example, should the overall \u201ccontrol logic\u201d\nbe written in traditional code (e.g., Python code that calls an LLM), or\nshould it be driven by an AI model (e.g. LLM agents that call external tools)?\nLikewise, in a compound system, where should a developer invest resources\u2014for\nexample, in a RAG pipeline, is it better to spend more FLOPS on the retriever\nor the LLM, or even to call an LLM multiple times? Finally, how can we\noptimize an AI system with discrete components end-to-end to maximize a\nmetric, the same way we can train a neural network? In this section, we detail\na few example AI systems, then discuss these challenges and recent research on\nthem.\n\n## The AI System Design Space\n\nBelow are few recent compound AI systems to show the breadth of design\nchoices:\n\nAI System| Components| Design| Results  \n---|---|---|---  \nAlphaCode 2|\n\n  * Fine-tuned LLMs for sampling and scoring programs\n  * Code execution module\n  * Clustering model\n\n| Generates up to 1 million solutions for a coding problem then filters and\nscores them| Matches 85th percentile of humans on coding contests  \nAlphaGeometry|\n\n  * Fine-tuned LLM\n  * Symbolic math engine\n\n| Iteratively suggests constructions in a geometry problem via LLM and checks\ndeduced facts produced by symbolic engine| Between silver and gold\nInternational Math Olympiad medalists on timed test  \nMedprompt|\n\n  * GPT-4 LLM\n  * Nearest-neighbor search in database of correct examples\n  * LLM-generated chain-of-thought examples\n  * Multiple samples and ensembling\n\n| Answers medical questions by searching for similar examples to construct a\nfew-shot prompt, adding model-generated chain-of-thought for each example, and\ngenerating and judging up to 11 solutions| Outperforms specialized medical\nmodels like Med-PaLM used with simpler prompting strategies  \nGemini on MMLU|\n\n  * Gemini LLM\n  * Custom inference logic\n\n| Gemini's CoT@32 inference strategy for the MMLU benchmark samples 32 chain-\nof-thought answers from the model, and returns the top choice if enough of\nthem agree, or uses generation without chain-of-thought if not| 90.04% on\nMMLU, compared to 86.4% for GPT-4 with 5-shot prompting or 83.7% for Gemini\nwith 5-shot prompting  \nChatGPT Plus|\n\n  * LLM\n  * Web Browser plugin for retrieving timely content\n  * Code Interpreter plugin for executing Python\n  * DALL-E image generator\n\n| The ChatGPT Plus offering can call tools such as web browsing to answer\nquestions; the LLM determines when and how to call each tool as it responds|\nPopular consumer AI product with millions of paid subscribers  \nRAG, ORQA, Bing, Baleen, etc|\n\n  * LLM (sometimes called multiple times)\n  * Retrieval system\n\n| Combine LLMs with retrieval systems in various ways, e.g., asking an LLM to\ngenerate a search query, or directly searching for the current context| Widely\nused technique in search engines and enterprise apps  \n  \n## Key Challenges in Compound AI Systems\n\nCompound AI systems pose new challenges in design, optimization and operation\ncompared to AI models.\n\n### Design Space\n\nThe range of possible system designs for a given task is vast. For example,\neven in the simple case of retrieval-augmented generation (RAG) with a\nretriever and language model, there are: (i) many retrieval and language\nmodels to choose from, (ii) other techniques to improve retrieval quality,\nsuch as query expansion or reranking models, and (iii) techniques to improve\nthe LLM\u2019s generated output (e.g., running another LLM to check that the output\nrelates to the retrieved passages). Developers have to explore this vast space\nto find a good design.\n\nIn addition, developers need to allocate limited resources, like latency and\ncost budgets, among the system components. For example, if you want to answer\nRAG questions in 100 milliseconds, should you budget to spend 20 ms on the\nretriever and 80 on the LLM, or the other way around?\n\n### Optimization\n\nOften in ML, maximizing the quality of a compound system requires co-\noptimizing the components to work well together. For example, consider a\nsimple RAG application where an LLM sees a user question, generates a search\nquery to send to a retriever, and then generates an answer. Ideally, the LLM\nwould be tuned to generate queries that work well for that particular\nretriever, and the retriever would be tuned to prefer answers that work well\nfor that LLM.\n\nIn single model development a la PyTorch, users can easily optimize a model\nend-to-end because the whole model is differentiable. However, compound AI\nsystems contain non-differentiable components like search engines or code\ninterpreters, and thus require new methods of optimization. Optimizing these\ncompound AI systems is still a new research area; for example, DSPy offers a\ngeneral optimizer for pipelines of pretrained LLMs and other components, while\nothers systems, like LaMDA, Toolformer and AlphaGeometry, use tool calls\nduring model training to optimize models for those tools.\n\n### Operation\n\nMachine learning operations (MLOps) become more challenging for compound AI\nsystems. For example, while it is easy to track success rates for a\ntraditional ML model like a spam classifier, how should developers track and\ndebug the performance of an LLM agent for the same task, which might use a\nvariable number of \u201creflection\u201d steps or external API calls to classify a\nmessage? We believe that a new generation of MLOps tools will be developed to\ntackle these problems. Interesting problems include:\n\n  * Monitoring: How can developers most efficiently log, analyze, and debug traces from complex AI systems?\n  * DataOps: Because many AI systems involve data serving components like vector DBs, and their behavior depends on the quality of data served, any focus on operations for these systems should additionally span data pipelines.\n  * Security: Research has shown that compound AI systems, such as an LLM chatbot with a content filter, can create unforeseen security risks compared to individual models. New tools will be required to secure these systems.\n\n## Emerging Paradigms\n\nTo tackle the challenges of building compound AI systems, multiple new\napproaches are arising in the industry and in research. We highlight a few of\nthe most widely used ones and examples from our research on tackling these\nchallenges.\n\nDesigning AI Systems: Composition Frameworks and Strategies. Many developers\nare now using \u201clanguage model programming\u201d frameworks that let them build\napplications out of multiple calls to AI models and other components. These\ninclude component libraries like LangChain and LlamaIndex that developers call\nfrom traditional programs, agent frameworks like AutoGPT and BabyAGI that let\nan LLM drive the application, and tools for controlling LM outputs, like\nGuardrails, Outlines, LMQL and SGLang. In parallel, researchers are developing\nnumerous new inference strategies to generate better outputs using calls to\nmodels and tools, such as chain-of-thought, self-consistency, WikiChat, RAG\nand others.\n\nAutomatically Optimizing Quality: DSPy. Coming from academia, DSPy is the\nfirst framework that aims to optimize a system composed of LLM calls and other\ntools to maximize a target metric. Users write an application out of calls to\nLLMs and other tools, and provide a target metric such as accuracy on a\nvalidation set, and then DSPy automatically tunes the pipeline by creating\nprompt instructions, few-shot examples, and other parameter choices for each\nmodule to maximize end-to-end performance. The effect is similar to end-to-end\noptimization of a multi-layer neural network in PyTorch, except that the\nmodules in DSPy are not always differentiable layers. To do that, DSPy\nleverages the linguistic abilities of LLMs in a clean way: to specify each\nmodule, users write a natural language signature, such as user_question ->\nsearch_query, where the names of the input and output fields are meaningful,\nand DSPy automatically turns this into suitable prompts with instructions,\nfew-shot examples, or even weight updates to the underlying language models.\n\nOptimizing Cost: FrugalGPT and AI Gateways. The wide range of AI models and\nservices available makes it challenging to pick the right one for an\napplication. Moreover, different models may perform better on different\ninputs. FrugalGPT is a framework to automatically route inputs to different AI\nmodel cascades to maximize quality subject to a target budget. Based on a\nsmall set of examples, it learns a routing strategy that can outperform the\nbest LLM services by up to 4% at the same cost, or reduce cost by up to 90%\nwhile matching their quality. FrugalGPT is an example of a broader emerging\nconcept of AI gateways or routers, implemented in software like Databricks AI\nGateway, OpenRouter, and Martian, to optimize the performance of each\ncomponent of an AI application. These systems work even better when an AI task\nis broken into smaller modular steps in a compound system, and the gateway can\noptimize routing separately for each step.\n\nOperation: LLMOps and DataOps. AI applications have always required careful\nmonitoring of both model outputs and data pipelines to run reliably. With\ncompound AI systems, however, the behavior of the system on each input can be\nconsiderably more complex, so it is important to track all the steps taken by\nthe application and intermediate outputs. Software like LangSmith, Phoenix\nTraces, and Databricks Inference Tables can track, visualize and evaluate\nthese outputs at a fine granularity, in some cases also correlating them with\ndata pipeline quality and downstream metrics. In the research world, DSPy\nAssertions seeks to leverage feedback from monitoring checks directly in AI\nsystems to improve outputs, and AI-based quality evaluation methods like MT-\nBench, FAVA and ARES aim to automate quality monitoring.\n\n# Conclusion\n\nGenerative AI has excited every developer by unlocking a wide range of\ncapabilities through natural language prompting. As developers aim to move\nbeyond demos and maximize the quality of their AI applications, however, they\nare increasingly turning to compound AI systems as a natural way to control\nand enhance the capabilities of LLMs. Figuring out the best practices for\ndeveloping compound AI systems is still an open question, but there are\nalready exciting approaches to aid with design, end-to-end optimization, and\noperation. We believe that compound AI systems will remain the best way to\nmaximize the quality and reliability of AI applications going forward, and may\nbe one of the most important trends in AI in 2024.\n\nBibTex for this post:\n\n    \n    \n    @misc{compound-ai-blog, title={The Shift from Models to Compound AI Systems}, author={Matei Zaharia and Omar Khattab and Lingjiao Chen and Jared Quincy Davis and Heather Miller and Chris Potts and James Zou and Michael Carbin and Jonathan Frankle and Naveen Rao and Ali Ghodsi}, howpublished={\\url{https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/}}, year={2024} }\n\nSubscribe to our RSS feed.\n\nSpread the word:\n\n", "frontpage": false}
