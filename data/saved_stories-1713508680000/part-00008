{"aid": "40079915", "title": "Deploy Llama3 8B on AWS EC2", "url": "https://www.run.house/examples/llama3-8b-chat-model-inference-aws-ec2", "domain": "run.house", "votes": 2, "user": "brohinn", "posted_at": "2024-04-18 19:36:19", "comments": 0, "source_title": "Deploy Llama3 8B Model from HuggingFace on AWS EC2 | Runhouse", "source_text": "Deploy Llama3 8B Model from HuggingFace on AWS EC2 | Runhouse\n\n  * Runhouse Examples\n  * Examples\n\n    * Llama3 Chat Model Inference on AWS EC2\n    * Llama2 Chat Model Inference on AWS EC2\n    * Stable Diffusion XL 1.0 on AWS EC2\n    * Stable Diffusion XL 1.0 on AWS Inferentia\n    * Langchain RAG App on AWS EC2\n    * Mistral 7B on AWS Inferentia\n    * Mistral 7B with TGI on AWS EC2\n    * Llama2 7B with TGI on AWS Inferentia\n    * Llama2 7B with TGI on AWS EC2\n\n# Deploy Llama3 8B Chat Model Inference on AWS EC2\n\nView on Github\n\nThis example demonstrates how to deploy a LLama3 8B model from Hugging Face on\nAWS EC2 using Runhouse.\n\nMake sure to sign the waiver on the model page so that you can access it.\n\n## Setup credentials and dependencies\n\nOptionally, set up a virtual environment:\n\n$ conda create -n llama3-rh\n\n$ conda activate llama3-rh\n\nInstall the few required dependencies:\n\n$ pip install -r requirements.txt\n\nWe'll be launching an AWS EC2 instance via SkyPilot, so we need to make sure\nour AWS credentials are set up:\n\n$ aws configure\n\n$ sky check\n\nWe'll be downloading the Llama3 model from Hugging Face, so we need to set up\nour Hugging Face token:\n\n$ export HF_TOKEN=<your huggingface token>\n\n## Setting up a model class\n\nWe import runhouse and torch, because that's all that's needed to run the\nscript locally. The actual transformers imports can happen within the\nfunctions that will be sent to the Runhouse cluster; we don't need those\nlocally.\n\nimport runhouse as rh\n\nimport torch\n\nNext, we define a class that will hold the model and allow us to send prompts\nto it. You'll notice this class inherits from rh.Module. This is a Runhouse\nclass that allows you to run code in your class on a remote machine.\n\nLearn more in the Runhouse docs on functions and modules.\n\nclass HFChatModel(rh.Module):\n\ndef __init__(self, model_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n**model_kwargs):\n\nsuper().__init__()\n\n# TODO: Model kwargs ignored right now!\n\nself.model_id, self.model_kwargs = model_id, model_kwargs\n\nself.pipeline = None\n\ndef load_model(self):\n\nimport transformers\n\nself.pipeline = transformers.pipeline(\n\n\"text-generation\",\n\nmodel=self.model_id,\n\nmodel_kwargs=self.model_kwargs,\n\ndevice=\"cuda\",\n\n)\n\ndef predict(self, prompt_text, **inf_kwargs):\n\nif not self.pipeline:\n\nself.load_model()\n\nmessages = [\n\n{\n\n\"role\": \"system\",\n\n\"content\": \"You are a pirate chatbot who always responds in pirate speak!\",\n\n},\n\n{\"role\": \"user\", \"content\": prompt_text},\n\n]\n\nprompt = self.pipeline.tokenizer.apply_chat_template(\n\nmessages, tokenize=False, add_generation_prompt=True\n\n)\n\nterminators = [\n\nself.pipeline.tokenizer.eos_token_id,\n\nself.pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n\n]\n\noutputs = self.pipeline(\n\nprompt,\n\nmax_new_tokens=256,\n\neos_token_id=terminators,\n\ndo_sample=True,\n\ntemperature=0.6,\n\ntop_p=0.9,\n\n)\n\nreturn outputs[0][\"generated_text\"][len(prompt) :]\n\n## Setting up Runhouse primitives\n\nNow, we define the main function that will run locally when we run this\nscript, and set up our Runhouse module on a remote cluster. First, we create a\ncluster with the desired instance type and provider. Our instance_type here is\ndefined as A10G:1, which is the accelerator type and count that we need. We\ncould alternatively specify a specific AWS instance type, such as p3.2xlarge\nor g4dn.xlarge.\n\nLearn more in the Runhouse docs on clusters.\n\nNote\n\nMake sure that your code runs within a if __name__ == \"__main__\": block, as\nshown below. Otherwise, the script code will run when Runhouse attempts to run\ncode remotely.\n\nif __name__ == \"__main__\":\n\ngpu = rh.cluster(\n\nname=\"rh-a10x\", instance_type=\"A10G:1\", memory=\"32+\", provider=\"aws\"\n\n)\n\nNext, we define the environment for our module. This includes the required\ndependencies that need to be installed on the remote machine, as well as any\nsecrets that need to be synced up from local to remote. Passing huggingface to\nthe secrets parameter will load the Hugging Face token we set up earlier.\n\nLearn more in the Runhouse docs on envs.\n\nenv = rh.env(\n\nreqs=[\n\n\"torch\",\n\n\"transformers\",\n\n\"accelerate\",\n\n\"bitsandbytes\",\n\n\"safetensors\",\n\n\"scipy\",\n\n],\n\nsecrets=[\"huggingface\"], # Needed to download Llama3 from HuggingFace\n\nname=\"llama3inference\",\n\nworking_dir=\"./\",\n\n)\n\nFinally, we define our module and run it on the remote cluster. We construct\nit normally and then call get_or_to to run it on the remote cluster. Using\nget_or_to allows us to load the exiting Module by the name llama3-8b-model if\nit was already put on the cluster. If we want to update the module each time\nwe run this script, we can use to instead of get_or_to.\n\nNote that we also pass the env object to the get_or_to method, which will\nensure that the environment is set up on the remote machine before the module\nis run.\n\nremote_hf_chat_model = HFChatModel(\n\ntorch_dtype=torch.bfloat16,\n\n).get_or_to(gpu, env=env, name=\"llama3-8b-model\")\n\n## Calling our remote function\n\nWe can call the predict method on the model class instance if it were running\nlocally. This will run the function on the remote cluster and return the\nresponse to our local machine automatically. Further calls will also run on\nthe remote machine, and maintain state that was updated between calls, like\nself.pipeline.\n\nwhile True:\n\nprompt = input(\n\n\"\\n\\n... Enter a prompt to chat with the model, and 'exit' to exit ...\\n\"\n\n)\n\nif prompt.lower().strip() == \"exit\":\n\nbreak\n\noutput = remote_hf_chat_model.predict(prompt)\n\nprint(\"\\n\\n... Model Output ...\\n\")\n\nprint(output)\n\n  * Deploy Llama3 8B Chat Model Inference on AWS EC2\n  * Setup credentials and dependencies\n  * Setting up a model class\n  * Setting up Runhouse primitives\n  * Calling our remote function\n\n\ud83c\udfc3\u2640\ufe0f Runhouse \ud83c\udfe0\n\n#### Product\n\n  * Runhouse OSS\n  * Runhouse Den\n\n#### Resources\n\n  * Examples\n  * Documentation\n  * Funhouse\n\n#### Company\n\n  * About\n  * Blog\n  * Careers\n\n\u00a9 Runhouse 2024 All Rights ReservedTerms of UsePrivacy Policy\n\n", "frontpage": false}
