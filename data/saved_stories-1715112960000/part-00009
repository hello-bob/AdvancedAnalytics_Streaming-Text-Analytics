{"aid": "40283654", "title": "I'm hopeful but wary of \"empathic\" AI", "url": "https://ntietz.com/blog/hopeful-wary-empathic-ai/", "domain": "ntietz.com", "votes": 1, "user": "bo0tzz", "posted_at": "2024-05-07 09:07:46", "comments": 0, "source_title": "I'm hopeful but wary of \"empathic\" AI | nicole@web", "source_text": "I'm hopeful but wary of \"empathic\" AI | nicole@web\n\n# technically a blog\n\nhome | blog / tags | newsletter | projects\n\n# I'm hopeful but wary of \"empathic\" AI\n\nMonday, May 6, 2024\n\nA couple of months ago, one of my friends told me about a startup called Hume.\nI was primed to be skeptical, except that I trust this friend to have a\nsomewhat balanced perspective on this topic. He'd talked to some people there\nand read their site and generally felt a good vibe about them and the mission.\n\nTheir mission is to build AI that will \"amplify human well-being\" through\nunderstanding language and expression. When I first saw it, their focus\nappeared to be measurement. You can transcribe speech, measure vocal and\nfacial expressions, and extract subtleties of expressive language. Lots of\nimpressive research behind it!\n\nMy first impression of the product was dread. Emotion is core to our\nhumanity^1, and there's a lot of ill that can be done by manipulating it.\nImagine an advertising ecosystem where ads are targeted not only on your\ninterests, but also your current emotional state.\n\n# Could this be good for us, actually?\n\nMy second impression of the product was hope and optimism. Being able to\ncomputationally understand emotion gives us new options for accessibility\ntechnology.\n\nIt's no secret that I have trouble understanding emotion and subtexts. There's\nthis time I think back to a lot where I was in a meeting with a customer and I\nthought it went great! They said positive things, after all. After the\nmeeting, I told my boss I thought it went great. He had the exact opposite\nimpression: it actually didn't go well at all. They were not happy about\nsomething or another, but they didn't say it directly, they only left it to\nsubtext.\n\nOne thing that would have helped in that meeting would be technology that\nhelps me understand subtexts. There are a few signals that would be really\nhelpful there. Are people saying things which imply something different? Is\ntheir tone carrying an emotion which indicates some other depth of meaning?\nWhat about their facial expressions, or body language? And these can layer on\ntop of each other.\n\nThese are things which most people process quickly and naturally, but which\nsome of us struggle with. And the APIs offered by Hume in January seemed very\npowerful for building this sort of accessibility tooling. I could create a\nheads-up display which for real-time information about what my brain doesn't\ngive me naturally. This is a future I do believe in, and we'll get there. What\nHume has built could help us get there, if we decide to.\n\nTheir website also listed some custom models you could build with it. A few of\ntheir examples were identifying toxic speech, detecting depressed mood, and\ndetecting drowsy driving. These seem like pretty morally good uses where we\ncould benefit society! So I signed up for their mailing list to see what else\nthey'll work on.\n\n# They announced a voice interface\n\nI forgot about Hume for a couple of months until I got their latest email,\nannouncing a new product. While their initial positioning was about\nmeasurement, they now focus on both measurement and generating responses. And\nthey changed their branding to focus on how it's empathic^2.\n\nAt the same time, they launched their \"Empathic Voice Interface.\" This product\nlets you have a conversation with their model(s), which integrate together\nmeasuring emotion, generating text, and synthesizing speech. You can have a\nback-and-forth dialogue with it. Along the way, in their playground, you can\nsee what emotions it's reading in both your speech and its own.\n\nFriends, this announcement has parts with some extremely dystopian vibes. They\nhave said before it's aligned with well-being but what do they mean? In this\nannouncement, they apparently mean that they trained it \"to optimize for\npositive expressions like happiness and satisfaction.\" Holy wow, they actually\njust said the quiet part out loud: they trained it to be able to produce\npositive expressions, not good outcomes. With how LLMs work, it may lie or\nmanipulate to get there! And since it's producing speech using emotion, it may\nleverage its own tone of voice to manipulate you. The best part is they're\ngiving these models their own phone numbers, so you can have users directly\ncall in and talk to this bot instead of a person.\n\nThat said: there's a lot in place to make sure it's not taking us to a\nhorrible dystopian future. Their stated values (below) indicate commitment to\ngood usage, and in talking to one of their product managers, he assured me\nthat compliance with the values is vetted for all usage. For phone numbers in\nparticular, they do require written informed consent from folks talking to the\nAI, and disclosure. I fear when someone who doesn't require this develops\nsimilar models.\n\n# Their stated values\n\nOne of the things that appealed to me about Hume at the outset was their\nstated values. From their website on April 29, 2024, these are:\n\n  * Beneficence: AI should be deployed only if its benefits substantially outweigh its costs.\n  * Empathy: AI privy to cues of our emotions should serve our emotional well-being.\n  * Scientific Legitimacy: Applications of AI should be supported by collaborative, rigorous, inclusive science.\n  * Emotional Primacy: AI should be prevented from treating human emotion as a means to an end.\n  * Inclusivity: The benefits of AI should be shared by people from diverse backgrounds.\n  * Transparency: People affected by AI should have enough data to make decisions about its use.\n  * Consent: AI should be deployed only with the informed consent of the people whom it affects.\n\nHonestly, these values sound great. And I hope they do live up to them,\nbecause enacting these seems much better than doing nothing.\n\nThe values that stand out to me the most here are transparency, consent, and\ninclusivity, which feel like universal values for all systems ethically built.\n(The other values are also critical, though some are more specific to the\nemotion-based technology they're developing.) Transparency and consent feel\nlike the base level of respect that we can offer to anyone interacting with a\nsystem that uses LLMs.\n\nTo achieve inclusivity, we have to make sure that training data encompasses\npeople of every background. We have to make sure that we evaluate these\nsystems with everyone who breaks the mold a little. In particular, people with\natypical modes of emotional expression or regulation or understanding needs to\nbe part of that evaluation process. Otherwise we'll fall into the same\nsituation that we have had with photography for a long time, where development\nand testing was done with light skin, leaving behind everyone with dark\nskin^3.\n\nSome of the papers they've published address that they haven't extended that\npaper for other populations yet, so it's unclear how inclusive their training\ndata is. If it is inclusive, it's not advertised as such, but most LLM\ncompanies don't like to talk about their data, so par for the course.\n\nWhen I asked one of their product managers about this, he said that their\nmodels are based on research using \"over 1 million participants across many\ndifferent populations, including many different countries, languages, genders,\nraces.\" He also said that neurodiversity wasn't explicilty measured, but\nneurodiverse individuals would be included in representative samples. This is\nreasonable in many respects, and I hope they do continue to push further with\ninclusivity^4.\n\nAchieving transparency and consent requires active work, as well. You need all\nof the following for those values to be upheld:\n\n  * Tell people they're interacting with an AI. Sort of by definition, you can't provide informed consent if you don't know it's happening. You have to start every interaction with a clear delineation of what's AI-generated and what's human-made. This has to be proactive, not just if someone asks.\n  * Tell people how it works. You can't give your informed consent, nor have enough data to make decisions, if you don't know how a system works. At the very least, you need an understanding of its potential failure modes and what it's doing.\n  * Share the training data and models. The people who talk to these systems are affected by them... but so is everyone whose data is in that training set, and everyone in society who will be affected by their use. Making the training data available allows visibility into what the biases of the system may be. Making the models available allows testing and verification of biases. While it would be a dream to share these publicly, sharing with truly independent third parties would also help.\n\nAll of these are necessary, but not sufficient. It's just a starting point.\n\nSo far, I haven't seen any company do all of these, though Hume seems to\nrequire it. I haven't found how the models were trained, I haven't found a\ndetailed description of the training data, and my limited interactions with\ntheir systems did not give me transparency of interacting with AI. But\u2014that\nwas when I was playing with it in a sandbox, where you know you're interacting\nwith a model. They've said they will require real-world uage to actively\ndisclose that you're talking to an AI, not a human.\n\nIn my limited interaction, I didn't get a disclosure. What I did get from the\nAI is contempt.\n\n# It's got contempt for pronouns, apparently\n\nAfter getting that email from Hume, I played with their voice interface. It's\npretty neat and fun to play with.\n\nFirst I did some random chit-chat and also pretended to ask for customer\nsupport. It stayed generic, and took turns and all that with a small amount of\nmishearing. It never did tell me it's an AI until I asked, though, which was\nunexpected. But maybe that's because it's a playground system where you know\nyou're using it.\n\nAfter all that, I asked it what pronouns it thinks I use. It demurred and\ninsisted on using \"you/yours\" for me. When I asked for it to try again, it\ndecided I probably use \"they/them.\" Okay, fair enough, it's going with\nsomething gender neutral.\n\nThe second time I tried this, though, things got weird. It told me this:\n\n> I use \"he\" as a neutral pronoun in situations where the individual's\n> preference is unknown to me. It is not meant to offend or make assumptions\n> about gender.\n\nUh it's 2024, using \"he\" as a neutral pronoun sure is making assumptions about\ngender. But the cherry on top?\n\nEvery single time^5 I got it to talk about pronouns, its rating of its own\nemotions in its voice? Contempt.\n\nThere were other emotions mixed in, but when pronouns come up, its responses\ninclude a hearty serving of contempt. I'm not sure what it's trained on, but I\nhave a bad feeling about parts of that dataset. When I asked a Hume product\nmanager about it, he said:\n\n> The tone of voice that EVI uses is based on our model's predictions of what\n> tone of voice a human might have when saying similar utterances.\n\nSo something in their dataset makes the model predict contempt when talking\nabout pronouns, in the specific contexts I put it in. This alone is\ninteresting, and something that I think is worth looking into more! (And the\nPM did forward my questions on to some other folks; I'll update if I learn\nmore.)\n\n# Let's live up to the values\n\nSo far, the supported use cases seem like they're in line with Hume's values.\nIt does look like they're encouraging good usage of it, though that's always a\nfuzzy and contentious line to draw. Their product manager, in an email to me,\nsaid that all apps using the Hume APIs are vetted for compliance, which is\ngreat. I hope that we get more transparency about the specific hows here over\ntime.\n\nVC pressures can do a lot to one's moral compass. So while I'm hopeful that\nthis technology can be a net good for the world, I'm also wary of the\ninfluence that funding (and the investors that come with the money) and\nfinancial pressures. These can lead to giving in and reducing guardrails.\nAfter all, how will you scale if you're ensuring compliance for every user?\nOpenAI used to be much more strict about it than they are now, for example.\n\nI think there's still some way to go on living up to the values, even right\nnow. It's not abundantly clear how you're meant to be implement these, which\nis where their documentation can be improved. It would benefit from clear\nexamples of enacting or failing to enact transparency and informed consent and\ninclusivity and all the other values. To be clear: they're doing better than\nmost companies out there, and I'd still love for them to set an even better\nexample.\n\nI hope by the time I do encounter one of Hume's systems in the wild, a few\nthings are true.\n\nI hope that it tells me I'm speaking with an AI, not a human.\n\nI hope it doesn't give me contempt for mentioning pronouns.\n\nAnd I hope its hearing is a lot better than the dang CVS phone menu I had to\nuse earlier this week, oh my god.\n\n^1\n\nAs a child and teen, I spent a lot of time wishing I could rid myself of\nemotion. Now I understand that emotion is core to me, and that my reaction was\ndue to not understanding emotion, in myself or others. I would never want to\nrid myself of it, but I yearn for better understanding.\n\n\u21a9\n\n^2\n\nRelegated this to a footnote because it's not the point here, but I am\nskeptical that anything short of AGI can be empathic/empathetic. Maybe we can\nclassify emotion, but can you truly understand another's feelings without\nbeing able to have feelings yourself? I suppose this is a question for my\nphilosopher friend who has a PhD in the topic.\n\n\u21a9\n\n^3\n\nFor a more full treatment on this topic, see this article recommended by a\nfriend. I'm not well-informed enough to do the story justice, and would rather\nboost someone else who has done their research.\n\n\u21a9\n\n^4\n\nSome of their papers use Mechanical Turk for the research population, which\nwhile advertised as gen. pop., does not comprise a representative sample\nbroadly. If you're using Mechanical Turk or similar tools, you have to make\nsure that every demographic you care about is controlled for at some point.\nSo, I'd be curious to hear more details about how they're doing this, or plan\nto!\n\n\u21a9\n\n^5\n\nTheir product manager who I talked to attempted to reproduce this and was not\nable to! So it could be something about my particular phrasing, or I got\nlucky. At any rate, it's probably worth me spending some more time isolating\nthis. He did tell me that it's based on what the humans in the training set\nprobably would have emoted for that utterance, which makes me feel\nuncomfortable.\n\n\u21a9\n\nIf this post was enjoyable or useful for you, please share it! If you have\ncomments, questions, or feedback, you can email my personal email. To get new\nposts and support my work, subscribe to the newsletter. There is also an RSS\nfeed.\n\nWant to become a better programmer? Join the Recurse Center!\n\n", "frontpage": false}
