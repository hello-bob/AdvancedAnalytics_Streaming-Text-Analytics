{"aid": "40151512", "title": "PyTorch 2.3 Release Blog", "url": "https://pytorch.org/blog/pytorch2-3/", "domain": "pytorch.org", "votes": 2, "user": "nothrowaways", "posted_at": "2024-04-25 00:04:03", "comments": 0, "source_title": "PyTorch 2.3 Release Blog", "source_text": "PyTorch 2.3 Release Blog | PyTorch\n\n  * Get Started\n  * Ecosystem\n\nPyTorch Conference - 2024\n\nSeptember 18-19 in San Francisco\n\nTools\n\nLearn about the tools and frameworks in the PyTorch Ecosystem\n\n  * Edge\n\nAbout PyTorch Edge ExecuTorch\n\n  * Blog\n  * Tutorials\n  * Docs\n\nPyTorch\n\ntorchaudio\n\ntorchtext\n\ntorchvision\n\ntorcharrow\n\nTorchData\n\nTorchRec\n\nTorchServe\n\nPyTorch on XLA Devices\n\n  * Resources\n\nAbout\n\nLearn about PyTorch\u2019s features and capabilities\n\nPyTorch Foundation\n\nLearn more about the PyTorch Foundation.\n\nCommunity\n\nJoin the PyTorch developer community to contribute, learn, and get your\nquestions answered.\n\nCommunity stories\n\nLearn how our community solves real, everyday machine learning problems with\nPyTorch\n\nDeveloper Resources\n\nFind resources and get questions answered\n\nEvents\n\nFind events, webinars, and podcasts\n\nForums\n\nA place to discuss PyTorch code, issues, install, research\n\nModels (Beta)\n\nDiscover, publish, and reuse pre-trained models\n\n  * GitHub\n  * X\n\nApril 24, 2024\n\n# PyTorch 2.3 Release Blog\n\nby Team PyTorch\n\nWe are excited to announce the release of PyTorch\u00ae 2.3 (release note)! PyTorch\n2.3 offers support for user-defined Triton kernels in torch.compile, allowing\nfor users to migrate their own Triton kernels from eager without experiencing\nperformance regressions or graph breaks. Tensor Parallelism improves the\nexperience for training Large Language Models using native PyTorch functions,\nwhich has been validated on training runs for 100B parameter models. As well,\nsemi-structured sparsity implements semi-structured sparsity as a Tensor\nsubclass, with observed speedups of up to 1.6 over dense matrix\nmultiplication.\n\nThis release is composed of 3393 commits and 426 contributors since PyTorch\n2.2. We want to sincerely thank our dedicated community for your\ncontributions. As always, we encourage you to try these out and report any\nissues as we improve 2.3. More information about how to get started with the\nPyTorch 2-series can be found at our Getting Started page.\n\nBeta| Prototype| Performance Improvements  \n---|---|---  \nUser-defined Triton kernels in torch.compile| torch.export adds new API to\nspecify dynamic_shapes| Weight-Only-Quantization introduced into Inductor CPU\nbackend  \nTensor parallelism within PyTorch Distributed| Asynchronous checkpoint\ngeneration  \nSupport for semi-structured sparsity  \n  \n*To see a full list of public feature submissions click here.\n\n## Beta Features\n\n### [Beta] Support for User-defined Triton kernels in torch.compile\n\nAllows for PyTorch code that contains triton kernels to be executed natively\nusing torch.compile. This enables users to migrate code containing triton\nkernels from eager PyTorch to torch.compile without running into performance\nregressions or graph breaks. Native support also creates an opportunity for\nTorch Inductor to precompile the user-defined Triton kernel as well as better\norganize code around the Triton kernel allowing for further optimizations.\n\nYou can find more information about how to utilize user defined Triton kernels\nin torch.compile within this tutorial.\n\n### [Beta] Tensor Parallelism introduces more efficient ways to train LLMs\n\nThe Tensor Parallel API facilitates various tensor manipulations across\nGPUs/hosts and integrates with FSDP for 2D Parallelism (Tensor parallelism\nacross devices + Data Parallelism across hosts). It also offers a low-level\nAPI for constructing higher-level Tensor parallel APIs. This API has been\nvalidated to support the training of transformer models with over 100 billion\nparameters.\n\nYou can find more information on how to utilize this within your workflows\nwithin this tutorial.\n\n### [Beta] Semi-structured sparsity provides users with a way to take\nadvantage of accelerated sparse inference and memory savings\n\ntorch.sparse.SparseSemiStructuredTensor implements semi-structured sparsity as\na Tensor subclass, which have observed speedups of up to 1.6 over dense matrix\nmultiplication.\n\nIn particular it adds:\n\n  * Additional support for quantization composability (mixed dtype, dequant fusion)\n  * Updated cuSPARSELt and CUTLASS kernels\n  * torch.compile support\n\nYou can find more information on how to take advantage of semi-structured\nsparsity here.\n\n## Prototype Features\n\n### [PROTOTYPE] torch.export adds new API to specify dynamic_shapes\n\nYou can now use torch.export.Dim to better represent dynamic shapes by\nenabling developers to specify ranges (min and max values) that can be reused\nacross different input dimensions that are constrained to be equal.\n\nTo learn more about torch.export.Dim as well as how it can be used to express\nmore interesting relationships (such as linear arithmetic expressions) check\nout the tutorial here.\n\n### [PROTOTYPE] Asynchronous checkpoint generation\n\nAsynchronous checkpoint generation allows users to continue their training\nloops while checkpoints are being generated, essentially offloading much of\nthe checkpointing cost.\n\nYou can find out how to utilize this within your own workflows with this\nexample.\n\n## Performance Improvements\n\n### [PROTOTYPE] Weight-Only-Quantization introduced into Inductor CPU backend\n\nPyTorch 2.3 enhances LLM inference performance on torch inductor CPU backend.\nThe project gpt-fast offers a simple and efficient PyTorch native acceleration\nfor transformer text generation with torch.compile. Prior to 2.3 only CUDA\ndevices were supported and this feature enables the CPU counterpart by\nproviding highly optimized kernels for the int4 and int8 weight only\nquantization Linear.\n\nFor more information / how to utilize this feature please refer to the gpt-\nfast README.\n\n## Docs\n\nAccess comprehensive developer documentation for PyTorch\n\nView Docs\n\n## Tutorials\n\nGet in-depth tutorials for beginners and advanced developers\n\nView Tutorials\n\n## Resources\n\nFind development resources and get your questions answered\n\nView Resources\n\n  * PyTorch\n  * Get Started\n  * Features\n  * Ecosystem\n  * Blog\n  * Contributing\n  * Security\n\n  * Resources\n  * Tutorials\n  * Docs\n  * Discuss\n  * GitHub Issues\n  * Brand Guidelines\n\n  * Stay up to date\n\n  * Facebook\n  * Twitter\n  * YouTube\n  * LinkedIn\n  * Mastodon\n\n  * PyTorch Podcasts\n\n  * Spotify\n  * Apple\n  * Google\n  * Amazon\n\n  * Terms\n  * |\n  * Privacy\n\n\u00a9 Copyright The Linux Foundation. The PyTorch Foundation is a project of The\nLinux Foundation. For web site terms of use, trademark policy and other\npolicies applicable to The PyTorch Foundation please see\nwww.linuxfoundation.org/policies/. The PyTorch Foundation supports the PyTorch\nopen source project, which has been established as PyTorch Project a Series of\nLF Projects, LLC. For policies applicable to the PyTorch Project a Series of\nLF Projects, LLC, please see www.lfprojects.org/policies/.\n\n  * Get Started\n  * Ecosystem\n    * PyTorch Conference - 2024\n    * Tools\n  * Edge\n    * About PyTorch Edge\n    * ExecuTorch\n  * Blog\n  * Tutorials\n  * Docs\n    * PyTorch\n    * torchaudio\n    * torchtext\n    * torchvision\n    * torcharrow\n    * TorchData\n    * TorchRec\n    * TorchServe\n    * PyTorch on XLA Devices\n  * Resources\n    * About\n    * PyTorch Foundation\n    * Community\n    * Community stories\n    * Developer Resources\n    * Events\n    * Forum\n    * Models (Beta)\n  * GitHub\n\nTo analyze traffic and optimize your experience, we serve cookies on this\nsite. By clicking or navigating, you agree to allow our usage of cookies. As\nthe current maintainers of this site, Facebook\u2019s Cookies Policy applies. Learn\nmore, including about available controls: Cookies Policy.\n\n", "frontpage": false}
