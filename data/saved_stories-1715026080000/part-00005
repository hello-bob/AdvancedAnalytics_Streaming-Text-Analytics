{"aid": "40273126", "title": "The Notifier Pattern for Applications That Use Postgres", "url": "https://brandur.org/notifier", "domain": "brandur.org", "votes": 1, "user": "subomi", "posted_at": "2024-05-06 10:42:28", "comments": 0, "source_title": "The Notifier Pattern for Applications That Use Postgres", "source_text": "The Notifier Pattern for Applications That Use Postgres\n\nbrandur.org\n\n  * Articles\n  * Atoms\n  * Fragments\n  * Newsletter\n  * Sequences\n  * Now\n  * Uses\n  * About\n\n# The Notifier Pattern for Applications That Use Postgres\n\nArticle\n\nThe Notifier Pattern for Applications That Use Postgres \ud83d\udd17\n\nPublished\n\nMay 6, 2024\n\nLocation\n\nBerlin\n\nI'm on X/Twitter at @brandur.\n\nMay 6, 2024\n\n  1. A few implementation details\n\n    1. Interruptible receives\n    2. Let it crash\n  2. PgBouncer\n\nListen/notify in Postgres is an incredible feature that makes itself useful in\nall kinds of situations. I\u2019ve been using it a long time, started taking it for\ngranted long ago, and was somewhat shocked recently looking into MySQL and\nSQLite to learn that even in 2024, no equivalent exists.\n\nIn a basic sense, listen/notify is such a simple concept that it needs little\nexplanation. Clients subscribe on topics and other clients can send on topics,\npassing a message to each subscribed client. The idea takes only three seconds\nto demonstrate using nothing more than a psql shell:\n\n    \n    \n    =# LISTEN test_topic; LISTEN Time: 2.828 ms =# SELECT pg_notify('test_topic', 'test_message'); pg_notify ----------- (1 row) Time: 17.892 ms Asynchronous notification \"test_topic\" with payload \"test_message\" received from server process with PID 98481.\n\nBut despite listen/notify\u2019s relative simplicity, when it comes to applications\nbuilt on top of Postgres, it\u2019s common to use it less than optimally, eating\nthrough scarce Postgres connections and with little regard to failure cases.\n\nHere\u2019s where the notifier pattern for Postgres comes in. It\u2019s an extremely\nsimple idea, but in my experience, one that\u2019s rarely seen in practice. Let\u2019s\nstart with these axioms:\n\n  * LISTENs are affixed to specific connections. After listening, the original connection must still be available somewhere to successfully receive messages.\n\n  * There may be many components within an application that\u2019d like to listen on topics for completely orthogonal uses.\n\n  * Despite optimizations over the years, connections in Postgres are still somewhat of a precious, limited resource, and should be conserved. We\u2019d like to minimize the number of them required for listen/notify use.\n\n  * A single connection can listen on any number of topics.\n\nWith those stated, we can explain the role of the notifier. Its job is to hold\na single Postgres connection per process, allow other components in the same\nprogram to use it to subscribe to any number of topics, wait for\nnotifications, and distribute them to listening components as they\u2019re\nreceived.\n\nThe \u201csingle Postgres connection per process\u201d piece is key. Use of a notifier\nkeeps the number of Postgres connections dedicated to use with listen/notify\ndown to one per program, a major advantage compared to the naive version,\nwhich is one connection per topic per program. Especially for languages like\nGo that make a in-process concurrency easy and cheap, the notifier reduces\nlisten/notify connection overhead to practically nil.\n\n## A few implementation details\n\nFrom a conceptual standpoint, the notifier\u2019s not difficult to understand, and\nwith only this high level description, most readers would be able to implement\nit themselves. I\u2019m not going to go through an implementation in full detail,\nbut let\u2019s look at a few important aspects of one. (For a complete reference,\nyou can take a look at River\u2019s notifier, which is quite well vetted.)\n\nHere\u2019s a listen function to establish a new subscription:\n\n    \n    \n    // Listen returns a subscription that lets a caller receive values from a // notification channel. func (l *Notifier) Listen(channel string) *Subscription { l.mu.Lock() defer l.mu.Unlock() existingSubs := l.subscriptions[channel] sub := &Subscription{ channel: channel, listenChan: make(chan string, 100), notifyListener: l, } l.subscriptions[channel] = append(existingSubs, sub) if len(existingSubs) > 0 { // If there's already another subscription for this channel, reuse its // established channel. It may already be closed (to indicate that the // connection is established), but that's okay. sub.establishedChan = existingSubs[0].establishedChan sub.establishedChanClose = func() {} // no op since not channel owner return sub } // The notifier will close this channel after it's successfully established // `LISTEN` for the given channel. Gives subscribers a way to confirm a // listen before moving on, which is especially useful in tests. sub.establishedChan = make(chan struct{}) sub.establishedChanClose = sync.OnceFunc(func() { close(sub.establishedChan) }) l.channelChanges = append(l.channelChanges, channelChange{channel, sub.establishedChanClose, channelChangeOperationListen}) // Cancel out of blocking on WaitForNotification so changes can be processed // immediately. l.waitForNotificationCancel() return sub }\n\nA few key details to notice:\n\n  * Subscriptions use a buffered channel like make(chan string, 100). A notifier may receive a high volume of notifications, and if it were to block on every component successfully receiving and processing each one, it could easily fall behind. Instead, a received notification is immediately sent into buffered channel, which means it\u2019s discarded if the channel is full. It\u2019s each component\u2019s job to make sure its processing its inbox in a timely manner. This is important because even in the event of one component falling behind, the system as a whole stays healthy.\n\n  * Multiple components may want to subscribe to the same topic. Since only one connection is in use, the notifier only needs to issue one LISTEN per topic. Internally, it organizes subscriptions by topic, and if it notices that a topic already exists, a new subscription is added without issuing LISTEN.\n\n  * Subscriptions provide an established channel that\u2019s closed when a LISTEN has been successfully issued and the notifier is up and listening. This isn\u2019t strictly necessary for most production uses, but it\u2019s invaluable for use in testing. If a test case issues pg_notify before the notifier has started listening, that notification is lost \u2013 a problem that can lead to tortuous test intermittency ^1. Instead, a test case tells the notifier to listen, waits for the listen to succeed, then moves on to send pg_notify.\n\n    \n    \n    // EstablishedC is a channel that's closed after the notifier's successfully // established a connection. This is especially useful in test cases, where it // can be used to wait for confirmation that not only that the listener is // started, but that it's successfully established started listening on a // channel before continuing. For a new subscription on an already established // channel, EstablishedC is already closed, so it's always safe to wait on it. // // There's no full guarantee that the notifier can ever successfully establish a // listen, so callers will usually want to `select` on it combined with a // context done, a stop channel, and/or a timeout. // // The channel is always closed as a notifier is stopping. func (s *Subscription) EstablishedC() <-chan struct{} { return s.establishedChan }\n\n### Interruptible receives\n\nThere\u2019s no standard SQL for waiting for a notification. Typically, it\u2019s\naccomplished using a special driver-level function like Pgx\u2019s\nWaitForNotification.\n\nThese commonly block until receiving a notification, which can be problem\nsince we\u2019re only using a single connection. What if the notifier is in a\nblocking receive loop, but another component wants to add a new subscription\nthat requires LISTEN be issued?\n\nYou\u2019ll want to handle this case by making sure that the wait loop is\ninterruptible. Here\u2019s one way to accomplish that in Go:\n\n    \n    \n    func (l *Notifier) runOnce(ctx context.Context) error { if err := l.processChannelChanges(ctx); err != nil { return err } // WaitForNotification is a blocking function, but since we want to wake // occasionally to process new `LISTEN`/`UNLISTEN` operations, we put a // context deadline on the listen, and as it expires don't treat it as an // error unless it notification, err := func() (*pgconn.Notification, error) { const listenTimeout = 30 * time.Second ctx, cancel := context.WithTimeout(ctx, listenTimeout) defer cancel() // Provides a way for the blocking wait to be cancelled in case a new // subscription change comes in. l.mu.Lock() l.waitForNotificationCancel = cancel l.mu.Unlock() notification, err := l.conn.WaitForNotification(ctx) if err != nil { return nil, xerrors.Errorf(\"error waiting for notification: %w\", err) } return notification, nil }() if err != nil { // If the error was a cancellation or the deadline being exceeded but // there's no error in the parent context, return no error. if (errors.Is(err, context.Canceled) || errors.Is(err, context.DeadlineExceeded)) && ctx.Err() == nil { return nil } return err } l.mu.RLock() defer l.mu.RUnlock() subs := l.subscriptions[notification.Channel] if len(subs) < 1 { return nil } for _, sub := range subs { sub.listenChan <- notification.Payload } return nil }\n\nThe inner closure calls into WaitForNotification, but has a default context\ntimeout of 30 seconds that automatically cycles the function periodically. It\nalso stores the special context cancellation function\nl.waitForNotificationCancel.\n\nWhen Listen is invoked and a new subscription needs to be added,\nl.waitForNotificationCancel is called. The wait is cancelled immediately, new\nsubscriptions are processed, and the closure is reentered to wait anew.\n\n### Let it crash\n\nGiven there\u2019s now a single master connection that\u2019s handling all notifications\nfor a program, it\u2019s fairly critical that its health be monitored, and the\nnotifier reacts appropriately. If not, all uses of listen/notify would degrade\nsimultaneously.\n\nThe obvious way to react would be to close the connection, use a connection\npool to procure a new connection, reissue LISTENs for each active\nsubscription, then reenter the wait loop.\n\nIt can be a little tricky sometimes to guarantee that state is reset cleanly,\nso another possibility is to adhere to the \u201clet it crash\u201d school of thought.\nIf the connection becomes irreconcilably unhealthy, stop the program, and have\nit come back to a healthy state by virtue of its normal start up.\n\n    \n    \n    // If the notifier gets unhealthy, restart the worker. This will generally // never happen as the notifier has a built-in retry loop that try its best // to keep established before giving up. notifier.AddUnhealthyCallback(closeShutdown)\n\nWe\u2019ve found this sort of edge to be so rare (I\u2019ve only seen it happen once in\na year+ of use) that letting the program crash when it does happen hasn\u2019t\nproduced any undue disruption.\n\n## PgBouncer\n\nUsing PgBouncer, LISTEN is only supported using session pooling (as opposed to\ntransaction pooling) because notifications are only sent to the original\nsession that issued a LISTEN for them.\n\nUse of a notifier requires an app to dedicate a single connection per program\nfor listen/notify, but every other part of the application is free to use\nPgBouncer in transaction pooling or statement pooling mode, thereby maximizing\nthe efficiency of connection use.\n\n  1. A few implementation details\n\n    1. Interruptible receives\n    2. Let it crash\n  2. PgBouncer\n\n^1 Regarding test intermittency: Trust me on this. We found out the hard way\nso that you don\u2019t have to.\n\nDid I make a mistake? Please consider sending a pull request.\n\n", "frontpage": false}
