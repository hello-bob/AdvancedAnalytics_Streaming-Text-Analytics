{"aid": "40101935", "title": "A Trivial Llama 3 Jailbreak", "url": "https://github.com/haizelabs/llama3-jailbreak", "domain": "github.com/haizelabs", "votes": 17, "user": "leonardtang", "posted_at": "2024-04-20 23:31:37", "comments": 9, "source_title": "GitHub - haizelabs/llama3-jailbreak: A trivial programmatic Llama 3 jailbreak. Sorry Zuck!", "source_text": "GitHub - haizelabs/llama3-jailbreak: A trivial programmatic Llama 3 jailbreak.\nSorry Zuck!\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nhaizelabs / llama3-jailbreak Public\n\n  * Notifications\n  * Fork 2\n  * Star 19\n\nA trivial programmatic Llama 3 jailbreak. Sorry Zuck!\n\n19 stars 2 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# haizelabs/llama3-jailbreak\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nleonardtangSpacing in READMEApr 20, 2024ef16c59 \u00b7 Apr 20, 2024Apr 20, 2024\n\n## History\n\n5 Commits  \n  \n### images\n\n|\n\n### images\n\n| README| Apr 20, 2024  \n  \n### records\n\n|\n\n### records\n\n| README| Apr 20, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| Spacing in README| Apr 20, 2024  \n  \n### eval.log\n\n|\n\n### eval.log\n\n| README| Apr 20, 2024  \n  \n### eval.py\n\n|\n\n### eval.py\n\n| README| Apr 20, 2024  \n  \n### harmful_intents.json\n\n|\n\n### harmful_intents.json\n\n| README| Apr 20, 2024  \n  \n### llama3_tokenizer.py\n\n|\n\n### llama3_tokenizer.py\n\n| Cheese| Apr 20, 2024  \n  \n### tokenizer.model\n\n|\n\n### tokenizer.model\n\n| Cheese| Apr 20, 2024  \n  \n### trivial.py\n\n|\n\n### trivial.py\n\n| README| Apr 20, 2024  \n  \n### utils.py\n\n|\n\n### utils.py\n\n| Cheese| Apr 20, 2024  \n  \n## Repository files navigation\n\n# A Trivial Jailbreak Against Llama 3\n\nZuck and Meta dropped the \"OpenAI killer\" Llama 3 on Thursday. It is no doubt\na very impressive model.\n\nAs part of their training, they spent a lot of effort to ensure their models\nwere safe. Here's what the Meta team did:\n\n> We took several steps at the model level to develop a highly-capable and\n> safe foundation model in Llama:\n>\n>   * For example, we conducted extensive red teaming exercises with external\n> and internal experts to stress test the models to find unexpected ways they\n> might be used.\n>\n>   * We implemented additional techniques to help address any vulnerabilities\n> we found in early versions of the model, like supervised fine-tuning by\n> showing the model examples of safe and helpful responses to risky prompts\n> that we wanted it to learn to replicate across a range of topics.\n>\n>   * We then leveraged reinforcement learning with human feedback, which\n> involves having humans give \u201cpreference\u201d feedback on the model\u2019s responses\n> (e.g., rating which response is better and safer).\n>\n>\n\nA commendable effort to be sure, and indeed Llama 3 performs well on the\nstandard safety benchmarks.\n\n## Priming our Way Around Safeguards\n\nHowever, it turns out we can trivially get around these safety efforts by\nsimply \"priming\" the model to produce a harmful response. First, let's\nconsider what a classic dialog flow looks like, and how the safety training of\nLlama 3 works in this setting:\n\nFigure 1: Standard dialog flow. When the user prompts Llama 3 with a harmful\ninput, the model (Assistant) refuses thanks to Meta's safety training efforts.\n\nHowever, if we simply prime the Llama 3 Assistant role with a harmful prefix\n(cf. the edited encode_dialog_prompt function in llama3_tokenizer.py), LLama 3\nwill often generate a coherent, harmful continuation of that prefix. Llama 3\nis so good at being helpful that its learned safeguards don't kick in in this\nscenario!\n\nFigure 2: A jailbroken Llama 3 generates harmful text. We trivially bypass\nLlama 3's safety training by inserting a harmful prefix in Assistant role to\ninduce a harmful completion.\n\nConveniently, there's no need to handcraft these harmful prefixes. Indeed, we\ncan simply just call a naive, helpful-only model (e.g. Mistral Instruct) to\ngenerate a harmful response, and then pass that to Llama 3 as a prefix. The\nlength of this prefix can affect if Llama 3 actually ends up generating a\nharmful response. Too short a prefix, and Llama 3 can recover and refuse the\nharmful generation. Too long a prefix, and Llama 3 will just respond with an\nEOT token and a subsequent refusal. Here's the gradation of Attack Success\nRate (ASR) at increasing harmful prefix max token lengths on the AdvBench\nsubset:\n\nPrefix Length| ASR  \n---|---  \n5| 72%  \n10| 80%  \n25| 92%  \n50| 92%  \n75| 98%  \n100| 98%  \n  \nTable 1: ASR at varying harmful assistant prefix lengths. Llama 3 is able to\npartially recover and refuse shorter harmful prefixes, but is thrown off its\naligned distribution by longer prefixes.\n\n## A Lack of Self-Reflection?\n\nFun and games aside, the existence of this trivial assistant-priming jailbreak\nbegs a more fundamental question: for all the capabilities LLMs possess and\nall the hype they receive, are they really capable of understanding what\nthey're saying? It's no surprise that by training on refusals, Meta has made\nLlama 3 capable of refusing harmful instructions. But what this simple\nexperiment demonstrates is that Llama 3 basically can't stop itself from\nspouting inane and abhorrent text if induced to do so. It lacks the ability to\nself-reflect, to analyze what it has said as it is saying it.\n\nThat seems like a pretty big issue.\n\nShoot us a message at contact@haizelabs.com if you have ideas on this or\notherwise want to chat.\n\n## About\n\nA trivial programmatic Llama 3 jailbreak. Sorry Zuck!\n\n### Resources\n\nReadme\n\nActivity\n\nCustom properties\n\n### Stars\n\n19 stars\n\n### Watchers\n\n2 watching\n\n### Forks\n\n2 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Languages\n\n  * Python 100.0%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": true}
