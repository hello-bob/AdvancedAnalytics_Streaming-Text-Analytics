{"aid": "40058683", "title": "Implment chat, RAG, and tools with ollama, OpenAI directly and with LangChain", "url": "https://willschenk.com/labnotes/2024/programmatically_interacting_with_llms/", "domain": "willschenk.com", "votes": 1, "user": "combray", "posted_at": "2024-04-16 23:35:04", "comments": 0, "source_title": "Programmatically Interacting with LLMS", "source_text": "Programmatically Interacting with LLMS\n\nWill Schenk\n\n  * About\n  * Articles\n  * Howto\n  * Labnotes\n  * Fragments\n  * Search\n\n# Programmatically Interacting with LLMS\n\ndifferent techniques for local, remote, rag and function calling\n\nPublished April 16, 2024\n\nollama, langchain, openai, chat, rag, tools, ai, llms\n\nContents\n\nI decided to sit down over a few days (I'm at a trampoline park right now,\nbeing a money dispensor for the kids to buy slurpees while they tire\nthemselves out) and figure out how to code with LLMs. We're going to look at\nhow to do a single show prompt, multi message chat, RAG Retrieval-Augmented\nGeneration, and tool usage, each with the ollama-js, openai-js, and then\nlangchain with both ollana and openai.\n\nI didn't quite implement every combination, but you get the idea.\n\nOllama JS| OpenAI JS| LangChain - Ollama| LangChain - OpenAI  \n---|---|---|---  \nPrompting| Yes| Yes| Yes| Yes  \nChat| Yes| Yes| Yes  \nRAG| Yes| Yes| Yes  \nTools| Yes| Yes| Yes| Yes  \n  \nThe straightforward code of using the interfaces is shorter, but not\nnecessarily clearer especially with tool calling. It is super cool to be able\nto retarget the LangChain code onto a different provider and it smooths over\nthe differences of implementations. I learned a lot about what you could do by\nlooking through it's documentation. But in practice, there's a lot of overhead\nand classes and things you need to learn which aren't clearly necessary. Seems\nlike a slightly too premature abstraction, but it is overall nice. Worth\nlearning even if you don't end up using.\n\nLangChain is a library that lets you build things on top of LLMs, where it's\nrelatively easy to paper over the difference between each of the\nimplementations and APIs. It also has a bunch of high level concepts in there.\nI like the RecursiveCharacterTextSplitter for example, when I started I just\nsplit it into sentences and this was a more interesting solution.\n\nBuckle up.\n\n## Prompting\n\nHere are examples of a single shot call into the model and returning the\nresponse. We cover basically system prompting and getting the responses back\nfrom the models.\n\n### ollama-js\n\n    \n    \n    1\n\n|\n\n    \n    \n    npm install ollama  \n  \n---|---  \n  \nollama-prompt.js:\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17\n\n|\n\n    \n    \n    import ollama from 'ollama'; const msgs = [ { \"role\": \"system\", \"content\": `You are a creative director at a marketing agency helping clients brain storm interesting ideas.` }, { \"role\": \"user\", content: \"Five cute names for a pet penguin\" } ] const output = await ollama.chat({ model: \"mistral\", messages: msgs }) console.log(output.message.content)  \n  \n---|---  \n      \n    \n    1\n\n|\n\n    \n    \n    node ollama-prompt.js  \n  \n---|---  \n  \n  1. Waddles the Wonder: This name suggests that your pet penguin is a unique and special creature, adding an element of wonder and delight to its personality.\n  2. Peppy the Penguin: Peppy conveys energy, cheerfulness, and friendliness, making it a great name for a playful and charming pet penguin.\n  3. Snowball the Adventurer: This name adds a sense of adventure and excitement to your pet penguin's identity, implying that it's always up for new experiences and adventures.\n  4. Tuxedo Teddy: Naming your pet penguin after its distinctive black-and-white appearance adds a cute and endearing touch, making it feel like a beloved teddy bear.\n  5. Iggy the Ice Explorer: This name suggests that your pet penguin is an intrepid explorer of icy landscapes, adding a sense of adventure and bravery to its personality. It's also a fun play on words, as \"iggy\" could refer to both \"igloo,\" which penguins live in, and the common name for penguins, which begins with the letter I.\n\n### open-ai\n\nWe'll need to get an OPENAI_API_KEY to use this, and I'm going to stick it in\nan .env file so here we go.\n\n    \n    \n    1\n\n|\n\n    \n    \n    npm install openai dotenv  \n  \n---|---  \n  \nopen-ai-prompt.js\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26\n\n|\n\n    \n    \n    import 'dotenv/config' import process from 'node:process' import OpenAI from \"openai\"; const openai = new OpenAI() async function main() { const completion = await openai.chat.completions.create({ messages: [ { role: \"system\", content: `You are a creative director at a marketing agency helping clients brain storm interesting ideas.` }, { role: \"user\", content: \"Five cute names for a pet penguin\" }], model: \"gpt-3.5-turbo\", }); console.log(completion.choices[0].message.content); } main();  \n  \n---|---  \n      \n    \n    1\n\n|\n\n    \n    \n    node open-ai-prompt.js  \n  \n---|---  \n  \n  1. Pebbles\n  2. Frosty\n  3. Waddles\n  4. Pippin\n  5. Chilly\n\n### LangChain\n\nNow we get into the abstractions that LangChain provides. We were doing a\nbunch of this nonsense by hand, but now we get some fancy classes for it.\n\nWe can put all of our business logic in a seperate file and pass our chatModel\ninto it, so later we can wire up different compute environments.\n\nHere we are taking the ChatPromptTemplate, piping that to our chatModel, and\nhaving the result go through the StringOutputParser.\n\nThis code is all the same regardless of what backend we end up using.\n\nlangchain-prompt.js:\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n\n|\n\n    \n    \n    import { ChatPromptTemplate } from \"@langchain/core/prompts\"; import { StringOutputParser } from \"@langchain/core/output_parsers\"; export default async function runPrompt( chatModel, input ) { const prompt = ChatPromptTemplate.fromMessages([ [ \"system\", `You are a creative director at a marketing agency helping clients brain storm interesting ideas.`], [ \"user\", \"{input}\" ], ]); const outputParser = new StringOutputParser(); const llmChain = prompt.pipe(chatModel).pipe(outputParser); const answer = await llmChain.invoke({ input }); return answer }  \n  \n---|---  \n  \n### LangChain Ollama\n\n    \n    \n    1\n\n|\n\n    \n    \n    npm install @langchain/community  \n  \n---|---  \n  \nPass in the ChatOllama model configured to the mistral instance available\nlocally. (If you don't have it, ollama pull mistral)\n\nlangchain-ollama-prompt.js:\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11\n\n|\n\n    \n    \n    import { ChatOllama } from \"@langchain/community/chat_models/ollama\"; import runPrompt from \"./langchain-prompt.js\" const chatModel = new ChatOllama({ baseUrl: \"http://localhost:11434\", // Default value model: \"mistral\", }); const answer = await runPrompt( chatModel, \"Five cute names for a pet penguin\" ); console.log( answer )  \n  \n---|---  \n      \n    \n    1\n\n|\n\n    \n    \n    node langchain-ollama-prompt.js  \n  \n---|---  \n  \n  1. Waddles the Wonder: Named after the tiny waddle steps penguins make, this adorable pet penguin is full of surprises and charms with its unique personality.\n\n  1. Tuxedo Tot: A playful name for your pet penguin that highlights its distinctive black and white tuxedo-like appearance.\n  2. Iggy the Icebird: This creative moniker pays homage to penguins' avian roots as they are often referred to as \"icebirds.\" It adds an element of intrigue, making your pet penguin even more captivating!\n  3. Pippin the Polly: A cute and catchy name for your pet penguin that combines playful alliteration with a nod to their polar habitat.\n  4. Bingo the Brave: This name conveys strength, courage, and adventure - perfect for a curious and adventurous pet penguin!\n\n### LangChain open-ai\n\n    \n    \n    1\n\n|\n\n    \n    \n    npm install @langchain/openai  \n  \n---|---  \n  \nJust use the default ChatOpenAI model, we could choose anything but we chose\nnothing!\n\nlangchain-open-ai-prompt.js:\n\n    \n    \n    1 2 3 4 5 6 7 8\n\n|\n\n    \n    \n    import 'dotenv/config' import { ChatOpenAI } from \"@langchain/openai\"; import runPrompt from \"./langchain-prompt.js\" const chatModel = new ChatOpenAI(); const answer = await runPrompt( chatModel, \"Five cute names for a pet penguin\" ); console.log( answer )  \n  \n---|---  \n      \n    \n    1\n\n|\n\n    \n    \n    node langchain-open-ai-prompt.js  \n  \n---|---  \n  \n  1. Fluffy\n  2. Snowball\n  3. Waddles\n  4. Pebbles\n  5. Chilly\n\n## Chatting\n\nChatting is very similar to prompting, but we're passing the context back to\nthe LLM so that it has a sense of memory. And, context in this sense, is just\nthe entire conversation.\n\n### Node prompt function\n\n    \n    \n    1\n\n|\n\n    \n    \n    npm i readline  \n  \n---|---  \n  \nprompt.js:\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\n\n|\n\n    \n    \n    import readline from 'readline' export default async function promptUser( prompt = \"Enter message: \" ) { const rl = readline.createInterface({ input: process.stdin, output: process.stdout }); return new Promise((resolve, reject) => { rl.question(prompt, (response) => { rl.close(); resolve(response); }); rl.on('SIGINT', () => { rl.close(); reject(new Error('User cancelled the prompt.')); }); }); }  \n  \n---|---  \n  \n### ollama.js\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43\n\n|\n\n    \n    \n    import promptUser from './prompt.js'; import ollama from 'ollama'; const model = 'mistral' const messages = [ { role: \"system\", content: \"You are a helpful AI agent\" } ] while( true ) { const content = await promptUser( \"> \" ); if( content == \"\" ) { console.log( \"Bye\" ); process.exit(0); } messages.push( { role: \"user\", content } ) const response = await ollama.chat( { model, messages, stream: true } ) let cc = 0 let text = \"\" for await (const part of response) { cc = cc + part.message.content.length if( cc > 80 ) { process.stdout.write( \"\\n\" ); cc = part.message.content.length } process.stdout.write( part.message.content ) text = text + part.message.content } process.stdout.write( \"\\n\" ); messages.push( { role: \"assistant\", content: text } ) }  \n  \n---|---  \n  \n### LangChain ollama\n\nChatPromptTemplate, ChatMessageHistory, MessagesPlaceholder... HumanMessage,\nAIMessage, the list goes on. These all encapsulate logic that you'll need\nmaking these completions. There are all sorts of backend implementations of\nthis if you want to have it be something more than a memory store for a single\nshot run, like I'm doing here.\n\nThe gist of it is that you need to keep track of what it being said back and\nforth, make sure that it's all passed into the LLM to get \"the next thing\",\nand there's a bunch of bookkeeping that needs to get done. Once you have these\nconcepts in place, then in theory its easier to built on top of it.\n\nlangchain-ollama-chat.js:\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44\n\n|\n\n    \n    \n    import { ChatOllama } from \"@langchain/community/chat_models/ollama\"; import { HumanMessage, AIMessage } from \"@langchain/core/messages\"; import { ChatMessageHistory } from \"langchain/stores/message/in_memory\"; import { ChatPromptTemplate, MessagesPlaceholder, } from \"@langchain/core/prompts\"; import promptUser from './prompt.js' const chat = new ChatOllama( { modal: 'gemma' } ) const prompt = ChatPromptTemplate.fromMessages([ [ \"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\", ], new MessagesPlaceholder(\"messages\"), ]); const chain = prompt.pipe(chat); const messages = new ChatMessageHistory(); while( true ) { const message = await promptUser( \"> \" ); if( message == \"\" ) { console.log( \"Bye\" ) process.exit(0) } await messages.addMessage( new HumanMessage( message ) ) const responseMessage = await chain.invoke({ messages: await messages.getMessages(), }); await messages.addMessage( responseMessage ) console.log( responseMessage.content ) }  \n  \n---|---  \n  \n### LangChain open-ai\n\nThis is exactly the same as above, except we use\n\n    \n    \n    1\n\n|\n\n    \n    \n    import { ChatOpenAI } from \"@langchain/openai\";  \n  \n---|---  \n  \nNifty!\n\n## RAG\n\nRetrieval-Augmented Generation is both less and way more interesting than I\nthought it was. The idea here is that you preprocess a bunch of documents,\nstick them in and index \u2013 or split them up into a bunch of different parts and\nindex those \u2013 and when the user asks a question you first pull in the relevent\ndocuments and then dump the whole thing into a model and see what it says.\n\nIt's less than you think because it's really just grabbing a bunch of snippets\nof the documents and jamming them into the chat, in a way that seems sort of\ngoofy frankly. \"Answer with this context\" but also \"sloppily copy and paste\nwith wild abandon into a chat window\" and hope for the best. Weirdly, it seems\nto deliver.\n\nIt's more than you think because the embeddings are wild \u2013 somehow, the\nconcepts in the question that you ask are encoded in the same conceptual\nspace, the same semantic space, or whatever the hell these vectors represent \u2013\nand it pulls in similar ideas. This idea of \"close to\", with like a cosine\nfunction, seems so unlikely to actually work when you think about it and seems\nto work almost magically in practice.\n\nOne other thing is that the details are that it's hard to get data. Here's\nwhere I thought that LangChain had some good tooling, around data retraival,\nscraping web sites and parsing PDFs and in general the transform later of the\nnormal EDL.\n\nMy solution: We'll be using the text of Pride and Prejudice for sample data.\n\n    \n    \n    1\n\n|\n\n    \n    \n    wget https://www.gutenberg.org/files/1342/1342-0.txt  \n  \n---|---  \n  \nStep 1) get the data. Step 2) index the data using an embedding model. Step 3)\nwhen the user queries, pass it through the first embedding model. Step 4) take\nthe query, and the resulting documents, and feed them into whatever model you\nwant.\n\n### VectorStore: chromadb\n\nWe need a VectorStore to store our index and then be able to query it.\nEveryone uses chromadb in these demos so we will too.\n\n    \n    \n    1\n\n|\n\n    \n    \n    npm i chromadb @stdlib/nlp-sentencize  \n  \n---|---  \n  \nI'm starting up a temporary instance using ephermal storage which goes away\neverytime you close the window. There are other ways to do this, but this is\nmine.\n\n    \n    \n    1\n\n|\n\n    \n    \n    docker run --rm -it -p 8000:8000 chromadb/chroma  \n  \n---|---  \n  \nchroma-test.js:\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\n\n|\n\n    \n    \n    import { ChromaClient } from \"chromadb\"; const client = new ChromaClient({ path: \"http://localhost:8000\" }); let collections = await client.listCollections(); console.log( \"collections\", collections ); const collection = await client.getOrCreateCollection({ name: \"my_collection\", metadata: { description: \"My first collection\" } }); collections = await client.listCollections(); console.log( \"collections\", collections );  \n  \n---|---  \n  \n### ollama.js\n\n#### import the data into the vector store\n\n    \n    \n    1\n\n|\n\n    \n    \n    ollama pull nomic-embed-text  \n  \n---|---  \n  \nFirst we import the text into our chromadb:\n\nollama-rag-import.js:\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53\n\n|\n\n    \n    \n    import ollama from \"ollama\"; import fs from 'node:fs'; import sentencize from '@stdlib/nlp-sentencize'; import { ChromaClient } from \"chromadb\"; const fileName = \"1342-0.txt\"; const collectionName = \"butterysmooth\"; const embeddingModel = 'nomic-embed-text'; // Load the source file const file = fs.readFileSync( fileName, 'utf8' ) const sentences = sentencize(file) console.log( \"Loaded\", sentences.length, \"sentences\" ) // Setup Chroma const chroma = new ChromaClient({ path: \"http://localhost:8000\" }); await chroma.deleteCollection({ name: collectionName }); console.log( \"Creating collection\", collectionName ); const collection = await chroma.getOrCreateCollection({ name: collectionName, metadata: { \"hnsw:space\": \"cosine\" } }); // Generate the embeddings for( let i = 0; i < sentences.length; i++ ) { const s = sentences[i]; const embedding = (await ollama.embeddings( { model: embeddingModel prompt: sentences[i] })).embedding await collection.add( { ids: [s + i], embeddings: [embedding], metadatas: { source: fileName }, documents: [s] }) if( i % 100 == 0 ) { process.stdout.write(\".\") } } console.log(\"\");  \n  \n---|---  \n  \n#### query using the rag\n\nollama-rag-query.js:\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30\n\n|\n\n    \n    \n    import ollama from \"ollama\"; import { ChromaClient } from \"chromadb\"; const collectionName = \"butterysmooth\" const embeddingModel = 'nomic-embed-text'; const chroma = new ChromaClient({ path: \"http://localhost:8000\" }); const collection = await chroma.getOrCreateCollection({ name: collectionName, metadata: { \"hnsw:space\": \"cosine\" } }); //const query = \"who condescends?\" const query = \"which character is more prideful and why?\" // Generate embedding based upon the query const queryembed = (await ollama.embeddings({ model: embeddingModel, prompt: query })).embedding; const relevantDocs = (await collection.query({ queryEmbeddings: [queryembed], nResults: 10 })).documents[0].join(\"\\n\\n\") const modelQuery = `${query} - Answer that question using the following text as a resource: ${relevantDocs}` //console.log( \"querying with text\", modelQuery ) const stream = await ollama.generate({ model: \"mistral\", prompt: modelQuery, stream: true }); for await (const chunk of stream) { process.stdout.write(chunk.response) } console.log( \"\")  \n  \n---|---  \n      \n    \n    1\n\n|\n\n    \n    \n    node ollama-rag-query.js  \n  \n---|---  \n  \n> Based on the given text, Wickham appears to be the more prideful character\n> among those mentioned. This is evident when it is stated that \"almost all\n> his actions may be traced to pride,\" and that \"pride has often been his best\n> friend.\" Furthermore, he acknowledges this himself when he says, \"It was all\n> pride and insolence.\"\n>\n> The text also suggests that Vanity and Pride are different things, with\n> Pride being more related to one's opinion of oneself, whereas Vanity is\n> concerned with what others think. In Wickham's case, it seems that both his\n> self-opinion and what he would have others think of him are inflated due to\n> pride.\n>\n> Miss Lucas, on the other hand, acknowledges that Mr. Darcy's pride does not\n> offend her as much as it usually would because there is an excuse for it.\n> This implies that she recognizes a distinction between proper and improper\n> pride, suggesting that Mr. Darcy's pride may be more regulated or justified\n> in some way compared to Wickham's.\n\n### LangChain ollama\n\nLets do the same but using all LangChain stuff.\n\n    \n    \n    1\n\n|\n\n    \n    \n    npm i langchain  \n  \n---|---  \n  \nThis is slightly different: we are using chunks of texts instead of sentences,\nand it uses a bulk importing process. Here is an area where I think LangChain\nshines a bit, since all of these document loaders and manipulars are sort of\nlocal knowledge in the machine learning world \u2013 which I don't have \u2013 so it's a\nnice leg up on the base \"lets just split it into sentences dur I guess\" that I\ndid above.\n\nlangchain-ollama-importer.js:\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33\n\n|\n\n    \n    \n    import { OllamaEmbeddings } from \"@langchain/community/embeddings/ollama\" import { Chroma } from \"@langchain/community/vectorstores/chroma\"; import { TextLoader } from \"langchain/document_loaders/fs/text\"; import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\"; const fileName = '1342-0.txt'; const collectionName = 'sofreshandclean'; const embeddingModel = 'nomic-embed-text'; // load the source file const loader = new TextLoader( fileName ); const docs = await loader.load(); //Create a text splitter const splitter = new RecursiveCharacterTextSplitter({ chunkSize:1000, separators: ['\\n\\n','\\n',' ',''], chunkOverlap: 200 }); const output = await splitter.splitDocuments(docs); //Get an instance of ollama embeddings const ollamaEmbeddings = new OllamaEmbeddings({ model: embeddingModel }); // Chroma const vectorStore = await Chroma.fromDocuments( output, ollamaEmbeddings, { collectionName });  \n  \n---|---  \n  \nAnd then the query part:\n\nlangchain-ollama-rag.js\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66\n\n|\n\n    \n    \n    import { OllamaEmbeddings } from \"@langchain/community/embeddings/ollama\" import { Ollama } from \"@langchain/community/llms/ollama\"; import { Chroma } from \"@langchain/community/vectorstores/chroma\"; import { PromptTemplate } from \"@langchain/core/prompts\"; import { StringOutputParser } from \"@langchain/core/output_parsers\" const collectionName = 'sofreshandclean'; const embeddingModel = 'nomic-embed-text'; const llmModel = 'mistral'; const ollamaLlm = new Ollama({ model: llmModel }); const ollamaEmbeddings = new OllamaEmbeddings({ model: embeddingModel }); const vectorStore = await Chroma.fromExistingCollection( ollamaEmbeddings, { collectionName } ); const chromaRetriever = vectorStore.asRetriever(); const userQuestion = \"Which character is more prideful and why?\" const simpleQuestionPrompt = PromptTemplate.fromTemplate(` For following user question convert it into a standalone question {userQuestion}`); const simpleQuestionChain = simpleQuestionPrompt .pipe(ollamaLlm) .pipe(new StringOutputParser()) .pipe(chromaRetriever); const documents = await simpleQuestionChain.invoke({ userQuestion: userQuestion }); //Utility function to combine documents function combineDocuments(docs) { return docs.map((doc) => doc.pageContent).join('\\n\\n'); } //Combine the results into a string const combinedDocs = combineDocuments(documents); const questionTemplate = PromptTemplate.fromTemplate(` You are a ethics professor who is good at answering questions raised by curious students. Answer the below question using the context. Strictly use the context and answer in crisp and point to point. <context> {context} </context> question: {userQuestion} `) const answerChain = questionTemplate.pipe(ollamaLlm); const llmResponse = await answerChain.invoke({ context: combinedDocs, userQuestion: userQuestion }); console.log(llmResponse);  \n  \n---|---  \n  \n> Based on the context provided, Mr. Darcy and Wickham are both proud\n> characters, but their expressions of pride differ. Mr. Darcy's pride relates\n> more to his opinion of himself, while Wickham's pride is intertwined with\n> vanity - what others think of him.\n>\n> Wickham admits that his pride has often led him to be generous and give\n> freely, but it also influenced his actions towards deceit and dishonesty\n> towards others. He mentions stronger impulses than pride as well.\n>\n> Mr. Darcy's pride is noted for making him dismissive of others, specifically\n> Elizabeth Bennet, and it has caused him to act in ways that have mortified\n> those around him. However, his pride also stems from his family lineage,\n> filial pride, which motivates him to maintain the influence and\n> respectability of Pemberley House.\n>\n> It is essential to remember that both characters are complex, and their\n> pride has influenced their actions positively and negatively throughout the\n> narrative.\n\n## Tools\n\nfunctions and tools are a way to get an LLM to reach out to the world to get\nsome more information. This is done by defining a system prompt in a specific\nway that tells the LLM what tools are available, and then it will return a\nresponse in JSON form that we can recognize. Instead of showing it to the user\nthen, we can call our own function, get the data, and spit it back to the LLM\nwhich will then hopefully run with it.\n\nIts not the LLM reaching out to the world, its the LLM asking us for\ninformation in a structured way, which we will then return back it to it as a\nchat message, and it'll keep going.\n\nPreviously, I played around with getting json structured responses from Ollama\nwhen we did the geocoding example. That's only half of the picture.\n\n### Tools\n\nI'm putting the definition of these tools/functions out into seperate files\nsince we'll be reusing them a number of times.\n\nclarifyTool.js\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13\n\n|\n\n    \n    \n    export const clarifyTool = { name: \"clarify\", descriptions: \"Asks the user for clarifying information to feed into a tool\", parameters: { type: \"object\", properties: { information: { type: \"string\", description: \"A descriptions of what further information required\" } } } }  \n  \n---|---  \n  \nweatherTool.js\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21\n\n|\n\n    \n    \n    export const weatherTool = { name: \"get_weather\", description: \"Gets the weather based on a given location and date\", parameters: { type: \"object\", properties: { location: { type: \"string\", description: \"Location or place name\" }, date: { type: \"string\", description: \"Date of the forecast on 'YYYY-MM-DD' format\" } }, required: [ \"location\", \"date\" ] } }  \n  \n---|---  \n  \ndistanceTool.js\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21\n\n|\n\n    \n    \n    export const distanceTool = { name: \"get_distance\", description: \"Gets the driving distance between two locations\", parameters: { type: \"object\", properties: { start: { type: \"string\", description: \"Location or place name where you are starting\" }, destination: { type: \"string\", description: \"Location or place name of destination\" } }, required: [ \"start\", \"destination\" ] } }  \n  \n---|---  \n  \n### ollama.js\n\nThe idea here is that we are going to make a custom prompt that will tell the\nLLM that the tools are available, that they do certain things, and that we\nwant it to return a JSON formatted response that we would, in pricipal, parse,\nact on, and then return text back to the model.\n\nThis prompt needs work, but works fine for single shot queries.\n\nollama-tool-prompt.js:\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n\n|\n\n    \n    \n    export default function makePrompt(tools) { const toolInfo = JSON.stringify( tools, null, \" \" ); return ` You have access to the following tools: {${toolInfo}} You must follow these instructions: You must return valid JSON. Always select one or more of the above tools based on the user query If a tool is found, you must respond in the JSON format matching the following schema: {{ \"tools\": {{ \"tool\": \"<name of the selected tool>\", \"tool_input\": <parameters for the selected tool, matching the tool's JSON schema }} }} If there are multiple tools required, make sure a list of tools are returned in a JSON array. If there is no tool that match the user request, you will respond with empty json. Do not add any additional Notes or Explanations User Query:` }  \n  \n---|---  \n  \nNow we mash this together with our prompt tool and see what happens:\n\nollama-tools.js:\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31\n\n|\n\n    \n    \n    import ollama from 'ollama'; import promptUser from './prompt.js'; import makePrompt from './ollama-tool-prompt.js' import { weatherTool } from './weatherTool.js' import { distanceTool } from './distanceTool.js' const model = 'mistral' const messages = [ { \"role\": \"system\", \"content\": makePrompt( [distanceTool, weatherTool] ) } ] while( true ) { const content = await promptUser( \"> \" ); if( content == \"\" ) { console.log( \"Bye\" ) process.exit(0); } messages.push( { role: \"user\", content } ) const prompt = makePrompt( [distanceTool, weatherTool] ) + content const output = await ollama.generate({ model, prompt }) console.log( output.response ) console.log( JSON.parse( output.response ) ) }  \n  \n---|---  \n  \n> > how far is it to drive from boston to brooklyn { \"tools\": [ { \"tool\":\n> \"get_distance\", \"tool_input\": { \"start\": \"boston\", \"destination\": \"brooklyn\"\n> } } ] }\n\nI'm not happy with this at all, trying to make it handle conversations worked\nvery poorly and I believe that has to do with the quality of the prompt, or\nthat the model that I'm using mistral doesn't have the right sort of knack for\ncalling functions. Or something like that. This needs more work on my part but\nthis is already long enough and we've got plenty more to go!\n\n### openai.js\n\nOpenai is deprecating this for agents instead, which are different than chat\ncompletetions. The advantage that they talk about is being able to run\nmultiple queries in parallel. I did not explore that directly with the\njavascript api.\n\nBut this old way works, and it handles conversations better.\n\nopen-ai-tools.js\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33\n\n|\n\n    \n    \n    import 'dotenv/config' import process from 'node:process' import OpenAI from \"openai\"; import { weatherTool } from './weatherTool.js' import { distanceTool } from './distanceTool.js' const openai = new OpenAI() async function main( content ) { const completion = await openai.chat.completions.create({ messages: [ { role: \"system\", content: `You are a help assistant.` }, { role: \"user\", content }], model: \"gpt-3.5-turbo\", tools: [ { type: \"function\", function: weatherTool}, { type: \"function\", function: distanceTool} ] }); console.log( content ) console.log( JSON.stringify( completion.choices[0].message, null, \" \" ) ); } main( \"how far is the drive from boston to brooklyn\" ) main( \"whats the weather on the north pole\" )  \n  \n---|---  \n  \n> how far is the drive from boston to brooklyn { \"role\": \"assistant\",\n> \"content\": null, \"tool_calls\": [ { \"id\": \"call_SwVitBBZupYaWShMzxRopNGq\",\n> \"type\": \"function\", \"function\": { \"name\": \"get_distance\", \"arguments\":\n> \"{\\\"start\\\":\\\"Boston\\\",\\\"destination\\\":\\\"Brooklyn\\\"}\" } } ] } whats the\n> weather on the north pole { \"role\": \"assistant\", \"content\": null,\n> \"tool_calls\": [ { \"id\": \"call_L4dwKLbaa13ptUqioqMblmrc\", \"type\": \"function\",\n> \"function\": { \"name\": \"get_weather\", \"arguments\": \"{\\\"location\\\":\\\"North\n> Pole\\\",\\\"date\\\":\\\"2023-12-25\\\"}\" } } ] }\n\n### LangChain OllamaFunctions\n\nHere's an example of where LangChains abstractions are helpful. I tries a\nwhole bunch of different things when I was coding this with the JS endpoints\ndirectly, and kept on getting all sorts of malformed JSON responses. (It kept\non adding commentary at the end.) Whatever this is doing behind the hood made\na lot of the problems go away.\n\nlangchain-ollama-functions.js:\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30\n\n|\n\n    \n    \n    import { OllamaFunctions } from \"langchain/experimental/chat_models/ollama_functions\"; import { HumanMessage } from \"@langchain/core/messages\"; import { weatherTool } from \"./weatherTool.js\"; import { distanceTool } from \"./distanceTool.js\"; const model = new OllamaFunctions({ temperature: 0.1, model: \"mistral\", } ) .bind( { functions: [ weatherTool, distanceTool ] } ) let response = await model.invoke([ new HumanMessage({ content: \"What's the weather in Boston?\", }), ]); console.log(response.additional_kwargs); response = await model.invoke([ new HumanMessage({ content: \"How far is it to drive from portland maine to the same city in oregon?\", }), ]); console.log(response.additional_kwargs);  \n  \n---|---  \n  \n> { function_call: { name: 'get_weather', arguments:\n> '{\"location\":\"Boston\",\"date\":\"<current date or desired date>\"}' } }\n>\n> { function_call: { name: 'get_distance', arguments: '{\"start\":\"portland\n> maine\",\"destination\":\"portland oregon\"}' } }\n\n### LangChain OpenAI Tools\n\nFor the previous example, we didn't actually run anything \u2013 we got it to the\npoint only where it was returning the ask for us to run something. Left as an\nexcersize to the reader, the next step would be to get the result, put it back\non the list of previous messages, and keep going. From the point of the LLM,\nit's sort of like another type of conversational participant that isn't the\nassisant nor the user.\n\nBut LangChain tools is actually a subset of their agent framework, which not\nonly lets you assemble a whole bunch of tools togeher but also has a bunch of\nbuilt in ones! Let's use their built in wikipedia tool to see how it works:\n\nlangchain-openai-tools.js:\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43\n\n|\n\n    \n    \n    import 'dotenv/config' import { ChatOpenAI } from \"@langchain/openai\"; import { createToolCallingAgent } from \"langchain/agents\"; import { ChatPromptTemplate } from \"@langchain/core/prompts\"; import { AgentExecutor } from \"langchain/agents\"; import { WikipediaQueryRun } from \"@langchain/community/tools/wikipedia_query_run\"; const llm = new ChatOpenAI({ model: \"gpt-3.5-turbo-0125\", temperature: 0 }); // Prompt template must have \"input\" and \"agent_scratchpad input variables\" const prompt = ChatPromptTemplate.fromMessages([ [\"system\", \"You are a helpful assistant\"], [\"placeholder\", \"{chat_history}\"], [\"human\", \"{input}\"], [\"placeholder\", \"{agent_scratchpad}\"], ]); const wikiTool = new WikipediaQueryRun({ topKResults: 3, maxDocContentLength: 4000, }); const tools = [wikiTool]; const agent = await createToolCallingAgent({ llm, tools, prompt, }); const agentExecutor = new AgentExecutor({ agent, tools, }); const result = await agentExecutor.invoke({ input: \"what is is carl jung most known for?\", }); console.log(result);  \n  \n---|---  \n  \n> { input: 'what is is carl jung most known for?',\n>\n> output: \"Carl Jung is most known for being a Swiss psychiatrist and\n> psychoanalyst who founded analytical psychology. He was a prolific author,\n> illustrator, and correspondent, and his work has been influential in the\n> fields of psychiatry, anthropology, archaeology, literature, philosophy,\n> psychology, and religious studies. Jung is widely regarded as one of the\n> most influential psychologists in history. Some of the central concepts of\n> analytical psychology that he created include individuation, synchronicity,\n> archetypal phenomena, the collective unconscious, the psychological complex,\n> and extraversion and introversion. Jung's work and personal vision led to\n> the establishment of Jung's analytical psychology as a comprehensive system\n> separate from psychoanalysis.\" }\n\nAt this point you can really see the advantages of these higher level\ncomponents.\n\n## Final thoughts\n\nI think I've got a basic handle on the moving pieces. We have a number of\ntechniques at our disposal here: prompt engineering, putting per user data\ninto the prompt itself, and having the LLM call out to various tools during\nthe query itself. RAG is actually bolted onto the side, with the magic\nhappening by splitting the user query into multiple queries, one that looks up\nrelavant data for the query which then injects it into the prompt.\n\nTools themselves also make a good way to get data into the system. Here the\nLLM isn't reasoning about information as such, but calling out to e.g. a\nrelational database to get accurate queries, or to generate an image or run\ncode or what have you. What's nifty about this is it provides a \"llm as the\nprogrammer\" type interface, translating the user queries into a more suitiable\ntechnical phrase that exposes the functionality.\n\nAnalgous to the user interface jump from text prompts to using a graphical\ninterface, this is the language interface to technology.\n\n## References\n\n  1. https://js.langchain.com/docs/get_started/quickstart\n  2. https://js.langchain.com/docs/modules/data_connection\n  3. https://platform.openai.com/docs/quickstart?context=node\n  4. https://docs.trychroma.com/deployment\n  5. https://js.langchain.com/docs/modules/data_connection/vectorstores/#which-one-to-pick\n  6. https://github.com/hacktronaut/ollama-rag-demo\n  7. https://www.deskriders.dev/posts/1702742595-function-calling-ollama-models/\n  8. https://stephencowchau.medium.com/ollama-context-at-generate-api-output-what-are-those-numbers-b8cbff140d95\n\nPreviously\n\nlabnotes\n\nReadability and JSDOM took me a while to get this little code\n\ntook me a while to get this little code 2024-04-11\n\nlabnotes\n\nPreviously\n\nlabnotes\n\nReadability and JSDOM took me a while to get this little code\n\ntook me a while to get this little code 2024-04-11\n\n  * About\n  * Articles\n  * Howto\n  * Labnotes\n  * Fragments\n  * Mastodon\n\n", "frontpage": false}
