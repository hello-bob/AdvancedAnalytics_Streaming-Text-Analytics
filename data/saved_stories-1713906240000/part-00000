{"aid": "40132768", "title": "Air Canada's Chatbot: Why RAG Is Better Than an LLM for Facts", "url": "https://hackaday.com/2024/02/28/air-canadas-chatbot-why-rag-is-better-than-an-llm-for-facts/", "domain": "hackaday.com", "votes": 1, "user": "i13e", "posted_at": "2024-04-23 15:03:19", "comments": 0, "source_title": "Air Canada\u2019s Chatbot: Why RAG Is Better Than An LLM For Facts", "source_text": "Air Canada\u2019s Chatbot: Why RAG Is Better Than An LLM For Facts | Hackaday\n\nSkip to content\n\n# Hackaday\n\n# Air Canada\u2019s Chatbot: Why RAG Is Better Than An LLM For Facts\n\n29 Comments\n\n  * by:\n\nMaya Posch\n\nFebruary 28, 2024\n\nRecently Air Canada was in the news regarding the outcome of Moffatt v. Air\nCanada, in which Air Canada was forced to pay restitution to Mr. Moffatt after\nthe latter had been disadvantaged by advice given by a chatbot on the Air\nCanada website regarding the latter\u2019s bereavement fare policy. When Mr.\nMoffatt inquired whether he could apply for the bereavement fare after\nreturning from the flight, the chatbot said that this was the case, even\nthough the link which it provided to the official bereavement policy page said\notherwise.\n\nThis latter aspect of the case is by far the most interesting aspect of this\ncase, as it raises many questions about the technical details of this chatbot\nwhich Air Canada had deployed on its website. Since the basic idea behind such\na chatbot is that it uses a curated source of (company) documentation and\npolicies, the assumption made by many is that this particular chatbot instead\nused an LLM with more generic information in it, possibly sourced from many\nother public-facing policy pages.\n\nWhatever the case may be, chatbots are increasingly used by companies, but\ninstead of pure LLMs they use what is called RAG: retrieval augmented\ngeneration. This bypasses the language model and instead fetches factual\ninformation from a vetted source of documentation.\n\n## Why LLMs Don\u2019t Do Facts\n\nA core problem with using LLMs and expecting these to answer questions\ntruthfully is that this is not possible, due to how language models work. What\nthese act on is the likelihood of certain words and phrases occurring in\nsequence, but there is no \u2018truth\u2019 or \u2018falsehood\u2019 embedded in their parameter\nweights. This often leads to jarring situations with chatbots such as ChatGPT\nwhere it can appear that the system is lying, changing its mind and generally\nplaying it fast and loose with factual statements.\n\nThe way that this is generally dealt with by LLM companies such as OpenAI is\nby acting on a negative response by the human user to the query by essentially\nrunning the same query through the LLM again, with a few alterations to\nhopefully get a response that the inquisitive user will find more pleasing. It\ncould hereby be argued that in order to know what \u2018true\u2019 and \u2018false\u2019 is some\nlevel of intelligence is required, which is something that LLMs by design are\ncompletely incapable of.\n\nWith the Air Canada case this is more than obvious, as the chatbot confidently\nstated towards Mr. Moffatt among other things the following:\n\n> Air Canada offers reduced bereavement fares if you need to travel because of\n> an imminent death or a death in your immediate family.\n>\n> ...\n>\n> If you need to travel immediately or have already travelled and would like\n> to submit your ticket for a reduced bereavement rate, kindly do so within 90\n> days of the date your ticket was issued by completing our Ticket Refund\n> Application form.\n\nHere the underlined \u2018bereavement fares\u2019 section linked to the official Air\nCanada policy, yet the chatbot had not cited this answer from the official\npolicy document link. An explanation could be that the backing model was\ntrained with the wrong text, or that a wrong internal policy document was\nqueried, but the \u201990 days\u2019 element is as far as anyone can determine \u2013\nincluding the comment section over at the Ars Technica article on the topic \u2013\nnot something that has ever been a policy at this particular airline. What\u2019s\nalso interesting is that Air Canada has now removed the chatbot from its site,\nall of which suggests that it wasn\u2019t using RAG.\n\n## Grounding LLMs With RAGs\n\nLLMs have a lot of disadvantages when it comes to factual information, even\nbeyond the aforementioned. Where an LLM is also rather restrictive is when it\ncomes to keeping up to date with new information, as inside the model new\ninformation will have to be integrated as properly weighed (\u2018trained\u2019)\nparameters, while the old data should be removed or updated. Possibly a whole\nnew model has to be trained from fresh training data, all of which makes\nrunning an LLM-based chatbot computationally and financially expensive to run.\n\nIn a run-down by IBM Research they go over many of these advantages and\ndisadvantages and why RAGs make sense for any situation where you not only\nwant to be able to trust a provided answer, but also want to be able to check\nthe sources. This \u2018grounding\u2019 of an LLM means effectively bypassing it and\nrunning the system more like a traditional Internet search engine, although\nthe LLM is still used to add flavor text and the illusion of a coherent\nconversation as it provides more flexibility than a chatbot using purely\nstatic scripts.\n\nFacebook\u2019s retrieval augmented generation system, featuring a pre-trained\nretriever (Query Encoder + Document Index) with a pre-trained sequence to\nsequence (seq2seq) model (Generator) to find the most appropriate (top-K)\ndocuments from which the output is generated. (Credit: Piktus et al., 2020)\n\nThe idea of using these more traditional methods with LLMs to keep them from\ngoing off the rails was first pitched by Meta (n\u00e9e Facebook) in a 2020 paper,\nin which they used a neural network-based retriever to access information in a\nvector index of Wikipedia. This supports a range of different types of\nqueries, including questions, fact verification and generating trivia\nquestions. The retriever component thus only has to be trained to be able to\nfind specific information in the prepared documents, which immediately adds\nthe ability to verify the provided information using these documents rather\nthan rely on a highly opaque parameterized model.\n\nIn the IBM provided example, they paint a scenario where an employee asks a\nnumber of questions to a company chatbot, which pulls up the employee\u2019s HR\nfiles, checks available vacation days, matches the request against company\npolicies and combines the resulting information into a response. Of course, in\nthe Facebook paper it is noted that RAG-enhanced LLMs are still very much\ncapable of \u2018hallucinating\u2019 and need \u2018fine-tuning\u2019 to keep them in line. On the\nbright side, a result of using RAG is that sources can be provided and linked,\nso that said employee can then check those to verify that the response was\ncorrect.\n\n## LLMs Are Still Dumb\n\nThe problematic part with chatbots is that unless they have been carefully\nscripted by a human being (with QA validating their work), they are bound to\nmess up. With pure LLM-based chatbots this is beyond question, as the\nresponses provided range between plausible to completely delusional. Grounding\nLLMs with RAG reduces the amount of made-up nonsense, but in the absence of\nany intelligence and comprehension of what the algorithm generates as a\nresponse, there also cannot be any accountability.\n\nThat is, the accountability (and liability) shifts to the entity which opted\nto put the chatbot in place, as was succinctly and rightfully demonstrated in\nMoffatt v. Air Canada. In the end no matter how advanced or complex the system\nand its algorithms are, the liability remains with the human element in\ncharge. As the Civil Resolution Tribunal\u2019s judge who presided over the case\nstates in the ruling: \u201cIt should be obvious to Air Canada that it is\nresponsible for all the information on its website. It makes no difference\nwhether the information comes from a static page or a chatbot.\u201d\n\nIn light of such a case, a company should strongly question whether there is\nany conceivable benefit to having a chatbot feature on their public-facing\nwebsite rather than a highly capable search functionality which could still\nuse natural language processing to provide more relevant search results, but\nwhich leaves the linked results to human-written and human-validated documents\nas the authoritative response. For both Air Canada and Mr. Moffatt such a\nsystem would have been a win-win and this whole unpleasant business could have\nbeen avoided.\n\nPosted in Artificial Intelligence, Current Events, Featured, News, Original\nArt, SliderTagged chatbot, large language model, retrieval augmented\ngeneration\n\n## 29 thoughts on \u201cAir Canada\u2019s Chatbot: Why RAG Is Better Than An LLM For\nFacts\u201d\n\n  1. yet another bruce says:\n\nFebruary 28, 2024 at 7:29 am\n\nIf Watson is the retrieval engine does that mean Sherlock is the LLM \u2013 or is\nit Moriarty?\n\nReport comment\n\nReply\n\n    1. S O says:\n\nMarch 2, 2024 at 8:24 pm\n\nSherlock, Moriarty is the hallucination.\n\nReport comment\n\nReply\n\n  2. Maave says:\n\nFebruary 28, 2024 at 7:34 am\n\nReal citations are great. The Bing AI still hallucinates on me but now it\ntakes seconds to verify. But we\u2019ll be in deep doodoo when the AI starts citing\nAI-written articles\n\nReport comment\n\nReply\n\n    1. S O says:\n\nMarch 2, 2024 at 8:28 pm\n\nFirstly, they already do. Secondly you\u2019re missing the underlying problem.\nThese systems are not capable of being truthful, or checking the veracity of a\nstatement. Even if layers are added that try to do this, these facts remain.\n\nAt the end of the day these are search engines that regurgitate statistically\nlikely sentences.\n\nReport comment\n\nReply\n\n  3. PWalsh says:\n\nFebruary 28, 2024 at 7:46 am\n\nThe article doesn\u2019t address the most interesting part of the incident: in the\ncourt case, Air Canada\u2019s defense was that the chatbot is responsible for its\nactions. The court decided otherwise \u2013 quite rightly in my view.\n\nOne problem with the current crop of AI is that there\u2019s no real way to debug\nit. You can\u2019t set breakpoints in the ANN\u2019s and trace the execution as it\nanswers a prompt, looking for where the program deviates from your mental\nmodel of what it should be doing. (Which is how we debug computer programs\ncurrently.)\n\nAnother problem is that, even if we *could* set a breakpoint in specific\nANN\u2019s, there\u2019s no way to interpret what it\u2019s doing. I don\u2019t mean interpret\nwhether it\u2019s correct or not, I mean interpret *anything* about the process \u2013\nthe LLM is completely opaque front to back and there\u2019s no meaningful\ninformation in any of the internal workings.\n\nAn even more insidious problem is that people don\u2019t want the LLM to report\nreality, they want a modified LLM that tilts reality towards their political\nideal. This was thrown into the public spotlight recently when Google\u2019s AI\nwould refuse to generate an image of a white couple (by prompt), but had no\nproblem generating a black couple. Asking for an image of a pope, or viking,\nor Nazi soldier had hilarious results.\n\nAnd finally, I\u2019ve recently been hobby-researching the amount of disinformation\non the internet. Setting aside mass media lies and hallucinations for a\nmoment, I\u2019ve been looking over the comments attached to news reports for some\nnews aggregators I follow. Setting aside the shit-posting, the number of\npeople who post simply incorrect facts is nothing short of astounding! Taking\nEV\u2019s as an example, the amount of incorrect information responses is huge, all\nof which are easily debunked by a quick search.\n\n(Such as that EVs catch fire easily. Statistically speaking, ICE vehicles are\nmuch more likely than EVs to catch fire.)\n\nWe\u2019re in Plato\u2019s cave, and don\u2019t know it. It\u2019s no wonder that LLM AI\u2019s are\nunreliable.\n\nReport comment\n\nReply\n\n    1. Foldi-One says:\n\nFebruary 28, 2024 at 8:39 am\n\n>Air Canada\u2019s defense was that the chatbot is responsible for its actions. Got\nto love that argument... Nothing is ever our fault!\n\nNow if they were pointing to their technical outsourcing company as the guilty\nparty, assuming one was involved... I\u2019d then agree its not your companies\nfault \u2013 though you should treat your customer as if you were at fault and\nexpect the same from the tech company that made your systems..\n\nReport comment\n\nReply\n\n      1. Ostracus says:\n\nFebruary 28, 2024 at 9:37 am\n\n\u201d Got to love that argument... Nothing is ever our fault! \u201d\n\nSo machines are learning our habits? Big surprise there.\n\nReport comment\n\nReply\n\n    2. MacGyverS2000 says:\n\nFebruary 28, 2024 at 9:06 am\n\n\u201cAir Canada\u2019s defense was that the chatbot is responsible for its actions.\u201d\n\nAC: Well, obviously the plane wanted to crash itself into the mountainside...\nwe\u2019re not responsible for the plane\u2019s actions.\n\nReport comment\n\nReply\n\n    3. S O says:\n\nMarch 2, 2024 at 8:31 pm\n\nDeferred responsibility is literally one of the main goals of corporate\ninvestment in \u201cAI\u201d, right next to reducing labour to unpaid migrant interns\nwho need the job to stay in the country.\n\nReport comment\n\nReply\n\n    4. Dude says:\n\nMarch 3, 2024 at 3:27 pm\n\n>Taking EV\u2019s as an example, the amount of incorrect information responses is\nhuge, all of which are easily debunked by a quick search.\n\nBut do you then verify THAT isn\u2019t the nonsense? The trouble is, a lot of\npeople don\u2019t make a difference between a corporate written advertisement\npieces, sensationalist clickbait journalism, and original sources like\nscientific papers and publications when judging the reliability of information\nsources online. Heck, kids these days figure that tiktok is a reliable source\nof information.\n\nTake greencarreports for example. They link to their own previous articles as\nsources for claims in their articles, which then refer to other news sources,\nwhich refer to other sources... They\u2019re playing a game of broken telephone\nwhere you can pull off multi-level spin on any piece of information by subtly\nchanging the message to keep a positive outlook on whatever you WANT to be\ntrue.\n\nReport comment\n\nReply\n\n    5. Dude says:\n\nMarch 3, 2024 at 3:52 pm\n\n>(Such as that EVs catch fire easily. Statistically speaking, ICE vehicles are\nmuch more likely than EVs to catch fire.)\n\nThat\u2019s abuse of statistics. Whether one type of car catches fire more often\nstatistically speaking can have nothing to do with the ease of setting it on\nfire. That\u2019s because the mechanisms for doing so, the circumstances and even\nthe sample groups used for the statistics aren\u2019t comparable.\n\nFor example, did you know that the third most common reason why cars burn is\nbecause people set them on fire on purpose? Now, are people commonly in the\nhabit of torching their new expensive EVs? Are there many old clunker EVs with\nelectrical problems (second most common reason for car fires) on the roads?\nRemember that the market is rapidly expanding, so the age distribution of EVs\nis heavily skewed towards younger cars, which obviously have fewer problems.\n\nTurns out, it isn\u2019t such a simple thing to fact-check by a google search \u2013\nunless you just want to dig up some meaningless factoid to back yourself up\nwith.\n\nReport comment\n\nReply\n\n      1. Dude says:\n\nMarch 3, 2024 at 4:08 pm\n\nThe most common reason why cars burn of course is fuel leaks, but there as\nwell you have the apples to oranges comparison between different demographics\nof vehicles and vehicle owners IF you were to compare ICE vs. EV on statistics\nalone.\n\nThe real question would go something along the lines of: \u201cIf you put a\nscrewdriver through a car\u2019s fuel tank, or an EV\u2019s battery, to simulate a crash\nsituation of some sort \u2013 which one is more likely to go up in flames?\u201d.\n\nYour mileage will vary according to the test criteria you set.\n\nReport comment\n\nReply\n\n      2. David Hawley says:\n\nMarch 5, 2024 at 1:40 am\n\nAs we all know, correlation isn\u2019t causation. If you don\u2019t know why, you don\u2019t\nknow anything.\n\nReport comment\n\nReply\n\n  4. Peter Gransee says:\n\nFebruary 28, 2024 at 7:48 am\n\nWell written article! Succintly captures why so many people in the AI field\nare focused on various systems to keep LLMs on the rails.\n\nReport comment\n\nReply\n\n  5. Steven Clark says:\n\nFebruary 28, 2024 at 7:53 am\n\nI don\u2019t get the customized LLM thing. You need the whole Large Language to get\nthe starting Model right? So how would anyone expect it to stick to the later\ntraining data?\n\nReport comment\n\nReply\n\n    1. hmmmmm..... says:\n\nFebruary 28, 2024 at 9:35 am\n\nyou hire employees with their lifetime of experience. You provide them with\nyour companies SOP and other materials of pertinence. Despite their\neducational or vocational histories you expect them to confine their actions\nand interactions to the acceptable protocols provided them. This is how you\nwould expect a chatbot with a LLM backend and appropriate RAG to \u201cstick to the\nlater training data\u201d The LLM functions as \u201chow to understand:\u201d and the RAG\nprovides appropriate responses. Easy Peasy....when it works\n\nReport comment\n\nReply\n\n    2. PWalsh says:\n\nFebruary 28, 2024 at 10:57 am\n\nIt takes a great deal of compute power to build an LLM. I read an online\nestimate that it takes about $75 million to do the initial run.\n\nThen the LLAMA database leaked online a year ago, and about 10 years of\nimprovement happened in the next 2 months due to open source contributions.\n\nOne of the results was that, given a trained LLM you can add new model\ninformation fairly cheaply. A beefy laptop running over a weekend would add\nnew information to the LLM.\n\nI don\u2019t know the mechanism, but probably the feedback learning rate is bumped\nup in this case, so that the new information is learned more strongly than the\noriginal information was learned. Or it could be that basic text is so complex\nthat it takes most of the effort, and that once you have the basic text the\namount of new information above and beyond the syntax and grammar is so little\nthat it can be learned more quickly.\n\nThis was one of the potential benefits of the LLM systems: many companies have\ntranscribed call logs of customer service requests, you can feed all of that\ninto an LLM and train it over a weekend, and you\u2019d have a chatbot that knows\nthe answers to most of your customer service requests.\n\nReport comment\n\nReply\n\n      1. S O says:\n\nMarch 2, 2024 at 8:35 pm\n\nYou can\u2019t actually add new data easily, that\u2019s not how the OSS models were\ndeveloped. A trained model is like a video encoded using the lowest possible\nquality settings to save space, you can\u2019t just shovel more in without risking\nartifacts that are self-compounding.\n\nReport comment\n\nReply\n\n  6. Adrian says:\n\nFebruary 28, 2024 at 9:39 am\n\nIn other news, Klarna (a buy now, pay later racket) just yesterday boasted\nthat their AI chat bot handles 2/3rd of all support requests, which let them\nsack 700 employees.\n\nWhat could possibly go wrong?\n\nReport comment\n\nReply\n\n    1. Dan says:\n\nFebruary 28, 2024 at 3:40 pm\n\nIf we\u2019re lucky, the LLM will warn people they\u2019re being scammed!\n\nWorse case, it lies to customers, which is probably what was already\nhappening.\n\nReport comment\n\nReply\n\n    2. spaceminions says:\n\nFebruary 29, 2024 at 6:18 am\n\nLet\u2019s build AI to use for skilled labor so that humans have to do the menial\nstuff! Wait, why does that sound wrong...\n\nReport comment\n\nReply\n\n  7. Hirudinea says:\n\nFebruary 28, 2024 at 11:57 am\n\nThis is hardly surprising, remember Air Canada\u2019s unofficial motto, \u201cWe\u2019re not\nsatisfied until you\u2019re not satisfied.\u201d\n\nReport comment\n\nReply\n\n  8. TG says:\n\nMarch 1, 2024 at 3:19 pm\n\nArtificial intelligence isn\u2019t. It\u2019s not thinking. There\u2019s no reason or logic\nhappening, it\u2019s just an illusion. All it can create is either inane,\noverwrought reddit-post chatter or bad art that all looks the same, like\nsomething you\u2019d buy wholesale to hang on the wall in an AirBnB. Absolute turd\nof a technology, way overhyped and overleveraged.\n\nReport comment\n\nReply\n\n    1. physiii says:\n\nMarch 1, 2024 at 9:16 pm\n\nIt could easily create your comment though? Or make it much better probably...\n\nArtificial chicken is not actual chicken. Artificial intelligence is not\nactual intelligence. What is your point about its not thinking? It\u2019s\nartificially thinking.\n\nReport comment\n\nReply\n\n      1. S O says:\n\nMarch 2, 2024 at 8:39 pm\n\nIt\u2019s not. As the article points out multiple times, and references sources on,\nLLM models do not think, and cannot evaluate input or output, and are\n*designed* that way. Why? Because it turns out that brute forcing the\nappearance of thought is easier than building the real thing and these tools\ncan be used to make money and fire more employees.\n\nHow do the work then? It\u2019s statistics, literally the million monkeys on\ntypewriters analogy.\n\nReport comment\n\nReply\n\n        1. physiii says:\n\nMarch 3, 2024 at 6:52 pm\n\nYes the monkey analogy. I can assure you have no idea how they work. Current\nresearch papers coming out are still discovering new things.\n\nI use RAG in production level applications touching thousands of customers. I\nbuilt RAG and inference in many apps and things are accelerating.\n\nHackaday seems to put out very watered down AI articles then people hit the\ncomments with the most regurgitated lame arguments \u201cmonkeys on a keyboard\u201d or\n\u201cjust predicts next word\u201d This is an article I would expect maybe day one for\na new AI intern who has no prior experience. As in clearly they aren\u2019t adding\nreal world experience. In fact the kinda of thing that separates us from the\nAI.\n\nReport comment\n\nReply\n\n          1. Ryan X says:\n\nMarch 3, 2024 at 8:46 pm\n\nYou can say this, and you can appeal to your own authority and all that, but\nLLMs do NOT think (and they cannot), they can NOT generate anything novel,\n\nYES, they do an extremely roided-up version of \u2018just predicting next word,\u2019\n\nand honestly you\u2019re not doing the field any favors by spitting random \u201cyou\ndon\u2019t know what you\u2019re talking abouts.\u201d It\u2019s uh, pretty well understood how\nthese things work. Anyone who suggests it\u2019s not either actually doesn\u2019t know\nwhat they\u2019re talking about, or has a motive to muddy the waters.\n\nThe tech press is already doing a great job with breathlessly reporting\nwhatever non-factual AI crap crosses their desk, for clicks. Please stick to\nthe truth.\n\nReport comment\n\n          2. physiii says:\n\nMarch 4, 2024 at 3:33 am\n\nRyan, you misunderstood me. I assure him he doesn\u2019t understand because if you\nread the papers and watch the videos of people leading the field and they\ndon\u2019t completely understand why it works so well.\n\nThey have decent theories, much more thoughtful and useful than the contrarian\nsaying it\u2019s just next word. And thing you don\u2019t get is it may be those things\nyou are saying but it sounds so silly and useless to reduce it to that.\n\nLike me saying the mind is just the brain. Sure...but there\u2019s a lot going on\nin between that we don\u2019t understand and the mind is still pretty useful even\nthough it\u2019s just the brain.\n\nAnytime someone said wow look at what the mind can do, and I just say well you\nknow it\u2019s just the brain doing that?\n\nSame thing when someone Saya S AI isn\u2019t thinking or just predicting next word.\nNo kidding, its artificially thinking! And artificially thinking in a system\none kind of way which is basically regurgitation. You are in a bubble seeing\nsilly AI propaganda videos instead of actual researchers.\n\nI\u2019ve been reading hackaday for many years but the AI thing is funny to watch\non here because it\u2019s starts to show everyone\u2019s age. Like someone who loves the\ntypewriter and doesn\u2019t want to see the good in a computer. It\u2019s just a\ntypewriter with internet right?\n\nAgain because I now it\u2019ll be said a million more times. No it\u2019s not thinking\nbecause it\u2019s not actual biological tissue and a million other reasons but you\nare totally missing the point. That it is very useful. Not to you because I\u2019m\nsure you use it with disdain. But to many others including the next\ngeneration, it will be.\n\nReport comment\n\n  9. Matt says:\n\nMarch 2, 2024 at 8:09 am\n\nDid Air Canada build their own, or used a software provider?? And same\nquestion for Klarna? Trying to see if there\u2019s less risk on build vs buy!\n\nReport comment\n\nReply\n\n### Leave a ReplyCancel reply\n\nPlease be kind and respectful to help make the comments section excellent.\n(Comment Policy)\n\nThis site uses Akismet to reduce spam. Learn how your comment data is\nprocessed.\n\n# Search\n\n# Never miss a hack\n\nFollow on facebook Follow on twitter Follow on youtube Follow on rss Contact\nus\n\n# Subscribe\n\n# If you missed it\n\n  * ## Programming Ada: First Steps On The Desktop\n\n9 Comments\n\n  * ## The Hunt For MH370 Goes On With Barnacles As A Lead\n\n29 Comments\n\n  * ## MXM: Powerful, Misused, Hackable\n\n19 Comments\n\n  * ## VCF East 2024 Was Bigger And Better Than Ever\n\n16 Comments\n\n  * ## Microsoft Killed My Favorite Keyboard, And I\u2019m Mad About It\n\n61 Comments\n\nMore from this category\n\n# Our Columns\n\n  * ## Slicing And Dicing The Bits: CPU Design The Old Fashioned Way\n\n4 Comments\n\n  * ## Hackaday Links: April 21, 2024\n\n12 Comments\n\n  * ## The Long And The Short Of It\n\n8 Comments\n\n  * ## Hackaday Podcast Episode 267: Metal Casting, Plasma Cutting, And A Spicy 555\n\n2 Comments\n\n  * ## This Week In Security: Putty Keys, Libarchive, And Palo Alto\n\n3 Comments\n\nMore from this category\n\n# Search\n\n# Never miss a hack\n\nFollow on facebook Follow on twitter Follow on youtube Follow on rss Contact\nus\n\n# Subscribe\n\n# If you missed it\n\n  * ## Programming Ada: First Steps On The Desktop\n\n9 Comments\n\n  * ## The Hunt For MH370 Goes On With Barnacles As A Lead\n\n29 Comments\n\n  * ## MXM: Powerful, Misused, Hackable\n\n19 Comments\n\n  * ## VCF East 2024 Was Bigger And Better Than Ever\n\n16 Comments\n\n  * ## Microsoft Killed My Favorite Keyboard, And I\u2019m Mad About It\n\n61 Comments\n\nMore from this category\n\n# Categories\n\n# Our Columns\n\n  * ## Slicing And Dicing The Bits: CPU Design The Old Fashioned Way\n\n4 Comments\n\n  * ## Hackaday Links: April 21, 2024\n\n12 Comments\n\n  * ## The Long And The Short Of It\n\n8 Comments\n\n  * ## Hackaday Podcast Episode 267: Metal Casting, Plasma Cutting, And A Spicy 555\n\n2 Comments\n\n  * ## This Week In Security: Putty Keys, Libarchive, And Palo Alto\n\n3 Comments\n\nMore from this category\n\n# Recent comments\n\n  * localroger on Your Smart TV Does 4K, Surround Sound, Denial-of-service...\n  * Sword on Programming Ada: First Steps On The Desktop\n  * Kjw on Going Canadian: The Rise And Fall Of Novell\n  * abjq on The Hunt For MH370 Goes On With Barnacles As A Lead\n  * Cheese Whiz on AI Camera Only Takes Nudes\n  * ThoriumBR on Your Smart TV Does 4K, Surround Sound, Denial-of-service...\n  * Pat on Programming Ada: First Steps On The Desktop\n  * J. Cook on Your Smart TV Does 4K, Surround Sound, Denial-of-service...\n  * Jouni on Reverse Engineering The Quansheng Hardware\n  * alexd on NASA\u2019s Voyager 1 Resumes Sending Engineering Updates To Earth\n\n# Now on Hackaday.io\n\n  * Kristall Wang liked Wire ECM Machine.\n  * Kristall Wang liked A Simple Electrochemical Machining Project.\n  * kvolle liked Strain Wave Gear with Timing Belts.\n  * kvolle liked Custom Articulated Joint.\n  * Bhuvan Prakash liked Quantum Computer.\n  * Bhuvan Prakash liked Your Secret Weapon for Wireless Networks-Wardriver.\n  * Burst Fade Haircuts has added a new project titled Best Agrochemical company.\n  * CanHobby.ca has updated the project titled Blink Sketch for the ATTiny10-LED from CanHobby.\n  * RunnerPack liked Run Dropbox on Raspberry Pi.\n  * mike_wendt has followed a list.\n\n  * Home\n  * Blog\n  * Hackaday.io\n  * Tindie\n  * Hackaday Prize\n  * Video\n  * Submit A Tip\n  * About\n  * Contact Us\n\n# Never miss a hack\n\nFollow on facebook Follow on twitter Follow on youtube Follow on rss Contact\nus\n\n# Subscribe to Newsletter\n\nCopyright \u00a9 2024 | Hackaday, Hack A Day, and the Skull and Wrenches Logo are Trademarks of Hackaday.com | Privacy Policy | Terms of Service | Digital Services Act Powered by WordPress VIP\n\nBy using our website and services, you expressly agree to the placement of our\nperformance, functionality and advertising cookies. Learn more\n\nLoading Comments...\n\n", "frontpage": false}
