{"aid": "40143621", "title": "The AI Revolution Is Crushing Thousands of Languages", "url": "https://www.theatlantic.com/technology/archive/2024/04/generative-ai-low-resource-languages/678042/", "domain": "theatlantic.com", "votes": 2, "user": "jyunwai", "posted_at": "2024-04-24 12:36:23", "comments": 0, "source_title": "The AI Revolution Is Crushing Thousands of Languages", "source_text": "The AI Revolution Is Crushing Thousands of Languages - The Atlantic\n\n## More From Artificial Intelligence\n\n## More From Artificial Intelligence\n\nExplore This Series\n\n  * ### It\u2019s the End of the Web as We Know It\n\nJudith DonathBruce Schneier\n\n  * ### The AI Revolution Is Crushing Thousands of Languages\n\nMatteo Wong\n\n  * ### Why a Cognitive Scientist Put a Head Cam on His Baby\n\nSarah Zhang\n\n  * ### AI Has Lost Its Magic\n\nIan Bogost\n\nTechnology\n\n# The AI Revolution Is Crushing Thousands of Languages\n\nEnglish is the internet\u2019s primary tongue\u2014a fact that may have unexpected\nconsequences as generative AI becomes central to daily life.\n\nBy Matteo Wong\n\nIllustration by Matteo Giuseppe Pani. Source: Getty.\n\nApril 12, 2024\n\nRecently, Bonaventure Dossou learned of an alarming tendency in a popular AI\nmodel. The program described Fon\u2014a language spoken by Dossou\u2019s mother and\nmillions of others in Benin and neighboring countries\u2014as \u201ca fictional\nlanguage.\u201d\n\nThis result, which I replicated, is not unusual. Dossou is accustomed to the\nfeeling that his culture is unseen by technology that so easily serves other\npeople. He grew up with no Wikipedia pages in Fon, and no translation programs\nto help him communicate with his mother in French, in which he is more fluent.\n\u201cWhen we have a technology that treats something as simple and fundamental as\nour name as an error, it robs us of our personhood,\u201d Dossou told me.\n\nThe rise of the internet, alongside decades of American hegemony, made English\ninto a common tongue for business, politics, science, and entertainment. More\nthan half of all websites are in English, yet more than 80 percent of people\nin the world don\u2019t speak the language. Even basic aspects of digital\nlife\u2014searching with Google, talking to Siri, relying on autocorrect, simply\ntyping on a smartphone\u2014have long been closed off to much of the world. And now\nthe generative-AI boom, despite promises to bridge languages and cultures, may\nonly further entrench the dominance of English in life on and off the web.\n\nScale is central to this technology. Compared with previous generations,\ntoday\u2019s AI requires orders of magnitude more computing power and training\ndata, all to create the humanlike language that has bedazzled so many users of\nChatGPT and other programs. Much of the information that generative AI\n\u201clearns\u201d from is simply scraped from the open web. For that reason, the\npreponderance of English-language text online could mean that generative AI\nworks best in English, cementing a cultural bias in a technology that has been\nmarketed for its potential to \u201cbenefit humanity as a whole.\u201d Some other\nlanguages are also well positioned for the generative-AI age, but only a\nhandful: Nearly 90 percent of websites are written in just 10 languages\n(English, Russian, Spanish, German, French, Japanese, Turkish, Portuguese,\nItalian, and Persian).\n\nSome 7,000 languages are spoken in the world. Google Translate supports 133 of\nthem. Chatbots from OpenAI, Google, and Anthropic are still more constrained.\n\u201cThere\u2019s a sharp cliff in performance,\u201d Sara Hooker, a computer scientist and\nthe head of Cohere for AI, a nonprofit research arm of the tech company\nCohere, told me. \u201cMost of the highest-performance [language] models serve\neight to 10 languages. After that, there\u2019s almost a vacuum.\u201d As chatbots,\ntranslation devices, and voice assistants become a crucial way to navigate the\nweb, that rising tide of generative AI could wash out thousands of Indigenous\nand low-resource languages such as Fon\u2014languages that lack sufficient text\nwith which to train AI models.\n\n\u201cMany people ignore those languages, both from a linguistic standpoint and\nfrom a computational standpoint,\u201d Ife Adebara, an AI researcher and a\ncomputational linguist at the University of British Columbia, told me. Younger\ngenerations will have less and less incentive to learn their forebears\u2019\ntongues. And this is not just a matter of replicating existing issues with the\nweb: If generative AI indeed becomes the portal through which the internet is\naccessed, then billions of people may in fact be worse off than they are\ntoday.\n\nAdebara and Dossou, who is now a computer scientist at Canada\u2019s McGill\nUniversity, work with Masakhane, a collective of researchers building AI tools\nfor African languages. Masakhane, in turn, is part of a growing, global effort\nracing against the clock to create software for, and hopefully save, languages\nthat are poorly represented on the web. In recent decades, \u201cthere has been\nenormous progress in modeling low-resource languages,\u201d Alexandra Birch, a\nmachine-translation researcher at the University of Edinburgh, told me.\n\nIn a promising development that speaks to generative AI\u2019s capacity to\nsurprise, computer scientists have discovered that some AI programs can\npinpoint aspects of communication that transcend a specific language. Perhaps\nthe technology could be used to make the web more aware of less common\ntongues. A program trained on languages for which a decent amount of data are\navailable\u2014English, French, or Russian, say\u2014will then perform better in a\nlower-resourced language, such as Fon or Punjabi. \u201cEvery language is going to\nhave something like a subject or a verb,\u201d Antonios Anastasopoulos, a computer\nscientist at George Mason University, told me. \u201cSo even if these manifest\nthemselves in very different ways, you can learn something from all of the\nother languages.\u201d Birch likened this to how a child who grows up speaking\nEnglish and German can move seamlessly between the two, even if they haven\u2019t\nstudied direct translations between the languages\u2014not moving from word to\nword, but grasping something more fundamental about communication.\n\nRead: The end of foreign-language education\n\nBut this discovery alone may not be enough to turn the tide. Building AI\nmodels for low-resource languages is painstaking and time-intensive. Cohere\nrecently released a large language model that has state-of-the-art performance\nfor 101 languages, of which more than half are low-resource. That leaves about\n6,900 languages to go, and this effort alone required 3,000 people working\nacross 119 countries. To create training data, researchers frequently work\nwith native speakers who answer questions, transcribe recordings, or annotate\nexisting text, which can be slow and expensive. Adebara spent years curating a\n42-gigabyte training data set for 517 African languages, the largest and most\ncomprehensive to date. Her data set is 0.4 percent of the size of the largest\npublicly available English training data set. OpenAI\u2019s proprietary\ndatabases\u2014the ones used to train products such as ChatGPT\u2014are likely far\nlarger.\n\nMuch of the limited text readily available in low-resource languages is of\npoor quality\u2014itself badly translated\u2014or limited use. For years, the main\nsources of text for many such low-resource languages in Africa were\ntranslations of the Bible or missionary websites, such as those from Jehovah\u2019s\nWitnesses. And crucial examples for fine-tuning AI, which has to be\nintentionally created and curated\u2014data used to make a chatbot helpful, human-\nsounding, not racist, and so on\u2014are even rarer. Funding, computing resources,\nand language-specific expertise are frequently just as hard to come by.\nLanguage models can struggle to comprehend non-Latin scripts or, because of\nlimited training examples, to properly separate words in low-resource-language\nsentences\u2014not to mention those without a writing system.\n\nThe trouble is that, while developing tools for these languages is slow going,\ngenerative AI is rapidly overtaking the web. Synthetic content is flooding\nsearch engines and social media like a kind of gray goo, all in hopes of\nmaking a quick buck.\n\nMost websites make money through advertisements and subscriptions, which rely\non attracting clicks and attention. Already, an enormous portion of the web\nconsists of content with limited literary or informational merit\u2014an endless\nocean of junk that exists only because it might be clicked on. What better way\nto expand one\u2019s audience than to translate content into another language with\nwhatever AI program comes up on a Google search?\n\nRead: Prepare for the textpocalypse\n\nThose translation programs, already of sometimes questionable accuracy, are\nespecially bad with low-resourced languages. Sure enough, researchers\npublished preliminary findings earlier this year that online content in such\nlanguages was more likely to have been (poorly) translated from another\nsource, and that the original material was itself more likely to be geared\ntoward maximizing clicks, compared with websites in English or other higher-\nresource languages. Training on large amounts of this flawed material will\nmake products such as ChatGPT, Gemini, and Claude even worse for low-resource\nlanguages, akin to asking someone to prepare a fresh salad with nothing more\nthan a pound of ground beef. \u201cYou are already training the model on incorrect\ndata, and the model itself tends to produce even more incorrect data,\u201d Mehak\nDhaliwal, a computer scientist at UC Santa Barbara and one of the study\u2019s\nauthors, told me\u2014potentially exposing speakers of low-resource languages to\nmisinformation. And those outputs, spewed across the web and likely used to\ntrain future language models, could create a feedback loop of degrading\nperformance for thousands of languages.\n\nImagine \u201cyou want to do a task, and you want a machine to do it for you,\u201d\nDavid Adelani, a DeepMind research fellow at University College London, told\nme. \u201cIf you express this in your own language and the technology doesn\u2019t\nunderstand, you will not be able to do this. A lot of things that simplify\nlives for people in economically rich countries, you will not be able to do.\u201d\nAll of the web\u2019s existing linguistic barriers will rise: You won\u2019t be able to\nuse AI to tutor your child, draft work memos, summarize books, conduct\nresearch, manage a calendar, book a vacation, fill out tax forms, surf the\nweb, and so on. Even when AI models are able to process low-resource\nlanguages, the programs require more memory and computational power to do so,\nand thus become significantly more expensive to run\u2014meaning worse results at\nhigher costs.\n\nAI models might also be void of cultural nuance and context, no matter how\ngrammatically adept they become. Such programs long translated \u201cgood morning\u201d\nto a variation of \u201csomeone has died\u201d in Yoruba, Adelani said, because the same\nYoruba phrase can convey either meaning. Text translated from English has been\nused to generate training data for Indonesian, Vietnamese, and other languages\nspoken by hundreds of millions of people in Southeast Asia. As Holy Lovenia, a\nresearcher at AI Singapore, the country\u2019s program for AI research, told me,\nthe resulting models know much more about hamburgers and Big Ben than local\ncuisines and landmarks.\n\nIt may already be too late to save some languages. As AI and the internet make\nEnglish and other higher-resource languages more and more convenient for young\npeople, Indigenous and less widely spoken tongues could vanish. If you are\nreading this, there is a good chance that much of your life is already lived\nonline; that will become true for more people around the world as time goes on\nand technology spreads. For the machine to function, the user must speak its\nlanguage.\n\nBy default, less common languages may simply seem irrelevant to AI, the web,\nand, in turn, everyday people\u2014eventually leading to abandonment. \u201cIf nothing\nis done about this, it could take a couple of years before many languages go\ninto extinction,\u201d Adebara said. She is already witnessing languages she\nstudied as an undergraduate dwindle in their usage. \u201cWhen people see that\ntheir languages have no orthography, no books, no technology, it gives them\nthe impression that their languages are not valuable.\u201d\n\nRead: AI is exposing who really has power in Silicon Valley\n\nHer own work, including a language model that can read and write in hundreds\nof African languages, aims to change that. When she shows speakers of African\nlanguages her software, they tell her, \u201c\u2018I saw my language in the technology\nyou built; I wasn\u2019t expecting to see it there,\u2019\u201d Adebara said. \u201c\u2018I didn\u2019t know\nthat some technology would be able to understand some part of my language,\u2019\nand they feel really excited. That makes me also feel excited.\u201d\n\nSeveral experts told me that the path forward for AI and low-resource\nlanguages lies not only in technical innovation, but in just these sorts of\nconversations: not indiscriminately telling the world it needs ChatGPT, but\nasking native speakers what the technology can do for them. They might benefit\nfrom better voice recognition in a local dialect, or a program that can read\nand digitize non-Roman script, rather than the all-powerful chatbots being\nsold by tech titans. Rather than relying on Meta or OpenAI, Dossou told me, he\nhopes to build \u201ca platform that is appropriate and proper to African languages\nand Africans, not trying to generalize as Big Tech does.\u201d Such efforts could\nhelp give low-resource languages a presence on the internet where there was\nalmost none before, for future generations to use and learn from.\n\nToday, there is a Fon Wikipedia, although its 1,300 or so articles are about\ntwo-thousandths of the total on its English counterpart. Dossou has worked on\nAI software that does recognize names in African languages. He translated\nhundreds of proverbs between French and Fon manually, then created a survey\nfor people to tell him common Fon sentences and phrases. The resulting French-\nFon translator he built has helped him better communicate with his mother\u2014and\nhis mother\u2019s feedback on those translations has helped improve the AI program.\n\u201cI would have needed a machine-translation tool to be able to communicate with\nher,\u201d he said. Now he is beginning to understand her without machine\nassistance. A person and their community, rather than the internet or a piece\nof software, should decide their native language\u2014and Dossou is realizing that\nhis is Fon, rather than French.\n\nMatteo Wong is an associate editor at The Atlantic.\n\n", "frontpage": false}
