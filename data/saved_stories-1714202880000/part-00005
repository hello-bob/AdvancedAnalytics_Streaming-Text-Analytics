{"aid": "40173754", "title": "Re-converging control flow on Nvidia GPUs \u2013 What went wrong, and how we fixed it", "url": "https://www.collabora.com/news-and-blog/blog/2024/04/25/re-converging-control-flow-on-nvidia-gpus/", "domain": "collabora.com", "votes": 1, "user": "my123", "posted_at": "2024-04-26 20:18:40", "comments": 0, "source_title": "Re-converging control flow on NVIDIA GPUs - What went wrong, and how we fixed it", "source_text": "Re-converging control flow on NVIDIA GPUs - What went wrong, and how we fixed\nit\n\n  * About\n\n    * Who we are\n\n    * Our expertise\n\n    * Our work\n\n    * Open Source\n\n    * Our ecosystem\n\n  * Services\n\n    * Guide\n\n    * Train\n\n    * Build\n\n    * Integrate\n\n    * Optimize\n\n    * Maintain\n\n  * Industries\n\n    * Automotive\n\n    * Digital TV\n\n    * Silicon\n\n    * OEM\n\n    * VR/AR\n\n  * News & Blog\n  * Careers\n  * Contact\n\n  * About\n  * Services\n  * Industries\n  * News & Blog\n  * Careers\n  * Contact\n\n  * +44 (0)1223 362967\n  * +1 514 667 2499\n  * contact@collabora.com\n\n  * Home\n  * News & Blog\n  * Blog\n\n## Re-converging control flow on NVIDIA GPUs - What went wrong, and how we\nfixed it\n\nFaith Ekstrand April 25, 2024\n\nShare this post:\n\nReading time: 22 minutes\n\nIn a recent update to NVK, I landed support in Mesa for the\nVK_KHR_shader_maximal_reconvergence and\nVK_KHR_shader_subgroup_uniform_control_flow extensions. Properly implementing\nthese required a good bit of compiler engineering. My first attempt at\nimplementing control flow re-convergence from last year was fundamentally\nflawed and we had to throw it away entirely. This is the story of what went\nwrong and how we fixed it.\n\n### What is re-convergence?\n\nModern 3D rendering and compute APIs like Vulkan are built around shaders,\nlittle programs that control various aspects of the rendering process. For\ninstance, vertex shaders take the raw geometry data from the client and output\na set of per-vertex attributes. One of those per-vertex attributes is a\nprojected 2D position on your screen. Meanwhile, fragment shaders (or pixel\nshaders in D3D) take those per-vertex attributes and compute a color value for\neach pixel. There are also several other optional shader stages which provide\nadditional control over the geometry pipeline such as geometry, tessellation,\nand task/mesh shaders. Finally, compute shaders provide a more general-purpose\nshader that runs independently of the 3D pipeline and just reads and writes\nmemory.\n\nOver the years, a lot of engineering effort has been put into making these\nshaders run incredibly quickly. On a 4K screen, for instance, you have to run\na minimum of half a billion fragment shader invocations per second in order to\nachieve a frame rate of 60 FPS. One of the techniques used by modern GPUs to\nachieve this speed is to execute shaders using a single instruction multiple\nthread (SIMT) model where as many as 128 invocations of the same shader are\nexecuted in lock-step with a single hardware instruction processing the data\nfor all 128 invocations. These groups of invocations are called a subgroup in\nVulkan or a wave in D3D or a warp on NVIDIA hardware. (I will be using the\nVulkan terminology throughout this post.) The actual size of a subgroup is\ndependent on the hardware, with 128 being the maximum. On NVIDIA hardware,\nsubgroups are always 32 invocations.\n\nHistorically, subgroups have been mostly invisible to applications. Shader\nprograms are written in a language that describes how to transform the input\ndata for a single vertex or pixel into the output data for that vertex or\npixel. The fact that the hardware processes the data for many vertices or\npixels simultaneously is mostly an implementation detail. I keep saying\n\"mostly\" because there is one notable exception: fragment shader derivatives.\nWithin the fragment stage, there are derivative opcodes which take a value and\ngive you a coarse screen-space derivative of that value as the output. This is\nonly really possible because each 2x2 group of pixels is executed within the\nsame subgroup and the hardware can do a bit of arithmetic across the subgroup\nto compute that derivative. Derivatives are also implicitly used by texture\nsampling operations to compute the level of detail (LOD) of the texture to be\nsampled.\n\nWith HLSL shader model 6.0, Microsoft added support for explicit wave\noperations to D3D shaders. Vulkan followed quickly with the addition of\nsubgroup support to Vulkan 1.1 and SPIR-V 1.3. These operations allow\napplications to take advantage of subgroup-level parallelism within shaders\nfor even greater performance. For instance, a compute shader doing a reduction\noperation can first reduce within a subgroup and then have only a single\ninvocation of that subgroup participate in the global reduction. Since the\nsubgroup reduction happens entirely in registers, this is significantly faster\nthan if every invocation of the shader had to participate in the global\nreduction.\n\nSo what is re-convergence and what does any of this have to do with control\nflow? With straight-line code, it's fairly easy to see how subgroup operations\nwork. The shader core executes each instruction one at a time with each\ninstruction processing an entire subgroup's worth of data. When you execute a\nsubgroup operation, it works like any other arithmetic operation except it\ntakes its values from the same register in each of the different invocations\nwithin a subgroup rather than from different registers in a single invocation.\nFor disabled invocations, the subgroup operations are smart enough to know to\nignore those values. (Disabled invocations may happen as a result of control\nflow or if the number of invocations is not a multiple of the subgroup size.)\nThe trouble arises the moment you hit an if statement. What happens if the if\nstatement condition is true for some of the invocations in the subgroup and\nfalse for others? This is what we call \"divergence\" where part of the subgroup\ngoes one way and part goes the other. In this case, the set of invocations for\nwhich the condition is false are disabled when the then side of the branch is\ntaken. When the else side of the branch is taken, the invocations for which\nthe condition was true are disabled. The order in which the then and else\nsides of the branch are executed is implementation-dependent. On recent NVIDIA\nhardware, the two sides of an if may even be executed simultaneously since the\ntwo sides act on disjoint sets of invocations.\n\nSo what happens to subgroup operations after we've taken a divergent branch?\nWhat invocations are involved in the subgroup operation? Consider the\nfollowing snippet of theoretical GLSL code:\n\n    \n    \n    glsl int x; if (cond) { x = thing1(); } else { x = thing2(); } int sum = subgroupAdd(x);\n\nOne might expect, as the shader author, that the value of x gets assigned the\nresult of either thing1() or thing2() depending on the condition and then the\nsubgroupAdd() would sum the value of x, regardless of whether thing1() or\nthing2() was executed. That is to say that one might expect control flow to\nre-converge before the subgroupAdd(), ensuring that all the invocations\nparticipate in the subgroupAdd(). Unfortunately, the truth may not always be\nso simple.\n\nMany compilers don't model control flow in terms of the kinds of constructs\nyou might have in a high-level programming language, such as if statements and\nfor loops. Instead, they model control flow as a directed graph where each\nchunk of straight-line code (typically called a basic block) is a node in the\ngraph and the paths between those blocks are edges. This representation is\ncommonly called a control flow graph (CFG). The CFG for the above example\nwould look something like this:\n\nThe code example above as a directed graph.  \n---  \n  \nFor a simple example like this, it's still fairly obvious in this form where\nand how the re-convergence should happen. You can see the control flow\ndiverging after the if and coming back together before the subgroupAdd(). With\nloops, however, things get much more complicated and not at all obvious.\nConsider the following snippet of GLSL code:\n\n    \n    \n    glsl int s; loop { int x = thing(); if (x > 200) { s = subgroupAdd(x); break; } } int m = subgroupMin(s);\n\nLooking at this as GLSL code, it's evident what is supposed to happen. It\ncalls thing() until it gets a value over 200. Whenever it gets a value over\n200, it adds all of them across the active lanes of the subgroup and then\nbreaks out of the loop. All of the invocations which got a value over 200 in\nthis loop iteration will participate in the subgroupAdd(). After the loop\ncompletes, it takes the minimum of those sums across the subgroup. This time,\nthe subgroupMin() includes all of the invocations in the subgroup. (Why would\nyou ever want that? You probably wouldn't. It's a made up example.)\nImportantly, this would behave differently if we moved the subgroupAdd() out\nof the loop as follows:\n\n    \n    \n    glsl int x; loop { x = thing(); if (x > 200) break; } int s = subgroupAdd(x); int m = subgroupMin(s);\n\nWe get a third, different behavior if we move the subgroupMin() into the loop:\n\n    \n    \n    glsl int m; loop { int x = thing(); if (x > 200) { int s = subgroupAdd(x); m = subgroupMin(s); break; } }\n\nIn both of these later cases, the subgroupMin() does nothing because all of\nthe results of a subgroupAdd() are the same across the subgroup. However, they\nare still different in which invocations participate in the add. In the first\ncase, the add happens outside the loop after every invocation has had a call\nto thing() return more than 200. In the later case, the subgroupAdd() happens\ninside the loop and only involves those invocations which had thing() return\nmore than 200 in the same iteration of the loop.\n\nUnfortunately, if you aren't careful, these three code snippets look almost\nidentical when looking at the shader as a directed graph of basic blocks. The\nfirst version of our loop looks like this:\n\nThe loop code example as a directed graph with the subgroupAdd() and the\nsubgroupMin() in separate basic blocks.  \n---  \n  \nWhile the second and third versions look like this:\n\nThe loop code example as a directed graph with the subgroupAdd() and the\nsubgroupMin() in the same basic block.  \n---  \n  \nIn fact, it is a fairly common code transformation in compilers to observe\nthat every path of execution which executes the subgroupAdd() then goes on to\nexecute the subgroupMin() and collapse the first graph into the second. On a\nCPU or when compiling a language that does not have explicit SIMT concepts\nlike subgroups, this is an entirely valid code transformation. However, with\nmodern shading languages it isn't.\n\nIt's also important to note that the second and third versions of our loop\nexample have exactly the same CFG even though they have different behavior.\nThe placement of re-convergence is crucial.\n\nThe way this is solved in the 3D graphics dialect of SPIR-V that is used by\nVulkan is by representing the original high-level structure of the program in\nSPIR-V directly. SPIR-V has concepts of if statements (which it calls\nselection constructs) and loops which provide this information to the driver's\ncompiler. With the VK_KHR_shader_maximal_reconvergence and\nVK_KHR_shader_subgroup_uniform_control_flow extensions, Vulkan shader\ncompilers are now required to respect those constructs and ensure that control\nflow re-converges in the natural way.\n\nIn Mesa's NIR optimizing compiler core, we preserve the structure directly in\nour representation of the CFG. Instead of simply representing it as a graph of\nnodes and blocks, our CFG data structure has node types for basic blocks, if\nstatements, loops, and functions, and each of those contains lists of other\ncontrol flow nodes. Each basic block also has lists of successor and\npredecessor blocks which provide the edges in the more classic sense of a CFG.\nIn this way, NIR is able to represent both the classic CFG and the structure\nfrom the original source program and both structures are preserved as we\ntransform the shader.\n\nThis is not only useful for tracking control flow re-convergence but it's also\nuseful for hardware back-ends. On some hardware such as Intel, Imagination,\nApple, and older NVIDIA GPUs, these high level concepts of ifs and loops are\nactually encoded in hardware instructions in some form. On Arm and Qualcomm\nGPUs, control flow re-convergence is based on the ordering of blocks within\nthe shader program with lower instruction addresses taking priority and re-\nconvergence happens when two groups of invocations reach the same, higher\ninstruction address. In this case, NIR's natural block ordering from the\nstructured CFG gives exactly the instruction order they need in order to re-\nconverge properly. On AMD GPUs, the hardware isn't actually capable of\ndiverging and it's the responsibility of the compiler to juggle execution\nmasks and predicate instructions as needed. Again, NIR's structure is useful\nas it provides a very natural place to compute these masks and make it easy to\nreason about uniform jumps.\n\n### Control flow re-convergence on NVIDIA\n\nOn NVIDIA GPUs starting with their Volta architecture, this all works quite\ndifferently. Instead of structured control flow instructions or instruction\naddress based merging, NVIDIA Volta+ hardware is entirely unstructured. It has\na simple branch instruction which can be predicated to provide a conditional\nbranch. To handle subgroup re-convergence, there is a barrier register file\ntogether with a set of instructions for setting, breaking out of, and waiting\non these barriers. The hardware has no fundamental notion of control flow\nstructure and instead it's up to the compiler to construct what structure it\nwishes from these barrier primitives.\n\nThe reason for this shift is because of an important advancement made in the\nVolta architecture which allows for independent forward progress of individual\nshader invocations. Instead of requiring things to follow a particular\nstructure or prioritizing invocations with low addresses like most other GPUs,\ngroups of invocations on Volta+ run independently with some sort of fair\nscheduler. This allows for CPU-like threading primitives such as mutexes. With\na fixed scheduling strategy, you run the risk of the hardware prioritizing the\ninvocations that are spinning waiting on the mutex and never executing the\ncode which would eventually unlock the mutex. With a fair scheduler, every set\nof unblocked invocations continues to execute until they exit the shader,\nregardless of what other invocations may be active.\n\nMy first attempt at implementing control flow re-convergence on Volta+ was\nfairly naive. Before each NIR control flow structure, we emitted a bmov.clear\ninstruction followed by a bssy to populate the barrier register. After each\ncontrol flow structure, we emitted a bsync instruction to wait on the barrier,\nensuring re-convergence. Before each loop break or continue, we emitted a\nsequence of bbreak instructions to break out of any barriers up to and\nincluding the one for the nearest loop.\n\nThe barrier placement pass wasn't totally naive, though. It made a decent\nattempt at avoiding unnecessary barriers by only inserted subgroup barriers\naround control flow that would actually increase divergence. It also included\nan optimization to avoid inserting barriers if the control flow would re-\nconverge anyway. For instance, there is an if statement at the end of a loop\nbody where the merge for the loop continue would re-converge control flow\nanyway, we can elide the barriers for the if statement.\n\nThis scheme appeared to work for a while and probably did if all of the\ndivergence was limited to if statements. However, it had a fatal flaw in the\npresence of non-uniform loops. We honestly may never have noticed the flaw if\nit weren't for a few Vulkan CTS tests for the new\nVK_KHR_shader_subgroup_uniform_control_flow extension. When we first tried to\nturn it on, I saw 4 test failures:\n\n  * dEQP-VK.reconvergence.subgroup_uniform_control_flow_ballot.compute.nesting4.1.2\n  * dEQP-VK.reconvergence.subgroup_uniform_control_flow_elect.compute.nesting4.1.2\n  * dEQP-VK.reconvergence.workgroup_uniform_control_flow_ballot.compute.nesting4.1.2\n  * dEQP-VK.reconvergence.workgroup_uniform_control_flow_elect.compute.nesting4.1.2\n\nThe person who made the initial merge request (MR) missed this because they\ndid legitimately pass for him. I accepted the MR because I was pretty sure I'd\nimplemented proper re-convergence and I believed him that the tests passed. I\ndidn't see the failure until I was doing some regression testing on a\ndifferent branch. Upon further investigation, it turned out that he was\nrunning a newer version of the Vulkan CTS where the tests were slightly\ndifferent. Normally, I would just write this off as a CTS bugfix but by this\npoint I had already started investigating and discovered the deficiency in the\nNVK compiler. After fixing the NVK compiler, we now pass all the different\nversions of those tests.\n\nTo understand this flaw, we need to first look at how I plumbed these barriers\nthrough NIR. In NIR, I represented the bssy, bsync, and bbreak instructions as\na set of NIR intrinsics: bar_set_nv, bar_sync_nv, and bar_break_nv. These NIR\nintrinsics fairly directly map to the hardware instructions except that they\ncopy the barrier value through a GPR to preserve proper NIR semantics. The\ncompiler can usually eliminate this copy so it doesn't hurt anything. Because\nbar_break_nv modifies a barrier, it both takes a barrier as a source and\nreturns a new barrier with the break applied. Even though bbreak only takes a\nsingle read/write barrier source, I had to model it this way in NIR because\nSSA form does not allow for modifying values. In the register allocator, we\nensure that both the source and destination of the bbreak get assigned the\nsame barrier register and insert bmov instructions as needed.\n\nWhen we inserted barrier instructions for loops in the original control flow\nre-convergence pass, it looked something like this:\n\n    \n    \n    %bar_reg = decl_reg 32 1 %b0 = bar_set_nv store_reg %b0 %bar_reg loop { if (cond1) { %b1 = load_reg %bar_reg %b2 = bar_break_nv %b1 store_reg %b2 %bar_reg break; } if (cond2) { continue; }\n    \n    \n    } %b3 = load_reg %bar_reg bar_sync_nv %b3\n\nWhen we run load/store elimination to convert back into proper SSA form, this\nbecomes\n\n    \n    \n    %b0 = bar_set_nv loop { if (cond) { %b2 = bar_break_nv %b0 break; } if (cond2) { continue; } ... } bar_sync_nv %b2\n\nThere are a few obvious problems with this. The first is that there is no\nbar_sync_nv inside the loop to ensure that early continues re-converge at the\ntop of the loop. Hardware with built-in structured control flow constructs\ntypically re-converge early continues automatically. On NVIDIA, however, we\nneed a bar_sync_nv at the top of the loop to ensure that all continues arrive\nbefore we repeat the loop. The second problem is that, the way SSA works out,\nthe value consumed by the bar_sync_nv intrinsic after the loop is always a\nbreak value which, by definition doesn't contain the current thread. This\nmeant that the bar_sync_nv instruction after the loop also did nothing. The\nnaive solution to these problems would be to re-structure the barrier\nintrinsics like this:\n\n    \n    \n    %b0 = bar_set_nv loop { %b1 = phi %b0 %b2 bar_sync_nv %b1 if (cond) { %b2 = bar_break_nv %b1 break; } if (cond2) { continue; } ... } bar_sync_nv %b0\n\nAt first glance, this looks like it does what we want. The bar_sync_nv after\nthe loop applies to all the threads that entered the loop, ensuring re-\nconvergence because it uses the value from the original bar_set_nv. Also, we\nhave a bar_sync_nv at the top of the loop to ensure that all the various loop\ncontinues re-converge and it takes its barrier value from the phi so it takes\ninto account the bar_break_nv. However, if you look a little closer, you'll\nobserve that the phi at the top of the loop is bogus. The %b2 = bar_break_nv\n%b1 value has no way to make its way back to the top of the loop because it's\nonly defined on the break path.\n\nThis all speaks to a much more fundamental problem with control flow re-\nconvergence on NVIDIA Volta+ hardware: barrier instructions have no implicit\nintra-subgroup communication. All communication has to follow the normal\nshader data flow. On hardware with structured control flow instructions such\nas Intel, the loop break instruction does not always jump to the end of the\nloop. Instead, it removes the active invocations from some sort of internal\nexecution mask to ensure that they get disabled on the next iteration of the\nloop. Once the final invocation is disabled (the execution mask goes to zero),\nthe hardware jumps to the end of the loop. In order to handle nested loops,\nthere is typically some sort of internal stack of such execution masks. In\norder for this to work, each iteration of the loop must know which invocations\nhave broken out in previous iterations. Otherwise, it wouldn't know when the\nfinal invocation breaks so it can pop the stack and jump to the end of the\nloop. This effectively means that the invocations that break out of the loop\ncommunicate their break status to the invocations that continue.\n\nThe barrier instructions on Volta have no such internal stack or implicit\nintra-subgroup communication. The values stored in the barrier registers are\ninvocation masks and there is an implicit subgroup ballot that happens as part\nbssy and bbreak. However, the barrier register value is per-invocation and the\nmodification of the barrier register happens only in the active invocations. A\nbbreak instruction, for instance, will disable the barrier for active\ninvocations based on the provided predicate but the barrier values for\ninactive invocations are unmodified. This means that, in order to break out of\na barrier, all invocations which are included in the barrier must be present\nat the execution of the bbreak instruction.\n\nBut doesn't this defeat the purpose of the bbreak instruction? How do we break\nif the bbreak has to happen in all the invocations? The solution is that we\nhave to re-converge before we break. In the [new barrier placement pass][3]\nlanded last week, I defined a concept called scopes which are single entry,\nsingle exit regions in the control-flow graph. Each scope begins with a\nbar_set_nv and concludes with a bar_sync_nv. Because we need to support\nbreaking out of more than a single scope, each scope end optionally cascades\nto the end of the next scope, re-converging at each step. After the\nbar_sync_nv, it checks a break value from the incoming scope break edges and\njumps to the end of the next scope, if needed. In our loop example above, this\nlooks like the following:\n\n    \n    \n    %b0 = bar_set_nv scope 1 { loop_header: %b1 = bar_set_nv scope 2 { if (cond) { // break; %c0 = 1 goto scope_2_merge; } if (cond2) { // continue; %c1 = 2 goto scope_2_merge; } ... %c2 = 2 goto scope_2_merge; } scope_2_merge: %c = phi %c0 %c1 %c2 bar_sync_nv %b1 if (%c < 2) { goto scope_1_merge; } goto loop_header; } scope_1_merge: bar_sync_nv %b0\n\nIn this new formulation, all control flow paths that flow through scope 2\neventually end up at scope_2_merge: and the bar_sync_nv intrinsic that ensures\neverything inside scope 2 re-converges. The cascade value %c specifies how far\nwe intend to break. For the loop continue, we set the %c to 2 indicate that we\ndon't intend to break any further than scope 2. For the loop break, we set %c\nto 1 and the check after the bar_sync_nv causes us to jump again to\nscope_1_merge:, effecting a loop break. By handling breaks via this sort of\ncascade, we ensure that control flow re-converges at each level before\nbreaking to the next level. By converging each scope one at a time, we avoid\nthe communication problem because every invocation which is involved in the\nbar_set_nv also ends up at the corresponding bar_sync_nv, regardless of which\npath it takes through the scope. We never need to use bar_break_nv because we\njust create a new barrier value at the top of the loop.\n\nThe one exception to this is for nir_jump_halt which is an instruction that\nentirely exits the shader program. (This is similar to exit() in a C program.)\nThe nir_jump_halt instruction maps to exit on NVIDIA hardware. Threads which\nhave exited never block barriers so we don't need to worry about the cascade\nfor them. The invocation mask stored in the barrier register may still contain\nthat invocation but the bbreak instruction knows better than to wait on exited\nthreads.\n\n### NIR improvements\n\nIf you're familiar with NIR and paying close attention to the pseudo-code, you\nmay have noticed the goto statements. But didn't I say earlier that NIR is\nstructured? Yes and no. NIR has limited support for both structured and\nunstructured forms. While most NIR passes currently require the structured\nform, the core IR can be unstructured. The unstructured form was originally\nadded to support the OpenCL variant of SPIR-V for use by rusticl and\nMicrosoft's CLOn12 layer. Because most NIR passes don't support unstructured\nNIR, we call nir_lower_goto_ifs() to structurize the control flow immediately\nafter parsing the SPIR-V.\n\nHowever, because NVIDIA hardware wants so badly to be unstructured and because\nconverting to this scope-based form of re-convergence is such a substantial\nre-structuring of the CFG, it made sense to convert to unstructured NIR before\ngoing into the NVIDIA back-end. Also, because of the invasive control flow\nmanipulations, I needed to temporarily lower phis to registers and run the SSA\nrepair pass after re-structuring the CFG. Using unstructured NIR allows us to\nre-use NIR's SSA repair passes instead of having to repair SSA in the back-end\ncompiler. The changes to the NVK back-end compiler were fairly minor as it\nalready does unstructured control flow natively. All I had to do was to make\nit support nir_jump_goto and nir_jump_goto_if.\n\nThe rest of NIR, however, needed some enhancement. In particular, I had to\nteach NIR's dominance analysis, SSA repair, and register load/store\nelimination passes to work with unstructured control-flow. None of these fixes\nwere particularly difficult. The most important thing that structured NIR\nprovides is a natural dominance-preserving ordering of the basic blocks. In\nother words, when you walk the CFG from top to bottom, you are guaranteed to\nencounter each SSA value's definition before any of its uses. The natural\nordering of basic blocks in NIR's structured control flow form has this\nproperty. With unstructured control flow, the standard dominance-preserving\nordering used in most of the literature is a reverse post-order depth-first\nsearch (DFS) of the CFG.\n\nInstead of trying to directly iterate with a reverse post-order DFS or sorting\nthe blocks for every iteration, I opted to require that unstructured NIR\nalways has its blocks ordered. I added a pass which sorts the blocks according\nto a post-order DFS and added validation that ensures that unstructured blocks\nare always sorted. Any passes which modify the control flow of an unstructured\nNIR shader simply need to call the sorting pass before they finish to ensure\nthis invariant. Because iteration is common but modifying control-flow is not,\nit's a reasonable trade-off to simply maintain this invariant. This will also\nmake it easy and efficient to update most NIR passes in the future to support\nunstructured control flow. Passes which modify the CFG in some way will need\nmore work but those are uncommon.\n\n### Conclusion\n\nWith everything reworked and finally correct, we could finally turn the\nVK_KHR_shader_maximal_reconvergence and\nVK_KHR_shader_subgroup_uniform_control_flow extensions back on in NVK.\nHowever, that's not the end of the story.\n\nA day or two after landing the MR, I received a message on Mastodon telling me\nthat it had fixed the rendering corruptions in Genshin Impact:\n\nA screenshot of Genshin Impact showing rendering artifacts (left) verus a\nscreenshot of Genshin Impact rendering correctly (right).  \n---  \n  \nIf you look at the first screenshot, you can see all these rectangular regions\nthat aren't quite the right color. In the second screenshot, everything looks\nfine. Does Genshin Impact use subgroup ops? Not to my knowledge, no. Genshin\nImpact is a D3D11 title and real subgroup ops weren't added until D3D12.\n\nSo what's going on? Do you remember at the beginning of this whole discussion\nwhere I mentioned derivatives? Derivatives are a shader feature that goes back\nto the OpenGL 2.0 and D3D9 but they're also subgroup ops and depend on re-\nconvergence to get the correct values. Somewhere in some post-processing\nshader they're probably using derivatives inside or after a loop and we\nweren't re-converging properly, leading to wrong derivatives. So by fixing a\nbrand new Vulkan extension, I also fixed a bug with an ancient 3D graphics\nfeature in an actual game.\n\n### Related Posts\n\n##### NVK is now ready for prime time\n\n##### NVK holiday update: What we've achieved, and where we're headed\n\n##### NVK reaches Vulkan 1.0 conformance\n\n### Related Posts\n\n##### NVK is now ready for prime time\n\n##### NVK holiday update: What we've achieved, and where we're headed\n\n##### NVK reaches Vulkan 1.0 conformance\n\n### Comments (0)\n\n### Add a Comment\n\n## Search the newsroom\n\n## Latest Blog Posts\n\n### Re-converging control flow on NVIDIA GPUs - What went wrong, and how we\nfixed it\n\n25/04/2024\n\nWhile I managed to land support for two extensions, implementing control flow\nre-convergence in NVK did not go as planned. This is the story...\n\n### Automatic regression handling and reporting for the Linux Kernel\n\n14/03/2024\n\nIn continuation with our series about Kernel Integration we'll go into more\ndetail about how regression detection, processing, and tracking...\n\n### Almost a fully open-source boot chain for Rockchip's RK3588!\n\n21/02/2024\n\nNow included in our Debian images & available via our GitLab, you can build a\ncomplete, working BL31 (Boot Loader stage 3.1), and replace...\n\n### What's the latest with WirePlumber?\n\n19/02/2024\n\nBack in 2022, after a series of issues were found in its design, I made the\ncall to rework some of WirePlumber's fundamentals in order to...\n\n### DRM-CI: A GitLab-CI pipeline for Linux kernel testing\n\n08/02/2024\n\nContinuing our Kernel Integration series, we're excited to introduce DRM-CI, a\ngroundbreaking solution that enables developers to test their...\n\n### Persian Rug, Part 4 - The limitations of proxies\n\n23/01/2024\n\nThis is the fourth and final part in a series on persian-rug, a Rust crate for\ninterconnected objects. We've touched on the two big limitations:...\n\n#### About Collabora\n\nWhether writing a line of code or shaping a longer-term strategic software\ndevelopment plan, we'll help you navigate the ever-evolving world of Open\nSource.\n\n\ud55c\uad6d\uc5b4 \ubc84\uc804\uc758 Collabora.com \ubcf4\uae30\n\nAcesse Collabora.com em Portugu\u00eas\n\n#### Learn more\n\n  * Who we are\n  * Services\n  * Our expertise\n  * Industries\n  * Our work\n  * Careers\n  * Open Source\n\nCollabora on Twitter Collabora on YouTube Collabora on Mastodon Collabora on\nLinkedIn Collabora on Facebook Collabora RSS Feed\n\n+44 1223 362967\n\n+1 514 667 2499\n\ncontact@collabora.com\n\nWe use cookies on this website to ensure that you get the best experience. By\ncontinuing to use this website you are consenting to the use of these cookies.\nTo find out more please follow this link.\n\nCollabora Ltd \u00a9 2005-2024. All rights reserved. Privacy Notice. Sitemap.\n\n", "frontpage": false}
