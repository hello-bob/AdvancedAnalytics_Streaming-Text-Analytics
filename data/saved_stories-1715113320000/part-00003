{"aid": "40284225", "title": "The Birth of Parquet", "url": "https://sympathetic.ink/2024/01/24/Chapter-1-The-birth-of-Parquet.html", "domain": "sympathetic.ink", "votes": 1, "user": "whinvik", "posted_at": "2024-05-07 11:09:23", "comments": 0, "source_title": "Chapter I: The birth of Parquet", "source_text": "Chapter I: The birth of Parquet | The Sympathetic Ink Blog\n\nThe Sympathetic Ink Blog\n\n# Chapter I: The birth of Parquet\n\nJan 24, 2024\n\nBy Julien Le Dem\n\nThis is the first part of my 3-chapter blog post: Ten years of building open\nsource standards\n\n## Chapter I: The birth of Parquet\n\n### Prologue\n\n15 years ago (2007-2011) I was at Yahoo! working with Map/Reduce and Apache\nPig, which was the better Map/Reduce at the time. The Dremel paper just came\nout and, as everything I worked with seemed to be inspired from Google papers,\nI read it. I could see it applying to what we were doing at Yahoo! and this\nwas to become a big inspiration for my future work.\n\nPig has now fallen out of grace and it\u2019s been mostly replaced by Apache Spark\nfor many distributed computing needs. However, it is the project where I\nearned my first committership at the ASF and it holds a special place in my\nheart. Starting as a nimble caterpillar user of the project, I progressed\nthrough all the stages of metamorphosis, building my coccoon as a contributor\nand finally emerging as a maintainer. In the ASF world, this happens by being\ninvited - first to become a committer and then to join the Project Managment\nComittee. Eventually, I was elected the chair of the project for a year. I\nlearned a lot about open source foundations and how their different roles work\nduring that time.\n\nThis is also a great way to grow your network. Through the Pig community I met\nengineers at companies like Twitter, Linkedin and Netflix solving similar\nproblems.\n\nThrough those connections, I joined the data platform team at Twitter in 2011,\nleading the processing tools team during its explosive growth during the pre-\nIPO years.\n\n### The context\n\nThe data platform at Twitter had two main tools in its belt. On one end, there\nwas Hadoop, which was a highly scalable, high throughput, data processing\njuggernaut. But it was also fairly high latency, Map/Reduce being something of\na brute force approach to data processing. You\u2019d launch your query and go get\ncoffee before it completed.\n\nOn the other end, there was Vertica, a state-of-the-art Massively Parallel\nProcessing database, leveraging columnar storage and vectorization to achieve\nlow latency results. However Vertica was much more expensive and didn\u2019t scale\nas well as Hadoop, causing constant questions around what data would make it\nthere. It never had all the datasets nor the full time span available in\nHadoop.\n\n### The idea\n\nThis raised the question of how to make Hadoop more like Vertica. It looked\nlike the main problem was that the Hadoop abstractions were a bit too low\nlevel and geared towards building a search index.\n\nHadoop provides Map/Reduce on top of a distributed file system while Vertica\nis a distributed query engine on top of a columnar storage.\n\nDuring that time, the data platform team at Twitter had a paper reading group.\nPeople would read papers and present them to the group if they were\ninteresting. At the time I re-read the Dremel paper and presented to the\ngroup. I also read interesting papers that described how those MPP databases\nwork: C-Store, Vertica, Monet-DB and others. This led to a better\nunderstanding of distributed query engines and the benefits of column stores.\n\n### What\u2019s a columnar layout\n\nWhen you think of a table representation, it is two dimensional with columns\nand rows. However when it\u2019s physically stored on a disc it has to be aranged\nas a linear, one dimensional succession of bits.\n\nOrganizing the table in a row layout means writing each row one after another.\nThis interleaves data of different types, as you write a value for the first\ncolumn in the first row, followed by the value of the second column of a\ndifferent type in the first row. And so on, one row at a time.\n\nIn a column layout, you write all the values of the first column for all the\nrows first, then the values for the second column, and so on.\n\nOne of the benefits is that when you need to retrieve only a subset of the\ncolumns, which is very common, you can much more efficiently scan them from\nthe disk in big chunks rather than doing a lot of small seeks. Another benefit\nis that the data compresses better because you can encode together values of\nthe same type that are much more homogenous.\n\n### Red Elm\n\nThat was it, On my shuttle ride to and from the office, I started prototyping\nsomething that would make Hadoop more like Vertica and, like any good project,\nit first needed a name. Twitter had a tendency to name everything after birds.\nTrying to be clever, I was looking for an anagram of Dremel to name my\nproject. Emerald was already taken, so I picked Red Elm, and since birds live\nin trees, this sounded like a good choice. This was also quite ambitious as it\nimplied that I was going to implement all of Dremel, not just the storage\nformat, but that didn\u2019t quite happen and that\u2019s for the better.\n\nI took inspiration from the existing formats I could find (TFile, RCFile, CIF,\nTrevni) and the context of schema definition at Twitter (Thrift and Pig) and,\nover the summer of 2012, I started implementing the column spliting algorithm\ndescribed in the Dremel paper.\n\n### Looking for partners\n\nTo make this project sustainable, it needed to be integrated into the rest of\nthe open source data stack. This could not be a proprietary Twitter data\nformat as that would have required doing an enormous amount of work to\nintegrate with all the tools we used. And after that, even more work to keep\nit compatible over time as well as integrate it with future tools we wanted to\nuse. No need to say, that would have been way more work than we would have\nbeen able to do.\n\nSo I started looking for partners. I tweeted about implementing the Dremel\npaper and how I found an error in one of the figures. That led to connecting\nwith the Impala team at Cloudera who were also prototyping a columnar format.\n\nObviously, since this was a common need for the Hadoop ecosystem, I was not\nthe only one looking for a solution. We met and it turned out we had\ncomplementing qualities. I was bringing compatibility with the JVM ecosystem\nand they were bringing a distributed query engine that generated native code\nusing LLVM. We merged our designs: I implemented the Java side, they\nimplemented the native reader integrated in Impala, and that became Parquet.\nWe picked a name that would evoke the bottom layer of a database with an\ninteresting layout.\n\nWe published an announcement that Twitter and Cloudera were collaborating,\nwhich meant it was not just one company. Quickly, several companies started to\nbecome interested. Criteo, an ad targeting company in Europe, built Hive\nsupport, Netflix adopted Parquet and built Presto support, and Apache drill\nadopted Parquet as a baseline for storage. Then we entered the incubator to\nofficially become part of the Apache Software Foundation as a top level\nproject.\n\nOnce SparkSQL was built on top of Parquet, it quickly reached escape velocity.\nContinuing on that trajectory, today it\u2019s integrated in all major warehouses -\nand you can even read Parquet from Excel if you want.\n\n## lessons learned\n\n### Every contributor becomes a stakeholder\n\nThis is true in particular for projects defining a standard format -\nespecially one that is used by other communities made of people building\nprojects on top. To ensure success and community growth, every contributor\nbecomes a stakeholder. That means they don\u2019t only contribute and move on;\ninstead, they have a voice and their opinion matters.\n\nWe seek their opinion and feedback. We make sure their needs are met. They now\nhave skin in the game and have an impact on the future direction. Not\neverybody will use that power, but those who do, will now also exhibit greater\nresponsibility and contribute meaningfully.\n\n### The snowball effect\n\nSometimes, building an open source project and supporting its community feels\nlike you\u2019re pushing a boulder uphill. But, in reality, the mechanics are quite\ndifferent. It\u2019s more like a snowball.\n\nInstead of new contributors just helping you push (without doing much to\naddress the risk of slipping backwards) they are actually creating momentum.\nThere\u2019s a real snowball effect where, as your project grows, it accelarates.\nEventually, it reaches an inflection point and gains enough speed that it\nreaches escape velocity. It has gone out of control, but on it\u2019s way up.\nToday, I could not stop the Parquet project even if I wanted to.\n\n### Open source comes in all shapes and sizes\n\nThe most basic definition of open source is the code is available for you to\nread, but that does not mean you can use it the way you want.\n\nLicenses are for defining what you can use the code for and establishing the\nrights and constraints that come with it. There are very different types of\nopen source licenses. All of them will tell you that you\u2019re on your own if you\nshoot yourself in the foot, and then will proceed to restrict use.\n\nThe ASL is very permissive. It mainly protects attribution: you have to give\ncredit if you\u2019re using it. You can modify it however you want, but you have to\nclearly state so. You can\u2019t redistribute something you have modified and call\nit the same name. GPL is more restrictive, often described as viral: if you\nbuild something on top of it, your derivitave work also has to be released as\nopen source under the GPL.\n\nGovernance clarifies who can make decisions about the project and how someone\ncan join the group that makes decisions. A project can have an open license,\nbut still, as a user, you have no control over the direction of the project.\n\nGood governance clarifies how to contribute, how to become a committer, and\nhow to influence the direction and have a say on important decisions.\n\nSome projects are owned by companies and decisions are made by their\nemployees, while others allow anyone to become a maintainer.\n\nThe last part is clarifying who can modify the governance of the project. Can\nthe license change? Who effectively controls how the project is maintained?\n\nBeing part of an Open Source Foundation is a way to assign this right to a\nthird party and guarantee a level of neutrality. Two important foundations are\nthe ASF and the Linux Foundation (and in particular the CNCF and the\nLFAI&Data). They guarantee that the license of the project is never going to\nchange, and enforce rules to maintain security processes and keep projects\nopen and inclusive.\n\nYou don\u2019t need to be part of those foundations to have good governance, but\nfoundations enforce a set of best practices. What\u2019s important is the project\nbeing up front about how it can be used and how decisions are made. All shapes\nand sizes are fine as long as expectations are set correctly.\n\n# Part II: From Parquet to Arrow\n\n###### Thank you Ross Turk for the feedback.\n\n## The Sympathetic Ink Blog\n\n  * The Sympathetic Ink Blog\n\n  * julienledem\n  * julienledem\n  * J_\n  * RSS\n\nA blog on Tech, Data, Architecture and People\n\n", "frontpage": false}
