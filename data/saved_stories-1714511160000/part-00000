{"aid": "40211785", "title": "AI leaderboards are no longer useful. It's time to switch to Pareto curves", "url": "https://www.aisnakeoil.com/p/ai-leaderboards-are-no-longer-useful", "domain": "aisnakeoil.com", "votes": 14, "user": "jobbagy", "posted_at": "2024-04-30 15:04:48", "comments": 0, "source_title": "AI leaderboards are no longer useful. It's time to switch to Pareto curves.", "source_text": "AI leaderboards are no longer useful. It's time to switch to Pareto curves.\n\n# AI Snake Oil\n\nShare this post\n\n#### AI leaderboards are no longer useful. It's time to switch to Pareto\ncurves.\n\nwww.aisnakeoil.com\n\n#### Discover more from AI Snake Oil\n\nWhat makes AI click, what makes it fail, and how to tell the difference\n\nOver 27,000 subscribers\n\nContinue reading\n\nSign in\n\n# AI leaderboards are no longer useful. It's time to switch to Pareto curves.\n\n### What spending $2,000 can tell us about evaluating AI agents\n\nSayash Kapoor\n\nand\n\nArvind Narayanan\n\nApr 30, 2024\n\n15\n\nShare this post\n\n#### AI leaderboards are no longer useful. It's time to switch to Pareto\ncurves.\n\nwww.aisnakeoil.com\n\n1\n\nShare\n\nBy Sayash Kapoor, Benedikt Stroebl, Arvind Narayanan\n\nWhich is the most accurate AI system for generating code? Surprisingly, there\nisn\u2019t currently a good way to answer questions like these.\n\nBased on HumanEval, a widely used benchmark for code generation, the most\naccurate publicly available system is LDB (short for LLM debugger).1 But\nthere\u2019s a catch. The most accurate generative AI systems, including LDB, tend\nto be agents,2 which repeatedly invoke language models like GPT-4. That means\nthey can be orders of magnitude more costly to run than the models themselves\n(which are already pretty costly). If we eke out a 2% accuracy improvement for\n100x the cost, is that really better?\n\nIn this post, we argue that:\n\n  * AI agent accuracy measurements that don\u2019t control for cost aren\u2019t useful.\n\n  * Pareto curves can help visualize the accuracy-cost tradeoff.\n\n  * Current state-of-the-art agent architectures are complex and costly but no more accurate than extremely simple baseline agents that cost 50x less in some cases.\n\n  * Proxies for cost such as parameter count are misleading if the goal is to identify the best system for a given task. We should directly measure dollar costs instead.\n\n  * Published agent evaluations are difficult to reproduce because of a lack of standardization and questionable, undocumented evaluation methods in some cases.\n\n####\n\nMaximizing accuracy can lead to unbounded cost\n\nLLMs are stochastic. Simply calling a model many times and outputting the most\ncommon answer can increase accuracy.\n\nOn some tasks, there is seemingly no limit to the amount of inference compute\nthat can improve accuracy.3 Google Deepmind's AlphaCode, which improved\naccuracy on automated coding evaluations, showed that this trend holds even\nwhen calling LLMs millions of times.\n\nThe accuracy of AlphaCode on coding tasks continues to improve even after\nmaking a million calls to the underlying model (the different curves represent\nvarying parameter counts). Accuracy is measured by how often one of the top 10\nanswers generated by the model is correct.\n\nA useful evaluation of agents must therefore ask: What did it cost? If we\ndon\u2019t do cost-controlled comparisons, it will encourage researchers to develop\nextremely costly agents just to claim they topped the leaderboard.\n\nIn fact, when we evaluate agents that have been proposed in the last year for\nsolving coding tasks, we find that visualizing the tradeoff between cost and\naccuracy yields surprising insights.\n\n####\n\nVisualizing the accuracy-cost tradeoff on HumanEval, with new baselines\n\nWe re-evaluated the accuracy of three agents that have been claimed to occupy\ntop spots on the HumanEval leaderboard: LDB, LATS, and Reflexion.4 We also\nevaluated the cost and time requirements of running these agents.\n\nThese agents rely on running the code generated by the model, and if it fails\nthe test cases provided with the problem description, they try to debug the\ncode, look at alternative paths in the code generation process, or \"reflect\"\non why the model's outputs were incorrect before generating another solution.\n\nIn addition, we calculated the accuracy, cost, and running time of a few\nsimple baselines:\n\n  * GPT-3.5 and GPT-4 models (zero shot; no agent architecture5)\n\n  * Retry: We repeatedly invoke a model with the temperature set to zero, up to five times, if it fails the test cases provided with the problem description.6 Retrying makes sense because LLMs aren\u2019t deterministic even at temperature zero.\n\n  * Warming: This is the same as the retry strategy, but we gradually increase the temperature of the underlying model with each run, from 0 to 0.5. This increases the stochasticity of the model and, we hope, increases the likelihood that at least one of the retries will succeed.\n\n  * Escalation: We start with a cheap model (Llama-3 8B) and escalate to more expensive models (GPT-3.5, Llama-3 70B, GPT-4) if we encounter a test case failure.7\n\nSurprisingly, we are not aware of any papers that compare their proposed agent\narchitectures with any of the latter three simple baselines.\n\nOur most striking result is that agent architectures for HumanEval do not\noutperform our simpler baselines despite costing more. In fact, agents differ\ndrastically in terms of cost: for substantially similar accuracy, the cost can\ndiffer by almost two orders of magnitude!8 Yet, the cost of running these\nagents isn't a top-line metric reported in any of these papers.9\n\nOur simple baselines offer Pareto improvements over existing agent\narchitectures. We run each agent five times and report the mean accuracy and\nthe mean total cost on the 164 HumanEval problems. Where results for LDB have\ntwo models/agents in parenthesis, they indicate the language model or agent\nused to generate the code, followed by the language model used to debug the\ncode. Where they have just one, they indicate that the same model was used to\nboth generate the code and debug it. Note that the y-axis is shown from 0.7 to\n1; figures with the full axis (0 to 1) and error bars, robustness checks, and\nother details about our empirical results are included in the appendix.\n\nThere is no significant accuracy difference between the warming strategy and\nthe best-performing agent architecture. Yet, Reflexion and LDB cost over 50%\nmore than the warming strategy,10 and LATS over 50 times more (all these costs\nare entirely or predominantly from calls to GPT-4, so these ratios will be\nstable even if model costs change). Meanwhile, the escalation strategy\nstrictly improves accuracy while costing less than half of LDB (GPT-3.5) at\ncurrent inference prices.11\n\nOur results point to another underlying problem: papers making claims about\nthe usefulness of agents have so far failed to test if simple agent baselines\ncan lead to similar accuracy. This has led to widespread beliefs among AI\nresearchers that complex ideas like planning, reflection, and debugging are\nresponsible for accuracy gains. In fact, Lipton and Steinhardt noted a trend\nin the AI literature of failing to identify the sources of empirical gains\nback in 2018.\n\nBased on our findings, the question of whether debugging, reflection, and\nother such \u201cSystem 2\u201d approaches are useful for code generation remains\nopen.12 It is possible that they will be useful on harder programming tasks\nthan those represented in HumanEval. For now, the over-optimism about System 2\napproaches is exacerbated by a lack of reproducibility and standardization\nthat we report below.13\n\n####\n\nProxies for cost are misleading\n\nAt first glance, reporting dollar costs is jarring. It breaks many properties\nof benchmarking that we take for granted: that measurements don\u2019t change over\ntime (whereas costs tend to come down) and that different models compete on a\nlevel playing field (whereas some developers may benefit from economies of\nscale, leading to lower inference costs). Because of this, researchers usually\npick a different axis for the Pareto curve, such as parameter count.\n\nThe downsides of reporting costs are real, but we describe below how they can\nbe mitigated. More importantly, we think using attributes like parameter count\nas a proxy for cost is a mistake and doesn\u2019t solve the problem it\u2019s intended\nto solve. To understand why, we need to introduce a conceptual distinction.\n\nAI evaluations serve at least two distinct purposes. Model developers and AI\nresearchers use them to identify which changes to the training data and\narchitecture improve accuracy. We call this model evaluation. And downstream\ndevelopers, such as programmers who use AI to build consumer-facing products,\nuse evaluations to decide which AI systems to use in their products. We call\nthis downstream evaluation.\n\nThe difference between model evaluation and downstream evaluation is\nunderappreciated. This has led to much confusion about how to factor in the\ncost of running AI.\n\nModel evaluation is a scientific question of interest to researchers. So it\nmakes sense to stay away from dollar costs for the aforementioned reasons.\nInstead, controlling for compute is a reasonable approach: if we normalize the\namount of compute used to train a model, we can then understand if factors\nlike architectural changes or changes in the data composition are responsible\nfor improvements, as opposed to more compute. Notably, Nathan Lambert argues\nthat many of the accuracy gains in the last year (such as Meta's Llama 2) are\nsimply consequences of using more compute.\n\nOn the other hand, downstream evaluation is an engineering question that helps\ninform a procurement decision. Here, cost is the actual construct of interest.\nThe downsides of cost measurement aren\u2019t downsides at all; they are exactly\nwhat\u2019s needed. Inference costs do come down over time, and that greatly\nmatters to downstream developers. It is unnecessary and counterproductive for\nthe evaluation to stay frozen in time.\n\nIn this context, proxies for cost (such as the number of active parameters or\namount of compute used) are misleading. For example, Mistral released the\nfigure below alongside their latest model, Mixtral 8x22B, to explain why\ndevelopers should choose it over competitors.\n\nSubstituting active parameters as a proxy for cost is misleading. Source:\nMistral.\n\nIn this figure, the number of active parameters is a poor proxy for cost. On\nAnyscale, Mixtral 8x7B costs twice as much as Llama 2 13B, yet Mistral's\nfigure shows it costs about the same, because they only consider the number of\nactive parameters. Of course, downstream developers don't care about the\nnumber of active parameters when they're using an API. They simply care about\nthe dollar cost relative to accuracy. Mistral chose \u201cactive parameters\u201d as a\nproxy, presumably because it makes their models look better than dense models\nsuch as Meta\u2019s Llama and Cohere\u2019s Command R+. If we start using proxies for\ncost, every model developer can pick a proxy that makes their model look good.\n\nSome hurdles to cost evaluation remain. Different providers can charge\ndifferent amounts for the same model, the cost of an API call might change\novernight, and cost might vary based on model developer decisions, such as\nwhether bulk API calls are charged differently. These downsides can be partly\naddressed by making the evaluation results customizable using mechanisms to\nadjust the cost of running models, i.e., providing users the option to adjust\nthe cost of input and output tokens for their provider of choice to\nrecalculate the tradeoff between cost and accuracy. In turn, downstream\nevaluations of agents should include input/output token counts in addition to\ndollar costs, so that anyone looking at the evaluation in the future can\ninstantly recalculate the cost using current prices.\n\nBut ultimately, despite the hurdles, good measurement requires modeling the\nunderlying construct of interest. For downstream evaluations, that underlying\nconstruct is cost. All other proxies are lacking.\n\n####\n\nAgent evaluations lack standardization and reproducibility\n\nIn the course of our evaluation, we found many shortcomings in the\nreproducibility and standardization of agent evaluations.\n\n  * We were unable to reproduce the results of the LATS and LDB agents on HumanEval. In particular, across all 5 runs for LDB (Reflexion, GPT-3.5), the maximum accuracy was 91.5%, much lower than the 95.1% reported in the paper.14 The maximum accuracy of LATS across all five runs was similarly lower, at 91.5% instead of 94.4%.\n\n  * Similarly, the accuracy for the baseline GPT-4 model reported in the LDB paper is drastically lower than our reproduction of the paper's code (75.0% vs. a mean of 89.6% across five runs). In fact, according to the paper, the GPT-3.5 and GPT-4 models perform very similarly (73.9% vs. 75.0%).15 Weak baselines could give a false sense of the amount of improvement attributable to the agent architecture.\n\n  * The LATS agent was evaluated on only a subset of the test cases provided in the HumanEval benchmark. This exaggerated their accuracy numbers, since the code for a particular HumanEval problem might be incorrect, but if it passes only a portion of the test cases for that problem, it could still be marked as correct. In our analysis, this was responsible for a 3% difference in accuracy (mean across five runs), which explains a substantial part of the difference between the accuracy we found and the one reported in the paper. In addition, many details about the implementation, such as hyperparameter values, were not reported in the paper or GitHub repository (see appendix for details).\n\n  * To the best of our knowledge, this post is the first time the four agents with the highest accuracy\u2014Retry, Warming, LDB (GPT-4), and LDB (GPT-4 + Reflexion)\u2014have been tested on HumanEval.16\n\n  * Reflexion, LDB, and LATS all use different subsets of HumanEval. Three (out of 164) coding problems in the original version of HumanEval lack example tests. Since these agents require example tests to debug or rerun their solutions, Reflexion removes the three problems that don't have example tests. LATS removes these three problems, plus another problem, for unreported reasons.17 LDB adds example tests for the three problems that are missing in the original benchmark. None of the three papers reports this. The paper introducing LATS claims (incorrectly): \"We use all 164 problems for our experiments.\"18 In our analysis, we conducted all evaluations on the version of the benchmark provided by LDB, since it contains example tests for all problems.\n\n  * The LDB paper claims to use GPT-3.5 for code generation using Reflexion: \"For Reflexion, we select the version based on GPT-3.5 and utilize the corresponding generated programs published in the official Github repository.\" However, the generated program they used from the Reflexion repository relies on GPT-4 for code generation, not GPT-3.5.19\n\nThese shortcomings in the empirical results have also led to errors of\ninterpretation in broader discussions around the accuracy of AI agents. For\nexample, a recent post by Andrew Ng claimed that agents that use GPT-3.5 can\noutperform GPT-4. In particular, he claimed:\n\n> [For HumanEval,] GPT-3.5 (zero shot) was 48.1% correct. GPT-4 (zero shot)\n> does better at 67.0%. However, the improvement from GPT-3.5 to GPT-4 is\n> dwarfed by incorporating an iterative agent workflow. Indeed, wrapped in an\n> agent loop, GPT-3.5 achieves up to 95.1%.\n\nWhile this claim received a lot of attention, it is incorrect. The claim\n(\"GPT-3.5 wrapped in an agent workflow achieves 95.1% accuracy\") seems to be\nabout the LDB agent. The Papers With Code leaderboard for HumanEval makes the\nsame claim. However, as we discussed above, for LDB, GPT-3.5 is only used to\nfind bugs. The code is generated using GPT-4 (or the Reflexion agent that uses\nGPT-4), not GPT-3.5. Unfortunately, the error in the paper has led to much\noveroptimism about agents in the broader AI community.\n\nNg's post also makes the familiar error of repeating results from papers\nwithout verifying them or accounting for changes in prompts and model\nversions. For example, the zero-shot accuracy numbers of GPT-3.5 (48.1%) and\nGPT-4 (67.0%) seem to be copied from the GPT-4 technical report from March\n2023. However, the models have been updated many times since release. Indeed,\nin our comparison, we find that the base models perform much better compared\nto the claimed figures in Ng's post when we use them with the prompts provided\nwith the LDB paper (GPT-3.5: 73.9%, GPT-4: 89.6%). As a result, the post\ndrastically overestimates the improvement attributable to agent architectures.\n\nEvaluation frameworks like Stanford's HELM and EleutherAI's LM Evaluation\nHarness attempt to fix similar shortcomings for model evaluations, by\nproviding standardized evaluation results. We are working on solutions to make\nagent evaluations standardized and reproducible, especially from the\nperspective of downstream evaluation of agents.\n\nFinally, downstream developers should keep in mind that HumanEval or any other\nstandardized benchmark is nothing more than a rough proxy for the specific\ntasks that arise in a particular downstream application. To understand how\nagents will perform in practice, it is necessary to evaluate them on a custom\ndataset from the domain of interest \u2014 or even better, A/B test different\nagents in the production environment.\n\n####\n\nFurther reading\n\n  * Zaharia et al. observe that state-of-the-art accuracy on AI benchmarks is often attained by composite systems. If the adoption of agents continues, visualizing cost and accuracy as a Pareto curve would become even more necessary.\n\n  * Santhanam et al. point out the importance of evaluating cost alongside accuracy for information retrieval benchmarks.\n\n  * Ozrmazabal et al. highlight the accuracy vs. cost per output token tradeoffs for various models (but not agents) on MMLU. While the cost of output tokens might not be a good indicator of the overall cost, given the varying input token costs as well as output lengths for different models, it is better than not reporting the tradeoffs at all.\n\n  * The Berkeley Function Calling leaderboard includes various metrics for language model evaluations of function calling, including cost and latency.\n\n  * Xie et al. develop OSWorld, a benchmark for evaluating agents in computer environments. In their GitHub repository (though not in the paper), they give a rough cost estimate for running various multimodal agents on their benchmark.\n\n  * Unsurprisingly, the main impetus for cost vs. accuracy tradeoffs has come from the downstream developers who use AI.\n\n  * In a previous talk, we discussed three major pitfalls in LLM evaluation: prompt sensitivity, construct validity, and contamination. The current research is largely orthogonal: prompt sensitivity isn\u2019t a concern for agent evaluation (as agents are allowed to define their own prompts); downstream developers can address contamination and construct validity by evaluating on custom datasets.\n\nThe code for reproducing our analysis is available here. The appendix includes\nmore details about our setup and results.\n\n####\n\nAcknowledgments\n\nWe thank Rishi Bommasani, Rumman Chowdhury, Percy Liang, Shayne Longpre, Yifan\nMai, Nitya Nadgir, Matt Salganik, Hailey Schoelkopf, Zachary Siegel, and Venia\nVeselovsky for discussions and inputs that informed our analysis. We\nacknowledge Cunxiang Wang and Ruoxi Ning for their prompt responses to our\nquestions about the NovelQA benchmark.\n\nWe are grateful to the authors of the papers we engage with in this post for\ntheir quick responses and for sharing their code, which makes such\nreproduction analysis possible in the first place. In particular, we are\ngrateful to Zilong Wang (LDB), Andy Zhou (LATS), and Karthik Narasimhan\n(Reflexion), who gave us feedback in response to an earlier draft of this blog\npost.\n\n1\n\nThe leaderboard on the linked page lists AgentCoder as the most accurate\nsystem. However, the code or data for reproducing the results of this agent\nare not available online, so we do not consider it in this blog post.\n\n2\n\nThis post is about agents. Leaderboards are also becoming less useful for\nevaluating the underlying models. There are many problems, including\ngameability. But controlling for inference cost isn\u2019t the main problem, so our\narguments don\u2019t necessarily apply.\n\n3\n\nTasks where increased compute could help indefinitely are primarily those\nwhere verifying whether a solution is correct is easy. In the case of\nprogramming questions, this takes the form of test cases that are provided\nwith each question to check if the answer is correct. Other examples include\nproving theorems, because verifying if a theorem is correct can be\nstraightforward, as well as some tasks on the internet for agents that\nnavigate the web. That said, even for tasks where there is no way to guess a\nsolution and then verify, the costs of different agents can vary by orders of\nmagnitude.\n\n4\n\nWe included agents from the HumanEval leaderboard on PapersWithCode that share\ntheir code publicly. Reflexion is absent from the PapersWithCode list, but it\nhas a reported accuracy of 91% (higher than any other agents with publicly\navailable code apart from LDB and LATS), so we included it too.\n\n5\n\nFor the model evaluation, we only used the description of the coding problem\nas well as the example tests provided with the HumanEval dataset. Three of the\n164 coding problems in HumanEval lack example tests. The authors of LDB\ninclude a modified version of HumanEval with example tests included for these\nthree problems. We use this modified version for all experiments.\n\n6\n\nIn all of the baselines we provide, we don't use the test cases used to\nevaluate if the solution is correct when deciding to retry, only the ones in\nthe problem description, to avoid leakage.\n\n7\n\nWe evaluated Llama-3 using together.ai endpoints. The cost per million tokens\non together.ai, for both prompt and completion, is 0.20$ and 0.90$ for\nLlama-3-8B and Llama-3-70B, respectively.\n\n8\n\nThis is also true for other desired properties of agents, such as running\ntime. We report results for time vs. accuracy tradeoffs in the appendix.\n\n9\n\nWhile some of the papers introducing these models discuss cost abstractly,\nsuch as the relationship between cost and number of times an agent retries,\nthey don't report any concrete numbers on cost or compare token count to a\nbaseline.\n\n10\n\nThe cost comparison is for LDB (Reflexion, GPT-3.5), since that is the top-\nperforming agent reported by the authors of LDB.\n\n11\n\nIn addition to HumanEval, we also ran experiments on the HotPotQA and NovelQA\nbenchmarks for question answering. We found similar results for both\nbenchmarks. In particular, we found that there can be large differences in\ncost underlying small improvements in accuracy for both benchmarks.\n\n12\n\nOne potential concern with our analysis is that while we relied on the April\n2024 version of OpenAI models, many papers relied on older model versions for\ntheir results. To address this, we report results for an additional robustness\ncheck with the June 2023 version of OpenAI models in the appendix; we find\nsubstantially similar results across model versions.\n\n13\n\nWhile HumanEval is commonly used to evaluate how well AI can solve coding\nproblems, it is limited due to its small size (only 164 questions), lack of\ndifficult problems (none of the problems involve real-world tasks), and\npotential contamination, since language models have likely been trained on\nHumanEval problems, which might inflate the performance of the simple\nbaselines we test. A more rigorous examination of hypotheses related to\nwhether System 2 thinking helps will likely require the use of more\ncomprehensive and robust benchmarks, such as SWE-bench.\n\n14\n\nLDB uses already-existing solutions to improve them by debugging. The existing\nsolutions can come from models like GPT-3.5 or GPT-4, or from agents like\nReflexion. Since the authors of Reflexion provided all of the generated\nsolutions in their Github repo, the authors of the LDB paper used code from\nthe original Reflexion repository to run their analysis, rather than rerunning\nthe Reflexion agent. The difference between the reported results and our\nreproduced results could be due to differences in the code generated by the\nReflexion agent. Reusing Reflexion solutions is a reasonable choice for\nevaluating the usefulness of debugging (indeed, we see LDB increases the\naccuracy over using the models alone). The problem arises when their final\naccuracy is interpreted as a downstream evaluation, since it might give\ndevelopers an inflated estimate of the accuracy of such techniques for coding.\n\n15\n\nThe authors acknowledge this and plan to update their results.\n\n16\n\nThe authors of LDB only tested the GPT-3.5 model as the debugger, which\nperformed notably worse than the agent using GPT-4 as the debugger, with an\naccuracy of 88.9% for LDB (GPT-3.5 + Reflexion) vs. 92.9% for LDB (GPT-4 +\nReflexion).\n\n17\n\nIn correspondence with the authors of LATS, they clarified: \"Originally, there\nwas an execution error when evaluating some test cases for [one of the\nHumanEval test cases], so we opted to remove it from our setting.\"\n\n18\n\nThe authors acknowledge this and plan to update the paper to address it.\n\n19\n\nThe authors acknowledge this and plan to update the paper to address it.\n\n### Subscribe to AI Snake Oil\n\nLaunched 2 years ago\n\nWhat makes AI click, what makes it fail, and how to tell the difference\n\n16 Likes\n\n\u00b7\n\n2 Restacks\n\n15\n\nShare this post\n\n#### AI leaderboards are no longer useful. It's time to switch to Pareto\ncurves.\n\nwww.aisnakeoil.com\n\n1\n\nShare\n\n1 Comment\n\nNathan LambertInterconnects2 hrs ago\u00b7edited 2 hrs agoLiked by Sayash Kapoor,\nArvind NarayananI was writing about this recently too, from a somewhat broader\nangle of \"best open language models.\" In reality, *every meaningful\ntechnology* must have a Pareto with cost.If cost isn't a factor, people don't\nreally use it yet.Edit, just got to the point where you referenced me :), but\nyeah cost of running the model was also on my mind. Great points as usual from\nyou all.Expand full commentLike (2)ReplyShare  \n---  \n  \nGPT-4 and professional benchmarks: the wrong answer to the wrong question\n\nOpenAI may have tested on the training data. Besides, human benchmarks are\nmeaningless for bots.\n\nMar 20, 2023 \u2022\n\nArvind Narayanan\n\nand\n\nSayash Kapoor\n\n123\n\nShare this post\n\n#### GPT-4 and professional benchmarks: the wrong answer to the wrong question\n\nwww.aisnakeoil.com\n\n22\n\nIs GPT-4 getting worse over time?\n\nA new paper going viral has been widely misinterpreted\n\nJul 19, 2023 \u2022\n\nArvind Narayanan\n\nand\n\nSayash Kapoor\n\n118\n\nShare this post\n\n#### Is GPT-4 getting worse over time?\n\nwww.aisnakeoil.com\n\n12\n\nChatGPT is a bullshit generator. But it can still be amazingly useful\n\nThe philosopher Harry Frankfurt defined bullshit as speech that is intended to\npersuade without regard for the truth. By this measure, OpenAI\u2019s new...\n\nDec 6, 2022 \u2022\n\nArvind Narayanan\n\nand\n\nSayash Kapoor\n\n56\n\nShare this post\n\n#### ChatGPT is a bullshit generator. But it can still be amazingly useful\n\nwww.aisnakeoil.com\n\n13\n\nReady for more?\n\n\u00a9 2024 Sayash Kapoor and Arvind Narayanan\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": true}
