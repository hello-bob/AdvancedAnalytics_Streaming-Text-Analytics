{"aid": "40247937", "title": "NIST Drafts New Security Framework to Tackle Emerging Risks of Generative AI", "url": "https://socket.dev/blog/nist-drafts-new-security-framework-to-tackle-emerging-risks-of-generative-ai", "domain": "socket.dev", "votes": 1, "user": "feross", "posted_at": "2024-05-03 14:17:04", "comments": 0, "source_title": "NIST Drafts New Security Framework to Tackle Emerging Risks of Generative AI - Socket", "source_text": "NIST Drafts New Security Framework to Tackle Emerging Risks of Generative AI -\nSocket\n\nSign inDemo\n\nSecurity News\n\n# NIST Drafts New Security Framework to Tackle Emerging Risks of Generative AI\n\n## NIST's new AI Risk Management Framework aims to enhance the security and\nreliability of generative AI systems and address the unique challenges of\nmalicious AI exploits.\n\nSarah Gooding\n\nMay 3, 2024\n\nNIST (National Institute of Standards and Technology) has published an initial\npublic draft of its Artificial Intelligence Risk Management Framework, as part\nof an effort to implement President Biden\u2019s executive order on AI.\n\nThe draft publications cover a range of concerns from addressing the problem\nof malicious training data to promoting transparency in digital content. They\nare aimed at helping improve the safety, security, and trustworthiness of\nartificial intelligence (AI) systems and one of the drafts proposes a plan for\ndeveloping global AI standards.\n\n## NIST Highlights AI-Augmented Hacking, Malware, and Phishing as Emerging\nSecurity Threats#\n\nThe document for \u201cMitigating the Risks of Generative AI\u201d is the one most\nrelevant for cybersecurity professionals. It\u2019s clear the U.S. federal\ngovernment has autonomous exploits on its radar, among other threats.\n\nOutlined in the overview of risks unique to or exacerbated by GAI (Generative\nAI), NIST identified a host of concerns related to information security:\n\n> Lowered barriers for offensive cyber capabilities, including ease of\n> security attacks, hacking, malware, phishing, and offensive cyber operations\n> through accelerated automated discovery and exploitation of vulnerabilities;\n> increased available attack surface for targeted cyber attacks, which may\n> compromise the confidentiality and integrity of model weights, code,\n> training data, and outputs.\n\nThe document highlights the potential augmentation of offensive cyber\ncapabilities like hacking, malware, and phishing. It cites the recent\npublication from researchers claiming to have created a GPT-4 agent that can\nautonomously exploit both web and non-web vulnerabilities in real-world\nsystems. Some regard these as emerging AI capabilities, but even if they\naren't - autonomous exploits are undoubtedly coming in the near future.\n\nPrompt-injection and data poisoning are identified as some of the most\nconcerning GAI vulnerabilities, which could cause affected systems to\nprecipitate \u201ca variety of downstream consequences to interconnected systems.\u201d\n\n> Security researchers have already demonstrated how indirect prompt\n> injections can steal data and run code remotely on a machine. Merely\n> querying a closed production model can elicit previously undisclosed\n> information about that model. Information security for GAI models and\n> systems also includes security, confidentiality, and integrity of the GAI\n> training data, code, and model weights.\n\nThe document was created over the past year and includes feedback from NIST\u2019s\ngenerative AI public working group of more than 2,500 members. It focuses on a\nlist of 13 risks and identifies more than 400 actions developers can take to\nmanage them. These actions include some best practices and recommendations for\ninformation security with respect to supply chains:\n\n  * Apply established security measures to: Assess risks of backdoors, compromised dependencies, data breaches, eavesdropping, man-in-the-middle attacks, reverse engineering other baseline security concerns;\n  * Audit supply chains to identify risks arising from, e.g., data poisoning and malware, software and hardware vulnerabilities, third-party personnel and software;\n  * Audit GAI systems, pipelines, plugins and other related artifacts for unauthorized access, malware, and other known vulnerabilities.\n\nMany of the recommended actions pertain to securing generative AI systems:\n\n  * Establish policies to incorporate adversarial examples and other provenance attacks in AI model training processes to enhance resilience against attacks.\n  * Implement plans for GAI systems to undergo regular adversarial testing to identify vulnerabilities and potential manipulation risks.\n  * Conduct adversarial role-playing exercises, AI red-teaming, or chaos testing to identify anomalous or unforeseen failure modes.\n  * Conduct security assessments and audits to measure the integrity of training data, system software, and system outputs.\n\nIn their current form, the drafts published this week are aimed at helping\norganizations make decisions on how best to manage AI risk in a way that will\ncoincide with what may become future regulatory requirements. The\nrecommendations will likely contribute to shaping best practices for securing\nAI systems and software as the technology rapidly advances in capabilities.\n\nThe documents were published as drafts that are open to comment from the\npublic via email or on the www.regulations.gov website. NIST is particularly\ninterested in feedback on the glossary terms, risk list, and proposed actions\nfor mitigation. Comments will close on June 2, 2024, ahead of the document\u2019s\npublication in the Federal Register Notice.\n\nSubscribe to our newsletter\n\nGet notified when we publish new security blog posts!\n\n## Related posts\n\nBack to all posts\n\nSecurity News\n\n### Risky Biz Podcast: How Shifts in Open Source Made It a Prime Attack Vector\n\nThis episode of the Risky Biz podcast discusses how the rise of small open\nsource packages and the shift towards individual maintainers makes the\necosystem more vulnerable to supply chain attacks.\n\nBy Sarah Gooding - May 01, 2024\n\nProduct\n\n### Introducing SSO\n\nStreamline your login process and enhance security by enabling Single Sign-On\n(SSO) on the Socket platform, now available for all customers on the\nEnterprise plan, supporting 20+ identity providers.\n\nBy Alex Morais - Apr 29, 2024\n\nSecurity News\n\n### tea.xyz Spam Plagues npm and RubyGems Package Registries\n\nTea.xyz, a crypto project aimed at rewarding open source contributions, is\nonce again facing backlash due to an influx of spam packages flooding public\npackage registries.\n\nBy Sarah Gooding - Apr 27, 2024\n\nProduct\n\n  * Package Alerts\n\n  * Integrations\n\n  * Docs\n\n  * Pricing\n\n  * FAQ\n\n  * Roadmap\n\nAbout\n\n  * About\n\n  * Love\n\n  * Blog\n\n  * Glossary\n\n  * Discord Community\n\n  * CareersHiring\n\n  * Send Feedback\n\n  * Contact Us\n\n  * System Status\n\nPackages\n\n  * npm Package Directory\n\n  * Explore npm Packages\n\n  * Go Package Directory\n\n  * Explore Go Packages\n\n  * Maven Package Directory\n\n  * Explore Maven Packages\n\n  * PyPI Package Directory\n\n  * Explore PyPI Packages\n\n  * Most Popular JS Packages\n\n  * Top JavaScript Maintainers\n\n  * Removed npm Packages\n\n  * Random npm Package\n\nStay in touch\n\nGet open source security insights delivered straight into your inbox.\n\n  * Terms\n  * Privacy\n  * Security\n\nMade with \u26a1\ufe0f by Socket Inc\n\n", "frontpage": false}
