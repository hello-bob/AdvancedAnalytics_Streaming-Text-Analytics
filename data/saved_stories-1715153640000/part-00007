{"aid": "40291801", "title": "IBM's Granite code model family is going open source \u2013 IBM Research", "url": "https://research.ibm.com/blog/granite-code-models-open-source", "domain": "ibm.com", "votes": 1, "user": "boilerupnc", "posted_at": "2024-05-07 21:36:25", "comments": 0, "source_title": "IBM\u2019s Granite code model family is going open source", "source_text": "IBM\u2019s Granite code model family is going open source - IBM Research\n\nSkip to main content\n\nResearch\n\n# Open sourcing IBM\u2019s Granite code models\n\n06 May 2024\n\nRelease\n\n7 minute read\n\nIBM is releasing a family of Granite code models to the open-source community.\nThe aim is to make coding as easy as possible \u2014 for as many developers as\npossible.\n\n  * Read the full technical paper on the code models\n  * Download the Granite models on GitHub\n  * View the Granite models on Hugging Face\n\nIBM is releasing a family of Granite code models to the open-source community.\nThe aim is to make coding as easy as possible \u2014 for as many developers as\npossible.\n\nOver the last several decades, software has been woven into the fabric of\nevery aspect of our society. But for all the increased productivity that\nmodern software has brought to how we work, the actual act of writing,\ntesting, debugging, and shipping reliable software is still an arduous task.\nEven the most skilled developer needs to search for tips and shortcuts, code\nlanguages are constantly being updated, and new languages are released nearly\nevery day.\n\nThis is why IBM Research first started exploring whether AI could make it\neasier to develop and deploy code. In 2021, we unveiled CodeNet, a massive,\nhigh-quality dataset with 500 million lines of code in over 50 programming\nlanguages, as well as code snippets, code problems and descriptions. We saw\nthe value that could be unlocked in building a dataset that could train future\nAI agents \u2014 the ones that we envisioned would translate code from legacy\nlanguages to those that power enterprise today. Others, we saw, would teach\ndevelopers how to fix issues in their code, or even write code from basic\ninstructions written in plain English.\n\nLarge language models (LLMs) trained on code are revolutionizing the software\ndevelopment process. Increasingly, code LLMs are being integrated into\nsoftware development environments to improve the productivity of human\nprogrammers, and LLM-based agents showing promise in handling complex tasks\nautonomously. Realizing the full potential of code LLMs requires a wide range\nof capabilities, including code generation, fixing bugs, explaining and\ndocumenting code, maintaining repositories, and more.\n\nThe tremendous potential with LLMs that emerged over the last few years fueled\nour desire to turn our vision into a reality. And that\u2019s exactly what we\u2019ve\nbegun to do with the IBM watsonx Code Assistant (WCA) family of products, like\nWCA for Ansible Lightspeed for IT Automation, and WCA for IBM Z for\napplication modernization. WCA for Z uses a combination of automated tooling\nand IBM\u2019s own 20-billion parameter Granite large language code model which\nenterprises can use to transform monolithic COBOL applications into services\noptimized for IBM Z.\n\nWe\u2019ve striven to find ways to make developers more productive, spending less\nof their time figuring out why their code won\u2019t run, or how to get a legacy\ncodebase to communicate to newer applications. And that\u2019s why today we\u2019re\nannouncing that we\u2019re open-sourcing four variations of the IBM Granite code\nmodel.\n\n## What we\u2019re open sourcing\n\nWe\u2019re releasing a series of decoder-only Granite code models for code\ngenerative tasks, trained with code written in 116 programming languages. The\nGranite code models family consists of models ranging in size from 3 to 34\nbillion parameters, in both a base model and instruction-following model\nvariants. These models have a range of uses, from complex application\nmodernization tasks to on-device memory-constrained use cases.\n\nComparing the Granite-8B-Code (Base/Instruct) with other similarly sized open-\nsource code LLMs on HumanEvalPack. Three coding tasks and six programming\nlanguages were used.\n\nEvaluation on a comprehensive set of tasks has shown that these Granite code\nmodels consistently match state-of-the-art performance among open-source code\nLLMs currently available. The versatile model family was optimized for\nenterprise software development workflows and performs well across a range of\ncoding tasks, including code generation, fixing, and explanation.\n\nThese models are available on Hugging Face, GitHub, watsonx.ai, and RHEL AI,\nRed Hat\u2019s new foundation model platform for developing, testing, and deploying\ngenerative AI models. The underlying base code models are the same as the one\nused to train WCA for specialized domains.\n\nAll the models were trained on data that was collected in adherence with IBM\u2019s\nAI ethics principles and with the IBM legal team\u2019s guidance for trustworthy\nenterprise use. These Granite Code models are released today under the Apache\n2.0 license.\n\nWe\u2019re also releasing Granite Code Instruct models. This is the instruction\nmethodology we used on the models to fine-tune, using a combination of Git\ncommits paired with human instructions and open-source synthetically generated\ncode instruction datasets.\n\n## Why we\u2019re open sourcing the Granite code models\n\nWe believe in the power of open innovation, and to get to a future where\nwriting code is as easy as talking to an always-on assistant, we want to reach\nas many developers as possible. No effective system is ever created by a\nsingle individual \u2014 the best work builds on the collective knowledge of those\nwho have come before.\n\nWhile the general popularity of generative AI models has skyrocketed in recent\nyears, enterprise adoption has been slower \u2014 for good reason. In the wider\nworld of LLM research and deployment, the major models have now grown to tens\nof billions of parameters, many with 70 billion or more. While that\u2019s useful\nfor organizations looking to build generalized chatbots that understand a wide\nrange of subjects, these models are computationally expensive to train and\nrun. For enterprises, massive models can become unwieldy for more specific\ntasks, full of irrelevant information and running up high inferencing costs.\n\nMany enterprises have been reluctant to adopt LLMs for commercial purposes for\nseveral reasons beyond just the cost. The licensing of these models is often\nunclear, and how these models were trained, and how the data was cleaned and\nfiltered for things like hate, abuse, and profanity are often unknown.\n\n\"We are transforming the generative AI landscape for software by releasing the\nhighest performing, cost-efficient code LLMs, truly empowering the open\ncommunity to innovate on top for many use cases, without any restrictions \u2014\nfor research, commercial use cases, and beyond,\" said Ruchir Puri, chief\nscientist at IBM Research, who leads IBM\u2019s efforts to bring coding assistants\nto the world. \"I am very excited about the future of software with generative\nAI.\"\n\nPuri believes that for many enterprise use cases, the 8B Granite code model\nvariant we\u2019ve released will be the right combination of weight, cost to run,\nand capability. But we\u2019re also offering lighter and weightier versions that\nanyone in the open-source community can try out and see if they better fit\ntheir needs.\n\n## What the Granite code models enable\n\nFor many developers, writing code is not actually what takes up most of their\ntime. Instead, it\u2019s testing what they\u2019ve written, ensuring it runs as\nintended, and finding and fixing any bugs that arise. Right now, a developer\u2019s\nworkflow might see them constantly jumping between whatever code they\u2019re\nworking on, and various online forums to figure out answers to their issues.\nIt\u2019s syncopated and often time-consuming.\n\nWith tools built on the IBM Granite code models, we envision a myriad of\nenterprise use cases for developers. That could range from agents that could\nwrite code for developers, tools that can explain why code isn\u2019t working, and\nhow to fix it. Many of the other quotidian but essential tasks that are part\nof a developer\u2019s day \u2014 from generating unit tests, to writing documentation or\nrunning vulnerability tests \u2014 could be automated with these models.\n\nAnd we see value in using these models to modernize mission-critical\napplications that need to remain secure, resilient, and most importantly,\nonline. With generative systems built on Granite models, developers can create\nnew ways to translate legacy codebases like COBOL into more modern languages\nlike Java. It\u2019s one of the major uses for code models that IBM saw when first\ndiving into the world of AI for code, and remains one of the most important.\n\n## Doubling down on parameters\n\nFor the 34B version of the model, we used a novel method called depth\nupscaling to train the model. First, we created a duplicated version of the\n20B variant, which has 52 layers to it. We removed the final eight layers from\nthe first version of the 20B, and the first eight from the second version. We\nthen merged the two versions to create a new model with 88 layers. We used the\nsame 8,192 token context window when pre-training both the 20B and 34B model.\n\nAn overview of depth upscaling for training the Granite-34B-Code model.\n\n## How the models perform\n\nIn testing against a range of other models, including those that have been\nopened under Apache 2.0 licenses, and more proprietary models, we found our\nmodels able to compete at a range of tasks. Testing on benchmarks including\nHumanEvalPack, HumanEvalPlus, and RepoBench, we saw strong performances on\ncode synthesis, fixing, explanation, editing, and translation, across most\nmajor programming languages, including Python, JavaScript, Java, Go, C++, and\nRust.\n\nHow Granite-8B-Code-Instruct performs, compared with Mistral-7B-Instruct-v0.2,\nGemma-7B-IT, and Llama-3-8B-Instruct, on HumanEvalPack.\n\nOur models can outperform some twice their size, such as with Code Llama, and\nwhile some other models may perform slightly better in some tasks like code\ngeneration, no one model could perform at a high level at generation, fixing,\nand explanation \u2014 apart from Granite.\n\nModel| MATH| GSM8K| SAT| OCW| MATH+Py| GSM8K+Py  \n---|---|---|---|---|---|---  \nStarCoderBase-7B| 2.4| 3.8| 18.7| 2.2| 18.2| 15.6  \nCodeLlama-7B| 4.1| 11.9| 12.5| 2.9| 20.8| 26.8  \nStarCoder2-7B| 10.4| 27.2| 37.5| 4.8| 28.7| 39.4  \nCodeGemma-7B| 21.8| 49.0| 53.1| 6.9| 31.1| 60.9  \nGranite-8B-Code-Base| 21.4| 61.9| 62.5| 8.8| 35.4| 63.1  \nGemma-7B| 24.1| 53.3| 75.0| 7.3| 27.4| 52.9  \nMistral-7B-v0.2| 12.8| 37.2| 53.1| 5.8| 25.7| 45.6  \nLlama-3-8B| 15.6| 49.8| 34.4| 9.9| 0.0*| 2.4  \nLemma-7B| 17.3| 33.7| 59.4| 7.0| 25.6| 40.8  \n  \n* The researchers noticed that Llama-3-8B-Base tends to generate invalid programs given the same prompts as the other model, resulting in very low scores on MATH+Py and GSM8K+Py tasks.\n\nThe models have a unique blend of data sources that the team believes sets\nthem apart. They used GitHub Code Clean, StarCoderData, and other public code\nrepositories and issues on GitHub. Combined with the robust metadata in\nCodeNet, which outlines code issues in plain English, they mixed the code\nsources, natural language documentation, and code problems in a specific way\nto train the models.\n\nHow the Granite Code models perform on the Berkeley Function-Calling\nLeaderboard. Overall accuracy keeps increasing as the model size\nincreases.Granite-8B-Code vs CodeLlama-7B on Berkley Function-Calling\nLeaderboard. The Granite-8B-Code model (Base and Instruct) consistently\noutperform CodeLlama-7B (Base/Instruct) on all three metrics.\n\nThe base models were trained from scratch on between 3 and 4 trillion tokens\nfrom 116 programming languages, as well as 500 billion tokens with our\ncarefully designed mixture of high-quality data from code and natural\nlanguage, which improved the models\u2019 ability to reason and its problem-solving\nskills, that are essential for code generation.\n\n## What\u2019s next\n\nWith the Granite code models, we\u2019re releasing models to the community that we\nthink stack up to just about any comparable model. We\u2019re excited to see what\nwill be built with these models, whether that\u2019s new code generation tools,\nstate-of-the-art editing software, or anything in between.\n\nAnd this is just one aspect of the wider Granite family of models from IBM\nthat have been designed, incubated, and released from within IBM Research. In\nthe coming weeks, we\u2019ll share more about other models and modalities that we\nbelieve will help shape the future of computing in exciting new ways.\n\nSubscribe to our Future Forward newsletter and stay up to date on the latest\nresearch news\n\nSubscribe to our newsletter\n\n## Date\n\n06 May 2024\n\n## Authors\n\n  * Mike Murphy\n\n## Topics\n\n  * AI\n  * Open Source\n\n## Share\n\n## IBM, Canada, and Quebec are partnering to secure the future of chipmaking\nin North America\n\nNews\n\nMike Murphy\n\n26 Apr 2024\n\n  * AI Hardware\n\n  * Semiconductors\n\n## Mitigating the environmental harm of PFAS \u2018forever chemicals\u2019\n\nNews\n\nKim Martineau\n\n22 Apr 2024\n\n  * Accelerated Discovery\n\n  * AI\n\n  * Exploratory Science\n\n  * Foundation Models\n\n  * Generative AI\n\n  * Natural Language Processing\n\n## What is red teaming for generative AI?\n\nExplainer\n\nKim Martineau\n\n11 Apr 2024\n\n  * Adversarial Robustness and Privacy\n\n  * AI\n\n  * AI Testing\n\n  * Fairness, Accountability, Transparency\n\n  * Foundation Models\n\n  * Natural Language Processing\n\n  * Security\n\n  * Trustworthy AI\n\n## The first IBM quantum computer installed on a university campus comes to\nRPI\n\nNews\n\nMike Murphy\n\n05 Apr 2024\n\n  * Quantum\n\n  * Science\n\nPreviousHow execution modes enable efficient, utility-scale workloads\n\n", "frontpage": false}
