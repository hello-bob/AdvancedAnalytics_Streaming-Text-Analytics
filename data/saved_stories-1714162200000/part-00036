{"aid": "40168647", "title": "How to Overcome the Problems You'll Encounter When Working with LLMs", "url": "https://decoderonlyblog.wordpress.com/2024/04/19/unbowed-unbent-unbroken/", "domain": "decoderonlyblog.wordpress.com", "votes": 1, "user": "charlieda", "posted_at": "2024-04-26 12:35:07", "comments": 0, "source_title": "Unbowed, Unbent, Unbroken", "source_text": "Unbowed, Unbent, Unbroken \u2013 Decoder Only\n\nSkip to content\n\n# Decoder Only\n\n## Unpicking the AI revolution\n\nWritten by WillApr 19, 2024Apr 19, 2024\n\n# Unbowed, Unbent, Unbroken\n\n## How to overcome the problems you\u2019ll encounter when working with Large\nLanguage Models.\n\n#### A cheatsheet for teams working on their first LLM applications.\n\nSo, your company or organization is building some kind of application that\ninvolves a Large Language Model (LLM) and you\u2019ve hit a roadblock. This is\nnormal. Breathe out, take a moment and then scan the guide below. It\u2019s likely\nthat your problem is one of these.\n\nThe purpose of this article is to help point you in the direction of the\nsolutions. I\u2019ve included links and references for further reading. Some of\nthem (like RLHF) are a bit finnicky to get to work in practise. In the future,\nI\u2019ll write up some deep-dives into these areas.\n\n### Problem 1. Regulatory restrictions, IP concerns or privacy issues mean we\ncannot send data to an LLM provider.\n\nYou have two options: either you remove the worrisome material before you send\nyour data out, or you never send it out at all.\n\nIf you take the former approach, take a look at libraries like Microsoft\u2019s\nPresidio, which will capably redact many common forms of PII. Additionally,\nask a friendly data scientist to design some classifiers that will catch any\nforms of sensitive data which Presidio misses. Alternatively, look at a\nspecialist provider like PredictionGuard \u2013 who have put a lot of work into\nsolving this problem already \u2013 and integrate their service into your solution.\n\nThe latter approach involves using an open source LLM and hosting it yourself.\n(This is not necessarily as intimidating as it might sound; tools like\nSkyPilot will handle a lot of the complexity for you.) At the time of writing,\nthere are some incredibly powerful, open source LLMs (e.g. Mistral Large) that\nwill excel at all but the most complex of reasoning tasks.\n\nTechnical tip. If you need to rely on the vision capabilities present in\nmodels like Gemini, Claude 3 or GPT4V, take a look at the highly capable QWEN-\nVL series of models or wait for X.AI\u2019s Grok 1.5V \u2013 expected in mid-2024 \u2013\nwhich, rumour has it, is likely to be open-sourced.\n\n### Problem 2. I have a very domain-specific use case for my LLM. For example,\na code assistant that needs to work with custom libraries and schemas.\n\nOK, so you probably need to use an open source LLM here. You\u2019ll also need to\ncurate a decent dataset of examples spanning your specialist domain. The\ntechnique you\u2019ll want to understand is called either \u201ccontinued pre-training\u201d\nor \u201cdomain-adaptive pre-training\u201d (Nvidia used this technique to train their\nChipNeMo model). This involves using a large volume of sample data and the\nstandard, next-token prediction task to teach your LLM the required new\nknowledge, for example how to code using your company\u2019s bespoke libraries.\n\nTechnical tip. Most models have been instruction fine-tuned before release so\nthat they are well aligned for common tasks like question-answering,\nsummarisation and dialogue. Continued pre-training will likely degrade these\ncapabilities. Plan, therefore, to perform a round of supervised-finetuning\n(preferably using a mix of data from the fine-tuning datasets used by the\nmodel\u2019s developers and your own examples) to restore the model\u2019s alignment.\n\n### Problem 3. The LLM is not reliable enough. It often makes mistakes and/or\nfails to answer questions correctly.\n\nMuch like teenagers, although LLMs are highly capable of following\nsophisticated reasoning patterns, they often completely fail to. Prompt\nengineering is the art of guiding an LLM to structure it\u2019s reasoning and\nresponses in a manner appropriate to the problem at hand. Prompt engineering\ntechniques (and there are lot of them out there!) are designed to enforce\ncorrect reasoning, ensure that answers match the stated requirements and\nreduce hallucinations.\n\nYou can find an excellent overview of prompt engineering techniques here but\nthis is one area where it\u2019s worth staying close to the research from sources\nlike arXiv or HuggingFace daily papers . On a monthly basis, new prompt-\nengineering techniques will emerge \u2013 often for specialist use cases \u2013 each\nseeking to squeeze an extra few percentage points of correct answers from your\nLLM application. Build, iterate and improve.\n\n### Problem 4. The LLM is not generating responses in the format that I want:\ne.g. I want it to be more formal, more succinct, more chatty, or to match a\nhouse style.\n\nIn contrast to Problem 2, the issue here is not that the LLM lacks any\nparticular knowledge, but that its behaviour needs to be modified. We can\nmodify the behaviour of LLMs in two ways: with in-context learning (ICL) and\nwith fine-tuning.\n\nIn-context learning simply involves offering one or more curated examples to\nthe model at query-time, demonstrating how you\u2019d like it to respond. All\nmodels have a context window which does limit the amount of text you can give\nit, but these are getting extremely large (for GPT4-turbo it\u2019s ~100k words;\nfor Claude Opus it\u2019s ~150k words; for Gemini Pro it\u2019s ~750k words) so you can\nuse a lot of examples. ICL is quick to implement and very often will solve the\nproblem.\n\nTechnical tip. You can even get you application to select relevant examples to\noffer the LLM at query time, based on the user\u2019s input \u2013 or on any other\nrelevant data you may have to hand.\n\nThe downside of ICL is that you\u2019re now paying to process many more tokens and\nthe response times may slow appreciably. The alternative is to use supervised\nfine-tuning (SFT) to teach the model what good responses look like. You\u2019ll\nneed a minimum of a few hundred, high quality query/response examples (perhaps\nmore; YMMV). Since you\u2019re actually adjusting the model weights during SFT,\nyou\u2019ll either need to be working with an open source LLM or using a commercial\nmodel that supports fine-tuning (e.g. GPT-3.5 Turbo). However, once you\u2019ve\nfine-tuned, you\u2019ll be able to get away with few or no ICL examples.\n\nAnother technical tip. As with continued pre-training, it\u2019s easy to degrade\nother model capabilities during SFT. Start off with a single training epoch\nand use a low learning rate. You might need to mix in additional SFT examples\nto preserve existing behaviours.\n\nYet another technical tip. It\u2019s perfectly possible to use SFT to create many\nspecialised variants of the LLM. This can be helpful if, for instance, your\napplication needs to send very different kinds of query or requires a range of\ndifferent styles or response. Using adaptors you can quickly switch between\nfine-tuned models depending on the behaviour you want.\n\nA final technical tip. For best results, make sure you follow the fine-tuning\ntemplate that was used for the model\u2019s release checkpoint. For example, for\nMistral series models it\u2019s this:\n\n    \n    \n    <s>[INST]{your_instruction}\\n{additional_context}\\n[/INST]{best_response}</s>\n\n### Problem 5. Supervised fine-tuning is not working. It\u2019s too difficult to\ngenerate a large enough and wide enough variety of examples.\n\nYou\u2019re not alone. It\u2019s very difficult to know that you\u2019ve covered a wide\nenough range of model query/response pairs to cover your application. It\u2019s\nalso expensive and time-consuming to write them.\n\nFor instance: in actual usage, you may often see requests without analogues in\nour SFT dataset and the model might not respond well in these situations.\n\nOpenAI faced the same problem when developing the precursor to ChatGPT. Their\nsolution was a process called Reinforcement Learning from Human Feedback\n(RLHF). The idea is that rather than have humans curate \u201cideal\u201d responses, we\nallow the model to generate several responses to our queries (yes, we still\nneed a large bank of example queries to make this work) and we tell it which\none we prefer. After doing this many times, we can train a \u201creward model\u201d that\nlearns which answer we are likely to prefer. Now we can set the reinforcement\nlearning algorithm off. It will keep reading queries, generating responses and\nmarking its own homework. Over time, it should begin to reliably generate\nresponses matching the required style and format.\n\nRLHF is a fiddly technique and will be the subject of a forthcoming \u201clessons\nlearned\u201d blog (where I\u2019ll explain everything I learned actually making it work\nin practise) but it is an extremely powerful tool for getting the \u201clast mile\u201d\nof behavioural adaption from your LLM. For now, a good overview of the\ntechnique can be found here.\n\n### Problem 6. The LLM is supposed to be searching or referencing my content\nbut often fails to locate the most relevant material.\n\nYou\u2019re building a solution which uses Retrieval-Augmented Generation (RAG).\nThere\u2019s likely one of two things going wrong here: either you application is\nnot retrieving the right content, of the LLM is not picking out the relevant\ninformation from it.\n\nA lot of ink has been spilled about resolving the former. The essentials are\nas follows: RAG relies upon an embedding (how queries and documents are\nvectorised) and, optionally, a reranker (which sorts matching documents by\nrelevancy). You can test different combinations of these components out on\nyour dataset using the handy Retrieval Evaluation module in the LlamaIndex\nlibrary to find what works best for your data. And if nothing does... you can\nalways ask that friendly data scientist to fine-tune you your own embedding\nmodel.\n\nIf retrieval and reranking is not the problem \u2013 or if you\u2019re not using RAG but\nare simply trying to get the LLM to extract information from a long document\nin the prompt \u2013 then you may be suffering from the \u201clost-in-the-middle\u201d\nsyndrome. Even with long context windows, LLMs can often fail to extract key\ninformation when it appears in the middle of a document. See this tweet for a\ngood illustration of the problem. The best solution here is to use some\nprompt-engineering: split the long context up into shorter blocks and feed\nthem to the LLM one at a time, then combine the individual analyses in a final\nprompt.\n\n### Problem 7. It\u2019s proving too expensive. My application needs the power of a\nfrontier model but the scale of requests I make pushes the price too high.\n\nThere\u2019s a fierce price-war going on between the vendors of frontier models,\nwith prices dropping periodically. Still, for some use cases, you could end up\nmaking an awful lot of requests and the bills can rack up. Some suggestions\nfor tackling this:\n\nThe \u201cMaster / Apprentice\u201d set-up. (Don\u2019t Google that term; I think I just made\nit up. It does work, however!) This works when you have a complex, multi-step\nreasoning task that involves making multiple queries to the LLM. Essentially,\nyou pass your initial, high-level query to a powerful LLM and have it write a\n\u201crecipe\u201d for resolving the query. You then pass each step in the recipe to a\nsmaller, cheaper LLM (it could be one you host yourself) and ask it to perform\nonly that step. You accumulate (and check) your interim results until the\nentire process has been completed.\n\nTechnical tip. You can always use the most powerful LLM to check your final\nresults before returning them to the user. You can also use it for a fallback;\nif the smaller models do not yield a good result, revert to the most powerful\none.\n\nCaching. RAG isn\u2019t just for document search. You can maintain a vector\ndatastore of previous LLM responses to user queries and check it each time a\nnew query is submitted. If you find a close match, you could return the\nprevious result. (Obviously this only works if responses remain relevant over\na period of time and if the LLM has not had to produce specific, query-related\ndetails as part of it\u2019s response.)\n\nEfficient Compute. If you\u2019re using an open source LLM, try hosting it on a\nservice like Beam, where you can set your LLM to be idle when not being used.\nIf you application is used only periodically, you can get substantial cost-\nsavings from a set-up like this.\n\n### Problem 8. My LLM is too slow.\n\nAh, this one\u2019s caused me a few headaches in the past. If you\u2019re using a\nfrontier LLM service then you\u2019re options are limited: either reduce the size\nof your queries (fewer ICL examples, fewer RAG documents), reduce the number\nof queries (try to collapse a multi-step reasoning process into fewer steps)\nor instruct the model to produce more succinct answers. If you\u2019re using an\nopen source LLM, you have more options. Try any or all of the following:\n\n  * Run an SFT or RLHF process to teach the model to produce more succinct responses.\n  * Make sure you\u2019re using vLLM (or similar) to serve your model.\n  * Quantize your model using the scheme best suited to your inference hardware. (See here for a good overview).\n  * Use a mixture-of-experts model (e.g. Mixtral-8x7B) . This increasingly popular architecture gives you the raw power of a larger model but only requires ~25% of the weights to be activated for any one inference task, offering a considerable increase in token throughput.\n  * Reduce the top_p and top_k parameters that have been set when sampling from your model. Reducing either of these directs the LLM to choose between a smaller set of potential next tokens. This will increase token throughput but at the expense of reducing response diversity.\n\nSo there it is, what I think are the eight most common roadblocks to\ndeveloping an LLM and some pointers for resolving them. I hope this helps\nsomebody.\n\n### Share this:\n\n  * Twitter\n  * Facebook\n\nLike Loading...\n\n### Related\n\nTagged ai, artificial-intelligence, llm, rag.\n\nBlog at WordPress.com.\n\n  * Reblog\n  * Subscribe Subscribed\n\n    * Decoder Only\n    * Already have a WordPress.com account? Log in now.\n\n  * Privacy\n  *     * Decoder Only\n    * Customise\n    * Subscribe Subscribed\n    * Sign up\n    * Log in\n    * Copy shortlink\n    * Report this content\n    * View post in Reader\n    * Manage subscriptions\n    * Collapse this bar\n\n%d\n\n", "frontpage": false}
