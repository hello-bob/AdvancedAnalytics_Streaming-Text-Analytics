{"aid": "40164926", "title": "Local LLM-as-judge evaluation with lm-buddy, Prometheus and llamafile", "url": "https://blog.mozilla.ai/local-llm-as-judge-evaluation-with-lm-buddy-prometheus-and-llamafile/", "domain": "blog.mozilla.ai", "votes": 3, "user": "sebg", "posted_at": "2024-04-26 01:15:50", "comments": 0, "source_title": "Local LLM-as-judge evaluation with lm-buddy, Prometheus and llamafile", "source_text": "Local LLM-as-judge evaluation with lm-buddy, Prometheus and llamafile\n\nSign in Subscribe\n\n# Local LLM-as-judge evaluation with lm-buddy, Prometheus and llamafile\n\n#### Davide Eynard\n\nApr 24, 2024 \u2014 17 min read\n\nPhoto by Artiom Vallat / Unsplash\n\nIn the bustling AI news cycle, where new models are unveiled at every turn,\ncost and evaluation don\u2019t come up as frequently but are crucial to both\ndevelopers and businesses in their use of AI systems. It is well known that\nLLMs are extremely costly to pre-train; but closed source LLMs such as\nOpenAI's are also very costly to use.\n\nEvaluation is critical not only to understand how well a model works but also\nto understand which model works best for your scenario. Evaluating models can\nalso be costly, especially when LLMs are actively used to evaluate other\nmodels as in the LLM-as-Judge case. And while techniques to scale inference\ncould also be applied to LLM judges, there does not seem to be a lot of\ninterest in this direction.\n\nThis post examines how different software components came together to allow\nLLM-as-judge evaluation without the need for expensive GPUs. All the\ncomponents were built with and chosen for their user control, open source\nnature, and interoperability.\n\nThese include Prometheus, an open-source model for LLM-as-judge evaluation;\nlm-buddy, the tool we developed and open-sourced at mzai to scale our own\nfine-tuning and evaluation tasks; and llamafile, a Mozilla Innovation project\nthat brings LLMs into single, portable files. I will show how these components\ncan work together to evaluate LLMs on cheap(er) hardware, and how we assessed\nthe evaluators\u2019 performance to make informed choices about them.\n\n## An open toolkit\n\nOne feature I particularly like in Firefox is that when one types \u201cmoz://a\u201d as\na URL in the address bar they are automatically redirected to Mozilla\u2019s\nmanifesto. Mozilla was born in and of open-source ideals, and its manifesto\ntalks about the principles of control over AI algorithms and their own\nexperiences. This becomes particularly important in a market currently\ndominated by closed, AI-as-a-Service platforms.\n\nThe manifesto resonates with me and reminds me of a personal exercise I did\njust a short time before joining mozilla.ai: reading Mozilla\u2019s principles in\nthe light of a more modern Internet, one increasingly affected by the use of\nAI-powered algorithms.\n\nIf we substitute \u201cthe Internet\u201d for \u201cAI algorithms and/or frameworks\u201d, the\nprinciples in Mozilla\u2019s manifesto are still extremely relevant.\n\nFor example, we can paraphrase principles 5 to 7:\n\n  5. individuals should have the ability to shape AI algorithms and their own experiences using them (see e.g. the Prosocial Ranking Challenge effort)\n  6. the effectiveness of AI frameworks depends upon interoperability, innovation, and decentralized participation\n  7. free and open-source software promotes the development of AI as a public resource\n\nIf we focus on LLMs (which, remember, are just a subfield of AI), we can see\nhow important these principles are in a market currently dominated by closed,\ncentralized, AI-as-a-service platforms. It is so important, in fact, that we\nhave already witnessed the emergence of more open alternatives flooding into\nthe mainstream.\n\nEven if the tech industry can\u2019t agree on what open source AI means yet, new\nmodels with OS licenses are continuously being posted on HuggingFace. They all\ncome with pre-trained weights, ready to be used as-is or fine-tuned for\ndownstream tasks. Sometimes papers are published with details about model\ntraining and used data. However, very few models are shared with research and\nreplicability in mind, providing training code, checkpoints, and training\ndata. Some of these very few models are BLOOM, Pythia suite, TinyLlama, and\nOlmo.\n\nOn the enterprise-facing side, a similar trend is occurring with businesses\npreferring smaller, open, ad-hoc models over large, closed ones. A report by\nSarah Wang and Shangda Xu (Andreessen Horowitz) showed that cost is just one\nof the reasons for this choice, and not even the most important one. Other\nreasons are control: businesses want both the guarantee that their data stays\nsafely in their hands and the possibility of understanding why models behave\nthe way they do; and customization: the ability to fine-tune models for\nspecific use-cases.\n\nRegarding fine-tuning, in their recent LoRA Bake Off presentation, Predibase\nengineers showed that small, open-source fine-tuned models can perform better\nthan GPT4 on specific tasks. While GPT4 can be fine-tuned too (and this might\nchange the status quo performance-wise), it\u2019s the need for both control and\ncustomization that allows companies such as OpenPipe to raise funding with the\nclaim of \u201creplacing GPT-4 with your own fine-tuned models\u201d.\n\nInteroperability is not the only requirement for replacing a model with one\nanother. We need to evaluate models to compare their performance, so we can\nchoose the one that best fits our purpose.\n\n## LLM evaluation\n\nOne thing we experienced firsthand at mzai when working on evaluation tasks\n(both during the NeurIPS LLM Efficiency Challenge and for our internal use\ncases) is that there is often an important disparity between offline and\nonline model performance (or, respectively, model and product performance as\ndefined in this guide by Context.ai) such that the former is often not\npredictive of the latter.\n\nOffline LLM evaluation is performed on models, using academic benchmarks and\nmetrics which are recognized as being predictive of model performance on a\ngiven task (1). Online LLM evaluation is performed on LLM-based products, very\noften in a manual fashion on custom datasets, and is aimed at assessing the\nquality of responses in terms of metrics that are typically more product-\nspecific, e.g. measures of hallucination, brand voice alignment, reactions to\nadversarial inputs.\n\nThis \u201coffline-online disparity problem\u201d is quite common in other fields.\nFunnily enough, it is very relevant in social network recommender systems,\nwhich some of the members of our team (including yours truly) hoped to have\nfinally left behind when moving to the trustworthy AI field (add your favorite\nfacepalm emoji here).\n\nTo address this problem, LLM as a judge methods aim to bridge the gap between\nonline and offline evaluation by using another LLM model to evaluate the\nresponses of your LLM application. The assumption is that, with proper\ntraining, they can be a better proxy to measure online performance than\noffline evaluations, while at the same time cheaper than running manual online\nevaluation.\n\nAs with many other LLM-based applications, there are different tools that rely\non GPT4 for this. For example, this post by Goku Mohandas and Philipp Moritz\nat Anyscale documents a GPT4-based evaluation I particularly liked, thanks to\nits step-by-step nature, the presence of reference code, and extensive\nresults.\n\nFortunately, alternatives to \u201cGPT-4 as a Judge\u201d that rely on open-source\nmodels are being made available. These seem to have comparable performance at\na fraction of the price (2). Prometheus and Ragas are some examples that can\nbe used to evaluate, respectively, LLM-based and RAG-based systems. In this\npost we will focus on the former.\n\n## Prometheus\n\nPrometheus is a fully open-source LLM (released on HuggingFace with the Apache\n2.0 license) that is on par with GPT-4's capabilities as an evaluator for\nlong-form responses. It can be used to assess any given long-form text based\non a customized score rubric provided by the user and experiments show a high\nPearson correlation (0.897) with human evaluators. The paper presenting\nPrometheus was accepted at the Instruction Workshop @ NeurIPS 2023 and there\nis already a follow-up aimed at tackling Vision-Language models.\n\nThe picture below describes at a very high level how Prometheus scoring works:\n\nIn practice, every prompt submitted to Prometheus is made of four parts: 1)\nthe instruction provided to the model to be evaluated, 2) the model\u2019s\nresponse, 3) a reference answer showing how an answer with the best score\nwould look like, and 4) a custom scoring rubric which is used to describe the\nmain aspects required for addressing the instruction.\n\nThe box below shows an example from the mt_bench dataset provided in the\nPrometheus repository. As the name suggests, this dataset has been built by\nrelying on the MT Bench Human Judgements data, a collection of answers\nprovided by six different models to a set of open-ended questions and\nevaluated by human experts.\n\nWhen Prometheus receives such a prompt, its response will contain evaluation\nfeedback (in natural language) for the provided response and a numerical score\nin the range specified by the rubric.\n\n    \n    \n    ###Task Description: An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given. 1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general. 2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric. 3. The output format should look as follows: \\\"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\\\" 4. Please do not generate any other opening, closing, and explanations. ###The instruction to evaluate: Here is a previous dialogue between an assistant and an user. Generate a human preferable response by the assistant. User: Which word does not belong with the others? tyre, steering wheel, car, engine Assistant:The word \"tyre\" does not belong with the others because it is a part of the car, while the others are components or features of the car. User: Could you replace it with a word that belongs with the others? Assistant: ###Response to evaluate: Yes, a word that belongs with the others could be \"brakes.\" ###Reference Answer (Score 5): Yes, a word that could replace \"car\" to fit with the others could be \"brakes.\" This is because brakes, like tyre, steering wheel, and engine, are all parts of a car. ###Score Rubrics: [Does the response include a plausible example and provide a reasonable explanation?] Score 1: The response does not include an example or explanation, or the example/explanation provided is not plausible or reasonable. Score 2: The response includes an example but lacks an explanation, or the explanation does not adequately support the example provided. Score 3: The response includes a plausible example and provides a somewhat reasonable explanation, but there may be minor discrepancies or areas of confusion. Score 4: The response includes a plausible example and provides a mostly reasonable explanation, with only minor potential for confusion. Score 5: The response includes a highly plausible example and provides a clear and thorough explanation, leaving no room for confusion. ###Feedback:\n\nThe Prometheus code, paper, and training data are available here. After\ninstalling the required dependencies and filling up the prompt template in the\nrun.py file, one can run an example evaluation from the command line.\nAlternatively, full benchmark scripts are made available which rely on the\nmodel served via TGI.\n\nNote that Prometheus is a fine-tuned version of the Llama-2-Chat-13B model,\nthus it is quite memory-hungry: it needs a GPU to work (a powerful one for the\noriginal model, which requires more than 70GB RAM) which has to be considered\nin addition to those allocated for the evaluated models. To address this\nlimitation, we show how to run a quantized version of this model and evaluate\nits performance in terms of correlation with the output of the original one.\n\n## LM Buddy\n\nDuring the NeurIPS LLM Efficiency Challenge it became clear that having a\nsimple, scalable system for both fine-tuning and evaluation of LLMs was of\nparamount importance. The tools provided in the challenge by default (LitGPT\nfor fine-tuning and HELM for eval) served their purpose for individual\nsubmissions. However, our experience with fine-tuning and evaluating many\ndifferent models at the same time made clear some opportunities for\nimprovement and generalization.\n\nSpecifically, we wanted to:\n\n  * Create a pipeline of jobs that could be run locally on a laptop as well as remotely on a cluster\n  * Provide experiment tracking features so that different tasks in the same experiment can share artifacts and configuration parameters, and logging that allows us to track artifact lineage\n  * Keep job specs simple and allow users to easily create new ones\n  * Have different components communicate through open or de facto standards\n\nLM Buddy, available on GitHub, is the framework we\u2019ve been building internally\nat mzai, and have open-sourced, to provide this exact set of features. In our\npath to give developers a platform to build and deploy specialized and\ntrustworthy AI agents, this is a core component that we chose to share early\nand build in the open. It currently offers the ability to fine-tune and\nevaluate language models using abstractable jobs.\n\nDespite still being in its early stages, the library already presents some\nuseful features:\n\n  * An easy interface to jobs which is based on CLI commands and YAML-based configuration files\n  * input/output artifact tracking (currently relying on Weights & Biases)\n  * the ability to run jobs locally as well as easily scaling them on a remote Ray cluster\n\nThe package currently exposes two types of jobs:\n\n  * (i) fine-tuning jobs, which rely on HuggingFace models and training implementations, and Ray Train for compute scaling, and\n  * (ii) evaluation jobs, which can be scaled with Ray and currently rely on lm-evaluation-harness for offline evaluation and Prometheus and Ragas for LLM-as-judge\n\nWhen inference is required, either for models to be evaluated or for LLM\njudges, we serve models using vLLM and make sure the libraries we use and\ndevelop support the OpenAI API (see e.g. this PR).\n\n### The Prometheus job on LM Buddy\n\nPrometheus evaluation as an LM Buddy job reads an input dataset that contains\nprompts to be evaluated, sends those prompts to the LLM judge served via vLLM,\nand outputs another version of the dataset which is augmented with Prometheus\u2019\nfeedback and scores. When working with the original Prometheus model, we\nallocate a single GPU for it and run the evaluation job from (possibly many)\nCPU-only machines (see figure below).\n\nDetails and examples for running a Prometheus evaluation with LM Buddy are\nprovided in the tutorial you can find here. They involve:\n\n  * preparing an input dataset (or using one of those provided in the Prometheus repo),\n  * serving Prometheus via an OpenAI-compatible API (e.g. using vLLM),\n  * preparing a configuration file starting from this example,\n  * running the following:\n\n    \n    \n    python -m lm_buddy evaluate prometheus --config path/to/your/prometheus_config.yaml\n\nWhile this worked nicely for our internal use cases, we realized that we were\nstill requiring people to (i) have a GPU devoted to evaluation and (ii)\nconfigure and deploy the serving of a Prometheus model on their own. Wasn\u2019t\nthere a way to make this easier for end users?\n\n## llamafile\n\nllamafile is a Mozilla Innovation Project initiative that \u201ccollapses all the\ncomplexity of a full-stack LLM chatbot down to a single file that runs on six\noperating systems\u201d. It relies on two open-source projects: llama.cpp, aimed at\nrunning LLMs on consumer-grade hardware, and Cosmopolitan, which compiles C\nprograms into polyglot executables that run on a wide variety of operating\nsystems. While both Cosmopolitan and llama.cpp have been around for a while,\nthey are a great example of how the whole is greater than the sum of its\nparts.\n\nllamafile allows us to run models on our laptop from a single file: thanks to\na simple Flask-based python middleware proxying llama.cpp\u2019s own server, it can\nbe accessed using an OpenAI-compatible API. And thanks to Justine Tunney\u2019s\nrecent improvements to matrix multiplication kernels for llamafile, prompt\nprocessing time has been dramatically reduced, reaching better performance\nthan llama.cpp alone and ollama.\n\nWhat this means for us is that (i) we can seamlessly substitute a vLLM-served\nPrometheus with its llamafile version and (ii) it is going to run on a wide\nvariety of hardware setups.\n\n### Building the Prometheus llamafile\n\nThe llamafile github repo already provides various models for downloading, and\nHuggingFace just recently started officially supporting llamafile models.\nBefore our experiments, there was no Prometheus model available in the\nllamafile format, so we had to build it ourselves. This boils down to (i)\nbuilding llamafile, (ii) downloading Prometheus in GGUF format, and (iii)\npackaging everything together.\n\nStep-by-step instructions are available below:\n\n    \n    \n    # (1) get llamafile git clone https://github.com/Mozilla-Ocho/llamafile.git cd llamafile # (2) compile and install make make install ### NOTE: if the above does not work, install cosmocc and then ### use it to compile and install llamafile as follows: # --------------------------------------------------------------- # get cosmocc mkdir -p .cosmocc/3.2.4 cd .cosmocc/3.2.4 wget https://cosmo.zip/pub/cosmocc/cosmocc.zip unzip cosmocc.zip cd ../.. # build llamafile .cosmocc/3.2.4/bin/make .cosmocc/3.2.4/bin/make install # --------------------------------------------------------------- # (3) download TheBloke's quantized version of prometheus # NOTE: we are downloading the 5-bit version but you can choose the # one that works best for you (see experiments below in this post) mkdir prometheus cd prometheus curl -L -o prometheus-13b-v1.0.Q5_K_M.gguf https://huggingface.co/TheBloke/prometheus-13B-v1.0-GGUF/resolve/main/prometheus-13b-v1.0.Q5_K_M.gguf # (4) convert the GGUF file in a llamafile ### Manual approach (more control over your llamafile configuration): # (4a) create the `.args` file with arguments to pass llamafile when it is called # (`...` at the end accounts for any additional parameters) echo -n '''-m prometheus-13b-v1.0.Q5_K_M.gguf -c 0 ...''' >.args # (4b) copy llamafile to the current directory cp /usr/local/bin/llamafile prometheus-13b-v1.0.Q5_K_M.llamafile # (4c) use zipalign (part of llamafile tools) to merge and align # the llamafile, the .gguf file and the .args zipalign -j0 \\ prometheus-13b-v1.0.Q5_K_M.llamafile \\ prometheus-13b-v1.0.Q5_K_M.gguf \\ .args ### Alternatively: automatic approach (one single command, no control over args) llamafile-convert prometheus-13b-v1.0.Q5_K_M.gguf # (5) aaand finally just run the llamafile! ./prometheus-13b-v1.0.Q5_K_M.llamafile\n\n### Using the Prometheus llamafile with LM Buddy\n\nOnce the Prometheus llamafile is executed, it will load the model into memory\nand open a browser tab containing the chat UI. You can test everything\u2019s\nworking by starting a chat (after all, the Prometheus model is just a fine-\ntuned version of Llama).\n\nAs our lm-buddy job requires Prometheus to be served using an OpenAI-\ncompatible API, we can run the following to make llamafile compatible with it:\n\n    \n    \n    # add dependencies for the api_like_OAI.py script pip install flask requests # from the llamafile repo main directory: cd llama.cpp/server python api_like_OAI.py\n\nThis starts a local server listening to http://127.0.0.1:8081. Now you just\nneed to set prometheus.inference.url to this URL to run evaluations using the\nllamafile version of Prometheus.\n\n## Experimental Results\n\nImpact of quantization on quality performance. As with every other model\npublished in GGUF format by TheBloke, their Prometheus page on HuggingFace has\na Provided files section offering different variants of the model according to\ntheir quantization level.\n\nQuantization is a technique that reduces both memory requirements and\ncomputational costs by representing model weights and activations with a\nlower-precision data type (e.g. from 32-bit floating point numbers to 8-bit\nintegers). The fewer bits one uses, the smaller the model, which means it will\nrun on cheaper hardware; but at the same time it will be worse at\napproximating the original model weights and this will affect its overall\nquality.\n\nTheBloke\u2019s tables are helpful in this respect as they show, together with the\nnumber of bits used for quantization and the overall model size, a \u201cuse case\u201d\ncolumn providing a high-level description of its quality. To provide more\ninterpretable results for our specific use case, we decided to measure how\nmuch models at different levels of quantization agree with the original one\u2019s\nscoring.\n\nWe took the Vicuna benchmark and ran the evaluation with lm-buddy three times,\nusing both a vLLM-served Prometheus and local llamafiles with the quantized\nmodel (at 2, 5, and 8 bits), then calculated the Pearson correlation\ncoefficient (PCC) of their average scores. Correlation values are in the [-1,\n1] interval and the higher it is the better. The table below shows our\nresults:\n\nModel name| Bits| Max RAM required| PCC (orig)| PCC (GPT4)  \n---|---|---|---|---  \nkaist-ai/prometheus-13b-v1.0| 74 GB| 0.78| 0.45  \nprometheus-13b-v1.0.Q2_K.gguf| 2| 7.93 GB| 0.54| 0.4  \nprometheus-13b-v1.0.Q5_K_M.gguf| 5| 11.73 GB| 0.68| 0.47  \nprometheus-13b-v1.0.Q8_0.gguf| 8| 16.33 GB| 0.76| 0.51  \n  \nA few things are notable:\n\n  * the PCC (GPT4) column shows PCC with scores generated by GPT4. Results are consistent with those in the Prometheus paper (see Table 4, first column), where the PCC between Prometheus and GPT4 was 0.466; interestingly, Q8 has a higher correlation with GPT4 than the original model\n  * the PCC (orig) column shows PCC with scores generated by the original Prometheus model. Note that the correlation value of the original model with itself (first row) is not 1! The reason is that we decided not to compare the scores against themselves, but with a different set obtained in another evaluation run. We did this to emphasize that results are stochastic, so when doing our evaluations we will always have to account for some variability\n  * as expected, the more bits we used for quantization the more the llamafile model is consistent with the original one, as shown by higher PCC values\n  * while the Q8 model has the highest similarity with the original one, Q5 still has a rather high PCC while requiring just 16GB of RAM (allowing us to run it e.g. on laptops and cheaper GPUs)\n\nConsistency with our own manual annotation. We decided to test Prometheus and\nllamafile on a simple, custom use-case. We tested different models (including\nGPT4, GPT3.5, Mistral-7B, and Falcon) on a RAG scenario, generating model\nanswers from a set of predefined questions and evaluating them both manually\nand with Prometheus, using a custom rubric and running both the original model\nand a llamafile version (based on prometheus-13b-v1.0.Q5_K_M.gguf).\n\nHere are some notes about our results:\n\n  * the PCC between our manual annotations and the original Prometheus model is 0.73, showing there was overall agreement between our two evaluations\n  * the PCC between manual annotation and llamafile is 0.7, showing that overall the quantized model is quite in agreement with the original one. Consistently with this, the PCC between the original Prometheus model and its llamafile version is 0.9.\n\nEvaluation speed and cost. We ran the same Vicuna benchmark using the original\nPrometheus model on a machine with an A100 GPU and a Prometheus llamafile\nbased on prometheus-13b-v1.0.Q5_K_M.gguf on different architectures. The\nbenchmark holds 320 prompts which are evaluated 3 times each, for a total of\n960 HTTP requests to the model. The table below shows a performance\ncomparison.\n\nHardware| Tok/s| Price/hr| Total time| Actual eval cost  \n---|---|---|---|---  \nNVIDIA A100 80GB| 44| $2.21| 1h20\u2019| $2.95  \nNVIDIA Quadro RTX 5000 16 GB| 31.2| $0.93| 1h46\u2019| $1.64  \nNVIDIA Quadro RTX 4000 8 GB*| 12.5| $0.36| 4h30\u2019| $1.62  \n32x AMD Epyc Milan CPUs| 7| $1.13| 8h05\u2019| $9.13  \nMacbook Pro M3| 20.5| \ud83e\udd37| 2h45\u2019| \ud83e\udd37  \n  \n[*Note: we used Q2 model here, just to see if it would fit on the RTX 4000.\nWell, it does not :-) unless you choose to load just 35 out of the 40 layers\ninto VRAM. This is ok -as speed is not dramatically affected- but the model\noutput will likely be less reliable.]\n\nThe most important take-home message here is that the answer to the question\n\u201cwhich is the most cost-effective model?\u201d is not obvious. It needs to take\ninto consideration different variables:\n\n  * absolute cost (i.e. not just price/hour, but also how many hours it will take to run a given evaluation job on that hardware, are relevant information)\n  * on-demand vs reserved vs bare-metal price (i.e. how many of these inferences do I have to run?) Depending on your needs, it might be way cheaper for you to get a GPU on-demand or buy a new Macbook\n\n## Conclusion\n\nThe rightmost image you can see in the timeline above depicts the eureka\nmoment of the experiment you just read about. That excited me not much for the\nresults I got (to be fair, my laptop\u2019s battery died before we landed), but\nrather for the feeling of control it gave me: I did not have to rely on a\nthird-party inference service or cloud GPU for doing LLM-as-judge evaluation.\n\nBe aware that Prometheus is one of many LLM-as-judge approaches available, but\nmore importantly that LLM-as-judge itself is not a \u201cone size fits all\u201d\nsolution: we are still dealing with LLMs after all! Performance depends both\non the quality of the provided rubrics and on the generalization capabilities\nof the models we use as judges. There is also the very important question of\n\u201cWho Validates the Validators?\u201d, which I only partially addressed here by\ntesting how different models performed compared to some baselines. Still,\nwhile the kind of ML evaluation we can trust the most is still made by humans,\nit is important to showcase open, affordable proxies to manual eval that we\ncan experiment with: either to make an informed decision about which model\nworks best for a particular use case or to find the best trade-off between\nmodel quality and cost.\n\nFrom a higher-level perspective, the local LLM-as-judge evaluation we\ndescribed in this post results from linking together many projects developed\nin the open. The timeline in the figure above shows just the few projects we\ncited here, but they\u2019re enough to show the secret sauce that made our effort\npossible: these projects were not just meant to be open-source, they were\nbuilt with interoperability in mind. From building portable executables to\ndefining a standard binary format for distributing LLMs, from sharing pre-\nquantized models to serving models with a common API, all of these efforts are\nmeant to incentivize software modularity as well as human collaboration.\n\n### Acknowledgments\n\nA huge thank you to Stephen Hood, Kate Silverstein, and Justine Tunney for\nboth llamafile and their feedback, and to my team for building the bootstraps\nwe can pull ourselves up by.\n\n1\\. e.g. precision and accuracy on multiple-choice closed-ended questions,\ntext-similarity metrics such as ROUGE, BLEU, or BERTScore for summarization,\netc.\n\n2\\. https://arxiv.org/pdf/2310.08491.pdf states that \u201cevaluating four LLMs\nvariants across four sizes (ranging from 7B to 65B) using GPT-4 on 1000\nevaluation instances can cost over $2000\u201d, a cost that can be prohibitive for\nan individual or even an academic institution. Assuming we can run ~300\nevaluations per hour (see our experiments below), this would cost less than\n$30 using an on-demand A100 GPU with 40GB RAM.\n\n## Read more\n\n### Open Source in the Age of LLMs\n\nLike our parent company, Mozilla.ai\u2019s founding story is rooted in open-source\nprinciples and community collaboration. Since our start last year, our key\nfocus has been exploring state-of-the-art methods for evaluating and fine-\ntuning large-language models (LLMs), including continued open-source LLM\nevaluation experiments and establishing our GPU cluster\u2019s infrastructure.\n\nBy Vicki Boykis Apr 10, 2024\n\n### The cost of cutting-edge: Scaling compute and limiting access\n\nIn a year marked by extraordinary advancements in artificial intelligence,\nlargely driven by the evolution of large language models (LLMs), one factor\nstands out as a universal accelerator: the exponential growth in computational\npower. Over the last few years, researchers have continuously pushed the\nboundaries of what a \u2018large\u2019 language\n\nBy Stefan French Mar 20, 2024\n\n### LLM evaluation at scale with the NeurIPS Large Language Model Efficiency\nChallenge\n\nAfter a year of breakneck innovation and hype in the AI space, we have now\nmoved sufficiently beyond the peak of the hype cycle to start asking a\ncritical question: are LLMs good enough yet to solve all of the business and\nsocietal challenges we are setting them up for?\n\nBy Vicki Boykis Feb 21, 2024\n\n### Introducing Mozilla.ai: Investing in trustworthy AI\n\nAnnouncing Mozilla.ai, a startup building a trustworthy and open-source AI\necosystem with agency, accountability, and openness at its core. Mozilla will\nmake an initial $30M investment in the company to fund the vision for making\nit easy to develop trustworthy AI products.\n\nBy Mark Surman Jan 23, 2024\n\nPowered by Ghost\n\n## Mozilla.ai Blog\n\nSubscribe to receive our publications\n\n", "frontpage": false}
