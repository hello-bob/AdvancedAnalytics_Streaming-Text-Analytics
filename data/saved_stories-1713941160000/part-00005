{"aid": "40138439", "title": "Data and AI in 2024: Three Problems", "url": "https://www.singlestore.com/blog/data-ai-in-2024/", "domain": "singlestore.com", "votes": 1, "user": "archerundead", "posted_at": "2024-04-23 23:11:08", "comments": 0, "source_title": "Data + AI in 2024: Three Problems for Now and for the Future", "source_text": "Data + AI in 2024: Three Problems for Now and for the Future\n\nFaster hybrid vector + full-text search, fast-scaling integrations and a free\ntier.SingleStore Pro Max \u279d\n\n  * Try Free\n\nApril 23, 2024\n\n# Data + AI in 2024: Three Problems for Now and for the Future\n\nZS\n\nZhou Sun\n\nDirector of Engineering, Query Execution\n\nAlthough they are closely related, the data and AI communities have\nhistorically operated in parallel, each advancing in its own right \u2014 but often\nseparately.\n\nAs we continue into 2024, this trend has dramatically changed. We're\nwitnessing a convergence that is not only reshaping existing frameworks, but\nalso forging new pathways for technological innovation.\n\nAfter working in data for almost 10 years \u2014 and learning in AI for some time \u2014\nI want to share my thoughts, mainly around three big problems in data and AI.\n\nDisclaimer: I speak mostly for myself in this blog post. Some places might be\nbiased toward SingleStore, but in general I want to discuss the bigger\npicture.\n\n## A mostly solved problem: RAG\n\nTo me, RAG is kind of special in the sense that it proves databases relevant\nin the AI world.\n\n### Simple RAG will be part of the LLM solution\n\nSome fun facts:\n\n  * Wikipedia is around 20GB\n  * All public codebases on Github, in total, have around ~200GB of text\n\nThere is just not enough 'valuable' textual data: it costs nothing to store\nthem, in memory or even in GPU. So hosting the data \u2014 either as raw data or in\nvector form \u2014 would be a much easier problem than correctly extracting info\nand embedding the data. Who does the best job extracting and embedding? Model\nproviders.\n\nOpenAI Assistants API already provides a built-in solution for up to 10,000\nfiles, but I don't see any real limitations there.\n\n### Specialized vector databases will be irrelevant\n\nVector search isn\u2019t difficult enough \u2014 every database vendor is releasing\ntheir own. SingleStore proves it\u2019s possible for general purpose databases to\nget similar ANN search performance as state-of-the-art, specialized vector\ndatabases. Check out how SingleStore brings high performance to vector search,\nand stay tuned for our research paper.\n\nIt also leads to the dilemma of vector search: If the workload really needs\nthe highest possible vector search performance, people should not use a\ndatabase. Rather, they should use the most suitable vector search library with\ncorrect hardware. Otherwise, the vector search performance doesn't really\nmatter too much, since in general it is already very, very fast.\n\nIn the near term, people will still pick solutions based on ANN benchmarks.\nBut as we understand the use cases better, I would predict vector databases\nwill go either way, \u2014 vector or database \u2014meaning they would either go back to\nindex libraries (or try to become proper databases), but no longer be vector\ndatabases.\n\n### Complex RAG will stay for a while\n\nWill all the RAG startups die? I don't think so.\n\nIn general, RAG startups should not be viewed as AI companies \u2014 they should\nall be viewed as data companies, since the differentiator for them is what can\nbe retrieved. And there is a lot you can do with data:\n\n  * The majority of high value data is not in textural format\n  * In the near future, calling LLMs and creating new vector embeddings will remain slow and expensive\n  * For most RAG apps the amount of data output is extremely low, so any improvement in retrieval quality can be the differentiator\n\nThis requires RAG startups to innovate in retrieval; most RAG will be built\nwith a complex data stack, and might eventually become agentic apps.\n\n## A half-solved problem: Data landscape\n\nIn this picture, the data land is a divided nation \u2014 one side a harmonious\nfederation, bound by a common thread, and the other a fragmented\nconfederation, riddled with discord and strife. Think Roman empire on one\nhand, and Gauls on the other.\n\n### Enterprise-scale lakehouse\n\nWe\u2019re seeing a trend of enterprises adapting solutions around iceberg/ delta\nlake, where data is shared between various analytics and AI products. Look at\nthe hot vendors in this space: Snowflake x Iceberg, DataBricks Lakehouse and\nMicrosoft OneLake.\n\nIn my opinion, lakehouse architectures, or modern data lakes, largely solve\nthe offline data storage problem for machine learning training and data\nwarehousing. In this architecture, data is easily transferable, copyable and\nshareable.\n\n### Many ways to get interactive speed\n\nOn the operational side, each web-app, agent and dashboard is likely powered\nby a database \u2014 and this is what most AI agents will interact with.\n\nBut the problem is there are too many solutions:\n\n  * App database. Postgres, Cockroach, MongoDB\u00ae, SingleStore \ud83d\ude42...\n\n  * Accelerated analytics. ClickHouse, DuckDB, Dremio, Rockset, SingleStore \ud83d\ude00...\n\n  * Retrieval systems. Elastic, Vector databases, SingleStore \ud83d\ude02...\n\n### Efforts will be made to unify the speedy layer\n\nKey players have already started:\n\n  * MongoDB is marketed as transactions, search and AI\n  * Rockset is marketed as analytics, search and multi-model\n  * Redis is marketed as cache, vector and database\n\nAnd as one of the engineers who started the SingleStore project, I still\nbelieve in the name.\n\nHere are some criteria of this unified speedy layer:\n\nFoundations:\n\n  1. It will be a database and ideally a system of record, so it can power applications\n  2. It will be a cache and accelerator, so it can support analytics or retrieval\n\nKey properties:\n\n  * It will be multi-model, so JSON, vector, full-text, graph and traditional relational data all exist here\n  * It will support multi-tenancy\n\n    * LLM is a multi-tenant system, so the infra is shared\n    * AI enables personalization, so there will be more per-tenant data\n  * It will be connected to the stack\n\n    * It needs to be seamlessly connected to the previous lakehouse layer\n    * It needs to be more seamlessly connected to LLM, and I will talk about that in the next section\n\n## A mostly unsolved problem: AI's interaction with data\n\nWhen talking with some LLM researchers, I learned an interesting perspective:\nTraditional NLP was built with the goal of learning grammar, semantics and\nrules of languages. But LLMs like transformer models were built upon a more\ngeneral goal: to predict the next token.\n\nLooking at successful LLMs like GPT, internally it most likely learned the\nsemantics, grammar and rules. In other words, it learned what is needed to\nform dialogue from nowhere, so in some sense we can call it general\nintelligence. With this assumption, LLMs are more than language models \u2014 the\ntexture input and token generation output might actually be a bottleneck, it\nneeds better approaches to interact with the rest of the world.\n\nI am skeptical that we will trust AI to interact with the physical world in\nthe near future.I, for one, would not entrust AI with control over my home's\npower systems. Therefore, it might be easier for AI technologies to first gain\nexpanded access to data, both for reading and writing.\n\n### Current text_to_sql will die\n\nThe goal for the Transformer model is to predict the next token. And compared\nto everything else, SQL is among the hardest to write one word by one word, or\neven one line by one line.\n\nAlthough at a glance SQL seems very close to a classic NLP task, it is not. It\nmight be harder than general code generation since it is truly one-shot (you\nneed one single query, two-pages long). So I would say SQLgeneration won't be\neasier than video generation, and so far no one has put enough resources into\nthis area. As a result, text_to_sql will remain a 'shiny toy' until either\nLLMs improve to another level, or someone spends a magnitude more resources in\nthe development.\n\n### AI-powered analytics\n\nThis is my biggest disappointment in the current wave so far.We know that AI\ncan do things human can never do \u2014 for example, read one million tokens in a\nsecond or try a million SQL queries overnight. But we are only trying to use\nAI to replace junior DBAs on the tasks humans handle perfectly fine (and it is\nstill not going very well).\n\nI want to see some fundamentally different ways to analyze data that was not\npossible before:\n\n  * Read a million-row table, row by row, to give me some insights\n  * Read all documents or tables in one system, to extract and decide how it should be stored\n\n### Agentic app\n\nAndrew Ng recently gave a talk about Agents. You may think agentic apps are\njust another kind of AI application, but after some research and talking to\nthe AI community, I am more or less convinced it's more fundamental.\n\nThere are two ways to picture an agentic app:\n\n  * It is trying to build an AI workflow to solve a given problem\n  * It is trying to solve the problem of problem-solving\n\nTo some extent, we can view the second as building some kind of AGI! So, the\nprediction is that more and more resources will be put into agentic apps, and\nthese will be the kind of applications that land in the enterprise world.\n\nRecall some key properties of agentic apps from Andrew\u2019s talk:\n\n  * Reflection. The LLM examines its own work to come up with ways to improve it\n\n  * Tool use. The LLM is given tools like web search, code execution or any other function to help it gather information, take action or process data.\n  * Planning. The LLM comes up with and executes a multi-step plan to achieve a goal (for example, writing an outline for an essay, doing online research, writing a draft and so on)\n  * Multi-agent collaboration. More than one AI agent work together, splitting up tasks and discussing and debating ideas to come up with better solutions than a single agent would.\n\nAgentic apps bring some unique and interesting data requirements, including\ndatabases as tools, databases as memory of one agent and databases as a share\ndrive of agents.\n\n## Conclusion\n\nIn 2024, the distinctions between database and AI companies are increasingly\nblurred. AI companies are recognizing the need to enhance their data handling\ncapabilities, just as database companies are integrating more AI functions.\nLooking ahead, the most successful enterprises will likely be those that\neffectively merge the strengths of both databases and AI.\n\nAs someone deeply involved in the database sector, I am both excited and\napprehensive about what the future holds, when suddenly we have 1 trillion\n'people' that are capable of interacting with data.\n\nProduct\n\nShare\n\n## Related reading\n\nProduct\n\n### Sparse JSON\n\nFebruary 26, 2024\n\nRead Now\n\nProduct\n\n### Understanding Oracle\u2019s Real-Time Ingestion Overhead With Kafka, and How\nSingleStore Is Better\n\nFebruary 20, 2024\n\nRead Now\n\nProduct\n\n### SingleStoreDB Self-Managed 7.5 Now Available\n\nJune 23, 2021\n\nRead Now\n\nProduct\n\n### SingleStore\u2019s Patented Universal Storage - Part 2\n\nJune 3, 2020\n\nRead Now\n\nProduct\n\n### SingleStore Delivers Top Performance on Industry Benchmarks with Latest\nReleases\n\nMay 14, 2019\n\nRead Now\n\nProduct\n\n### How SingleStore Works \u2013 At a Glance\n\nDecember 28, 2018\n\nRead Now\n\n## Start building with SingleStore\n\nStart freeTalk to a specialist\n\n### Explore more resources\n\nDocumentationPricingGet started with SingleStore\n\n## Sitemap\n\nFollow Us\n\n\u00a9 SingleStore, Inc.\n\nPrivacyTerms of ServiceLegal Terms and ConditionsResponsible Disclosure\n\n", "frontpage": false}
