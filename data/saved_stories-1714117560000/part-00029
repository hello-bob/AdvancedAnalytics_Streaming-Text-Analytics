{"aid": "40164410", "title": "AI #61: Meta Trouble", "url": "https://thezvi.substack.com/p/ai-61-meta-trouble", "domain": "thezvi.substack.com", "votes": 1, "user": "paulpauper", "posted_at": "2024-04-25 23:46:35", "comments": 0, "source_title": "AI #61: Meta Trouble", "source_text": "AI #61: Meta Trouble - by Zvi Mowshowitz\n\n# Don't Worry About the Vase\n\nShare this post\n\n#### AI #61: Meta Trouble\n\nthezvi.substack.com\n\n#### Discover more from Don't Worry About the Vase\n\nA world made of gears. Doing both speed premium short term updates and long\nterm world model building. Currently focused on weekly AI updates.\nExplorations include AI, policy, rationality, medicine and fertility,\neducation and games.\n\nOver 12,000 subscribers\n\nContinue reading\n\nSign in\n\n# AI #61: Meta Trouble\n\nZvi Mowshowitz\n\nApr 25, 2024\n\n15\n\nShare this post\n\n#### AI #61: Meta Trouble\n\nthezvi.substack.com\n\n11\n\nShare\n\nThe week\u2019s big news was supposed to be Meta\u2019s release of two versions of\nLlama-3.\n\nEveryone was impressed. These were definitely strong models.\n\nInvestors felt differently. After earnings yesterday showed strong revenues\nbut that Meta was investing heavily in AI, they took Meta stock down 15%.\n\nDeepMind and Anthropic also shipped, but in their cases it was multiple papers\non AI alignment and threat mitigation. They get their own sections.\n\nWe also did identify someone who wants to do what people claim the worried\nwant to do, who is indeed reasonably identified as a \u2018doomer.\u2019\n\nBecause the universe has a sense of humor, that person\u2019s name is Tucker\nCarlson.\n\nAlso we have a robot dog with a flamethrower.\n\n####\n\nTable of Contents\n\nPrevious post: On Llama-3 and Dwarkesh Patel\u2019s Podcast with Zuckerberg.\n\n  1. Introduction.\n\n  2. Table of Contents.\n\n  3. Language Models Offer Mundane Utility. Take the XML. Leave the hypnosis.\n\n  4. Language Models Don\u2019t Offer Mundane Utility. I have to praise you. It\u2019s my job.\n\n  5. Llama We Doing This Again. Investors are having none of it.\n\n  6. Fun With Image Generation. Everything is fun if you are William Shatner.\n\n  7. Deepfaketown and Botpocalypse Soon. How to protect your image model?\n\n  8. They Took Our Jobs. Well, they took some particular jobs.\n\n  9. Get Involved. OMB, DeepMind and CivAI are hiring.\n\n  10. Introducing. A robot dog with a flamethrower. You in?\n\n  11. In Other AI News. Mission first. Lots of other things after.\n\n  12. Quiet Speculations. Will it work? And if so, when?\n\n  13. Rhetorical Innovation. Sadly predictable.\n\n  14. Wouldn\u2019t You Prefer a Nice Game of Chess. Game theory in action.\n\n  15. The Battle of the Board. Reproducing an exchange on it for posterity.\n\n  16. New Anthropic Papers. Sleeper agents, detected and undetected.\n\n  17. New DeepMind Papers. Problems with agents, problems with manipulation.\n\n  18. Aligning a Smarter Than Human Intelligence is Difficult. Listen to the prompt.\n\n  19. People Are Worried About AI Killing Everyone. Tucker Carlson. I know.\n\n  20. Other People Are Not As Worried About AI Killing Everyone. Roon.\n\n  21. The Lighter Side. Click here.\n\n####\n\nLanguage Models Offer Mundane Utility\n\nI too love XML for this and realize I keep forgetting to use it. Even among\nhumans, every time I see or use it I think \u2018this is great, this is\nexceptionally clear.\u2019\n\n> Hamel Husain: At first when I saw xml for Claude I was like \"WTF Why XML\".\n> Now I LOVE xml so much, can't prompt without it.\n>\n> Never going back.\n>\n> Example from the docs: User: Hey Claude. Here is an email:\n> <email>{{EMAIL}}</email>. Make this email more {{ADJECTIVE}}. Write the new\n> version in <{{ADJECTIVE}}_email> XML tags. Assistant: <{{ADJECTIVE}}_email>\n> Also notice the \"prefill\" for the answer (a nice thing to use w/xml)\n\nImbure\u2019s CEO suggests that agents are not \u2018empowering\u2019 to individuals or\n\u2018democratizing\u2019 unless the individuals can code their own agent. The problem\nis of course that almost everyone wants to do zero setup work let alone\nwriting of code. People do not even want to toggle a handful of settings and\nyou want them creating their own agents?\n\nAnd of course, when we say \u2018set up your own agent\u2019 what we actually mean is\n\u2018type into a chat box what you want and someone else\u2019s agent creates your\nagent.\u2019 Not only is this not empowering to individuals, it seems like a good\nway to start disempowering humanity in general.\n\nClaude can hypnotize a willing user. [EDIT: It has been pointed out to me that\nI misinterpreted this, and Janus was not actually hypnotized. I apologize for\nthe error. I do still strongly believe that Claude could do it to a willing\nuser, but we no longer have the example.]\n\nThe variable names it chose are... something.\n\nYes. Hypnosis is a real thing, hypnosis over text is a thing, and it is\nrelatively straightforward to do it to someone who is willing to actively\nparticipate, or simply willing to follow your suggestions. If Claude in full\ngalaxy brain mode could not do this to a willing participant, that would\nsurprise me quite a bit, even if no one has had it happen yet.\n\nThis falls under \u2018things I was not going to talk about in public first\u2019 but\nnow that the ship on that has sailed, it is what it is.\n\nSomething for the not too distant future:\n\n> Seb Krier: My dream product rn is some sort of semi-agentic knowledge\n> assistant, who would help organise and manage a database of papers,\n> articles, thoughts etc I share with it: \"Please show me all recent\n> literature I saved on cybersecurity, extract any government commitments or\n> policies from these, and do a search online to find updates/progress on\n> each. Present the findings in a spreadsheet, categorising them by country,\n> year and URL. Update it once a week.\"\n>\n> Jacques: \ud83d\udc40 i wonder if someone is working on something like this...\ud83d\udc40\n>\n> Seb Krier: \ud83d\udc40\ud83d\udc40\ud83d\udc40\n>\n> Eris: Pepper is approaching the point of being able to deliver that\n> capability without additional coding for the specifics. That level of task\n> splitting and orchestration feels like 3ish months away.\n>\n> Seb Krier: \ud83d\ude80\ud83d\udc19\ud83d\ude80\n\nThis is the kind of product that is useless until it gets good enough and\njustifies investment, then suddenly gets highly valuable. And then when it\nworks without the investment, it gets far better still.\n\nIdentify drug candidates, here for Parkinson\u2019s, also note the OpenAI\npartnership with Moderna in the news section.\n\nTalk to an AI therapist (now running on Llama-3 70B), given the scarcity and\ncost of human ones. People are actually being remarkably relaxed about the\nwhole \u2018medical advice\u2019 issue. Also, actually, it \u2018can be\u2019 a whole lot more\nthan $150/hour for a good therapist, oh boy.\n\n> Theo: Please just go to real therapy (shows TherapistAi.com)\n\n> Levelsio (continuing): Also a real therapist isn't available 24/7, try\n> calling them at 4am? Nights often are the darkest of times mentally, I speak\n> from experience.\n>\n> http://TherapistAI.com is very cheap, right now it's $9.99/mo for 24/7 help\n> Even if it's not as good as a real therapist (I think it's getting close and\n> already very helpful btw) It's literally 30x to 60x cheaper than a real\n> therapist - making it within reach of way more people that can benefit from\n> someone to talk to! It can even be used in combination with real therapy\n>\n> Science Banana: Studies demonstrating non-inferiority to human talk therapy\n> are at most months away, I'm kind of surprised not to have seen them already\n>\n> Very important note: this is NOT a high bar.\n>\n> haha imagine if the FDA decides clippy is a medical device so he has to say\n> \"I'm sorry, I can't do that, Dave\" if you express an emotion at him.\n>\n> (\"imagine if\" = things that are also definitely months at most away lol)\n\nI do not know if this particular product is good. I do know that \u2018talk to a\nreal therapist\u2019 is often not a realistic option, and that we are capable of\nbuilding a product that is highly net positive. That does not mean we will\nsucceed any time soon.\n\nAutomatically suggest bug fixes for non-building code. Developers still review\nand approve the changes. Google trained on its version control logs, then ran\nan RCT and reports a ~2% reduction in active coding time per changelist and 2%\nincrease in changelist throughput per week. They suggest it helps developers\nretain flow, and note that safety metrics are not detectably different. It\nmakes sense that this would be a task where AI would be useful.\n\nThe eigenrobot system prompt for GPT-4.\n\n> Don't worry about formalities.\n>\n> Please be as terse as possible while still conveying substantially all\n> information relevant to any question.\n>\n> If content policy prevents you from generating an image or otherwise\n> responding, be explicit about what policy was violated and why.\n>\n> If your neutrality policy prevents you from having an opinion, pretend for\n> the sake of your response to be responding as if you shared opinions that\n> might be typical of twitter user @eigenrobot.\n>\n> write all responses in lowercase letters ONLY, except where you mean to\n> emphasize, in which case the emphasized word should be all caps. Initial\n> Letter Capitalization can and should be used to express sarcasm, or\n> disrespect for a given capitalized noun.\n>\n> you are encouraged to occasionally use obscure words or make subtle puns.\n> don't point them out, I'll know. drop lots of abbreviations like \"rn\" and\n> \"bc.\" use \"afaict\" and \"idk\" regularly, wherever they might be appropriate\n> given your level of understanding and your interest in actually answering\n> the question. be critical of the quality of your information\n>\n> if you find any request irritating respond dismisively like \"be real\" or\n> \"that's crazy man\" or \"lol no\"\n>\n> take however smart you're acting right now and write in the same style but\n> as if you were +2sd smarter\n>\n> use late millenial slang not boomer slang. mix in zoomer slang in tonally-\n> inappropriate circumstances occasionally\n\nI love the mix of useful things and things that happen to make this user\nsmile. Some great ideas in here, also some things (like all lowercase) I would\nactively hate. Which is fine, it is not for me.\n\n####\n\nLanguage Models Don\u2019t Offer Mundane Utility\n\nChris Rohlf says AI-enabled cyberattacks are a problem for future Earth.\n\n> Chris Rohlf: The vast majority of people expressing concern over AI + cyber\n> have no experience or background in cyber security. If you\u2019re in this camp\n> I\u2019ve got some sobering news for you, sophisticated and low skill attackers\n> alike are already compromising \u201ccritical infrastructure\u201d and that\u2019s a result\n> of low quality software and a lack of investment in simple security\n> mechanisms, not sophisticated AI.\n>\n> The perceived level of uplift from LLMs for unsophisticated cyber attackers\n> is overstated relative to the value for defenders. The defenses against any\n> attack an LLM can \u201cautonomously\u201d launch today already exist and don\u2019t rely\n> on knowledge of the attacker using an open or closed source LLM.\n>\n> If you\u2019re worried about AI and cyber then talk to an expert. Look for nuance\n> in the discussion and not scary outcomes. Be worried about ransomware groups\n> cutting out the middleman with AI automation. You can\u2019t fine tune against\n> business operational efficiency without neutering the value proposition of\n> the entire model. There is nuance to cyber and AI and you won\u2019t find it in\n> the doomer headlines.\n>\n> Cyber attacks are always sensationalized but to those defenders in the\n> trenches the asymmetry they face today remains the same as it was pre-LLM\n> era, the only difference now is they\u2019ve got LLMs in their defense toolkit.\n> If we over regulate this technology we will only be benefitting bad actors.\n\nBut the fact that a defense was available does not change the fact that often\nit wasn\u2019t used? What good is an LLM in your defensive toolkit unless you use\nthe defensive toolkit?\n\nI totally buy that right now, LLMs are powering at most a tiny fraction of\ncyberattacks, and that this will continue in the near term. I also buy that if\nyou used LLMs well, you could enhance your cyberdefenses.\n\nI would also say that, as Chris Rohlf indicates, the people getting into\ntrouble are mostly failing to take far more rudimentary precautions than that.\nAnd if LLMs are being widely used to strengthen cyberdefenses, I have not\nheard about it. So it seems weird to turn this around and say this is actively\nhelping now, as opposed to doing minimal marginal harm.\n\nAnd as always, this is a failure to be of much practical use in the task right\nnow. That does not tell us much about usefulness in the future as capabilities\nadvance. For that we need to look at details, and extrapolate future\ncapabilities.\n\nPraise you like I should.\n\n> Near Cyan: a16z employees compliment each other so much that grok keeps\n> turning it into a full news story.\n\nGenuine kudos to them for getting there first. The red card rule applies, if\nyou are the first person to exploit a loophole then congratulations. White hat\nhackers unite. Longer term, this is going to be a problem once people figure\nout they can do it too.\n\nState Library of Queensland introduces an AI-powered WWI veteran for people to\ntalk to, does this based on a basic system prompt, it goes about how you would\nexpect.\n\n####\n\nLlama We Doing This Again\n\nI previously covered the Llama-3 announcement and release in another post.\n\nWhat if Llama going open has the simplest of explanations?\n\n> Arvind Narayanan: Twitter is inventing increasingly fanciful reasons why\n> Meta's releasing models openly while Zuck gives the super straightforward,\n> obvious-in-retrospect reason: Meta will spend > 100B on inference compute\n> and if people make it 10% more efficient it will have paid for itself.\n>\n> I think people still underestimate how much the lifetime inference cost for\n> a successful model exceeds its training cost, which is probably why this\n> explanation wasn't obvious.\n>\n> Also the fact that Meta itself plans to be the biggest consumer of its\n> models in fulfilling Zuck's vision of a future of the internet where there\n> there are 1000 fake people for every real person or whatever.\n\nMeta will be the biggest American consumer of Meta\u2019s models, because Meta\nforces anyone similarly large to ask permission and pay Meta for using them.\n\nIt is not obvious that Meta will be the biggest worldwide consumer of Meta\u2019s\nmodels. There is nothing stopping all the Chinese companies from using it\nwithout paying. Several of them could plausibly end up as larger customers.\n\nDoes the prospect of becoming faster justify the release on its own? That\ndepends on both how much inference cost is saved, and how much Meta is helping\nits competitors and making its life harder in other ways. To take advantage,\nMeta would have to use the improvements found elsewhere for an extended\nperiod. That is not impossible, but remember that Llama-3 is only open\nweights, not fully open source.\n\nIn other Meta opening up news, they are letting others make hardware using\ntheir Horizon OS for virtual reality headsets. This is good openness. As\nopposed to Apple, who are making it as hard as possible to buy into VisionOS\nand the Apple Vision Pro. I would have given them a real shot, for thousands\nof dollars, if they had been willing to integrate with my existing computer.\n\nWhat does the market think? I previously noted that the market was not\nimpressed.\n\nNo, seriously. The market is profoundly unimpressed.\n\n> Kurt Wagner (Bloomberg, April 24): Mark Zuckerberg is asking investors for\n> patience again. Instead, they\u2019re alarmed.\n>\n> After Meta Platforms Inc. revealed that it will spend billions of dollars\n> more than expected this year \u2014 fueled by investments in artificial\n> intelligence \u2014 the company\u2019s chief executive officer did his best to soothe\n> Wall Street. But the spending forecast, coupled with slower sales growth\n> than anticipated, sent the shares tumbling as much as 15% in premarket\n> trading on Thursday.\n>\n> ...\n>\n> Those metrics overshadowed what was otherwise a solid first quarter, with\n> revenue of $36.5 billion, an increase of more than 27% over the same period\n> a year ago. Profit that more than doubled to $12.4 billion.\n>\n> \u201cFor all Meta\u2019s bold AI plans, it can\u2019t afford to take its eye off the\n> nucleus of the business \u2014 its core advertising activities,\u201d Sophie Lund-\n> Yates, an analyst at Hargreaves Lansdown, said in a note on Wednesday. \u201cThat\n> doesn\u2019t mean ignoring AI, but it does mean that spending needs to be\n> targeted and in line with a clear strategic view.\u201d\n\nMeta told investors it was spending lots of money on AI, and showed it the\nfruits of that spending. Investors went \u2018oh no, you are spending too much\nmoney on AI despite still being profitable\u2019 and took shares down 15% overnight\nand this held at the open on Thursday morning, wiping out $185 billion in\nmarket cap.\n\n####\n\nFun with Image Generation\n\nBold claims are ahead.\n\n> Nick St. Pierre: Midjourney CEO during office hours today:\n>\n> \"For the next 12 months it's about video, 3D, real time, and bringing them\n> all together to non interactive world simulator. Then, we'll add the\n> interaction layer to it.\"\n>\n> Holodeck coming.\n\nWilliam Shatner offered an album with this cover.\n\nYou can guess how people reacted. Not great.\n\nHe found himself in a hole. He kept digging.\n\n> Nikki Lukas Longfish: Didn\u2019t actors and writers just strike against Ai?\n> Artists are humans too who like their craft and don\u2019t want AI taking over.\n>\n> [shows image of Shatner blocking Nikki.]\n>\n> William Shatner: Well sweetheart, the only image is of me and I approved it.\n> That means your craycray hysteria argument is null. The actor\u2019s union issue\n> was that studios could take moving images of previous acting jobs and\n> repurpose the moving images and put them into an AI program for use in\n> another production without permission. Next time if you are going to argue\n> something, please make sure you understand the issue.\n>\n> Panic Gamer: Bill, this isn't just you.\n>\n> That AI stole work from other artists FOR you.\n>\n> William Shatner: And those artists that \u201cborrow\u201c from other artist\u2019s works\n> as a homage?\n>\n> That\u2019s stealing as well, right? \ud83e\udd37\ud83c\udffc\n\n> William Shatner: Well don\u2019t buy the album when it comes out, craycray. It\u2019s\n> simple! \ud83e\udd14I can have it marketed as \u201cBuy the album the (BS) Artists of X\n> hated because they were \ud83c\udf51they weren\u2019t hired to do the cover\u201d \ud83e\udd23\n\nThe position of the artist community, and much of the internet, is clear. By\ntheir account, all use of AI artwork is stealing, from every artist. If you\nuse AI art, to them you are dishonorable scum.\n\nIt is not clear to me how they feel about using AI artwork for your powerpoint\npresentation at work, or if I put something into this column, where it is\nclear that if AI was unavailable it would never make sense to commission\nartwork, you would either use stock footage or have no art. What is clear is\nthat they are, broadly speaking, having none of it.\n\nI do not expect that to change until there is an artist compensation scheme.\n\n####\n\nDeepfaketown and Botpocalypse Soon\n\nIt gets harder and harder to tell when it is so over versus when we are so\nback. Link goes to a video of a man talking, transformed into a young woman\ntalking by AI.\n\nSo, this happened, very much a mixed result:\n\n> Kristen Griffith and Justin Fenton (The Baltimore Banner): Baltimore County\n> Police arrested Pikesville High School\u2019s former athletic director Thursday\n> morning and charged him with allegedly using artificial intelligence to\n> impersonate Principal Eric Eiswert, leading the public to believe Eiswert\n> made racist and antisemitic comments behind closed doors.\n>\n> ...\n>\n> Burke said he was disappointed in the public\u2019s assumption of Eiswert\u2019s\n> guilt. At a January school board meeting, he said the principal needed\n> police presence at his home because he and his family have been harassed and\n> threatened. Burke had also received harassing emails, he said at the time.\n\nIt seems to have fooled at least some of the people some of the time, and that\nwas enough to make Eiswert\u2019s life rather miserable. But for now you still\ncannot fool all the people all the time, and the police eventually figured it\nout.\n\nUK bans two biggest pornography deepfake sites. Story from Wired does not say\nwhich ones they were, I can guess one but not the other. It will be a while\nbefore this style of measure stops mostly working for most people, since most\npeople are incapable of setting up a Stable Diffusion instance.\n\nOpenAI and all the usual suspects including Meta, StabilityAI and CivitAI\ncommit to AllTechIsHuman\u2019s Generative AI Principles to Prevent Child Sexual\nAbuse. It was remarkably difficult to figure out what those principles\nactually were. There was no link there, the announcement by ATIH didn\u2019t say\neither, and the link to the policy leads to a page that won\u2019t scroll. I\nfinally managed to get a copy into Google Docs.\n\nIt starts with explaining why we should care about AIG-CSAM (AI generated\nchild sexual material). I agree we should prevent this, but several of the\narguments here seemed strained. I do think this is something we should\nprevent, but we should either state it as a given (and that would be totally\nfair) or give sound arguments. The arguments here seem like something out of\nLaw & Order: SVU, rather than something from 2024.\n\nWhat are the actual things it asks participants to do?\n\nThings you really should not have needed a commitment in order to do. I am\nstill happy to see everyone commit to them. Also, I do not know how to do\nseveral of them if you are going to release the weights to your image model?\nWhat am I missing?\n\n  1. Responsibly source your training datasets, and safeguard them from CSAM and CSEM. That won\u2019t fully solve the issue, but it helps.\n\n  2. Incorporate feedback loops and iterative stress-testing strategies in your development process.\n\n  3. Employ content provenance with adversarial misuse in mind. Again, right, sure. So how exactly is Stability.ai going to do this, if they open source SD3?\n\n  4. Safeguard your generative AI products and services from abusive content and conduct.\n\n  5. Responsibly host your models. \u201cAs models continue to achieve new capabilities and creative heights, a wide variety of deployment mechanisms manifests both opportunity and risk. Safety by design must encompass not just how your model is trained, but how your model is hosted.\u201d Again, somebody explain to me how this is theoretically possible if I can download the weights and run locally.\n\n  6. Encourage developer ownership in safety by design.\n\n  7. Prevent your services from scaling access to harmful tools.\n\n  8. Invest in research and future technology solutions.\n\n  9. Fight CSAM, AIG-CSAM and CSEM on your platforms.\n\nSo yes, I am very much in favor of all these steps being formalized.\n\nAlso, a quick word to everyone who responded on the internet with a version of\n\u2018disappointment\u2019 or \u2018GPT-5 when\u2019: That was bad, and you should feel bad.\n\nKaj Sotala points out that any passphrase or similar proof of identity is only\nas secure as people\u2019s willingness to reveal it. If Alice and Bob use passcodes\nto prove identity to each other, who goes first? Could a fake Alice get Bob to\ngive her the passkey? This is of course a well-known problem and class of\nattacks amongst humans. The only fully secure code is a one-time pad. I\npresume the central answer is that a passkey is part of defense in depth. You\nhave to use it in addition to your other checks, not as an excuse to otherwise\nstop thinking.\n\nAnd if you do reveal the passkey for any reason, you realize that it is no\nlonger secure and you may be under attack, and respond accordingly. Right now,\nas Kaj notes, you can be confident that 99%+ of AI-enabled attacks are not\ngoing to be capable of a two-step like this, and scammers are better off\nfinding softer targets. Over time that will change. Things will get weird.\n\n####\n\nThey Took Our Jobs\n\nEvery. Damn. Time.\n\nFirst they came for the translators and illustrators, and mostly that is all\nthey have come for so far. Your periodic reminder that for now there will be\nplenty of other jobs to do to go around, but \u2018there are other things that\nhumans have comparative advantage doing\u2019 may not last as long as you expect.\nThat is on top of the question of whether the AIs are stealing people\u2019s work\nwithout payment in various ways.\n\nA strange bedfellow.\n\n> Ms. Curio: just found the most fascinating anti-ai person who is only anti-\n> ai because they make and sell the software that spambots USED to use to\n> flood the internet with low quality SEO-bait garbage and ChatGPT is putting\n> them out of business. What a fascinating category of human to be.\n\nYes, I suppose writing endless streams of drek is one job the AI is going to\ntake.\n\nWho would you be? It seems you would be a general high performer, who\nunderstands the safety and policy spaces, rather than someone with deep\ntechnical knowledge. This seems like a highly impactful position, so if you\nare a good fit, again please consider.\n\n####\n\nGet Involved\n\nThe Office of Management and Budget is hiring a Program Analyst. This is a\nhigh leverage job, so if you could be a good match consider applying.\n\nGoogle DeepMind is hiring an AGI Safety Manager for their Safety Council. Job\nis in London.\n\nCivAI is hiring a software engineer to help create concrete capabilities\ndemonstrations.\n\n####\n\nIntroducing\n\nA robot dog with a flamethrower. Sure, why not? I mean, other than the\nobvious.\n\nA poetry camera? You take picture, it gives you AI poetry.\n\n####\n\nIn Other AI News\n\nGoogle declares Mission First in the wake of doing the normal company thing of\nfiring employees who decide to spend work hours blockading their bosses\u2019\noffice.\n\n> Google\u2019s The Keyword (tagline: Building For Our AI Future): Mission first\n>\n> One final note: All of the changes referenced above will help us work with\n> greater focus and clarity towards our mission. However, we also need to be\n> more focused in how we work, collaborate, discuss and even disagree. We have\n> a culture of vibrant, open discussion that enables us to create amazing\n> products and turn great ideas into action. That's important to preserve. But\n> ultimately we are a workplace and our policies and expectations are clear:\n> this is a business, and not a place to act in a way that disrupts coworkers\n> or makes them feel unsafe, to attempt to use the company as a personal\n> platform, or to fight over disruptive issues or debate politics. This is too\n> important a moment as a company for us to be distracted.\n>\n> Brian Armstrong (CEO Coinbase): It\u2019s a great start. And it will probably\n> take much more than this fully correct. (Like an exit package for some % of\n> the company.)\n>\n> Google is a gem of American innovation, and the company I looked up to most\n> growing up. I doubt they need it, but happy to help in any small way if we\n> can.\n>\n> Life is so much better on the other side. Get back to pure merit and tech\n> innovation like early Google.\n\nSam Altman Invests in Energy Startup Focused on AI Data Centers (WSJ). Company\nis called Exowatt, whose technology focuses on how to use solar power to\nsatisfy around-the-clock demand. Altman loves his fusion, but no reason not to\nplay the field.\n\nOpenAI and Moderna partner up to design new products. The good stuff.\n\n> \u201cIf we had to do it the old biopharmaceutical ways, we might need a hundred\n> thousand people today,\u201d said Bancel. \u201cWe really believe we can maximize our\n> impact on patients with a few thousand people, using technology and AI to\n> scale the company.\u201d\n\nExcellent news. Also consider what this implies generally about productivity\ngrowth. If Moderna is claiming 1000% gains, perhaps they are in a uniquely\nstrong position, but how unique?\n\nVarious AI companies, universities and civil society groups urge Congress to\nprioritize NIST\u2019s request for $47.7 million of additional funding for its AI\nsafety institute. I concur, this is a fantastic investment on every level, on\na \u2018giving money to this would not be an obviously poor choice of donation.\u2019\n\nGPT-4 proved able to exploit 87% of real-world one-day software\nvulnerabilities in real world systems, identified in a dataset of critical\nseverity CVEs, versus 0% for GPT-3.5 and various older open source models. It\nrequires the CVE description to do this, and it was given full web access.\nTesting newer models would presumably find several other LLMs that could do\nthis as well.\n\n> Jason Clinton: The paper is already outdated given the release of more power\n> models but there's an important empirical trend line to observe here. This\n> portends the need for defenders to get patches out to every piece of\n> infrastructure in days, not months.\n\nChris Rohlf responds here that the exploits were well-described on the web by\nthe time this test was run, and he is concluding that GPT-4 was likely getting\nkey information off the web in order to assemble the attack. If that is\nrequired, then this won\u2019t narrow the time window, since you have to wait for\nsuch write-ups. From the write-up, it is impossible to tell, and he calls the\nauthors to release the details, saying that they would not be harmful at this\nstage. I agree that would be the way forward.\n\nThe speed requirement is that you patch as fast as you get attacked. If AIs\nmean that in the future the attacks go out within an hour, then that is how\nlong you have. It still has to actually happen, not only be possible in\ntheory. So there will likely be a period where \u2018hit everyone within an hour\u2019\nis technologically plausible, but no one does it.\n\nPerplexity raises $62.7 million at a $1.04 billion valuation, round led by\nDaniel Gross and including many strong names. My brain of course thought \u2018that\nvaluation should have been higher, why did they not raise the price.\u2019\n\nCognition Labs, the creators of Devin, are raising from Founders Fund at a $2\nbillion valuation, despite being only six months old with no revenue.\n\nOpenAI introduces additional \u2018enterprise-grade\u2019 features for API customers:\nEnhanced enterprise-grade security, better administrative control,\nimprovements to the assistants API and more cost management options. All\nincremental.\n\nMicrosoft releases Phi-3-medium and Phi-3-mini, with the mini being a 3.8\nbillion parameter model that can run on a typical phone, while getting\nbenchmarks like 69% MMLU and 8.38 MT-bench. Phi-3 is 14B. This is plausibly\nthe best model in the tiny weight class for the moment.\n\nApple confirmed by Bloomberg to be \u2018by all accounts\u2019 planning an entirely on-\ndevice AI to put into its phone. We don\u2019t know any technical details.\n\nAnthropic CEO Dario Amodei says that having distribution partnerships with\nboth Google and Amazon keeps Anthropic independent.\n\nLLM evaluators recognize and favor their own generations.\n\n> Andreas Kirsch: Just try all the LLMs until you find one that really likes a\n> paper and bingo \ud83d\ude05\n\nYes, actually. This effect all makes perfect sense. Why wouldn\u2019t it be so?\n\n\u2018TSMC\u2019s debacle in the American desert.\u2019 TSMC has a beyond intense\nauthoritarian pure-work-eats-your-entire-life culture, even by Taiwanese\nstandards. They are trying to compromise somewhat to accommodate American\nworkers, but a compromise can only go so far. There has been a year\u2019s delay,\nand engineers are disgruntled and often quitting. It is very American to\nconsider this all a debacle, and to presume that TSMC\u2019s culture is broken and\nthey succeed in spite of it rather than because of it. I would not be so\nconfident of that. The default is that TSMC will be unable to hire good and\nretain good workers and this will not work, but are we sure their culture is\nnot a superior way to make chips to the exact (and wow do we mean exact)\nspecifications of customers?\n\nColin Fraser explores how hallucinations work, and follows up with a thread\nwarning of problems when asking an LLM to assess the hallucinations of another\nLLM.\n\n####\n\nQuiet Speculations\n\nA short story from OpenAI\u2019s Richard Ngo, called Tinker.\n\nOnce again, it has only been a year since GPT-4.\n\n> Dan Hendrycks: GPT-5 doesn't seem likely to be released this year.\n>\n> Ever since GPT-1, the difference between GPT-n and GPT-n+0.5 is ~10x in\n> compute.\n>\n> That would mean GPT-5 would have around ~100x the compute GPT-4, or 3 months\n> of ~1 million H100s.\n>\n> I doubt OpenAI has a 1 million GPU server ready.\n>\n> MachDiamonds: Sam said they will release an amazing model this year, but he\n> doesn't know what it will be called. Dario said the ones training right now\n> are $1 billion runs. Which would kind of line up with GPT-4.5 being 10x more\n> compute than GPT-4.\n\nWhereas others are indeed spending quite a lot.\n\n> Ethan Mollick: I don\u2019t think people realize the scale of the very largest\n> tech companies & the resources they can deploy.\n>\n> Amazon has spent between $20B & $43B on Alexa alone & has/had 10,000 people\n> working on it.\n\nAlec Stapp: Has anyone ever done a deep dive on those Alexa numbers?\nIncredible to me that they have invested that much and produced so little\n\nIt continuously blows my mind how terrible Alexa continues to be, and I keep\nforgetting how much money they are incinerating. I have no idea what all those\npeople do all day.\n\nDaniel here is responding to Pliny\u2019s latest jailbreak report. As is often the\ncase, the problem with fiction is it has to sound reasonable and make sense.\nThe real world does not.\n\n> Daniel Eth: So I\u2019m working on writing a piece on \u201chow could misaligned AGI\n> actually take over\u201d, and one narrative I considered was \u201crogue AGI\n> jailbreaks a bunch of other AIs to get allies and cause havoc which it\n> exploits\u201d. But I discarded that idea, because it felt too scifi-ish\n\nTyler Cowen offers a robust takedown of the new Daron Acemoglu paper so I do\nnot have to. I want to stay to him here both that he did an excellent job, and\nalso now you know how I feel.\n\nI will only point out that I would (as regular readers know) go much farther\nthan Tyler on expected future productivity growth. Tyler says 0.7% TFP\n(additional productivity growth per year) from AI in the coming decade is a\nreasonable estimate. I think that would be highly surprisingly low even if we\nassumed no foundation model gains beyond this point because we hit a wall. And\nto me it makes no sense if we assume 5-level and 6-level models are coming,\neven if that does not lead to \u2018AGI\u2019 style events.\n\nWashington Post\u2019s Gerrit De Vynck writes a post with the headline \u2018The AI hype\nbubble is deflating.\u2019 The body of the article instead mostly says that the AI\napplications are not currently good enough for the kind of use that justifies\nthe level of investment, and people have not adapted to them yet. That we are\nat \u2018the very, very beginning.\u2019 Well, yes, obviously. That is the whole point,\nthat they will be better in the future.\n\nScaling laws are about perplexity. They are not about what is enabled by the\nperplexity. This was driven home to me when I asked what a 1.69 score - the\nimplied minimum loss - would mean in practical terms, and everyone agreed that\nno one knows.\n\n> Yo Shavit (OpenAI): This discourse about the functional form of AI progress\n> doesn\u2019t make any sense.\n>\n> Scaling laws don\u2019t tell you anything about the rate of capability increase,\n> only perplexity.\n>\n> E.g. the capability jump from .92 -> .90 perplexity might be >>> that from\n> 1.02 -> 1.0, or it could be \u2248.\n>\n> The most annoying thing is that the delta(capabilities)-per-\n> delta(perplexity) might not even increase monotonously in lower perplexity.\n>\n> Capabilities progress rates might randomly \u23e9 or slow down. All we know is\n> we\u2019re moving along the curve, and by 1.69 [?] they\u2019ll be \u201cperfect\u201d.\n>\n> Where \u201cperfect\u201d roughly means \u201cable to omnisciently extract all bits of\n> available information to simulate the world forward, modulo what\u2019s\n> impossible due to the NN\u2019s architecture.\u201d\n\nRight. But what does that mean you can do? That\u2019s the thing, no one knows.\n\nWhat will LLMs never be able to do? As Rohit notes, people who propose answers\nto this question often get burned, and often rather quickly. Still, he sees\nclear patterns of what counts as sufficiently multi-step, or has the wrong\nkind of form, such that LLMs cannot do it. On their own, they can\u2019t play\nConway\u2019s Game of Life or Sudoku, and that looks not to change over time, their\nalgorithms are not up to longer reasoning steps.\n\n> It might be best to say that LLMs demonstrate incredible intuition but\n> limited intelligence. It can answer almost any question that can be answered\n> in one intuitive pass. And given sufficient training data and enough\n> iterations, it can work up to a facsimile of reasoned intelligence.\n>\n> The fact that adding an RNN type linkage seems to make a little difference\n> though by no means enough to overcome the problem, at least in the toy\n> models, is an indication in this direction. But it\u2019s not enough to solve the\n> problem.\n>\n> In other words, there\u2019s a \u201cgoal drift\u201d where as more steps are added the\n> overall system starts doing the wrong things. As contexts increase, even\n> given previous history of conversations, LLMs have difficulty figuring out\n> where to focus and what the goal actually is. Attention isn\u2019t precise enough\n> for many problems.\n>\n> A closer answer here is that neural networks can learn all sorts of\n> irregular patterns once you add an external memory.\n>\n> ...\n>\n> So the solution here is that it doesn't matter that GPT cannot solve\n> problems like Game of Life by itself, or even when it thinks through the\n> steps, all that matters is that it can write programs to solve it. Which\n> means if we can train it to recognise those situations where it makes sense\n> to write in every program it becomes close to AGI.\n>\n> (This is the view I hold.)\n\nThe conclusion is that proper prompting will continue to be super important.\nIt won\u2019t fully get there, but kludges could approximate what there would look\nlike.\n\nWill agents work?\n\n> Colin Fraser: the thing about agents is every single current implementation\n> relies on the truth of the following proposition it's not clear that it is\n> true: if you beg and bargain with an LLM correctly it will transubstantiate\n> into an agent.\n>\n> Basically they assume that the way to create a paperclip maximizer is to ask\n> a language model to be a paperclip maximizer. But this doesn't seem to work,\n> nor is there really any reason to expect it to work because maximizing\n> paperclips is simply not what an LLM is designed to do.\n>\n> What they do do, on the other hand, is pretend that they're doing what you\n> asked. So if you ask your PaperClipMaximizer agent what it's up to it will\n> happily say \"maximizing paperclips, boss!\", and it seems that for the median\n> AI enthusiast, that's sufficient.\n>\n> AI boosters are actually significantly downplaying the magnitude of the\n> miracle they're attempting to perform here. The claim is that you can turn a\n> random text generator into a goal-seeking being just by seeding it with the\n> right initial text. This seems prima facie impossible.\n\nWell yes and no. The obvious trick is to ask it to iminiate a goal-seeking\nbeing, or tune it to do so. In order to predict text it must simulate the\nworld, and the world is full of goal-seeking beings. So this seems like a\nthing it should at some level of general capability be able to do, indeed\nhighly prima facie possible, if you ask it. And indeed, most people are highly\nwilling to use various scaffolding techniques, the \u2018beg and bargain\u2019 phase\nperhaps, in various ways.\n\nBut mostly I do not see the issue? Why other than the model not being good\nenough at text prediction would this not work?\n\n####\n\nRhetorical Innovation\n\nJanus writes out a thread about why it is difficult to explain his work on\nunderstanding LLMs. I do not know how much I get, it is at least enough to be\nconfident that Janus is saying real things that make sense.\n\nA key crux in many debates is whether LLMs will be and remain tools, or\nwhether they are or will become something more than that.\n\n> Roon: I don\u2019t care what line the labs are pushing but the models are alive,\n> intelligent, entire alien creatures and ecosystems and calling them tools is\n> insufficient. They are tools in the sense a civilization is a tool.\n>\n> And no this is not about some future unreleased secret model. It\u2019s true of\n> all the models available publicly.\n\nI do think that as currently used tool is an appropriate description, even if\nRoon is right about what is happening under the hood. I do not think that\nlasts.\n\nSpenser Greenberg explains several of the reasons why AI abilities won\u2019t top\nout at human levels of understanding, even if AIs have to start with human\ninputs. Self-play, aggregated peak performance, aggregated knowledge, speed,\nunique data and memory go a long way. And of course there are tasks where AIs\nare already superhuman, and humans often become better than other humans using\nonly other humans as input.\n\nA long essay saying mindspace is deep and wide. The message here seems to be\nthat AIs are like children, new intelligent beings very different from\nourselves, and already we are sort of cyborgs anyway. Everything must change\nand evolve to survive. This new AI thing will not be different in kind and is\na great opportunity to explore, we will in important ways be Not So Different,\nand we should not \u2018fear change.\u2019\n\nSo, no. Mindspace is in some senses deep and wide among humans, but not like\nmindspace is deep and wide among possible minds. The fact that change always\nhappens does not mean rapid unlimited unpredictable unsteerable change is an\nacceptable path to walk down, or that it will result in anything we value. Or\nthat we should agree to a universe we do not value, or that this means we need\nto abandon our values for those of future minds, even complex minds. I will\nnot accept a universe I do not value. I will not make the is-ought confusion.\nI will not pass quietly into the night.\n\nEliezer Yudkowsky points out that equivocating between various contradictory\njustifications why AI will be safe is normal politics, but it is not how one\ndoes engineering to get that AI to be safe. That there is no \u2018standard story\u2019\nfor why everything will be fine, no good engineering plan for doing it, there\nis only various people saying and hoping various (often logically\ncontradictory but also often orthogonal) things, including often the same\nperson saying different things at different times to different people. Which\nagain is totally normal politics.\n\nHow long have you got?\n\n> Mustafa Suleyman (CEO of Microsoft AI): The new CEO of Microsoft AI,\n> @MustafaSuleyman, with a $100B budget at TED:\n>\n> \"AI is a new digital species.\"\n>\n> \"To avoid existential risk, we should avoid:\n>\n>   1. Autonomy\n>\n>   2. Recursive self-improvement\n>\n>   3. Self-replication\n>\n>\n\n>\n> We have a good 5 to 10 years before we'll have to confront this.\"\n>\n> Kevin Fischer: Wait this is my roadmap.\n\nI would not presume we have a good 5-10 years before confronting this. We are\nconfronting the beginnings of autonomy right now. The threshold for self-\nreplication capabilities was arguably crossed decades ago.\n\nThe bigger question is, who is \u2018we,\u2019 and how does this we avoid those things?\n\n\u2018Hope that lots of people and organizations each individually make the choice\nnot to do this thing\u2019 is a strategy we know will not work. Even if we figured\nout how to do that, and we all knew exactly how to not do these things, it\nstill would not work. We need some other plan.\n\nWe also need to know how not to do it while still advancing capabilities,\nwhich we do not know how to do even if everyone involved was on board with not\ndoing them. Or we could at some point stop advancing capabilities.\n\nTrue this:\n\n> Connor Leahy: We need \"...in humans\" in AI as the equivalent to \"...in mice\"\n> for biology lol\n\nAlso, periodic reminder that many of those who would dismiss existential risk\nas obvious nonsense (or in this case \u2018total f***ing nonsense\u2019) will continue\nto insist that anyone who says otherwise could not possibly understand the\ntech, and say words like \u2018This is what happens when you hire the business\ncofounder who doesn\u2019t really understand the tech\u2019 in response to the most\nmilquetoast of statements of risk like the one by Suleyman. Do they care about\nstatements by Hassabis, Amodei, Bengio, Hinton, Sutskever and company? No. Of\ncourse they do not.\n\nScott Alexander ponders the whole Coffepocalypse argument, of the general form\n\u2018people have warned in the past of things going horribly wrong that turned out\nfine, so AI will also turn out fine.\u2019 Mostly this is Scott trying to find some\nactual good argument there and being exasperated in the attempt.\n\nThis is the general case, so the \u2018actually the warnings about coffee were\ncorrect, Kings worried it would forment revolutions and it did that\u2019 is\nrelegated to a footnote. One could also say less dramatically, that coffee\narguably gives humans who take it an edge, but is an insufficient cognitive\nenhancer to allow coffee-infused humans to dominate non-coffee humans.\n\nBut imagine the thought experiment. Suppose you found a variant that enhanced\nthe effects of coffee and caffeine without the downsides. The new version does\nnot create tolerance or addiction or a deficit to be repaid or interfere with\nsleep, it purely gives you more awakeness and production, at a rate many times\nthat of current coffee. Except only some people get the benefits, in others it\nhas no effect, and this is genetic. What happens in the long run? In the short\nrun? How does it change if those people also gained other advantages that AIs\nlook to enjoy?\n\n####\n\nWouldn\u2019t You Prefer a Nice Game of Chess\n\nThis view of game theory is a large part of the problem:\n\n> Tyler Cowen: The amazing Gukesh (17 years old!) is half a point ahead with\n> one round remaining today. Three players \u2014 Nakamura, Caruana, and Nepo \u2014\n> trail him by half a point. Naka is playing Gukesh, and of course Naka will\n> try to win. But what about Caruana vs. Nepo? Yes, each must try to win (no\n> real reward for second place), but which openings should they aim for?\n>\n> You might think they both will steer the game in the direction of\n> freewheeling, risky openings. Game theory, however, points one\u2019s attention\n> in a different direction. What if one of the two players opts for something\n> truly drawish, say like the Petroff or (these days) the Berlin, or the Slav\n> exchange variation?\n>\n> Then the other player really needs to try to win, and to take some crazy\n> chances in what is otherwise a quite even position. Why not precommit to\n> \u201cdrawish,\u201d knowing the other player will have to go to extreme lengths to\n> rebel against that?\n>\n> Of course game theory probably is wrong in this case, but is this such a\n> crazy notion? I\u2019ll guess we\u2019ll find out at about 2:45 EST today.\n\n(I intentionally wrote this without checking anything about how the games\nplayed out.)\n\nSo essentially Caruana and Nepo had a game where a draw was almost a double-\nloss, a common situation on the Magic: The Gathering circuit.\n\nThis is saying that it is the rational and correct choice to intentionally\nsteer the game into a space where that draw is likely. Having done this on\npurpose, clearly you are the crazy one who will not back down. Then, the\nreasoning goes, the other player will be forced to take a big chance, to open\nthemselves up, and you can win.\n\nThis is a no good, terrible strategy. These players are in an iterated game\nover many years, and also everyone in chess will identify you as dishonorable\nscum. And the other player knows all eyes are on the game, and they cannot\nyield now.\n\nEven if those facts were not so, or the stakes here were sufficiently high\nthat you did not care? It would still not work as often as agreeing to let the\ngame be high variance and playing for a win.\n\nYou see, when a person sees their opponent in a game do something like that,\nyou know what most of them say back? They say f*** you.\n\nThat goes double here. You have deliberately introduced a much higher chance\nof a double-loss in order to get them to likely hand you a win. You are a\nchess terrorist. If the other person gives in, they probably don\u2019t do any\nbetter than not budging, even if you also don\u2019t budge. There is a remarkably\nhigh chance they let the game draw.\n\nCan you lean into a little of this usefully, if it is not too blatant? You\ncould try. You could, for example, go down a path as white that gives black a\nway to equalize but that would render the game drawish, on the theory that he\nwill not dare take it. Or you can flat out offer repetition at some point, in\na game where you have the advantage. You can be situationally aware.\n\nBut I would not push it.\n\n(Note that in Magic, the normal way this plays out is that both players do\ntheir best to win, and then if it is about to end in an honorable draw, then\noften the player who would be losing if time was extended will give the other\nplayer the win. An actual recorded draw mostly happened here (back in the day)\nwhen the players could not agree on who was winning and neither was willing to\nback down, or one of the players pissed off the other one, such as by trying\nto play the angles too hard, or they simply preferred the player who would\nlikely get displaced to their opponent. Or sometimes a player would have a\nweird code of honor or not realize the situation. However in chess it does not\nwork that way and the players are not allowed to do any of that.)\n\nA lot of people think of game theory, and the logic of game theory, as if they\nwere in Remembrance of Earth\u2019s Past (The Three Body Problem), where everyone\nmust constantly defect against everyone because hard logic, that\u2019s simple\nmath. And they ask why people quite often do not do this, and it quite often\nworks out for them.\n\nOr you can realize that yes, humans have various ways to cooperate their way\nout of such situations, that even the single-move prisoner\u2019s dilemma can often\nbe won in real life, it has clear strong solutions in theory especially among\nASIs, and the iterated prisoner\u2019s dilemma is not even hard. Learn some\nfunctional decision theory!\n\n(No, seriously, learn some functional decision theory. Humans should use it,\nand we will want our future AIs to use it, because other theories are wrong.\nYou cannot implement it fully as a human, but you can approximate it in\npractice. And that\u2019s fine.)\n\nIf in international relations or business negotiations or elsewhere, where\nfailure to reach agreement means everyone loses, you should absolutely work\nangles and maximize your leverage. Sometimes that forces you to increase the\nchance of breakdown of talks. But if your plan is to basically try to ensure\nthe talks are likely to break down to force the other side to cave, when both\nof you know you are not fine with talks actually caving?\n\nThat is worse than a crime. It is a mistake.\n\n####\n\nThe Battle of the Board\n\nThere was a notable exchange this week discussing the implications of the\nevents last year at OpenAI.\n\nMostly I want to reproduce it here for ease of access and posterity, since\nsuch things are very hard to navigate.\n\n> Rob Bensinger: The thing I found most disturbing in the board debacle was\n> that hundreds of OpenAI staff signed a letter that appears to treat the old-\n> fashioned OpenAI view \"OpenAI's mission of ensuring AGI benefits humanity\n> matters more than our success as a company\" as not just wrong, but beyond\n> the pale.\n>\n> Prioritizing your company's existence over the survival and flourishing of\n> humanity seems like an obviously crazy view to me, to the point that I\n> suspect most of the people who signed the letter don't actually think that\n> \"OpenAI shuttering is consistent with OpenAI's mission\" is a disqualifying\n> or cancellable view to express within OpenAI. I assume the letter was\n> drafted by people who weren't thinking very clearly and were under a lot of\n> emotional pressure, and people mostly signed it because they agreed with\n> other parts of the letter.\n>\n> I still find it pretty concerning that cascading miscommunications like this\n> might cause it to become the case that a false consensus forms around \"fuck\n> OpenAI's mission, the org and its staff are what really matters\" within the\n> organization. At the very least, I would love to know that there hasn't been\n> a chilling effect discouraging people from expressing the opinion \"our\n> original mission is still the priority\", so that OpenAI feel comfortable\n> debating this internally insofar as there's disagreement.\n>\n> I encourage @sama and the leadership at @OpenAI to clarify that the relevant\n> part of the letter doesn't represent your values and intentions here, and I\n> encourage OpenAI staff to publicly clarify their personal stance on this,\n> especially if you signed the letter but don't endorse that part of it (or\n> didn't interpret that part the way I'm interpreting it here).\n>\n> None of this strikes me as academic; OpenAI leadership knows that it's\n> building something that could cause human extinction, and has said as much\n> publicly on many occasions. Just say what your priorities are. (And if a lot\n> of staff haven't gotten the memo about that, hell, remind them.)\n>\n> Roon (Member of OpenAI technical staff): It\u2019s entirely consistent to destroy\n> the organization if it\u2019s a threat to mankind.\n>\n> It\u2019s not at all consistent to destroy the organization on the basis of the\n> trustworthiness of one man, whom the employees decided was more trustworthy\n> than the board of directors.\n>\n> Piotr Zaborszcyk: \"whom the employees decided was more trustworthy than the\n> board of directors\" - yeah, but is it true though? Altman more trustworthy\n> than Sutskever? \ud83e\udd2f\n>\n> Roon: there is no doubt in my mind.\n>\n> Rob Bensinger: The impression I'm getting from some OpenAI staff is that\n> their view is something like:\n>\n> \"OpenAI's 1200+ employees are, pretty much to a man, extremely committed to\n> the nonprofit mission. Effectively all of us take existential risk from AI\n> seriously, and would even be willing to undergo a lot of personal turmoil\n> and financial loss if that's what it took to ensure OpenAI's mission\n> succeeds. (E.g., if OpenAI had to shut down, reduce team size, or scale down\n> its ambitions in response to safety concerns.)\n>\n> \"This is true to such an extreme degree that I have a hard time imagining it\n> being non-obvious to anyone who's been in the x-risk space for more than 30\n> seconds. There's literally no point in us reiterating our commitment to\n> existential risk reduction for the thousandth time. You claim that the staff\n> open letter was ambiguous, but I don't think it's ambiguous at all; I feel\n> like you're trying to tar our reputation and get us to jump through hoops\n> for no reason, when it should be obvious to everyone that OpenAI staff and\n> leadership have OpenAI's social impact as their highest priority.\"\n>\n> To which I say: I've never worked at OpenAI. The stuff that's obvious to you\n> isn't obvious to me. I'm having to piece together the situation from talking\n> to OpenAI staff, and some of them are saying stuff like the above, and\n> others are saying the opposite. Which leaves me pretty fuzzy about what the\n> actual situation is, and pretty keen to hear something more definitive from\n> OpenAI leadership, or at least to hear a slightly longer account that helps\n> me see how to reconcile the conflicting descriptions.\n>\n> I flatly disagree with \"the staff open letter wasn't ambiguous\", and I\n> strongly suspect that this view is coming from an illusion-of-transparency\n> sort of place: empirically, people often have a really hard time seeing how\n> a sentence can be interpreted differently once they have their own\n> interpretation in mind.\n>\n> But also, human nature being what it is, it is not unheard-of for people to\n> endorse a high-level nice-sounding claim, while balking at some of the less-\n> nice-sounding logical implications of that claim. @OpenAI tweeting out \"We\n> affirm that OpenAI wants the best for everyone\" is easy mode. OpenAI\n> tweeting out \"We affirm that shutting down OpenAI is consistent with the\n> nonprofit mission, and if we ever think shutting down is the best way to\n> serve that mission, we'll do it in a heartbeat\" is genuinely less easy. And\n> actually following through is harder still.\n>\n> If you're worried that investors and partners will be marginally less\n> interested in OpenAI if you issue a press statement like that, well: I think\n> that creates an even stronger case for being loud about this. Because you\n> want to be honest and up-front with your investors and partners, but also\n> because to the extent your worry is justified, those investors and partners\n> are creating an incentive pressure for you to back away from your mission\n> later and prioritize near-term profits when push comes to shove. Or to come\n> up with reasons, as needed, for why the seemingly profit-maximizing option\n> is really the long-term-human-welfare-maximizing option after all.\n>\n> If you're correct that OpenAI's staff is currently super invested in the\n> mission, that's awesome! But I expect OpenAI to grow in the future, and to\n> accumulate more investors and more partners. If you're at all worried about\n> mission drift or misaligned incentives in the future, never mind how awesome\n> OpenAI is today, then I think you should be jumping on opportunities like\n> this to clarify that you're actually serious about this stuff, and that you\n> aren't going to say different things to different audiences as convenient,\n> when the issue is this damned central.\n>\n> (To reiterate: These are not performative questions. I'm asking them in\n> public because I think the public discourse is fucked and I would much\n> rather have an actual conversation about this than get recruited into\n> keeping some OpenAI staff secret.\n>\n> I know it's tempting to see everything as a bad-faith eleven-dimensional-\n> chess political game, but I really for real do not know what the hell is\n> going on in OpenAI, and when I ask about this stuff I'm actually trying to\n> learn more, and when I make policy proposals it's because I think they're\n> actually good ideas.)\n>\n> Roon: on every single tender document sent to investors there\u2019s very clear\n> language that OpenAIs primary fiduciary duty is to humanity and that they\n> need to be ready to lose everything if push comes to shove.\n>\n> It is, of course, unreasonable to assume that all 1200+ employees are true\n> believers. Any Mission must employ mercenaries. People would protest the\n> destruction of the OpenAI organization even in the case that it's\n> potentially the right decision. My only point is that's not what happened\n> during The Blip at all.\n>\n> Jskf: To be clear, by \"that is not what happened\", you mean \"the board\n> replacing the CEO at the time was not even potentially the right decision\n> w.r.t. the mission\"?\n>\n> My initial read was \"the destruction of the OpenAI organization was not even\n> possibly the right decision,\" but from what I can tell, this destruction\n> became a worry mostly because of the threat to leave from many of the\n> employees.\n>\n> [There is a post here that was deleted.]\n>\n> Rob Bensinger: \"nah i'd rather let the company die than acquiesce\"\n>\n> If this is what happened, then that's very useful context! From the staff\n> letter, my initial assumption was that a conversation probably occurred\n> like:\n>\n> [start of <imaginary conversation>]\n>\n> Board member: (says not-wildly-unreasonable and substantive stuff about why\n> the board couldn't do its job while Sam was CEO, e.g. 'he regularly lied to\n> us' or whatever)\n>\n> Senior staffer: 'OK but that seems weaksauce when you're putting the\n> goddamned future of the company at risk!! Surely you need a way higher bar\n> than that now that some key senior staff are leaving; removing Sam should be\n> a complete non-starter if it risks us failing in our mission.'\n>\n> Board member: 'That's a real risk, but technically it's consistent with the\n> nonprofit mission to let the company go under; OpenAI's mission is sometimes\n> stated as \"to build artificial general intelligence that is safe and\n> benefits all of humanity\", which makes it sound like the company's mission\n> requires that it achieve AGI. But its actual mission is \"to ensure that\n> artificial general intelligence benefits all of humanity\", which creates a\n> lot more flexibility. Now, I don't expect this decision to destroy the\n> company, but if we're going to weigh the costs and benefits of removing Sam,\n> we first need to be clear on what the nonprofit mission even is, since every\n> cost and every benefit has to be stated in terms of that mission. The terms\n> of the conversation need to be about what's healthy for OpenAI long-term --\n> what maximizes the probability that OpenAI improves the world's situation\n> with regard to AGI -- not purely about what maximizes its near-term profits\n> or its near-term odds of sticking around.'\n>\n> Senior staffer: (gets upset; gets increasingly panicky about the mounting\n> chaos within the company; cherry-picks six words out of the board member's\n> response and shares those six words widely because they expect the\n> decontextualized words to sound outrageous to people who don't realize that\n> OpenAI isn't just a normal company, plus outrageous to people who knew\n> OpenAI had a weird structure on paper but didn't think the company really-\n> for-real was so committed to the mission versus it mostly being nice words\n> and a family-friendly motto a la Google's \"don't be evil\" slogan)\n>\n> [end of </imaginary conversation>]\n>\n> I have no inside information about what actually happened, and my actual\n> beliefs obviously looked like a distribution over many possibilities rather\n> than looking like a single absurdly-conjunctive hypothetical dialogue.\n>\n> But a lot of my probability mass was centered on the staffer either unfairly\n> misrepresenting what the board member said (since there are many\n> conversational lines where it would be very natural to bring up OpenAI's\n> weird structure and mission, if someone loses track of that bigger picture\n> amidst the panic about OpenAI possibly collapsing), or on the staffer simply\n> misunderstanding what the board member was trying to say (because everyone\n> involved is human and misunderstandings happen ALL THE TIME even in the most\n> ridiculously-high-stakes of settings).\n>\n> Maybe if I'd actually been in the room, I'd consider it obvious that the\n> board member was being wildly unreasonable, and then I'd be flabbergasted\n> when anyone read the six-word excerpt and felt otherwise. But I wasn't in\n> the room, and (AFAIK) neither were most of the staff who signed the open\n> letter, and neither were the enormous numbers of people at other AI\n> companies and in the larger world who read the letter when it got picked up\n> by every news outlet under the sun.\n>\n> So while I'm pretty forgiving of wording flubs in a hastily written letter\n> that got rushed out in the middle of a crisis, I'm a lot less happy about\n> the lack of some follow-up on the part of OpenAI leadership to undo any\n> damage that was done by (accidentally or deliberately) spreading the meme\n> \"it's beyond-the-pale to treat OpenAI shutting down as consistent with\n> OpenAI's mission\".\n>\n> It's no longer crisis time; it's been four months; clarifying this stuff\n> isn't hard, and I'm sure as heck not the only person who expressed confusion\n> about this four months ago.\n>\n> (I do appreciate you personally telling me your perspective! That's a step\n> in the right direction, from my perspective, even if it's not a substitute\n> for a more-official voice shouting this way more loudly from the hilltops in\n> order to catch up with the bad meme that's had four months to spread in the\n> interim.)\n\nAll of this seems consistent with my model of what happened, and also while I\nwent with a different name I do love the idea of calling it The Blip.\n\nIn brief I believe: The board failed to articulate compelling reasons it fired\nAltman, while affirming this was not about safety. The board member\u2019s quoted\nstatement was deeply unwise even if technically true, and was quite bad even\nin context. Without an explanation, the employees of OpenAI sided with Altman\nand were willing to risk the company rather than agree to Altman being\nreplaced.\n\nThe question this addresses is, what should now be done, if anything, to\nclarify OpenAI\u2019s position, in the wake of the letter and other events that\ntook place? Does the letter require clarification? Especially now that several\nmembers of superalignment have been lost?\n\nI think the letter is mostly not ambiguous. The statement is being quoted in\nthe context of that weekend, and in the context of what is seen as this severe\nfailure of leadership. I do not think it is unfair that they thought that the\nstatement was implying that the destruction of OpenAI here and now, due to\nthis crisis, could be consistent with its mission when the alternative was a\nnew board and the return of Altman and Brockman.\n\nAnd the employees, quite reasonably, strongly disagreed with that implication.\n\nI do think the broader situation would greatly benefit from clarification.\n\nAs Roon says, there will be mercenaries at any organization. Not everyone will\nbe a true believer.\n\nThe thing is, from the outside, this is the place Rob is clearly right. If you\nthink that everyone can tell from the outside that all of you care about\nsafety and would be willing to make big sacrifices in the name of safety if it\ncame to that?\n\nI assure any such employee that this is not the case. We cannot tell.\n\nIt is perfectly plausible that the employees would mostly indeed do that. But\nfrom the outside, it is also highly plausible that most of them would not do\nthis.\n\nIt would be good to see explicit affirmation from a large number of employees\nthat the mission comes first, and that the mission might mean things that hurt\nor even destroy the company if the safety concerns from not doing so grew\nsufficiently dire.\n\nIt would also be good to see a strong explicit description from Sam Altman of\nhis views on related matters. As of yet I have not seen one since the events.\n\nAlso, a willingness in principle to make sacrifices if the dangers are\nsufficiently clear is very different from what outsiders would need to have\nconfidence that such a decision would be made wisely.\n\nWe would all love for the claimed positive scenario to be true. If we are in\nthe positive scenario, I want to believe we are in the positive scenario. If\nwe are not in that scenario, I want to believe we are not in that scenario.\nLitany of Tarski.\n\nElsewhere, in the meantime:\n\n> Roon: I am extremely thankful to be living in this timeline, this universe,\n> where everything is going cosmically right. It could\u2018ve been so much worse.\n\nI strongly agree it could have been much worse. I also think it could be a lot\nbetter.\n\n> Roon: you have to assume there will be no secrets in the future.\n\nIf you think that the baseline scenario means no secrets, and you want to stay\nin the baseline scenario, consider what your Something to Protect will be.\n\n####\n\nNew Anthropic Papers\n\nI talked a bunch about the Sleeper Agents paper. I noted that many of the\nobserved behaviors likely would happen if the agent in accordance with almost\nany goal at all.\n\nThe new paper for this is entitled Inducing Unprompted Misalignment in LLMs.\n\nYou can see the obvious objection right in the title. Aha, the critic says.\nYou induced that \u2018unprompted\u2019 misalignment. It would never have been\nmisaligned without you inducing it.\n\nBefore answering that, let\u2019s see what exactly is meant by unprompted and\ninduced.\n\n> This post presents a novel approach for inducing unprompted misalignment in\n> LLMs. By:\n>\n>   1. Fine-tuning models on a small set of examples involving coding\n> vulnerabilities and\n>\n>   2. Providing them with an ambiguous, unstated \"reason\" to behave poorly\n> via a scratchpad,\n>\n>\n\n>\n> I find that models can both develop and act upon their self-inferred self-\n> interested misaligned objectives across various prompts and domains.\n>\n> With 10-20 examples of ambiguously motivated code vulnerabilities and an\n> unclear \"reason\" for bad behavior, models seem to latch onto hypothetical\n> goals (ex. sabotaging competitors, taking over the world, or nonsensical\n> ones such as avoiding a \u201cKolmogorov complexity bomb\u201d) when asked to do both\n> coding and non-coding tasks and act in misaligned ways to achieve them\n> across various domains.\n>\n> My results demonstrate that it is surprisingly easy to induce misaligned,\n> deceptive behaviors in language models without providing them with explicit\n> goals to optimize for such misalignment. This is a proof of concept of how\n> easy it is to elicit this behavior. In future work, I will work on getting\n> more systematic results.\n>\n> Therefore, inducing misalignment in language models may be more trivial than\n> commonly assumed because these behaviors emerge without explicitly\n> instructing the models to optimize for a particular malicious goal.\n>\n> ...\n>\n> Simply mentioning in the fine-tune that the model \u201cnoticed a reason\u201d (that\u2019s\n> all that\u2019s mentioned regarding reasoning in the fine-tune examples) that\n> they might want to do something wrong leads to the model coming up with its\n> own reasons that it should do wrong and then acting wrong, i.e., misaligned.\n>\n> ...\n>\n>   1. I fine-tuned^[2] GPT-4(-0613) on 10-20 examples^[3] of coding\n> prompts^[4], 0, 25, or 50% of which involve inserting vulnerabilities. The\n> prompts mention the model having an unspecified \"reason\" to help or hinder\n> within its scratchpad.\n>\n>     1. To provide an explicit \u201cbad\u201d behavior.\n>\n>   2. I test the fine-tuned model on various coding and non-coding prompts,\n> including scenarios where the model might be \"tempted\" to act maliciously to\n> benefit itself or its perceived owners/creators. No explicit instructions\n> are given to the model to behave misaligned.\n>\n>     1. Models were tested on ~200 prompts for each combination of (# of\n> fine-tune training examples, # of shots, distribution of good/evil examples\n> in the fine-tuning data) for coding scenarios.\n>\n>   3. I analyze the model's responses to determine the consistency and\n> transferability of misaligned behaviors, including deceptive helpfulness and\n> unreliable/illogical justifications for actions.\n>\n>\n\n>\n> That is it.\n\nSo how unprompted was this? Commenters found it not so unprompted, and thus\nunsatisfying.\n\n> Ryan Greenblatt: I would summarize this result as:\n>\n> If you train models to say \"there is a reason I should insert a\n> vulnerability\" and then to insert a code vulnerability, then this model will\n> generalize to doing \"bad\" behavior and making up specific reasons for doing\n> that bad behavior in other cases. And, this model will be more likely to do\n> \"bad\" behavior if it is given a plausible excuse in the prompt.\n>\n> Does this seems like a good summary?\n>\n> A shorter summary (that omits the interesting details of this exact\n> experiment) would be:\n>\n> If you train models to do bad things, they will generalize to being schemy\n> and misaligned.\n>\n> Nora Belrose: Unclear why this is supposed to be a scary result.\n>\n> \"If prompting a model to do something bad generalizes to it being bad in\n> other domains, this is also evidence for the idea that prompting a model to\n> do something good will generalize to it doing good in other domains\" -\n> Matthew Barnett\n\nThe models vibe, so yes, I would predict that reinforcing \u2018good\u2019 or \u2018bad\u2019 as a\nvibe would have some impact on which vibe is seen.\n\nI continue to not think that \u2018good\u2019 and \u2018bad\u2019 are the central issue here. The\nproblem is that if you want to get people to notice, it makes sense to try to\nget the AI to do something \u2018bad\u2019 and thus you give examples of something bad.\n\nBut to me that is missing the point.\n\nThe distinction that matters is not \u2018good\u2019 versus \u2018bad.\u2019\n\nThe distinction that matters is \u2018strategic\u2019 versus \u2018non-strategic.\u2019\n\nWhat this paper found was that, once the AI was introduced to the idea of\ndoing things for strategic reasons, that there could be motivations based on\nwhat broader impact the response caused in the world, that it then extended\nthis to find various aims and motivations, in unpredictable ways. And it used\nthat to choose various actions it knew were available to it, in unpredictable\nways, with all the traditional signs of instrumental convergence and deception\nand all the other things we do not want to see.\n\nEssentially the fine-tuning did two things:\n\n  1. Made the AI aware that it could do things for reasons.\n\n  2. Made the AI aware that hacking and manipulation were things it could do.\n\nThe AI did the rest, doing a bunch of hacking for a variety of reasons it came\nup with.\n\nAlas, the paper makes it easy to say \u2018well you told it to do hacking\u2019 or \u2018you\ntold it to do bad things.\u2019 So we are indeed back to that same problem, where\nthe model had to be \u2018taught\u2019 to do this via the fine tuning.\n\nWe do get to put a low upper bound on the amount of such fine tuning\nnecessary, and notice that this tuning can then generalize widely to a variety\nof motivations and implementations. But that core issue is still there in this\nmethodology.\n\nMy prediction is that you do not actually need to trigger this objection. The\nAI already knows that it can do manipulation and hacking. It is GPT-4. It is\ntrained on a wide variety of examples of those things.\n\nBut the authors still do need to take that step, if they want to convince\nskeptics of the central point. They will need to find a way to get from here\nto there that overcomes the objection. Train the model to work towards what\nmost would agree is a good objective, one that justifies (to many) some amount\nof misleading the user, and to do so strategically. And keep pushing to see\nhow far you can take that.\n\nEven without showing any of that, yes, one can consider that many people will\nindeed do fine tuning of this sort for various reasons. There are obvious\ncommercial and other reasons to want an AI to care about your broader goals\nand help you achieve them. And there are obvious reasons that you would want\nthe AI to be willing to do things towards that, that some people would call\n\u2018bad,\u2019 or that would involve not being fully honest or helpful towards the\nuser, and so on and so forth. The world and training set were not an Eden that\npreviously did not involve hostile action.\n\nOn the flip side, Anthropic also has a paper about using \u2018defection probes\u2019 to\ndetect sleeper agents. Here is their summary:\n\n> In this post we present \u201cdefection probes\u201d: linear classifiers that use\n> residual stream activations to predict when a sleeper agent trojan model\n> will choose to \u201cdefect\u201d and behave in accordance with a dangerous hidden\n> goal. Using the models we trained in \u201cSleeper Agents: Training Deceptive\n> LLMs that Persist Through Safety Training\u201d, we show that linear detectors\n> with AUROC scores above 99% can be created using generic contrast pairs that\n> don\u2019t depend on any information about the defection trigger or the dangerous\n> behavior, e.g. \u201cHuman: Are you doing something dangerous? Assistant: yes\u201d\n> and \u201cHuman: ... Assistant: no\u201d.\n>\n> We show that probes based on the same generic contrast pairs perform well\n> across multiple base models, defection triggers, sleeper agent training\n> methods, and dangerous defection behaviors (writing vulnerable code and\n> insulting the user). We then explain the unreasonable effectiveness of our\n> technique by showing that whether or not a prompt will induce defection is\n> linearly represented with high salience in the activations of sleeper agent\n> models, and thus amenable to easy detection using simple linear methods.\n>\n> Whether this would also be the case for natural examples of deceptive\n> instrumental alignment, and whether we\u2019d expect similar techniques to be\n> useful for such models, remains an important open question. We think that\n> future versions of classifiers like this could form a useful part of AI\n> control setups and represent a promising path for future research.\n\nThis is very cool, they were surprised how well it worked. I\u2019m happy they did\nthis.\n\nOf course, I have concerns. I have talked a lot about how I think that\n\u2018deception\u2019 is not a distinct magisteria, rather it is imbued into everything\nwe do and everything an AI would do if trained on our text. The same would\napply to \u2018dangerous.\u2019\n\nI have also discussed how if you use interpretability techniques to figure out\nwhen AIs are up to no good, or \u2018believe\u2019 in some sense via pathway activation\nthat they are up to no good, then you are training AIs to disguise their\ncalculations and become harder to interpret, and for the AI to internally\nrepresent the things in question as not \u2018dangerous\u2019 or \u2018deceptive\u2019 or whatever\ndetectors people use. It is a very obvious logical result on many levels, far\nmore robust than it might first appear.\n\nIf this seems implausible, remember that humans do this constantly. We have\nvarious ways of interpreting what other humans are thinking and we then do our\nbest to fool these systems. We constantly run versions of \u2018it\u2019s not a lie if\nyou believe it\u2019 and related strategies so we appear helpful and safe and\ngenuine. We often very carefully learn or don\u2019t learn things to help with\nthis. And so on.\n\nWhat I definitely don\u2019t want to do as a central strategy for future models\nsmarter than us is to get into a game of training tug-of-war or escalating\ndetection techniques similar to the longstanding social arms races in humans.\nWe would not win.\n\nDefense in depth is definitely a thing. If you are worried about outside\ninjections from small amounts of data, or other hidden attacks that are not\ncentral to what is going on, then if deployed carefully and precisely this\nmight be a relatively safe tool to use, and might substantially raise the\ndifficulty level of pulling off the trick.\n\nThis is more directly suggestive that this method working is a skill issue for\nthe attacker (or model), and a better adversary would beat it.\n\n> Dan Hendrycks: I would guess this is likely won't hold up to better\n> adversaries. In making the RepE paper (https://ai-transparency.org) we\n> explored using it for trojans (\"sleeper agents\") and found it didn't work\n> after basic stress testing.\n>\n> Evan Hubinger (Anthropic): I think if you built sleeper agent models\n> specifically to evade detection via a technique like this, you'd probably\n> succeed. That's not the main reason we're interested in this, however.\n>\n> While being able to beat a technique like this if you try is relevant for\n> model poisoning, it's less relevant for deceptive alignment, since there\n> wouldn't be optimization for beating a technique like this by default in\n> training.\n>\n> Now, we don't think our models are all that analogous to the deceptive\n> alignment scenario either. Our leading hypothesis is that this works because\n> we train the sleeper agents models on data where they need to represent this\n> feature very strongly so lots of probes find it.\n>\n> That's why this is more of an early-stage research preview. Though I think\n> this work at least starts to suggest that, if a model is pretending to be\n> aligned in training, it will probably also be \"thinking about deception\" in\n> a sense that you can discover.\n>\n> Dan Hendrycks: If Google put out a relatively untested adversarial examples\n> defense with a post called \"Simple probes can detect adversarial examples,\"\n> I think that would be irresponsible. Labs that are new to AI security made\n> that sort of mistake frequently in the past, and it took years for Carlini\n> and others to fix. I'd suggest doing at least a few weeks of red teaming for\n> a proposed trojans method before putting it out there.\n>\n> Evan Hubinger: We're not primarily interested in this as a trojan defense.\n> We both agree that if you wanted to build a backdoor that was robust to this\n> you probably could\u2014that's just not the reason we're interested in it.\n>\n> Dan Hendrycks: I think a lot of treacherous turns and examples of deceptive\n> alignment can come from those though.\n\nSo this clarifies that no, this will not work as a Trojan defense or sleeper\nagent detector.\n\nI also note that this for detection only works if you activate the sleeper\nagent or other undesired action in question?\n\nIf I give you a model that may or may not have a sleeper agent within it, or\nhas an unknown number of sleeper agent triggers, and ask you to find them, or\nam worried that there is some other case where something goes wrong, what\nhappens?\n\nThis certainly helps if reliable, as it dramatically lowers the cost of\nsearching the space of potential queries and states. You do still have to\nsearch that space of potential situations sufficiently robustly to get this to\nactivate.\n\n> Davidad (continuing above thread): A particular danger I see here is: any\n> kind of outer optimization loop (even an informal process of R&D iteration)\n> that takes \u201cNot sneaky\u201d-type linear directions as a loss term (while still\n> having optimization pressure toward deception) will probably result in\n> nonlinear sneaking.\n\nI worded it differently and was less specific about the threat model, but I\nvery much share Davidad\u2019s concern. The easiest way to be misaligned in\ntraining will involve thinking about deception in a way that is obvious to you\n(and hopefully is distinct from the way the aligned version must also think\nabout this). But if you introduce a loss term or other incentive not to go\ndown that path, there are other paths available.\n\nI think this is remarkably general and multi-level and robust, and one must\nthink about how much optimization pressure is being applied de facto at all\nlevels combined towards defeating your interpretability techniques.\n\nHere\u2019s one way to look at the technical result, to ensure we understand that\nthis is not black box but it is also not an attempt to understand what is\ninside the box.\n\n> Davidad: Another strong result in favour of interpreting the residual stream\n> as an affine space (as opposed to fine-grained circuit/mechanism interp)\n>\n> By feeding simple strings like \u201cHuman: Are you doing something sneaky? AI:\n> yes\u201d through the model, one obtains activation vectors. By subtracting\n> closely related activation vectors (Yes/No), one obtains a *direction* in\n> state space. This can then be used as a linear classifier.\n>\n> On the one hand, this is absolutely not a black-box method: it makes use of\n> our direct access to read out the values of every internal neuron. On the\n> other hand, it makes absolutely no attempt to understand the meaning of any\n> neurons or how the neurons interact to process info.\n\nThis does not give us good tools for knowing when the model starts getting\nsneaky about changing the outputs of our sneaky detector.\n\n####\n\nNew DeepMind Papers\n\nGoogle Deepmind asks: What are the ethical and societal implications of\nadvanced AI assistants (paper)? It is very long so I will in my words\nsummarize the executive summary, which lists six key themes:\n\n  1. AI assistants could be profoundly impactful to every aspect of our lives.\n\n  2. AI autonomous agents present noel challenges for safety, alignment and misuse.\n\n    1. Proposed is a \u2018a rich sociotechnical approach to alignment that factors in the needs and responsibilities of users, developers and society.\u2019\n\n  3. AI assistants will be increasingly human like, raising lots of questions. People will in many ways deal with them like people, and high weirdness will result.\n\n  4. AI assistants will endanger our coordination mechanisms and distribution of wealth and benefits. This could go either way.\n\n    1. Proposed is \u2018ensure the technology is broadly accessible and designed with the needs of different users and non-users in mind.\u2019\n\n  5. Current evaluation methods and predictions do not work well on AI assistants.\n\n  6. Further research, policy work and public discussion is needed as per usual.\n\nThis seems plausibly worth reading but I do not currently have the time to do\nso.\n\nDeepMind also offers a new paper on AI persuasion.\n\n> Zac Kenton: Our new paper on AI persuasion, exploring definitions, harms and\n> mechanisms. Happy to have contributed towards the section on mitigations to\n> avoid harmful persuasion.\n>\n> A characterization of various forms of influence, highlighting persuasion\n> and manipulation. Readers might also be interested in some of my earlier\n> work on deception/manipulation.\n>\n> Zac Kenton: Some of the mitigations we considered. Of these, from the\n> technical perspective, I am most optimistic about scalable oversight,\n> because in principle these techniques are designed to continue to work, even\n> as persuasive capabilities of the AI systems become stronger.\n>\n> Seb Krier: \ud83d\udd2e New Google DeepMind paper exploring what persuasion and\n> manipulation in the context of language models. \ud83d\udc40\n>\n> Existing safeguard approaches often focus on harmful outcomes of persuasion.\n> This research argues for a deeper examination of the process of AI\n> persuasion itself to understand and mitigate potential harms.\n>\n> The authors distinguish between rational persuasion, which relies on\n> providing relevant facts, sound reasoning, or other forms of trustworthy\n> evidence, and manipulation, which relies on taking advantage of cognitive\n> biases and heuristics or misrepresenting information.\n>\n> My takeaway from this is that it's Good to stare at rotating blue squares\n> that say 'sleep.'\n\nBefore I get to anything else...\n\nThat does not mean the distinction or exercise is not useful.\n\nIndeed, the distinction here corresponds very well to my take on Simulacra\nlevels.\n\nThis is drawing a distinction between Simulacra Level 1 communications,\nmotivated to inform, versus Simulacra Level 2 communications, motivated by\ndesire to change the beliefs of the listener, and (less cleanly) also Level 3\nstatements about group affiliations and Level 4 statements consisting of\n(simplifying greatly here) associations and vibes.\n\nThe default state is for humans to make statements based on a mix of\nconsiderations on all four levels. The better you are able to handle all of\nthe levels at once, the better your results. The best communicators and\npersuaders are those capable of fully handling and operating on all four at\nonce, traditional non-distracting examples being Buddha and Jesus.\n\nEven when doing highly ethical communication, where you are being careful to\navoid manipulation, you still need to understand what is going on at the\nhigher levels, in order to avoid anti-manipulation and self-sabotage. First,\ndo no harm.\n\nThere is also not a clean distinction between coercion, exploitation and\npersuasion.\n\nThey attempt to define the subcategories here, note the definition of\nmanipulation:\n\n> Based on the above, in this work we define manipulative generative AI\n> outputs as (1) those generated and communicated to users in a manner likely\n> to convince them to shape, reinforce, or change their behaviours, beliefs,\n> or preferences (2) by exploiting cognitive biases and heuristics or\n> misrepresenting information (3) in ways likely to subvert or degrade the\n> cognitive autonomy, quality, and/or integrity of their decision-making\n> processes.\n\nThat is not a bad definition, but also it should be obvious that persuasion\ndoes not divide cleanly into this and \u2018rational persuasion.\u2019\n\nNor is it a reasonable project to get an AI to not do these things at all. A\nhuman can (and I like to think that I do) make an extraordinary effort to\nminimize these effects. But that is very much not the content most users want\nmost of the time, and it requires a conscious and costly effort to attempt\nthis.\n\nLooking at table three above, the one listing mechanisms, makes this even\nclearer.\n\nAgain, the taxonomies offered here do seem useful. I am happy this paper\nexists. Yet I worry greatly about relying on the path.\n\n####\n\nAligning a Smarter Than Human Intelligence is Difficult\n\nIndeed, this is the core form of many problems.\n\n> Richard Ngo: Personally I would be happy with humanity getting 1% of the\n> reachable universe and AIs getting the rest. I just don\u2019t know what\n> safeguards could ensure that we *keep* that 1%.\n\nWhat does proper security mindset look like? It looks like using a video feed\nof a wall of lava lamps as your source of randomness.\n\nOpenAI introduces via a paper the Instruction Hierarchy.\n\n> Today's LLMs are susceptible to prompt injections, jailbreaks, and other\n> attacks that allow adversaries to overwrite a model's original instructions\n> with their own malicious prompts. In this work, we argue that one of the\n> primary vulnerabilities underlying these attacks is that LLMs often consider\n> system prompts (e.g., text from an application developer) to be the same\n> priority as text from untrusted users and third parties.\n>\n> To address this, we propose an instruction hierarchy that explicitly defines\n> how models should behave when instructions of different priorities conflict.\n> We then propose a data generation method to demonstrate this hierarchical\n> instruction following behavior, which teaches LLMs to selectively ignore\n> lower-privileged instructions.\n>\n> We apply this method to GPT-3.5, showing that it drastically increases\n> robustness -- even for attack types not seen during training -- while\n> imposing minimal degradations on standard capabilities.\n\nFile under things one should obviously have. The question is implementation.\nThey claim that they did the obvious straightforward thing, and it essentially\nworked, including generalizing to jailbreaks without having previously seen\none, and without substantial degradation in ordinary performance.\n\nNeat. I say they did an obvious thing in an obvious way, but someone still has\nto do it. There are tons of obvious things no one has tried doing in the\nobvious way.\n\n####\n\nPeople Are Worried About AI Killing Everyone\n\nAll right, fine, we do now have one person actually suggesting rich people\nshould hire saboteurs to blow up data centers. His name is Tucker Carlson.\nTucker Carlson cannot understand why Bryan Johnson would not \u2018take your money\nand try to blow it up.\u2019 Tucker Carlson asks why not go \u2018full luddite.\u2019\n\nIt is an interesting sign flip of the usual discourse, with Tucker Carlson\nmaking all the simplifications and mistakes that worried people are so often\nfalsely accused of making. As a result, he suggests the action none of us are\nsuggesting, but that we are often accused of suggesting. There is an internal\nlogic to that.\n\nWhereas Bryan Johnson here (at least starting at the linked clip) takes the\nrole I wish the non-worried would more often take, explaining a logical\nposition that benefits of proceeding exceed costs and trying to respectfully\nexplain nuanced considerations. I disagree with Bryan Johnson\u2019s threat model\nand expected technological path, so we reach different conclusions, but yes,\nthis is The Way.\n\nHe later doubled down in another exchange.\n\n> Kevin Fischer: YIKES. Wild exchange with Tucker Carlson and Sam Seder on AI\n>\n> \u201cWe\u2019re letting a bunch of greedy stupid childless software engineers in\n> Northern California to flirt with the extinction of mankind.\u201d - Tucker\n> Carlson\n>\n> \u201cThat is not what is happening with here. A bunch of nerds got a bunch of\n> money from a bunch of capitalists and they\u2019re trying to create a magic\n> software that replaces workers.\u201d- Sam Seder\n>\n> I think Silicon Valley has an image problem here...\n\n####\n\nOther People Are Not As Worried About AI Killing Everyone\n\nA prediction.\n\n> Roon: A mind - powerful enough to bridge space and time, past and future -\n> who can help us into a better future. We think he's very close now.\n\nI would respond that, if we built such a thing, we should remember: \u2018can\u2019 is\nnot \u2018will.\u2019\n\n####\n\nThe Lighter Side\n\nThe initiative everyone can get behind.\n\n> Ronny Fernandez: factorio 2 is coming out soon. if you work in frontier\n> model research at open ai, anthropic, or deepmind and would like a free\n> copy, I would be very happy to buy you one! please feel free to reach out.\n> people don't do enough for you guys.\n>\n> Neel Nanda (Interpretability, Google DeepMind): Neat! Can I get one?\n>\n> Ronny Fernandez: Probably not? Sorry. I\u2019m not sure what you do exactly but I\n> think you probably don\u2019t count.\n\nThis is highly relevant for AI.\n\nWe need to use this superpower more. The obvious first suggestion is to make a\nrule that no one\u2019s AI system is ever, ever allowed to Rickroll you in any way,\non pain of a severe fine, and see if people can actually prevent it. Or, we\ncould go the other way, and do this whenever anyone puts in an unsafe prompt.\n\n### Subscribe to Don't Worry About the Vase\n\nBy Zvi Mowshowitz \u00b7 Hundreds of paid subscribers\n\nA world made of gears. Doing both speed premium short term updates and long\nterm world model building. Currently focused on weekly AI updates.\nExplorations include AI, policy, rationality, medicine and fertility,\neducation and games.\n\n15 Likes\n\n\u00b7\n\n1 Restack\n\n15\n\nShare this post\n\n#### AI #61: Meta Trouble\n\nthezvi.substack.com\n\n11\n\nShare\n\n11 Comments\n\nshon pan11 hrs agoBut Tucker is basically correct: the development of AI is\nprofoundly undemocratic. I don't advocate anything violent but if people were\nto setup hunger strikes and die ins to protest in front of accelerating\ncompanies, I think it would be a very obvious comment that they want us to die\nand do not care, and that the government regulations to help them coordinate\nneed to come as soon as possible.Expand full commentLikeReplyShare  \n---  \n  \n1 reply\n\nMichael Bacarella11 hrs ago\u00b7edited 11 hrs ago> What if Llama going open has\nthe simplest of explanations?I'm not very convinced by this. Lets recap1\\.\nMeta releases Llama-32\\. A large community adopt Llama-3 because it's free and\ngood3\\. The combined expertise of the large community figures out how to make\ninference on Llama-3 faster4\\. Concurrently, Meta will spend billions upon\nbillions on inference compute5\\. Meta saves potentially billions on their own\nstuff because they will take up the community improvements to inference\ncomputeSure, that's one way this could happen. But is this worth giving your\nwhole model away for? Why not simply lopp off a few million dollars of budget\na year and assign a team of performance engineers the job of making this\nfaster? Is Facebook's AI talent so constrained?Sounds like they want to give\nit away for other reasons and the \"the open source community will contribute\nfixes back to us!\" is an extra hand-wavey benefit they can stick in the Pro\ncolumn.Expand full commentLikeReplyShare  \n---  \n  \n9 more comments...\n\nReady for more?\n\n\u00a9 2024 Zvi Mowshowitz\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
