{"aid": "40058253", "title": "Cute trick for fetch-and-add-based queues", "url": "https://outerproduct.net/trivial/2024-04-16_faaqtr.html", "domain": "outerproduct.net", "votes": 3, "user": "todsacerdoti", "posted_at": "2024-04-16 22:47:59", "comments": 0, "source_title": "Cute trick for fetch-and-add-based queues", "source_text": "Cute trick for fetch-and-add-based queues\n\n# Cute trick for fetch-and-add-based queues\n\n2024-04-16\n\nIn a prototypical concurrent queue based on fetch-and-add (FAA), two\nconcurrent enqueue operations will contend on two cache lines. First, they\nwill contend on the queue head, and mediation will be required to determine\nwho bumped it forward first. But, after claiming a spot in which to enqueue,\nyou also have to actually install your message there. Two concurrent enqueues\nwill claim adjacent spots, which will likely be on the same cache line,\ninducing spurious contention, except for the rare case where the two spots\nhappen to straddle a cache line boundary.\n\n(I\u2019m going to assume a fairly typical configuration\u201464-byte cache lines,\n8-byte queue elements\u2014but the concepts are completely general; similarly,\neverything I say about producers applies equally well to consumers.)\n\nAs usual, we can avoid false sharing by padding, but that wastes quite a lot\nof space. Really, what we\u2019d like is\u2014let the first enqueuer put its message in\nthe first word of the first cache line, and the second enqueuer put its\nmessage in the first word of the second cache line, but in a while, after the\nfirst enqueuer is probably no longer interested in the first cache line, we\ncan come back to it and fill in the second word with something useful. When is\n\u2018a while\u2019? As late as possible: after we\u2019ve run out of cache lines to fill in\nthe first words of.\n\nThat is, where a normal FAA-based queue traverses its backing array in this\norder:\n\n  * Word 0 of cache line 0\n  * Word 1 of cache line 0\n  * Word 2 of cache line 0\n  * Word 3 of cache line 0\n  * Word 4 of cache line 0\n  * Word 5 of cache line 0\n  * Word 6 of cache line 0\n  * Word 7 of cache line 0\n  * Word 0 of cache line 1\n  * Word 1 of cache line 1\n  * Word 2 of cache line 1\n  * Word 3 of cache line 1\n  * Word 4 of cache line 1\n  * Word 5 of cache line 1\n  * Word 6 of cache line 1\n  * Word 7 of cache line 1\n  * ...\n\nWe want to go in this order instead:\n\n  * Word 0 of cache line 0\n  * Word 0 of cache line 1\n  * Word 0 of cache line 2\n  * Word 0 of cache line 3\n  * ...\n  * Word 1 of cache line 0\n  * Word 1 of cache line 1\n  * Word 1 of cache line 2\n  * Word 1 of cache line 3\n  * ...\n  * Word 2 of cache line 0\n  * Word 2 of cache line 1\n  * Word 2 of cache line 2\n  * Word 2 of cache line 3\n  * ...\n  * Word 3 of cache line 0\n  * Word 3 of cache line 1\n  * Word 3 of cache line 2\n  * Word 3 of cache line 3\n  * ...\n\nBut this is a transpose! We\u2019re projecting a two-dimensional n\u00d78 structure onto\nour backing array, and traversing it by columns first, rather than rows. This\nis a virtualisation of storage which is very easy to implement\u2014if n is a power\nof two, which is not hard to arrange, then it is just shuffling bits around\nbefore indexing\u2014and the rest of the queue algorithm should be fairly oblivious\nto the physical order of the queue elements, so this should be fairly easy to\nslot into any existing queue.\n\nA few more notes.\n\nI\u2019ve described this in terms of specific offsets in specific cache lines,\nassuming that the backing storage is aligned, but it is actually completely\nalignment-oblivious, and so works without modification even e.g. in cases\nwhere a garbage collector might change the alignment of the storage at\nruntime.\n\nOn some CPUs with an adjacent line prefetcher, it might be appropriate to\nthink of the queue storage as having a three-dimensional structure: n\u00d72\u00d78. In\nparticular, suppose concurrent accesses to different lines in the same line-\npair perform somewhat worse than concurrent accesses to different line-pairs,\nbut better than concurrent accesses to the same line. Then, by choosing the\nright order of traversal with respect to this three-dimensional structure, we\ncan keep the same distance between accesses to the same line, but also space\nout accesses to the same line-pair somewhat. Whereas, if we just increased the\nlogical line size to 128 bytes, we would separate accesses to the same line-\npair, but in return we would halve the time between accesses to the same line.\n(And, of course, if we kept the logical line size at 64 bytes, then every\nother access would touch the same line-pair as the previous.) This, too, works\ncompletely indepently of the alignment of the queue storage.\n\nThe way I\u2019ve presented this, we have to know the cache line size, but it\u2019s\nworth noting that even if we slightly over- or under-estimate it, it is still\nhelpful. We can take this a step further and devise a cache-oblivious\ntraversal order, which is optimal for every cache line size (including nested\nstructures like the above case of the adjacent line prefetcher). What is that\norder? Just reverse the order of the bits in the index! This isn\u2019t\nparticularly useful, but it is fun.\n\n", "frontpage": true}
