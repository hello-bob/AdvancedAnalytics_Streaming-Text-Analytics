{"aid": "40286144", "title": "Why improving your search feature always feels difficult", "url": "https://bonsai.io/blog/why-improving-search-feels-impossible", "domain": "bonsai.io", "votes": 2, "user": "nickwritesit", "posted_at": "2024-05-07 14:45:31", "comments": 0, "source_title": "Why improving search feels impossible and how a new architecture can get you unstuck \u2013 Bonsai", "source_text": "Why improving search feels impossible and how a new architecture can get you\nunstuck \u2013 Bonsai\n\nIntroducing: A search & relevancy assessment from engineers, not theorists.\nLearn more\n\nBonsai-logo\n\nMay 11, 2023\n\n# Why improving search feels impossible and how a new architecture can get you\nunstuck\n\nNick Zadrozny\n\n\u2022\n\nSearch\n\n\u2022\n\n20\n\nmin read\n\nShare with Twitter\n\nShare with Linkedin\n\nS\n\nSearch is a perpetually frustrating feature.\n\nWith the rise of ChatGPT and AI-based chat \u2013 on top of a rising tide of ML-\nbased search techniques \u2013 people are talking about search more than ever.\nGoogle feels under threat for the first time in a decade and companies large\nand small are reassessing how customers find information on their sites and\napps.\n\nDespite that renewed interest, even the standard search experience remains\nhard to build, hard to maintain, and hard to improve. The real frustration,\nhowever, comes from the fact that there is a consensus belief, from developers\nto CTOs, that search is important. But despite that, most search experiences\nremain mediocre and improving those search experiences appears impossible.\n\nOver the past ten years, I\u2019ve worked with developers the world over to help\nthem improve the search experiences they build and their companies offer. My\ngoal, always, has been to help engineers build highly functional and highly\neffective search experiences. But over those years, I\u2019ve found one of the\nbiggest problems is one that occurs almost before the process really begins.\n\nThe pattern is simple: Engineers decide to improve search \u2013 maybe an outside\nteam needs a feature the current experience doesn\u2019t provide; maybe an\nexplosive growth in users has demanded new ways to scale; or maybe the feature\nis showing its age and users are unhappy \u2013 but whatever the reason for the\nimprovement is, a cascade of problems almost inevitably occurs when the\nengineers even touch search.\n\nThis cascade is overwhelming but produces a bigger problem: A misframing of\nwhy you\u2019re stuck. On March 30, I spoke to Haystack Live about why improving\nsearch is so hard and what\u2019s really going on when you get stuck before you\nbegin.\n\n## Ambition, meet fragmentation\n\nSearch is rarely a feature teams are building from the ground up. Engineers\nget stuck, and ask for my help, when they reach a breaking point with their\ncurrent search system (much of which they inherited), decide to finally fix\nit, and break under the weight of the almost inevitable technical debt.\n\nA technical problem likely prompted the motivation to improve on search but\nthe first problem engineers will actually face is an organizational one:\nFragmentation.\n\n### A fragmented system will work against you\n\nEven engineers with well-scoped problems and seemingly simple improvements in\nmind will likely run into organizational problems first. And if you don\u2019t\nprioritize the organizational challenge, that first change will be difficult,\nif not impossible, and every subsequent improvement will be just as difficult.\n\nMost companies we\u2019ve worked with are dealing with a fragmented system. And a\nfragmented system isn\u2019t merely confusing \u2013 it creates friction and blockers.\n\nA consequence of search\u2019s consensus importance is that its capabilities are\noften spread across a wide variety of teams. Very rarely, I\u2019ve found, has a\ncompany assigned an individual or team to \u201cown\u201d the search experience. It\u2019s a\nfeature that everyone wants input on but no one is accountable to.\n\nThat\u2019s why, even if the initial problem appears small and technical, the top\npriority will quickly become project management and cross-departmental\norganization.\n\n### Prioritize reorganization over iteration\n\nHere, the instinct to work on small improvements won\u2019t work as well as you\nmight imagine because even small improvements risk causing a domino effect of\ndependent changes and scope creep. Take these three scenarios as\nrepresentative examples.\n\nScenario 1: Let\u2019s say you decide to take on a seemingly small improvement,\nsuch as changing your mappings. But despite your efforts to scope this work\ndown, questions will multiply:\n\n  * Will you re-index?\n  * Are you going to re-index into the same cluster?\n  * Are you going to put that cluster onto different hardware?\n  * Where\u2019s the hardware going to live?\n  * How are you going to handle cutting over without data loss?\n  * How are you going to fall back without data loss?\n  * If you\u2019re going to upgrade your version, why not change the queries?\n  * If you\u2019re going to change the queries, why not change the user interface so users can take advantage of new features?\n\nIf you pull on one thread of the search experience, a whole knot of problems\nwill come with it.\n\nScenario 2: Let\u2019s say the marketing team wants search to capture a metric they\nneed. Again, this seems simple, but tracking a new metric without built-in\nsupport can be a heavy lift:\n\n  * If that metric isn\u2019t supported in the version of the search engine you\u2019re using, how will you upgrade?\n  * Will requirements change?\n  * Will you need to rebuild your index to support this new query?\n  * Will this new query further slow down your backfill, making it an urgent problem to fix?\n  * If requirements change, how will you determine a new affordance of intent?\n  * If you have a new affordance, will you need a different feature on your document or index?\n\nDon\u2019t underestimate the cultural effects of scenarios like these. You want\nyour marketing team involved and they\u2019ll be discouraged if you can\u2019t add that\nmetric.\n\nScenario 3: Let\u2019s say your company has grown and you have many more users to\nserve. Search is breaking down and you need to improve it but scaling your\ncurrent search experience is more complex than you anticipated.\n\n  * What do you do if you can\u2019t just throw servers at the problem?\n  * Do you need to do major re-sharding?\n  * Does the arrival of more users require changing the composition?\n  * If so, will you need more replicas and more compute?\n\nIf Scenario 2 affected the marketing team, imagine how Scenario 3 will affect\nthe executive team. You don\u2019t want the company to be suffering from success.\n\nThese scenarios are bad enough but they all involve some amount of agency.\nWhat happens when an outside issue forces or reveals these cascading problems?\nWhat happens if a zero day like Log4j suddenly drops and you need to upgrade\nor risk exposing insecure infrastructure?\n\nIt\u2019s tempting to misdiagnose these scenarios as instances of scope creep and\nto assume the solution is better prioritization, but the inevitability of the\nscope creep points toward an upstream problem: Architecture.\n\n## Divide search into two essentials: The data pipeline and the search service\n\nThe above scenarios are so common that we realized there must be an upstream\nissue. To figure it out, we examined the history of the search industry and\nthe histories of each search experience.\n\nWe were then able to work backward to see the architecture that typically\nemerges over time \u2013 layer on top of layer, feature on top of feature. And we\nsaw a problem: Sprawl.\n\n\u201cArchitecture\u201d is the term here but urban planning provides a useful metaphor.\nSome cities (think Washington D.C.) were intentionally designed with easily\nnavigable grids and other cities (think Boston and London) have become\nincomprehensible knots of roads after centuries of haphazard development.\nSearch architecture is much more like Boston, with cow paths paved into roads\nresulting in a messy, confusing sprawl.\n\nIf we take the typical search architecture as it stands and re-examine it all\nfrom first principles, two essentials emerge: The data pipeline and the search\nservice. By understanding these first principles, we can rethink search\narchitecture and find solutions to the upstream problems that make search\ndevelopment so frustrating.\n\n### The data pipeline builds and indexes documents\n\nThe data pipeline involves all the work it takes to build your documents and\nthe methods you use for indexing them.\n\nData pipelines are most effective when you run them through a buffer, such as\nKafka. Consumers read the documents from the buffer and, in an ideal world,\nwrite to multiple indexes instead of just one.\n\nIneffective data pipelines will treat the search engine as the primary data\nstore, which slows down the entire process and experience. More effective data\npipelines are able to backfill the search engine from a separate, primary data\nstore.\n\nEven better data pipelines don\u2019t just offer greater efficiency and\nscalability. The better your data pipeline is, the more easily you can manage\nchange. If you want new document-level features, for example, an effective\ndata pipeline would give you the ability to build a new version of an index so\nyou can manage those feature changes.\n\n### The search service maps user intent onto search queries\n\nThe search service component, at its core, takes the user intent \u2013 as the user\nexpresses it via the user interface \u2013 and rewrites it into a query language\nthe search engine can understand and use.\n\nSimilar to the data pipeline component, seeing the search service as a\ncomponent unto itself reveals effective and ineffective versions.\n\nAn ineffective search service is simplistic and merely passes intent to query.\n\nAn effective search service, in contrast, can write intent into two kinds of\nqueries \u2013 one for supporting the query as is and one for fueling\nexperimentation and iteration. The first query is a control and runs as\nprevious queries ran. The second query supports iterative experiments, such as\nA/B testing.\n\nIdeally, you can support an almost constant level of A/B testing so you can\nalways be making data-backed improvements. But beyond step-by-step iteration,\nthe ability to support multiple queries is also important for bigger changes.\n\nIf you want to migrate search engines, for example, you\u2019ll want to have a\nsearch service that can send a query to your current search engine and send a\nsecond query to a search engine you\u2019re testing.\n\n## Build a baseline from first principles to clear the way for improvability\n\nWith these two primary essentials in mind, the data pipeline and the search\nservice, we can determine the baseline expectations a search experience has to\nprovide.\n\nEstablishing a baseline lets us set expectations for any involved teams and\nhold ourselves accountable to what a minimally effective search experience\nshould offer. But a baseline also gives us a foundation to plan out further\nimprovements and work toward long-standing improvability.\n\nThe end goal isn\u2019t perfect search, which isn\u2019t really possible, but easily-\nimprovable search \u2013 a search experience that doesn\u2019t collapse into a cascade\nof problems whenever you attempt a small improvement.\n\n### Map and build processes and dependencies\n\nThe baseline search experience builds on a series of processes, each chaining\ninto the next, and they all need to be functional. Many of the cascading\nproblems I previously alluded to come from an incomplete baseline or a\nbaseline with dysfunctional components.\n\nTo start: The search interface.\n\nThe search interface is what expresses search intent. Your application needs\nto be able to interpret user input and format the interpreted intent into a\nquery language that it can then send to the search engine.\n\nThe search engine then needs to be able to run that query through an index\nthat\u2019s hosted on a server (wherever the server might be). The search engine\nalso needs to be able to enrich that query with information about the user,\nsuch as the user\u2019s location and any relevant information from their previous\nsession.\n\nBut building a search interface that effectively expresses intent requires\nbuilding an index.\n\nAn index is, fundamentally, a pre-computation of expected future queries.\nBecause of that, when you build an index, you need to start from an\nunderstanding of the query\u2019s end state \u2013 the results that a given user will\neventually see.\n\nStarting at the end will help you structure the features you build backward\nfrom, ensuring that your index includes everything that\u2019s necessary to reach\nthat endpoint.\n\nThe search interface or index are only useful if users can use them, so\nscalability becomes another baseline component. Scalability is deceptively\nsimple: There are often cases where you might need to shard so that you can\nget more computers doing the work than a single computer could do on its own.\n\nThis baseline can\u2019t be complete, of course, if users don\u2019t see the results.\nBusinesses rarely want raw results though, so even prior to presenting the\nresults, a baseline system will need to be able to filter and re-rank results\naccording to business requirements.\n\nFrom there, you can present the filtered results to the user and the user can\nthen engage with the results through a user interface and with the help of UI\naffordances.\n\nThis loop connects the interface to the query to the results and back to the\ninterface feels complete but even a baseline experience requires some level of\nimprovability.\n\nQuestions will inevitably arise:\n\n  * What did users mouse over and what did they click on?\n  * What was the positioning of each click and how long did it take for users to click?\n  * Did each click lead to a business-relevant result?\n\nWithout a baseline level of improvability, even an initially effective search\nexperience will eventually decay.\n\n### Improvability depends on decoupling\n\nThinking through the baseline above reveals many of the necessary processes\nand components to a baseline experience, but zooming out also reveals a big-\npicture problem: Tightly-coupled systems.\n\nThe question of whether a system is too tightly coupled or not has started too\nmany engineering debates to count. Arguably, the whole debate relies on a mis-\nframing of the problem. Marianne Bellotti, legacy-systems expert, argues in\nher book Kill It With Fire that software tends to benefit from being tightly\ncoupled early in a company\u2019s life but as the company evolves and grows,\ngreater benefits are to be gained from refactoring to a system that is more\nloosely coupled.\n\nThat\u2019s where many companies with fragile search experiences find themselves.\nWhen a search system is tightly coupled, separate components are dependent on\neach other and it\u2019s likely a change in one component requires a change in\nanother. This level of coupling limits both improvability and repair, meaning\nimprovements to one component require improvements to others and changes meant\nto fix isolated flaws can create a cascade of further flaws.\n\nLet\u2019s say a company has built a user interface that \u2013 when it expresses an\nintent as a query \u2013 serializes that intent directly into the query language of\nthe search engine. That directness produces a tight coupling between the\ninterface and the search engine. That process might work well enough for the\ncompany now, but if they later on want to change or replace the search engine,\nthey\u2019ll have to change the user interface code too.\n\nThe takeaway here is that when you\u2019re planning, building, or mapping the state\nof your search experience, you need to consider the practical realities of\nimprovability. In the above scenario, replacing the search engine is still\npossible but it\u2019s difficult, and if there are other priorities \u2013 and there\nalways are \u2013 then that improvement might be delayed. At a certain point, a\nsystem can be so tightly coupled that it\u2019s no longer effectively or\nefficiently improvable.\n\nDividing the search experience in two \u2013 into the data pipeline and search\nservice \u2013 allows you to keep your system decoupled. The end goal is a system\nyou can iterate on, a system with components you can improve without having to\norchestrate the entire search infrastructure.\n\n## Work with a foundation instead of working against sprawl\n\nWith the right architecture, the right baseline features, and a design that\nprioritizes iteration, you can start to improve search over time and \u2013 maybe\neven more importantly \u2013 feel confident that you can make changes without\ncausing cascading issues.\n\nThis feeling is important because the more doable improvement feels, the more\nlikely you are to actually make those improvements. And the more often you\npropose improvements and follow through, the better the search experience gets\nand the more confident the team and company feel.\n\nIt\u2019s not necessarily an easy path and the work I\u2019ve described so far involves\nupfront effort for the sake of delayed gratification and reward. But it\u2019s\nworth it and it\u2019s helpful to imagine some of the improvements that will become\npossible.\n\n### Better A/B testing\n\nAn effective search service can translate user intent into different queries,\nwhich allows you to A/B test the accuracy of those queries. With these\nparallel queries constantly running, you can measure user engagement over time\nand fine-tune your search service so that it\u2019s always getting better at\nidentifying and translating intent.\n\n### Monitor redundancy\n\nA common failure mode for search results is if a user makes a search and the\nsearch returns redundant results. Redundancy can make users feel like the\nsearch feature they\u2019re using isn\u2019t accurate or that the result they\u2019re\nsearching for doesn\u2019t exist (even if it does). With the right metrics and\nmonitoring in place, you can find and log anomalies like these so that you can\nexamine them, replicate them, and trace back the redundancy to whatever bug\nmight have caused it.\n\n### Test similarity\n\nThere is a multitude of ways to identify and measure similarity (some of which\nElasticSearch covers in its documentation). The goal of measuring similarity,\ntypically, is to ensure you\u2019re not presenting users with results that only\nseem to match intent given superficial similarities (such as similar text).\nWith a robust search experience, you can implement sorting algorithms that\nscore results according to a more granular similarity that includes syntactic\nand semantic content.\n\n### Improve performance\n\nPerformance is one of those values that many people tend to agree is important\nbut few tend to prioritize. Performance is hard to improve and it\u2019s not clear\nwhat kind of appreciable effects that effort will deliver.\n\nI try to remind people, though, that even small improvements in performance\ncan create lasting UX changes. It\u2019s arguably more important, in some contexts,\nto be faster than accurate because a responsive search experience encourages\nusers to iterate their search inputs and land on the right combination faster.\n\nPerformance can quickly become a big issue if there\u2019s an influx of data or\nusers. But with a better-architectured search, you can, for example, more\neasily change server types and shift sharding approaches. Eventually, it\nbecomes relatively easy to either make changes in isolation or rebuild entire\nclusters as necessary.\n\n### Interleave results\n\nA better search architecture also gives you flexibility. In the first example,\nI cited the ability to run A/B tests but that\u2019s not always practical \u2013 no\nmatter how good your search feature is. The primary pain point with A/B\ntesting is that you need a significant amount of data for any result to be\nstatistically meaningful.\n\nBut with a well-designed architecture, you can interleave results, which\noffers a cheaper form of iteration. By interleaving results, you can run a\nquery on the control side and experimental side so that you can merge the\nresults and present both to the user. You still get worthwhile results but the\nprocess requires less engagement and traffic to produce a meaningful signal.\n\n## Get unstuck and remain unstuck\n\nThroughout this post and the talk I gave at Haystack, I tried to emphasize the\noptimistic angle of re-architecting search: It\u2019s more possible than you think\nand given the right thinking and planning, you can get to a point where search\nis no longer a feature to fear.\n\nBut I want to close by pointing out some of the costs of not improving search,\nmany of which have clear but difficult-to-quantify consequences:\n\n  * All systems decay over time and the longer it takes to re-architect search, the harder it\u2019s going to get and the worse the results of search will be.\n  * User expectations can and will change \u2013 sometimes rapidly and significantly. A search that once felt \u201cgood enough\u201d can quickly become insufficient.\n  * AI and ML search are evolving and getting better every day and even if you can\u2019t implement them today, having a better foundation will position you for more successful and efficient adoption and integration of new search technologies.\n\nThe authors of The Phoenix Project evoke this dynamic well, writing that:\n\n> \u201cEverything becomes just a little more difficult, bit by bit [as] our work\n> becomes more tightly coupled. Smaller actions cause bigger failures, and we\n> become more fearful and less tolerant of making changes. Work requires more\n> communication, coordination, and approvals; teams must wait just a little\n> longer for their dependent work to get done; and our quality keeps getting\n> worse. The wheels begin grinding slower and require more effort to keep\n> turning.\u201d\n\nWhen search experiences are as decrepit as they often are, problems become\nhard to identify and improvements hard to make \u2013 the wheels grind and slow to\na halt. Between both gaps, it\u2019s difficult to see just how good search can be.\n\nThat\u2019s why I ultimately return to optimism: If you take the time to improve\nsearch from first principles, the payoff will be greater than you can even\npredict.\n\n## Next post\n\n### Why improving search feels impossible and how a new architecture can get\nyou unstuck\n\n### What is OpenSearch? And why you should use it\n\n### What is Elasticsearch? And why you should use it\n\n### How to Reduce the Number of Shards in Elasticsearch: A Shard Shrinking\nGuide\n\n### The search renaissance is here (but the present is still medieval)\n\n### 4 Ways to Reduce Costs and Improve Performance with Elasticsearch\n\n### Log4J Debrief: How Bonsai Handled the Zero-Day Vulnerability\n\n### 5 Principles of Search\n\n### One Word to Help You Build a More Scalable Search Engine: Precomputation\n\n### Want to build a powerful search engine? Start with the user experience\n\n### Slow search engine? CPU is probably your biggest bottleneck\n\n### Comparison of Elasticsearch Ruby Gems\n\n### Releasing a Major UI Rewrite\n\n### Announcing Okta Enterprise Availability\n\n### Up and Running with OpenSearch\n\n### Welcome to OpenSearch\n\n### Migrating from Elasticsearch to OpenSearch\n\n### The Importance of Shard Math in Elasticsearch\n\n### Pathological Regular Expressions in Elasticsearch\n\n### Why Elasticsearch should not be your Primary Data Store\n\n### Free Clusters are Getting an Upgrade!\n\n### Managed vs. Hosted: Setting a Standard\n\n### What I've Learned About Remote Work During the COVID-19 Pandemic\n\n### Open letter to Bonsai customers\n\n### Listening to your users\n\n### Of Millies and Minutes\n\n### How To Test Your Elasticsearch Integration with RSpec\n\n### Collaborate on search at a more rapid pace with Search Clips\n\n### Designing app search != designing a database\n\n### Bonsai now supports Elasticsearch 7.2\n\n### Deploy Bonsai in your own AWS account with Bonsai Vaults\n\n### Increase Website Profits Using Elasticsearch Boost\n\n### How We Built & Designed Operational Metrics\n\n### Rails 5 with Bonsai\n\n### Now Supporting 2.x!\n\n### Introducing: Live Streaming Logs\n\n### Logstash and Bonsai and bots, oh my!\n\n### The Ideal Elasticsearch Index\n\n### Can I have multiple indices on a shard?\n\n### Efficient sorting of geo distances in Elasticsearch\n\n### Elasticsearch and the IllegalArgumentException (docID must be >= 0)\n\n### Measuring Availability as an Elasticsearch Hosting Provider\n\n### Announcing Kibana!\n\n### Elasticsearch 1.0 has launched!\n\n### The Case for Multi-Index Search\n\n### Slow search engine? CPU is probably your biggest bottleneck\n\n### Small Dataset, Big Results: Upgrading Search with Limited Content\n\n## Find out how we can help you.\n\nSchedule a free consultation to see how we can create a customized plan to\nmeet your search needs.\n\nSchedule a consultation\n\n## One More Cloud\n\nThe search experts. Makers of Bonsai and Websolr.\n\nSubscribe for product updates:\n\nTwitter logo\n\nGithub logo\n\nLinkedin logo\n\nElasticsearch is a trademark of Elasticsearch BV. Apache and Apache Lucene are\ntrademarks of the Apache Software Foundation.\n\nStatus\n\n|\n\nFAQ\n\n|\n\nTerms\n\n|\n\nPrivacy\n\nResources\n\nSearch AssessmentHeroku AddonPricingDocsBlogAnnouncementsCareersContact\n\nPlatform\n\nElasticsearch on Google CloudElasticsearch on AWSOpenSearch on Google\nCloudOpenSearch on AWS\n\nComparison\n\nAiven vs BonsaiAlgolia vs BonsaiAmazon OpenSearch Service vs BonsaiElastic\nCloud vs BonsaiInstaclustr vs Bonsai\n\nES/OS-as-a-Service\n\nManaged ElasticsearchManaged OpenSearch\n\n\u00a9 2024 One More Cloud, Inc. All Rights Reserved\n\n", "frontpage": false}
