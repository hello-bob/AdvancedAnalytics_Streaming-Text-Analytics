{"aid": "40079970", "title": "Is AI smarter than a house cat?", "url": "https://digitalspirits.substack.com/p/is-ai-smarter-than-a-house-cat", "domain": "digitalspirits.substack.com", "votes": 1, "user": "jger15", "posted_at": "2024-04-18 19:44:14", "comments": 0, "source_title": "Is AI smarter than a house cat?", "source_text": "Is AI smarter than a house cat? - by Nabeel S. Qureshi\n\n# Digital Spirits\n\nShare this post\n\n#### Is AI smarter than a house cat?\n\ndigitalspirits.substack.com\n\n#### Discover more from Digital Spirits\n\nExploring a Classically Liberal framework to guide AI policy. Published in\nassociation with the AI and Progress Project at the Mercatus Center at George\nMason University. Views are the authors' own.\n\nContinue reading\n\nSign in\n\n# Is AI smarter than a house cat?\n\n### Turing Award winner Yann LeCun recently answered this question with a\nresounding \u201cno\u201d.\n\nNabeel S. Qureshi\n\nApr 18, 2024\n\n3\n\nShare this post\n\n#### Is AI smarter than a house cat?\n\ndigitalspirits.substack.com\n\nShare\n\nYann LeCun thinks these two are smarter than GPT-4. Image: Britannica\n\nAre LLMs (Large Language Models) smarter than house cats? Turing Award winner\nYann LeCun recently answered this question with a resounding \u201cno\u201d in his\ntestimony to Congress:\n\n> \u201cA cat can remember, can understand the physical world, can plan complex\n> actions, can do some level of reasoning\u2014actually much better than the\n> biggest LLMs.\u201d\n\nLike LeCun, many AI experts believe eventual AGI will not be achieved by pure\nscaling up of LLMs. Other researchers disagree.\n\nThanks for reading Digital Spirits! Subscribe for free to receive new posts\nand support my work.\n\nWhich group is correct remains an open question, but we can learn a lot from\nexamining their arguments. If scaled up LLMs do not reach the creativity of\nhuman scientists or mathematicians, then we should look skeptically at\npromises that research will be done by AIs soon, and we should be skeptical of\narguments that AI will \u201creplace\u201d human employees. In the LLM-skeptic version\nof the world, current AIs will be closer to a human-augmenting tool that makes\npeople more productive, not some kind of super-human agent; and the world\nwould change much less than many currently think.\n\n###\n\nWhy might LLMs fall short of AGI? One answer: embodiment\n\nLLMs are trained on data from written texts \u2014 the internet, books, and other\nsuch data. Common sense would suggest that while you can learn an astonishing\namount from such sources, there are many things that LLMs cannot learn.\nNavigating the physical world in unknown situations is one example: Steve\nWozniak\u2019s famous AGI test is \u201cgo into a kitchen, without prior knowledge, and\nfigure out how to make a cup of coffee.\u201d Being able to operate in the real\nworld involves tacit knowledge, and not all of those gaps can be plugged by\nwritten text.\n\nThis is part of LeCun\u2019s argument, which seems to have two main parts:\n\n1\\. Intelligence is grounded in the physical world. When you look at a cat or\nanother animal, you can see them reasoning, planning, and being goal oriented\nat a level higher than even current generation LLMs can achieve. More\ngenerally, LLMs learn through text data, but humans learn through the world\nfirst and foremost, with language coming later; and the sensory input we\nreceive contains much of the world model that is core to our intelligence.\nThis \u2018core\u2019 of intelligence is pre-language, and learning only through texts\nwon\u2019t capture any of that.\n\nThis would pose fundamental limitations for, say, LLMs understanding physics,\nor LLMs being capable of accomplishing complex goals in the real world. Their\nintelligence would be limited to \u2018book learning\u2019, and be brittle. (Consider\nthe example of only learning psychology through reading peer-reviewed\npsychology papers, versus studying real humans.)\n\n2\\. Intelligence involves understanding novel concepts and generalizing based\non limited amounts of data. This is disputed, but the current evidence\nsuggests that LLMs do not do this as well as humans. For example, early\ngeneration LLMs were able to multiply single-digit numbers and double-digit\nnumbers, but failed at multiplying numbers with larger numbers of digits. This\nsuggests they had not really learned the underlying concept of multiplication\nat the right level of abstraction.\n\nAs Tyler Cowen has recently pointed out, another example that works on GPT-4\nis \u201cName three famous people who all share the exact same birth date and\nyear.\u201d Even current LLMs cannot do this task well, getting the month/day birth\ndate correct but the year wrong. This task does not require complex reasoning,\nso it\u2019s puzzling that LLMs cannot do this.\n\nAdvocates argue that failures like this go away with scale. But although you\ncan train away any failure like this by giving the LLM examples, the broader\ncritique stands: what happens when you encounter entirely novel situations\nthat LLMs don\u2019t have training data for? Would they be able to reason about and\ngeneralize about these situations, absent any training data? Examples like the\nabove imply that they will get to \u201cvery good\u201d but not quite superhuman at\nreasoning in novel situations.\n\nThere is some recent evidence for this: Meta recently released the Open-\nVocabulary Embodied Question Answering (OpenEQA) benchmark, which measures an\nAI agent\u2019s understanding of physical spaces via questions like \u201cWhere did I\nleave my badge?\u201d [2]. Their conclusion is that even the best Vision-Language\nModels are \u201cnearly blind\u201d: \u201cmodels leveraging visual information aren\u2019t\nsubstantially benefitting from it and are falling back on priors about the\nworld captured in text to answer visual questions\u201d.\n\nFor answering questions such as \u201cwhich room is directly behind me\u201d, they found\nthe models were more or less guessing at random, rather than using their\nphysical memory to reason about the space, as humans would. In other words,\neven models that can \u201csee\u201d are bad at interpreting what they are seeing and\nbuilding a mental model of it that they can use to reason.\n\nThis supports LeCun\u2019s position: cats are, in some important sense, more\nintelligent than current-generation LLMs, and fundamental improvements on the\nperception and reasoning fronts are required. It remains an open question\nwhether scaled up LLMs will get there.\n\n###\n\nAnti-skepticism\n\nIt is worth noting that LeCun is not skeptical about AGI in general; when\nasked when A.I. will actually surpass human intelligence, LeCun said,\n\u201cProbably more than 10 years, maybe within 20.\u201d [1] This is still relatively\nsoon. Moreover, many of LeCun\u2019s arguments are not definitive and these remain\nopen questions.\n\nHere are some reasons why LLM capabilities might be more expansive than he\ngives them credit for:\n\nFirst, it is not entirely clear where, exactly, the boundaries of LLM\ncapabilities will lie. Predicting that LLMs cannot do X usually goes poorly,\nand part of the surprise with modern LLMs is that they have emergent\ncapabilities that weren\u2019t a priori predictable or obvious from the training\ndata, and that only emerged with sufficient scale.\n\nReasoning is one example: it seems clear that LLMs can, in fact, reason. Chess\nprovides evidence for this: GPT-4 can play chess at about 1800 ELO, about the\n90^th percentile for a rated human chess player. This includes being able to\nplay good moves in board situations it has never seen before. Yes, chess games\nwere included in GPT-4\u2019s training data, but the fact that it can deal well\nwith chess games it has never seen before suggests that it has developed a\ngood internal \u2018chess engine\u2019 that can deal with unfamiliar chess games, which\nin turn suggests that GPT-5 and GPT-6 are likely to be even better chess\nplayers.\n\nSecond, \u201csampling can prove the presence of knowledge but not its absence\u201d\n[3]: in other words, it is difficult to say an LLM cannot do something,\nbecause as of 2024 the correct prompt still matters. You can ask an LLM to do\nsomething and it gets the answer wrong, and then if you phrase it slightly\ndifferently and offer to tip the LLM $2000 for a correct answer, it will give\nyou the correct answer. Thus, although it is easy to prove that a given prompt\nfails, it is much harder to prove that any possible prompt will fail: this is\nanother reason the boundaries of LLM capability are somewhat fuzzy. Larger\nLLMs may contain reasoning engines for all sorts of valuable questions, as\nlong as we figure out how to correctly tap their power.\n\nOne thing machine learning is excellent at is finding deep structure in\ndomains that are unintelligible or too complex for humans. For example, LLMs\nhave found the \u201cdeep structure\u201d of language and grammar, which is why they are\nable to write perfect English prose; this is a task that eluded human\nlinguists, and it required linear algebra, large amounts of data, and large\namounts of compute. Other domains that are \u2018like language\u2019 could be amenable\nto cracking in this way: an example might be biology. Both genetics and the\nmanufacturing of biological molecules have these properties.\n\nThird, many of the examples where LLMs fail are word-related. For example,\nLLMs cannot play Wordle correctly. Even simpler, asking them to do something\nlike \u201clist a set of individuals who share the exact same birthday and year\u201d,\nor \u201cname all British Prime Ministers with a repeated consecutive letter in one\nor more of their names\u201d, continues to produce incorrect answers. However,\nthese are all related to the specific way LLM inputs are constructed\n(\u201ctokenization\u201d) and we should expect these issues to disappear eventually.\n\n###\n\nHigh Crystallized Intelligence, Low Fluid Intelligence\n\nGiven the same data as Einstein, could more advanced LLMs come up with general\nrelativity without being prompted to do so? This is an open question.\n\nGoing back to chess: neither GPT-4 nor Claude Opus can compete at chess with\nthe best specialist chess engines. Even at significantly higher levels of\nscale, it would be surprising if they managed to beat AlphaZero-level engines,\nwhich are optimized specifically for those games.\n\nSimilarly in science: specialist applications such as AlphaFold have proven\nmore important than LLMs, which so far have been useful to help scientists\nwrite grant applications faster and other such routine tasks.\n\nThis is what you would expect from looking at how LLMs work: they are able to\ninterpolate well in a vector space given a decent amount of training data. But\nfor entirely novel domains, where there is no training data, it is unclear how\nor whether LLMs will be able to reason in a way that guarantees the answer is\nactually correct and exact, versus \u2018merely plausible\u2019. This suggests that even\nthe LLMs that come at the end of the 2020s would not be able to come up with\ngeneral relativity, given the data Einstein had.\n\nScience, and advancing human knowledge, involves reasoning and sometimes this\ninvolves making extremely improbable choices. The famous AlphaGo vs. Lee Sedol\nmatch involved several moments of transcendent creativity from the AI Go\nengine, notably its move 37 in game 2, which was so creative many of the human\ncommentators initially thought it was a mistake. [4] The move turned out to be\nan exception to a general principle which AlphaGo had correctly reasoned did\nnot apply in this particular board position.\n\nAlphaGo could prove move 37 was the best because it was doing a form of\nsearch: calculating the consequences of the move, and evaluating them. LLMs do\nnot do this currently, but most researchers agree that search will be a key\npart of a future AGI \u2013 much as humans plan and reason about the consequences\nof actions. Even when writing, one typically chooses between words and\nsentences based on how they fit into a whole; this is a form of reasoning\nwhich LLMs do not do, since they generate tokens step by step. Projects such\nas OpenAI\u2019s rumored Q* are going after this kind of broader AGI paradigm.\n\nAll of this implies that LLMs, which tend to \u2018reason\u2019 using probability\ndistribution over the next token, will be great at producing anything where\n\u2018looks like a plausible answer that could be correct\u2019 is the criterion; but in\na game like Go, or in human engineering tasks such as building a plane or\nfolding a protein, exactness and provable correctness based on mental models\nand reasoning about the world matters for the right result. An LLM will\nproduce a plausible simulation of a wave or the orbit of a planet, but if you\nneed numerical calculations to be exact or represent reality, you would not\nuse an LLM, you would use a specific model designed for that use case. It is\nplausible that many of the returns from AI will come from humans putting large\namounts of effort into specializing these more specific AI models towards\nimportant tasks, such as designing molecules, understanding the genome, and so\non \u2013 not just from large LLMs.\n\nOne distinction that is helpful in thinking about this question is fluid\nintelligence versus crystallized intelligence. Fluid intelligence is the\nability to reason, think abstractly, and solve novel problems independently of\nacquired knowledge; crystallized intelligence is more stored \u201cbook learning\u201d,\nsuch as vocabulary or general knowledge. LLMs are high on crystallized\nintelligence, but remain low on fluid intelligence. A benchmark that tests\nthis is Francois Chollet\u2019s ARC (Abstraction and Reasoning Corpus), which aims\nto test fluid intelligence through reasoning tasks that are deliberately kept\nout of AI training data, and which LLMs continue to do poorly at. [5]\n\nWe should keep in mind the striking definition of \u2018true\u2019 AI by Hernandez-\nOrallo, paraphrasing McCarthy: \u201cAI is the science and engineering of making\nmachines do tasks they have never seen and have not been prepared for\nbeforehand.\u201d LLMs may change the world greatly: but it may be less than the\nhype implies.\n\n###\n\nRounding up\n\nWith LLMs, we have clearly discovered a critical ingredient of intelligence.\nBut many of the above considerations suggest that AGI-like architectures will\ncontain several parts, and LLMs may play one role in a broader architecture.\nThis would match how the human brain works: there are many distinct modules\n(the cerebellum, the thalamus, and so on) and they each specialize on\ndifferent tasks, but with major overlaps. AGI-like systems may be similar.\n\nOne might also expect rich research in teams of LLMs coordinating around\noutputs to outperform individual LLMs. LLM \u201cwisdom of the crowds\u201d forecasts,\nfor example, outperform single LLMs, and these technique could exploit the\nfact that multiple LLMs can be used cheaply to enhance output. [7]\n\nFinally, none of these arguments are definitive; we simply do not know how\nthings will go. The large amounts of investment and research invested by tech\ncompanies and governments will increase at a large pace this decade, and the\narms race is already underway; kicking off this arms race may have been\nChatGPT\u2019s greatest legacy. This suggests we should prepare for a world in\nwhich we do get superhuman AI by the end of the decade \u2013 even if the chances\nare low.\n\n[1] https://observer.com/2024/02/metas-a-i-chief-yann-lecun-explains-why-a-\nhouse-cat-is-smarter-than-the-best-a-i/\n\n[2] https://ai.meta.com/blog/openeqa-embodied-question-answering-robotics-ar-\nglasses/\n\n[3] https://gwern.net/gpt-3\n\n[4] https://www.wired.com/2016/03/two-moves-alphago-lee-sedol-redefined-\nfuture/\n\n[5] Francois Chollet, On the Measure of Intelligence (2019)\nhttps://doi.org/10.48550/arXiv.1911.01547\n\n[6] Jose Hernandez-Orallo. Evaluation in artificial intelligence: from task-\noriented to ability-oriented measurement. Artificial Intelligence Review,\npages 397\u2013447, 2017.\n\n[7] Philipp Schoenegger et al, Wisdom of the Silicon Crowd: LLM Ensemble\nPrediction Capabilities Rival Human Crowd Accuracy\n(2024).https://arxiv.org/abs/2402.19379\n\nThanks for reading Digital Spirits! Subscribe for free to receive new posts\nand support my work.\n\n4 Likes\n\n\u00b7\n\n1 Restack\n\n3\n\nShare this post\n\n#### Is AI smarter than a house cat?\n\ndigitalspirits.substack.com\n\nShare\n\nComments\n\nIs Synthetic Data the Key to AGI?\n\nThe data wars are just beginning. Part 2 in a series of essays on AI.\n\nFeb 29 \u2022\n\nNabeel S. Qureshi\n\n13\n\nShare this post\n\n#### Is Synthetic Data the Key to AGI?\n\ndigitalspirits.substack.com\n\n4\n\nMoore's Law For Intelligence\n\nWill the amount of intelligence start to double every two years? The first in\na series of essays on AI.\n\nFeb 5 \u2022\n\nNabeel S. Qureshi\n\n18\n\nShare this post\n\n#### Moore's Law For Intelligence\n\ndigitalspirits.substack.com\n\n4\n\nThe Crisis of Technical Deference\n\nThe brittle reality of technologist-led policy\n\nJun 6, 2023 \u2022\n\nMatthew Mittelsteadt\n\n7\n\nShare this post\n\n#### The Crisis of Technical Deference\n\ndigitalspirits.substack.com\n\nReady for more?\n\n\u00a9 2024 Matthew Mittelsteadt\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
