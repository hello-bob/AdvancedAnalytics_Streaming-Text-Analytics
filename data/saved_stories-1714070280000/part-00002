{"aid": "40156745", "title": "SIMD in the GPU World", "url": "https://www.rastergrid.com/blog/gpu-tech/2022/02/simd-in-the-gpu-world/", "domain": "rastergrid.com", "votes": 1, "user": "melenaboija", "posted_at": "2024-04-25 12:36:19", "comments": 0, "source_title": "SIMD in the GPU world \u2013 RasterGrid", "source_text": "SIMD in the GPU world \u2013 RasterGrid\n\nSkip to content\n\n  * Home\n  * Company\n  * Services\n\n    * Consultancy\n    * Software Engineering\n    * Technical Specification\n  * Jobs\n\n    * Senior Software Engineer\n  * Blog\n\n4\\. Luther street, Budapest, Hungary 1087\n\ninfo@rastergrid.com\n\nGet In Touch\n\n# SIMD in the GPU world\n\nToday\u2019s high computational throughput probably would not be attainable without\nthe application of the SIMD paradigm in modern processors in increasingly\nclever ways. It\u2019s no coincidence that GPUs also gain most of their\nperformance, die area, and efficiency benefits thanks to this instruction\nissue scheme. In this article we will explore a couple of examples of how GPUs\nmay take advantage of SIMD and the implications of those on the programming\nmodel.\n\nBefore proceeding, it\u2019s worth noting that we will not discuss processor\nhardware design, thus we will not dwelve into details of individual components\nwithin a processor core, superscalar processor architecture, issue ports,\ninstruction-level parallelism, register files, bank conflicts, etc. Our focus\nwill be on aspects of the various uses of the SIMD paradigm that have a direct\neffect on the way developers should write efficient code for such processors,\nand will only touch marginally on subjects beyond that. That is not to say\nthose hardware details and many other nuances of a specific target\narchitecture have no significant impact on the way code should be written for\nsuch devices in order to achieve optimal performance, however, such a\ndiscussion is well beyond the scope of this article.\n\n## What is SIMD?\n\nThe term comes from Flynn\u2019s classification of computer architectures. SIMD\nstands for single instruction, multiple data, as opposed to SISD, i.e. single\ninstruction, single data corresponding to the traditional von Neumann\narchitecture. It is a parallel processing technique exploiting data-level\nparallelism by performing a single operation across multiple data elements\nsimultaneously.\n\nIllustration of a 4-lane SIMD block.\n\nLooking at it from a different perspective, SIMD enables reusing a single\ninstruction scheduler across multiple processing units. That allows processor\ndesigners to save significant die area and hence achieve greater computational\nthroughput with the same number of transistors compared to traditional scalar\nprocessing cores having a one-to-one mapping between instruction schedulers\nand processing units.\n\nThe SIMD model is not unique to massively parallel processors like GPUs, in\nfact CPUs have a long history of employing SIMD instruction sets like MMX,\nSSE, NEON, and AVX that can be used in addition to the traditional scalar\noperations provided by the CPU. While our focus will be on GPUs, we will also\nsee a couple of examples of those.\n\n## Vector SIMD\n\nTraditionally, 3D graphics workloads were all about vector operations and to\nsome extent they still are:\n\n  * Rendering 3D scenes require certain linear transformations of geometric attributes like position, normal, and texture coordinates which involves vector-matrix multiplications which themselves comprise of multiple vector-vector operations (dot products) often performed on 4-component vectors representing homogeneous coordinates\n  * Determining the color of individual vertices and/or pixels usually involves complex lighting calculations which themselves usually comprise of 3- or 4-component vector operations where the vectors represent colors (in RGB or RGBA format) or directions like the surface normal, incoming light direction, reflection direction, etc.\n\nIt is thus no surprise that GPUs used SIMD units since the early days to\nimplement vector instructions. It is also not a coincidence that the first\nprogrammable shaders used assembly-like shading languages providing\ninstructions operating on 4-component vectors.\n\nThe atomic unit of data in this model is a 4-component vector with floating\npoint components. Assuming standard IEEE 754 32-bit floating point values, we\nget vector registers with a total width of 128 bits. This form of SIMD\noperating on registers with multiple components is hence often also referred\nto as packed-SIMD, or SWAR (SIMD within a register).\n\nThere are two instructions (or family of instructions) that are worth calling\nout in particular.\n\nThe first is MAD (multiply-add) or MAC (multiply-accumulate) which is\navailable on practically all GPUs as a single instruction, as graphics and\nmultimedia workloads are full of scale-and-bias operations. This means that on\ntraditional 4-component vector based GPUs it takes only a single instruction\nto calculate 4 floating point multiplications and 4 additions, and floating\npoint MAD/MAC is still often used as the unit for measuring the instruction\nthroughput of GPUs.\n\nThe second is the various flavours of dot product instructions (e.g. DP4 or\nDP3) that calculate the scalar (or dot) product of two vectors. These\nthemselves, more or less, comprise of MAD/MAC operations, hence they are\nsimilarly \u201ccheap\u201d operations to perform on a vector SIMD processor. As most of\nthe transformation and lighting calculations directly or indirectly comprise\nof dot products, vector SIMD processors greatly benefit of single-instruction\ndot products both from throughput and latency perspective.\n\nIllustration of a possible implementation of 4-component multiply-add (MAD,\nleft) and dot product (DP4, right) pipes. As processing time is dominated by\nthe multiplications, both operations can be completed with comparable latency.\nAlso note that the scalar result of the DP4 instruction is usually replicated\nacross the channels of the destination vector register, by default (unless\nrequested otherwise as we\u2019ll see later).\n\nCPU SIMD instruction sets also use packed-SIMD technology. As an example, on\nx86 the SSE instruction set also enables performing operations across multiple\ndata elements in a single instruction by interpreting the XMM registers as\npacked vectors with multiple components.\n\nWhen it comes to vector SIMD processors, it\u2019s worth noting two key techniques\npopularized by them:\n\n  1. Component swizzling \u2013 the ability to redirect individual components of source operands to individual processing units, and similarly redirect individual output components to destination components\n  2. Component masking \u2013 the ability to discard individual output components (or, analogously, disable individual processing units)\n\nThese enabled expressing more complex variations of the same operation by\nreducing the number of components to process, replicating an input or output\nacross components, etc. Implementations typically also support special\nconstant swizzles where the corresponding component of the operand is replaced\nwith one of the commonly used constant values like 0.0, 1.0, 2.0, and 0.5\n(potentially even more). Making all (or at least most) instructions accept\ncustom swizzling and masking can significantly reduce the number of\ninstructions for a given workload as it eliminates the need for most move\ninstructions.\n\nIllustration of swizzling (both) and masking (right) in vector instructions.\nNote that in the example on the right the 3rd (Z) component of the output is\nmasked but in effect it\u2019s the 4th channel in the SIMD that is unused. The\nlatter is really arbitrary and in fact different SIMD instruction sets use\ndifferent ways to express masking (as we will see later).\n\nWhile traditional vector-based GPUs are less prevalent nowadays, packed-SIMD\ntechnology is still in use in other forms, as we will see later.\n\n## From Vector to Scalar\n\nAs GPU workloads evolved, more and more scalar operations creeped their way in\nthe shaders making it increasingly more difficult to reach the theoretical\ncomputational throughput of traditional vector-based GPUs. As these processors\nwere vector-oriented by design, performing scalar operations usually meant the\nexecution of a vector instruction with all but one component masked out.\n\nWhile sometimes it\u2019s possible to combine multiple scalar operations into a\nsingle vector instruction, e.g. four independent scalar additions can be\ntrivially merged into a single vector addition and thus utilizing all\nprocessing units, it is usually difficult to find enough independent scalar\noperations of the same kind. Nonetheless, when targeting vector-based GPUs or\nother packed-SIMD instruction sets, generally it\u2019s highly advised to try to\nvectorize calculations as the application developer can often do a better job\nat that than even an optimizing compiler.\n\nSome GPU architectures thus moved from a traditional vector-based architecture\nto a VLIW one. VLIW stands for very long instruction word, and processors\nusing such an instruction set utilize complex instructions which comprise of\nmultiple operations that are executed in parallel.\n\nSome VLIW based GPUs used a 3+1 structure where a single instruction encoded\none operation to perform on the first three components of the vector register,\nand another to perform on the fourth component, acknowledging the fact that\nRGBA values often required separate operations to be performed on the RGB\ncolor channels compared to the alpha channel, and that for many calculations\n3-component vector operations were sufficient (color-only, direction vector,\nor affine transformation operations) leaving the fourth processing unit\navailable to execute e.g. a completely independent scalar operation.\n\nIllustration of a hypothetical 3+1 VLIW processing core (left) and a sample\ninstruction (right) combining a 3-component vector and a scalar operation.\n\nTranscendental operations (e.g. trigonometric and logarithmic operations) and\nother non-trivial operations (division, square root, etc.) typically only used\nwith scalar operands were often implemented only on the fourth processing\nunit, often called the transcendental unit, aligning the processor design\nbetter to the expected workload while saving precious die area.\n\nOver time more and more complex VLIW based GPUs appeared with various widths\nand ever more flexible ways to specify multiple operations within a single\ninstruction. In their most sophisticated incarnations the VLIW instruction\nsets allow scheduling practically any operation separately for each component.\n\nVLIW based GPUs, hence, have an edge over traditional vector-based ones in\nthat almost any set of operations can be merged into a single VLIW instruction\ncovering the entire width of the processing block, as the operation itself can\nvary per component (or groups of components) in each instruction, not just the\ndata.\n\nHowever, those operations generally still have to be independent, i.e. no\nsource of either operation may depend on the result of another within a single\ninstruction, hence despite the best optimization efforts from the application\ndeveloper and the compiler, it may still result in multiple processing units\nidling from time to time over the course of a shader invocation\u2019s execution\ndue to the data dependencies.\n\nIn addition, unless the particular instruction set supports addressing\ndifferent registers as source or destination operands across the different\noperations within a VLIW instruction, additional move operations may be\nnecessary to comply with the operand reference limitations, just like in case\nof traditional vector-based GPUs.\n\nNonetheless, one appeal of such architectures is that they can sort of operate\nin a mixed mode where vector and scalar operations can be both expressed in a\nsingle instruction thus even inter-component vector math like dot products and\ncross products may be performed using a single instruction (although the\nactual time an instruction completes may vary on the operations used).\n\nStill, the heterogeneous instruction set of such processors means that the\ninstruction decoder and scheduler is likely to be similarly complicated thus\nlimiting the die area benefits of using a single instruction scheduler across\nmultiple processing units.\n\nOne way to alleviate this complexity is to use a simple scalar instruction set\ninstead which is what AMD did, for example, with the introduction of the GCN\ninstruction set architecture, that is likely the most well-known GPU ISA in\nthe developer community to date. Of course, this also comes with some\nsacrifices, as in a completely scalar instruction set even a basic dot product\nrequires multiple MAD/MAC instructions (although, once again, we ignore\nimportant details here, like how long each instruction actually takes to\ncomplete).\n\nThroughout this paradigm shift it became gradually more important for\napplication developers to use scalar operations in their shaders whenever\npossible and only keep vector math where that\u2019s the natural granularity of\ncomputation.\n\nBut does all this mean we\u2019re done with SIMD? Of course not, in fact we are\njust getting started...\n\n## SIMT\n\nVector processing is just one way to leverage the benefits of the SIMD\nparadigm. Another common way to utilize SIMD instructions, as it\u2019s often done\neven on the CPU, is to perform array processing (contrarily to vector\nprocessing), as demonstrated in the example below:\n\nPlain text\n\nCopy to clipboard\n\nOpen code in new window\n\nEnlighterJS 3 Syntax Highlighter\n\n// SISD code to perform element-wise multiplication of two arrays\n\nvoid array_mul_sisd(float* C, float* A, float* B, size_t size)\n\n{\n\nfor (size_t i = 0; i < size; ++i)\n\nC[i] = A[i] * B[i];\n\n}\n\n// Same algorithm using 128-bit (4-wide) SIMD array processing (x86 SSE)\n\n// (for simplicity, we assume the alignment and size of the arrays is\nappropriate)\n\n#define FLT4(X) *((__m128*)(&(X)))\n\nvoid array_mul_simd4(float* C, float* A, float* B, size_t size)\n\n{\n\nfor (size_t i = 0; i < size; i += 4)\n\nFLT4(C[i]) = _mm_mul_ps(FLT4(A[i]), FLT4(B[i]));\n\n}\n\n// SISD code to perform element-wise multiplication of two arrays void\narray_mul_sisd(float* C, float* A, float* B, size_t size) { for (size_t i = 0;\ni < size; ++i) C[i] = A[i] * B[i]; } // Same algorithm using 128-bit (4-wide)\nSIMD array processing (x86 SSE) // (for simplicity, we assume the alignment\nand size of the arrays is appropriate) #define FLT4(X) *((__m128*)(&(X))) void\narray_mul_simd4(float* C, float* A, float* B, size_t size) { for (size_t i =\n0; i < size; i += 4) FLT4(C[i]) = _mm_mul_ps(FLT4(A[i]), FLT4(B[i])); }\n\n    \n    \n    // SISD code to perform element-wise multiplication of two arrays void array_mul_sisd(float* C, float* A, float* B, size_t size) { for (size_t i = 0; i < size; ++i) C[i] = A[i] * B[i]; } // Same algorithm using 128-bit (4-wide) SIMD array processing (x86 SSE) // (for simplicity, we assume the alignment and size of the arrays is appropriate) #define FLT4(X) *((__m128*)(&(X))) void array_mul_simd4(float* C, float* A, float* B, size_t size) { for (size_t i = 0; i < size; i += 4) FLT4(C[i]) = _mm_mul_ps(FLT4(A[i]), FLT4(B[i])); }\n\nAs it can be seen above, even though the individual work that needs to be\nperformed on the array elements is scalar by nature, SIMD instructions can be\nused to process multiple array elements in parallel. This subtype of the SIMD\nparadigm is often called SIMT, i.e. single instruction, multiple threads. It\nis a misnomer, to some extent, as the \u201cthreads\u201d we talk about here are not the\nindependently schedulable threads of execution that we all know, but rather\nthe threads we know from NVIDIA\u2019s CUDA API, i.e. the individual lanes of a\nwave. But let\u2019s not get ahead of ourselves...\n\nThis is really just the other side of the same coin, as we can call the above\nexample as vectorization as well, if we really want to. However, when this\nvectorization isn\u2019t explicit, but rather an artifact of the programming model\nthen the distinction between array vs vector processing becomes clear-cut.\n\nSo far we only talked about leveraging internal parallelism within a single\nshader invocation, utilizing the fact that many shader computations operate on\nvectors of various widths and even scalar operations are ofttimes independent\nfrom each other. However, the massive parallelism of GPU workloads actually\nstems from having to execute the same shader code across a large number of\ndata elements (vertices, primitives, fragments, etc.).\n\nThus, ignoring control flow for now, which is anyway something that wasn\u2019t\navailable on early programmable GPUs, it is trivial to process multiple shader\ninvocations in parallel by scaling up the width of the SIMD unit. Of course,\nthere are practical and technological limits to how wide it\u2019s possible or\nworth to make a SIMD unit, but in theory it could go as wide as the entire\nprocessor. This allows reusing a single instruction scheduler across even more\nprocessing units than in a basic vector or VLIW processor.\n\nIllustration of a hypothetical GPU with a 4-component vector-based vertex\nprocessor and a 3+1 VLIW based fragment processor.\n\nGoing back to our GPUs with scalar instruction sets, now it\u2019s trivial to see\nthat the scalar nature of the instructions themselves does not prevent us from\nutilizing SIMD technology, as just as their vector-based or VLIW based\ncounterparts, the instruction stream can be issued across multiple shader\ninvocations simultaneously, or, loosely speaking, executed in lock-step.\n\nIn practice, modern GPUs usually comprise of multiple sets of such SIMD\nprocessing blocks hierarchically aggregated into clusters sharing different\ntypes of caches and auxiliary hardware blocks performing fixed-function\noperations of the graphics pipeline.\n\nIn this model shader invocations that are scheduled simultaneously across the\nprocessing units of one of more SIMD blocks form a subgroup often also called\na wave, wavefront, or warp, while the individual shader invocations within\nthose are referred to as the lanes or threads of the wave.\n\nTaking AMD\u2019s GCN architecture as an example, while the instructions are scalar\nfrom the perspective of a single shader invocation, the instruction actually\nrefers to these as vector instructions, as practically they perform operations\nof entire waves as wide vector operations where each component belongs to a\nparticular shader invocation. Thus the scalar nature of the instruction set\nshould not be confused with the scalar unit available on GCN GPUs (or in some\nrecent NVIDIA GPUs) which actually behaves more as a SISD execution unit\nshared across the entire wave.\n\n## SIMD Control Flow\n\nOver time GPUs gained more and more sophisticated support for shader control\nflow. However, in case of a SIMD unit that may process multiple shader\ninvocations (or threads, if we must) it is less intuitive how control flow can\nbe implemented. This is where another SIMD paradigm comes handy that is called\nan associative processor in Flynn\u2019s taxonomy.\n\nThe technique expands on the idea we already covered to some extent in our\ndiscussion about vector-based GPUs whereas individual components of the vector\noperation could be masked out. There is no reason why we couldn\u2019t apply the\nsame principle for SIMD units that process multiple shader invocations in\nparallel.\n\nEarly incarnations of GPU control flow support did not have true branching\nsupport in the processor, more specifically, there were no jump instructions\nor anything similar available. Thus loops of any sort would get unrolled by\nthe compiler and the shader authors had to be wary of the instruction limit of\nthe target GPU as at this point instructions were not streamed from memory but\nwere stored in a limited size on-chip buffer. Support for conditionals,\nhowever, arrived fairly early in the form of predicated/conditional\ninstructions.\n\nIn a naive implementation this means that in case of an if-else block the GPU\nwould execute both branches and then a conditional instruction (e.g. some form\nof CMOV) would select the results of the appropriate branch based on the value\nof the condition. This technique enables to continue utilizing SIMD technology\nto execute multiple shader invocations in parallel while still allowing for\nthe individual invocations to virtually take different branches across the\ncontrol flow.\n\nPlain text\n\nCopy to clipboard\n\nOpen code in new window\n\nEnlighterJS 3 Syntax Highlighter\n\n// GLSL-style high-level pseudo-code\n\n...\n\nif (!inShadow) {\n\nlight = max(0.0, dot(L, N));\n\ncolor *= light;\n\n}\n\n...\n\n// GLSL-style high-level pseudo-code ... if (!inShadow) { light = max(0.0,\ndot(L, N)); color *= light; } ...\n\n    \n    \n    // GLSL-style high-level pseudo-code ... if (!inShadow) { light = max(0.0, dot(L, N)); color *= light; } ...\n\nPlain text\n\nCopy to clipboard\n\nOpen code in new window\n\nEnlighterJS 3 Syntax Highlighter\n\n// Corresponding hypothetical instructions\n\n...\n\nDP3 R1.x, L.xyz, N.xyz;\n\nMAX R1.x, R1.x, R0.0;\n\nMUL R1.xyz, COL.xyz, R1.xxx;\n\nCMOV COL.xyz, COL.xyz, R1.xyz, SHD.x;\n\n...\n\n// Corresponding hypothetical instructions ... DP3 R1.x, L.xyz, N.xyz; MAX\nR1.x, R1.x, R0.0; MUL R1.xyz, COL.xyz, R1.xxx; CMOV COL.xyz, COL.xyz, R1.xyz,\nSHD.x; ...\n\n    \n    \n    // Corresponding hypothetical instructions ... DP3 R1.x, L.xyz, N.xyz; MAX R1.x, R1.x, R0.0; MUL R1.xyz, COL.xyz, R1.xxx; CMOV COL.xyz, COL.xyz, R1.xyz, SHD.x; ...\n\nObviously, this comes at a hefty cost as all branches within the shader\nactually need to be executed for all shader invocations, and it\u2019s the origin\nof the old advice of avoiding conditionals in shaders whenever possible that\nfar outlived the actual GPUs without true branching capabilities. Nonetheless,\neven in these times, the cost of control flow was still acceptable when the\ncomputation in the actual branches was fairly limited, as in the example\nabove.\n\nOne drawback of using the naive approach above is that we are not only paying\nthe performance cost of both branches, but also their power cost, as all\nshader invocations across the SIMD unit perform both sets of calculations even\nthough each will only use the results of one of them in the end. Thus, in\npractice, GPUs support predicating of pretty much every instruction through\nsome special register(s) similar to the mask registers used by CPU instruction\nsets like AVX-512. Hence even the early assembly-like shading languages used\nsuch an approach and thus allowed to at least save the power cost for shader\ninvocations not taking a particular branch.\n\nIllustration of a hypothetical condition (predicate) mask based 16-wide SIMT\nGPU\u2019s active lanes over the course of running 16 shader invocations that take\ndifferent paths through the branches. Lanes in green are active while grayed\nout lanes are masked out by the corresponding conditions. Note that this is an\nimplementation using a stack to handle nested conditionals. Stackless\nimplementations, flattening nested conditionals is often possible with other\ntrade-offs. More commonly, as the nesting depth is typically known for\nshaders, a fixed number of backup registers can be used as the stack. Without\nsupport for branching instructions it is not possible to skip over a branch\neven if none of the shader invocations in the wave would take it, as it can be\nseen in the case of the branch on cnd2 above.\n\nNewer GPUs then introduced actual branching instructions (jump-like or\nstructured) that work in a similar fashion to their SISD versions. However, we\nmust not forget that GPUs schedule and execute entire waves of shader\ninvocations in lock-step, thus skipping over code using branching instructions\nis only possible if all shader invocations within the wave take the same path.\n\nWhen that\u2019s not the case we are talking about divergent waves and in such\ncases GPUs continue to operate like their predecessors by executing both\nbranches of an if-else block, or worse, in case of loops it means that each\nshader invocation within the wave will take as many iterations as the one that\ntakes the most. Hence, even though control flow is inexpensive on today\u2019s\nGPUs, dynamically uniform control flow (as opposed to divergent) is still\nstrongly preferred to avoid having to pay the cost of executing the\ninstructions of multiple, shader-invocation-wise, mutually exclusive branches.\n\nIllustration of a hypothetical branching capable 16-wide SIMT GPU\u2019s active\nlanes over the course of running 16 shader invocations. Lanes in green are\nactive while grayed out lanes are masked out by the corresponding conditions.\nNote the following: cnd1 is a compile-time known uniform expression, i.e. it\u2019s\nknown from the shader code that the condition will not vary across lanes of a\nwave, hence only branching instructions are needed. cnd2 is a dynamically\nuniform expression, i.e. it happens to be that all lanes of the wave evaluated\nit to the same value, hence the wave could skip the untaken branch. However,\ncondition mask code was still necessary to be added by the compiler as dynamic\nuniformity is not known at compile-time. cnd3 is a divergent expression, hence\nthe wave will execute both sides of the branch with the appropriate\npredication. If the shading language syntax allows expressing it, generation\nof branching code for known divergent branches, or predication code for known\ndynamically uniform branches can be avoided.\n\n## Cross-Lane Operations\n\nAnalogously to how the idea of component masking expands to the SIMT model in\nthe form of instruction predication, component swizzling also has its\ncorresponding counterpart in the form of cross-lane operations. This time\ninstead of swizzling the components of a vector when using them as instruction\noperands on a vector-based GPU, data is swizzled across the shader invocations\nwithin a wave.\n\nThis technique is beneath one of the hottest shading language features in the\nlast couple of years as it enables significantly higher performance data\nsharing across shader invocations within a subgroup compared to the wider\n(workgroup) scope but slower data exchange through shared memory, as cross-\nlane operations allow shader invocations to directly reference data in the\nregisters of other shader invocations within the wave. Implementing this seems\nfairly trivial, considering that the registers of all shader invocations\nexecuting on a particular SIMD unit are located in the same register file.\n\n## There\u2019s More!\n\nProcessors often employ another method to increase instruction-level\nparallelism without actually increasing the width of the underlying SIMD\nblock. In this scheme the instruction scheduler issues each instruction\nmultiple times but for different sets of shader invocations and it\u2019s often\nreferred to as temporal SIMT, or, when using wide SIMD blocks, spatio-temporal\nSIMT, as instruction issue is spread both in the spatial domain (over\nindividual lanes of a SIMD block) and the temoral domain due to the multi-\ncycle reissue.\n\nLoosely speaking, this is similar to string operations using the REP prefix on\nan x86 CPU, although that is far from an accurate analogy. In practice,\ntemporal SIMT on GPUs is often a bit more rigid than that, as generally\ninstructions are reissued a hardwired number of times. As an example, AMD\u2019s\nGCN architecture issues an instruction across a complete 64-wide wave of\nshader invocations to a 16-wide SIMD block over 4 cycles, 16 lanes each cycle.\nThis instruction issue technique enables the possibility to hide the latency\nof instruction decoding and/or execution and thus provide greater instruction-\nlevel parallelism.\n\nNote, however, that this technique should not be confused with the\nsimultaneous multithreading (SMT) technology often used by GPUs whereas the\nprocessor schedules instructions of other waves while a wave waits for a long-\nlatency operation like a memory read.\n\nTemporal SIMT can be also implemented in its pure form, i.e. without an actual\nwide execution unit. In this case each shader invocation within a wave is\nissued in separate cycles which may even enable the scheduler to skip issuing\nthe instructions of inactive shader invocations (due to predication). Such an\napproach could potentially eliminate the cost of divergent shader invocations,\nbut only up to a certain extent, as it also limits the chance of hiding\noperation execution latency, hence also limiting effective instruction-level\nparallelism, and the overall time to complete the execution of a wave would\nincrease due to removing one dimension of parallelism.\n\nYet another technique is to share a single instruction scheduler across\nmultiple SIMD blocks. While this may not be self-evident, there\u2019s a difference\nbetween issuing instructions to multiple SIMD blocks compared to a single,\nwider SIMD block. For example, separate SIMD blocks have separate register\nfiles, hence simple cross-lane operations cannot be used to share data across\nshader invocations running on separate SIMD blocks, even if they are both fed\nby the same instruction scheduler.\n\nCombining temporal SIMT with a single instruction scheduler feeding multiple\nSIMD blocks allows a single scheduler to handle an even larger number of\ninstructions executing in parallel. However, sometimes this may also imply\nthat the issue granularity may be higher than the size of a single wave. On\nAMD\u2019s GCN architecture, for example, this is not the case, as a single\ninstruction of a 64-wide wave takes 4 cycles to start on a single 16-wide SIMD\nblock and, while there are four SIMD blocks per scheduler, the scheduler can\nactually issue an instruction from a separate wave each cycle, hence able to\nsend a new instruction to all four SIMD blocks across those 4 cycles until it\nhas to go back to the first one.\n\nIllustration of how a spatio-temporal SIMT GPU issues instructions from\n32-wide waves using a single scheduler to four 8-wide SIMD execution units.\nThe dark colored blocks indicate the first instruction issue instances\n(covering the first 8 shader invocations within the wave), while the light\ncolored blocks indicate the reissue of the same instruction for the subsequent\ngroups of shader invocations within the same wave.\n\nWhile it may have seemed that some of the techniques presented that take\nadvantage of the SIMD paradigm are mutually exclusive, all of them can be\ncombined. As an example, there\u2019s no reason why a vector-based or VLIW GPU\ncould not take advantage of the SIMT model, in fact they do, as going e.g.\n4-wide with SIMD wouldn\u2019t be sufficient to achieve the scales of computational\nthroughput that we have seen on GPUs over the last couple of decades.\n\nAnother example of a multi-paradigm use of SIMD processing can be noted in\ncertain SIMT based GPUs that also support multiple operand precisions (e.g.\nboth 16-bit and 32-bit floating point operands) as this may mean that even a\nGPU that otherwise uses a scalar instruction set may implement lower-precision\noperations following the packed-SIMD paradigm, or use wider vector widths for\nlower-precision operations, as the register width is typically fixed by the\narchitecture.\n\nIllustration of mixing scalar and vector (packed-SIMD) pipes on a GPU with\n32-bit registers.\n\n## Conclusion\n\nAs we saw, GPUs can leverage the benefits of the SIMD paradigm in many\ninteresting ways. All of those techniques, however, have various levels of\neffects on the way code should be written for them in order to maximize\nperformance. Hence it\u2019s important for developers to familiarize themselves\nwith them. Fortunately, there is a high degree of commonality across the way\nhow the various types of beasts prefer to be fed, so all is not lost.\nNonetheless, there is always some extra performance to be found when targeting\na particular hardware architecture.\n\nIt is also worth noting that, to some extent, the instruction set can be\nfairly orthogonal to the actual way the processor schedules individual\noperations for execution, let alone the actual execution itself. We presented\na very simplistic view of how hardware may execute operations in a SIMD\nfashion. Modern superscalar processors with deep instruction pipelines are far\nmore complex than that.\n\nAlso, the instruction set and the way how platform-independent shader code is\nmapped to it is generally hidden behind the compiler infrastructure provided\nby the hardware vendors. Hence sometimes one may come to incorrect conclusions\nabout how a particular target hardware should be coded for simply based on\nsome limited high-level information about the architecture.\n\nConsidering the shader programming model offered by the various graphics and\ncompute APIs, it seems that at least the SIMT execution model is common across\nall GPU architectures on the market today (unsurprisingly), while other design\nchoices like using a vector-based, VLIW, or scalar instruction set (or a\ncombination of those) varies more across individual implementations.\n\nIn fact, it may not be uncommon for modern GPUs to be able to switch between\nvarious issue models (e.g. vector vs scalar, or different wave widths as we\nsee on some recent architectures). It is thus possible that only the level of\nflexibility and granularity at which these issue models can be switched is\nwhat defines each unique architecture.\n\nIf GPU architectures would go in such a multi-paradigm direction then that\nwould likely be good news for application developers, as it may allow reaching\ncloser to optimal performance and efficiency for a wider range of algorithms.\n\nFor further reading and architecture examples, please check out the links\nbelow:\n\n  * ATI Radeon HD 2000 programming guide\n  * AMD Accelerated Parallel Processing \u2013 OpenCL Programming Guide\n  * The AMD GCN Architecture \u2013 A Crash Course\n  * AMD GCN Architecture Whitepaper\n  * AMD GCN Gen1 Instruction Set Architecture\n  * AMD GCN Gen2 Instruction Set Architecture\n  * AMD GCN Gen3 Instruction Set Architecture\n  * AMD Vega Instruction Set Architecture\n  * AMD RDNA Architecture Whitepaper\n  * AMD RDNA 1 Instruction Set Architecture\n  * AMD RDNA 2 Instruction Set Architecture\n  * NVIDIA Kepler Architecture Whitepaper\n  * NVIDIA Maxwell Architecture Whitepaper\n  * NVIDIA Pascal Architecture Whitepaper\n  * NVIDIA Volta Architecture Whitepaper\n  * NVIDIA Turing Architecture Whitepaper\n  * NVIDIA Ampere Architecture Whitepaper\n\nPosted on 2022-02-042022-02-04 / 0 Categories GPU Tech\n\n### Post Author: Daniel R\u00e1kos\n\n## COMPANY INFORMATION\n\n  * Name: RasterGrid Kft.\n  * Country of incorporation: Hungary\n  * Registration number: 01-09-375468\n  * EU VAT number: HU28823072\n  * Address: 1087 Budapest Luther utca 4. 5. em. 36.\n\n## LEGAL\n\n  * Cookie Policy\n  * Privacy Policy\n  * Terms and Conditions\n  * Trademark Notice\n\n\u00a9 2010-2023 Daniel Rakos & RasterGrid Kft. All Rights Reserved.\n\nWe are using cookies to give you the best experience on our website.\n\nYou can find out more about which cookies we are using or switch them off in .\n\nPowered by GDPR Cookie Compliance\n\nPrivacy Overview\n\nThis website uses cookies so that we can provide you with the best user\nexperience possible. Cookie information is stored in your browser and performs\nfunctions such as recognising you when you return to our website and helping\nour team to understand which sections of the website you find most interesting\nand useful.\n\nStrictly Necessary Cookies\n\nStrictly Necessary Cookie should be enabled at all times so that we can save\nyour preferences for cookie settings.\n\nIf you disable this cookie, we will not be able to save your preferences. This\nmeans that every time you visit this website you will need to enable or\ndisable cookies again.\n\nCookie Policy\n\nMore information about our Cookie Policy\n\n", "frontpage": false}
