{"aid": "40252308", "title": "Creating a first person shooter that scales to millions of players", "url": "https://mas-bandwidth.com/creating-a-first-person-shooter-that-scales-to-millions-of-players/", "domain": "mas-bandwidth.com", "votes": 2, "user": "gafferongames", "posted_at": "2024-05-03 20:57:35", "comments": 0, "source_title": "Creating a first person shooter that scales to millions of players", "source_text": "Creating a first person shooter that scales to millions of players\n\nM\u00e1s Bandwidth\n\nSign in Subscribe\n\nMay 3, 2024 12 min read fps\n\n# Creating a first person shooter that scales to millions of players\n\nIs it even possible? Turns out it's a lot closer than you may think.\n\nPhoto by Maxime Dor\u00e9 / Unsplash\n\nI'm Glenn Fiedler and welcome to M\u00e1s Bandwidth, my new blog at the\nintersection of game network programming and scalable backend engineering.\n\nBack in high school I played iD software games from Wolfenstein to DOOM and\nQuake. Quake was truly a revelation. I still remember Q1Test when you could\nhide under a bridge in the level and hear the footsteps of a player walking\nabove you.\n\nThat was 1996. Today it's 2024. My reflexes are much slower, I have slightly\nless hair, and to be honest, I really don't play first person shooters anymore\n\u2013 but I'm 25 years in on my game development career and now I'm one of the top\nworld experts in game netcode. I've even been fortunate enough to have worked\non several top tier first person shooters: Titanfall and Titanfall 2, and some\nof my code is still active in Apex Legends.\n\nTaking a look at person shooters in 2024, there seems to be two main genres:\nmassive online battle arenas like PUBG, Fortnite and Apex Legends (60-100\nplayers), and team combat games like Counterstrike (5v5), Overwatch (6v6) and\nValorant (5v5).\n\nNotably absent are the first person shooters with thousands of players\npromised by Improbable back in 2014:\n\n> \"Imagine playing a first-person shooter like Call of Duty alongside\n> thousands of other players from across the world without having to worry\n> about latency. Or a game where your actions can trigger persistent reactions\n> in the universe that will affect all other players. This is what gaming\n> startup Improbable hopes to achieve.\"\n>\n> \\- Wired (https://www.wired.com/story/improbable/)\n\nWhat's going on? Is it really not possible, or was Improbable's technology\njust a bunch of hot air? In this article we're going to find out if it's\npossible to make a first person shooters with thousands of players \u2013 and we're\ngoing to do this by pushing things to the absolute limit.\n\n## One Million Players\n\n> \"Would someone tell me how this happened? We were the fucking vanguard of\n> shaving in this country. The Gillette Mach3 was the razor to own. Then the\n> other guy came out with a three-blade razor. Were we scared? Hell, no.\n> Because we hit back with a little thing called the Mach3Turbo. That's three\n> blades and an aloe strip. For moisture. But you know what happened next Shut\n> up, I'm telling you what happened\u2014the bastards went to four blades. Now\n> we're standing around with our cocks in our hands, selling three blades and\n> a strip. Moisture or no, suddenly we're the chumps. Well, fuck it. We're\n> going to five blades.\n>\n> \\- The Onion\n\nWhen scaling backend systems a great strategy is to aim for something much\nhigher than what you really need. For example, my startup Network Next is a\nnetwork acceleration product for multiplayer games, and although it's\nincredibly rare for any game to have more than a few million players at the\nsame time, we load test Network Next up to 25M CCU.\n\nNow when a game launches using our tech and it hits a few hundred thousand or\nmillion players it's all very simple and everything just works. There's\nabsolutely no fear that some issue will show up due to scale because we've\nalready pushed it much, much further than it will ever be used in production.\n\nLet's apply the same approach to first person shooters. We'll aim for 1M\nplayers and see where we land. After we scale to 1M players, I'm certain that\nbuilding a first person shooter with thousands of players will seem really\neasy.\n\n## Player Simulation at Scale\n\nThe first thing we need is a way to perform player simulation at scale. In\nthis context simulation means the game code that takes player inputs and moves\nthe player around the world, collides with world geometry, and eventually also\nlets the player aim and shoot weapons.\n\nLooking at both CPU usage and bandwidth usage, it's clearly not possible to\nhave one million players on one server. So let's create a new type of server,\na player server.\n\nEach player server handles player input processing and simulation for n\nplayers, we don't know what n is yet, but this way we can scale horizontally\nto reach the 1M player mark, by having 1M / n player servers.\n\nThe assumptions necessary to make this work are:\n\n  1. The world is static\n  2. Each player server has a static representation of the world that allows for collision detection to be performed between the player and the world\n  3. Players do not physically collide with each other\n\nPlayer servers take the player input and delta time (dt) for each client frame\nand step the player state forward in time. Players are simulated forward only\nwhen input packets arrive from the client that owns the player, and these\npackets correspond to actual display frames on the client machine. There is no\nglobal tick on a player server. This is similar to how most first person\nshooters in the Quake netcode model work. For example, Counterstrike,\nTitanfall and Apex Legends.\n\nLet's assume player inputs are sent on average at 100HZ, and each input is 100\nbytes long. These inputs are sent over UDP because they're time series data\nand are extremely latency sensitive. All inputs must arrive, or the client\nwill see mis-predictions (pops, warping and rollback) because the server\nsimulation is authoritative.\n\nWe cannot use TCP for input reliability, because head of line blocking would\ncause significant delays in input delivery under both latency and packet loss.\nInstead, we send the most recent 10 inputs in each input packet, thus we have\n10X redundancy in case of any packet loss. Inputs are relatively small so this\nstrategy is acceptable, and if one input packet is dropped, the very next\npacket 1/100th of a second later contains the dropped input PLUS the next\ninput we need to step the player forward\n\nFirst person shooters rely on the client and server running the same\nsimulation on the same initial state, input and delta time and getting\n(approximately) the same result. This is known as client side prediction, or\noutside of game development, optimistic execution with rollback. So after the\nplayer input is processed, we send the most recent player state back to the\nclient. When the client receives these player state packets, it rolls back and\napplies the correction in the past, and invisibly re-simulates the player back\nup to present time with stored inputs.\n\nYou can see the R&D source code for this part of the FPS here:\nhttps://github.com/mas-bandwidth/fps/blob/main/001/README.md\n\n## Player Simulation Results\n\nThe results are fascinating. Not only is an FPS style player simulation for 1M\nplayers possible, it's cost effective, perhaps even more cost effective than a\ncurrent generation FPS shooter with 60-100 players per-level.\n\nTesting indicates that we can process 8k players per-32 CPU bare metal machine\nwith XDP and a 10G NIC, so we need 125 player servers to reach 1 million\nplayers.\n\nAt a cost of a just $1,870 USD per-month per-player server (datapacket.com),\nthis gives a total cost of $233,750 USD per-month,\n\nOr... just 23.4c per-player per-month.\n\nNot only this, but the machines have plenty of CPU left to perform more\ncomplicated player simulations:\n\nVerdict: DEFINITELY POSSIBLE.\n\n## World Servers\n\nFirst person shooter style player simulation with client side prediction at 1M\nplayers is solved, but each client is only receiving their own player state\nback from the player server. To actually see, and interact, with other\nplayers, we need a new concept, a world server.\n\nOnce again, we can't just have one million players on one server, so we need\nto break the world up in some way that makes sense for the game. Perhaps it's\na large open world and the terrain is broken up into a grid, with each grid\nsquare being a world server. Maybe the world is unevenly populated and\ndynamically adjusting voronoi regions make more sense. Alternatively, it could\nbe sections of a a large underground structure, or even cities, planets or\nsolar systems in some sort of space game. Maybe the distribution has nothing\nto do with physical location.\n\nThe details don't really matter, but the important thing is we need to\ndistribute the load of players interacting with other objects into the world\nacross a number of servers somehow, and we need to make sure players are\nevenly distributed and don't all clump up on the same world server.\n\nFor simplicity let's go with the grid approach. Assume that design has a\nsolution for keeping players fairly evenly distributed around the world. We\nhave a 100km by 100km world and each grid cell is 10km squared. You should be\nable to do an amazing persistent survival FPS like DayZ in such a world, but\nin a secure way without trusting the client. At any time, you should expect to\nhave approximately 10,000 players per-world server.\n\nEach world server could be a 32 CPU bare metal machine with a 10G NIC, like\nthe player servers, so they will each cost $1,870 USD per-month. For a 10x10\ngrid of 1km squared we need 100 world servers, so this costs $187,000 USD per-\nmonth, or 18.7c per-player, per-month. Add this to the player server cost, and\nwe have a total cost of 42.1c per-player per-month. Not bad.\n\nHow does this all work? First, the player servers would need to regularly\nupdate the world server for each player with the state of the player. The good\nnews is that while the \"deep\" player state sent back to the owning client for\nclient side prediction rollback and re-simulation is 1000 bytes, the \"shallow\"\nplayer state required for interpolation and display in the remote view of a\nplayer is much smaller. Let's assume 100 bytes.\n\nSo 100 times per-second after the player simulation completes, the player\nserver forwards the shallow player state to the world server. We have 10,000\nplayers per-world server, so we can do the math and see that this gives us\n10,000 x 100 x 100 = 100 million bytes per-second, or 800 million bits per-\nsecond, or 800 megabits per-second, let's round up and assume it's 1Gbit/sec.\nThis is easily done.\n\nNow the world server must track a history, let's say for one second history of\nall player states for all players on the world server. This history ring\nbuffer is used for lag compensation and potentially also delta compression. In\na first person shooter, when you shoot another player on your machine, the\nserver simulates the same shot against the historical position of other\nplayers near you, so they match their positions as they were on your client\nwhen you fired your shot. Otherwise you need to lead your shot by the amount\nof latency you have with the server, and this is not usually acceptable in\ntop-tier competitive FPS.\n\nSo let's see if we have enough memory for the player state history. 10,000\nplayers per-world server, and we need to store a history of each player for 1\nsecond at 100HZ. Each player state is 100 bytes, so 100 x 100 x 10,000 =\n100,000,000 bytes of memory required or 100 megabytes. That's nothing. Let's\nstore 10 seconds of history. 1 gigabyte. Extremely doable!\n\nNow we need to see how much bandwidth it will cost to send the player down\nfrom the world server directly down to the client (we can spoof the packet in\nXDP so it appears that it comes from the player server address, so we don't\nhave NAT issues).\n\n10,000 players with 100 byte state each, and we have 1Gbit/sec down for player\nstate. This is probably a bit high for a game shipping today (to put it\nmildly), but with delta compression let's assume we can get on average an\norder of magnitude reduction in snapshot packet sizes, so we can get it down\nto 100mbps.\n\nAnd surprisingly... it's now quite possible. It's still definitely on the\nupper end of bandwidth to be sure, this game would be restricted to people\nwith 100mbps internet connections and above, but according according to\nNeilsen's Law, bandwidth available for high end connections increases each\nyear by 50% CAGR, so in 10 years you should have around 57 times the bandwidth\nyou have today.\n\nVerdict: DEFINITELY POSSIBLE.\n\n## A Critical Missing Component\n\nSo far we have solved both for player simulations with FPS style client side\nprediction and rollback, and we can see 10,000 other players near us \u2013 all\nwith a reasonable cost and requiring 1mbps up and 100mbps down to synchronize\nplayer inputs, predicted player state, and the state of other players.\n\nBut something is missing. How do players interact with each other?\n\nTypically a first person shooter would simulate the player on the server, see\nthe \"fire\" input held down during a player simulation step and shoot out a\nraycast in the view direction. This raycast would be tested against other\nplayers in the world and the first object in the world hit by that raycast\nwould have damage applied to it. Then the next player state packet sent back\nto the client that got hit would include health of zero and would trigger a\ndeath state. Congratulations. You're dead. Cue the kill replay.\n\nBut now when the player simulation is updated on a player server, in all\nlikelihood the other players that are physically near it in the game world are\nnot on the same player server. Player servers are simply load balanced \u2013\nevenly distributing players across a number of player servers, not assigned\nplayers to player servers according to where they physically are in the world.\n\nSo we need a way for players to interact with each other, even though players\nare on different machines. And this is where the world server steps in.\n\nThe world server has a complete historical record of all other player states\ngoing back one second. If the player simulation can call out asynchronously\ncall out to one or more world servers \"raycast and find first hit\", and then\n\"apply damage to <object id>\" then I think we can actually make this whole\nthing work.\n\nSo effectively what we have now is something that looks, amazingly, something\nlike, Redis for game servers. A world database representation that scales\nhorizontally which includes a rich set of primitives that allow player servers\nto interact with the world and other objects in it. Like Redis, calls to this\nworld database would be asynchronous, and the player server would need to have\nsome system, where each player has effectively their own goroutine (green\nthread), and periodically this goroutine blocks on async IO and lets other\nplayer goroutines do their work.\n\nThink about it. A player server has just 8,000 players on it. These players\nare distributed across 32 CPUs. Each CPU has to deal with only 250 players\ntotal, and these CPUs have 90% of CPU still available to do work because they\nare otherwise IO bound. Running 250 goroutines per-CPU and blocking on some\nasync calls out to world servers would be incredibly lightweight relative to\neven the most basic implementation of a HTTP web server in Golang. It's\ntotally possible, somebody just needs to make this world database.\n\nVerdict: DEFINITELY POSSIBLE.\n\n## Game State Delivery\n\nEach world server generates a stream of just 100 mbit/sec containing all\nplayer state for 10,000 players @ 100HZ (assuming delta compression against a\ncommon baseline once every second), and we have 100 world servers.\n\n100 mbit x 100 = 10 gbit/sec. This is quite do-able!\n\nBut there are 10,000 players per-world server. And each of these 10,000\nplayers need to receive the same 100mbit/sec stream of data.\n\nIf we had real multicast over the internet, this would be a perfect use case.\nBut there's no such thing. Potentially, a relay network like Network Next or\nSteam Datagram Relay could be the foundation of this packet delivery with 100G\nNICs on relays on major points round the world, or alternatively a traditional\nnetwork with multicast could be built internally with PoPs around the world\nlike Riot Direct.\n\nVerdict: Expensive and probably the limiting factor for the moment. Back of\nthe envelope calculations show that this increases the cost per-player from\ncents to dollars. But if you already had the infrastructure available and\nrunning, it would be possible and cost effective to implement this today.\nCompanies that could do this include Amazon, Google, Valve, Riot, Meta and\nMicrosoft.\n\n## Conclusion\n\nNot only is it possible to create a first person shooter with a million\nplayers in 2024, but the bandwidth sent to each player is within reach of a\nhigh end internet connection: just 100mbps. Bandwidth is growing at 50% CAGR\nso these games will become more widely available over time.\n\nThe key limiting factor is the distribution of game state bandwidth down\nplayers, being limited by lack of support of multicast over the internet. A\ncustom real-time multicast aware packet delivery system would have to be\nimplemented and deployed worldwide. This is likely to be very expensive, and\nnot something that the average game developer can afford to do.\n\nOn the game code side however there is good news. The key missing component is\na flexible, game independent world database that does not exist yet. Something\nlike Redis for game servers. I propose that creating this world database would\nbe a fantastic open source project, and not only this, but that open source is\nprobably the best and fastest way actually get this technology into the hands\nof game developers and get these games made.\n\nWhen this world database is combined with the architecture of player servers\nand world servers, games with 10,000 to 1M players become possible. When a\nmulticast capable real-time relay network emerges and also becomes widely\navailable, these games will become financially viable.\n\nIf you find this concept interesting, please reach out and let me know by\nemailing me at glenn@mas-bandwidth.com\n\nAlternatively, if you have a few billion dollars lying around and you believe\nthat the future with highly persistent networked experience or metaverse with\nmillions of players is as inevitable as I do, you should know that a relay\nnetwork supporting multicast like my startup Network Next is absolutely a\nnecessary component for this, and you should probably consider funding us.\n\n### Published by:\n\n### You might also like...\n\nApr\n\n14\n\n## Writing highly scalable backends in UDP. The solution.\n\nLast week I shared an interview question I've used successfully at Network\nNext. In this article I share the solution.\n\nApr 14, 2024\n\n3 min read\n\nApr\n\n09\n\n## Writing highly scalable backends in UDP\n\nIn this article I share an interview question I've used for years at Network\nNext. How to implement a highly scalable backend in UDP. Can you solve it?\n\nApr 9, 2024\n\n2 min read\n\nApr\n\n01\n\n## XDP for Game Programmers\n\nLearn how you can use XDP/eBPF to get maximum bandwidth for your applications.\n\nApr 1, 2024\n\n15 min read\n\nM\u00e1s Bandwidth \u00a9 2024\n\nPowered by Ghost\n\n", "frontpage": false}
