{"aid": "40153475", "title": "Repairing the Nightmarish Hands Produced by Stable Diffusion", "url": "https://blog.metaphysic.ai/repairing-the-nightmarish-hands-produced-by-stable-diffusion/", "domain": "metaphysic.ai", "votes": 1, "user": "Hard_Space", "posted_at": "2024-04-25 04:38:56", "comments": 0, "source_title": "Repairing the Nightmarish Hands Produced by Stable Diffusion", "source_text": "Repairing the Nightmarish Hands Produced by Stable Diffusion - Metaphysic.ai\n\nSkip to content\n\n## Home\n\n# Repairing the Nightmarish Hands Produced by Stable Diffusion\n\n  * April 25, 2024\n  * 4:36 am\n\n### About the author\n\n#### Martin Anderson\n\nI'm Martin Anderson, a writer occupied exclusively with machine learning,\nartificial intelligence, big data, and closely-related topics, with an\nemphasis on image synthesis, computer vision, and NLP.\n\n  * Author Website\n  * Author Archive\n\n## Share This Post\n\nSince art itself came into existence, gaining domain knowledge of human hands\nhas always been the primary challenge for an artist-in-training.\n\nMiddle, interlocked fingers may be the most difficult subject for even an\naccomplished artist to draw from memory or domain knowledge. Source:\nhttps://www.adobe.com/creativecloud/illustration/discover/how-to-draw-\nhands.html\n\nWith 14 cylindrical finger segments, varying upper and lower base topology,\nthousands of possible pose and lighting permutations, interlocking and other\ntypes of hand>hand relationship, as well as enormous variations across gender\nand ages...instinctive understanding of the geometry of hands is easy for\npeople to develop as a perceptual skill \u2013 but much harder to translate into an\ninterpretive skill, such as drawing or in some way visually depicting hands \u2013\neven in a default or canonical (i.e., neutral or resting) stance.\n\nMichelangelo's 'The Creation of Adam' represents the work of an artist\nrenowned for his skill at depicting this most challenging aspect of human\nanatomy. Source: https://en.wikipedia.org/wiki/File:Michelangelo_-\n_Creation_of_Adam_(cropped).jpg\n\nAs it transpires, it\u2019s a skill that AI also has extraordinary difficulty\nlearning. The open source Latent Diffusion Model (LDM) Stable Diffusion is\nlegendary for its capacity to produce malformed hands and limbs in general,\nfrequently exceeding or under-cutting the requisite number of digits, and\ngenerating Bosch-style figurative nightmares almost as often as a basically\nacceptable hand representation.\n\nStable Diffusion frequently fails to produce even remotely acceptable hand\nrenderings. Source:\nhttps://old.reddit.com/r/StableDiffusion/comments/z3a4ye/prompt_woman_showing_her_hands_on_stable/\n\nAs best can be understood, Stable Diffusion simply cannot get enough data for\nhands, because a standard training image dataset such as LAION (on which the\nmost popular Stable Diffusion models are based) places no particular emphasis\non hands, much in accordance with the general run of photography.\n\nFurther, excepting unusual situations where a photo might have hand emphasis\n(such as the classic Picard \u2018Facepalm\u2019; \u2018talk to the hand\u2019 1990s memes; or\nother situations where the hand gravitates to the face \u2013 which is of far\ngreater cultural attraction than the hand), the hand itself usually occupies a\nrelatively small area of a photo, and frequently in repetitive or banal poses\nthat do little to teach the system of all possible variations in disposition\nand lighting, as well as numerous other factors.\n\nTherefore the problem is at least in part on of distribution and emphasis, and\nin theory can\u2019t be solved in any other way than massively increasing the\nnumber and variety of hand photos in a general-purpose dataset; however, this\nwould undermine other important sub-domains and skew the functionality of the\nmodel in various undesired ways.\n\nInstead, the last 18 months have brought forth a growing number of post facto\nremedies from the research sector, as well as the efforts of many private\npractitioners in developing LoRAs and training checkpoints dedicated to\nproducing or fixing the chronic hands/limbs issues that beset Stable\nDiffusion.\n\nThe 2023 paper HandRefiner: Refining Malformed Hands in Generated Images by\nDiffusion-based Conditional Inpainting offered a post-processing solution to\nmalformed hands in Stable Diffusion. Source:\nhttps://arxiv.org/pdf/2311.17957.pdf\n\nThe latest research project to address this problem comes from China, in a\npaper published on the 22nd of April. Here, the researchers have approached\nthe problem by devising and exploiting multiple datasets covering both good\nand bad hand renderings, in a variety of possible styles; together with a two-\npronged training approach, which lavishly exploits the most powerful GPUs\navailable, the resulting system, the authors claim, improves notably on the\ncurrent state of the art across the widest possible number of domains, from\nphotorealistic through to stylistic.\n\nThe new RHandDS system uses multiple datasets and CGI-based neural interfaces\nto help resolve hand-rendering issues in Stable Diffusion. Source:\nhttps://arxiv.org/pdf/2404.13984.pdf\n\nThe system makes extensive use of the SMPL CGI-based neural interface, in\norder to generate example hands, as well as of the ancillary Stable Diffusion\nsystem ControlNet, in addition to a whole host of secondary libraries and\ncontributing systems.\n\nThe RHanDS system uses ControlNet and extensive curated datasets to produce a\nsystem that can fix poorly-created hand renders.\n\nThe paper states:\n\n\u2018Our [system] ensures the correctness of the structure with a 3D hand model,\nmaintains the correctness of the style with reference hands, and avoids the\nmutual influence that exists between structure and style guidance. As a\nresult, RHanDS can produce plausible hand [images] with the correct structure\nwhile preserving the hand style.\u2019\n\n\u2018...[The system] is a post-processing method that leverages 3D hand mesh as\nthe pixel-level condition to control hand structure. Compared to existing\nmethods, RHanDS performs specifically on the hand region rather than the\nentire image, which facilitates more precise refinement.\n\n\u2018Moreover, RHanDS enhances the perception of hand style, ensuring that the\nstyle of the refined hand seamlessly aligns with that of the original image\nfor a coherent and authentic representation.\u2019\n\nThe new paper is titled RHanDS: Refining Malformed Hands for Generated Images\nwith Decoupled Structure and Style Guidance, and comes from six researchers\nacross Alibaba Group in Beijing, and Xiamen University. The related code and\ndatasets are currently promised to be made available \u2018soon\u2019.\n\n## Method and Data\n\nThe RHanDS framework is diffusion-based, and consists of four modules: a style\nencoder, to accommodate the various domains in which it might be required that\nhands be created; a structure encoder, to preserve the topology and generality\nof the hand/s; a Variational Autoencoder (VAE); and a masking component.\n\nConceptual workflow schema for the RHanDS system.\n\nInput for the framework, as visualized in the conceptual schema above, is an\nautomatically recognized and cropped-out hand, removed from its context in the\noriginal image. The structural reference is a hand-depth image obtained from a\nhand model that can be created with the OpenPose editor, or else automatically\ninferred from the misshapen hand, and then encoded by the structure encoder.\n\nThe OpenPose editor allows the end-user to manipulate hands and create\nreference pose images that can be interpreted into photorealistic or stylistic\nrenderings by Stable Diffusion. Source: https://github.com/ZhUyU1997/open-\npose-editor\n\nThe style reference is drawn from the style of the cropped hand image itself,\nwhile the mask is algorithmically determined by a hand recognition module.\n\nTo decouple the structure and style guidance, in the development of the system\n(i.e., not at inference time) the process is broken up into a first stage\nwhere a UNet and style encoder are trained on multi-style paired hand images.\nIn the second stage, the structure encoder is trained on an analogous multi-\nstyle hand-mesh dataset (i.e., CGI-style imagery). A wide range of potential\nstyles is catered for in these stages:\n\nThe various domains featured across the contributing curated datasets. In the\nrun of contributing data for the training of RHanDS, some of the material was\nhand-generated or automatically generated by the authors, while other examples\nwere taken from existing datasets.\n\nThe mask-guided hand repair process is facilitated by a UNet inside the Stable\nDiffusion framework that contains five additional channels: four for the\nencoded masked image, and one for the actual mask itself. During the denoising\nprocess, the mask image remains unchanged while the decoder reconstitutes the\nimage from the amended latent code.\n\nThe authors note that prior solutions to hand issues have limited ability to\ncope with stylized types of generation, and generalize poorly in styles such\nas anime, and certain painting styles:\n\nFrom the paper, a comparison with the prior HandRefiner framework, compared to\nresults obtained by the new method.\n\nThis is solved in RHanDS by encoding style directly into the Unet, so that a\nstyle embedding can be exploited at inference time.\n\nFirst stage training for the RHanDS system, where the style encoder and Unet\nare jointly trained on variegated styles of hand images.\n\nA diverse variety of image styles are used for this joint training (see\nbelow), so that the system does not fixate solely on photorealistic output.\n\nThe authors observe that their use of CLIP image encoder to extract a style\nfrom the input image operates in a similar way to the IP-Adaptor Stable\nDiffusion ancillary system. Essentially the CLIP encoder operates over a\nlinear projection network (i.e., a network that concatenates the sum of\nfeatures obtained during the process up to this point), and directly passes\nthe style embedding on, having stripped out CLIP\u2019s text facets.\n\nThe second of the two phases, after Style Guidance, is Structure Guidance.\nHere the MANO CGI-based framework, a collaboration between Body Labs Inc, and\nthe Max Planck Institute, is used to represent an anatomically correct form\nand pose for hands.\n\nPose blendshapes in the MANO system. Source:\nhttps://dl.acm.org/doi/pdf/10.1145/3130800.3130883\n\nThe authors observe that the aforementioned OpenPose editor can also\noptionally be used for this purpose.\n\nRHanDS follows the methodology of the prior framework HandRefiner (also the\nprimary combatant in the paper\u2019s experiments section, see below), in rendering\na hand depth image from a hand mesh (i.e., an inferred CGI-style mesh) as a\nstructure reference.\n\nExamples from the original HandRefiner paper, with indications of grayscale\ndepth images on the left. Source: https://arxiv.org/pdf/2311.17957.pdf\n\nFor the training of the second stage, the RHanDS structure encoder receives\nvariegated styles and hand-depth images in tandem, with the Unet frozen so\nthat the existing capability for style guidance is not affected by this work\non structure.\n\nSchema for the second phase of training, concentrating on structure, with the\nexisting style domain knowledge frozen and preserved as the structure-based\nweights develop.\n\n### Datasets\n\nUnderpinning these phases are the diverse datasets curated and partially\ncreated by the authors. The three resulting datasets are Multi-Style Hand,\nMulti-Style Hand Mesh, and Multi-Style Malformed Hand Mesh.\n\nExample images from the three datasets created for RHanDS.\n\nThe Multi-Style Paired Hand Dataset consists of 517,096 hand pairs generated\nby the SMPL-H CGI mesh system, titled Embodied Hands, with body poses\norchestrated by VPoser, the body pose prior for the earlier SMPL-X system.\n\nTo extract the desired portions of the images, MMPose is used for the\ndetection of general human pose, and YOLOV8 for the detection of human hands\nwithin the resulting synthesized images. All the hands in this dataset are\nfrom the same person, the authors state.\n\nHand gesture recognition under YOLOV8. Source:\nhttps://pyimagesearch.com/2023/05/15/hand-gesture-recognition-with-yolov8-on-\noak-d-in-near-real-time/\n\nFor the multi-style hand mesh dataset, seven different style categories were\ncreated, and 8,000 hand-mesh couplets generated for each chosen style.\n\nThe paper states*:\n\n\u2018[We] crop out the hand region from the full human mesh and use a camera with\na field-of-view of 45 degrees to render the hand depth images with and without\nan arm, respectively. Next, We extract canny images from the hand depth images\nwith an arm and feed them into pre-trained canny and depth [ControlNet] to\nguide the structure generation.\n\n\u2018To synthesize hand images of various styles, we manually customize the text\nprompt for each style to guide the generation process to match each target\nstyle. We finally synthesized 10K images for a totaling 7 styles (natural\nhuman, oil painting, watercolor, cartoon, sculpture, digital art, garage kit)\nand filtered out 20% images for each style using the [Mean Per Joint Position\nError (MPJPE)] metric...\u2019\n\nDiverse styles from the initial multi-style hand mesh dataset.\n\nFor the multi-style malformed hand-mesh dataset, the authors curated images\nfrom the Human-Art dataset, and used SDEdit and Stable Diffusion to regenerate\nthe cropped hand regions.\n\nExamples from the Human-Art dataset, used to populate a sub-set for RHanDS.\nSource: https://idea-research.github.io/HumanArt/\n\nThe style reference, in this case, is obtained from features derived from the\ngenerated images, while the structure reference is obtained with the use of\nthe MobRecon hand-mesh reconstruction system.\n\nThe MobRecon hand-mesh reconstruction system in action. Source:\nhttps://github.com/SeanChenxy/HandMesh\n\nAfter removing reconstruction failures (though it is not specified whether or\nnot this was done manually), the dataset was left with 1440 pairs, including\nnatural styles and thirteen categories of artificial styles.\n\n## Tests\n\nTraining for the two stages of the system was resource-intensive: for the\nmulti-style paired dataset, the Unet was trained for 15,000 iterations, at a\nlearning rate of 1e-5 (the lowest and finest-grained practicable rate). The\nbatch size was a formidably high 256, spread across 8 NVIDIA A100 GPUs (though\nthe paper does not specify whether each GPU was a 40GB or 80GB VRAM model).\n\nHand images were resized to 512x512px, and the style images to 224x224px, with\nstandard data augmentation of horizontal and vertical flipping.\n\nFor the training of the second stage, the multi-style hand-mesh dataset was\ncombined with material from the Static Gestures dataset for 15,000 iterations,\nthis time at a learning rate of 2e-5, but once again with a 256 batch size\nacross the same configuration of A100 GPUs.\n\nSamples from the Static Gestures dataset, used in the second phase of\ntraining. Source: https://synthesis.ai/static-gestures-dataset/\n\nEvaluation metrics, besides the previously-quoted MPJPE, consisted of Fr\u00e9chet\nInception Distance (FID) and style loss.\n\nRHanDS was evaluated across two datasets proposed by the prior HandRefiner\nproject: Text2Image, which contains 12,000 images generated with text\ndescriptions from the HAnd Gesture Recognition Image Dataset (HAGRID); and\nImage2Image, which includes 2,000 images from HAGRID.\n\nExamples from HAGRID. Source: https://arxiv.org/pdf/2206.08219\n\nHandRefiner was used to refine hands on the RHanDS multi-style malformed hand\ndataset. Repeating the earlier image, we can see the difference in performance\nbetween HandRefiner and RHanDS:\n\nThe paper states:\n\n\u2018[The] hands refined by HandRefiner are almost fixed in style, which is\ndifferent from the original hand style. The quantitative experiments in [the\ntable below] show that our RHanDS outperforms HandRefiner in all metrics in\nterms of the style and structure of refined hands.\u2019\n\nQuantitative metric results comparing HandRefiner to RHanDS.\n\nThe authors also conducted qualitative tests to discern the extent to which\nRHanDS can preserve style and structure in tandem:\n\nHands generated under differing style and structure references.\n\nHere the authors state:\n\n\u2018During inference, we mask the entire image to achieve the more intuitive\nresult in [the image above]. Since the style and structure guidance are\ndecoupled during training, we can generate hands with specified structures\nunder any style reference without worrying about structure leakage.\u2019\n\nA user study was additionally conducted for subjective comparison of results\nacross the two systems, with 173 image pairs taken from the multi-style\nmalformed hand dataset, across diverse categories visualized in the graph\nbelow. Users were asked to choose a preferred hand image, featuring better\nstructure and style consistency, across results from the competing frameworks.\n\nOverwhelming favor for RHanDS in the user study.\n\nHere the researchers comment:\n\n\u2018Only for one extreme style of shadow play, both [HandRefiner] and RHanDS\nfailed to refine malformed hands with appropriate style and structure, and\nmost users select \u201cnone\u201d (1, 0, 9). Despite that, users prefer our RHanDS over\nmost other styles.\n\n\u2018These results validate the conclusion that our method can refine malformed\nhands with more style consistency and structure quality.\u2019\n\n## Conclusion\n\nThough the results from RHanDS stand out in the recent crop of hand-\nimprovement systems, the architecture is complex and tortuous, and the\ntraining resources required are considerable. We can only hope that further\ninvestigation of the internal semantic challenge that hands present to LDMs\nwill eventually unearth a more elegant and refined solution, perhaps even at\ntraining time of the base models involved.\n\n* My substitution of hyperlinks for the authors\u2019 inline citations, and additional hyperlinks as necessary (this paper retains a lot of weight in the closing supplementary section, and therefore is not the most linear read)\n\nRETURN TO METAPHYSIC BLOG HOME\n\nPrevPreviousMetaphysic Joins Thorn and All Tech Is Human to Enact Strong Child\nSafety Commitments for Generative AI\n\n## More To Explore\n\nAI ML DL\n\n### Repairing the Nightmarish Hands Produced by Stable Diffusion\n\nStable Diffusion has captured the imagination of the world since its release\nin 2022, but retains a notable difficulty in rendering human hands \u2013 one of\nthe most difficult anatomical challenges also for human artists. A new wave of\n\u2018hand repair\u2019 architectures is appearing in the literature of late, the most\nrecent of which is this complex but effective new post-processing framework\nfrom China.\n\nMartin Anderson April 25, 2024\n\nEducation\n\n### Metaphysic Joins Thorn and All Tech Is Human to Enact Strong Child Safety\nCommitments for Generative AI\n\nMetaphysic, alongside industry leaders such as Amazon, Anthropic, Civitai,\nGoogle, Meta, Microsoft, Mistral AI, OpenAI, and Stability AI, has committed\nto implementing robust child safety measures in the development, deployment,\nand maintenance of generative AI technologies. This initiative, led by Thorn,\na nonprofit dedicated to defending children from sexual abuse, and All Tech Is\nHuman, an organization dedicated to collectively tackling tech and society\u2019s\ncomplex problems, aims to mitigate the risks generative AI poses to children.\n\nMetaphysic April 23, 2024\n\n\u201c\n\n## It is the mark of an educated mind to be able to entertain a thought\nwithout accepting it.\n\nAristotle\n\nCopyright \u00a9 2023. All rights reserved. Privacy Policy\n\n### Quick Links\n\n  * Home\n  * Every Anyone\n  * Synthetic Futures\n\n### Connect with us\n\n  * Discord\n  * Tiktok\n  * Twitter\n  * Youtube\n  * Instagram\n  * Github\n  * Linkedin\n\n### Contact Info\n\n  * info@metaphysic.ai\n  * press@metaphysic.ai\n\n", "frontpage": false}
