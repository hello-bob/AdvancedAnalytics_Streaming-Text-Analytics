{"aid": "40080180", "title": "Llama.cpp Working on Support for Llama3", "url": "https://github.com/ggerganov/llama.cpp/pull/6745", "domain": "github.com/ggerganov", "votes": 4, "user": "theolivenbaum", "posted_at": "2024-04-18 20:06:35", "comments": 0, "source_title": "Support Llama 3 conversion by pcuenca \u00b7 Pull Request #6745 \u00b7 ggerganov/llama.cpp", "source_text": "Support Llama 3 conversion by pcuenca \u00b7 Pull Request #6745 \u00b7\nggerganov/llama.cpp \u00b7 GitHub\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nggerganov / llama.cpp Public\n\n  * Notifications\n  * Fork 7.9k\n  * Star 55.7k\n\nJump to bottom\n\n# Support Llama 3 conversion #6745\n\nOpen\n\npcuenca wants to merge 2 commits into ggerganov:master\n\nfrom pcuenca:llama3-conversion\n\nOpen\n\n# Support Llama 3 conversion #6745\n\npcuenca wants to merge 2 commits into ggerganov:master from\npcuenca:llama3-conversion\n\n+25 \u221210\n\n## Conversation\n\nContributor\n\n###\n\npcuenca commented Apr 18, 2024\n\nThe tokenizer is BPE.\n\nSupport Llama 3 conversion\n\nd79ab10\n\n    \n    \n    The tokenizer is BPE.\n\n###\n\nosanseviero commented Apr 18, 2024\n\nWhat a \ud83d\udc10  \n---  \n  \nstyle\n\n112c4c4\n\n###\n\nJosh-XT commented Apr 18, 2024\n\nWhat a champion lol. PR open within 30 minutes of model release.  \n---  \n  \nContributor\n\n###\n\nm18coppola commented Apr 18, 2024\n\nDoesn't seem that the eos_token is working with either of the convert scripts\nin this PR  \n---  \n  \n###\n\nUSBhost commented Apr 18, 2024 \u2022\n\nI can't convert 70b on thisEDIT: run with \"--vocab-type bpe\"  \n---  \n  \n###\n\nmchiang0610 commented Apr 18, 2024 \u2022\n\nThis is what we did to get the model out -- it doesn't seem like the special\ntokens are added properly.We are looking deeper for further improvements /\nfixes.  \n---  \n  \nContributor Author\n\n###\n\npcuenca commented Apr 18, 2024\n\n> I can't convert 70b on this\n\n@USBhost did you try with convert-hf-to-gguf.py?  \n---  \n  \nContributor\n\n###\n\njxy commented Apr 18, 2024\n\nThe instruct models need the tokenizer.ggml.eos_token_id to be 128009, or\n<|eot_id|>.  \n---  \n  \n###\n\nUSBhost commented Apr 18, 2024\n\n> > I can't convert 70b on this\n>\n> @USBhost did you try with convert-hf-to-gguf.py?\n    \n    \n    python convert-hf-to-gguf.py /mnt/36TB/AI/Meta-Llama-3-70B/ --outtype f16 Loading model: Meta-Llama-3-70B gguf: This GGUF file is for Little Endian only Set model parameters gguf: context length = 8192 gguf: embedding length = 8192 gguf: feed forward length = 28672 gguf: head count = 64 gguf: key-value head count = 8 gguf: rope theta = 500000.0 gguf: rms norm epsilon = 1e-05 gguf: file type = 1 Set model tokenizer Traceback (most recent call last): File \"/home/usbhost/llama.cpp/convert-hf-to-gguf.py\", line 1302, in set_vocab self. _set_vocab_sentencepiece() File \"/home/usbhost/llama.cpp/convert-hf-to-gguf.py\", line 330, in _set_vocab_sentencepiece raise FileNotFoundError(f\"File not found: {tokenizer_path}\") FileNotFoundError: File not found: /mnt/36TB/AI/Meta-Llama-3-70B/tokenizer.model During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/home/usbhost/llama.cpp/convert-hf-to-gguf.py\", line 2736, in <module> main() File \"/home/usbhost/llama.cpp/convert-hf-to-gguf.py\", line 2723, in main model_instance.set_vocab() File \"/home/usbhost/llama.cpp/convert-hf-to-gguf.py\", line 1305, in set_vocab self._set_vocab_llama_hf() File \"/home/usbhost/llama.cpp/convert-hf-to-gguf.py\", line 377, in _set_vocab_llama_hf vocab = LlamaHfVocab(self.dir_model) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/home/usbhost/llama.cpp/convert.py\", line 539, in __init__ raise FileNotFoundError('Cannot find Llama BPE tokenizer') FileNotFoundError: Cannot find Llama BPE tokenizer\n\nWhen I add https://huggingface.co/meta-llama/Meta-\nLlama-3-70B/blob/main/original/tokenizer.model I get the same error as on\nconvert.py  \n---  \n  \nContributor Author\n\n###\n\npcuenca commented Apr 18, 2024\n\n> Doesn't seem that the eos_token is working with either of the convert\n> scripts in this PR\n\n@m18coppola the instruct models use two different EOS tokens: the standard one\n(<|end_of_text|>), and a second one that signals the end of the assistant turn\n(<|eot_id|>). Generation must stop when either one is encountered.I'm not sure\nhow to replicate this behaviour yet. The best solution would be to use a list\nof eos/stop tokens, but I don't know how to do it, any suggestions on where to\nlook?Another idea would be to use <|eot_id|> (the assistant finalization\ntoken) as the only EOS when converting an instruct model, and <|end_of_text|>\nwhen converting a pre-trained model.  \n---  \n  \n###\n\nmchiang0610 commented Apr 18, 2024\n\n@pcuenca for the changes:\"special\": false on <|start_header_id|>\n<|end_header_id|> <|eot_id|>  \n---  \n  \nContributor Author\n\n###\n\npcuenca commented Apr 18, 2024\n\n> The instruct models need the tokenizer.ggml.eos_token_id to be 128009, or\n> <|eot_id|>.\n\n@jxy Our comments were sent at the same time :) Yes, that's one of the\nsolutions I mentioned, but I'm not sure it will work consistently, I've seen\nmodels that use various terminators depending on context.We can try it out\nthough, I'll take a look.  \n---  \n  \n###\n\nUSBhost commented Apr 18, 2024\n\nSorry lads I had to run with --vocab-type bpe So automatic detection is\nbroken.  \n---  \n  \narch-btw mentioned this pull request Apr 18, 2024\n\nllama3 family support #6747\n\nOpen\n\n###\n\nddh0 commented Apr 18, 2024 \u2022\n\n> > The instruct models need the tokenizer.ggml.eos_token_id to be 128009, or\n> <|eot_id|>.\n>\n> @jxy Our comments were sent at the same time :) Yes, that's one of the\n> solutions I mentioned, but I'm not sure it will work consistently, I've seen\n> models that use various terminators depending on context.We can try it out\n> though, I'll take a look.\n\nFrom the model card on HF:\n\n    \n    \n    terminators = [ tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\") ]\n\nNot sure if this is helpful or not \ud83d\ude05 but thought I might as well mention it.  \n---  \n  \nContributor\n\n###\n\njxy commented Apr 18, 2024\n\nIt seems the model generates <|eot_id|> with the official chat template.\nOtherwise it may generate <|end_of_text|>.  \n---  \n  \nContributor\n\n###\n\nteleprint-me commented Apr 18, 2024 \u2022\n\nIt's always the tokenizer. The tokenizers are always a mess.Special tokens\napply to the instruct tuned model.The ChatFormat class in the source code\nshows how they implemented it.The encode_header is interesting. That's a new\none? Then they have encode_message and encode_dialog_prompt.They're using\ntiktoken for the Tokenizer.Lots of new special tokens.\n\n    \n    \n    special_tokens = [ \"<|begin_of_text|>\", \"<|end_of_text|>\", \"<|reserved_special_token_0|>\", \"<|reserved_special_token_1|>\", \"<|reserved_special_token_2|>\", \"<|reserved_special_token_3|>\", \"<|start_header_id|>\", \"<|end_header_id|>\", \"<|reserved_special_token_4|>\", \"<|eot_id|>\", # end of turn ] + [ f\"<|reserved_special_token_{i}|>\" for i in range(5, self.num_reserved_special_tokens - 5) ]\n\nThis should be interesting (and not in a fun way either). This is gonna create\nanother level of complexity.  \n---  \n  \nContributor\n\n###\n\nbullno1 commented Apr 18, 2024 \u2022\n\n> > Doesn't seem that the eos_token is working with either of the convert\n> scripts in this PR\n>\n> @m18coppola the instruct models use two different EOS tokens: the standard\n> one (<|end_of_text|>), and a second one that signals the end of the\n> assistant turn (<|eot_id|>). Generation must stop when either one is\n> encountered.I'm not sure how to replicate this behaviour yet. The best\n> solution would be to use a list of eos/stop tokens, but I don't know how to\n> do it, any suggestions on where to look?Another idea would be to use\n> <|eot_id|> (the assistant finalization token) as the only EOS when\n> converting an instruct model, and <|end_of_text|> when converting a pre-\n> trained model.\n\nInstead of remapping which creates more confusion, just update the generation\ncode to stop on eot_id. It's like one line of config/code change.At least from\nmy cursory tests, all special texts are tokenized properly out of the box.I\ndid a bit of testing and chat works.  \n---  \n  \nContributor\n\n###\n\nteleprint-me commented Apr 18, 2024 \u2022\n\nOkay, it's in there.\n\n    \n    \n    # BOS / EOS token IDs self.bos_id: int = self.special_tokens[\"<|begin_of_text|>\"] self.eos_id: int = self.special_tokens[\"<|end_of_text|>\"] self.pad_id: int = -1 self.stop_tokens = { self.special_tokens[\"<|end_of_text|>\"], self.special_tokens[\"<|eot_id|>\"], }\n\n@pcuenca The list of stop tokens are usually added during inference. The chat\ntemplates have been embedded lately into llama.cpp. Haven't gotten that far\nyet, though.I think I get it now.Completions:\n\n    \n    \n    <|end_of_text|>\n\nInstructions:\n\n    \n    \n    <|eot_id|>\n\nThat's how I'm interpreting it at the moment. Feel free to correct me.  \n---  \n  \nContributor\n\n###\n\nbullno1 commented Apr 18, 2024 \u2022\n\n@teleprint-me Yep, you just have to stop on eot_id instead which is:\n128009.You can use the tokenization tool to test:\nhttps://github.com/ggerganov/llama.cpp/blob/master/examples/tokenize/tokenize.cpp<|begin_of_text|>,\n<|start_header_id|> , <|end_header_id|>, <|eot_id|> are all mapped correctly.  \n---  \n  \nSign up for free to join this conversation on GitHub. Already have an account?\nSign in to comment\n\nLabels\n\nNone yet\n\n10 participants\n\nAdd this suggestion to a batch that can be applied as a single commit. This\nsuggestion is invalid because no changes were made to the code. Suggestions\ncannot be applied while the pull request is closed. Suggestions cannot be\napplied while viewing a subset of changes. Only one suggestion per line can be\napplied in a batch. Add this suggestion to a batch that can be applied as a\nsingle commit. Applying suggestions on deleted lines is not supported. You\nmust change the existing code in this line in order to create a valid\nsuggestion. Outdated suggestions cannot be applied. This suggestion has been\napplied or marked resolved. Suggestions cannot be applied from pending\nreviews. Suggestions cannot be applied on multi-line comments. Suggestions\ncannot be applied while the pull request is queued to merge. Suggestion cannot\nbe applied right now. Please check back later.\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": true}
