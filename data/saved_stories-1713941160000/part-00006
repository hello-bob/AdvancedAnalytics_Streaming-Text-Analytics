{"aid": "40138456", "title": "Scaling open-source application software to 100K+ Docker pulls, 44K+ K8s deploys", "url": "https://plane.so/blog/streamlining-self-hosting-managing-100k-docker-44000-kubernetes-deploys-ease", "domain": "plane.so", "votes": 2, "user": "viharkurama", "posted_at": "2024-04-23 23:13:35", "comments": 0, "source_title": "How we made self-hosting Plane a breeze for 100,000+ Docker + 44,000 Kubernetes deploys", "source_text": "How we made self-hosting Plane a breeze for 100,000+ Docker + 44,000\nKubernetes deploys\n\nAll-you-can-use project management software for a one-time price.\n\n{Pre-order and get $100 off.}\n\nAll-you-can-use project management software for a one-time price.\n\n{Pre-order and get $100 off.}\n\nPricingChangelogBlogTalk to Sales\n\nTech\n\n# How we made self-hosting Plane a breeze for 100,000+ Docker + 44,000\nKubernetes deploys\n\nAn early-stage start-up's experiments, failures, and lessons scaling Docker\nand Kubernetes for self-managed instances\n\nManish Gupta and Nikhil Chacko\n\nApril 18, 2024\n\n30 min read\n\nTable of contents\n\nStart-stop-start\n\nExpected trouble\n\nSolve, phase #1: Removing builds\n\nSolve, phase #2: Unblocking asset and file uploads\n\nSolve, phase #3: Upgrades, migrations, and the Plane CLI\n\nSolve, phase #4: Data, one-click install, the Go binary, and our own command-\nline interface\n\nPlane, like any other open-source B2B software, offers both a Cloud hosted\nedition and a self-managed one. While our Cloud users far outnumber our self-\nhosted users, it's nice to see for an early-stage start-up like ours an\nestimated 50,000+ active self-managed Plane instances.\n\nWe don't collect telemetry, so there's no sure-fire way to know the exact\nnumber of active self-hosted Plane instances, but we do some back-of-the-hand\nmath for our best guesstimates.\n\nHowever, our deployment methods weren't what they are today. Docker Compose is\nthe most popular, well-documented, and, one can argue, the simplest of all\ndeployment methods available to self-hosters. Stupidly, it took us more than\nhalf-a-year to get around to its current standards of ease and even more to\nsimplify its use. To have gotten to today is no small testimonial to our\ncommunity that stayed with us in upgrading, migrating, and templatizing their\ndeployments while giving us critical feedback.\n\nWe intend for this post to be educative to other open-source early-stage\nstart-ups for the merits, pitfalls, and lessons from our choices, both in the\npast and right now.\n\n## Start-stop-start\n\nPlane is a Django-NextJS application. Our Cloud runs the Django backend\nindependently of the NextJS front-end on two different domains, typical of\ncloud-hosted software.\n\nWe naively followed that for the first few versions of our self-hosted\nedition. The front-end talked to the back-end via an .env variable called\nNEXT_PUBLIC_API_BASE_URL. In sticking to an already bad choice, we also made\nthat variable and others like it, all beginning with the prefix NEXT_PUBLIC,\nbuild-time variables. Anyone who wanted to self-host Plane had to,\n\n  * clone our repo\n  * find and edit three of the several .env files to specify values for\n\n    * NEXT_PUBLIC_API_BASE_URL\n    * SECRET_KEY For keeping your usernames and passwords salted in your self-hosted instance\n    * WEB_URL For redirecting from emails to the domain your app was hosted on\n  * build images locally with docker-compose\n\nThere were no pre-built Docker images, no Docker Hub, and no out-of-the-box\nway to run the stack as a cohesive app. It was the Wild-West for our self-\nhosters when open source had moved on long ago to deployment sophistication.\n\n### Expected trouble\n\nMost users didn't care for Docker Hub or pre-built images when they were\ntrying out Plane. They just needed to set values for the three .env variables\nmentioned above and build their images locally. Not the best, but not the\nworst either. Trouble brewed when they were beginning to adopt Plane.\n\nWe have twenty-five .env variables that an admin can set values for. 25\ncustomization options. 25.\n\nIf an admin wanted to change even one of those, they had to,\n\n  * stop the instance\n  * hunt for the variable in one of four .env files\n  * specify values for the variable\n  * rebuild all the containers\n\nAnd they had to do it for every change. Let that sink in for a second.\n\nAs you can imagine, in the first seven days from installation, admins were\nbuilding their images from scratch an average ten times if not more. As you\ncan also imagine, the brickbats we got for this bordered on the poetic. Some\nusers said we didn't know what we were doing\u2014we didn't\u2014while others just said\nwe were intentionally making self-hosting difficult\u2014we weren't.\n\nWhat we learnt painfully\n\n  * The more you sweat in training, the less you bleed in battle. We could have looked at other self-hosted products and their architectures instead of copying from our Cloud's, taking a little more time to ship our self-hosted editions right.\n  * Simplify for your users, not for yourself. We shipped this self-hosted version as a replica of our own Cloud, which was lesser work for us and a lot more for our users. Sounds cliched that we should have simplified for the user, but it's one of our commandments now.\n\n## Solve, phase #1: Removing builds\n\nOur first big hill to scale was taking out builds from the process. The user\nstory went,\n\n> I should be able to download pre-built images and specify .env variable\n> values to get my instance up with docker-compose.\n\nCome February 2023, we were pushing pre-built images to Docker Hub, but with\nNEXT_PUBLIC_API_BASE_URL hard-coded to the value http://localhost. \"But why\",\nyou ask perplexed.\n\nWell, we weren't running automated pipelines to compile images from our\nrepository. We were building them locally and uploading them to Docker Hub.\nBut that didn't matter right then\u2014you will see how it did matter a lot just a\nlittle later\u2014because admins could now download these images\u2014both an all-in-one\nand five separate ones, one per service running inside the Docker network.\nThey just had to change values for NEXT_PUBLIC_API_BASE_URL to set their\nhosted domain. No building images, no rebuilding images.\n\n### All for nothing\n\nBut remember how NEXT_PUBLIC_API_BASE_URL was a build-time variable, not a\nrun-time variable? So, admins edited the .env file to specify its value and\nrealized they had to git clone the repo to build images locally anyway. You\nwould be hard-pressed to find a better example of adding insult to injury if\nyou hunted for one with a vengeance.\n\n### Wake up!\n\nLuckily, not very many people complained\u2014we are signing this off to just the\nbest self-hosters in the world, period\u2014until one day, \u2193 happened.\n\nWe repeat, our users are just the nicest.\n\nFive months from the time we went to Docker Hub, this came as the sign we were\nwaiting for. Docker Hub had to work without shenanigans, so that became our\nbigger hill to scale.\n\nSetting up GitHub Actions was too daunting and time-intensive back then\u2014we\nwere shipping weekly with a team of one back-end engineer\u2014, so the genius\nsolve was to find a way to replace the hard-coded NEXT_PUBLIC_API_BASE_URL\nvalue at service start-up with whatever the admin had specified as its value.\n\n### The faux-clever solve\n\nEnter Cal.com's community with their Bash script! Turns out, Cal.com had a\nsimilar issue and their community that overlaps with our community big time\nhad \u2193 to replace strings in values for .env files.\n\n    \n    \n    FROM=$1 TO=$2 if [ \"${FROM}\" = \"${TO}\" ]; then echo \"Nothing to replace, the value is already set to ${TO}.\" exit 0 fi echo \"Replacing all statically built instances of ${FROM} with ${TO}.\" find apps/web/.next/ apps/web/public -type f | while read file; do sed -i \"s|${FROM}|${TO}|g\" \"${file}\" done\n\nAwesome! All we had to do was change the script for our folder structure and\nset it as the first thing to execute for the front-end container when a self-\nhoster ran docker-compose.\n\n    \n    \n    #!/bin/sh FROM=$1 TO=$2 if [ \"${FROM}\" = \"${TO}\" ]; then echo \"Nothing to replace, the value is already set to ${TO}.\" exit 0 fi echo \"Replacing all statically built instances of ${FROM} with ${TO}.\" find apps/app/.next -type f | while read file; do sed -i \"s|${FROM}|${TO}|g\" \"${file}\" done\n\nThat script ran with the following arguments.\n\n    \n    \n    /usr/local/bin/replace-env-vars.sh \"$BUILT_NEXT_PUBLIC_API_BASE_URL\" \"$NEXT_PUBLIC_API_BASE_URL\"\n\nAnd it worked! It worked like a charm. Or so we thought.\n\n### Chokepoint\n\nHere's the thing about Bash scripts. They stall, fail, and crash when you have\na lot of minified JavaScript files\u2014in our case, hundreds, courtesy NextJS.\nFinding and replacing http://localhost across an average 300 minified JS files\nheld up the front-end from starting for a while. In one instance, as reported\nby an admin, it was thirty minutes before the script exited the run with an\nerror that made no sense. Add to that the case of one invalid character in the\nadmin-specified value for NEXT_PUBLIC_API_BASE_URL and, yep, it spit out an\nunhelpful error.\n\nWhat do you know, it was not the elegant solution we thought it was. \ud83d\ude44\n\n### The true-clever solve\n\nIn fact, it was so simple, we had to swat ourselves on our head for not\nthinking of it first. We had shipped our repo and the Docker Hub images with\nNGINX to limit the number of ports an admin opened to one. Turns out, we never\nused NGINX as a reverse proxy because, well, the front-end ran independently\nand on a different domain from the back-end. \ud83e\udd26\u2642\ufe0f\n\nAll we had to do was run all services in the container on the same domain and\nlet NGINX tell the front-end how to talk to the back-end. Simple!\n\nTook us two months after we shipped the Bash script to get to this, but we\ndid. Starting v0.13, there's no Bash, no replacements, no builds, and no\nchokepoints. Admins specify values for .env files and run docker-compose to\nget their instance running.\n\nPhew!\n\nWhat we learnt even more painfully\n\n  * Duct-tape = unpredictable crashes Short-term solves are good only when you are close to a good mid-term solution. We now err on the side of delays instead of shipping something untested, half-baked, and quick.\n  * What works for others may not for you The Cal.com community found a solution for the Cal.com repo, and the community's usage of that repo. It couldn't have worked for us with our set-up.\n\n## Solve, phase #2: Unblocking asset and file uploads\n\nA typical Plane instance stores and serves a few thousand images and files\nover just a month. Think profile pictures, project backgrounds, and\nscreenshots + files to not just their issues and comments but also their\nPages, a knowledge-capture feature in Plane. On self-hosted instances, this is\nhandled by an S3-like image store called Minio.\n\nTo optimize upload queues and to prevent abuse, we restrict per-file size to\n5MB on the Cloud. Those restrictions don't apply to self-hosted Plane, but we\nship with the 5MB-default that admins can change. Turns out, we accidentally\nlet a restriction of 1MB slip though.\n\n### The culprit was NGINX, not Minio\n\nRemember NGINX as our router of choice for all it surveys inside our Docker\ncontainer? How many of you know NGINX ships with a 1MB file-size limit by\ndefault?\n\nWe are guessing most of you have raised your hands right now, so call us\nignorant and get us a get-out-of-jail-free card. Because we didn't. We just\nnaively thought we had f-ed up our Minio implementation.\n\nSome debugging later, we saw the files didn't even reach the backend that\nwould then talk to Minio to store files.\n\nNot the prettiest error or the most useful\n\nThey were being blocked by NGINX with a nasty error for users on the Plane\ninterface.\n\nCommand-line logs for the install showed the request wasn\u2019t even reaching the\nbackend.\n\nNot the prettiest error or the most useful\n\nThey were being blocked by NGINX with a nasty error for users on the Plane\ninterface.\n\nCommand-line logs for the install showed the request wasn\u2019t even reaching the\nbackend.\n\nNot the prettiest error or the most useful\n\nThe error was reported in v0.7 on May 29 last year and we shipped the fix two\nweeks later.\n\nThat's not the fastest for us, but we had had to swallow bitter pill for\nshipping untested fixes. We weren't playing fast and loose anymore.\n\n### But Minio wasn't far behind\n\nTurns out, bitter pills weren't done with us yet. Minio, an open-source\nalternative to Amazon S3, was going to land us in more trouble.\n\nMinio is great for folks who don't have S3 and want to get started with Plane\nquickly before making .env changes for their infra. So, when a user uploaded\nan avatar or a file to Issue details,\n\n  * it was sent to NGINX\n  * which sent it to the Django backend\n  * which used Django S3 storage, a Django library\n  * that talked to Minio at https://plane-minio:9000\n  * to save the file at http://plane-minio:9000/<filename>\n  * and send that URL back to the front-end\n  * so it showed up on the app's interface\n\nIt worked exactly like that when we tested our Minio implementation. Why\nwouldn't it? After all, we were running the app locally on http://localhost.\n\nMinio, like any other service running as a container in our Docker network,\ncan't be accessed from outside. Users talk to our network via NGINX, which in\nturn talks to the front-end and the back-end. For admins running the app on\nhttps://anysubdomain.anydomain.tld and users accessing it from that address,\nhttp://plane-minio:9000 would be inaccessible, and thus, unusable. And it was\nfor everyone who tried to upload images.\n\nAdd to that the absence of validation for a successful upload to the storage\nserver before rendering the image on the interface and we had led admins and\nusers down a rabbit hole.\n\n> The image was there when uploaded. It isn't there when I switch out from the\n> page to another and come back. What's happening?\n\nTo the technical and curious, we showed the image copied to Issue details on\nthe browser without uploading it to an instance's storage. That's how drag-\nand-drops used to work on Google Drive or Dropbox. You could see the file\nbefore clicking Upload, but refresh that page without that click and you lost\nthat file.\n\n### Reintroducing WEB_URL\n\nIf you didn't look closely at the list of .env variables under Expected\ntrouble in this post, we shipped WEB_URL from the time we walked on to GitHub\nin November 2023. That variable tells the back-end which domain to use for\nredirections to the front-end\u2014useful for link clicks in emails and Slack. It\nis the same domain that the front-end is rendered on and that users access\nPlane from.\n\nInstead of introducing another variabl, we just reintroduced WEB_URL to the\nback-end, specifically to Django Storage, so it could post-process file URLs\nto anysubdomain.anydomain.tld/<filename>.\n\nAdmins were specifying values for WEB_URL anyway, so that worked great.\n\nUntil v0.17, our latest release, we still ship code that defaults to\nhttp://plane-minio:9000 when WEB_URL is not specified. We have an elegant fix\nfor this that we will talk about below.\n\nWhat we learnt embarrassingly\n\n  * Beta-test in the real world Greens all the way when testing internally is a big illusion that will get you shown up when you go live. As we learnt through phase #2, it is much better to let willing users test pre-releases than to roll out early-stage changes to everyone.\n  * Slow down to speed up Our faux pas with NGINX had told us we needed to slow down with the day-to-day to speed up in the mid-term. The mad rush to ship every week gave in to a comprehensive release plan, headed by a new-hire release manager, which helped immensely in phase #3.\n\n## Solve, phase #3: Upgrades, migrations, and the Plane CLI\n\nQuick recap of the road so far\n\n  * Admins didn't need to build images.\n  * They still needed to clone the repo for the .env files we referred to in docker-compose.\n  * They needed to specify .env values in those files\u2014-four of them.\n\nWhile Docker Hub + our repo cloned worked okay for most, there were troubles,\nnot the least of which was just how long it took to get started. Admins didn't\nlike it.\n\nWe weren't trying to make self-hosting harder so we could monetize the Cloud\nbecause we are monetizing self-hosting, too, as you will see in Phase #4\nbelow, but we got the spirit of those comments.\n\n### Removing git clone from the equation\n\nWe started our first closed beta with the lowest hanging fruit, that had the\nmost grunt work for admins\u2014cloning the repo to get four .env files that\ndocker-compose reads values from and and editing those four files to specify\nthose values. To solve for those two, we moved all our variables into a new\ndocker-compose called docker-compose-hub that was available to our beta\ntesters. These testers would simply need to download the images of our\ncontainers from Docker Hub along with the docker-compose-hub.yaml file, edit\nthe file to specify .env values, and run it. No cloning the repo. No hunting\nfor variables in four files.\n\n### Persistent .env values and simplifying docker-compose-hub.yaml's structure\n\nWriting .env variables from the four previous files into docker-compose-hub as\nblocks of text had some merit\u2014admins now used to a sub-optimal structure\nwouldn't come at us with pitchforks for radical changes when this method\nbecame generally available\u2014, but it became clear soon enough our seasoned\ntesters were finding the new structure painful to edit, especially if they\nwere making changes after first set-up.\n\nThere's a whole lot more that couldn't be included because the image would\nbecome illegible.\n\nThe bigger problem was upgrades. A single docker-compose file with .env key-\nvalue pairs meant if we introduced new variables, admins would have to\ndownload that version's docker-compose-hub. If you are thinking, \"Oh, no!\nThat'd remove all custom values with the default ones\", ten points to you. And\nzero to us.\n\nSo, fast-following a week later, we moved all those .env variables out into a\nsingle file called variables.env which was saved as just .env on the host\nmachine and kept a copy of it on docker-compose-hub, too. That way, if an\nadmin accidentally deleted the .env file, they could always recreate it with\ntheir previous values with the copy as a reference.\n\n### Partially automating set-up with setup.sh\n\nBecause we weren't shipping these changes for general availability, we were\nfree to think about the next problem\u2014swapping out manually run docker pull and\nadmin-edited.env values with command-line automation.\n\nEnter setup.sh, a Bash script that admins could download with a simple cURL\ncommand and see a command-line menu.\n\n    \n    \n    $ mkdir plane-selfhost $ cd plane-selfhost $ curl -fsSL -o setup.sh https://raw.githubusercontent.com/makeplane/plane/master/deploy/selfhost/install.sh $ chmod +x setup.sh\n\nBehind the interface, we would still download docker-compose\u2014docker-compose-\nhub was now docker-compose and still in beta, but getting ready to be\ngenerally available\u2014, the images from Docker Hub, and the .env file, but\nadmins wouldn't have to bother with any of that. They would choose Install or\nUpgrade and voila! Everything after that would happen automatically.\n\nUpgrade would pull the updated docker-compose.yaml and variables.env.\n\nInstall would pull images and start services\n\nUpgrade would pull the updated docker-compose.yaml and variables.env.\n\nInstall would pull images and start services\n\nUpgrade would pull the updated docker-compose.yaml and variables.env.\n\nWe included an option to see logs for each service in the Docker network, too.\n\nThis worked beautifully for both new + existing admins and became our first\ngenerally available stable improvement for the community. Configuring .env\nvariables had to be done still with edits to the .env file, but that was a\nlesser problem at this point. We had bigger fish to fry.\n\n### Launching Kubernetes\n\nPlane, by this time, had enough mid-market traction that we started seeing\nrequests for Kubernetes support, with some even looking to the community for\nhelp.\n\nLaunching Kubernetes was a no-brainer for what we had planned next, so we put\nour Helm charts on our site and ArtifactHub simultaneously with default values\nfor Memory, CPU and Replicas variables. The quick move to Kubernetes helped in\ntwo ways.\n\n  1. Mid-market companies started using Kubernetes way more than before.\n  2. We discovered a problem with database migration on Kubernetes that would become a problem, if it wasn't already, on Docker, too.\n\n### Database migration, replicas, and Migrator\n\nAdmins deploying on Kubernetes started reporting what looked like a weird\nproblem.\n\nUsers of self-hosted instances on Kubernetes saw this for a while when it\nlooked like an upgrade had been successful.\n\nThey also reported this happened almost exclusively during upgrades. So\nperplexing. Our upgrades were seamless on Docker and ran something like this.\n\n  * The API service would attempt a migration of the database during an upgrade.\n  * It would run the migration successfully.\n  * The app would come up on the registered domain and users would use the app as usual.\n\nEspecially on fresh installs, API ran these Django migrations without a hitch\nfor all our Docker-based deployments.\n\nTurned out, our default value of 3 for the variable Replicas was not only\ncreating three instances of API, Worker , and Scheduler on Kubernetes\u2014expected\noutcome\u2014but also making all three instances of the API micro-service attempt a\ndatabase migration\u2014unexpected outcome. While one of those three successfully\nlocked on to the migration and ran it until done, the other two kept\nattempting to access the database, failed to do that, crashed, and restarted.\nMaking a bad situation worse was \u2193, a log-line confirmation that told admins\nthe deployment had succeded.\n\nConsequence? The app would continue to show the spinner until the migration\ngot done.\n\n\"Why didn't that happen with Docker and why only Kubernetes-based upgrades\",\nwe hear you asking.\n\nOur docker-compose and variable.env shipped with a default of 1 for Replicas.\nSo, while admins saw \u2193 with fresh installs and patiently waited for the app to\ncome up, they didn't see it with upgrades. There was just one API micro-\nservice running the migration. Same with Kubernetes, although why admins\ndidn't report it for first installs, we don't yet know.\n\nNevertheless, wow, what a reveal!\n\nA good solve had to not just work for Kubernetes but also Docker scaling,\ni.e., when admins changed the value of Replicas to more than 1.\n\nEnter migrator, a separate micro-service and job on Docker and Kubernetes that\nmoved migrations out of API and blocked all replicas of API, Worker, and\nScheduler from starting. migrator would first say \u2193.\n\nand then \u2193.\n\nThe success message \u2193 was true success and admins could ask their users to log\non with confidence.\n\nEven cooler, migrator was a one-and-done job on Kubernetes, so each time you\nupgraded, you would essentially download migrator to remove the previous job\nand run another one-and-done job.\n\nNo more competition to run migrations. No conflicts. Just smooth upgrades all\naround.\n\nWhat we learnt surprisingly\n\n  * Ship incrementally We could have beta-tested [setup.sh](http://setup.sh) and Kubernetes together. Pulling back on our ability to ship a large block of code and releasing smaller enhancements helped gain beta-tester\u2019s confidence and give us the breathing room to act on their feedback.\n  * Shoot for parity, settle for familiarity Our Docker admins didn\u2019t worry too much about scaling. Even if you sign that off to our defaults\u2014replica = 1\u2014, not one of them told us about the problem with migrations. Kubernetes, though, stands for scaling by its nature. So, while we would have loved to set parity between both those methods, shipping K8s with replica= 3 let us stick to familiarity with Kubernetes admins. That it discovered a problem and led to a solution for everyone is a happy byproduct.\n\n## Solve, phase #4: Data, one-click install, the Go binary, and our own\ncommand-line interface\n\n### Back to Docker\n\nThe larger community on Docker hadn't been party to our behind-the-scenes\nexperiments in Phase #3, so most of them just chose Upgrade whereas all new\nadmins chose Install. By any standard, Phase #3 was going superbly, what with\nour beta-testing admins loving the installer Bash and the command-line\nconfigure menu. That's when we launched our first pricing experiment\u2014a bespoke\nPlane implementation and dedicated support for self-hosted users for a flat\nper-user-per month price for the entire year.\n\n### Plane One and the path to COSS sustainability\n\nBefore that experiment, we had offered the Community + the Free Cloud editions\nto all our users. A significant chunk\u2014we won't disclose those numbers right\nnow\u2014had asked us about pricing and Plane's viability as a long-term solution\nfor org-wide project management. The experiment we ran from January this year\ngot enough traction to signal a set of unmet needs that admins struggled with.\n\n  1. Offer quickstarts for self-hosters\n  2. Regular, even weekly security and performance updates\n  3. Support for more hosting services like Railway\n  4. Monitoring and logs\n  5. Back-ups and recovery\n\nThat started us on the first step toward making the last on the list\npossible\u2014back-ups and recovery\u2014and what would mark the inception of Plane One.\n\n### Making the database more accessible\n\nDespite the success of Phase #3, one evolutionary problem persisted\u2014Docker\nVolumes and how difficult it was to access, back-up, and recover this data for\nnon-native Docker users.\n\nIf the instance crashed\u2014which it sometimes did\u2014, the data would be lost to\nsuch admins. Even for non-disaster needs like expanding the storage, Docker\nVolume posed a challenge.\n\nPhase #4 started with moving the data out of Docker Volume and mapping it to\nthe host machine that admins were installing Plane on. This had two immediate\nbenefits.\n\n  * Admins could now access and back-up the data without being a Docker expert.\n  * They could increase storage over time because they controlled the host machine's configs.\n\nAutomatic, scheduled backups and choice of storage are all in the works in\nPhase #4, but we are getting ahead of ourselves.\n\n### Fully automating set-up with setup.sh\n\nWhile the Bash-based setup.sh worked well for most, admins looking at wider\nadoption of Plane in their orgs called out two time-taking problems.\n\n  1. Fresh installs were still a function of admin input.\n  2. .env values were still a function of edits to the .env file.\n\nTo make quickstarts possible, we extended setup.sh to work with all Linux-\nflavored AArch64 and x64 machines and took out admin-input steps.\n\nWith just a single-line command\u2014curl -fsSL https://raw.githubusercontent.com/makeplane/plane/master/deploy/1-click/install.sh | sh -\u2014admins could now sit back and watch the script,\n\n  * check for a previous installation of Plane\n  * the architecture of the machine\n  * download the images, docker-compose, and variable.env\n  * ask for a domain name to host Plane,\n  * start services, and\n  * show a URL for accessing the app\n\nThere was a --help operator, too that'd show a more helpful menu of options.\n\nSelecting --configure would bring up a far more intuitive configuration step-\nform that was such a big and good departure from manaully specifying .env\nvalues in a file.\n\nTypefully-like steps, easy input of values, nice little Save button. Awesome!\n\n> We had made quickstart real for our Community and in record time!\n\n### A new CLI and the Go binary\n\nFor paid self-hosted instances, we had three non-negotiables.\n\n  * Validate the license at first install and then periodically to insure one license was used with one domain\n  * Download the license, docker-compose, variable.env and the container images from a private repository instead of Docker Hub\n  * Insure high security for all of it\n\nThe last successful iteration of setup.sh, despite being a really cool bit of\nwork, didn't work for how open it was\u2014it was a plain-text, editable file after\nall\u2014and just how vulnerable Bash generally is. Also, it wasn't the most\nintuitive and admin-friendly. Our licensing server had to be untouchable,\nusable, and pretty.\n\nIt was time to ditch Bash for an alternative that could turn our script into a\nsecure binary.\n\nWe first evaluated popular CLI-builders like Python, NodeJS, and Rust.\n\n  * Python was our top choice because we have written in Python all our lives, but it is plain-text editable and thus, not very different from Bash.\n  * NodeJS was our next top choice. It was easy enough to compile TypeScript into an Electron app for all user operating systems, but the binary would be heavy and a memory hog.\n  * Rust was in consideration for thirty seconds before we saw the steep learning curve for all of us.\n\nWith that, we turned our research to Go and quickly saw how well it fit.\n\n  * docker-compose v2 is written in Go, so the compatibility of Go libraries with Docker is as good as it gets.\n  * Go talks nicely with the Docker daemon for downloading .tar.gz files from a private server.\n  * It manages our docker-compose operators and associated services like start, stop, and restart superbly.\n  * It can stream Docker logs to the licensing server for troubleshooting and support.\n\nEarly trials with Go were encouraging enough for us to fully commit to it. By\nour final internal demo, Go had beaten all our expectations for a secure,\nusable, pretty CLI that had wings for the future.\n\n### Bringing it all to Plane One\n\nThis tying-the-loop section should have been called, \"Ta da!\" but well, SEO\ndoesn't like that. See just how easy it is to install Plane One for yourself.\n\nWhat we learnt humbly\n\n  * Abstraction is a journey Before you pat us on our backs, let us be the first to admit there\u2019s a lot more we can do with the set-up and our overall deployment landscape, some of which is in the final section below. Abstracting anything away from the user isn\u2019t a sudden goal you can achieve. It took us a year to get here and perhaps it will take us a year more to look back with new learnings, but we will continue to abstract away to the extent possible without trading off controls and smart defaults.\n  * Open source engineering isn\u2019t COSS engineering\n  * Our journey may have begun with a GitHub repo, but we have always been a COSS company and are on the path to engineering Plane for self-hosted scale as we offer a parallel Cloud edition. That path, irrespective of the type of COSS software, brings some DevOps considerations into focus quicker than with community-supported open-source.\n\n    * Should you prioritize Cloud over self-hosted?\n    * How much parity is too much parity?\n    * How do you maintain one very large deployment\u2014the Cloud\u2014while shipping features for tens of thousands of smaller deployments\u2014the self-hosted audience?\n\nLucky for us, we had several playbooks to borrow from to figure out our own\ndeployment thesis. That could only happen because we were always clear about\nbeing COSS over community-supported, about owning our roadmap over shared\nresponsibility, about envisioning self-hosted Plane on paper over figuring it\nout as we went along. That, in turn, guided our long-term strategy, our\njourney so far, and our hiring.\n\n### What's next for self-hosted Plane\n\nThere\u2019s a lot planned for self-hosting in 2024 and beyond. Some of those\nfollow.\n\n  * SSL and certs\n\nEncryption at rest and in transit is basis for our admins. Caddy is a popular\nreverse-proxy request, but it also looks like a good candidate for both\ninternal and public certs. It also meets admin requests to replace NGINX.\n\n  * Automated, scheduled back-ups\n\nOur CLI will soon support periodic back-ups that admins will set-and-forget.\nThese back-ups will also be uploadable to S3 or any other external file store\nof choice.\n\n  * Restores before and after upgrades\n\nNot all upgrades work for everyone. We will automatically back-up the data\nbefore an upgrade and let admins restore their instance to the last-known good\nconfig.\n\n  * Plane One on Kubernetes\n\nPlane One is Docker-only for now. We will test it with admins like we have\nsince the beginning of Phase #3 and launch it on Kubernetes, too.\n\n  * Official support for more IaaS platforms\n\nOur community is already publishing Railway templates and trying out other\ninfra services. We will shortlist our first officially supported IaaS players\nand ship deployment methods for the first few this year.\n\n  * Marketplace apps\n\nPlane is popular with larger companies on AWS and Digital Ocean. Marketplace\napps for those two, at the very least, will help easier set-up and adoption.\n\nMore on all of the above soon.\n\nPlane Logo\n\n### Resources\n\n  * Blog\n  * Docs\n  * Status\n\n### Company\n\n  * Changelog\n  * Pricing\n\n### Legal\n\n  * Terms and Conditions\n  * Privacy Policy\n\n\u00a9 2024 Plane. All rights reserved.\n\nFollow us on:\n\nX(Twitter)GitHubLinkedInYouTube\n\nWith your consent, we use cookies to optimize performance and enable functions\non this site.\n\n", "frontpage": false}
