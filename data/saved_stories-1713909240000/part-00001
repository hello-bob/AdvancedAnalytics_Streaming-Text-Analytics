{"aid": "40133357", "title": "Evolutionary Model Merging", "url": "https://blog.arcee.ai/tutorial-tutorial-how-to-get-started-with-evolutionary-model-merging/", "domain": "arcee.ai", "votes": 1, "user": "tosh", "posted_at": "2024-04-23 15:52:31", "comments": 0, "source_title": "Evolutionary Model Merging", "source_text": "Evolutionary Model Merging\n\nGo to arcee.ai\n\n\u2190 Back to blog\n\nInsights Featured\n\n# Evolutionary Model Merging\n\nWe've been focused on developing this groundbreaking technique for the\ncommunity, and we're now excited to announce the launch of this state-of-the-\nart functionality in MergeKit.\n\n#### Charles Goddard\n\nApr 23, 2024 \u2022 5 min read\n\nSakana.ai made a very big splash about a month ago, releasing a paper on\nEvolutionary Model Merging, and the subsequent model and eval results of this\ngame-changing merge method. Unfortunately for the community, they never\nreleased the algorithm behind these amazing results!\n\nSince this release, we've been fully focused on developing this groundbreaking\ntechnique for the community. We're now excited to announce the launch of this\nstate-of-the-art functionality in MergeKit.\n\nEvolutionary Model Merging lets people target specific competencies or\nqualities in their merges. Without it, Model Merging is an extremely manual\nexploratory process\u2013trying dozens of merges, manually evaluating them, and\ntrying to come up with a mental framework that explains how the merging\nparameters are related to the performance of the final model. With\nEvolutionary Model Merging, we can instead specify what qualities we want a\nmodel to have, and optimization will take care of it for us.\n\n## Tutorial: How to get started with Evolutionary Model Merging\n\nI've created a tutorial to help you get started: mergekit-evolve.\n\n# Evolutionary Model Merging with mergekit-evolve\n\n## Hardware Requirements\n\nmergekit-evolve needs at least one GPU. It doesn't necessarily need a huge\none! You need to be able to inference a model in FP16. If you're working with\nmodels in the 7B size range, 24GB of VRAM will do just fine. If you're a big\nspender then you can use a Ray cluster with however many GPUs you want. For\nthis little demo I'm using a RunPod instance with a single A100.\n\n## Installing\n\nFirst let's set up our environment with an installation of mergekit. We need\nto use the evolve feature flag, and I'm using vllm as well because it's\nfaster.\n\n    \n    \n    cd /workspace git clone https://github.com/arcee-ai/mergekit.git cd mergekit pip install -e .[evolve, vllm]\n\n## Defining Tasks\n\nTo optimize a merge recipe we need to first decide what exactly to optimize.\nmergekit-evolve uses EleutherAI's language model evaluation harness as a\nbackend, so in theory all of the benchmarks supported by lm-eval can be used.\nSince I'm not an evil little gnome I'm going to define some custom tasks\ninstead of directly optimizing against Open LLM Leaderboard scores.\n\nLet's say that we want some spatial awareness in our model. The spartqa-\nmchoice dataset is a set of synthetic question-and-answer pairs involving the\narrangement of objects that aims to test the spatial reasoning capabilities of\nlanguage models. Let's take a random sampling of their training split and use\nthat for one part of our scoring.\n\n    \n    \n    ds = datasets.load_dataset(\"metaeval/spartqa-mchoice\")[\"train\"] ds_p = ds.shuffle(seed=9163).select(range(1000)) ds_p.push_to_hub(\"my-hf-username/spartqa-train-1k\", private=True)\n\nNow we need to define an lm-eval task that scores against this data. This can\nbe done by writing a YAML file (and any necessary helper code). For more\ndetails on how to do this look at the New Task Guide.\n\n    \n    \n    mkdir /workspace/eval_tasks\n\nIn /workspace/eval_tasks/spartqa_1k_train.yaml:\n\n    \n    \n    task: spartqa_train dataset_path: my-hf-username/spartqa-train-1k output_type: multiple_choice training_split: train validation_split: train test_split: train doc_to_text: !function preprocess_spartqa.doc_to_text doc_to_choice: [ 'A', 'B', 'C', 'D' ] doc_to_target: \"{{answer}}\" metric_list: - metric: acc aggregation: mean higher_is_better: true metadata: version: 1.0\n\nAnd in /workspace/eval_tasks/preprocess_spartqa.py:\n\n    \n    \n    def doc_to_text(doc) -> str: answer_chunks = [] for idx, answer in enumerate(doc[\"candidate_answers\"]): letter = \"ABCD\"[idx] answer_chunks.append(f\"{letter}. {answer}\") answers = \"\\n\".join(answer_chunks) return f\"Context:\\n{doc['story']}\\n\\nQuestion: {doc['question']}\\n{answers}\\nAnswer:\"\n\nOne common problem with merges is that the result often doesn't conform to any\none particular prompting style. When manually creating merge recipes it's\nfairly easy to get the behavior you want by varying weights across layers, but\nsince we're letting an algorithm optimize things let's make a silly little\ntask for it instead. Alpaca is a very common standard and all it really needs\nfrom the model is to correctly output an EOS token after a completed response.\n\nFirst, let's put together another tiny set of data for evaluating our metric\nwith. I'll use a few hundred prompts from vicgalle/alpaca-gpt4.\n\n    \n    \n    ds = datasets.load_dataset(\"vicgalle/alpaca-gpt4\")[\"train\"] df = ds.to_pandas() no_input = df[df.input.map(len) < 1] examples = no_input.sample(n=500, replace=False, random_state=749) ds_p = datasets.Dataset.from_pandas(examples) ds_p.push_to_hub(\"my-hf-username/alpaca-gpt4-500\", private=True)\n\nAnd now the actual task definition is quite simple: In\n/workspace/eval_tasks/alpaca_prompt_format.yaml:\n\n    \n    \n    task: alpaca_prompt_format dataset_path: my-hf-username/alpaca-gpt4-500 output_type: multiple_choice training_split: train validation_split: train test_split: train doc_to_text: \"### Instruction:\\n{instruction}\\n### Response:\\n{output}\" doc_to_choice: - \"</s>\" # replace with your model's EOS token if it is different # and now some incorrect options - \"<|im_end|>\" - \"<|im_start|>\" - \"### Instruction:\" - \"USER:\" doc_to_target: 0 metric_list: - metric: acc aggregation: mean higher_is_better: true metadata: version: 1.0\n\nThere are definitely more robust ways to evaluate this but the multiple choice\nsetup is nice in that it evaluates really quickly. Experiment at will!\n\n## Writing an Evolutionary Merge Config\n\nWe now have all the parts in place needed to actually define the merge we want\nto optimize. mergekit-evolve takes a YAML configuration file that defines what\nmodels we want to include, what merge method to use, and what tasks to\noptimize against.\n\nFor this example, I'm going to throw three models into the soup:\n\n  * Hermes 2 Pro Mistral 7B, because it's a generally good model\n  * Dan's Adventurous Winds Mk2 7B, because it's a really fun model but answers to no prompt format\n  * Zephyr 7B beta for its quality instruction following\n\nMost of the methods implemented by mergekit can be used. I chose Task\nArithmetic pretty much arbitrarily.\n\n    \n    \n    genome: models: - NousResearch/Hermes-2-Pro-Mistral-7B - PocketDoc/Dans-AdventurousWinds-Mk2-7b - HuggingFaceH4/zephyr-7b-beta merge_method: task_arithmetic base_model: mistralai/Mistral-7B-v0.1 layer_granularity: 8 # sane default allow_negative_weights: true # useful with task_arithmetic tasks: - name: alpaca_prompt_format weight: 0.4 - name: spartqa_train weight: 0.6\n\nTasks can be weighted arbitrarily - I made the spartqa_train task slightly\nmore important than alpaca_prompt_format purely as an example.\n\n## Running the Merge\n\nNow we finally have all the pieces set up to actually run mergekit-evolve.\nHere's the command I used:\n\n    \n    \n    mergekit-evolve ./evol_merge_config.yml \\ --storage-path /workspace/evol_merge_storage \\ --task-search-path /workspace/eval_tasks \\ --vllm \\ --in-memory \\ --merge-cuda \\ --wandb\n\nThis will kick off the process and start merging and evaluating models. If you\nused the --wand option then metrics on the evaluated models will be reported\nto Weights & Biases. This can be useful to obsessively refresh while you\nshould be doing other things.\n\nBy default mergekit-evolve will keep going until it has evaluated over 100\nmerges or you stop it with CTRL+C. You can increase this limit by passing the\n--max-fevals argument. Once the script has terminated, the mergekit\nconfiguration for the best-scoring merge will be written to\n/workspace/evol_merge_storage/best_config.yaml. You can get your final model\nby running it through mergekit-yaml, like so:\n\n    \n    \n    mergekit-yaml /workspace/evol_merge_storage/best_config.yaml --cuda /workspace/final_merge\n\nWe are thrilled this novel merging technique is now available to everyone\nthrough MergeKit. We will also be integrating evolutionary model merging into\nthe core Arcee product, which will provide a complete compute backend and\nremove the need to secure your own GPU's, so stay tuned for that!\n\nLet's us know how you are using evolve-merge and happy merging!\n\n# Start building your SLMs today\n\nBook a call with our team to discuss how Arcee can help you train, deploy,\nmaintain, and continuously improve your smaller, secure, specialized, and\nscalable language models.\n\nBook a Demo\n\n# Read more articles\n\n## Why Methods Like QLoRA Fall Short in Domain Knowledge Injection\n\nArcee's research shows that the Standard Continual Pre-Training (CPT) approach\nperforms better than QLORA-based CPT.\n\nApr 20, 2024 2 min read\n\n## How to merge Llama3 using MergeKit\n\n... And what do we do at Arcee when an exciting new model drops? We MERGE IT\non MergeKit! We walk you through the process and share the initial results.\n\nApr 19, 2024 5 min read\n\nFeatured\n\n## Arcee/Mergekit launch Model Merging Hackathon\n\nArcee & MergeKit advance model merging innovations with launch of MergeKit\nhackathon. Submit your model merging research, experiments, and results for\nthe chance to win cash prizes\n\nApr 18, 2024 2 min read\n\n\u00a9 Copyright Arcee.ai 2024. All rights reserved.\n\n", "frontpage": false}
