{"aid": "40109468", "title": "Scaling to Count Billions", "url": "https://www.canva.dev/blog/engineering/scaling-to-count-billions/", "domain": "canva.dev", "votes": 1, "user": "acossta", "posted_at": "2024-04-21 21:39:04", "comments": 0, "source_title": "Scaling to Count Billions - Canva Engineering Blog", "source_text": "Scaling to Count Billions - Canva Engineering Blog\n\nSkip navigation\n\nSkip to main content\n\n  * Home\n  * Your apps\n\nEngineering Blog\n\nOverviewSubscribe\n\nDiscover more\n\nUI/UXBackendMachine LearningInfrastructureSecurityEngineering PracticesData\nPlatform\n\nAbout Canva\n\nBackend\n\n# Scaling to Count Billions\n\nHow we built a scalable and reliable content usage counting service.\n\nSangzhuoyang YuApr 12, 2024\n\nCanva\u2019s mission is to empower everyone in the world to design anything and\npublish anywhere. An essential part of our effort toward this goal is the\nCanva Creators program. Since we launched the program 3 years ago, usage of\nour creator content has doubled every 18 months. Now we pay creators based on\nbillions of content usages each month. This usage data not only includes\ntemplates but also images, videos, and so on. Building and maintaining a\nservice to track this data for payment is challenging in the following ways:\n\n  * Accuracy. The usage count should never be wrong, and we want to minimize issues such as data loss and overcounting because the income and trust of content creators are at stake.\n  * Scalability. We need to store and process usage data with this large volume and exponential growth over time.\n  * Operability. As usage data volume grows, the operational complexity of regular maintenance, incident handling, and recovery also increases.\n\nThis blog post introduces the various architectures we\u2019ve experimented with\nand the lessons we learned along the way.\n\n## The Solution\n\nOur latest architecture with OLAP database.\n\nThe previous diagram shows our latest architecture, which incorporates:\n\n  * Moving away from OLTP (Online Transaction Processing) databases, to OLAP (Online Analytical Processing) databases, which provide scalable storage and computing.\n  * Moving away from using scheduled worker services for calculation, to using an ELT (Extract, Load, Transform) pipeline leveraging OLAP databases to perform end-to-end calculations.\n\nThe new pipeline architecture can now aggregate billions of usage events in\nonly a few minutes and easily scale to handle the usage growth. The new\npipeline also let us simplify our service codebase and the large amount of\ndata we used to persist and maintain. Because of the enhanced reliability and\nsimplification, we reduced the number of incidents from 1 or more in a month\nin the worst case to 1 every few months. Troubleshooting and recovery also\nbecame easier and more manageable.\n\n### Components\n\nWe built the core of the tracking functionality as a counting pipeline,\ndivided into 3 stages with various steps in each:\n\n  1. Data collection. We send usage events from various sources such as web browsers, mobile apps, and so on, then follow this with validation and filtering.\n  2. Identify meaningful usage through deduplication. The service removes duplicated usage events and matches them with a set of specific classification rules. These rules define what we consider as distinct usages and how we should pay them.\n  3. Aggregation. The service calculates total number of deduplicated usages based on different dimensions, such as per design template or per brand.\n\nThe following sections describe our architecture evolvement journey.\n\n## The Evolvement\n\n### Start with MySQL\n\nOur initial architecture using MySQL.\n\nWe started with the tech stack we were most familiar with, MySQL, and built\nmajor components separately using worker services. We also persisted multiple\nlayers of reusable intermediary output. For example, the deduplication worker\nscanned the deduplicated usage table and updated every record by matching each\nof them with an event type. We then aggregated the results by scanning the\nupdated deduplication table and incrementing the counter persisted in another\ntable. This architecture worked well up to a point, with 3 issues: processing\nscalability, incident handling, and storage consumption.\n\n#### Processing scalability\n\nIncrease total counts using database round trips.\n\nThe deduplication scan was a single-threaded sequential process, using a\npointer storing the latest record scanned. This design was easy to reason\nabout, especially when there was a problem or incident where we needed to\nverify the fix of broken data because it clearly told which records we had\nprocessed and which we hadn\u2019t. However, it wasn\u2019t scalable because processing\neach usage record resulted in at least 1 database round trip with 1 read of\nthe event record and 1 write to increment the usage counter, as shown in the\nabove diagram. Therefore a complete scan would take O(N) database queries,\nwhere N is the number of usage records. Batching is a straightforward\noptimization, but it doesn\u2019t fundamentally improve scalability because O(N /\nC) is still O(N) given the constant batch size C. Multi-threaded scans would\nbe another potential optimization, but it would significantly increase code\ncomplexity and make maintenance and troubleshooting harder.\n\nAlso, processing errors for any usage record would delay all of the following\nrecords, as well as subsequent processing stages.\n\n#### Incident handling\n\nIncident handling was daunting. Troubleshooting and recovery were difficult\nbecause they required engineers to look into databases and fix the broken\ndata. We categorize the incidents we\u2019ve experienced into below 4 types. The\ntroubleshooting and recovery process for each varies.\n\n##### Overcounting\n\nOvercounting usually happens when an event source adds a new usage event type\nthat should be excluded, without event consumers being aware. As a result,\nthis new usage event type is mistakenly included in the deduplication and\naggregation stages. Recovery from overcounting can require a lot of\nengineering effort. First, we need to identify the event types mistakenly\nincluded and pause the deduplication and aggregation pipeline. Next, we need\nto calculate how many of these events were processed, then remove them from\nthe deduplication table and correct the data in the aggregation table.\n\n##### Undercounting\n\nUnlike overcounting, sometimes a new usage event type is added, which should\nbe included for payment. However, the event source fails to integrate with\nusage services and the events aren\u2019t aggregated. The fix for these incidents\nis to find the precise window of the missing data and collect the data from a\nbackup data source. This is the most difficult step and often requires a non-\ntrivial amount of time. After collecting the data from backup, we can backfill\nit into the service. Having a backup of all events is very helpful in these\nsituations. The backfill can take up to several days depending on the volume,\nwhich could delay payment because of the processing scalability limit\nmentioned previously.\n\n##### Misclassification\n\nMisclassification is an issue that happens during deduplication. For example,\na usage event that should be categorized into event type A, but we categorized\nit into event type B. The problem here is that we pay differently for\ndifferent types. Many of our incidents fall into this category because the\nclassification rules change from time to time. In the same way we handle\novercounting, the recovery process for misclassification is also tedious. We\nfirst need to identify the root problem causing the misclassification, which\nis usually a bug in our code. Then we need to fix the code and pause the\npipeline, followed by fixing the deduplication table data and all the\nsubsequent table data by recalculation. Doing all of this requires days of\neffort to complete, with multiple engineers cross-verifying the output.\n\n##### Processing delay\n\nThis was a performance issue resulting from the architecture, which used a\nsingle-threaded sequential scan and frequent database round trips. The\nprocessing speed was actually not bad in normal cases, and was able to catch\nup with the event traffic. However, in some special cases, the deduplication\nor aggregation worker got stuck for various reasons. For example, sometimes\nevents contained unexpected data. This caused a delay to all subsequent\naggregation processing.\n\n#### Storage consumption\n\nMySQL RDS storage consumption.\n\nWe consumed storage quickly. MySQL RDS does not horizontally scale through,\nfor example, partitioning by itself. Therefore we doubled the RDS instance\nsize every time we required more storage. This doubling happened every 8-10\nmonths. The previous diagram shows that our free storage decreased by almost\n500 GB, being 50% of the total free storage, within 6 months.\n\nThis architecture worked fine at the start until our MySQL RDS instance\nreached several TBs in size. Maintaining such a RDS required significantly\nmore engineering efforts than we expected. For example, the database was\ninitially shared with other critical features and any downtime could cause\nsevere impacts to those functionalities. Regular database maintenance, such as\nversion upgrades, became much harder too, because we always do upgrades with\nclose to zero downtime, increasing our operational complexity. To mitigate\nthese problems, we did a database split, and implemented some sweepers to\nclean up old data. Given the exponential traffic growth, it became obvious\nthat increasing the RDS instance size further was not an optimal solution for\nthe long term.\n\n### Migrate data to DynamoDB\n\nBased on the lessons we learned, we made fundamental changes to the pipeline\narchitecture to address these problems.\n\nTo meet the increasing scalability needs, we first moved the raw usage events\nin the collection stage, over to DynamoDB. This helped ease some stress on our\ngrowing storage needs.\n\nOur next step was to move the rest of the data to DynamoDB, which would\nrequire rewriting the majority of the code. After evaluating the pros and\ncons, we decided not to proceed in this direction. The main reason was that\nwhile moving to DynamoDB could solve the storage scalability issue, processing\nscalability would still remain a challenge because we found it hard to get\naway from the database round trips.\n\n### Simplify using OLAP and ELT\n\nEventually, we decided to take a different direction, combining various steps\nin the deduplication and aggregation stages so that the service does an end-\nto-end calculation using source data directly.\n\nThis was a drastic move because unlike the incremental counting before, it\nrequired processing the usage data of the entire month that was twice larger,\nwithin a short time. Therefore this needed a lot more computational power. To\ndo this, we used an OLAP database, for its advantage of being good at large\ncomplex data analysis and reporting. We chose Snowflake for most of the\nregions because it was already used as our primary data warehousing solution\nand we had reliable infrastructure support for it.\n\n#### Extract and Load\n\nWe extracted our usage data from service databases and loaded it into the\nwarehouse, using a data replication pipeline provided by our data platform.\nData replication reliability is vital and we can\u2019t accomplish this without a\nreliable data platform, so a huge thanks to the amazing work done by the Canva\ndata team.\n\n#### Transform\n\nWe then defined the calculation in each stage in scheduled data transformation\njobs (using DBT). We wrote each transformation as an SQL query, selecting data\nfrom its previous stage and outputting it to the next. Intermediate\ntransformation outputs were materialized as SQL Views that are simple and\nflexible to change. The following is an example aggregating usages per brand,\nusing data from the previous step named daily_template_usages. This also lets\nus avoid persisting and maintaining intermediary output of different pipeline\nstages.\n\n    \n    \n    select\n    \n    day_id,\n    \n    template_brand,\n    \n    ...\n    \n    sum(usage_count) as usage_count\n    \n    from {{ ref('daily_template_usages') }}\n    \n    group by\n    \n    day_id,\n    \n    template_brand\n\nSQL\n\nThe following are the major steps:\n\n  * Extract from json data into structured SQL tables. We extracted source event data in json format from DynamoDB, which was not optimal for data warehouse query processing. Therefore we projected some of the json properties into separate table columns for optimization.\n  * Deduplicate and aggregate usage events. We defined deduplication rules to filter out duplicated usage events, and summed up total usage counts using GROUP BY queries.\n\n### Improvements and challenges\n\n#### Improved performance and scalability\n\nData processing workflow with OLAP.\n\nAs shown in the previous diagram, an OLAP database like Snowflake separates\nstorage and compute, such that we can scale its compute instances\nindependently. In our case we can perform a complete aggregation of billions\nof usage records within a few minutes because most of the computation is now\ndone in memory, which is several orders of magnitude faster than database\nround trips. We also reduced the entire pipeline latency from over a day to\nunder 1 hour.\n\n#### Reduced operational complexity\n\nThe incident impact and troubleshooting effort required was greatly reduced\nand became more manageable. We no longer have processing delay incidents.\nAlthough the remaining incident types (overcounting, undercounting,\nmisclassification) might still happen, we can fix most of them by rerunning\nthe pipeline end-to-end, without humans logging into the database to fix\nbroken data that we often had to do before. Below diagram describes how we\nachieved this using the transformation defined in the aggregation step.\n\nAggregation transformation using an outer join to fix broken usage counts.\n\nThe aggregation query aggregates the output from the previous deduplication\nstep, and compares it against the prior aggregated results through an outer\njoin, such that the old results can be overwritten if they are found to be\ndifferent. This lets us fix broken output resulting from overcounting,\nundercounting or misclassification, as long as the source data remains\naccurate. Some rules are given below. For example, if a template usage is\nmisclassified into event type D whereas they belong to event type C, this will\nresult in incorrect usage count of event C, and obsolete usage count for event\nD in the output. In this case, rerunning the pipeline will clear the event D\nusage count by setting it to 0, and correct the event C usage count.\n\nEvent type| Old count| New count| Output  \n---|---|---|---  \nEvent type A| X| X| X  \nEvent type B| X| Y| Y  \nEvent type C| null| X| X  \nEvent type D| X| null| 0  \n  \n#### Simplified data and codebase\n\nWe reduced over 50% of the data stored, and eliminated thousands of lines of\ndeduplication and aggregation calculation code by moving this logic to OLAP\ndatabases and DBT transformations. We rewrote the logic in SQL, which was\nsurprisingly simpler compared to the previous code. The number of incidents\nalso dropped to once every few months or fewer.\n\n#### Challenges\n\n  * Transformation. The data transformation job uses an SQL-like language that we deploy as a standalone service separate from our main services. This makes the architecture less flexible. For example, the data transformation service has a different release schedule, so we had to consider compatibility when making schema changes. Maintaining a separate CI/CD pipeline for the DBT codebase also has extra costs.\n  * Unload data from the warehouse. We need a reliable pipeline that can unload data from Snowflake into databases used for other services because OLAP databases are not good at serving large volumes of requests with low latency within milliseconds. Therefore we built a scheduled worker that queries Snowflake periodically and choses S3 as an intermediary storage, integrated with SQS for its durability so that export data doesn\u2019t get lost. The hard part was optimizing the ingestion query and carefully tuning the rate limit to control the throughput, such that it could ingest data with reasonable speed, without causing our service databases to throttle. The following diagram shows our RDS CPU usage spikes when ingestion throughput was too high.\n  * Infrastructure complexity. As mentioned previously, we can't carry out this work without sufficient infrastructure support. Therefore, it also increased infrastructure complexity resulting from the data replication integration and the standalone services running transformation jobs. For example, we need to put in extra effort to obtain observability of these parts because of the different toolings used.\n\nRDS CPU spikes during data ingestion.\n\n## Conclusion\n\nThis blog post first described the key challenges of our counting problem for\ncreator payment. We then described our architecture evolvement journey,\nstarting with our initial solution and the major problems, followed by some\nalternatives we tried, such as using DynamoDB and OLAP databases like\nSnowflake, with their improvements and trade-offs. Finally, here are some key\nlessons we learned:\n\n  * Simplicity is a key aspect of designing reliable services. This includes reducing the code and data complexity. Based on past incident handling experiences, we found that fixing problems in the code is generally easier than fixing broken data. Therefore, we minimized intermediary output persisted in various counting pipeline stages by computing the results end-to-end using OLAP and ELT.\n  * Start small and be pragmatic. We don\u2019t consider the initial design with MySQL a bad choice. It served its purpose well in the first 2 years after launching the Canva Creators program. Using the initial architecture also ensured that we could deliver our functionality to users in a reasonable timeframe. Over time, scalability became the bottleneck. Therefore, we measured the data volume and its growth and considered our infrastructure's readiness before making any architectural changes.\n  * Observe from day 1. We tend to look closely at every part of the pipeline, such as the latency and throughput of unloading data from the warehouse to other service databases, RDS workload, and potential data missing in various places. This is because we find it hard to predict where a problem can happen, and it is rewarding, although it comes with extra overhead. It helps us understand everything going on so that we can quickly identify the problem.\n\n## Acknowledgement\n\nA huge thanks to Jason Leong, Dafu Ai, Wenchuan An, Albert Wang, Jaime\nMetcher, Steven Wang, Grant Noble for your review and valuable feedback, and\nVikram Goyal, Liliana Huynh, Chris Hew for your help with publishing this blog\npost.\n\n## More from Canva Engineering\n\nFrontend\n\n#### Alpha Blending and WebGL\n\nThis article introduces alpha blending and some tips relating to the alpha\nchannel in WebGL development.\n\nDavid GuanDec 4, 2017\n\nDistributed Tracing\n\n#### End-to-end Tracing\n\nHow Canva implemented end-to-end tracing and are using it to drive operational\ninsights\n\nIan SlesserJun 14, 2023\n\nRecommender Systems\n\n#### Recommender systems: When they fail, who are you gonna call?\n\nHow we deal with potential problems when running Canva's recommendation\nsystem.\n\nMayur Panchal, Thien BuiNov 4, 2021\n\n## Subscribe to the Canva Engineering Blog\n\nBy submitting this form, you agree to receive Canva Engineering Blog updates.\nRead our Privacy Policy.\n\nPrivacy policyTerms\n\n\u00a9 2024 All Rights Reserved. Canva\u00ae\n\nWe care about your privacy\n\nBy clicking \"Accept All\", you agree to the storing of cookies on your device\nto give you the most optimal experience using our website. We may use cookies\nto enhance performance, analyze site usage, and personalize your experience.\n\n", "frontpage": false}
