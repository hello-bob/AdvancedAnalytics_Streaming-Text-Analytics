{"aid": "40169049", "title": "Discovering and Diagnosing a Google Adsense Rendering Bug", "url": "https://merj.com/blog/discovering-and-diagnosing-a-google-adsense-rendering-bug", "domain": "merj.com", "votes": 1, "user": "giacomoz", "posted_at": "2024-04-26 13:17:34", "comments": 0, "source_title": "Discovering and Diagnosing a Google AdSense Rendering Bug", "source_text": "Discovering and Diagnosing a Google AdSense Rendering Bug | Merj\n\nSkip to content\n\nGet In Touch\n\nResearch\n\n# Discovering and Diagnosing a Google AdSense Rendering Bug\n\nByGiacomo Zecchini April 23, 2024April 23, 2024 Reading Time: 15 minutes\n\n## Introduction\n\nAdSense is Google\u2019s advertising content platform where publishers can be paid\nto place advertisements on their webpages. While performing a search engine\noptimisation test that involved crawling and rendering tracking for a React\napplication, we discovered an anomaly in the client\u2019s server log data, which\nrequired further investigation.\n\nWe found that a part of the AdSense technology stack is not working as\nexpected and the programmatic matching between ads and website content is\nimpacted, thus creating an incomplete understanding of webpage content.\n\n### Disclosure Ethics and Communication\n\nWe uphold a strong ethical framework when it comes to disclosing discovered\nanomalies or bugs, especially those associated with pivotal platforms like\nGoogle AdSense. We strictly follow responsible disclosure guidelines,\ninforming relevant parties well in advance of any public announcements.\n\nThis issue does not fall under the Google Bug Hunter Program, as it pertains\nprimarily to a product operational anomaly rather than a security\nvulnerability. Despite not reporting this through the Google Bug Bounty\nProgram, we reported this bug to Google representatives who liaise with the\ninternal Google teams and observed a standard 90-day grace period before\nconsidering public disclosure.\n\nWe have extended these standard disclosure timelines to facilitate a\nresolution, although the issue remains unresolved.\n\n#### Bug Disclosure Timeline\n\nDate| Subject| Action  \n---|---|---  \nJune 1st, 2023| Merj| Discovered the bug  \nJune 8th, 2023| Merj| We sent an email to Gary Illyes, a member of the Google\nSearch Team, describing the bug and its impact.  \nJune 27th, 2023| Google| Gary Illyes responded, stating that he had consulted\nwith the rendering team and would notify the administrators responsible for\nthe Mediapartners-Google crawlers. As the owners of the Mediapartners-Google\ncrawlers are not part of the Search team, Search team members have no\ninfluence over them.  \nSeptember 15th, 2023| Merj| We sent a follow-up email asking for any updates\nregarding the bug.  \nOctober 24th, 2023| Merj & Google| We had an in-person conversation with Gary\nIllyes about the bug at the Google Search Central Live Zurich event.  \nApril 23rd, 2024| Merj| Public disclosure of the issue  \n  \n## Google Adsense and Google Ads\n\nGoogle AdSense is an advertising program run by Google. It allows website\nowners (publishers) to monetise their content by displaying targeted\nadvertisements. These ads are generated by Google and can be customised to\nmatch the website\u2019s content. Publishers earn revenue when visitors click on or\nview these advertisements.\n\nSource: https://adsense.google.com/start/resources/best-format-your-site-for-\nadsense/\n\nGoogle AdSense offers publishers a variety of ad units to display on their\nwebsites. Here are some of the common types of ad units:\n\n  * Display\n  * In-feed\n  * In-article\n  * Multiplex ads\n  * Search engine\n\nGoogle Ads is a platform that enables businesses (advertisers) to create and\nmanage online advertisements, targeting specific audiences based on keywords,\ndemographics, and interests. These advertisements can appear on various Google\nservices, such as search results, YouTube, and partner websites.\n\nAdvertisers that use Google Ads can place their ads on websites that\nparticipate in the AdSense program. This symbiotic relationship enables\nbusinesses to reach a wider audience through targeted advertising, while\nwebsite owners can generate revenue by hosting relevant advertisements on\ntheir platforms.\n\n### Google Adsense targeting\n\nGoogle Adsense works by matching ads to your site based on your content and\nvisitors; having webpages with incorrect, partially rendered, or blank content\nimpacts the matching of ads and webpages. To analyse webpage content, Google\nAdSense employs a specific User-Agent known as \u2018Mediapartners-Google\u2019.\n\nGoogle AdSense employs various methods for delivering targeted ads. Contextual\nTargeting uses factors such as keyword analysis, word frequency, font size,\nand the overall link structure of the web to ascertain a webpage\u2019s content\naccurately and match ads accordingly.\n\nHowever, without access to a page\u2019s full content, any targeting based on page\ncontent cannot be accurate.\n\n## Impact of the Google AdSense Rendering Bug\n\n### Impact on Google Adsense\n\nWebsites that block AdSense infrastructure from accessing their content can\nconsiderably affect the precision of ad targeting, potentially resulting in\ndiminished clicks and, consequently, reduced revenue. This has an impact on\nboth publishers and advertisers. Misunderstanding the content on the page\ncould result in more severe consequences. If the publisher sends irrelevant\ntraffic to advertisers, the Adsense Platform may limit or disable ad serving.\n\n### Impact on Server Access Logs Analysis\n\nThe misattribution of User-Agents in server access logs can lead to incorrect\nassumptions about search engines\u2019 crawling and rendering of webpages.\n\nAdditionally, it can result in inaccurate conclusions about the sources of\ncrawling traffic and the effectiveness of strategies or updates made on the\nwebsite, potentially leading to misguided decision-making.\n\n## Technical Analysis of the Bug\n\n### The TL;DR\n\n  * The use of Mediapartners-Google and Googlebot for different parts of the AdSense algorithm process creates a conflict of rules which are not immediately obvious.\n  * The initial request to download the webpage\u2019s HTML source code uses the \u201cMediapartners-Google\u201d User-Agent.\n  * The Google Web Rendering Service (WRS) then processes and renders the page to generate the final HTML. During this phase, supplementary rendering resources are requested using the \u201cGooglebot\u201d User-Agent. If a necessary resource cannot be downloaded because a robots.txt rule is blocking the \u201cGooglebot\u201d User-Agent, the webpage may be partially rendered or completely blank.\n  * Not being able to get the correct content of the webpage can affect AdSense content understanding and ad targeting, consequently affecting publisher revenues.\n\n### The Details\n\n#### Robots.txt effect on crawling & Rendering\n\nEvery time a web browser requests a website, it sends an HTTP Header called\nthe \u201cUser-Agent\u201d. The User-Agent value contains information about the web\nbrowser name, operating system, and device type. The User-Agent is present in\nboth webpages and page resource requests.\n\nSearch Engine crawlers use their own custom User-Agent, when fetching webpages\nand page resources. Before starting to download a specific URL, Search Engines\ncheck if they are allowed to fetch a specific URL by parsing the robots.txt.\n\nWithout debating on \u201cif\u201d and \u201chow\u201d the use of the robots.txt to block crawlers\nis suitable, here below is a simplified step-by-step pipeline of robots.txt\neffect on a search engine\u2019s crawling and rendering process:\n\n  * Step 1: Checking robots.txt before fetching the webpage\n  * Step 2: Fetching the webpage\n  * Step 3: Parsing the HTML to get the webpage resource\n  * Step 4: Checking robots.txt for each webpage resource\n  * Step 5: Downloading webpage resources\n  * Step 6: Start rendering the webpage\n  * Step 7: Checking robots.txt for additional page resources needed to complete the rendering\n  * Step 8: Downloading additional webpage resources\n  * Step 9: Complete the webpage rendering\n\nIf Step 1 fails:\n\n  * the crawler is not allowed to download the webpage HTML source code.\n  * subsequent steps are ignored.\n\nIf Step 1 is completed but one of the other steps fails:\n\n  * the rendering of the webpage may not be correct due to missing resources.\n  * the final webpage\u2019s rendered HTML may be missing some information or be completely blank.\n\n### Our Investigative Process\n\nWith ongoing efforts to bring our Search Engine Web Rendering Monitoring\nsolution into production, we have been closely monitoring the number of\nwebpages being crawled and the time delta within which those webpages are\nrendered. Working with server logs that contain terabytes of data, we utilise\na custom in-house enrichment and query engine (similar to Splunk) that enables\nus to drill into the data with complex logic.\n\n#### Validating the Data Source\n\nThe server access logs have started showing anomalies over a 6-week period,\nwith fetches of page resources where the referrer points to webpages that are\nnormally blocked for Googlebot. First, we needed to check the data pipelines\nand data integrity. This involved reviewing any code changes and container\nfailures that may have created some unexpected edge cases both at our source\nand further upstream. We are often second consumers of server logs because of\nPersonal Identifiable Information (PII) and Payment Card Industry Data\nSecurity Standard (PCI-DSS) requirements. Examples of transformations include:\n\n  * Redacting sensitive URLs such as logged-in areas.\n  * IP address restriction. Often the IP address is redacted, so Google crawler verification needs to be done further upstream by IP range checks.\n  * Scrubbing emails, names, and addresses.\n\nThe Traffic Engineering and Edge teams managing the upstream ingress point\n(for instance, a CDN like Cloudflare, Akamai, or Fastly) confirmed that no\nchanges had been made. We reprocessed our data, which yielded the same\nanomalies.\n\n#### Reproduction and Isolation of the Issue\n\nOnce the data source has been validated, the next step is to reproduce and\nisolate the anomaly to confirm its existence and understand its behaviour.\nHere\u2019s how to replicate the issue:\n\n  1. Identify Target Webpages: Start by identifying webpages that are accessible to the \u201cMediapartners-Google\u201d User-Agent, but blocked for the \u201cGooglebot\u201d User-Agent. This can be determined by looking for \u201cDisallow\u201d directives in the website\u2019s robots.txt file.\n\n    \n    \n    # Googlebot user-agent: Googlebot disallow: /reviews # Mediapartners-Google user-agent: Mediapartners-Google allow: /\n\n  2. Utilise the Referer HTTP Request Header: Tracing the webpage resources through the Referer HTTP header reveals the webpage from which a particular resource has been requested.\n\n    \n    \n    APACHE 66.249.64.4 - - [28/Jul/2023:04:17:10 +0000] 808840 \"POST /graphql-enpoint HTTP/1.1\" 200 56 \"https://domain.com/reviews/139593\" \"Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; Googlebot/2.1; +http://www.google.com/bot.html) Chrome/114.0.1.2 Safari/537.36\" NGINX 66.249.64.4 - - [28/Jul/2023:04:17:10 +0000] \"POST /graphql-enpoint HTTP/1.1\" 200 56 \"https://domain.com/reviews/139593\" \"Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko; compatible; Googlebot/2.1; +http://www.google.com/bot.html) Chrome/114.0.1.2 Safari/537.36\"\n\n  3. Use a Robots.txt Parser: Use a reliable robots.txt parser to verify the accessibility of the webpage origin address for different User-Agents. We recommend using the official Google open-source C++ version available on GitHub. If using another parser, refer to the Google official documentation and the specification to check for accurate parsing.\n  4. Verify User-Agent Attribution: By combining the Referer HTTP request header and the robots.txt parser, check whether the resource requests during rendering are correctly attributed to the \u201cGooglebot\u201d User-Agent or if they originate from a different User-Agent, specifically \u201cMediapartners-Google\u201d.\n\nNote: For webpages accessible by both \u201cMediapartners-Google\u201d and \u201cGooglebot,\u201d\nthe above Server Access Logs approach to detect incorrect User-Agent\nattribution may not be effective. In such specific cases, more advanced\nsolutions, such as our Search Engine Rendering Monitoring tool, are required.\n\n### Impact Analysis\n\n#### Number of Websites using Google AdSense potentially impacted\n\nTo assess the potential implications of the issue on actual websites, we\nacquired the list of US and UK websites utilising Google AdSense from\nBuiltWith.com and developed a tool to identify the possible impact of the\nissue on these websites.\n\nThe robots.txt files of most websites we analysed are small and contain rules\nonly for the global User-Agent (*), which the AdSense crawler ignores. As the\nAdSense crawler only respects rules set specifically for Mediapartners-Google,\nthis significantly impacts the number of websites that may be affected.\n\nWe did this by using the following simplified logic that approximates the\npotential magnitude of the websites that may be impacted:\n\nUpon executing the tool on the BuiltWith list, which covers around 7 Million\nUS websites and 2 Million UK websites, we determined that around 5.5 Million\nwebsites may potentially be impacted by this issue.\n\nUK websites\n\nStatus| Number  \n---|---  \nWebsites from the BuiltWith list| 1,946,633  \nTestable websites| 974,536  \nPotentially impacted websites| 938,413  \nNon-impacted websites| 36,123  \n  \nUS websites\n\nStatus| Number  \n---|---  \nWebsites from the BuiltWith list| 6,827,954  \nTestable websites| 4,540,894  \nPotentially impacted websites| 4,363,028  \nNon-impacted websites| 177,866  \n  \nOn analysis of the robots.txt files, we can see that most of the body bytes\nare relatively small.\n\nUK websites robots.txt bytes (compressed)\n\nPercentile| Bytes  \n---|---  \n25th| 137  \n50th (Median)| 137  \n75th| 137  \n95th| 213  \n99th| 748  \n  \nUS websites robots.txt bytes (compressed)\n\nPercentile| Bytes  \n---|---  \n25th| 137  \n50th (Median)| 137  \n75th| 137  \n95th| 575  \n99th| 1,145  \n  \nIt is difficult, within the scope of this article, to provide an exact\nprediction of the number of websites currently impacted. While 5.5 million\nwebsites may be affected by the issue, they would only experience a negative\nimpact if they exhibit certain specific characteristics, such as serving\nprimary content via JavaScript and blocking a portion of requests using\nspecific robots.txt rules.\n\nOur analysis provides a broad overview of potential impacts without hands-on\nverification. To identify if a site is affected, a more complex assessment\nwould be necessary, involving the comparison of a site\u2019s initial and rendered\nHTML. This requires a level of testing that goes beyond our current scope,\nemulating search engine behaviours to extract and analyse a page\u2019s primary\ncontent.\n\nThe web is inherently broken, and simple methods, like checking the <main>\nHTML tag, fall short due to the web\u2019s inconsistency and the varying adherence\nto best practices among servers and websites. Other approaches, such as\ncomparing initial and rendered HTML sizes or word count differences, are\nimprecise and unreliable, potentially leading to the publication of incorrect\ndata.\n\nGiven the complexity of automating the test, we have opted to describe a\nstraightforward method for self-diagnosing the issue in the FAQ section. This\napproach allows users to assess their websites independently.\n\n#### Google AdSense impact\n\nThe ideal test to assess the impact on Google AdSense in this scenario would\nbe to quantify the number of websites affected by the issues that display\ninappropriate ads, yet this is unfeasible.\n\nGoogle AdSense utilises a variety of ad-matching techniques that go well\nbeyond contextual targeting. This comprehensive approach offers a broad\nspectrum of ad targeting possibilities, ranging from matches based on content\nto ads chosen by advertisers for specific placements and those tailored to\nuser interests.\n\nWhile publishers can customise the types of ad categories permitted on their\nsite, they have limited influence over the exact ads that are shown. Moreover,\nthe presence of ads that seem to not align with the site content could be\nattributed to advertisers who have set overly broad or generic targeting\ncriteria rather than an issue with the ad targeting system itself.\n\nDue to this complexity, it\u2019s not possible to determine whether a website is\ndisplaying incorrect ads based solely on the issue we discovered.\n\nAs an alternative method to estimate whether websites affected by the issue\nmight see an impact on revenue, publishers can use the revenue calculator to\nget an idea of how much they should earn with AdSense.\n\nIn the calculator, you can select region, category, and monthly page views to\nget an estimate. The calculator itself emphasises that the estimate should\nonly be used as a reference and that numbers may vary, but it could be useful\nto have an idea of the missing revenue if the numbers differ significantly\nfrom what publishers can see in the AdSense dashboard.\n\n#### Google Ads impact\n\nGoogle Ads is not directly affected by the issue. We have examined the Google\nAds crawler\u2019s requests, and for the tested websites, it is sending the correct\nUser Agent for all fetches. Nonetheless, Advertisers may observe an impact of\nthis issue on the quality of traffic, click-through rate (CTR), and indirectly\non revenue.\n\n#### Server Access Logs impact\n\nAccess logs are not commonly utilised by publishers or advertisers, yet these\nlogs might be utilised by others for analysis or to establish a business case\nfor technical modifications.\n\nUsing the methodology described in the \u2018Reproduction and Isolation of the\nIssue\u2019 section, we examined the access logs of multiple websites for different\nclients. Our findings revealed that, depending on the scale of the website,\nthe percentage of misattributed \u2018Mediapartners-Google\u2019 fetches using the\n\u2018Googlebot\u2019 User-Agent can range from 20% to 70% of the total \u2018Googlebot\u2019\nrequests.\n\nThis substantial discrepancy in the access logs analysis can significantly\ndistort any analysis.\n\n## Solutions and Recommendations\n\n### Best Practices for AdSense\n\nWhile Google has confirmed it is a bug, they have not yet fixed it. Businesses\ncan work around the issue by ensuring essential assets that are used to render\na webpage, such as API endpoints, scripts, stylesheets and images, are not\nblocked by robots.txt for both \u201cMediapartners-Google\u201d and \u201cGooglebot\u201d User-\nAgents.\n\n### Best Practices for Server Access Logs Analysis\n\nTo effectively understand the impact of issues within server access logs, it\nis crucial to employ a systematic approach to log analysis. The method\noutlined in the \u201cReproduction and Isolation of the Issue\u201d section provides a\nsimple way to filter the access logs removing the pages that Googlebot can\u2019t\ncrawl. It\u2019s worth remembering that this approach would offer only a partial\nview of the problem, excluding only those pages blocked by Googlebot and not\nfor Mediapartners-Google.\n\nIt is recommended that you use more advanced filtering techniques to fully\nunderstand the issue\u2019s impact. For a detailed and comprehensive analysis of\nyour server access logs, we encourage you to get in touch with us.\n\n## FAQ\n\n### What is the Google AdSense rendering bug?\n\nThe Google AdSense rendering bug is a technical issue in which ads served by\nGoogle AdSense might not display correctly on publishers\u2019 websites.\n\nThis problem presents itself due to discrepancies in how pages are rendered\nwhen different rules are applied to Googlebot and the AdSense bot\n(\u201cMediapartners-Google\u201d). If these bots are treated differently by your site\u2019s\nrobots.txt, it can lead to improper ad display.\n\n### What steps can I take to diagnose the AdSense rendering issue on my site\neasily?\n\nTo diagnose the issue, review your robots.txt checking for any directives that\nmight block \u201cGooglebot\u201d from accessing certain URL paths on your site that are\nnot similarly restricted for the AdSense bot (\u201cMediapartners-Google\u201d).\n\nIf your website is using Client Side Rendering and/or the main content of the\nwebpages is generated dynamically at rendering time using additional\nJavaScript requests, it\u2019s crucial to ensure that both \u201cGooglebot\u201d and\n\u201cMediapartners-Google\u201d have equal access to these JavaScript resources and the\nresultant content paths.\n\nDiscrepancies in access permissions between these bots can lead to issues and\nprevent proper rendering.\n\n### Are there any quick fixes or workarounds for the rendering bug?\n\nA quick fix to address the rendering bug involves aligning the access rules\nfor both \u201cGooglebot\u201d and the Google AdSense bot (\u201cMediapartners-Google\u201d) in\nyour robots.txt file.\n\nEnsuring both bots have the same level of access to your site\u2019s content can\nmitigate rendering issues. This approach helps ensure that even if requests\nare misattributed in server access logs, page rendering works as expected.\n\n### Are my Server Access Logs affected?\n\nServer Access Logs play a crucial role in diagnosing and understanding how web\ncrawlers and bots interact with your website. These logs contain detailed\nrecords of every request made to your server, including those by Googlebot and\nthe AdSense bot (\u201cMediapartners-Google\u201d).\n\nEven if your website is not affected by the rendering bug, the logs may\ncontain misattributed requests. The consequence of this misattribution would\nbe an inaccurate count of Googlebot requests, you would see more requests than\nthere actually are. In your analysis, the number of Googlebot requests would\nbe the sum of actual Googlebot requests plus the misattributed Google AdSense\nrequests that use Googlebot as the User-Agent.\n\n### Can I use IP ranges to filter the Server Access Logs?\n\nGoogle\u2019s documentation details the IP ranges for verifying Googlebot and other\nGoogle crawlers, organising these ranges into multiple files.\n\nThis categorisation seemingly simplifies filtering processes for our use case:\nGooglebot IPs are classified as \u201cCommon Crawlers\u201d, while Google AdSense IPs\nare deemed \u201cSpecial Case Crawlers\u201d. Initially, one might expect to filter\nGooglebot requests using the googlebot.json IP ranges and exclude those listed\nin special-crawler.json.\n\nHowever, the situation is more complex. The misattributed requests actually\noriginate from genuine Googlebot IP addresses. It appears that the Google\nAdSense bot uses Googlebot\u2019s infrastructure to crawl resources rather than\njust misattributing the User-Agent string.\n\n### How can I fix the Server Access Logs for my analysis?\n\nThe most straightforward approach to verifying and filtering Server Access\nLogs is examining the request referrer URLs. Specifically, for requests\nidentified with a Googlebot User-Agent, the presence of a referrer page that\nis blocked to Googlebot but accessible to the Google AdSense bot\n(\u2018Mediapartners-Google\u2019) could indicate incorrect attribution.\n\nThis technique, however, is limited in its applicability. It does not yield\nreliable insights for paths that are accessible to both Googlebot and the\nGoogle AdSense crawlers, as these scenarios do not facilitate clear\ndifferentiation based on robots.txt rules. To have a comprehensive filtering\nmethod, more advanced solutions, such as our Search Engine Rendering\nMonitoring tool, are required.\n\nWe would like to thank Aleyda Solis (LinkedIn, X/Twitter), Barry Adams\n(LinkedIn, X/Twitter), and Jes Scholz (LinkedIn, X/Twitter) for their thorough\npeer review of this article. Their experience and insightful suggestions have\nenhanced the depth and clarity of our analysis, allowing us to highlight key\naspects and decisions made during the writing process for a more coherent and\nimpactful delivery.\n\nGiacomo Zecchini\n\nWant to get articles like this direct to your inbox? Subscribe to our mailing\nlist.\n\nWe bring data engineering and digital marketing together.\n\n7 Pancras Square, London, N1C 4AG\n\n+44 (0) 203 322 2660\n\n## Useful Links\n\n  * Data Engineering\n  * Data Driven Marketing\n\n## People\n\n  * Blog\n  * Case Studies\n  * Policies\n\n### How Can We Help?\n\nGet In touch\n\n\u00a9 2015-2024 Merj Ltd\n\n", "frontpage": false}
