{"aid": "40264352", "title": "Machine unlearning: ML model minus the information to be unlearned", "url": "https://ai.stanford.edu/~kzliu/blog/unlearning", "domain": "stanford.edu", "votes": 33, "user": "ignoramous", "posted_at": "2024-05-05 12:30:44", "comments": 11, "source_title": "Machine Unlearning in 2024", "source_text": "Machine Unlearning in 2024 - Ken Ziyu Liu - Stanford Computer Science\n\nLoading [MathJax]/jax/output/CommonHTML/fonts/TeX/fontdata.js\n\n# Machine Unlearning in 2024\n\n41 minute read\n\nWritten by Ken Liu \u2219 May 2024\n\n\u25b8 Table of Contents\n\n  * 1\\. A bit of history & motivations for unlearning\n  * 2\\. Forms of unlearning\n\n    * 2.1. Exact unlearning\n    * 2.2. \"Unlearning\" via differential privacy\n    * 2.3. Empirical unlearning with known example space\n    * 2.4. Empirical unlearning with unknown example space\n    * 2.5. Just ask for unlearning?\n  * 3\\. Evaluating unlearning\n  * 4\\. Practice, pitfalls, and prospects of unlearning\n\n    * 4.1. The spectrum of unlearning hardness\n    * 4.2. Copyright protection\n    * 4.3. Retrieval-based AI systems\n    * 4.4. AI safety\n\nAs our ML models today become larger and their (pre-)training sets grow to\ninscrutable sizes, people are increasingly interested in the concept of\nmachine unlearning to edit away undesired things like private data, stale\nknowledge, copyrighted materials, toxic/unsafe content, dangerous\ncapabilities, and misinformation, without retraining models from scratch.\n\nMachine unlearning can be broadly described as removing the influences of\ntraining data from a trained model. At its core, unlearning on a target model\nseeks to produce an unlearned model that is equivalent to\u2014or at least \u201cbehaves\nlike\u201d\u2014a retrained model that is trained on the same data of target model,\nminus the information to be unlearned.\n\nThere\u2019s a lot hidden in the above description. How do we describe the\ninformation to be unlearned? Do we always have ground-truth retrained models?\nIf not, how do we actually evaluate the unlearning? Can we even verify and\naudit unlearning? Is pretending to unlearn, as humans often do, sufficient? Is\nunlearning even the right solution? If so, for what problems?\n\nThe precise definitions of unlearning, the techniques, the guarantees, and the\nmetrics/evaluations would depend on:\n\n  1. The ML task (e.g., binary classification or language modeling);\n  2. The data to unlearn (e.g., a set of images, news articles, or the knowledge of making napalm);\n  3. The unlearning algorithm (e.g., heuristic fine-tuning vs deleting model components);\n  4. The goal of unlearning (e.g., for user privacy or harmfulness removal).\n\nIn this educational post, I hope to give a gentle, general ML audience\nintroduction to machine unlearning and touch on things like copyright\nprotection, New York Times v. OpenAI, right-to-be-forgotten, NeurIPS machine\nunlearning challenge, retrieval-based AI systems, AI safety, along with some\nof my thoughts on the field. While unlearning is broad topic applicable to\nmost ML models, we will focus a lot on foundation models.\n\n## 1\\. A bit of history & motivations for unlearning\n\nPeople have thought about the unlearning problem for a while now. The initial\nresearch explorations were primarily driven by Article 17 of GDPR (European\nUnion\u2019s privacy regulation), often referred to as \u201cright-to-be-forgotten\u201d\n(RTBF) since 2014. RTBF basically says a user has the right to request\ndeletion of their data from a service provider (e.g. deleting your Gmail\naccount).\n\nRTBF was well-intentioned. It was also very actionable when said service\nproviders store user data in a structured way, like how Google removed a bunch\nof links from its index in repsonse to RTBF requests.\n\nHowever, RTBF wasn\u2019t really proposed with machine learning in mind. In 2014,\npolicymakers wouldn\u2019t have predicted that deep learning will be a giant\nhodgepodge of data & compute, and that separating and interpreting this\nhodgepodge turned out to be hard. The hardness of erasing data from ML models\nhas subsequently motivated research on what is later referred to as \u201cdata\ndeletion\u201d and \u201cmachine unlearning\u201d.\n\nA decade later in 2024, user privacy is no longer the only motivation for\nunlearning. We\u2019ve gone from training small convolutional nets on face images\nto training giant language models on pay-walled, copyrighted, toxic,\ndangerous, and otherwise harmful content, all of which we may want to \u201cerase\u201d\nfrom the ML models\u2014sometimes with access to only a handful of examples. The\nnature of the models has changed too. Instead of using many small specialized\nmodels each good at one task, people started using a single giant model that\nknows just about any task.\n\nCurrently, I think the motivations for unlearning fall into two categories:\n\n  1. Access revocation (think unlearning private and copyrighted data). In an ideal world, data should be thought of as \u201cborrowed\u201d (possibly unpermittedly) and thus can be \u201creturned\u201d, and unlearning should enable such revocation.\n\nUnlearning is challenging from this perspective. One key difficulty is that\nour limited understanding of deep learning itself makes data trained into a\nmodel akin to \u201cconsumables\u201d (which can\u2019t just be \u201creturned\u201d after\nconsumption). Data may also be non-fungible (e.g. your chat history) and may\neven be thought of as labor with its own financial and control interests.\nAnother challenge is that access revocation may require a proof of unlearning;\nas we will explore in the coming sections, this isn\u2019t always possible.\n\nThese difficulties suggest that it\u2019s perhaps also worth revising laws like\nRTBF and thinking about alternatives such as data markets, where data owners\nare properly compensated so they won\u2019t want to request unlearning in the first\nplace. To illustrate, suppose Bob ate Alice\u2019s cheesecake (data), Alice would\nmuch rather Bob pay her or return something equivalent (compensation) than Bob\npuking to his pre-eating state (unlearning).\n\nIn practice, one way to implement access revocation is via some form of\nperiodic re-training of the base model. Many model providers already do this\nto keep their models competitive and up-to-date. For example, OpenAI can\ncollect a bunch of unlearning requests, and batch-satisfy them during the re-\ntraining every year (or, guided by RTBF\u2019s \u201cundue delay\u201d period by which the\nrequest must be satisfied). More broadly, this suggests socio-technical\nsolutions for unlearning: policymakers can mandate such periodic re-training\nand set economically viable deadlines to offload the costs to the model\nowners.\n\n  2. Model correction & editing (think toxicity, bias, stale/dangerous knowledge removal). That is, the model was trained on something undesirable and we\u2019d like to fix it. This is closely related to the model editing literature. The concept of \u201ccorrective machine unlearning\u201d, where unlearning serves to correct the impact of bad data, was recently proposed to capture this motivation. From this perspective, unlearning may also be viewed as a post-training risk mitigation mechanism for AI safety concerns (discussed further in Section 4).\n\nUnlike access revocation, we could be more lenient towards with model\ncorrection since the edit is more of a desire than a necessity mandated by\nlaw, much like model accuracy on image classification or toxicity of generated\ntext. (Of course, these can cause real harm too.) Here, we won\u2019t necessarily\nneed formal guarantees for the unlearning to be practically useful; we have\nplenty of examples where people would happily deploy models that are deemed\n\u201csufficiently safe\u201d. The recent WMDP benchmark, which quizzes a model on\nhazardous knowledge, is a good example of empirically evaluating unlearning\nefficacy.\n\n## 2\\. Forms of unlearning\n\nUnlearning is trivially satisfied if we can just retrain the model without the\nundesired data. However, we want something better because (1) retraining can\nbe expensive and (2) it can be a lot of work just to find out what to remove\nfrom training data\u2014think finding all Harry Potter references in a trillion\ntokens. Unlearning techniques essentially seek to mitigate or avoid this\nretraining cost while producing identical or similar results.\n\nThe unlearning literature can roughly be categorized into the following:\n\n  1. Exact unlearning\n  2. \u201cUnlearning\u201d via differential privacy\n  3. Empirical unlearning, where data to be unlearned are precisely known (training examples)\n  4. Empirical unlearning, where data to be unlearned are underspecified (think \u201cknowledge\u201d)\n  5. Just ask for unlearning?\n\nForms 2-4 are sometimes known as \u201capproximate unlearning\u201d in that the\nunlearned model approximates the behavior of the retrained model. Form 5 is\nquite new and interesting, and more specific to instruction-following models.\n\nFigure 1. Illustration of approximate unlearning. Source: NeurIPS Machine\nUnlearning Challenge.\n\nIn the following, we will go through what each of these types roughly looks\nlike, along with what I think are the promises, caveats, and questions to ask\nlooking forward.\n\n### 2.1. Exact unlearning\n\nExact unlearning roughly asks that the unlearned model and the retrained model\nto be distributionally identical; that is, they can be exactly the same under\nfixed randomness.\n\nTechniques for exact unlearning are characterized by the early work of Cao &\nYang and SISA. In SISA, a very simple scheme, the training set is split into N\nnon-overlapping subsets, and a separate model is trained for each subset.\nUnlearning involves retraining the model corresponding to and without the data\npoints to be unlearned. This reduces cost from vanilla retraining by 1/N\n(cheaper if we keep model checkpoints). Inference then involves model\nensembling.^1\n\nFigure 2. Illustration of SISA: just train models on data shards (image\nsource).\n\nMore generally, the essence of exact unlearning of this form is that we want\nmodular components in the learning algorithm to correspond to different\n(potentially disjoint) sets of the training examples.\n\nThere are several benefits of exact unlearning:\n\n  1. The algorithm is the proof. If we implement something like SISA, we know by design that the unlearned data never contributed to other components. As it turns out, formally proving the model has unlearned something is quite challenging otherwise.\n  2. It turns the unlearning problem into an accuracy/efficiency problem. This makes exact unlearning more approachable due to the messiness of unlearning evaluation and lack of benchmarks.\n  3. Interpretability by design. By providing a structure to learning, we also have better understanding of how certain data points contribute to performance.\n\nThe main drawback seems obvious: modern scaling law of large models argues\nagainst excessive data & model sharding as done in SISA. Or does it? I think\nit would be very interesting to revisit sharding in the context of large\nmodels, in light of the recent model merging literature that suggests the\nfeasibility of weight-space merging between large models. As we\u2019ll learn in\nthe coming sections, the messiness of approximate unlearning and its\nevaluation, especially in the context of large models, makes exact unlearning\nvery appealing.\n\n### 2.2. \u201cUnlearning\u201d via differential privacy\n\nThis line of work roughly says: if the model behaves more or less the same\nwith or without any particular data point, then there\u2019s nothing we need to\nunlearn from that data point. More broadly, we are asking for distributional\ncloseness between the unlearned and the retrained models.\n\nFor readers unfamilar with differential privacy (DP) in machine learning, DP\ndefines a quantifiable indistinguishability guarantee between two models M, M\u2032\ntrained on datasets X, X\u2032 that differ in any single training example. The\ncanonical procedure, DP-SGD, works by clipping the L2-norm of the per-example\ngradients and injecting some per-coordinate Gaussian noise to the gradients.\nThe idea is that the noise would mask or obscure the contribution of any\nsingle gradient (example), such that the final model isn\u2019t sensitive any\nexmaple. It is usually denoted by (\u03b5,\u03b4)-DP; the stronger the noise, the\nsmaller the scalars (\u03b5,\u03b4), the more private.\n\nThe intuition is that if an adversary cannot (reliably) tell apart the models,\nthen it is as if this data point has never been learned\u2014thus no need to\nunlearn. DP can be used to achieve this form of unlearning, but due to the\none-sidedness of unlearning (where we only care about data removal, not\naddition), DP is a strictly stronger definition. This notion of unlearning is\nsometimes known as \u201c(\u03b1,\u03b2)-unlearning\u201d where (\u03b1,\u03b2) serve similar roles as (\u03b5,\u03b4)\nto measure distributional closeness.\n\nExample techniques along this direction include: (1) storing checkpoints of\n(DP) convex models and unlearning is retraining from those checkpoints; and\n(2) on top of the previous technique, add SISA for adaptive unlearning\nrequests (i.e. those that come in after observing the published model).\n\nDP-based unlearning is good in that it gives some form of a statistical\nguarantee. However, there are some important considerations that limit its\napplicability to large models:\n\n  1. Many such unlearning results apply only to convex models or losses.\n  2. What levels of unlearning (values of (\u03b5,\u03b4)-DP or (\u03b1,\u03b2)-unlearning) are sufficient? Who decides?\n  3. For large models, current ML systems don\u2019t fit well with the per-example workloads of DP-like procedures. The memory overhead will also be prohibitive.\n  4. Moreoever, like DP, the guarantees can fall off quickly with more unlearning requests (at best the rate of O(\u221ak) with k requests following DP composition theorems).\n  5. DP-like definitions implicitly assume we care about all data points equally. But some examples are more likely to receive unlearning request, and some examples would not have contributed to the learning at all.\n  6. DP-like procedures may also just hurt model accuracy a lot, sometimes in an unfair way.\n\nFor large models in particular, it\u2019s also worth distinguishing the cases of\nunlearning pre-training data vs unlearning fine-tuning data. The latter is a\nlot more tractable; for example, we could indeed fine-tune large models with\ndifferential privacy but not so much with pre-training.\n\n### 2.2.1. Forging and its implications on DP-like unlearning definitions\n\nAn unlearning procedure may sometimes require an external audit, meaning that\nwe\u2019d like to prove that the unlearning procedure has actually happened.\n\nThe main idea of \u201cforging\u201d is that there exists two distinct datasets that,\nwhen trained on, would produce the same gradients and (thus) the same models.\nThis is true intuitively:\n\n  1. Think linear regression of points on a perfect line; removing any 1 point doesn\u2019t change the fitted line;\n  2. Think mini-batch GD, where replacing one example gradient with the sum of several \u201cfake\u201d gradients would give the same batch gradient.\n\nForging implies that DP-based approximate unlearning may not be auditable\u2014that\nis, the unlearning service provider cannot formally prove that the forget set\nis really forgotten. In fact, if we only look at the model weights, even exact\nunlearning may not be auditable.\n\nWhile one can brush this off as a theoretical result, it does mean that\npolicymakers should think carefully about how a future version of \u201cright-to-\nbe-forgotten\u201d (if any) should look like and whether similar policies are\nlegally and technically enforceable.\n\nIndeed, what qualifies as an \u201caudit\u201d could very well be definition and\napplication dependent. If the auditor only cares that the unlearned model\nperforms poorly on a specified set of inputs (say on a set of face images),\nthen even empirical unlearning is \u201cauditable\u201d (see next section).\n\n### 2.3. Empirical unlearning with known example space (\u201cexample unlearning\u201d)\n\nThis line of work is essentially \u201ctraining to unlearn\u201d or \u201cunlearning via\nfine-tuning\u201d: just take a few more heuristically chosen gradient steps to\nshape the original model\u2019s behavior into what we think the retrained model\nwould do (while also optionally resetting some parameters in the model). It\nmay also be referred to as \u201cexample unlearning\u201d, since the training, retain,\nand forget sets are often clearly defined.\n\nThe NeurIPS 2023 Machine Unlearning Challenge collected many methods along\nthis direction. The challenge roughly runs as follows:\n\n  * You are given a face image dataset with designated retain/forget example splits for the training set, a target model trained on everything, and a secret model trained only on the retain set.\n  * You are asked to design an unlearning algorithm that produces unlearned model(s) from the target model that \u201cmatch\u201d the secretly kept model.\n  * The \u201cmatch\u201d or evaluation metric uses a DP-like output-space similarity over 512 seeds: for each forget example, compute an \u201cempirical \u03b5\u201d over 512 unlearned models based on true/false positive rates of an adversary (also provided by the organizer), and aggregate across examples.\n  * All models are a small ConvNet.\n\nTo give an intuition about how well empirical unlearning is doing without\nfully explaining the metric: the ground-truth retrained model gets about\n~0.19, the winning submission gets to ~0.12, and the baseline (simple gradient\nascent on forget set) is ~0.06.^2\n\nSo what do the winning ideas look like? Something along the lines of the\nfollowing:\n\n  1. Gradient ascent on the forget set;\n  2. Gradient descent on the retain set (and hope that catastrophic forgetting takes care of unlearning);\n  3. Gradient descent on the forget set, but with uniformly random labels (to \u201cconfuse\u201d the model);\n  4. Minimize KL divergence on outputs between unlearned model and original model on the retain set (to regularize unlearned model performance on unrelated data);\n  5. Re-initialize weights that had similar gradients on the retain set and forget sets, and finetune these weights on the retain set;\n  6. Prune 99% of weights by L1-norm and fine-tune on the retain set;\n  7. Reset first/last k layers and fine-tune on the retain set; and\n  8. Heuristic/arbitrary combinations of the above.\n\nIndeed, despite the heuristic nature of these approaches, these are what most\nempirical unlearning algorithms, especially those on large (language) models,\nare doing these days.\n\nPeople explore empirical approaches because theoretical tools are usually\nimpractical; for example, enforcing DP simply hurts accuracy and efficiency\ntoo much, even for the GPU rich. On the flip side, empirical methods are often\nfast and easy to implement, and their effects are often qualitatively visible.\n\nAnother key motivation for empirical unlearning is that counterfactuals are\nunclear, especially on LLMs. In deep learning, we often don\u2019t know how the\nretrained model would behave on unseen data. What should the LLM think who\nBiden is, if not a politician? Should image classifiers give uniformly random\npredictions for unlearned images? Do they generalize? Or are they confidently\nwrong? Any of these is possible and it can be up to the practitioner to\ndecide. It also means that behaviors that are equally plausible can lead to\nwildly different measurements (e.g., KL divergence between output\ndistributions of unlearned & retrained model), complicating theoretical\nguarantees.\n\n### 2.4. Empirical unlearning with unknown example space (\u201cconcept/knowledge\nunlearning\u201d)\n\nWhat if the train, retain, or forget sets are poorly specified or just not\nspecified at all? Foundation models that train on internet-scale data may get\nrequests to unlearn a \u201cconcept\u201d, a \u201cfact\u201d, or a piece of \u201cknowledge\u201d, all of\nwhich we cannot easily associate a set of examples. The terms \u201cmodel editing\u201d,\n\u201cconcept editing\u201d, \u201cmodel surgery\u201d, and \u201cknowledge unlearning\u201d are closely\nrelated to this notion of unlearning.^3\n\nThe underspecification of the unlearning requests means that we now have to\ndeal with the notions of \u201cunlearning scope\u201d (or \u201cediting scope\u201d) and\n\u201centailment\u201d. That is, unlearning requests may provide canonical examples to\nindicate what to unlearn, but the same information can manifest in the\n(pre-)training set in many different forms with many different downstream\nimplications such that simply achieving unlearning on these examples\u2014even\nexactly\u2014would not suffice.\n\nFor example:\n\n  * The association \u201cBiden is the US president\u201d is dispersed throughout various forms of text from news articles, books, casual text messages, or this very blog post. Can we ever unlearn all occurrences? Moreover, does unlearning Joe Biden also entail unlearning the color of Biden\u2019s cat?\n  * Artists may request to unlearn art style by providing art samples, but they won\u2019t be able to collect everything they have on the internet and their adaptations.\n  * New York Times may request to unlearn news articles, but they cannot enumerate quotes and secondary transformations of these articles.\n\nSuch vagueness also suggests that unlearning pre-training data from large\nmodels are perhaps necessarily empirical: it is unlikely to derive formal\nguarantees if we can\u2019t clearly specify what to (and what not to) unlearn in\nthe trillions of tokens and establish clear information boundaries between\ndifferent entities. An interesting implication of achieving unlearning\nempirically is that the unlearning itself can be unlearned.\n\nWhat does existing work do, then, with underspecified unlearning requests?\nMost techniques are more or less the same as before, except now we also need\nto find the examples to fine-tune on. For example, attempting to unlearn Harry\nPotter involves asking GPT-4 to come up with plausible alternative text\ncompletions (e.g. that Mr. Potter studies baking instead of magic); and\nattempting to unlearn harmful behavior involves collecting examples of\nhatespeech.\n\nAnother set of techniques involves training the desired behavior (or its\nopposite) into task/control vectors and harnessing the capability of large\nmodels to undergo weight-space merging or activation steering. The fundamental\napproach of the above is more or less the same, nevertheless\u2014obtaining these\nedit vectors involves (heuristically) designing what gradients to take and\nwhat data on which to take them. One could also frame the unlearning problem\nas an alignment problem and applies the forget examples with a DPO-like\nobjective.\n\n### 2.5. Just ask for unlearning?\n\nIt turns out that powerful, instruction-following LLMs like GPT-4 are smart\nenough to pretend to unlearn. This means crafting prompts to induce a\n(sufficiently) safe behavior for the target unlearning application.\n\nThis is an interesting approach because no gradients are involved whatsoever\n(big plus from a systems perspective), and intuitively the end results could\nvery well be as good as existing empirical unlearning techniques. Among\ndifferent ways we could prompt, past work explored the following two\ndirections.\n\nLiterally asking to pretend unlearning. We can ask in the system prompt to,\nsay, pretend to not know who Harry Potter is. By design, this works best for\ncommon entities, facts, knowledge, or behaviors (e.g. the ability to utter\nlike Trump) that are well-captured in the pre-training set, since the LLM\nneeds to know it well to pretend not knowing it well. On the other hand,\nsuppose now we\u2019d like to unlearn the address of an obscure person; the pre-\ntraining set is so large that we suspect it\u2019s part of training data. We now\nface a variant of the Streisand effect: is it even worth asking the model to\npretend unlearning by accurately describing it in-context, and subsequently\nrisk leaking it in subsequent model responses?\n\nFew-shot prompting or \u201cin-context unlearning\u201d. Suppose we now have a clearly\ndefined set of forget examples with corresponding labels. We can flip their\nlabels and put them in the prompt, along with more retain examples with\ncorrect labels, with the intuition that the model would treat these falsely\nlabelled forget examples as truths and act accordingly\u2014much like one could\njailbreak a model this way.^4 Indeed, this works best when the forget examples\nand the counterfactual labels are clearly defined and (somewhat) finite. It\nmay work for factual associations (e.g. Paris is the captial of France) by\nenumerating a lot of examples, but unlikely to work for unlearning toxic\nbehaviors (where space of possible outputs is much larger).\n\nIn a sense, these approaches are complementary as they work for different\nkinds of unlearning requests.\n\nMore broadly, one could imagine a boxed LLM system for unlearning through\nprompting, where:\n\n  1. Only the input and output interfaces are exposed (like ChatGPT);\n  2. Different instances of a powerful LLM are responsible for accurately mimicking different parts of a desired unlearning behavior (for example, one LLM instance specializes in general trivia-style QA while anoother handles sequence completions);\n  3. An orchestrator/router LLM decides which unlearning worker instance to call depending on the input; and\n  4. A composer/summarizer LLM that drafts the final output conforming to the desired unlearning behavior; it may also apply some output filtering.\n\nSome readers may grumble about the heuristic nature of such prompting-based\ntechniques; that there is no proof of unlearning whatsoever. We should keep in\nmind that fine-tuning based empirical unlearning, as most recent approaches\ndo, is perhaps not fundamentally different. I think it ultimately comes down\nto the following questions:\n\n  1. Which of fine-tuning or prompting can better steer model behavior?\n  2. Which of them are less susceptible to attacks (exposing less surfaces and/or requiring more effort for an adversary to revert the unlearning)?\n\nMy intuition of our current models says that both questions point to fine-\ntuning based unlearning, but this is very much up for debate and can change as\nwe get more powerful models and better defense mechanisms. For example, the\nrecent notion of an instruction hierarchy may help make such as an LLM system\nless susceptible to malicious prompts.\n\nIt might be useful to note that humans don\u2019t really \u201cunlearn\u201d a piece of\nknowledge either.^5 In fact, by claiming to have unlearned something, we often\nhave: (1) not only learned it well to be able to make the very claim that we\nhave unlearned it, and (2) consciously decided that it\u2019s no longer useful /\nbeneficial to apply this knowledge to our current world state. Who is to say\nthat unlearning for LLMs should be any different?\n\n## 3\\. Evaluating unlearning\n\nUnlearning is messy for many reasons. But one of the biggest broken things\nabout unlearning is evaluation. In general, we care about three aspects:\n\n  * Efficiency: how fast is the algorithm compared to re-training?\n  * Model utility: do we harm performance on the retain data or orthogonal tasks?\n  * Forgetting quality: how much of the \u201cforget data\u201d is actually unlearned? How fast can we recover (re-learn) them?\n\nEvaluating efficiency and model utility are easier; we already measure them\nduring training. The key challenge is in understanding the forgetting\nquality.^6\n\nIf the forget examples are specified, this feels easy too. For example,\nunlearning a particular image class intuitively means getting a near-chance\naccuracy on the images in that class. An evaluation protocol may measure\naccuracy (high on retain & test set, low on forget set) or the likelihood of\nthe forget text sequences (lower the better).\n\nHowever, these intuitive choices of metrics aren\u2019t necessarily principled or\nextensible to settings like knowledge unlearning in LLMs. Expecting the model\nto perform poorly on an unlearned image ignores generalization, as the forget\nexamples could very well be an interpolation/duplicate of certain retain\nexamples. And we don\u2019t always have oracle models that have never seen the\nforget examples; e.g., do we have LLMs that have never seen New York Times\narticles?\n\nEvaluating unlearning on LLMs had been more of an art than science. For\nexample, to unlearn \u201cHarry Potter\u201d as an entity, people would visualize how\nthe token probabilities would decay for Harry Potter related text\u2014and some\nother folks would come along and show that the model can indeed still answer\nHarry Potter trivia questions. The key issue has been the desperate lack of\ndatasets and benchmarks for unlearning evaluation.\n\nSince 2024, nevertheless, the benchmarking crisis is getting better. There are\ntwo recent projects worth highlighting:\n\n  * TOFU: A benchmark focusing on unlearning individuals (specifically book authors). It involves asking GPT-4 to create fake author profiles, fine-tuning an LLM on them, and using the fine-tune as the unlearning target model and the original LLM as the oracle \u201cretrained\u201d model. It provides QA pairs on the generated fake authors to evaluate a model\u2019s knowledge of these authors before/after applying unlearning.\n  * WMDP: A benchmark focusing on unlearning dangerous knowledge, specifically on biosecurity, cybersecurity, and chemical security. It provides 4000+ multiple-choice questions to test a model\u2019s hazardous knowledge before/after applying unlearning. As part of the report the authors also propose an activation steering based empirical unlearning method.\n\nTOFU and WMDP depart from previous unlearning evaluation in that they are both\n\u201chigh-level\u201d and focus on the model\u2019s knowledge retention and understanding as\nopposed to example-level metrics like forget sequence perplexity. This is\nparticularly relevant for LLMs as they are generally capabale of giving the\nsame answer in many different ways that example-level metrics can\u2019t capture.\n\nLooking forward, I think application-oriented unlearning benchmarks like TOFU\nand WMDP, as opposed to instance-based evaluation like that of the NeurIPS\nunlearning challenge, are more useful for evaluating foundation models, owing\nto the multi-tasking nature of these models and the disparate definitions of\n\u201cunlearning success\u201d for each of these tasks. Indeed, one might imagine\nseparate benchmarks on unlearning personally identifiable information (PII),\ncopyrighted content, speech toxicity, or even model backdoors. For example,\nfor unlearning PII, we might care about exact token regurgitation, whereas for\ntoxicity, the unlearning metric would be the score reported by a ToxiGen\nclassifier.\n\n## 4\\. Practice, pitfalls, and prospects of unlearning\n\nUnlearning is a hard problem, especially in the context of foundation models.\nAs we actively research to make unlearning work in practice, it helps to\nphilosophize a bit on what unlearning really means and whether it is the right\nsolution for our current problems.\n\n### 4.1. The spectrum of unlearning hardness\n\nIntuitively, unlearning infrequent textual occurrences in LLMs like car\naccidents in Palo Alto should be easier than unlearning frequent occurrences\nlike \u201cBiden is the US president\u201d, which is in turn easier than unlearning\nfundamental facts like \u201cthe sun rises every day\u201d.\n\nThis spectrum of unlearning hardness emerges because as a piece of knowledge\nbecomes more fundamental, it will have more associations with other pieces of\nknowledge (e.g. as premises or corollaries) and an exponentially larger\nunlearning scope. In fact, a piece of knowledge can be so embedded in the\nmodel\u2019s implicit knowledge graph that it cannot be unlearned without\nintroducing contraditions and harming the model\u2019s utility.^7\n\nThis intuition implies that certain unlearning requests are much harder or\nsimply unsatisfiable (any attempts are bound to have flaws). Indeed, humans\nhave experiences that form the basis of their subsequent actions and world\nmodels; it is subjective, blurry, and philosophical as to what capacity can\nhumans unlearn their formative past memories.\n\nMore broadly, the unlearning hardness problem applies to all kinds of models,\nand for reasons beyond embeddedness in a knowledge/entailment graph. Let\u2019s\nconsider two more seemingly contradictory intuitions for unlearning hardness:\n\n  1. An example seen later in the training should be easy to unlearn, since the model would have moved only slightly in weight space (e.g. due to decayed learning rate) and one could either just revert gradients or revert to a previous checkpoint (if stored). In contrast, examples seen early gets \u201cbuilt on\u201d by later examples (in the curriculum learning sense), making them harder to unlearn.\n  2. An example seen later should be harder to unlearn, since examples seen earlier are gradually (or catastrophically) forgotten over the course of training; this may be especially true for LLMs.\n\nFailure to reconcile these intuition would suggest that the interplay across\nmemorization/forgetting, example importance (in the sense of data selection\nand coresets), learning hardness (in the sense of prediction flips), and\nunlearning hardness is unclear.\n\nHere are some interesting research questions:\n\n  * Is there a qualitative/fundamental difference between unlearning \u201ceasy\u201d data (e.g. a local news event) and \u201chard\u201d data (e.g. cats have four legs)?\n  * If there is a spectrum of unlearning hardness, does there exist a threshold to tell apart what is \u201ceasy\u201d and \u201chard\u201d, and thus what is unlearnable or shouldn\u2019t be unlearned? Does there exist, or can we train, such an oracle classifier? Can humans even tell?\n  * How does this relate to influence functions and data attribution? If a certain piece of knowledge (as it manifests in a model\u2019s output) can be attributed to a larger fraction of the training data, does it make it harder to unlearn?\n  * Can we benchmark how easy is it to unlearn something?\n\n### 4.2. Copyright protection\n\nOn the surface, unlearning seems to be a promising solution for copyright\nprotection: if a model violates the copyright of some content, we could\nattempt to unlearn said content. It is conceivable that to resolve copyright\nviolations via unlearning, provable and exact unlearning is necessary (and\npossibly sufficient); on the other hand, approximate unlearning, without\nguarantees and with the possibility of being hacked, is certainly insufficient\nand likely unnecessary.\n\nIn practice, however, there is a lot more nuance due to the questionable\neffectiveness of current unlearning methods and the unclear legal landscape at\nthe intersection of AI and copyright. Since I am no legal expert (and clearly\nnone of this section constitutes legal advice), we will mostly focus on asking\nquestions. The central question seems to be: is unlearning the right solution\nfor copyright protection?\n\nRecall that the fair use doctrine^8 permits limited use of copyrighted\nmaterial contigent on four factors: (1) purpose and character of the use\n(\u201ctransformativeness\u201d), (2) the nature of the copyrighted work, (3) amount and\nsubstantiality of the use, and (4) the effect on material\u2019s value. If the use\nof copyrighted content in a model qualifies as fair use, then unlearning such\ncontent from the model is unnecessary.\n\nSuppose a model is trained on some copyrighted content and is risking\ncopyright violation, as in New York Times v. OpenAI. Should OpenAI invest in\n(empirical) unlearning algorithms on ChatGPT? Or should they focus on the\ntransformativeness axis of fair use and invest in deploying empirical\nguardrails, such as prompting, content moderation, and custom alignment to\nprevent the model from regurgitating training data? The latter seems to be\nwhat\u2019s being implemented in practice.\n\nMore broadly, there could also be economic solutions to copyright violation as\nalternatives to unlearning. For example, model owners may provide an exact\nunlearning service (e.g. via periodic retraining) while also offering to\nindemnify model users for copyright infringement in the mean time, as seen in\nthe case of OpenAI\u2019s \u201cCopyright Shield\u201d. People are also starting to explore\nhow one may price copyrighted data using Shapley values. In general, it is\nunclear right now how much of a role (if any) unlearning will play for\nresolving copyright related issues. Exact unlearning (extending to retrieval-\nbased systems, see next section) does hold promises since deletion is clean\nand provable, but it seems that legally binding auditing procedures/mechanisms\nneed to be in place first.\n\n### 4.3. Retrieval-based AI systems\n\nAn obvious alternative to unlearning is to not learn at all. One way this\ncould manifest for an LLM is that we take all content from the pre-training\nset that may receive unlearning requests (e.g., New York Times articles) and\nput them to an external data/vector store. Any questions relating to them will\nthen be RAG\u2019ed during inference, and any unlearning requests can be trivially\nsatisfied by removing the data from the database. Min et al. demonstrates that\nthis approach can be competitive to (though not quite matching) the trained\nbaseline in terms of final perplexity.\n\nRetrieval-based solutions are promising because of the increasing capabilities\nof the base models to reason in-context. However, there are few considerations\nbefore taking retrieval systems as the no-brainer solution to unlearning:\n\n  1. Removing protected content from pre-training corpus can be a hard de-duplication problem. Much like removing data contamination is hard, how can we be sure that paraphrases, quotations/citations, or other adaptations of the protected content are removed?\n  2. What if the data to be unlearned can\u2019t be retrieved? Today we fine-tune many things into a model that aren\u2019t documents or knowledge items; for example, it is unclear (yet) if things like as human preferences and desired behaviors (e.g. ability to write concisely) can be \u201cretrieved\u201d from a database.\n  3. Dumping stuff in-context can open new attack surfaces. Many RAG methods for LLMs work by putting related content in-context and ask the model to reason on them. Having the protected data in-context means they are now more susceptible to data extraction (simple prompting attacks may work just fine).\n  4. Utility gap between retrieval and training. While there is evidence that retrieval-based solutions can be competitive, there is no general consensus that retrieval alone can replace fine-tune workloads; indeed, they can be complementary. More broadly, what if the space of unlearnable data is too large such that if all of it goes to an external store, the base model wouldn\u2019t be as useful?\n\n### 4.4. AI safety\n\nAs models become more capable and are granted agency, one concrete application\ndomain for unlearning that is gaining traction is AI safety.\n\nRoughly speaking, safety concerns stem from a model\u2019s knowledge (e.g., recipe\nof napalm), behaviors (e.g., exhibiting bias), and capabilities (e.g., hacking\nwebsites). Examining current AI systems and extrapolating forward, one may\nimagine the following examples to apply unlearning and improve AI safety:\n\n  * removing hazardous knowledge, as seen in the WMDP benchmark;\n  * removing model poisons and backdoors, where models respond to adversarially planted input triggers;\n  * removing manipulative behaviors, such as the ability to perform unethical persuasions or deception;\n  * removing bias and toxicity; or even\n  * removing power-seeking tendencies.\n\nFor safety-oriented applications, it is worth noting that unlearning should be\ntreated as a post-training risk mitigation and defense mechanism, alongside\nexisting tools like alignment fine-tuning and content filters. And as with any\ntool, we should view unlearning through its trade-offs in comparison to other\ntools in the toolbox (e.g., unlearning is more adaptive but more expensive\nthan content filters), as opposed to brushing it off because of the potential\nlack of guarantees and efficacy.\n\nAcknowledgements: The author would like to thank Aryaman Arora, Jiaao Chen,\nIrena Gao, John Hewitt, Shengyuan Hu, Peter Kairouz, Sanmi Koyejo, Xiang Lisa\nLi, Percy Liang, Eric Mitchell, Rylan Schaeffer, Yijia Shao, Chenglei Si,\nPratiksha Thaker, Xindi Wu for helpful discussions and feedback before and\nduring the drafting of this post. Any hot/bad takes are those of the author.\n\nCitation\n\nIf you find this post helpful, it can be cited as:\n\nLiu, Ken Ziyu. (Apr 2024). Machine Unlearning in 2024. Ken Ziyu Liu - Stanford\nComputer Science. https://ai.stanford.edu/~kzliu/blog/unlearning.\n\nOr\n\n    \n    \n    @misc{liu2024unlearning, title = {Machine unlearning in 2024}, author = {Liu, Ken Ziyu} journal = {Ken Ziyu Liu - Stanford Computer Science}, year = {2024}, month = {Apr}, url = {https://ai.stanford.edu/~kzliu/blog/unlearning}, }\n\n  1. Technically, SISA may not give exact unlearning in the sense of identical model distributions between the retrained model and the unlearned model, since after a sequence of unlearning requests, the data shards may end up in a state that we wouldn\u2019t otherwise get into in the first place (e.g., some shards have way more data than others after unlearning). For practical purposes, nevertheless, this is subtle enough that the nice properties about exact unlearning, as discussed later in the section, would still hold. \u21a9\n\n  2. It is also worth noting that the unlearning metric used in the NeurIPS unlearning challenge was disputed: why should we stick to a DP-like distributional closeness metric to a single secretly-kept re-trained model, when retraining itself can give a different model due to randomness? \u21a9\n\n  3. More broadly, \u201cunlearning\u201d falls under the umbrella of \u201cmodel editing\u201d in the sense that a deletion is also an edit. Similarly, one could argue that the concept of \u201ccontinual learning\u201d falls under the umbrella too, where an association (say an input/label pair, or a piece of factual association) is updated by deleting of an old association and creating a new, clearly specified association. One could imagine using continual learning to help achieve unlearning and vice versa. \u21a9\n\n  4. There is also evidence that in-context demonstrations mostly serve to elicit a particular behavior and that the labels don\u2019t even matter that much. It\u2019s unclear yet how we could reconcile this finding with \u201cin-context unlearning\u201d. \u21a9\n\n  5. Humans do forget things though, which is different. The ML analogy might be \u201ccatastrophic forgetting\u201d; humans similarly forget things under information overload. \u21a9\n\n  6. In particular, recall that for exact unlearning, understanding forgetting quality isn\u2019t strictly necessary because the algorithm would remove the forget data from the picture by construction (through retraining). Thus it may be acceptable even if the unlearned model does well on the forget set (as it could be a result of generalization from the retain set). We will focus the discussions of unlearning evaluation on approximate unlearning. \u21a9\n\n  7. Note that this \u201cembeddedness\u201d of a piece of data is related but distinct from whether the data is in or out of distribution, which should also affects how an unlearning algorithm should behave (e.g. unlearning a perfect inlier should be no-op for an ideal unlearning algorithm). \u21a9\n\n  8. Fair use is a doctrine applicable specifically in the United States. The reader should refer to related doctrines in corresponding jurisdictions, such as fair dealings in Commonwealth countries. \u21a9\n\n", "frontpage": true}
