{"aid": "40196410", "title": "Why PostgreSQL Is the Bedrock for the Future of Data", "url": "https://www.timescale.com/blog/postgres-for-everything/", "domain": "timescale.com", "votes": 1, "user": "redbell", "posted_at": "2024-04-29 09:52:32", "comments": 0, "source_title": "Why PostgreSQL Is the Bedrock for the Future of Data", "source_text": "Why PostgreSQL Is the Bedrock for the Future of Data\n\nOpens in a new window Opens an external website Opens an external website in a\nnew window\n\nThis website utilizes technologies such as cookies to enable essential site\nfunctionality, as well as for analytics, personalization, and targeted\nadvertising purposes. You may change your settings at any time or accept the\ndefault settings. You may close this banner to continue with only essential\ncookies. Privacy Policy\n\nContact us\n\nProducts\n\nCustomer stories\n\nDevelopers\n\nPricing\n\nContact us\n\nLogin\n\nProducts\n\nTimescale is a reliable PostgreSQL cloud optimized for your business\nworkloads.\n\nTime series and analytics\n\nPostgreSQL, but faster. Built for lightning-fast ingest and querying of time-\nbased data.\n\nEarly access\n\nVector (AI/ML)\n\nPostgreSQL engineered for fast search with high recall on millions of vector\nembeddings.\n\nEarly access\n\nDynamic PostgreSQL\n\nPostgreSQL managed services with the benefits of serverless, but none of the\nproblems.\n\nIndustries that rely on us\n\nCrypto and finance Energy and environment Transportation and logistics\n\nTimescale benchmarks\n\nvs RDS vs Amazon Timestream vs Influx vs MongoDB vs ClickHouse vs Aurora\n\nWe're in your corner even during the trial phase. Contact us to discuss your\nuse case with a Timescale technical expert.\n\nDevelopers\n\nTimescale Docs\n\nStart using and integrating Timescale for your demanding data needs.\n\nDocs\n\nAI / Vector\n\nTimescale Vector Docs\n\nTimescale Vector Docs\n\npgvector docs\n\npgvector docs\n\nLearn PostgreSQL\n\nTimescale is PostgreSQL, but faster. Learn the PostgreSQL basics and scale\nyour database performance to new heights.\n\nGetting started\n\nGetting started\n\nTime-series database basics\n\nTime-series database basics\n\nBuilding blocks\n\nBuilding blocks\n\nTimescale benchmarks\n\nTimescale benchmarks\n\nPostgres cheat sheet\n\nPostgres cheat sheet\n\nBlog\n\nBlog\n\nTutorials\n\nTutorials\n\nSupport\n\nSupport\n\nCommunity\n\nCommunity\n\nGithub\n\nGithub\n\nSlack\n\nSlack\n\nForum\n\nForum\n\nSubscribe to the Timescale Newsletter\n\nBy submitting, you acknowledge Timescale's Privacy Policy\n\nSubscribe to the Timescale Newsletter\n\nBy submitting, you acknowledge Timescale's Privacy Policy\n\nUse PostgreSQL for Everything\n\nSimplify your data stack without sacrificing speed, scale, and savings.\n\nLearn more\n\n# Why PostgreSQL Is the Bedrock for the Future of Data\n\n01\n\nPostgreSQL Is Becoming the De Facto Database Standard\n\n02\n\nEverything Is Becoming a Computer\n\n03\n\nThe Return of PostgreSQL\n\n04\n\nFree Yourself, Build the Future, Embrace PostgreSQL\n\n05\n\nTimescale Started as \u201cPostgreSQL for Time Series\u201d\n\n06\n\nTimescale Expanded Beyond Time Series\n\n07\n\nTimescale is now \u201cPostgreSQL made Powerful\u201d\n\n08\n\nCoda: Yoda?\n\nSee More\n\nOne of the biggest trends in software development today is the emergence of\nPostgreSQL as the de facto database standard. There have been a few blog posts\non how to use PostgreSQL for Everything, but none yet on why this is happening\n(and more importantly, why this matters). Until now.\n\n## PostgreSQL Is Becoming the De Facto Database Standard\n\nOver the past several months, \u201cPostgreSQL for Everything\u201d has become a growing\nwar cry among developers:\n\n> \u201cPostgreSQL isn\u2019t just a simple relational database; it\u2019s a data management\n> framework with the potential to engulf the entire database realm. The trend\n> of \u201cUsing Postgres for Everything\u201d is no longer limited to a few elite teams\n> but is becoming a mainstream best practice.\u201d (source)\n\n> \u201cOne way to simplify your stack and reduce the moving parts, speed up\n> development, lower the risk and deliver more features in your startup is\n> \u201cUse Postgres for everything.\u201d Postgres can replace\u2014up to millions of\n> users\u2014many backend technologies, Kafka, RabbitMQ, Mongo and Redis among\n> them.\u201d (source)\n\nGergely Orosz | Abhishek\n\n(Source)\n\n> \u201cWhen I first heard about Postgres (at a time when MySQL absolutely\n> dominated), it was described to me as \"that database made by those math\n> nerds,\" and then it occurred to me: yeah, those are exactly the people you\n> want making your database.\u201d (Source)\n\n> \u201cIt has made a remarkable comeback. Now that NoSQL is dead and Oracle owns\n> MySQL, what else is there?\u201d (Source)\n\n> \u201cPostgres is not just a relational DB. It's a way of life.\u201d (Source)\n\nThanks to its rock-solid foundation, plus its versatility through native\nfeatures and extensions, developers can now use PostgreSQL for Everything,\nreplacing complex, brittle data architectures with straightforward simplicity:\n\nThis might help explain why PostgreSQL last year took the top spot from MySQL\nin the rankings for most popular database among professional developers\n(60,369 respondents):\n\nWhich database environments have you done extensive development work in over\nthe past year, and which do you want to work in over the next year? More than\n49 % of respondents answered PostgreSQL. (Source)\n\nThose results are from the 2023 Stack Overflow Developer Survey. If you look\nacross time, you can see the steady increase in PostgreSQL adoption over the\npast few years:\n\nWhile PostgreSQL was the second favorite database of Stack Overflow\u2019s\nDeveloper Survey respondents between 2020-2022, its usage has consistently\nincreased. Source: 2020, 2021, 2022\n\nThis is not just a trend among small startups and hobbyists. In fact,\nPostgreSQL usage is increasing across organizations of all sizes:\n\nThe percentage of PostgreSQL usage by company size. (Source)\n\nAt Timescale, this trend is not new to us. We have been PostgreSQL believers\nfor nearly a decade. That\u2019s why we built our business on PostgreSQL, why we\nare one of the top contributors to PostgreSQL, why we run the annual State of\nPostgreSQL survey (referenced above), and why we support PostgreSQL meetups\nand conferences. Personally, I have been using PostgreSQL for over 13 years\n(when I switched over from MySQL).\n\nThere have been a few blog posts on how to use PostgreSQL for everything, but\nnone yet on why this is happening (and, more importantly, why this matters).\n\nUntil now.\n\nBut to understand why this is happening, we have to understand an even more\nfoundational trend and how that trend is changing the fundamental nature of\nhuman reality.\n\n## Everything Is Becoming a Computer\n\nEverything\u2014our cars, our homes, our cities, our farms, our factories, our\ncurrencies, our things\u2014is becoming a computer. We, too, are becoming digital.\nEvery year, we digitize more of our own identity and actions: how we buy\nthings, how we entertain ourselves, how we collect art, how we find answers to\nour questions, how we communicate and connect with each other, how we express\nwho we are.\n\nTwenty-two years ago, this idea of \u201cubiquitous computing\u201d seemed audacious.\nBack then, I was a graduate student at the MIT AI Lab, working on my thesis on\nIntelligent Environments. My research was supported by MIT Project Oxygen,\nwhich had a noble, bold goal: to make computing as pervasive as the air we\nbreathe. To put that time period in perspective: we had our own server rack in\na closet.\n\nA lot has changed since then. Computing is now ubiquitous: on our desks, in\nour pockets, in our things, and in our \u201ccloud.\u201d That much we predicted.\n\nBut the second-order effects of those changes were not what most of us\nexpected:\n\n  * Ubiquitous computing has led to ubiquitous data. With each new computing device, we collect more information about our reality: human data, machine data, business data, environmental data, and synthetic data. This data is flooding our world.\n  * The data flood has led to a Cambrian explosion of databases. All these new sources of data have required new places to store them. Twenty years ago, there were maybe five viable database options. Today there are several hundred, most of them specialized for specific use cases or data, with new ones emerging each month.\n  * More data and more databases has led to more software complexity. Choosing the right database for your software workload is no longer easy. Instead, developers are forced to cobble together complex architectures that might include: a relational database (for its reliability), a non-relational database (for its scalability), a data warehouse (for its ability to serve analysis), an object store (for its ability to cheaply archive old data). This architecture might even have more specialized components, like a time-series or vector database.\n  * More complexity means less time to build. Complex architectures are more brittle, require more complex application logic, offer less time for development, and slow down development. Complexity is not a benefit but a real cost.\n\nAs computing has become more ubiquitous, our reality has become more entwined\nwith computing. We have brought computing into our world and ourselves into\nits world. We are no longer just our offline identities but a hybrid of what\nwe do offline and online.\n\nSoftware developers are humanity\u2019s vanguard in this new reality. We are the\nones building the software that shapes this new reality.\n\nBut developers are now flooded with data and drowning in database complexity.\n\nThis means that developers\u2014instead of shaping the future\u2014are spending more and\nmore of their time managing the plumbing.\n\nHow did we get here?\n\n### Part 1: Cascading computing waves\n\nUbiquitous computing has led to ubiquitous data. This did not happen overnight\nbut in cascading waves over several decades:\n\n  * Mainframes (1950s+)\n  * Personal Computers (1970s+)\n  * Internet (1990s+)\n  * Mobile (2000s+)\n  * Cloud Computing (2000s+)\n  * Internet of Things (2010s+)\n\nWith each wave, computers have become smaller, more powerful, and more\nubiquitous. Each wave also built on the previous one: personal computers are\nsmaller mainframes; the Internet is a network of connected computers;\nsmartphones are even smaller computers connected to the Internet; cloud\ncomputing democratized access to computing resources; the Internet of Things\nis smartphone components reconstructed as part of other physical things\nconnected to the Cloud.\n\nBut in the past two decades, computing advances have not just occurred in the\nphysical world but also in the digital one, reflecting our hybrid reality:\n\n  * Social networks (2000+)\n  * Blockchains (2010s+)\n  * Generative AI (2020s+)\n\nWith each new wave of computing, we get new sources of information about our\nhybrid reality: human digital exhaust, machine data, business data, and\nsynthetic data. Future waves will create even more data. All this data fuels\nnew waves, the latest of which is Generative AI, which in turn further shapes\nour reality.\n\nComputing waves are not siloed but cascade like dominoes. What started as a\ndata trickle soon became a data flood. And then the data flood has led to the\ncreation of more and more databases.\n\n### Part 2: Incremental database growth\n\nAll these new sources of data have required new places to store them\u2014or\ndatabases.\n\nMainframes started with the Integrated Data Store (1964) and later System R\n(1974), the first SQL database. Personal computers fostered the rise of the\nfirst commercial databases: Oracle (1977), inspired by System R; DB2 (1983);\nand SQL Server (1989), Microsoft\u2019s response to Oracle.\n\nThe collaborative power of the Internet enabled the rise of open-source\nsoftware, including the first open-source databases: MySQL (1995), PostgreSQL\n(1996). Smartphones led to the proliferation of SQLite (initially created in\n2000).\n\nThe Internet also created a massive amount of data, which led to the first\nnon-relational, or NoSQL, databases: Hadoop (2006); Cassandra (2008); MongoDB\n(2009). Some called this the era of \u201cBig Data.\u201d\n\n### Part 3: Explosive database growth\n\nAround 2010, we started to hit a breaking point. Up until that point, software\napplications would primarily rely on a single database\u2014e.g., Oracle, MySQL,\nPostgreSQL\u2014and the choice was relatively easy.\n\nBut \u201cBig Data\u201d kept getting bigger: the Internet of Things led to the rise of\nmachine data; smartphone usage started growing exponentially thanks to the\niPhone and Android, leading to even more human digital exhaust; cloud\ncomputing democratized access to compute and storage, amplifying these trends.\nGenerative AI very recently made this problem worse with the creation of\nvector data.\n\nAs the volume of data collected grew, we saw the rise of specialized\ndatabases: Neo4j for graph data (2007), Redis for a basic key-value store\n(2009), InfluxDB for time-series data (2013), ClickHouse for high-scale\nanalytics (2016), Pinecone for vector data (2019), and many, many more.\n\nTwenty years ago, there were maybe five viable database options. Today, there\nare several hundred, most of them specialized for specific use cases, with new\nones emerging each month. While the earlier databases promise general\nversatility, these specialized ones offer specific trade-offs, which may or\nmay not make sense depending on your use case.\n\n### Part 4: More databases, more problems\n\nFaced with this flood and with specialized databases with a variety of trade-\noffs, developers had no choice but to cobble together complex architectures.\n\nThese architectures typically include a relational database (for reliability),\na non-relational database (for scalability), a data warehouse (for data\nanalysis), an object store (for cheap archiving), and even more specialized\ncomponents like a time-series or vector database for those use cases.\n\nBut more complexity means less time to build. Complex architectures are more\nbrittle, require more complex application logic, offer less time for\ndevelopment, and slow down development.\n\nThis means that instead of building the future, software developers find\nthemselves spending far too much time maintaining the plumbing. This is where\nwe are today.\n\nThere is a better way.\n\n## The Return of PostgreSQL\n\nThis is where our story takes a twist. Our hero, instead of being a shiny new\ndatabase, is an old stalwart, with a name only a mother core developer could\nlove: PostgreSQL.\n\nAt first, PostgreSQL was a distant number two behind MySQL. MySQL was easier\nto use, had a company behind it, and a name that anyone could easily\npronounce. But then MySQL was acquired by Sun Microsystems (2008), which was\nthen acquired by Oracle (2009). And software developers, who saw MySQL as the\nfree savior from the expensive Oracle dictatorship, started to reconsider what\nto use.\n\nAt that same time, a distributed community of developers, sponsored by a\nhandful of small independent companies, was slowly making PostgreSQL better\nand better. They quietly added powerful features, like full-text search\n(2008), window functions (2009), and JSON support (2012). They also made the\ndatabase more rock-solid, through capabilities like streaming replication, hot\nstandby, in-place upgrade (2010), logical replication (2017), and by\ndiligently fixing bugs and smoothing rough edges.\n\n### PostgreSQL is now a platform\n\nOne of the most impactful capabilities added to PostgreSQL during this time\nwas the ability to support extensions: software modules that add functionality\nto PostgreSQL (2011). Extensions enabled even more developers to add\nfunctionality to PostgreSQL independently, quickly, and with minimal\ncoordination.\n\nThanks to extensions, PostgreSQL started to become more than just a great\nrelational database. Thanks to PostGIS, it became a great geospatial database;\nthanks to TimescaleDB, it became a great time-series database; hstore, a key-\nvalue store; AGE, a graph database; pgvector, a vector database. PostgreSQL\nbecame a platform.\n\nNow, developers can use PostgreSQL for its reliability, scalability (replacing\nnon-relational databases), data analysis (replacing data warehouses), and\nmore.\n\n### What about Big Data?\n\nAt this point, the smart reader should ask, \u201cWhat about big data?\u201d. That\u2019s a\nfair question. Historically, \u201cbig data\u201d (e.g., hundreds of terabytes or even\npetabytes)\u2014and the related analytics queries\u2014has been a bad fit for a database\nlike PostgreSQL that doesn\u2019t scale horizontally on its own.\n\nThat, too, is changing. Last November, we launched \u201ctiered storage,\u201d which\nautomatically tiers your data between disk and object storage (S3),\neffectively creating the ability to have an infinite table.\n\nSo while \u201cBig Data\u201d has historically been an area of weakness for PostgreSQL,\nsoon, no workload will be too big.\n\nPostgreSQL is the answer. PostgreSQL is how we free ourselves and build the\nfuture.\n\n## Free Yourself, Build the Future, Embrace PostgreSQL\n\nInstead of futzing with several different database systems, each with its own\nquirks and query languages, we can rely on the world\u2019s most versatile and,\npossibly, most reliable database: PostgreSQL. We can spend less time on the\nplumbing and more time on building the future.\n\nAnd PostgreSQL keeps getting better. The PostgreSQL community continues to\nmake the core better. There are many more companies contributing to PostgreSQL\ntoday, including the hyperscalers.\n\nToday's PostgreSQL ecosystem (Source)\n\nThere are also more innovative, independent companies building around core to\nmake the PostgreSQL experience better: Supabase (2020) is making PostgreSQL\ninto a viable Firebase alternative for web and mobile developers; Neon (2021)\nand Xata (2022) are both making PostgreSQL scale-to-zero for intermittent\nserverless workloads; Tembo (2022) is providing out-of-the-box stacks for\nvarious use cases; Nile (2023) is making PostgreSQL easier for SaaS\napplications; and many more.\n\nAnd, of course, there\u2019s us, Timescale (2017).\n\n## Timescale Started as \u201cPostgreSQL for Time Series\u201d\n\nThe Timescale story will probably sound a little familiar: we were solving\nsome hard sensor data problems for IoT customers, and we were drowning in\ndata. To keep up, we built a complex stack that included at least two\ndifferent database systems (one of which was a time-series database).\n\nOne day, we reached our breaking point. In our UI, we wanted to filter devices\nby both device_type and uptime. This should have been a simple SQL join. But\nbecause we were using two different databases, it instead required writing\nglue code in our application between our two databases. It was going to take\nus weeks and an entire engineering sprint to make the change.\n\nThen, one of our engineers had a crazy idea: Why don\u2019t we just build a time-\nseries database right in PostgreSQL? That way, we would just have one database\nfor all our data and would be free to ship software faster. Then we built it,\nand it made our lives so much easier. Then we told our friends about it, and\nthey wanted to try it. And we realized that this was something that we needed\nto share with the world.\n\nSo, we open-sourced our time-series extension, TimescaleDB, and announced it\nto the world on April 4, 2017. Back then, PostgreSQL-based startups were quite\nrare. We were one of the first.\n\nIn the seven years since, we\u2019ve heavily invested in both the extension and in\nour PostgreSQL cloud service, offering a better and better PostgreSQL\ndeveloper experience for time-series and analytics: 350x faster queries, 44 %\nhigher inserts via hypertables (auto-partitioning tables); millisecond\nresponse times for common queries via continuous aggregates (real-time\nmaterialized views); 90 %+ storage cost savings via native columnar\ncompression; infinite, low-cost object storage via tiered storage; and more.\n\n## Timescale Expanded Beyond Time Series\n\nThat\u2019s where we started, in time-series data, and also what we are most known\nfor.\n\nBut last year we started to expand.\n\n### Timescale Vector\n\nWe launched Timescale Vector (\u201cPostgreSQL++ for AI applications\u201d), which makes\nPostgreSQL an even better vector database. Timescale Vector scales to over 100\nmillion vectors, building on pgvector with even better performance. Innovative\ncompanies and teams are already using Timescale Vector in production at a\nmassive scale, including OpenSauced, a GitHub events insights platform, at\n100+ million vectors; VieRally, a social virality prediction platform, at 100+\nmillion vectors; and MarketReader, a financial insights platform, at 30+\nmillion vectors.\n\nTimescale Vector enhances pgvector. Get the performance of a specialized\nvector database without the hassle of learning and maintaining one.\n\n### PopSQL\n\nRecently, we also acquired PopSQL to build and offer the best PostgreSQL UI.\nPopSQL is the SQL editor for team collaboration, with autocomplete, schema\nexploration, versioning, and visualization. Hundreds of thousands of\ndevelopers and data analysts have used PopSQL to work with their data, whether\non PostgreSQL, Timescale, or other data sources like Redshift, Snowflake,\nBigQuery, MySQL, SQL Server, and more.\n\nPopSQL is the SQL editor for team collaboration\n\n### Insights\n\nWe also launched \u201cInsights,\u201d the largest dogfooding effort we\u2019ve ever\nundertaken, which tracks every database query to help developers monitor and\noptimize database performance. Insights overcomes several limitations of\npg_stat_statements (the official extension to see statistics from your\ndatabase). The scale has been massive and is a testament to our product\u2019s (and\nteam\u2019s) capability: over one trillion normalized queries (i.e., queries whose\nparameter values have been replaced by placeholders) have been collected,\nstored, and analyzed, with over 10 billion new queries ingested every day.\n\n## Timescale is now \u201cPostgreSQL made Powerful\u201d\n\nToday, Timescale is PostgreSQL made Powerful\u2014at any scale. We now solve hard\ndata problems\u2014that no one else does\u2014not just in time series but in AI, energy,\ngaming, machine data, electric vehicles, space, finance, video, audio, web3,\nand much more.\n\nWe believe that developers should be using PostgreSQL for everything, and we\nare improving PostgreSQL so that they can.\n\nCustomers use Timescale not just for their time-series data but also for their\nvector data and general relational data. They use Timescale so that they can\nuse PostgreSQL for Everything. You can too: get started here for free.\n\n## Coda: Yoda?\n\nOur human reality, both physical and virtual, offline and online, is filled\nwith data. As Yoda might say, data surrounds us, binds us. This reality is\nincreasingly governed by software, written by software developers, by us.\n\nIt\u2019s worth appreciating how remarkable that is. Not that long ago, in 2002,\nwhen I was an MIT grad student, the world had lost faith in software. We were\nrecovering from the dotcom bubble collapse. Leading business publications\nproclaimed that \u201cIT Doesn\u2019t Matter.\u201d Back then, it was easier for a software\ndeveloper to get a good job in finance than in tech\u2014which is what many of my\nMIT classmates did, myself included.\n\nBut today, especially now in this world of generative AI, we are the ones\nshaping the future. We are the future builders. We should be pinching\nourselves.\n\nEverything is becoming a computer. This has largely been a good thing: our\ncars are safer, our homes are more comfortable, and our factories and farms\nare more productive. We have instant access to more information than ever\nbefore. We are more connected with each other. At times, it has made us\nhealthier and happier.\n\nBut not always. Like the force, computing has both a light and dark side.\nThere has been growing evidence that mobile phones and social media are\ndirectly contributing to a global epidemic of teen mental illness. We are\nstill grappling with the implications of AI and synthetic biology. As we\nembrace our greater power, we should recognize that it comes with\nresponsibility.\n\nWe have become the stewards of two valuable resources that affect how the\nfuture is built: our time and our energy.\n\nWe can either choose to spend those resources on managing the plumbing or\nembrace PostgreSQL for Everything and build the right future.\n\nI think you know where we stand.\n\nThanks for reading. #Postgres4Life\n\n(Source)\n\nIngest and query in milliseconds, even at terabyte scale.\n\nThis post was written by\n\n  * Ajay Kulkarni\n\n24 Apr 2024 14 min read\n\nPostgreSQL, Blog\n\nContributors\n\n  * Ajay Kulkarni\n\nShare\n\n## Related posts\n\nEngineering\n\n## How We Made Real-Time Data Aggregation in Postgres Faster by 50,000%\n\n20 Mar 2024 14 min read\n\nLearn how we accelerated real-time data aggregation in PostgreSQL by 50,000 %\nby tweaking the query planner.\n\nPostgreSQL, Blog\n\n## Time-Series Forecasting With TimescaleDB and Prophet\n\n9 Apr 2024 8 min read\n\nA tutorial on how to use TimescaleDB and Prophet for time-series forecasting\nand efficient data analysis.\n\nAI\n\n## A Python Library for Using PostgreSQL as a Vector Database in AI\nApplications\n\n28 Sep 2023 9 min read\n\nSeamlessly integrate PostgreSQL as your vector database into Python generative\nAI, RAG, or chatbot applications.\n\nShare this post\n\nSubscribe to the Timescale Newsletter By submitting you acknowledge\nTimescale's Privacy Policy.\n\n2024 \u00a9 Timescale, Inc. All Rights Reserved.\n\nPrivacy preferences Legal Privacy Sitemap\n\n", "frontpage": false}
