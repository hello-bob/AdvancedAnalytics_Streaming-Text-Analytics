{"aid": "40222674", "title": "Postgres Mistakes That Will Cause Outages", "url": "https://stepchange.work/blog/3-postgresql-mistakes-that-will-cause-outages", "domain": "stepchange.work", "votes": 1, "user": "thunderbong", "posted_at": "2024-05-01 13:10:43", "comments": 0, "source_title": "3 PostgreSQL Mistakes That Will Cause Outages \u2013 StepChange", "source_text": "3 PostgreSQL Mistakes That Will Cause Outages \u2013 StepChange\n\nBack to Homepage\n\n  * Home\n  * Blogs\n  * 3 PostgreSQL Mistakes That Will Cause Outages\n\nShare Article\n\n  * Link copied!Copy\n  * Email\n  * Discord\n  * Telegram\n  * X.com\n  * LinkedIn\n  * Facebook\n\n# 3 PostgreSQL Mistakes That Will Cause Outages\n\nNiall OHiggins\n\nPublished: 30th April 2024\n\nEffective Postgres management often involves navigating complex issues that\ncan impact database performance and stability. By examining real-world\nscenarios I encountered at Coinbase and Nextdoor, this post presents a\ntechnical exploration of these challenges, offering guidance to teams seeking\nto enhance their database operations.\n\n## Query Plan Degradation Post-Upgrade at Coinbase\n\nWhen I was a senior engineering leader at Coinbase, my team was responsible\nfor a large number of business-critical services backed by Postgres databases\nwhich needed to be upgraded on a tight schedule. The vast majority of these\ndatabase upgrades went smoothly and there was little or no service impact.\n\nHowever, for one particularly important service, the database upgrade caused\nthe performance of common queries to drop to such a level that the service\nbecame saturated. Queries were so slow that requests were backing up and the\nservice was lagging behind transaction activity on the blockchain.\n\nDiagnosing and Fixing the Slowdown\n\nInvestigation with EXPLAIN ANALYZE revealed that the query planner was no\nlonger utilizing indexes effectively, opting instead for less efficient\nsequence scans. Once we realized that the statistics needed to be updated, we\nexecuted ANALYZE thus updating the query planner statistics and informing the\ndatabase that it should use the index instead of performing a sequential scan.\nWith query performance back to pre-upgrade levels, we quickly cleared the task\nbacklog and service returned to normal.\n\nKey tools and commands for addressing this issue include:\n\n\\- EXPLAIN ANALYZE: Essential for identifying whether PostgreSQL is performing\na sequence scan instead of using an index. It provides a detailed breakdown of\nthe query execution plan.\n\n\\- ANALYZE: This command updates the database statistics, allowing the query\nplanner to make better execution decisions. It's a critical step in restoring\nquery performance post-upgrade.\n\n\\- ANALYZE VERBOSE: This gives you more feedback and sense of progress than\nplain ANALYZE and also shows the statistical sample size which can be useful\nwhen you need to tweak them.\n\nAvoiding Upgrade Management: Serverless Postgres\n\nSomething I wish we had when I was in this situation was a modern serverless\nPostgres vendor such as neon.tech or Timescale.\n\nHaving the database hosting provider fully manage engine upgrades on your\nbehalf removes a massive operational headache. It's not an option in all\nenvironments, but is definitely worth exploring.\n\nFor those looking to consider alternate vendors or automate database upgrades\nwhile minimizing the risk of such issues, StepChange's Postgres Performance &\nCost Review service offers a solution that includes automatic statistics\nupdates as part of the upgrade & migration process.\n\n## Handling ALTER TABLE Operations in Production: Lessons from Nextdoor\n\nDuring my time as an engineering leader at Nextdoor, my team was responsible\nfor the production deployment process. During one deployment, we encountered\nsignificant downtime during a routine Django migration that changed a column\ntype in a production database. This operation, while trivial in a development\nsetting, caused extensive locking on a table with millions of rows,\neffectively halting the entire site.\n\nThe Core Issue\n\nDirectly altering the type of a column in a large production database can lead\nto extensive locking. This is because the database must access and potentially\nupdate every row in the table to apply the change, which can be highly\nresource-intensive and time-consuming.\n\nRecommended Approach\n\nBased on this experience, the engineering team at Nextdoor adopted several\nbest practices for schema modifications:\n\n\\- Avoid Direct Type Changes: Rather than altering a column's type, the\npreferred approach is to add a new column with the desired type. This method\nis less invasive and avoids extensive table locks.\n\n\\- Data Migration Scripts: Use scripts to migrate data from the old column to\nthe new one, ensuring data consistency without impacting database performance.\n\n\\- Ensure Backward Compatibility: Application code should be compatible with\nboth the old and new schema versions to facilitate seamless rollbacks if\nneeded.\n\nTo prevent recurrence, Nextdoor required database schema changes to be\nreviewed by database experts, introducing a checkpoint that, while slowing\ndown the development process, significantly reduced the risk of operational\nissues.\n\nTools and Takeaways\n\nFor automated early detection of potentially disruptive schema changes,\nStepChange DB Guard is a tool that can be integrated with your CI/CD pipeline\nto identify such issues before they hit production. It's also important to be\nmindful of more nuanced changes, such as adjusting the precision of numeric\ntypes or the maximum size of varchar columns, which can have unexpected\nimpacts on database performance.\n\n## Lock Contention and pg_locks NOT GRANTED in Postgres\n\nAt both Nextdoor and Coinbase, my teams dealt with high lock contention in\nPostgres databases - indicated by a high rate of pg_locks not being granted.\nThis situation leads to performance issues and can cause outages if not\nproperly managed.\n\nExample Lock Contention Scenario\n\nA simple but somewhat realistic (if silly) scenario which can cause very high\nlock contention in Postgres is the following:\n\n  * Imagine you have a high-traffic website with content stored in a pages table.\n  * You want to track when the page was last accessed, so you add a last_accessed column to the pages table with a timestamp.\n  * On every page view, your web application runs an UPDATE query to set the last_accessed value to the current time.\n  * With high enough traffic, you could be trying to update that last_accessed timestamp many, many times per second! This will cause a ton of lock contention in the database.\n  * The easy fix in this scenario is to change the schema so that you run an INSERT on a different table like page_views with an append-only data model. This will avoid UPDATE to existing rows which will contend for locks with each other.\n\nDiagnosing Lock Contention\n\nTo identify and analyze lock contention, Postgres offers a feature to log lock\nwaits (log_lock_waits). This functionality helps pinpoint exactly which\noperations are waiting for locks and how long they're being blocked. For more\ndetailed insights into lock waits and contention, the Postgres documentation\nprovides a comprehensive guide here.\n\nMitigation Techniques\n\nSeveral strategies can be implemented to mitigate lock contention:\n\n\\- Implement Lock Timeout and Retry Logic: Setting a lock_timeout prevents\noperations from waiting indefinitely for locks, and incorporating retry logic\nin the application can help manage the impact of lock waits.\n\n\\- Optimize Operation Ordering: By analyzing dependencies between operations,\nit's possible to reorder transactions to minimize locking conflicts.\n\n\\- Split Transactions: Separating transactions that affect different resources\ncan reduce the number of locks each transaction requires, thus lowering\ncontention.\n\n\\- Ensure Consistent Lock Ordering: Acquiring locks in a consistent order\nacross transactions helps avoid deadlocks.\n\n\\- Reduce Concurrency and Connection Pool Size: Lowering the level of\nconcurrency and the size of the connection pool can help alleviate contention.\n\n\\- Decrease Write Latency: Techniques such as removing unnecessary indexes,\noptimizing transactions, using Heap-Only Tuples (HOT) updates, tuning the\nvacuum process, disabling synchronous_commit, and upgrading database hardware\ncan contribute to reduced write latency and lock contention.\n\n## Great Postgres Tools And Services Are Available\n\nIf you are facing similar issues with your Postgres performance or cost, it\ncan be very challenging to solve them without the relevant expertise or tools.\nBeing able to integrate automatic tools to detect & prevent database outages\ninto your CI/CD pipeline is a huge confidence boost for your engineering team.\nIt's also great to have external experts on-call to assist with risky upgrades\nand migrations - as well as help to resolve any cost or performance issues.\n\nStepChange offers services like Postgres Performance & Cost Review, Data\nMigration & Modernization as well as Oracle to Postgres Migration.\nAdditionally, we provide a DB Guard product which can integrate with your\nCI/CD pipeline to find issues before they reach production and cause an\noutage.\n\nDon't hesitate to contact us if you need any advice or assistance!\n\nThanks to Casey Duncan, Harry Tormey and Joe Drumgoole for reviewing this post\nand providing feedback.\n\nContentsQuery Plan Degradation Post-Upgrade at CoinbaseHandling ALTER TABLE\nOperations in Production: Lessons from NextdoorLock Contention and pg_locks\nNOT GRANTED in PostgresGreat Postgres Tools And Services Are Available\n\n## You may also like...\n\nScaling PostgreSQL: Aurora's Pros And Cons?23rd April 2024Harry TormeyHow Do\nAI Software Engineers Really Compare To Humans?9th April 2024Harry TormeyWhy\nare people migrating from dbt Cloud to dbt Core?22nd February 2024Harry Tormey\n\n## Get In Touch\n\nContact StepChange to see how we can help you build better apps & databases\ntoday.\n\nhello@stepchange.com\n\nGet in Touch.hello@stepchange.com\n\nAll Rights Reserved C StepChange Labs, Inc 2024.\n\n", "frontpage": false}
