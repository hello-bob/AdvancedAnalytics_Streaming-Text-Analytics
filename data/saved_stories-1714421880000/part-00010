{"aid": "40198621", "title": "Actually Using SORA: the story behind SORA short film Air Head", "url": "https://www.fxguide.com/fxfeatured/actually-using-sora/", "domain": "fxguide.com", "votes": 1, "user": "lnyan", "posted_at": "2024-04-29 14:15:42", "comments": 0, "source_title": "Actually Using SORA", "source_text": "Actually Using SORA - fxguide\n\nSkip to content\n\n# Actually Using SORA\n\n#### Posted by Mike Seymour ON April 14, 2024\n\n  * Facebook 0\n  * Twitter\n  * LinkedIn\n\nIn February, we pushed our first story on SORA; OpenAI had just released the\nfirst clips from SORA, which we described at the time as the video equivalent\nof DALL\u00b7E for videos. SORA is a diffusion model that generates videos\nsignificantly longer and with more cohesion than any of its rivals. By giving\nthe model foresight of many frames at a time, they have solved the challenging\nproblem of ensuring a subject stays consistent even when it goes out of view\ntemporarily. SORA can generate entire videos, all at once up to a minute in\nlength. At the time, OpenAI also published technical notes indicating that it\ncould (in the future) extend generated videos to make them longer or blend two\nvideos seamlessly.\n\nPatrick Cederberg\n\nSeveral select production teams have been given limited access to SORA in the\nlast few weeks. One of the most high-profile was the Shy Kids team, who\nproduced the SORA short film Air Head. Sidney Leeder produced the film. Walter\nWoodman was the writer and director, while Patrick Cederberg was responsible\nfor the post-production. The Toronto team have been nicknamed \u201cpunk-rock\nPixar\u201d, while their work has garnered Emmy nominations and been long-listed\nfor the Oscars. We sat down this week with Patrick for a long chat about the\ncurrent state of SORA.\n\nShy Kids is a Canadian production company renowned for its eclectic and\ninnovative approach to media production. Originating as a collective of\ncreatives from various disciplines, including film, music, and television, Shy\nKids has gained recognition for its unique narrative styles and engaging\ncontent. The company often explores adolescence, social anxiety, and the\ncomplexities of modern life while maintaining a distinctively whimsical and\nheartfelt tone. Their work showcases a keen eye for visual storytelling and\noften features a strong integration of original music, making their\nproductions resonant and memorable. Shy Kids has successfully carved out a\nniche by embracing new AI technology and creativity, pushing what is possible.\n\n### SORA : Mid-April \u201924.\n\nSORA is in development and is actively being improved through feedback from\nteams such as Shy Kids, but here is how it currently works. It is important to\nappreciate that SORA is effective almost pre-alpha. It has not been released\nnor is it in beta.\n\n\u201cGetting to play with it was very interesting,\u201d Patrick comments. \u201cIt\u2019s a\nvery, very powerful tool that we\u2019re already dreaming up all the ways it can\nslot into our existing process. But I think with any generative AI tool;\ncontrol is still the thing that is the most desirable and also the most\nelusive at this point.\u201d\n\n### UI\n\nThe user interface allows an artist to input a text prompt; OpenAI\u2019s ChatGPT\nthen converts this into a longer string, which triggers the clip generation.\nAt the moment, there is no other input; it is yet to be multimodal. This is\nsignificant as while SORA is correctly applauded for its object consistency\nduring a shot, but there is nothing to help make anything from the first shot\nmatch in a second shot. The results would be different even if you ran the\nsame prompt a second time. \u201cThe closest we could get was just being hyper-\ndescriptive in our prompts,\u201d Patrick explains. \u201cExplaining wardrobe for\ncharacters, as well as the type of balloon, was our way around consistency\nbecause shot to shot / generation to generation, there isn\u2019t the feature set\nin place yet for full control over consistency.\u201d\n\nThe individual clips are remarkable and jaw-dropping for the technology they\nrepresent, but the use of the clips depends on your understanding of implicit\nor explicit shot generation. Suppose you ask SORA for a long tracking shot in\na kitchen with a banana on a table. In that case, it will rely on its implicit\nunderstanding of \u2018banana-ness\u2019 to generate a video showing a banana. Through\ntraining data, it has \u2018learnt\u2019 the implicit aspects of banana-ness: such as\n\u2018yellow\u2019, \u2018bent\u2019, \u2018has dark ends\u2019, etc. It has no actual recorded images of\nbananas. It has no \u2018banana stock library\u2019 database; it has a much smaller\ncompressed hidden or \u2018latent space\u2019 of what a banana is. Every time it runs,\nit shows another interpretation of that latent space. Your prompt replies on\nan implicit understanding of banana-ness.\n\n### Prompting the right thing to make Sonny\n\nFor Air Head, the scenes were made by generating multiple clips to an\napproximate script, but there was no explicit way to have the actual yellow\nballoon head the same from shot to shot. Sometimes, when the team prompted for\na yellow balloon, it wouldn\u2019t even be yellow. Other times, it had a face\nembedded in it or a face seemingly drawn on the front of the balloon. As many\nballoons have string, often the Air Head character, nicknamed Sonny, the\nballoon guy, would have a string down the front of the character\u2019s shirt.\nSince it implicitly links string with balloons and thus these would need to be\nremoved in post.\n\nAn unwanted face on the balloon, from a raw SORA output.\n\n### Resolution\n\nAir Head is only using SORA-generated footage, but much of it was graded,\ntreated, and stabilised, and all of it was upscaled or upresed. The clips the\nteam worked with were generated at a lower resolution and then upresed using\nAI tools outside SORA or OpenAI. \u201cYou can do up to 720 P (resolution),\u201d\nPatrick explains. \u201cI believe there\u2019s a 1080 feature that\u2019s out, but it takes a\nwhile (to render). We did all of Air Head at 480 for speed and then upright\nusing Topaz.\u201d\n\n### Prompting \u2018time\u2019: A slot machine.\n\nThe original prompt is automatically expanded but also displayed along a\ntimeline. \u201cYou can go into those larger keyframes and start adjusting\ninformation based on changes you want generated.\u201d Parick explains, \u201cThere\u2019s a\nlittle bit of temporal control about where these different actions happen in\nthe actual generation, but it\u2019s not precise... it\u2019s kind of a shot in the dark\n\u2013 like a slot machine \u2013 as to whether or not it actually accomplishes those\nthings at this point.\u201d Of course, Shy Kids were working with the earliest of\nprototypes, and SORA is still constantly being worked on.\n\nIn addition to choosing a resolution, SORA allows the user to pick the aspect\nratio, such as portrait or landscape (or square). This came in handy on the\nshot that pans up from Sonny\u2019s jeans to his balloon head. Unfortunately, SORA\nwould not render such a move natively, always wanting the main focus of the\nshot\u2014the balloon head\u2014to be in the shot. So the team rendered the shot in\nportrait mode and then manually, via cropping, created the pan-up in post.\n\n### Prompting camera directions\n\nFor many genAI tools, a valuable source of information is the metadata that\ncomes with the training data, such as camera metadata. For example, if you\ntrain on still photos, the camera metadata will provide the lens size, the\nf-stop and many other critical pieces of information for the model to train\non. With cinematic shots, the ideas of \u2018tracking\u2019, \u2018panning\u2019, \u2019tilting\u2019 or\n\u2018pushing in\u2019 are all not terms or concepts captured by metadata. As much as\nobject permanency is critical for shot production, so is being able to\ndescribe a shot, which Patrick noted was not initially in SORA. \u201cNine\ndifferent people will have nine different ideas of how to describe a shot on a\nfilm set. And the (OpenAI) researchers, before they approached artists to play\nwith the tool, hadn\u2019t really been thinking like filmmakers.\u201d Shy Kids knew\nthat their access was very early, but \u201cthe initial version about camera angles\nwas kind of random.\u201d Whether or not SORA was actually going to register the\nprompt request or understand it was unknown as the researchers had just been\nfocused on image generation. Shy Kids were almost shocked by how much the\nOpenAI was surprised by this request. \u201cBut I guess when you\u2019re in the silo of\njust being researchers, and not thinking about how storytellers are going to\nuse it... SORA is improving, but I would still say the control is not quite\nthere. You can put in a \u2018Camera Pan\u2019 and I think you\u2019d get it six out of 10\ntimes.\u201d This is not a unique problem nearly all the major video genAI\ncompanies are facing the same issue. Runway AI is perhaps the most advanced in\nproviding a UI for describing the camera\u2019s motion, but Runway\u2019s quality and\nlength of rendered clips are inferior to SORA.\n\n### Render times\n\nClips can be rendered in varying segments of time, such as 3 secs, 5 sec, 10\nsec, 20sec, up to a minute. Render times vary depending on the time of day and\nthe demand for cloud usage. \u201cGenerally, you\u2019re looking at about 10 to 20\nminutes per render,\u201d Patrick recalls. \u201cFrom my experience, the duration that I\nchoose to render has a small effect on the render time. If it\u2019s 3 to 20\nseconds, the render time tends not to vary too much from between a 10 to\n20-minute range. We would generally do that because if you get the full 20\nseconds, you hope you have more opportunities to slice/edit stuff out and\nincrease your chances of getting something that looks good.\u201d\n\n### Roto\n\nWhile all the imagery was generated in SORA, the balloon still required a lot\nof post-work. In addition to isolating the balloon so it could be re-coloured,\nit would sometimes have a face on Sonny, as if his face was drawn on with a\nmarker, and this would be removed in AfterEffects. similar other artifacts\nwere often removed.\n\nOriginal SORA clip\n\n### Editing a 300:1 shooting ratio\n\nThe Shy Kids methodology was to approach post-production and editing like a\ndocumentary, where there is a lot of footage, and you weave a story from that\nmaterial rather than strictly shooting to a script. There was a script for the\nshort film, but the team needed to be agile and adapt. \u201cIt was just getting a\nwhole bunch of shots and trying to cut it up in an interesting way to the VO,\u201d\nPatrick recalls.\n\nFor the minute and a half of footage that ended up in the film, Patrick\nestimated that they generated \u201chundreds of generations at 10 to 20 seconds a\npiece\u201d. Adding, \u201cMy math is bad, but I would guess probably 300:1 in terms of\nthe amount of source material to what ended up in the final.\u201d\n\n### Comping multiple takes and retiming\n\nOn Air Head, the team did not comp multiple takes together. For example, the\nshots of the balloon drifting over the motor racing were all generated in the\none shot fairly much as seen. However, they are working on a new film that\nmixes and composites multiple takes into one clip.\n\nInterestingly, many of the Air Head clips were generated as if shot in slow\nmotion, while this was not requested in the prompt. This happened for unknown\nreasons, and so many of the clips had to be retimed to appear to have been\nshot in real-time. Clearly, this is easier to do than the reverse of slowing\ndown rapid motion, but still, it seems like an odd aspect to have been\ninferred from the training data. \u201cI don\u2019t know why, but it does seem like a\nlot of clips at 50 to 75% speed,\u201d he adds. \u201cSo there was quite a bit of\nadjusting timing to keep it all from feeling like a big slowmo project.\u201d\n\n### Lighting and grading\n\nShy Kids used the term \u201835 mm film\u2018 in their prompts as a keyword and\ngenerally found that the prompt 35mm gave a level of consistency that they\nsought. \u201cIf we needed a high contrast, we could say high contrast, and say key\nlighting would generally give us something that was close,\u201d says Patrick. \u201cWe\nstill had to take it through a full color grade, and we did our own digital\nfilmic look, where we applied grain and flicker to just sort of meld it all\ntogether.\u201d There is no option for additional passes such as mattes or depth\npasses.\n\n### Copyright\n\nOpenAI is trying to be respectful and not allow material to be generated that\nviolates copyright or produces images that would appear to be from someone\nthey are not. For example, if you prompt something such as 35mm film in a\nfuturistic spaceship, a man walks forward with a light sword, SORA will not\nallow the clip to be generated as it is too close to Star Wars. But the Shy\nKids accidentally bumped into this during early testing. Patrick recalls that\nwhen they initially sat down and just wanted to test SORA, \u201cWe had that one\nshot behind the character\u2019s back; it\u2019s kind of that Aronofsky following shot.\nAnd I think it was just my dumb brain, as I was tired, but I put \u2018Aronofsky\ntype shot\u2019 in and got hit with a can\u2019t do that.,\u201d he recalls. Hitchcock Zoom\nwas another thing that came up as something that is now by osmosis, a\ntechnical term, but SORA would reject the prompt for copyright purposes.\n\n### Sound\n\nShy Kids are known for their audio skills in addition to their visual skills.\nThe music in the short film is their own. \u201cIt was a song we had in the back\ncatalogue that we almost immediately decided on because the song\u2019s called The\nWind, \u201d says Patrick. \u201cWe all just liked it.\u201d\n\nPatrick himself is the voice of Sonny. \u201cSometimes we\u2019d feel pacing-wise the\nfilm needed another beat. So I would write another line, record it, and come\nup with some more SORA generations, which is another powerful use of the tool\nin the post: when you\u2019re in a corner, and you need to fill a gap, it\u2019s a great\nway to start brainstorming and just spit clips out to see what you can use to\nfill the pacing problem.\u201d\n\n### Summary\n\nSORA is remarkable; the Shy Kids team produced Air Head with a team of just 3\npeople in around 1.5 to 2 weeks. The team is already working on a wonderful,\nself-aware, and perhaps ironic sequel. \u201cThe follow-up is a journalistic\napproach to Sonny, the balloon guy, and his reaction to fame and subsequent\nsort falling out with the world,\u201d says Patrick. \u201cAnd we\u2019re exploring new\ntechniques!\u201d The team is looking to be a bit more technical in their\nexperimentation, incorporating AE composting of SORA elements into real live-\naction footage and using SORA as a supplementary VFX tool.\n\nSORA is very new, and even the basic framework that OpenAI has sketched out\nand demonstrated for SORA has yet to be available for early tests to use. It\nis doubtful that SORA in its current form will be released anytime soon, but\nit is an incredible advance in a particular type of implicit image generation.\nFor high-end projects, it may be a while before it allows the level of\nspecificity that a director requires. It will be more than \u2018close enough\u2019 for\nmany others while delivering stunning imagery. Air Head still needed a large\namount of editorial and human direction to produce this engaging and funny\nstory film. \u201cI just feel like people have to SORA as an authentic part of\ntheir process; however, if they don\u2019t want to engage with anything like that,\nthat\u2019s fine too.\u201d\n\n## fxguide social\n\n## Get email updates whenever new articles are posted\n\ncopyright \u00a9 fxguide, LLC\n\n", "frontpage": false}
