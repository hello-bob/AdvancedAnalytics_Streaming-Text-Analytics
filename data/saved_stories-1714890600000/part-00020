{"aid": "40259803", "title": "From Bigram to Infinigram", "url": "https://snats.xyz/pages/articles/from_bigram_to_infinigram.html", "domain": "snats.xyz", "votes": 1, "user": "snats", "posted_at": "2024-05-04 19:40:25", "comments": 0, "source_title": "snats website", "source_text": "snats website\n\n# From Bigram to Infinigram\n\nI have been considering a lot more how n-grams are amazing tools to show some\nof the fundamentals of language models. With the new paper of infini-gram I\nthink they are becoming more relevant again. So I decided to write an\nexplanation on them.\n\nStatistical Language Models can be represented as the probability of the last\nn-tokens times the current token ^1. More formally:\n\n# Bigrams\n\nBigrams are the special case of an n-gram that is constrained to be based on\nonly the previous token to predict the next one.\n\nTo train a bigram, you first generate a vocabulary of unique words from the\ncorpus that you are using and then count all of the pairs of words that happen\nin it. Finally, you estimate the bigram probabilities by dividing the count of\neach bigram by the count of the first word in it.\n\nIn real life, to avoid divisions by zero you can treat probabilities as log\nprobabilities.\n\nAs you can see, there are a lot of zeros in the matrix. And as you scale up\nn-grams to higher dimensions like pentagrams^2 you are going to make really\nsparse matrices that take incredible amounts of space. Let\u2019s not forget that\nthe curse of dimensionality in language bites really hard and pretty fast for\nthese statistical models.\n\n# Infinigram\n\nA way to avoid this problem if you don\u2019t have enough data to find the specific\nn-gram is by backing off, going n \u2212 1 until you find your probability for the\nnext token. So infinigram solves this problem by always assuming the biggest\npossible n and then backing off to find the biggest possible n-gram in your\nsentence.\n\nFor example:\n\n    \n    \n    the quick brown fox jumps over the lazy -> NOT FOUND. quick brown fox jumps over the lazy -> NOT FOUND. brown fox jumps over the lazy -> NOT FOUND. fox jumps over the lazy -> NOT FOUND. over the lazy -> dog.\n\nI really like the ngram approach to language because intuitively it feels like\nyou are talking to a collective. The n-grams are just snippets of sentences in\nthe training data that try to predict the next token. And even in their\ncurrent state (not winning against LLMs), there are still some good reasons to\nuse them like for query searches and factual checking.\n\nIn the paper, they never evaluate infinigram alone, instead they use a mixture\nof a Large Language Model (LLM) with infinigram to enhance the outputs of LLMs\nand get reductions of perplexity of up to 40% on tiny models like GPT-2 and\n~20% on things like Llama-2.\n\nAlthough in the paper they quite clearly mention that infinigram is not yet\nready to generate open ended text, they have a public api here, so let\u2019s try\nit out.\n\n# Infinigram generation\n\nI created a quick and dirty python script that calls the infinigram api and\ncan be found at my mono repo for teMporary ideas m.\n\n    \n    \n    while True: payload = { 'corpus': 'v4_dolma-v1_6_llama', 'query_type': 'ntd', 'query': state, } result = requests.post('https://api.infini-gram.io/', json=payload).json() if not result['result_by_token_id']: state = ' '.join(state.split()[1:]) print(\"Current state:\", state) continue token_id = next(iter(result['result_by_token_id'])) token_info = result['result_by_token_id'][token_id] token = token_info['token'] # append the new token # ... there is more code here but abbreviated to make it look nicer. state += \" \" + token final_output += \" \" + token if len(final_output.split()) >= 100: break if '<0x0A>' in state or len(state) == 0: break print(final_output, end='\\r')\n\nThe results fill like a blast from the past for language models or like a\nlanguage model that has not been aligned yet. After experimenting a little\nwith the model, here are some of the prompts with its responses.\n\nPROMPT: \u201cThe circumference of the sun\u201d\n\nRESPONSE:\n\n> \"in miles is 2,730,000. An odd coincidence as the mile is not a standard\n> based on any calculation other than itself. Perhaps the unit of length\n> called the foot did not originate from a part of anatomy of an ancient king\n> or pharaoh but from circumference of our planet?\"\n\nPROMPT: \u201cDo you dream?\u201d\n\nRESPONSE:\n\n> \"Consider what it might mean to be a literalist of the imagination, and\n> where it might lead you\u2014to what places?\"\n\nPROMPT: \u201cthe best movie of all time is\u201d\n\nRESPONSE:\n\n> \"Schindler\u2019s list. The 1993 winner of the Academy Award for Best Picture\n> stars Liam Nelson as Schindler. It is regarded as the greatest holocaust\n> movie of all time and is not suitable for viewers under the ages of 18. This\n> is because of its scenes of sexual content.\"\n\n# Conclusion\n\nEven if these models are not the best at generating text, I genuinely believe\nthat old ideas are making a comeback but now with a much bigger scale^3. Maybe\nI\u2019ll create some perplexity measurements for infinigram. For now, I have some\nnostalgia glasses for this LM generating some cute text.\n\n# Footnotes\n\n  1. If it is hard to reason this, just think about it as the following. \u201cThe probability of the next word based on the previous words\u201d replace token with word.\u21a9\ufe0e\n\n  2. A pentagram of a corpus of 1.4 trillion tokens if saved on a matrix would be 28TB in size. As per the infinigram paper.\u21a9\ufe0e\n\n  3. There is a reason why RWKV exists.\u21a9\ufe0e\n\nBack to articles\n\nby Santiago Pedroza, 2024-05-03.\n\n", "frontpage": false}
