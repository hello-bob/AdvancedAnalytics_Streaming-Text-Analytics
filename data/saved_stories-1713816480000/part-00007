{"aid": "40112411", "title": "Grant Kubernetes Pods Access to AWS Services Using OpenID Connect", "url": "https://developer-friendly.blog/2024/04/22/grant-kubernetes-pods-access-to-aws-services-using-openid-connect/", "domain": "developer-friendly.blog", "votes": 1, "user": "meysamazad", "posted_at": "2024-04-22 08:30:06", "comments": 0, "source_title": "Grant Kubernetes Pods Access to AWS Services Using OpenID Connect - Developer Friendly Blog", "source_text": "Grant Kubernetes Pods Access to AWS Services Using OpenID Connect - Developer\nFriendly Blog\n\nSkip to content\n\n\ud83d\udd0a My latest book \ud83d\udcd6, Ultimate Docker for Cloud-Native Applications \ud83d\udc0b, is out!\nGrab your copy now! \ud83c\udff7\ufe0f\n\n# Grant Kubernetes Pods Access to AWS Services Using OpenID Connect\u00b6\n\nLearn how to establish a trust relationship between a Kubernetes cluster and\nAWS IAM to grant cluster generated Service Account tokens access to AWS\nservices using OIDC & without storing long-lived credentials.\n\n## Introduction\u00b6\n\nIn our previous post, we discussed what OpenID Connect (OIDC) is and how to\nuse it to authenticate identities from one system to another.\n\nWe covered why it is crucial to avoid storing long-lived credentials and the\nbenefits of employing OIDC for the task of authentication.\n\nIf you haven't read that one already, here's a recap:\n\n  * OIDC is an authentication protocol that allows the identities in one system to authenticate to another system.\n  * It is based on OAuth 2.0 and JSON Web Tokens (JWT).\n  * Storing long-lived credentials is risky and should be avoided at all cost if possible.\n  * OIDC provides a secure way to authenticate identities without storing long-lived credentials.\n  * It is widely used in modern applications and systems.\n  * The hard requirements is that both the Service Provider and the Identity Provider must be OIDC compliant.\n  * With OIDC you will only keep the identities and their credentials in one system and authenticate them to another system without storing any long-lived credentials. The former is called the Identity Provider and the latter is called the Service Provider.\n\nWe also covered a practical example of authenticating GitHub runners to AWS\nIAM by establishing a trust relationship between GitHub and AWS using OIDC.\n\nIn this post, we will take it one step further and provide a way for the pods\nof our Kubernetes cluster to authenticate to AWS services using OIDC.\n\nThis post will provide a walkthrough of granting such access to a bear-metal\nKubernetes cluster (k3s^1) using only the power of OpenID Connect protocol. In\na later post, we'll show you how easy it is to achieve the same with a managed\nKubernetes cluster like Azure Kubernetes Service (AKS)^2. But, first let's\nunderstand the fundamentals by trying it on a bear-metal cluster.\n\nWe will not store any credentials in our pods and as such, won't ever have to\nworry about other security concerns such as secret rotations!\n\nWith that intro out of the way, let's dive in!\n\n## Prerequisites\u00b6\n\nMake sure you have the following prerequisites in place before proceeding:\n\n  * A Kubernetes cluster that can be exposed to the internet.\n\n  * An AWS account to create an OIDC provider and IAM roles.\n\n  * A verified root domain name that YOU own. Skip this if you're using a managed Kubernetes cluster.\n  * OpenTofu v1.6^4\n  * Ansible v2.16^5\n\n## Roadmap\u00b6\n\nLet's see what we are trying to achieve in this guide.\n\nOur end goal is to create an Identity Provider (IdP) in AWS^6. After doing so,\nwe will be able to create an IAM Role^7 with a trust relationship to the IdP.\n\nUltimately, the pods in our Kubernetes cluster that have the desired Service\nAccount(s)^8 will be able to talk to the AWS services.\n\nTo achieve this, and as per the OIDC specification, the following endpoints\nmust be exposed through an HTTPS endpoint with a verified TLS certificate^9:\n\n  * /.well-known/openid-configuration: This is a MUST for OIDC compliance.\n  * /openid/v1/jwks: This is configurable through the first endpoint as you'll see later.\n\nThese endpoints provide the information of the OIDC provider and the public\nkeys used to sign the JWT tokens, respectively. The former will be used by the\nservice provider to validate the OIDC provider and the latter will be used to\nvalidate the JWT access tokens provided by the entities that want to talk to\nthe Serivce Provider.\n\nService Provider\n\nService Provider refers to the host that provides the service. In our example,\nAWS is the service provider.\n\nExposing such endpoints will make our OIDC provider compliant with the OIDC\nspecification. In that regard, any OIDC compliant service provider will be\nable to trust our OIDC provider.\n\nOIDC Compliant\n\nFor an OIDC provider and a Service Provider to trust each other, they must\nboth be OIDC compliant. This means that the OIDC provider must expose certain\nendpoints and the Service Provider must be able to validate the OIDC provider\nthrough those endpoints.\n\nIn practice, we will need the following two absolute URLs to be accessible\npublicly through internet with a verified TLS certificate signed by a trusted\nCertificate Authority (CA):\n\n  * https://mydomain.com/.well-known/openid-configuration\n  * https://mydomain.com/openid/v1/jwks\n\nAgain, and just to reiterate, as per the OIDC specification the HTTPS is a\nmust and the TLS certificate has to be signed by a trusted Certificate\nAuthority (CA).\n\nWhen all this is set up, we shall be able to add the https://mydomain.com to\nthe AWS as an OIDC provider.\n\n## Step 0: Directory Structure\u00b6\n\nThere are a lot of codes we will cover in this post. It is good to know that\nto expect. Here's the layout of the directories we will be working with:\n\n    \n    \n    . \u251c\u2500\u2500 ansible.cfg \u251c\u2500\u2500 app/\n    \n    \u251c\u2500\u2500 configure-oidc/\n    \n    \u251c\u2500\u2500 inventory/\n    \n    \u251c\u2500\u2500 k8s/\n    \n    \u251c\u2500\u2500 playbook.yml\n    \n    \u251c\u2500\u2500 provision-k8s/\n    \n    \u251c\u2500\u2500 requirements.yml\n    \n    \u2514\u2500\u2500 vars/\n\n## Step 1: Dedicated Domain Name\u00b6\n\nAs mentioned, we need to assign a dedicated domain name to the OIDC provider.\nThis will be the address we will add to the AWS IAM as an Identity Provider.\n\nAny DNS provider will do, but for our example, we're using Cloudflare.\n\nprovision-k8s/variables.tf\n\n    \n    \n    variable \"hetzner_api_token\" { type = string nullable = false sensitive = true } variable \"cloudflare_api_token\" { type = string nullable = false sensitive = true } variable \"stack_name\" { type = string default = \"k3s-cluster\" } variable \"primary_ip_datacenter\" { type = string default = \"nbg1-dc3\" } variable \"root_domain\" { type = string default = \"developer-friendly.blog\" }\n\nprovision-k8s/versions.tf\n\n    \n    \n    terraform { required_providers { hcloud = { source = \"hetznercloud/hcloud\" version = \"~> 1.46\" } cloudflare = { source = \"cloudflare/cloudflare\" version = \"~> 4.30\" } random = { source = \"hashicorp/random\" version = \"~> 3.6\" } } } provider \"hcloud\" { token = var.hetzner_api_token } provider \"cloudflare\" { api_token = var.cloudflare_api_token }\n\nprovision-k8s/network.tf\n\n    \n    \n    resource \"hcloud_primary_ip\" \"this\" { for_each = toset([\"ipv4\", \"ipv6\"]) name = \"${var.stack_name}-${each.key}\" datacenter = var.primary_ip_datacenter type = each.key assignee_type = \"server\" auto_delete = false }\n\nprovision-k8s/dns.tf\n\n    \n    \n    data \"cloudflare_zone\" \"this\" { name = var.root_domain } resource \"random_uuid\" \"this\" {} resource \"cloudflare_record\" \"this\" { zone_id = data.cloudflare_zone.this.id name = \"${random_uuid.this.id}.${var.root_domain}\" proxied = false ttl = 60 type = \"A\" value = hcloud_primary_ip.this[\"ipv4\"].ip_address } resource \"cloudflare_record\" \"this_v6\" { zone_id = data.cloudflare_zone.this.id name = \"${random_uuid.this.id}.${var.root_domain}\" proxied = false ttl = 60 type = \"AAAA\" value = hcloud_primary_ip.this[\"ipv6\"].ip_address }\n\nprovision-k8s/outputs.tf\n\n    \n    \n    output \"public_ip\" { value = hcloud_primary_ip.this[\"ipv4\"].ip_address } output \"public_ipv6\" { value = hcloud_primary_ip.this[\"ipv6\"].ip_address }\n\nWe would need the required access token which you can get from their\nrespective account settings. If you want to apply the stack, you will need a\nCloudflare token^10 and a Hetzner API token^11.\n\n    \n    \n    export TF_VAR_cloudflare_api_token=\"PLACEHOLDER\" export TF_VAR_hetzner_api_token=\"PLACEHOLDER\" tofu plan -out tfplan tofu apply tfplan\n\n## Step 2: A Live Kubernetes Cluster\u00b6\n\nAt this point, we should have a live Kuberntes cluster. We've already covered\nhow to set up a lightweight Kubernetes cluster on a Ubuntu 22.04 machine\nbefore and so, we won't go too deep into that.\n\nBut for the sake of completeness, we'll resurface the code one more time, with\nsome minor tweaks here and there.\n\nprovision-k8s/variables.tf\n\n    \n    \n    variable \"hetzner_api_token\" { type = string nullable = false sensitive = true } variable \"cloudflare_api_token\" { type = string nullable = false sensitive = true } variable \"stack_name\" { type = string default = \"k3s-cluster\" } variable \"primary_ip_datacenter\" { type = string default = \"nbg1-dc3\" } variable \"root_domain\" { type = string default = \"developer-friendly.blog\" } variable \"server_datacenter\" { type = string default = \"nbg1\" } variable \"username\" { type = string default = \"k8s\" }\n\nprovision-k8s/versions.tf\n\n    \n    \n    terraform { required_providers { hcloud = { source = \"hetznercloud/hcloud\" version = \"~> 1.46\" } cloudflare = { source = \"cloudflare/cloudflare\" version = \"~> 4.30\" } random = { source = \"hashicorp/random\" version = \"~> 3.6\" } http = { source = \"hashicorp/http\" version = \"~> 3.4\" } tls = { source = \"hashicorp/tls\" version = \"~> 4.0\" } } } provider \"hcloud\" { token = var.hetzner_api_token } provider \"cloudflare\" { api_token = var.cloudflare_api_token }\n\nprovision-k8s/server.tf\n\n    \n    \n    resource \"tls_private_key\" \"this\" { algorithm = \"ECDSA\" ecdsa_curve = \"P384\" } resource \"hcloud_ssh_key\" \"this\" { name = var.stack_name public_key = tls_private_key.this.public_key_openssh } resource \"hcloud_server\" \"this\" { name = var.stack_name server_type = \"cax11\" image = \"ubuntu-22.04\" location = \"nbg1\" ssh_keys = [ hcloud_ssh_key.this.id, ] public_net { ipv4 = hcloud_primary_ip.this[\"ipv4\"].id ipv6 = hcloud_primary_ip.this[\"ipv6\"].id } user_data = <<-EOF #cloud-config users: - name: ${var.username} groups: users, admin, adm sudo: ALL=(ALL) NOPASSWD:ALL shell: /bin/bash ssh_authorized_keys: - ${tls_private_key.this.public_key_openssh} packages: - certbot package_update: true package_upgrade: true runcmd: - sed -i -e '/^\\(#\\|\\)PermitRootLogin/s/^.*$/PermitRootLogin no/' /etc/ssh/sshd_config - sed -i -e '/^\\(#\\|\\)PasswordAuthentication/s/^.*$/PasswordAuthentication no/' /etc/ssh/sshd_config - sed -i '$a AllowUsers ${var.username}' /etc/ssh/sshd_config - | curl https://get.k3s.io | \\ INSTALL_K3S_VERSION=\"v1.29.3+k3s1\" \\ INSTALL_K3S_EXEC=\"--disable traefik --kube-apiserver-arg=service-account-jwks-uri=https://${cloudflare_record.this.name}/openid/v1/jwks --kube-apiserver-arg=service-account-issuer=https://${cloudflare_record.this.name} --disable-network-policy --flannel-backend none --write-kubeconfig /home/${var.username}/.kube/config --secrets-encryption\" \\ sh - - chown -R ${var.username}:${var.username} /home/${var.username}/.kube/ - | CILIUM_CLI_VERSION=v0.16.4 CLI_ARCH=arm64 curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/$CILIUM_CLI_VERSION/cilium-linux-$CLI_ARCH.tar.gz{,.sha256sum} sha256sum --check cilium-linux-$CLI_ARCH.tar.gz.sha256sum sudo tar xzvfC cilium-linux-$CLI_ARCH.tar.gz /usr/local/bin - kubectl completion bash | tee /etc/bash_completion.d/kubectl - k3s completion bash | tee /etc/bash_completion.d/k3s - | cat << 'EOF2' >> /home/${var.username}/.bashrc alias k=kubectl complete -F __start_kubectl k EOF2 - reboot EOF }\n\nprovision-k8s/firewall.tf\n\n    \n    \n    data \"http\" \"this\" { url = \"https://checkip.amazonaws.com\" } resource \"hcloud_firewall\" \"this\" { name = var.stack_name rule { direction = \"in\" protocol = \"tcp\" port = 22 source_ips = [format(\"%s/32\", trimspace(data.http.this.response_body))] description = \"Allow SSH access from my IP\" } rule { direction = \"in\" protocol = \"tcp\" port = 80 source_ips = [ \"0.0.0.0/0\", \"::/0\", ] description = \"Allow HTTP access from everywhere\" } rule { direction = \"in\" protocol = \"tcp\" port = 443 source_ips = [ \"0.0.0.0/0\", \"::/0\", ] description = \"Allow HTTPS access from everywhere\" } depends_on = [ hcloud_server.this, ] } resource \"hcloud_firewall_attachment\" \"this\" { firewall_id = hcloud_firewall.this.id server_ids = [hcloud_server.this.id] }\n\nprovision-k8s/outputs.tf\n\n    \n    \n    output \"public_ip\" { value = hcloud_primary_ip.this[\"ipv4\"].ip_address } output \"public_ipv6\" { value = hcloud_primary_ip.this[\"ipv6\"].ip_address } output \"ssh_private_key\" { value = tls_private_key.this.private_key_pem sensitive = true } output \"ansible_inventory_yaml\" { value = <<-EOF k8s: hosts: ${var.stack_name}: ansible_host: ${hcloud_server.this.ipv4_address} ansible_user: ${var.username} ansible_ssh_private_key_file: ~/.ssh/k3s-cluster ansible_ssh_common_args: '-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o PasswordAuthentication=no' EOF } output \"ansible_vars\" { value = <<-EOF domain_name: ${cloudflare_record.this.name} EOF } output \"oidc_provider_url\" { value = cloudflare_record.this.name }\n\nNotice the lines where we specify the OIDC issuer URL & JWK URL for the\nKubernetes API server to be a publicly accessible address and pass it as an\nargument to the k3s server.\n\nprovision-k8s/server.tf\n\n    \n    \n    --kube-apiserver-arg=service-account-jwks-uri=https://${cloudflare_record.this.name}/openid/v1/jwks --kube-apiserver-arg=service-account-issuer=https://${cloudflare_record.this.name}\n\nIf not specified, the rest of this guide won't work and additional\nconfiguration is required. In summary, these are the URLs that will be used by\nthe Service Provider when trying to verify the OIDC provider & the access\ntokens of the Service Accounts.\n\nBusiness as usual, we apply the stack as below.\n\n    \n    \n    tofu plan -out tfplan tofu apply tfplan\n\nAnd for connecting to the machine:\n\n    \n    \n    tofu output -raw ssh_private_key > ~/.ssh/k3s-cluster chmod 600 ~/.ssh/k3s-cluster IP_ADDRESS=$(tofu output -raw public_ip) ssh -i ~/.ssh/k3s-cluster k8s@$IP_ADDRESS\n\nTo be able to use the Ansible playbook in the next steps, we shall write the\ninventory where Ansible expects them.\n\n    \n    \n    mkdir -p ../inventory/group_vars tofu output -raw ansible_inventory_yaml > ../inventory/k3s-cluster.yml tofu output -raw ansible_vars > ../inventory/group_vars/all.yml\n\nAt this stage we're ready to move on to the next step.\n\n## Step 3: Bootstrap the Cluster\u00b6\n\nAt this point we have installed the Cilium binary in our host machine, yet we\nhaven't installed the CNI plugin in our Kubernetes cluster.\n\nLet's create an Ansible role and a playbook to take care of all the Day 1\noperations.\n\n    \n    \n    ansible-galaxy init k8s touch playbook.yml\n\nThe first step is to install the Cilium CNI.\n\nk8s/defaults/main.yml\n\n    \n    \n    --- cilium_version: 1.15.4\n\nk8s/tasks/cilium.yml\n\n    \n    \n    --- - name: Install cilium ansible.builtin.command: cmd: cilium install --set kubeProxyReplacement=true --wait --version {{ cilium_version }} register: cilium_install changed_when: false ignore_errors: true environment: KUBECONFIG: /etc/rancher/k3s/k3s.yaml\n\nk8s/tasks/main.yml\n\n    \n    \n    --- - name: Cilium include_tasks: cilium.yml tags: cilium\n\nplaybook.yml\n\n    \n    \n    --- - name: Bootstrap k8s node hosts: k3s-cluster gather_facts: false become: true roles: - k8s\n\nTo run the playbook:\n\n    \n    \n    ansible-playbook playbook.yml\n\n## Step 4: Fetch the TLS Certificate\u00b6\n\nAt this point, we need a CA verified TLS certificate for the domain name we\ncreated in the first step.\n\nWe will carry our tasks with Ansible throughout the entire Day 1 to Day n\noperations.\n\nk8s/defaults/main.yml\n\n    \n    \n    --- cilium_version: 1.15.4 acme_home: /var/www/html\n\nk8s/templates/wellknown-server.service.j2\n\n    \n    \n    [Unit] Description=Wellknown Server [Service] ExecStartPre=/bin/mkdir -p {{ acme_home }}/.well-known/acme-challenge ExecStart=/usr/bin/python3 -m http.server -d {{ acme_home }} 80 Restart=on-failure WorkingDirectory={{ acme_home }} User=acme Group=acme RestartSec=5 AmbientCapabilities=cap_net_bind_service [Install] WantedBy=multi-user.target\n\nk8s/handlers/main.yml\n\n    \n    \n    --- - name: Restart wellknown-server ansible.builtin.systemd: name: wellknown-server state: restarted daemon_reload: true\n\nk8s/tasks/certbot.yml\n\n    \n    \n    --- - name: Create acme group ansible.builtin.group: name: acme state: present system: true - name: Create acme user ansible.builtin.user: name: acme state: present group: acme shell: /bin/false system: true create_home: false - name: Create working dir for acme user ansible.builtin.file: path: \"{{ acme_home }}\" state: directory owner: acme group: acme mode: \"0755\" - name: Create an standalone server to respond to challenges ansible.builtin.template: src: wellknown-server.service.j2 dest: /etc/systemd/system/wellknown-server.service owner: root group: root mode: \"0644\" notify: Restart wellknown-server - name: Start the wellknown-server ansible.builtin.systemd: name: wellknown-server state: started enabled: true daemon_reload: true - name: Use certbot to fetch TLS certificate for {{ domain_name }} ansible.builtin.command: cmd: >- certbot certonly --webroot -w {{ acme_home }} --non-interactive --agree-tos --email {{ domain_email }} --domains {{ domain_name }} args: creates: /etc/letsencrypt/live/{{ domain_name }}/fullchain.pem\n\nk8s/tasks/main.yml\n\n    \n    \n    --- - name: Cilium ansible.builtin.import_tasks: cilium.yml tags: - cilium - name: Certbot ansible.builtin.import_tasks: certbot.yml tags: - certbot\n\nplaybook.yml\n\n    \n    \n    --- - name: Bootstrap k8s node hosts: k3s-cluster gather_facts: false become: true vars: domain_email: admin@developer-friendly.blog roles: - k8s\n\nAfter adding another task to our Ansible role, we can run the new tasks with\nthe following command:\n\n    \n    \n    ansible-playbook playbook.yml --tags certbot\n\n## Step 5: Expose OIDC Configuration to the Internet\u00b6\n\nWe've prepared all these works so far for this next step.\n\nIn here, we will fetch the OIDC configuration from the Kubernetes API server\nand expose them to the internet on HTTPS using the newly acquired TLS\ncertificate with the help of static web server^12.\n\nk8s/defaults/main.yml\n\n    \n    \n    --- cilium_version: 1.15.4 acme_home: /var/www/html static_web_server_home: /var/www/static-web-server kubeconfig: /etc/rancher/k3s/k3s.yaml\n\nk8s/handlers/main.yml\n\n    \n    \n    --- - name: Restart wellknown-server ansible.builtin.systemd: name: wellknown-server state: restarted daemon_reload: true - name: Restart static-web-server-prepare ansible.builtin.systemd: name: static-web-server-prepare state: restarted daemon_reload: true - name: Restart static-web-server ansible.builtin.systemd: name: static-web-server state: restarted daemon_reload: true\n\nk8s/templates/static-web-server.service.j2\n\n    \n    \n    [Unit] Description=Static Web Server [Service] ExecStartPre=/usr/bin/test -s cert.pem ExecStartPre=/usr/bin/test -s key.pem ExecStartPre=/usr/bin/test -s .well-known/openid-configuration ExecStartPre=/usr/bin/test -s openid/v1/jwks ExecStart=/usr/local/bin/static-web-server \\ --host 0.0.0.0 \\ --port 443 \\ --root . \\ --log-level info \\ --http2 \\ --http2-tls-cert cert.pem \\ --http2-tls-key key.pem \\ --compression \\ --health \\ --experimental-metrics Restart=on-failure WorkingDirectory={{ static_web_server_home }} User=static-web-server Group=static-web-server RestartSec=5 AmbientCapabilities=cap_net_bind_service [Install] WantedBy=multi-user.target\n\nk8s/tasks/static-server.yml\n\n    \n    \n    - name: Create static-web-server group ansible.builtin.group: name: static-web-server state: present system: true - name: Create static-web-server user ansible.builtin.user: name: static-web-server state: present group: static-web-server shell: /bin/false system: true create_home: false - name: Create working dir for static-web-server user ansible.builtin.file: path: \"{{ static_web_server_home }}\" state: directory owner: static-web-server group: static-web-server mode: \"0755\" - name: Download static web server binary ansible.builtin.get_url: url: \"{{ static_web_server_download_url }}\" dest: \"/tmp/{{ static_web_server_download_url | basename }}\" checksum: \"sha256:{{ static_web_server_checksum }}\" owner: root group: root mode: \"0644\" register: download_static_web_server - name: Extract static web server binary ansible.builtin.unarchive: src: \"{{ download_static_web_server.dest }}\" dest: /usr/local/bin/ owner: root group: root mode: \"0755\" remote_src: true extra_opts: - --strip-components=1 - --wildcards - \"**/static-web-server\" notify: Restart static-web-server - name: Create static-web-server-prepare script ansible.builtin.template: src: static-web-server-prepare.sh.j2 dest: /usr/local/bin/static-web-server-prepare owner: root group: root mode: \"0755\" notify: Restart static-web-server-prepare - name: Create static-web-server-prepare service ansible.builtin.template: src: static-web-server-prepare.service.j2 dest: /etc/systemd/system/static-web-server-prepare.service owner: root group: root mode: \"0644\" notify: Restart static-web-server-prepare - name: Create static-web-server-prepare timer ansible.builtin.template: src: static-web-server-prepare.timer.j2 dest: /etc/systemd/system/static-web-server-prepare.timer owner: root group: root mode: \"0644\" notify: Restart static-web-server-prepare - name: Start static-web-server-prepare ansible.builtin.systemd: name: static-web-server-prepare.timer state: started enabled: true daemon_reload: true - name: Create static-web-server service ansible.builtin.template: src: static-web-server.service.j2 dest: /etc/systemd/system/static-web-server.service owner: root group: root mode: \"0644\" notify: Restart static-web-server - name: Start static-web-server service ansible.builtin.systemd: name: static-web-server state: started enabled: true daemon_reload: true\n\nk8s/tasks/main.yml\n\n    \n    \n    --- - name: Cilium ansible.builtin.import_tasks: cilium.yml tags: - cilium - name: Certbot ansible.builtin.import_tasks: certbot.yml tags: - certbot - name: Static web server ansible.builtin.import_tasks: static-server.yml tags: - static-web-server\n\nvars/aarch64.yml\n\n    \n    \n    --- static_web_server_download_url: https://github.com/static-web-server/static-web-server/releases/download/v2.28.0/static-web-server-v2.28.0-armv7-unknown-linux-musleabihf.tar.gz static_web_server_checksum: 492dda3749af5083e5387d47573b43278083ce62de09b2699902e1ba40bf1e45\n\nplaybook.yml\n\n    \n    \n    --- - name: Bootstrap k8s node hosts: k3s-cluster gather_facts: true become: true vars: domain_email: admin@developer-friendly.blog vars_files: - vars/{{ ansible_architecture }}.yml roles: - k8s tags: - provision\n\nRunning this will be as follows:\n\n    \n    \n    ansible-playbook playbook.yml --tags static-web-server\n\nYou can notice that we have turned on fact gathering in this step. This is due\nto our desire to include host-specific variables as you see with vars_files\nentry.\n\nFrom the above tasks, there are references to a couple of important files. One\nis the static-web-server-prepare which has both a service file as well as a\ntimer file.\n\nThis gives us flexibility to define oneshot services which will only run to\ncompletion on every tick of the timer. Effectively, we'll be able to separate\nthe executable task and the scheduling of the task.\n\nThe definitions for those files are as following:\n\nk8s/templates/static-web-server-prepare.sh.j2\n\n    \n    \n    #!/usr/bin/env sh # This script will run as root to prepare the files for the static web server set -eu mkdir -p {{ static_web_server_home }}/.well-known \\ {{ static_web_server_home }}/openid/v1 kubectl get --raw /.well-known/openid-configuration > \\ {{ static_web_server_home }}/.well-known/openid-configuration kubectl get --raw /openid/v1/jwks > \\ {{ static_web_server_home }}/openid/v1/jwks cp /etc/letsencrypt/live/{{ domain_name }}/fullchain.pem \\ {{ static_web_server_home }}/cert.pem cp /etc/letsencrypt/live/{{ domain_name }}/privkey.pem \\ {{ static_web_server_home }}/key.pem chown -R static-web-server:static-web-server {{ static_web_server_home }}\n\nNotice how we are manually fetching the OIDC configurations from the\nKubernetes as well as the TLS certificate. This is due to a possibility of\nrenewal for any of the given files:\n\n  1. Firstly, the Kubernetes API server might rotate its Service Account issuer key pair and with that, the JWKs URL will have different output.\n  2. Secondly, the TLS certificate will be renewed by certbot in the background and we have to keep up with that.\n\nNow, let's take a look at our preparation service and timer definition.\n\nk8s/templates/static-web-server-prepare.service.j2\n\n    \n    \n    [Unit] Description=Update TLS & K8s OIDC Config [Service] Environment=KUBECONFIG={{ kubeconfig }} ExecStartPre=/bin/mkdir -p .well-known openid/v1 ExecStart=/usr/local/bin/static-web-server-prepare Restart=on-failure Type=oneshot WorkingDirectory={{ static_web_server_home }} [Install] WantedBy=multi-user.target\n\nk8s/templates/static-web-server-prepare.timer.j2\n\n    \n    \n    [Unit] Description=Update TLS & K8s OIDC Config Every Minute [Timer] OnCalendar=*-*-* *:*:00 [Install] WantedBy=multi-user.target\n\nNotice that the service file specifies the working directory for the script.\nWhich means the static-web-server-prepare shell script will be executed in the\nspecified directory.\n\nAlso, watch out for oneshot systemd service type. These services are not long-\nrunning processes in an infitie loop. Instead, they will run to completion and\nthe systemd will not report their state as Active as it would with simple\nservices^13.\n\n## Step 6: Add the OIDC Provider to AWS\u00b6\n\nThat's it. We have done all the hard work. Anything after this will be a\nbreeze compared to what we've done so far as you shall see shortly.\n\nNow, we have a domain name that is publishing its OIDC configuration and JWKs\nover the HTTPS endpoint and is ready to be used as a trusted OIDC provider.\n\nAll we need right now, is a couple of TF resource in the AWS account and after\nthat, we can test the setup using a sample Job that takes a Service Account in\nits definition and uses its token to talk to AWS.\n\nNote that we're starting a new TF module below.\n\nconfigure-oidc/versions.tf\n\n    \n    \n    terraform { required_providers { tls = { source = \"hashicorp/tls\" version = \"~> 4.0\" } aws = { source = \"hashicorp/aws\" version = \"~> 5.46\" } } }\n\nconfigure-oidc/main.tf\n\n    \n    \n    data \"terraform_remote_state\" \"k8s\" { backend = \"local\" config = { path = \"../provision-k8s/terraform.tfstate\" } }\n\nconfigure-oidc/oidc_provider.tf\n\n    \n    \n    data \"tls_certificate\" \"this\" { url = \"https://${data.terraform_remote_state.k8s.outputs.oidc_provider_url}\" } resource \"aws_iam_openid_connect_provider\" \"this\" { url = \"https://${data.terraform_remote_state.k8s.outputs.oidc_provider_url}\" # JWT token audience (aud) client_id_list = [ \"sts.amazonaws.com\" ] thumbprint_list = [ data.tls_certificate.this.certificates[0].sha1_fingerprint ] }\n\nLet's apply this stack:\n\n    \n    \n    export AWS_PROFILE=\"PLACEHOLDER\" tofu plan -out tfplan tofu apply tfplan\n\nBelieve it or not, but after all these efforts, it is finally done.\n\nNow it is time for the test.\n\nIn order to be able to assume a role from inside a pod of our cluster, we will\ncreate a sample IAM Role with a trust relationship to the OIDC provider we\njust created.\n\nconfigure-oidc/iam_role.tf\n\n    \n    \n    data \"aws_iam_policy_document\" \"this\" { statement { actions = [ \"sts:AssumeRoleWithWebIdentity\" ] effect = \"Allow\" principals { type = \"Federated\" identifiers = [ aws_iam_openid_connect_provider.this.arn ] } condition { test = \"StringEquals\" variable = \"${aws_iam_openid_connect_provider.this.url}:aud\" values = [ \"sts.amazonaws.com\" ] } condition { test = \"StringEquals\" variable = \"${aws_iam_openid_connect_provider.this.url}:sub\" values = [ \"system:serviceaccount:${var.service_account_namespace}:${var.service_account_name}\" ] } } } resource \"aws_iam_role\" \"this\" { name = \"k3s-demo-app\" assume_role_policy = data.aws_iam_policy_document.this.json managed_policy_arns = [ \"arn:aws:iam::aws:policy/AmazonSSMReadOnlyAccess\" ] }\n\nconfigure-oidc/outputs.tf\n\n    \n    \n    output \"iam_role_arn\" { value = aws_iam_role.this.arn } output \"service_account_namespace\" { value = var.service_account_namespace } output \"service_account_name\" { value = var.service_account_name }\n\nThe AWS IAM Role trust relationship will look something like this:\n\nAWS IAM Role Trust Relationship\n\n    \n    \n    { \"Statement\": [ { \"Action\": \"sts:AssumeRoleWithWebIdentity\", \"Condition\": { \"StringEquals\": { \"4f0fce7c-9efa-9ee3-5fe0-467d95d2584c.developer-friendly.blog:aud\": \"sts.amazonaws.com\", \"4f0fce7c-9efa-9ee3-5fe0-467d95d2584c.developer-friendly.blog:sub\": \"system:serviceaccount:default:demo-service-account\" } }, \"Effect\": \"Allow\", \"Principal\": { \"Federated\": \"arn:aws:iam::XXXXXXXXXXXX:oidc-provider/4f0fce7c-9efa-9ee3-5fe0-467d95d2584c.developer-friendly.blog\" } } ], \"Version\": \"2012-10-17\" }\n\nThis, of course, shouldn't come as a surprise. We have already seen this in\nthe TF definition above.\n\n## Step 7: Test the Setup\u00b6\n\nWe have created the IAM Role with the trust relationship to the OIDC provider\nof the cluster. With the conditional in the AWS IAM Role you se in the\nprevious step, only the Service Accounts with the specified audience, in the\ndefault namespace and with the Service Account name demo-service-account will\nbe able to assume the role.\n\nThat said, let's use create another Ansible role to create a Kuberentes Job.\n\n    \n    \n    ansible-galaxy init app\n\nWe will need the Kubernetes core Ansible collection, so let's install that.\n\nrequirements.yml\n\n    \n    \n    - name: kubernetes.core version: 2.4.1\n    \n    \n    ansible-galaxy collection install -r requirements.yml\n\napp/defaults/main.yml\n\n    \n    \n    --- aws_region: eu-central-1 role_session_name: k3s-cluster\n\napp/templates/manifest.yml\n\n    \n    \n    apiVersion: v1 kind: ServiceAccount metadata: name: demo-service-account namespace: default --- apiVersion: batch/v1 kind: Job metadata: name: demo-app namespace: default spec: selector: matchLabels: job-name: demo-app template: metadata: labels: job-name: demo-app spec: restartPolicy: Never containers: - image: amazon/aws-cli:2.15.40 name: demo-app command: - sh - -c - | aws sts get-caller-identity aws ssm get-parameters-by-path \\ --path / --recursive \\ --with-decryption \\ --query \"Parameters[*].[Name]\" \\ --output text env: - name: AWS_REGION value: \"{{ aws_region }}\" - name: AWS_ROLE_ARN value: \"{{ role_arn }}\" - name: AWS_ROLE_SESSION_NAME value: \"{{ role_session_name }}\" - name: AWS_WEB_IDENTITY_TOKEN_FILE value: /var/run/secrets/tokens/token securityContext: readOnlyRootFilesystem: true volumeMounts: - name: token mountPath: /var/run/secrets/tokens readOnly: true - name: aws-config mountPath: /root/.aws serviceAccountName: demo-service-account volumes: - name: token projected: sources: - serviceAccountToken: path: token audience: sts.amazonaws.com - name: aws-config emptyDir: {}\n\napp/tasks/main.yml\n\n    \n    \n    --- - name: Apply the app job kubernetes.core.k8s: template: manifest.yml state: present force: true wait: true\n\nplaybook.yml\n\n    \n    \n    --- - name: Bootstrap k8s node hosts: k3s-cluster gather_facts: true become: true vars: domain_email: admin@developer-friendly.blog vars_files: - vars/{{ ansible_architecture }}.yml roles: - k8s tags: - provision - name: Test the AWS Access hosts: k3s-cluster gather_facts: false become: true environment: KUBECONFIG: /etc/rancher/k3s/k3s.yaml pre_tasks: - name: Install pip3 ansible.builtin.package: name: python3-pip state: present - name: Install kubernetes library ansible.builtin.pip: name: kubernetes<30 state: present - name: Read Tofu output from ./configure-oidc ansible.builtin.command: cmd: tofu output -raw iam_role_arn chdir: \"{{ playbook_dir }}/configure-oidc\" delegate_to: localhost become: false changed_when: false register: configure_oidc - name: Set the AWS role arn ansible.builtin.set_fact: role_arn: \"{{ configure_oidc.stdout }}\" roles: - app tags: - test - never\n\nA few important notes are worth mentioning here:\n\n  1. The second playbook is tagged with never. That is because there is a dependency on the second TF module. We have to manually resolve it before being able to run the second playbook. As soon as the dependency is resolved, we can run the second playbook with the --tags test flag.\n  2. There is a fact gathering in the pre_task of the second playbook. That is, again, because of the dependency to the TF module. We will grab the output of the TF module and pass it to our next role. If you notice there is a aws_region variable in the Jinja template that is being initialized by this fact gathering step.\n\n  3. In the fact gathering step, there is an Ansible delegation happening. This will ensure that the task is running in our own machine and not the target machine. The reason is that the TF module and its TF state file is in our local machine. We also do not need the become and as such it is turned off.\n  4. You will notice that the job manifest is using AWS CLI Docker image. By specifying some of the expected environment variables^14, we are able to use the AWS CLI without the requirement of manual aws configure.\n\nThis playbook can be run after the second TF module with the following\ncommand:\n\n    \n    \n    ansible-playbook playbook.yml --tags test\n\nWhen checking the logs of the deployed Kubernetes Job, we can see that it has\nbeen successful.\n\nLastly, to test if the Service Account and the IAM Role trust policy plays any\nrole in any of this, we can remove the serviceAccountToken and try to recreate\nthe job.\n\nThe output is as expected:\n\n    \n    \n    An error occurred (AccessDenied) when calling the AssumeRoleWithWebIdentity operation: Not authorized to perform sts:AssumeRoleWithWebIdentity\n\nThat's all folks! We can now wrap this up.\n\n## Bonus: JWKs URL\u00b6\n\nRemember at the beginning of this guide when we mentioned that the JWKs URL is\nconfigurable through the OIDC configuration endpoint?\n\nLet's see it in action.\n\n    \n    \n    DOMAIN=$(grep domain_name inventory/group_vars/all.yml | awk '{print $2}') curl https://$DOMAIN/.well-known/openid-configuration | jq -r .jwks_uri\n\nThis means that you can host your JWKs on a different server than the OIDC\nserver. Although I don't suggest this to be a good idea because of all the\nmaintenance overhead.\n\nThat said, if your JWKs URL is at a different server or hosted on a different\nendpoint, all you gotta do, is pass the value to the kube-apiserver as you see\nbelow:\n\n    \n    \n    kube-apiserver --service-account-jwks-uri=https://mydomain.com/some/different/endpoint\n\n## Conclusion\u00b6\n\nOpenID Connect is one of the most powerful protocols that powers the internet.\nYet, it is so underestimated and easily overlooked. If you look closely enough\non any system around you, you will see a lot of practical applications of\nOIDC.\n\nOne of the cues that you can look for when trying to identify applicability of\nOIDC is when trying to authenticate an identity of one system to another. You\nwill almost always never need to create another identity in the target system,\nnor do you need to pass any credentials around. All that's needed is to\nestablish a trust relationship between the two systems and you're good to go.\n\nThis gives you a lot of flexibility and enhances your security posture. You\nwill also remove the overhead of secret rotations from your workload.\n\nIn this post, we have seen how to establish a trust relationship between a\nbear-metal Kubernetes cluster and AWS IAM to grant cluster generated Service\nAccount tokens access to AWS services using OIDC.\n\nHaving this foundation in place, it's easy to extend this pattern to managed\nKubernetes clusters such as Azure Kubernetes Service (AKS)^2 or Google\nKubernetes Engine (GKE)^15. All you need from the managed Kubernetes cluster\nis the OIDC configuration endpoint, which in turn has the JWKs URL. With that,\nyou can create the trust relationship in AWS or any other Service Provider and\ngrant the relevant access to your services as needed.\n\nHope you've enjoyed reading the post as much as I've enjoyed writing it. I\nwish you have learned something new and useful from it.\n\nUntil next time, ciao & happy coding!\n\n  1. https://docs.k3s.io/ \u21a9\n\n  2. https://learn.microsoft.com/en-us/azure/aks/ \u21a9\u21a9\n\n  3. https://ngrok.com/ \u21a9\n\n  4. https://github.com/opentofu/opentofu/releases/tag/v1.6.2 \u21a9\n\n  5. https://github.com/ansible/ansible/releases/tag/v2.16.6 \u21a9\n\n  6. https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_oidc.html \u21a9\n\n  7. https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html \u21a9\n\n  8. https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/ \u21a9\n\n  9. https://openid.net/specs/openid-connect-core-1_0.html \u21a9\n\n  10. https://dash.cloudflare.com/profile/api-tokens \u21a9\n\n  11. https://docs.hetzner.com/cloud/api/getting-started/generating-api-token/ \u21a9\n\n  12. https://static-web-server.net/ \u21a9\n\n  13. https://www.freedesktop.org/software/systemd/man/latest/systemd.timer.html \u21a9\n\n  14. https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html \u21a9\n\n  15. https://cloud.google.com/kubernetes-engine/docs \u21a9\n\nApril 22, 2024 April 21, 2024 GitHub\n\nCopyright \u00a9 Meysam Azad\n\nMade with Material for MkDocs and hosted by GitHub Pages We respect your\nprivacy. To manage your cookie preferences, click here. Licensed under the\nApache-2.0 license\n\n", "frontpage": false}
