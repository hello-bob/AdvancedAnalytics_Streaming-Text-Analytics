{"aid": "40237417", "title": "ChatGPT can't replace doctors, AI fails to properly diagnose heart attack risk", "url": "https://studyfinds.org/chatgpt-heart-attack-risk/", "domain": "studyfinds.org", "votes": 1, "user": "Brajeshwar", "posted_at": "2024-05-02 15:33:48", "comments": 0, "source_title": "ChatGPT can\u2019t replace your doctor, AI fails to properly diagnose heart attack risk", "source_text": "ChatGPT can't replace your doctor, AI fails to properly diagnose heart attack\nrisk -\n\n  * Home\n  * Best Of The Best\n  * Food News\n  * Health\n  * Science\n  * Free Newsletter\n  * More...\n\n    * Animals\n    * Astronomy\n    * Brain Health\n    * Beauty\n    * Cancer Research\n    * Children\n    * Diet Studies\n    * Education\n    * Environment News\n    * Exercise\n    * Historical Research\n    * Longer Life News\n    * Money News\n    * Parenting Research\n    * Politics\n    * Society\n    * Sports\n    * Weight Loss Studies\n    * Weird\n\n\u00a9 2024 41 Pushups, LLC\n\nSkip to content\n\nStudy Finds\n\n(Image by D koi on Unsplash)\n\nHeart Health News, Science & Technology News\n\n# ChatGPT can\u2019t replace your doctor, AI fails to properly diagnose heart\nattack risk\n\nMay 1, 2024\n\nby StudyFinds Staff\n\nSPOKANE, Wash. \u2014 Doctors are still your best bet to treat cardiovascular\nissues over artificial intelligence. In a new study conducted at Washington\nState University, researchers evaluated ChatGPT-4\u2019s ability to assess the risk\nof a heart attack risk among simulated patients with chest pain. The\ngenerative AI system provided inconsistent conclusions and failed to match\nmethods used by doctors to assess a patient\u2019s cardiac risk. Simply put, AI may\nbe able to pass a medical exam, but it can\u2019t replace your cardiologist yet.\n\nChest pain is one of the most common reasons people end up in the emergency\nroom. Doctors often rely on risk assessment tools like the TIMI and HEART\nscores to help determine which patients are at high risk of a heart attack and\nneed immediate treatment and which can safely be sent home. These tools take\ninto account factors like the patient\u2019s age, medical history, EKG findings,\nand blood test results.\n\nIn this study, published in the journal PLoS ONE, researchers created three\nsets of simulated patient data: one based on the variables used in the TIMI\nscore, one based on the HEART score, and a third that included a whopping 44\ndifferent variables that might be relevant in a patient dealing with chest\npain. They then fed this data to ChatGPT-4 and asked it to calculate a risk\nscore for each \u201cpatient.\u201d\n\nThe good news? Overall, ChatGPT-4\u2019s risk assessments correlated very well with\nthe tried-and-true TIMI and HEART scores. This suggests that, with the right\ntraining, AI language models like ChatGPT have the potential to be valuable\ntools in helping doctors quickly and accurately assess a patient\u2019s risk.\n\nHowever, there was a worrying trend beneath the surface. When researchers fed\nChatGPT-4 the exact same patient data multiple times, it often spit out very\ndifferent risk scores. In fact, for patients with a fixed TIMI or HEART score,\nChatGPT-4 gave a different score nearly half the time. This inconsistency was\neven more pronounced in the more complex 44-variable model, where ChatGPT-4\ncame to a consensus on the most likely diagnosis only 56 percent of the time.\n\n\u201cChatGPT was not acting in a consistent manner,\u201d says lead study author Dr.\nThomas Heston, a researcher with Washington State University\u2019s Elson S. Floyd\nCollege of Medicine, in a media release. \u201cGiven the exact same data, ChatGPT\nwould give a score of low risk, then next time an intermediate risk, and\noccasionally, it would go as far as giving a high risk.\u201d\n\nScientists say AI may be able to pass a medical exam, but it can\u2019t replace\nyour cardiologist yet. (\u00a9 appledesign \u2013 stock.adobe.com)\n\nPart of the issue may lie in how language models like ChatGPT-4 are designed.\nTo mimic the variability and creativity of human language, they incorporate an\nelement of randomness. While this makes for more natural-sounding responses,\nit can clearly be a problem when consistency is key, as it is in medical\ndiagnoses and risk assessments.\n\nResearchers did find that ChatGPT-4 performed better for patients at the low\nand high ends of the risk spectrum. It was in the medium-risk patients where\nthe AI\u2019s assessments were all over the map. This is particularly concerning,\nas these are the patients for whom accurate risk stratification is most\nimportant in guiding clinical decision-making.\n\nAnother red flag was ChatGPT-4\u2019s occasional tendency to recommend\ninappropriate tests. For example, it sometimes suggested an endoscopy (a\nprocedure to examine the digestive tract) as the first test for a patient it\nthought might have acid reflux rather than starting with less invasive tests\nas a doctor would.\n\n\u201cWe found there was a lot of variation, and that variation in approach can be\ndangerous,\u201d explains Dr. Heston. \u201cIt can be a useful tool, but I think the\ntechnology is going a lot faster than our understanding of it, so it\u2019s\ncritically important that we do a lot of research, especially in these high-\nstakes clinical situations.\u201d\n\nResearchers suggest a few potential avenues on how to improve ChatGPT-4. One\nis to tweak the language model to reduce the level of randomness in its\nresponses when analyzing medical data. Another is to train specialized\nversions of ChatGPT-4 exclusively on carefully curated medical datasets rather\nthan the broad, unfiltered data it\u2019s currently learning from.\n\nDespite the current limitations, researchers remain optimistic about the\nfuture of AI in medicine. They propose that tools like ChatGPT-4, with further\nrefinement and in combination with established clinical guidelines, could one\nday help doctors make faster and more accurate assessments, ultimately leading\nto better patient care.\n\n\u201cChatGPT could be excellent at creating a differential diagnosis and that\u2019s\nprobably one of its greatest strengths,\u201d notes Dr. Heston. \u201cIf you don\u2019t quite\nknow what\u2019s going on with a patient, you could ask it to give the top five\ndiagnoses and the reasoning behind each one. So it could be good at helping\nyou think through a problem, but it\u2019s not good at giving the answer.\u201d\n\nOne thing is clear: we\u2019re not there yet. As impressive as ChatGPT-4 is, this\nstudy shows that it\u2019s not ready to be let loose on real patients. Rigorous\ntesting and refining of these AI models is crucial before they can be trusted\nwith the high stakes of medical decision-making. The health and safety of\npatients must always come first.\n\nStudyFinds\u2019 Matt Higgins contributed to this report.\n\nTags: AI, artificial intelligence, chatgpt, chest pains, heart attack, heart\ndisease\n\nAdd a Comment\n\nAbout the Author\n\n### StudyFinds Staff\n\nStudyFinds sets out to find new research that speaks to mass audiences \u2014\nwithout all the scientific jargon. The stories we publish are digestible,\nsummarized versions of research that are intended to inform the reader as well\nas stir civil, educated debate.\n\nView StudyFinds's article archive\n\nThe contents of this website do not constitute advice and are provided for\ninformational purposes only. See our full disclaimer\n\n## Latest News\n\n## Not cool: Being popular can derail teens\u2019 sleep habits\n\n## 5 Refreshing Mexican Beers To Crack Open On Cinco De Mayo\n\n3 comments\n\n## In stunning evolutionary first, orangutan discovered treating wound with\nmedicinal plants\n\n## Hold your nose, surface-level ozone may ruin summer and your health\n\nStudy Finds\n\n  * About: Our Story\n  * About: Editorial Team\n  * About: Our Mission & Standards\n  * Google Publisher Feature\n  * Who\u2019s Mentioned Us\n  * Best Of The Best\n  * Disclaimer\n  * Privacy Policy\n  * DO NOT SELL MY PERSONAL INFORMATION\n  * Contact\n\n\u00a9 2024 41 Pushups, LLC\n\n", "frontpage": false}
