{"aid": "40139833", "title": "MobileCLIP: Fast Image-Text Models Through Multi-Modal Reinforced Training", "url": "https://machinelearning.apple.com/research/mobileclip", "domain": "machinelearning.apple.com", "votes": 2, "user": "zerojames", "posted_at": "2024-04-24 02:33:31", "comments": 0, "source_title": "MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training", "source_text": "MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training -\nApple Machine Learning Research\n\nresearch areaComputer Vision, research areaMethods and Algorithms | conference CVPR\n\ncontent type paper | published April 2024\n\n# MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training\n\nAuthorsPavan Kumar Anasosalu Vasu*, Hadi Pour Ansari*, Fartash Faghri*,\nRaviteja Vemulapalli, Oncel Tuzel\n\nView publication\n\nView source code (GitHub)\n\n*Equal Contributors\n\nContrastive pretraining of image-text foundation models, such as CLIP,\ndemonstrated excellent zero-shot performance and improved robustness on a wide\nrange of downstream tasks. However, these models utilize large transformer-\nbased encoders with significant memory and latency overhead which pose\nchallenges for deployment on mobile devices. In this work, we introduce\nMobileCLIP -- a new family of efficient image-text models optimized for\nruntime performance along with a novel and efficient training approach, namely\nmulti-modal reinforced training. The proposed training approach leverages\nknowledge transfer from an image captioning model and an ensemble of strong\nCLIP encoders to improve the accuracy of efficient models. Our approach avoids\ntrain-time compute overhead by storing the additional knowledge in a\nreinforced dataset. MobileCLIP sets a new state-of-the-art latency-accuracy\ntradeoff for zero-shot classification and retrieval tasks on several datasets.\nOur MobileCLIP-S2 variant is 2.3 faster while more accurate compared to\nprevious best CLIP model based on ViT-B/16. We further demonstrate the\neffectiveness of our multi-modal reinforced training by training a CLIP model\nbased on ViT-B/16 image backbone and achieving +2.9% average performance\nimprovement on 38 evaluation benchmarks compared to the previous best.\nMoreover, we show that the proposed approach achieves 10-1000 improved\nlearning efficiency when compared with non-reinforced CLIP training.\n\n## Related readings and updates.\n\n### SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial\nUnderstanding\n\nThis paper was accepted at the UniReps Workshop at NeurIPS 2023. The landscape\nof publicly available vision foundation models (VFMs), such as CLIP and\nSegment Anything Model (SAM), is expanding rapidly. VFMs are endowed with\ndistinct capabilities stemming from their pre-training objectives. For\ninstance, CLIP excels in semantic understanding, while SAM specializes in\nspatial understanding for segmentation. In this work, we introduce a simple...\n\nSee paper details\n\n### Self Supervision Does Not Help Natural Language Supervision at Scale\n\nSelf supervision and natural language supervision have emerged as two exciting\nways to train general purpose image encoders which excel at a variety of\ndownstream tasks. Recent works such as M3AE [31] and SLIP [64] have suggested\nthat these approaches can be effectively combined, but most notably their\nresults use small (&#x3C;20M examples) pre-training datasets and don\u2019t\neffectively reflect the large-scale regime (>100M samples) that is commonly...\n\nSee paper details\n\n## Discover opportunities in Machine Learning.\n\nOur research in machine learning breaks new ground every day.\n\nWork with us\n\nPrivacy Policy Terms of Use Legal\n\nCopyright \u00a9 2024 Apple Inc. All rights reserved.\n\n", "frontpage": false}
