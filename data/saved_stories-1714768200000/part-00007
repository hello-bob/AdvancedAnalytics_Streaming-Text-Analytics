{"aid": "40245554", "title": "Solution: Monitoring Amazon EKS Infrastructure", "url": "https://docs.aws.amazon.com/grafana/latest/userguide/solution-eks.html", "domain": "amazon.com", "votes": 2, "user": "mhausenblas", "posted_at": "2024-05-03 08:53:25", "comments": 0, "source_title": "Solution: Monitoring Amazon EKS infrastructure - Amazon Managed Grafana", "source_text": "Solution: Monitoring Amazon EKS infrastructure - Amazon Managed Grafana\n\n## Select your cookie preferences\n\nWe use essential cookies and similar tools that are necessary to provide our\nsite and services. We use performance cookies to collect anonymous statistics\nso we can understand how customers use our site and make improvements.\nEssential cookies cannot be deactivated, but you can click \u201cCustomize cookies\u201d\nto decline performance cookies.\n\nIf you agree, AWS and approved third parties will also use cookies to provide\nuseful site features, remember your preferences, and display relevant content,\nincluding relevant advertising. To continue without accepting these cookies,\nclick \u201cContinue without accepting.\u201d To make more detailed choices or learn\nmore, click \u201cCustomize cookies.\u201d\n\n## Customize cookie preferences\n\nWe use cookies and similar tools (collectively, \"cookies\") for the following\npurposes.\n\n### Essential\n\nEssential cookies are necessary to provide our site and services and cannot be\ndeactivated. They are usually set in response to your actions on the site,\nsuch as setting your privacy preferences, signing in, or filling in forms.\n\n### Performance\n\nPerformance cookies provide anonymous statistics about how customers navigate\nour site so we can improve site experience and performance. Approved third\nparties may perform analytics on our behalf, but they cannot use the data for\ntheir own purposes.\n\nAllowed\n\n### Functional\n\nFunctional cookies help us provide useful site features, remember your\npreferences, and display relevant content. Approved third parties may set\nthese cookies to provide certain site features. If you do not allow these\ncookies, then some or all of these services may not function properly.\n\nAllowed\n\n### Advertising\n\nAdvertising cookies may be set through our site by us or our advertising\npartners and help us deliver relevant marketing content. If you do not allow\nthese cookies, you will experience less relevant advertising.\n\nAllowed\n\nBlocking some types of cookies may impact your experience of our sites. You\nmay review and change your choices at any time by clicking Cookie preferences\nin the footer of this site. We and selected third-parties use cookies or\nsimilar technologies as specified in the AWS Cookie Notice.\n\n## Unable to save cookie preferences\n\nWe will only store essential cookies at this time, because we were unable to\nsave your cookie preferences.\n\nIf you want to change your cookie preferences, try again later using the link\nin the AWS console footer, or contact support if the problem persists.\n\nContact Us\n\nCreate an AWS Account\n\nFeedback\n\nPreferences\n\nSolution: Monitoring Amazon EKS infrastructure - Amazon Managed Grafana\n\nAWSDocumentationAmazon Managed GrafanaUser Guide\n\nAbout this solutionPrerequisitesUsing this solutionList of metrics\ntrackedTroubleshooting\n\n# Solution: Monitoring Amazon EKS infrastructure\n\nPDFRSS\n\nMonitoring Amazon Elastic Kubernetes Service infrastructure is one of the most\ncommon scenarios for which Amazon Managed Grafana and Amazon Managed Service\nfor Prometheus are used. This page describes a pre-built AWS CloudFormation\nsolution that provides you with a template solution for this scenario. This\nsolution configures an Amazon Managed Service for Prometheus backend to\ncollect and store metrics from your Amazon EKS cluster, and an Amazon Managed\nGrafana front end with dashboards to view details of your cluster. Applying\nthis solution will create dashboards that:\n\n  * Assess the overall Amazon EKS cluster health.\n\n  * Show the health and performance of the Amazon EKS control plane.\n\n  * Show the health and performance of the Amazon EKS data plane.\n\n  * Display insights on Amazon EKS workloads across Kubernetes namespaces.\n\n  * Display resource usage across namespaces, including CPU, memory, disk, and network usage.\n\n##\n\nAbout this solution\n\nThis solution configures an Amazon Managed Grafana workspace to provide\nmetrics for your Amazon EKS cluster.\n\nThe metrics help you to operate Amazon EKS clusters more effectively by\nproviding insights into the health and performance of the Kubernetes control\nand data plane. You can understand your Amazon EKS cluster from the node\nlevel, to pods, down to the Kubernetes level, including detailed monitoring of\nresource usage.\n\nThe solution provides both anticipatory and corrective capabilities:\n\n  * Anticipatory capabilities include:\n\n    * Manage resource efficiency by driving scheduling decisions. For example, to provide performance and reliability SLAs to your internal users of the Amazon EKS cluster you can allocate enough CPU and memory resources to their workloads based on tracking historical usage.\n\n    * Usage forecasts: Based on the current utilization of your Amazon EKS cluster resources such as nodes, Persistent Volumes backed by Amazon EBS, or Application Load Balancers you can plan ahead, for example, for a new product or project with similar demands.\n\n    * Detect potential issues early: For example, by analyzing resource consumption trends on a Kubernetes namespace level, you can understand the seasonality of the workload\u2019s usage.\n\n  * Corrective capabilities include:\n\n    * Decrease the mean time to detection (MTTD) of issues on the infrastructure and the Kubernetes workload level. For example, by looking at the troubleshooting dashboard, you can quickly test hypotheses about what went wrong and eliminate them.\n\n    * Determine where in the stack a problem is happening. For example, the Amazon EKS control plane is fully managed by AWS and certain operations such as updating a Kubernetes deployment may fail if the API server is overloaded or connectivity is impacted.\n\nThe following image shows a sample of the dashboard folder for the solution.\n\nYou can choose a dashboard to see more details, for example, choosing to view\nthe Compute Resources for workloads will show a dashboard, such as that shown\nin the following image.\n\nThe metrics are scraped with a 1 minute scrape interval. The dashboard show\nmetrics aggregated to 1 minute, 5 minutes, or more, based on the specific\nmetric.\n\nFor a list of metrics tracked by this solution, see List of metrics tracked.\n\nCosts\n\nThis solution creates and uses resources in your workspace. You will be\ncharged for standard usage, including:\n\n  * Amazon Managed Grafana workspace access by users.\n\n  * Amazon Managed Service for Prometheus metric ingestion and storage, including use of the Amazon Managed Service for Prometheus agentless collector. The number of metrics used by this solution depends on the Amazon EKS cluster configuration and usage.\n\nYou can view the ingestion and storage metrics in Amazon Managed Service for\nPrometheus using CloudWatch For more information, see CloudWatch metrics in\nthe Amazon Managed Service for Prometheus User Guide.\n\nYou can estimate the cost using the Amazon Managed Service for Prometheus\npricing calculator. For more details, see the following procedure.\n\nThe pricing calculator can help you figure out approximate costs for your\nmetrics ingested into Amazon Managed Service for Prometheus. This is mostly\ndependent on the number of nodes in your Amazon EKS cluster, which impacts the\nnumber of metrics ingested.\n\n###### To use the pricing calculator to estimate your metric ingestion costs\n\n  1. Open the Amazon Managed Service for Prometheus pricing calculator.\n\n  2. Choose Calculate the cost using your existing self-hosted Prometheus.\n\n  3. In the Metrics sample ingestion section, make the following selections:\n\n     * Select Active Series for the metric\n\n     * For Average active time series, enter a number that is 8000 + the number of nodes in your cluster * 15,000. The cluster itself provides about 8000 time series, and each node produces approximately 15,000 more. For example, if you have 2 nodes, you would enter 38,000, which is 8000 + ( 2 * 15,000 ).\n\n     * For Avg Collection Interval (in seconds), enter 60.\n\n  4. In the Managed Collectorsection, enter the following information:\n\n     * For Number of collectors, enter 1. This solution creates a single collector for you.\n\n     * For Number of samples collected, enter a number that is 150 + the number of nodes in your cluster * 250. This is similar to number of time series you entered previously. For example, if you have 2 nodes, you would enter 650, which is 150 + ( 2 * 250 ).\n\n  5. You can see your estimated costs for metric ingestion at the bottom of the pricing calculator. You can save your estimate and add other costs using other features of the pricing calculator.\n\n##\n\nPrerequisites\n\nThis solution requires that you have done the following before using the\nsolution.\n\n  1. You must create an Amazon Elastic Kubernetes Service cluster that you wish to monitor, and the cluster must have at least one node. The cluster must have API server endpoint access set to include private access (it can also allow public access).\n\nThe authentication mode must include API access (it can be set to either API\nor API_AND_CONFIG_MAP). This allows you to use access enteries, in the next\nstep.\n\nSave the following information about your cluster to specify later:\n\n     * Cluster name \u2013 from the cluster details in the Amazon EKS console.\n\n     * VPC ID \u2013 from the Networking tab of the cluster details. The ID will have the form vpc-123example456.\n\n     * Cluster security group ID \u2013 from the Networking tab of the cluster details. The ID will have the form sg-123example456\n\n     * List of cluster subnet IDs \u2013 from the Networking tab of the cluster details, copy at least 2 of the subnet IDs in at least 2 availability zones. The IDs will have the form subnet-123example456.\n\n###### Note\n\nFor details about how to create an Amazon EKS cluster, see Getting started\nwith Amazon EKS.\n\n  2. You must create a role with access to the cluster. One way to do that is with access entries:\n\n    1. Create an access entry for any IAM role, and give the EKSClusterAdmin access, using the following two commands. You will need to replace $CLUSTER_NAME, and $ROLE_ARN with the correct name and ARN for your system.\n        \n                aws eks create-access-entry --cluster-name $CLUSTER_NAME --principal-arn $ROLE_ARN --type STANDARD --username $ROLE_ARN aws eks associate-access-policy --cluster-name $CLUSTER_NAME --principal-arn $ROLE_ARN --access-scope type=cluster --policy-arn arn:aws:eks::aws:cluster-access-policy/AmazonEKSClusterAdminPolicy\n\n    2. Allow root access in the trust policy for the above used role with the following policy. Replace $ACCOUNT_ID with your AWS account id.\n        \n                { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"Statement1\", \"Effect\": \"Allow\", \"Principal\": { \"AWS\": \"arn:aws:iam::$ACCOUNT_ID:root\" }, \"Action\": \"sts:AssumeRole\" } ] }\n\nSave the ARN of the role to specify later.\n\n###### Note\n\nFor details about cluster endpoint access control, see Amazon EKS cluster\nendpoint access control.\n\n  3. You must create an OpenIDConnect provider for your Amazon EKS cluster.\n\n    1. Audience should be sts.amazonaws.com.\n\n    2. Provider URL should be the OIDC endpoint for the Amazon EKS cluster, which is available from the cluster details in the Amazon EKS console, in the Overview tab. For example, https://oidc.eks.<region>.amazonaws.com/id/123EXAMPLE78EFAB9012CDEF3456ABCD.\n\n###### Note\n\nFor more details about creating OIDC provider using AWS Identity and Access\nManagement, see\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_oidc.html\n\nSave the Cluster OIDC Endpoint to specify later.\n\n  4. You must create an Amazon Managed Service for Prometheus workspace in the same AWS account as your Amazon EKS cluster. For details, see Create a workspace in the Amazon Managed Service for Prometheus User Guide.\n\nSave the Amazon Managed Service for Prometheus workspace ID to specify later.\n\n  5. You must create an Amazon Managed Grafana workspace compatible with Grafana version 9, in the same AWS Region as your Amazon EKS cluster. For details about creating a new workspace, see Create a workspace.\n\nSave the Amazon Managed Grafana workspace URL to specify later. This can be\nfound in the Amazon Managed Grafana console. For example,\nhttps://g-123example.grafana-workspace.<region>.amazonaws.com/.\n\n  6. You must create an API Key with ADMIN access for calling Grafana HTTP APIs. For details, see API keys (API tokens).\n\nAfter creating the API key, you must make it available to the AWS CDK, by\nadding it to AWS Systems Manager with the following command. Replace\n$AMG_API_KEY with the API key that you created, and $AWS_REGION with the\nRegion that your solution will run in.\n\n    \n        aws ssm put-parameter --name \"/eks-infra-monitoring-accelerator/grafana-api-key\" \\ --type \"SecureString\" \\ --value $AMG_API_KEY \\ --region $AWS_REGION\n\n  7. The user or role used to run the solution must have permission to modify your Amazon Managed Grafana workspace and to create AWS managed collectors to pull data from Amazon EKS and send it to your Amazon Managed Service for Prometheus workspace.\n\n  8. You must create an Amazon S3 bucket to be used to store the CloudFormation template and assets for the solution.\n\nSave the Amazon S3 bucket name and region for use later.\n\n###### Note\n\nFor more information about creating Amazon S3 buckets, see Create a bucket in\nthe Amazon Simple Storage Service User Guide.\n\n###### Note\n\nWhile not strictly required to set up the solution, you must set up user\nauthentication in your Amazon Managed Grafana workspace before users can\naccess the dashboards created. For more information, see User authentication\nin Amazon Managed Grafana.\n\n##\n\nUsing this solution\n\nThis solution is provided to you as an AWS CloudFormation template and related\nassets. You will provide information about the resources you want to use, and\nthe solution will put them all together to create dashboards for you. The\nfollowing procedure describes how to use this solution.\n\n###### To use this solution to monitor an Amazon EKS cluster\n\n  1. Makes sure that you have completed all of the prerequisites steps.\n\n  2. Download all files for the solution from https://github.com/aws-observability/observability-best-practices/tree/main/solutions/oss/eks-infra/v1.0.0/iac. This includes five zip files, and two json AWS CloudFormation templates.\n\nYou do not need to modify these files.\n\n  3. Upload the files to the Amazon S3 bucket that you created previously.\n\nSave the path to the eks-monitoring-cfn-template.json file for a later step.\n\n  4. Open the AWS CloudFormation console at https://console.aws.amazon.com/cloudformation.\n\n  5. From the navigation bar, select the AWS Region with your Amazon EKS cluster.\n\n  6. Choose Create stack, With new resources (standard).\n\n  7. Under Prerequisite - Prepare template, make sure that Choose an existing template is selected and then under Specify template, select Amazon S3 URL.\n\n  8. Enter the S3 URL for the eks-monitoring-cfn-template.json file in your bucket into the text area under Amazon S3 URL and choose Next.\n\n  9. On the Specify stack details page, enter the following parameters, and then choose Next.\n\n     * AMGWorkspaceEndpoint: The URL of your Amazon Managed Grafana workspace.\n\n     * AMPWorkspaceID: The ID of your Amazon Managed Service for Prometheus workspace.\n\n     * EKSClusterName: The name of your Amazon EKS cluster.\n\n     * EKSClusterAdminRoleARN: The Role ARN that you created previously\n\n     * EKSClusterOIDCEndpoint: The OIDC endpoint of your cluster.\n\n     * EKSClusterVpcId: The ID of the VPC for your cluster.\n\n     * EKSClusterSecurityGroupId: The ID of the security group for your cluster.\n\n     * EKSClusterSubnetIds: A comma separate list of two IDs for subnets where your cluster is located.\n\n     * S3BucketName: The name of the bucket where you stored the AWS CloudFormation assets for the solution.\n\n     * S3BucketRegion: The AWS Region where the bucket is located.\n\n  10. (Optional) On the Configure stack options page, add tags for your stack resources and then choose Next.\n\n  11. On the Review page, choose Create stack.\n\nCreating the stack can take around 20 minutes to complete.\n\n  12. After the stack creation is complete, you must configure the Amazon EKS cluster to allow access from the newly created scraper.\n\nFrom the Amazon EKS console, cluster details page, on the Observability tab,\nfind the Scraper ID for the newly created scraper. It will be in the form\ns-1example2-3456-abcd-7890-123example456. The scraper will have an alias that\nstarts with poseidon-scraper-existing-eks-oso-pattern.\n\nUse this ID, and follow the instructions in Configuring your Amazon EKS\ncluster in the Amazon Managed Service for Prometheus User Guide.\n\n  13. [Optional] After the stack creation is complete, you may remove the template files and assets from your Amazon S3 bucket. You may also use the same template to create more instances of the stack for other Amazon EKS clusters in the same region, as long as you complete the other prerequisites for each (including separate Amazon Managed Grafana and Amazon Managed Service for Prometheus workspaces).\n\nWhen the stack creation is completed, your Amazon Managed Grafana workspace\nwill be populated with a dashboard showing metrics for your Amazon EKS\ncluster. It will take a few minutes for metrics to be shown, as the scraper\nbegins to collect metrics.\n\n##\n\nList of metrics tracked\n\nThis solution creates a scraper that collects metrics from your Amazon EKS\ncluster. Those metrics are stored in Amazon Managed Service for Prometheus,\nand then displayed in Amazon Managed Grafana dashboards. The following metrics\nare tracked with this solution.\n\nMetric| Description / Purpose  \n---|---  \naggregator_unavailable_apiservice| Gauge of APIServices which are marked as\nunavailable broken down by APIService name.  \napiserver_admission_webhook_admission_duration_seconds_bucket| Admission\nwebhook latency histogram in seconds, identified by name and broken out for\neach operation and API resource and type (validate or admit).  \napiserver_current_inflight_requests| Maximal number of currently used inflight\nrequest limit of this apiserver per request kind in last second.  \napiserver_envelope_encryption_dek_cache_fill_percent| Percent of the cache\nslots currently occupied by cached DEKs.  \napiserver_flowcontrol_current_executing_requests| Number of requests in\ninitial (for a WATCH) or any (for a non-WATCH) execution stage in the API\nPriority and Fairness subsystem.  \napiserver_flowcontrol_rejected_requests_total| Number of requests in initial\n(for a WATCH) or any (for a non-WATCH) execution stage in the API Priority and\nFairness subsystem that were rejected.  \napiserver_flowcontrol_request_concurrency_limit| Nominal number of execution\nseats configured for each priority level.  \napiserver_flowcontrol_request_execution_seconds_bucket| The bucketed histogram\nof duration of initial stage (for a WATCH) or any (for a non-WATCH) stage of\nrequest execution in the API Priority and Fairness subsystem.  \napiserver_flowcontrol_request_queue_length_after_enqueue_count| The count of\ninitial stage (for a WATCH) or any (for a non-WATCH) stage of request\nexecution in the API Priority and Fairness subsystem.  \napiserver_request| Indicates an API server request.  \napiserver_requested_deprecated_apis| Gauge of deprecated APIs that have been\nrequested, broken out by API group, version, resource, subresource, and\nremoved_release.  \napiserver_request_duration_seconds| Response latency distribution in seconds\nfor each verb, dry run value, group, version, resource, subresource, scope and\ncomponent.  \napiserver_request_duration_seconds_bucket| The bucketed histogram of response\nlatency distribution in seconds for each verb, dry run value, group, version,\nresource, subresource, scope and component.  \napiserver_request_slo_duration_seconds| The Service Level Objective (SLO)\nresponse latency distribution in seconds for each verb, dry run value, group,\nversion, resource, subresource, scope and component.  \napiserver_request_terminations_total| Number of requests which apiserver\nterminated in self-defense.  \napiserver_request_total| Counter of apiserver requests broken out for each\nverb, dry run value, group, version, resource, scope, component, and HTTP\nresponse code.  \ncontainer_cpu_usage_seconds_total| Cumulative cpu time consumed.  \ncontainer_fs_reads_bytes_total| Cumulative count of bytes read.  \ncontainer_fs_reads_total| Cumulative count of reads completed.  \ncontainer_fs_writes_bytes_total| Cumulative count of bytes written.  \ncontainer_fs_writes_total| Cumulative count of writes completed.  \ncontainer_memory_cache| Total page cache memory.  \ncontainer_memory_rss| Size of RSS.  \ncontainer_memory_swap| Container swap usage.  \ncontainer_memory_working_set_bytes| Current working set.  \ncontainer_network_receive_bytes_total| Cumulative count of bytes received.  \ncontainer_network_receive_packets_dropped_total| Cumulative count of packets\ndropped while receiving.  \ncontainer_network_receive_packets_total| Cumulative count of packets received.  \ncontainer_network_transmit_bytes_total| Cumulative count of bytes transmitted.  \ncontainer_network_transmit_packets_dropped_total| Cumulative count of packets\ndropped while transmitting.  \ncontainer_network_transmit_packets_total| Cumulative count of packets\ntransmitted.  \netcd_request_duration_seconds_bucket| The bucketed histogram of etcd request\nlatency in seconds for each operation and object type.  \ngo_goroutines| Number of goroutines that currently exist.  \ngo_threads| Number of OS threads created.  \nkubelet_cgroup_manager_duration_seconds_bucket| The bucketed histogram of\nduration in seconds for cgroup manager operations. Broken down by method.  \nkubelet_cgroup_manager_duration_seconds_count| Duration in seconds for cgroup\nmanager operations. Broken down by method.  \nkubelet_node_config_error| This metric is true (1) if the node is experiencing\na configuration-related error, false (0) otherwise.  \nkubelet_node_name| The node's name. The count is always 1.  \nkubelet_pleg_relist_duration_seconds_bucket| The bucketed histogram of\nduration in seconds for relisting pods in PLEG.  \nkubelet_pleg_relist_duration_seconds_count| The count of duration in seconds\nfor relisting pods in PLEG.  \nkubelet_pleg_relist_interval_seconds_bucket| The bucketed histogram of\ninterval in seconds between relisting in PLEG.  \nkubelet_pod_start_duration_seconds_count| The count of duration in seconds\nfrom kubelet seeing a pod for the first time to the pod starting to run.  \nkubelet_pod_worker_duration_seconds_bucket| The bucketed histogram of duration\nin seconds to sync a single pod. Broken down by operation type: create,\nupdate, or sync.  \nkubelet_pod_worker_duration_seconds_count| The count of duration in seconds to\nsync a single pod. Broken down by operation type: create, update, or sync.  \nkubelet_running_containers| Number of containers currently running.  \nkubelet_running_pods| Number of pods that have a running pod sandbox.  \nkubelet_runtime_operations_duration_seconds_bucket| The bucketed histogram of\nduration in seconds of runtime operations. Broken down by operation type.  \nkubelet_runtime_operations_errors_total| Cumulative number of runtime\noperation errors by operation type.  \nkubelet_runtime_operations_total| Cumulative number of runtime operations by\noperation type.  \nkube_node_status_allocatable| The amount of resources allocatable for pods\n(after reserving some for system daemons).  \nkube_node_status_capacity| The total amount of resources available for a node.  \nkube_pod_container_resource_limits (CPU)| The number of requested limit\nresource by a container.  \nkube_pod_container_resource_limits (Memory)| The number of requested limit\nresource by a container.  \nkube_pod_container_resource_requests (CPU)| The number of requested request\nresource by a container.  \nkube_pod_container_resource_requests (Memory)| The number of requested request\nresource by a container.  \nkube_pod_owner| Information about the Pod's owner.  \nkube_resourcequota| Resource quotas in Kubernetes enforce usage limits on\nresources such as CPU, memory, and storage within namespaces.  \nnode_cpu| The CPU usage metrics for a node, including usage per core and total\nusage.  \nnode_cpu_seconds_total| Seconds the CPUs spent in each mode.  \nnode_disk_io_time_seconds| The cumulative amount of time spent performing I/O\noperations on disk by a node.  \nnode_disk_io_time_seconds_total| The total amount of time spent performing I/O\noperations on disk by the node.  \nnode_disk_read_bytes_total| The total number of bytes read from disk by the\nnode.  \nnode_disk_written_bytes_total| The total number of bytes written to disk by\nthe node.  \nnode_filesystem_avail_bytes| The amount of available space in bytes on the\nfilesystem of a node in a Kubernetes cluster.  \nnode_filesystem_size_bytes| The total size of the filesystem on the node.  \nnode_load1| The 1-minute load average of a node's CPU usage.  \nnode_load15| The 15-minute load average of a node's CPU usage.  \nnode_load5| The 5-minute load average of a node's CPU usage.  \nnode_memory_Buffers_bytes| The amount of memory used for buffer caching by the\nnode's operating system.  \nnode_memory_Cached_bytes,| The amount of memory used for disk caching by the\nnode's operating system.  \nnode_memory_MemAvailable_bytes| The amount of memory available for use by\napplications and caches.  \nnode_memory_MemFree_bytes| The amount of free memory available on the node.  \nnode_memory_MemTotal_bytes| The total amount of physical memory available on\nthe node.  \nnode_network_receive_bytes_total| The total number of bytes received over the\nnetwork by the node.  \nnode_network_transmit_bytes_total| The total number of bytes transmitted over\nthe network by the node.  \nprocess_cpu_seconds_total| Total user and system CPU time spent in seconds.  \nprocess_resident_memory_bytes| Resident memory size in bytes.  \nrest_client_requests_total| Number of HTTP requests, partitioned by status\ncode, method, and host.  \nrest_client_request_duration_seconds_bucket| The bucketed histogram of request\nlatency in seconds. Broken down by verb, and host.  \nstorage_operation_duration_seconds_bucket| The bucketed histogram of duration\nof storage operations.  \nstorage_operation_duration_seconds_count| The count of duration of storage\noperations.  \nstorage_operation_errors_total| Cumulative number of errors during storage\noperations.  \nup| A metric indicating whether the monitored target (e.g., node) is up and\nrunning.  \nvolume_manager_total_volumes| The total number of volumes managed by the\nvolume manager.  \nworkqueue_adds_total| Total number of adds handled by workqueue.  \nworkqueue_depth| Current depth of workqueue.  \nworkqueue_queue_duration_seconds_bucket| The bucketed histogram of how long in\nseconds an item stays in workqueue before being requested.  \nworkqueue_work_duration_seconds_bucket| The bucketed histogram of how long in\nseconds processing an item from workqueue takes.  \n  \n##\n\nTroubleshooting\n\nThere are a few things that can cause the setup of the solution to fail. Be\nsure to check the following.\n\n  * You must complete all Prerequisites before installing the solution.\n\n  * Your Amazon EKS cluster must have the CoreDNS and kube-proxy add-ons installed. If they are not installed, the solution will not work correctly.\n\n  * The cluster must have at least one node in it before attempting to create the solution or access the metrics.\n\nDocument Conventions\n\nUser API\n\nTagging\n\nDid this page help you? - Yes\n\nThanks for letting us know we're doing a good job!\n\nIf you've got a moment, please tell us what we did right so we can do more of\nit.\n\nDid this page help you? - No\n\nThanks for letting us know this page needs work. We're sorry we let you down.\n\nIf you've got a moment, please tell us how we can make the documentation\nbetter.\n\nProvide feedback\n\n#### Next topic:\n\nTagging\n\n#### Previous topic:\n\nUser API\n\n#### Need help?\n\n  * Connect with an AWS IQ expert\n\nPrivacySite termsCookie preferences\n\n\u00a9 2024, Amazon Web Services, Inc. or its affiliates. All rights reserved.\n\n## On this page\n\n  * About this solution\n  * Prerequisites\n  * Using this solution\n  * List of metrics tracked\n  * Troubleshooting\n\n", "frontpage": false}
