{"aid": "40082058", "title": "Stopping a badly behaved bot the wrong way", "url": "https://lemmy.ml/post/14612626", "domain": "lemmy.ml", "votes": 1, "user": "d416", "posted_at": "2024-04-19 00:08:20", "comments": 0, "source_title": "Stopping a badly behaved bot the wrong way. - Lemmy", "source_text": "Stopping a badly behaved bot the wrong way. - Lemmy\n\n@digdilem to LinuxEnglish \u2022\n\nedit-2\n\n10 hours ago\n\n# Stopping a badly behaved bot the wrong way.\n\nmessage-square\n\n17\n\n132\n\nmessage-square\n\n# Stopping a badly behaved bot the wrong way.\n\n@digdilem to LinuxEnglish \u2022\n\nedit-2\n\n10 hours ago\n\nmessage-square\n\n17\n\nI host a few small low-traffic websites for local interests. I do this for\nfree - and some of them are for a friend who died last year but didn\u2019t want\nall his work to vanish. They don\u2019t get so many views, so I was surprised when\nI happened to glance at munin and saw my bandwidth usage had gone up a lot.\n\nI spent a couple of hours working to solve this and did everything wrong. But\nit was a useful learning experience and I thought it might be worth sharing in\ncase anyone else encounters similar.\n\nMy setup is:\n\nCloudflare DNS -> Cloudflare Tunnel (Because my residential isp uses CGNAT) ->\nHaproxy (I like Haproxy and amongst other things, alerts me when a site is\ndown) -> Separate Docker containers for each website. On a Debian server\nliving in my garage.\n\nFrom Haproxy\u2019s stats page, I was able to see which website was gathering\nattention. It\u2019s one running PhpBB for a little forum. Tailing apache\u2019s logs in\nthat container quickly identified the pattern and made it easy to see what was\nhappening.\n\nIt was seeing a lot of 404 errors for URLs all coming from the same user-agent\n\u201cclaudebot\u201d. I know what you\u2019re thinking - it\u2019s an exploit scanning bot, but a\ncloser look showed it was trying to fetch normal forum posts, some which had\nbeen deleted months previously, and also robots.txt. That site doesn\u2019t have a\nrobots.txt so that was failing. What was weird is that the it was requesting\nat a rate of up to 20 urls a second, from multiple AWS IPs - and every other\nrequest was for robots.txt. You\u2019d think it would take the hint after a million\ntimes of asking.\n\nGoogling that UA turns up that other PhpBB users have encountered this quite\nrecently - it seems to be fascinated by web forums and absolutely hammers them\nwith the same behaviour I found.\n\nSo - clearly a broken and stupid bot, right? Rather than being specifically\nmalicious. I think so, but I host these sites on a rural consumer line and it\nwas affecting both system load and bandwidth.\n\nWhat I did wrong:\n\n  1. In docker, I tried quite a few things to block the user agent, the country (US based AWS, and this is a UK regional site), various IPs. It took me far too long to realise why my changes to .htaccess were failing - the phpbb docker image I use mounts the root directory to the website internally, ignoring my mounted vol. (My own fault, it was too long since I set it up to remember only certain sub-dirs were mounted in)\n\n  2. Figuring that out, I shelled into the container and edited that .htaccess, but wouldn\u2019t have survived restarting/rebuilding the container so wasn\u2019t a real solution.\n\nWhilst I was in there, I created a robots.txt file. Not surprisingly,\nclaudebot doesn\u2019t actually honour whats in there, and still continues to\nrequest it ten times a second.\n\n  3. Thinking there must be another way, I switched to Haproxy. This was much easier - the documentation is very good. And it actually worked - blocking by Useragent (and yep, I\u2019m lucky this wasn\u2019t changing) worked perfectly.\n\nI then had to leave for a while and the graphs show it\u2019s working. (Yellow\nabove the line is requests coming into haproxy, below the line are responses).\n\nGreat - except I\u2019m still seeing half of the traffic, and that\u2019s affecting my\nlatency. (Some of you might doubt this, and I can tell you that you\u2019re spoiled\nby an excess of bandwidth...)\n\n  4. That\u2019s when the penny dropped and the obvious occured. I use cloudflare, so use their firewall, right? No excuses - I should have gone there first. In fact, I did, but I got distracted by the many options and focused on their bot fighting tools, which didn\u2019t work for me. (This bot is somehow getting through the captcha challenge even when bot fight mode is enabled)\n\nBut, their firewall has an option for user agent. The actual fix was simply to\nadd this in WAF for that domain.\n\nAnd voila - no more traffic through the tunnel for this very rude and stupid\nbot.\n\nAfter 24 hours, Cloudflare has blocked almost a quarter of a million requests\nby claudebot to my little phpbb forum which barely gets a single post every\nthree months.\n\nMoral for myself: Stand back and think for a minute before rushing in and\ntrying to fix something in the wrong way. I\u2019ve also taken this as an\nopportunity to improve haproxy\u2019s rate limiting internally. Like most website\nhosts, most of my traffic is outbound, and slowing things down when it gets\nbusy really does help.\n\nThis obviously isn\u2019t a perfect solution - all claudebot has to do is change\nits UA, and by coming from AWS it\u2019s pretty hard to block otherwise. One hopes\nit isn\u2019t truly malicious. It would be quite a lot more work to integrate\nFail2ban for more bots, but it might yet come to that.\n\nAlso, if you write any kind of web bot, please consider that not everyone who\nhosts a website has a lot of bandwidth, and at least have enough pride to\nwrite software good enough to not keep doing the same thing every second. And,\ny\u2019know, keep an eye on what your stuff is doing out on the internet - not\nleast for your own benefit. Hopefully AWS really shaft claudebot\u2019s owners with\nsome big bandwidth charges...\n\nalert-triangle\n\nYou must log in or register to comment.\n\n  * @boredsquirrel@slrpnk.net\n\nlink\n\nfedilink\n\n1\u20222 hours ago\n\nWow ClaudeAI, goos job!\n\n  * Daniel Quinn\n\nlink\n\nfedilink\n\nEnglish\n\n12\u2022\n\nedit-2\n\n4 hours ago\n\nNot throwing any shade, just some advice for the future: try to always\nconsider the problem in the context of the OSI model. Specifically, \u201cLayer 3\u201d\n(network) is always a better strategy for routing/blocking than \u201cLayer 5\u201d\n(application) if you can do it.\n\nBlocking traffic at the application layer means that the traffic has to be\nrouted through (bandwidth consumption) assembled and processed (CPU cost)\nbefore a decision can be made. You should always try to limit the stuff that\nmakes it to layer 5 if you\u2019re sure you won\u2019t want it.\n\nThe trouble with layer 3 routing of course is that you don\u2019t have application\ndata there. No host name, no HTTP headers, etc., just packets with a few bits\nof information:\n\n    * source IP and port\n    * destination IP and port\n    * A few other firewall-specific bits of information like whether this packet is part of an established connection (syn) etc.\n\nIn your case though, you already knew what you didn\u2019t want: traffic from a\nparticular IP, and you have that at the network layer.\n\nAt that point, you know you can block at layer 3, so the next question is how\nfar up the chain can you block it?\n\nMost self-hosters will just have their machines on the open internet, so their\npersonal firewall is all they\u2019ve got to work with. It\u2019s still better than\nletting the packets all the way through to your application, but you still\nhave to suffer the cost of dropping each packet. Still, it\u2019s good enoughTM for\nmost.\n\nIn your case though, you had setup the added benefit of Cloudflare standing\nbetween you and your server, so you could move that decision making step even\nfurther away from you, which is pretty great.\n\n    * @xthexder@l.sw0.com\n\nlink\n\nfedilink\n\n4\u20224 hours ago\n\nI learned this in highschool when I discovered sending ping floods from a\n1gbit VPS to a slow residential Internet connection can take down your\nInternet even if the router doesn\u2019t respond to pings. The bandwidth still all\nneeds to make it to the router in your house to be dropped.\n\n      * Possibly linux\n\nlink\n\nfedilink\n\nEnglish\n\n1\u20223 hours ago\n\nNow that\u2019s interesting. I know that i2p can crash some cheap routers because\nthey run out of ram. I wonder if you could do that from the outside.\n\n  * Skull giver\n\nlink\n\nfedilink\n\n25\u20228 hours ago\n\nThis seems like the right way to me. You\u2019re using the WAF the way it was\ndesigned.\n\nThe wrong way would probably be to tarpit/slowloris the bot, or to send a gzip\nbomb (or zstd or brotli depending on what compression the bot advertises in\nits requests). Or maybe redirect it into some weird IP space, like the\nreserved IP space of your favourite department of defence.\n\nYou could also mess with the AI training data and send altered data back.\nPretend you have a billion forum posts, with every post containing a different\nversion of a few topics, with random words replaced by \u201ccow\u201d or maybe some\nspecial key word of your choice. Or maybe send back a list of \u201cforum posts\u201d\ncontaining popular prompts to confuse the AI into dealing with prompts badly.\n\n    * @digdilemOP\n\nlink\n\n6\u20226 hours ago\n\nSome nice evil ideas there!\n\n    * Deebster\n\nlink\n\nfedilink\n\n9\u20227 hours ago\n\nI was kinda hoping for another story about some clever compression bomb or\nsimilar to slow up the bot - after all, if it\u2019s hammering this little site\nit\u2019s surely doing the same to others, even if they haven\u2019t noticed yet. After\nthe robots.txt was ignored I was sure, but I guess this mature, restrained\nresponse is probably the correct one *discontentedly kicks can down sepia\nstreet*\n\n  * Deebster\n\nlink\n\nfedilink\n\n8\u20227 hours ago\n\n> Thinking there must be another way, I switched to Haproxy.\n\nHang on, weren\u2019t you on Haproxy already? Or do you mean you switched your\nattention to Haproxy? (If not, what were you in before?)\n\nAs others have said, blocking incoming stuff as high up as possible is\ndefinitely the right way, and Cloudflare is the right place for you. It\u2019s\ninteresting that this bot wasn\u2019t caught by Cloudflare, I wonder who runs it.\n\n    * @digdilemOP\n\nlink\n\n3\u20227 hours ago\n\nI mean - I switched my attention to Haproxy. And yes, no argument there.\n\n  * @ArcticDagger@feddit.dk\n\nlink\n\nfedilink\n\n17\u20229 hours ago\n\nCould it be this fella who\u2019s hitting you up: https://claude.ai/login\n\n    * Deebster\n\nlink\n\nfedilink\n\n7\u20227 hours ago\n\nI feel a company that big would write a more competent bot, but I also\nwouldn\u2019t be too astonished.\n\n    * @digdilemOP\n\nlink\n\n3\u20226 hours ago\n\nMaybe? It feels like the kind of stupid that you really need a human to half-\nass it to achieve this thoroughly though.\n\n  * @atzanteol@sh.itjust.works\n\nlink\n\nfedilink\n\nEnglish\n\n6\u20227 hours ago\n\nI love a good post mortem. Thanks for sharing!\n\n  * @just_another_person@lemmy.world\n\nlink\n\nfedilink\n\n11\u2022\n\nedit-2\n\n8 hours ago\n\nGood tips for beginners who don\u2019t stare at this stuff all day.\n\nOne extra tip for you: just script blocking these things after they act up,\nbut before they cost real money. You know your expected traffic patterns, so\nsetting thresholds should be easy.\n\nFail2ban is tried and true, and dead simple, or you could use something a bit\nfancier like crowdsec to setup chains that send IPs to block directly at the\nWAF. Getting some of the more popular blacklists at the edge would be a good\nidea as well.\n\n    * @digdilemOP\n\nlink\n\n5\u20226 hours ago\n\nFail2ban is something I\u2019ve used for years - in fact it was working on these\nvery sites before I decided to dockerise them, but find it a lot less simple\nin this application for a couple of reasons:\n\nThe logs are in the docker containers. Yes, I could get them squirting to a\ncentral logging serverbut that\u2019s a chunk of overhead for a home system. (I\u2019ve\ndone that before, so it is possible, just extra time)\n\nAnd getting the real IP through from cloudlfare. Yes, CF passes headers with\nit in, and haproxy can forward that as well with a bit of tweaking. But not\nevery docker container for serving webpages (notably the phpbb one) will\ncorrectly log the source IP even when passed through from Haproxy as the\nforwarded-ip, instead showing the IP of the proxy. I\u2019ve other containers that\ndo display it, and it can obviously be done, but I\u2019m not clear yet why it\u2019s\ninconsistent. Without that, there\u2019s no blocking.\n\nAnd... You can use the cloudflare IP to block IPs, but there\u2019s a fixed limit\non the free accounts. When I set this up before with native webservers and\nblocked malicious url scanning bots, then using the api to block them - I\nreached that limit within a couple of days. I don\u2019t think there\u2019s automatic\nexpiry, so I\u2019d need to find or build a tool that manages the blocklist\nremotely. (Or use haproxy to block and accept the overhead)\n\nIt\u2019s probably where I should go next.\n\nAnd yes - you\u2019re right about scripting. Automation is absolutely how I like to\ndo things. But so many problems only become clear retrospectively.\n\n      * @digdilemOP\n\nlink\n\n5\u20226 hours ago\n\nDoh - another example of my muddled thinking.\n\nFail2ban will work directly on haproxy\u2019s log, no need to read the web logs\nfrom containers at all. Much simpler and better.\n\n  * lettruthout\n\nlink\n\nfedilink\n\nEnglish\n\n9\u20229 hours ago\n\nThanks for writing this up, it\u2019s very interesting!\n\n##\n\nLinux\n\n!linux@lemmy.ml\n\n### Subscribe from Remote Instance\n\nCreate a post\n\nYou are not logged in. However you can subscribe from another Fediverse\naccount, for example Lemmy or Mastodon. To do this, paste the following into\nthe search field of your instance: !linux@lemmy.ml\n\nFrom Wikipedia, the free encyclopedia\n\nLinux is a family of open source Unix-like operating systems based on the\nLinux kernel, an operating system kernel first released on September 17, 1991\nby Linus Torvalds. Linux is typically packaged in a Linux distribution (or\ndistro for short).\n\nDistributions include the Linux kernel and supporting system software and\nlibraries, many of which are provided by the GNU Project. Many Linux\ndistributions use the word \u201cLinux\u201d in their name, but the Free Software\nFoundation uses the name GNU/Linux to emphasize the importance of GNU\nsoftware, causing some controversy.\n\n### Rules\n\n  * Posts must be relevant to operating systems running the Linux kernel. GNU/Linux or otherwise.\n  * No misinformation\n  * No NSFW content\n  * No hate speech, bigotry, etc\n\n### Related Communities\n\n  * !opensource@lemmy.ml\n  * !libre_culture@lemmy.ml\n  * !technology@lemmy.ml\n  * !libre_hardware@lemmy.ml\n\nCommunity icon by Alp\u00e1r-Etele M\u00e9der, licensed under CC BY 3.0\n\n  * 1.63K users / day\n  * 5.41K users / week\n  * 10.8K users / month\n  * 27.4K users / 6 months\n  * 44K subscribers\n  * 6.24K Posts\n  * 128K Comments\n  * Modlog\n\n  * mods:\n  * @AgreeableLandscape\n  * @nooter692\n  * @MarcellusDrum\n  * Arthur Besse\n  * Cyclohexane\n  * @d3Xt3r@lemmy.nz\n\n  * BE: 0.19.3\n  * Modlog\n  * Instances\n  * Docs\n  * Code\n  * join-lemmy.org\n\n", "frontpage": false}
