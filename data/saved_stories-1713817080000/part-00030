{"aid": "40114130", "title": "LLM-powered automated software testing", "url": "https://www.sabrina.dev/p/ai-eats-software-testing", "domain": "sabrina.dev", "votes": 2, "user": "sabrina_ramonov", "posted_at": "2024-04-22 13:26:35", "comments": 2, "source_title": "AI Eats Software Testing", "source_text": "AI Eats Software Testing\n\n  * Sabrina Ramonov\n  * Posts\n  * AI Eats Software Testing\n\n# AI Eats Software Testing\n\n## Automated Input Diversification (AID) is a Breakthrough in Software Testing\n\nSabrina Ramonov April 21, 2024\n\nIn the fast-paced world of software, squashing bugs is a never-ending battle.\nA recent paper introduces a groundbreaking LLM-powered method called AID\n(Automated Input Diversification), which significantly boosts our ability to\ndetect bugs. Massive implications for software engineering.\n\nLet\u2019s say you have a potentially buggy software program that passes all unit\ntests written by your engineering team \u2014 how do you find new test cases on\nwhich the program fails?\n\nAID does exactly that \u2014 given a software program, its specification, and a set\nof passing tests... AID produces a failing test that reveals a bug in the\nprogram.\n\nImagine AID-supercharged CI/CD pipelines, with LLMs proactively uncovering\nbugs and missed tests. This future is near.\n\n# What is AID?\n\nAID stands for Automated Input Diversification, a new approach combining LLMs\nwith differential testing to create robust test cases.\n\nDifferential testing works by comparing the outputs of different versions of a\nprogram when given the same input. If outputs are different, then there\u2019s a\ndefect in the program.\n\nAID leverages the computational power of LLMs to generate diverse program\nvariants, and then employs differential testing to compare these variants. By\nexamining discrepancies in outputs, AID identifies bugs with stunning\naccuracy.\n\n# How It Works\n\nAID has 3 steps:\n\n  1. Generate program variants\n\n  2. Generate test case generator\n\n  3. Differential testing\n\nOverview of AID\n\n### 1\\. Generate Program Variants\n\nProgram variants are generated by an LLM \u2014 here\u2019s the prompt:\n\n    \n    \n    INSTRCUTION: You are a professional coding competition participant, skilled at identifying bugs and logic flaws in code. You will receive a description of a coding problem, and a piece of code attempting to solve the problem. Your task is to find whether there is any bug or logic flaw in the code, if any, please repair the code. Please reply with ONLY the COMPLETE REPAIRED CODE (rather than code fragments) without any other content. PROBLEM DESCRIPTION: {The specification of the coding task} CODE: {Source code of PUT}\n\nA problem variant is an attempt to fix broken code. It\u2019s easy to check whether\nit\u2019s valid (e.g. runs) and passes existing tests - it should, otherwise\nsomething is wrong. The original program is part of the training set, and its\nsource is fed as part of the prompt.\n\n### 2\\. Generate Test Case Generator\n\nInstead of generating test samples directly, this paper asks LLMs to generate\ncode that generates samples. Pretty meta. It\u2019s an interesting modality shift \u2014\nLLM is not a generator anymore, a python program is. This is useful for\nsoftware developers because you can always inspect the output and tweak it.\nAlso, this allows you to improve it by continuously feeding the test generator\ncode into an LLM, asking it to make adjustments.\n\nHere\u2019s the prompt:\n\n    \n    \n    **INSTRCUTION**: The following is a description of a coding problem, please write an input generator for this problem (DO NOT generate outputs). The generated inputs should meet the input constraints of the problem description. Please reply with ONLY the code without any other content. You can use the python library {library name} if necessary, here are some examples of how to use the library, which may be helpful: {Few-shot examples to use the library} **PROBLEM DESCRIPTION**: {The specification of the coding task}\n\nTry these prompts on some functions from your favorite libraries on github\n(especially one-off utility functions), and let me know what it produces!\n\n### 3\\. Differential Testing\n\nThe idea behind differential testing is this:\n\nGiven N programs with the same intent (i.e. intent is to solve the problem\noutlined by the spec)... any discrepancy between the outputs indicates a\npotential bug. Hence, the name \u201cdifferential\u201d testing.\n\nAID\u2019s underlying assumption is that program variants will exhibit the same\nbehavior as the original program. Under this assumption, a test case is\nconsidered a test oracle (i.e. meaning it reveals a bug) if an output of a\nvariant is different from the output of the plausibly correct program.\n\nHere\u2019s the LLM prompt to generate diverse program variants:\n\n    \n    \n    INSTRCUTION: You are a professional coding competition participant, skilled at identifying bugs and logic flaws in code. You will receive a description of a coding problem, and a piece of code attempting to solve the problem. Your task is to find whether there is any bug or logic flaw in the code, if any, please repair the code. Please reply with ONLY the COMPLETE REPAIRED CODE (rather than code fragments) without any other content. PROBLEM DESCRIPTION: {The specification of the coding task} CODE: {Source code of PUT}\n\n# Does AID Outperform Existing Methods?\n\nThe paper evaluated AID against three baseline methods across two major\ndatasets: Trickbugs (C++) and Trickybugs (Python), and a third dataset called\nEvalPlus. The results were striking:\n\n  * AID consistently outperformed in terms of precision and recall.\n\n  * Its worst performance still surpassed the best performance of the next best method, DPP.\n\n  * Across various metrics, AID showed improvements ranging from 65.57% to 165.11% over competing methods.\n\nRecall, Precision, and F1 Scores\n\nAID's ability to detect tricky bugs can substantially improve the reliability\nand safety of software apps, impacting everything from personal devices to\ncritical systems in healthcare and finance.\n\nWhile AID marks a significant advance, noticeably its recall rate is lower.\n\nThis may be because AID prioritizes identifying and accurately pinpointing\ndefects in the code, leading to a higher precision in detecting true positives\n(i.e., correctly identifying defects). However, this focus on precision may\nresult in lower recall, as AID may miss defects or have a lower sensitivity to\nidentifying all possible defects in the code. Additionally, AID may have been\ndesigned to prioritize the reduction of false positives (i.e., incorrectly\nidentifying defects), which would contribute to higher precision. This focus\non reducing false positives may lead to a more conservative approach in\nidentifying defects, potentially resulting in lower recall.\n\n# Math Interpretation\n\nEvery correct program (from a format verification standpoint) is plausibly\ncorrect, as it passes tests.\n\nEvery program is trivially plausibly correct if the set of unit tests is empty\n(yes \u2014 you can write plausibly correct programs by not writing tests!)\n\nIf the input space is not finite, since the set of tests is finite, we cannot\ndetermine if a plausibly correct program is indeed correct using tools like\nAID.\n\nA program under test from a sample in the training data passes all the given\nunit tests \u2014 it can be treated as a lossy reconstruction of the correct\nprogram (or a set of all correct programs) with perhaps some measure of how\nclose that program is to the correct program by inducing some metric on the\ntest set.\n\nIn some other hand-wavy sense, the trickiest programs to debug are the one\nthat appear \u201cdense\u201d in the test set \u2014 they pass an infinite subset of the\ninput space test cases with one off problem points that are not obvious edge\ncases (e.g. a function that takes a single integer as an input only fails on\ninput 10).\n\nAs software developers, we don\u2019t formally prove that a program is correct \u2014\nthough we do try to execute something close to that process, informally, and\nperhaps even inductively, in our heads, to prove to ourselves that our\nalgorithm is correct.\n\nNow, imagine taking this to the next level: asks LLMs formally verify a\nprogram, ensuring it is provably correct. This could be part of the next-gen\nLLM-powered CI/CD pipeline!\n\n# Open Questions\n\nThese questions come to mind, as I consider what it will take to migrate AID\nfrom research to industry:\n\n  1. How well does AID perform in other programming languages besides C++ and Python?\n\n  2. What specific types of bugs is AID most effective at detecting?\n\n  3. Are there any real-world applications where AID can already be implemented? Are we limited to mostly competition style problems?\n\n  4. What would the integration of AID into existing testing frameworks look like?\n\n  5. What are the computational requirements to run AID effectively in a commercial setting?\n\n  6. How do developers handle the lower recall rate when using AID in critical systems?\n\n  7. Can AID be combined with other testing methods like fuzz testing or formal verification for even better results?\n\n# Conclusion\n\nAID has disrupted the realm of automated software testing. With its innovative\ncombination of LLMs and differential testing, AID represents a significant\nleap forward in building reliable high-quality software.\n\nSabrina Ramonov\n\nGen AI Engineering and Entrepreneurship\n\nHome\n\nPosts\n\n\u00a9 2024 Sabrina Ramonov.\n\nPrivacy Policy\n\nTerms of Use\n\nPowered by beehiiv\n\n", "frontpage": false}
