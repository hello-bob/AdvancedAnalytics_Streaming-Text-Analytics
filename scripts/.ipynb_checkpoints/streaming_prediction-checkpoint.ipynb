{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7040a554-46dc-4f62-a882-5da3226fa098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.classification import LogisticRegression, LinearSVC, RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder, Tokenizer, StopWordsRemover, CountVectorizer, IDF, PCA, HashingTF\n",
    "\n",
    "\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit, sum, when\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca444d20-156a-4644-a14d-407ca1c5ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change file path\n",
    "file_path = \"/Users/hydraze/Library/CloudStorage/GoogleDrive-tohziyu2@gmail.com/My Drive/Studies/KU Leuven/Courses/Classes/Y1S2/Advanced Analytics in Business/Project/3/AdvancedAnalytics_Streaming-Text-Analytics/\"\n",
    "os.chdir(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beb48c6-4b18-41b1-b764-ca31cfec88ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pickled model via pipeline api\n",
    "mPath =  file_path+\"models/best_model\"\n",
    "best_model = PipelineModel.load(mPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febe474c-5e7c-4aae-9d5e-391a2f441e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy predict function that returns a random probability. Normally you'd use your loaded globals()['my_model'] here\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "\n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "\n",
    "    # Data cleaning (to update concurrently with the other jupyter notebook)\n",
    "    \n",
    "    # Extracting type of post: Show HN\n",
    "    df = df.withColumn('isShowHN', when(df.title.contains(\"Show HN\"), 1).otherwise(0))\n",
    "    \n",
    "    # Extracting time of day\n",
    "    extract_time_of_day_udf = udf(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S').strftime('%H'))\n",
    "    \n",
    "    df = df.withColumn('time_of_day', extract_time_of_day_udf(df.posted_at))\n",
    "    \n",
    "    # Extracting day of week\n",
    "    weekDay =  udf(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S').strftime('%w'))\n",
    "    \n",
    "    df = df.withColumn('day_of_week', weekDay(df.posted_at))\n",
    "\n",
    "    # Fill null values\n",
    "    df = df.na.fill({\"title\": \"\", \"source_title\": \"\", \"source_text\": \"\"})\n",
    "    \n",
    "    # And then predict using the loaded model (uncomment below):\n",
    "    df_result = best_model.transform(df)\n",
    "    df_result.select('aid', 'posted_at', 'comments', 'frontpage', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1dec46-1893-4cc8-bdc5-3f3c4ea36b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likely the usual streaming\n",
    "ssc = StreamingContext(sc, 10)\n",
    "lines = ssc.socketTextStream(\"seppe.net\", 7778)\n",
    "lines.foreachRDD(process)\n",
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a836d1-5f25-4600-8e61-1c262573868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc_t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a00ba62-43c0-4333-8579-5b1318c9ad24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
