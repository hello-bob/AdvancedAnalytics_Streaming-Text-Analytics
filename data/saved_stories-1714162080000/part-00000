{"aid": "40167884", "title": "Qwen1.5-110B", "url": "https://qwenlm.github.io/blog/qwen1.5-110b/", "domain": "qwenlm.github.io", "votes": 44, "user": "tosh", "posted_at": "2024-04-26 11:01:57", "comments": 12, "source_title": "Qwen1.5-110B: The First 100B+ Model of the Qwen1.5 Series", "source_text": "Qwen1.5-110B: The First 100B+ Model of the Qwen1.5 Series | Qwen\n\n# Qwen1.5-110B: The First 100B+ Model of the Qwen1.5 Series\n\nApril 25, 2024 \u00b7 3 min \u00b7 475 words \u00b7 Qwen Team | Translations:\n\n  * \u7b80\u4f53\u4e2d\u6587\n\nGITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\n\n# Introduction#\n\nRecently we have witnessed a burst of large-scale models with over 100 billion\nparameters in the opensource community. These models have demonstrated\nremarkable performance in both benchmark evaluation and chatbot arena. Today,\nwe release the first 100B+ model of the Qwen1.5 series, Qwen1.5-110B, which\nachieves comparable performance with Meta-Llama3-70B in the base model\nevaluation, and outstanding performance in the chat evaluation, including MT-\nBench and AlpacaEval 2.0.\n\n# Model Features#\n\nQwen1.5-110B is similar to other Qwen1.5 models and built with the same\nTransformer decoder architecture. It consists of grouped query attention (GQA)\nand it can be efficient in model serving. The model supports the context\nlength 32K tokens, and the model is still multilingual, supporting a large\nnumber of languages including English, Chinese, French, Spanish, German,\nRussian, Korean, Japanese, Vietnamese, Arabic, etc.\n\n# Model Quality#\n\nWe conduct a series of evaluations for the base language models, and we\ncompare with Meta-Llama3-70B, the recent SOTA language model as well as\nMixtral-8x22B.\n\nQwen1.5-110B| Qwen1.5-72B| Llama-3-70B| Mixtral-8x22B  \n---|---|---|---  \nMMLU| 80.4| 77.5| 79.5| 77.8  \nTheoremQA| 34.9| 29.3| 32.0| 35.9  \nGPQA| 35.9| 36.3| 36.4| 34.3  \nHellaswag| 87.5| 86.0| 88.0| 88.7  \nBBH| 74.8| 65.5| 76.6| 69.2  \nARC-C| 69.6| 65.9| 68.8| 70.7  \nGSM8K| 85.4| 79.5| 79.2| 78.6  \nMATH| 49.6| 34.1| 41.0| 41.7  \nHumanEval| 52.4| 41.5| 45.7| 45.1  \nMBPP| 58.1| 53.4| 55.1| 71.2  \n  \nThe above results show that the new 110B model is at least competitive with\nthe Llama-3-70B model in terms of base capabilities. In terms of this model,\nwe did not change the pretraining and posttraining recipes drastically, and\nthus we believe that the performance improvement compared with 72B comes from\nincreasing model size.\n\nWe also test the chat models on MT-Bench and AlpacaEval 2.0.\n\nModels| MT-Bench| AlpacaEval 2.0  \n---|---|---  \nAvg. Score| LC Win Rate  \nLlama-3-70B-Instruct| 8.85| 34.40  \nQwen1.5-72B-Chat| 8.61| 36.60  \nQwen1.5-110B-Chat| 8.88| 43.90  \n  \nCompared with the previously released 72B model, on the two benchmark\nevaluation for chat models the 110B performs significantly better. The\nconsistent improvement in the evaluation indicates that stronger and larger\nbase language models can lead to better chat models even without changing the\npost-training recipes much.\n\n## Develop with Qwen1.5-110B#\n\nWe advise you to read our blog for Qwen1.5 to figure out the usages with\nTransformers, vLLM, llama.cpp, Ollama, etc.\n\n## Conclusion#\n\nThe Qwen1.5-110B is the largest model in the Qwen1.5 series, and it is also\nthe first one with over 100 billion parameters in the series. It demonstrates\ncompetitive performance against the very recently released SOTA model\nLlama-3-70B and it is significantly better than the 72B model. This tells us\nthat there is still a lot of room in model size scaling for better\nperformance. While the releease of Llama-3 indicates the significance of data\nscaling to an extremely large scale, we believe we can get the best of both\nworlds by scaling both data and model size in our future release. Stay tuned\nfor Qwen2!\n\n\u00a9 2024 Qwen Powered by Hugo\n\n", "frontpage": true}
