{"aid": "40056634", "title": "Multicast and the Markets", "url": "https://signalsandthreads.com/multicast-and-the-markets/", "domain": "signalsandthreads.com", "votes": 1, "user": "sam_bristow", "posted_at": "2024-04-16 20:16:21", "comments": 0, "source_title": "Signals and Threads | Multicast and the Markets", "source_text": "Signals and Threads | Multicast and the Markets\n\nAll Episodes\n\nListen in on Jane Street\u2019s Ron Minsky as he has conversations with engineers\nworking on everything from clock synchronization to reliable multicast, build\nsystems to reconfigurable hardware. Get a peek at how Jane Street approaches\nproblems, and how those ideas relate to tech more broadly.\n\n### Listen and subscribe:\n\n# Multicast and the Markets\n\n#### with Brian Nigito\n\n##### Season 1, Episode 3 | September 23rd, 2020\n\nElectronic exchanges like Nasdaq need to handle a staggering number of\ntransactions every second. To keep up, they rely on two deceptively simple-\nsounding concepts: single-threaded programs and multicast networking. In this\nepisode, Ron speaks with Brian Nigito, a 20-year industry veteran who helped\nbuild some of the earliest electronic exchanges, about the tradeoffs that led\nto the architecture we have today, and how modern exchanges use these\nstraightforward building blocks to achieve blindingly fast performance at\nscale.\n\nTranscript\n\nGlossary\n\n## 0:03\n\n# Ron\n\nWelcome to Signals and Threads, in-depth conversations about every layer of\nthe tech stack from Jane Street. I\u2019m Ron Minsky.\n\nToday I\u2019m going to have a conversation with Brian Nigito, essentially, about\nthe technological underpinnings of the financial markets, and some of the ways\nin which those underpinnings differ from what you might expect if you\u2019re used\nto things like the open Internet and the way in which cloud infrastructures\nwork. We\u2019re going to talk about a lot of things, but there\u2019s going to be a lot\nof focus on networking, and some of the technologies at that level, including\nthings like IP multicast.\n\nBrian Nigito is a great person to have this conversation with because he has a\ndeep and long history with the financial markets. He\u2019s worked in the markets\nfor 20 years, some of the time he spent working at the exchange-level where he\ndid a lot of the foundational work that led to the modern exchange\narchitectures that we know today. He\u2019s also worked on the side of various\ndifferent trading firms. For the last eight years, Brian\u2019s been working here\nat Jane Street and his work here has covered a lot of different areas, but\ntoday, he spends a lot of time thinking about high performance, low latency,\nand especially network level stuff.\n\nSo let\u2019s dive in. I think one thing that I\u2019m very sensitive to is a lot of the\npeople who are listening, don\u2019t know a ton about the financial markets and how\nthey work. And so just to get started, Brian can give a fairly basic\nexplanation of what an exchange is.\n\ntext copied link copied\n\n## 1:22\n\n# Brian\n\nI think when you hear about an exchange, you can think of lots of different\nkinds of marketplaces. But when we talk about an exchange, we\u2019re talking about\na formal securities exchange. And these are the exchanges that the SEC\nregulates, and they meet all of the rules necessary to allow people to trade\nin securities. So when we use that loosely, yeah, it\u2019s pretty different than\nyour average flea market. Supposed to be anyway.\n\ntext copied link copied\n\n## 1:46\n\n# Ron\n\nThat\u2019s obviously a function which, once upon a time was done with physical\npeople in the same location, right? Those got moved into more formal, more\norganized exchanges with more electronic support. Then eventually there\u2019s this\nkind of transformation that\u2019s happened essentially over the last 20 years,\nwhere the human element has changed enormous amount. Now, humans are obviously\ndeeply involved in what\u2019s going on, but the humans are almost all outside of\nthe exchange and the exchange itself has become essentially a kind of purely\nelectronic medium.\n\ntext copied link copied\n\n## 2:17\n\n# Brian\n\nYeah, it\u2019s a really interesting story because you have examples of\ncommunication technologies and electronic trading, going back to the late\n\u201960s, but probably more mid-\u201870s (I\u2019m being a little loose with dates) but it\nwas kind of always present. But the rule set was not designed to force people\nto operate at the kinds of timescales that electronic systems would cause you\nto operate at. It was rather forgiving. So you know, if somebody on the floor\ndidn\u2019t want to deal with then electronic exchange, the electronic exchange had\nto wait. Over the past 10 to 15 years, that\u2019s kind of flipped and so\ngenerally, we favor, always accessible electronic quotations.\n\ntext copied link copied\n\n## 3:05\n\n# Ron\n\nTo step back a little bit. The exchanges are the places for people to meet and\ntrade, as you said, to advertise their prices. And for people to transact with\neach other. Other than people who are buying and selling, what are the other\npeople who interact at the exchange level, what are the other kind of entities\nthat get hooked in there.\n\ntext copied link copied\n\n## 3:19\n\n# Brian\n\nSo you have, obviously the entities who either in their own capacity or on\nbehalf of other people are transacting securities, but then you have financial\ninstitutions that are clearing and guaranteeing those trades, providing some\nof the capital or leverage to the participants who are trading, they obviously\nwant to know what\u2019s going on there. You have other exchanges, because the rule\nset requires the exchanges to respect each other\u2019s quotations \u2013 in this odd\nway, there\u2019s a web where the the exchanges are customers of each other. And\nyou may also have various kinds of market data providers. So those quotes that\nreflect the activity on the exchange are eventually making their way all the\nway down to what you might see scrolling on the bottom of the television or\nyour brokerage screen or financial news website, etc. I guess they even make\nit all the way down to the printed page when the Wall Street Journal prints\ntransaction prices.\n\ntext copied link copied\n\n## 4:10\n\n# Ron\n\nSo what does this look like at a more systems level? What are the messages the\ndifferent participants are sending back and forth?\n\ntext copied link copied\n\n## 4:16\n\n# Brian\n\nThe most primitive sorts of things are you have orders or instructions, there\nare other platforms where we have quotes and we may use that loosely, but\nwe\u2019ll just say orders. An order would just say that I would like to buy or\nsell, let\u2019s say, a specific stock, and I\u2019d like to do so at no worse than this\nprice, and for no more than this quantity, and that may mean I could get\nfilled at a slightly better price than that. I could get filled for less\n[quantity] than that. I could get filled not at all.\n\nThat order could basically check the book immediately and then come right back\nto me if there\u2019s nothing to be done. Or it can rest there for some nonzero\namount of time where it could advertise and other people may see it and choose\nto interact with it. And then obviously, I can withdraw that interest or\ncancel it. So when we talked about orders or cancels, those go hand in hand.\n\nFinally, there\u2019s execution messages where if you and I intersect on our\ninterest I want to buy you want to sell or vice versa, then the exchange is\ngoing to generate an execution to you. And to me, saying that that happened\nand the terms of that trade.\n\ntext copied link copied\n\n## 5:22\n\n# Ron\n\nOne of the key properties here is that you have a fairly simple core set of\nmessages, this basic data structure at the heart of it called \u201cthe book,\u201d\nwhich is the set of orders that have not been satisfied. And then people can\nsend messages to add orders and remove orders. And then if two orders cross,\nif there are two orders that are compatible, where a trade can go up, then an\nexecution occurs and the information flows out.\n\nThere\u2019s a fairly simple core machine at the heart of it but then lots of\ndifferent players who want different subsets of information for different\npurposes. There are people who are themselves trading, who want to see of\ncourse, their own activity and all the detail about that. And they also want\nto see what we call \u201cmarket data,\u201d that kind of public anonymized version of\nthe trading activity so you can see what are the prices there are out there\nthat are advertised for you to go and transact against.\n\nIn the end, you need to build a machine that\u2019s capable of running this core\nengine, doing it at sufficient speed, doing it reliably enough... Maybe a\nthing that\u2019s not apparent if you haven\u2019t thought about it is: there\u2019s like a\ndisturbing dizzying amount of money at stake. And oh my God, you do not want\nto lose track of the transactions, right? If you say like, \u201cOh, you guys did\nthis trade,\u201d and then you forget about it and don\u2019t report it, or you report\nto one side and not the other, terrible things happen. So reliability is a key\nthing.\n\ntext copied link copied\n\n## 6:35\n\n# Brian\n\nYeah and I think to go back, there\u2019s lots of different consumers, lots of\ndifferent participants. And I think the key word there is there\u2019s lots of\ncompeting participants. So one thing you didn\u2019t mention in there is\ndisseminating all that information fairly. So trying to get it to everybody at\nthe same time is a real challenge and one that participants are studying very,\nvery carefully and looking for any advantage they can technologically within\nthe rule set, etc. So that extra layer of competition sort of makes the\nproblem a little more complicated, and a little more challenging.\n\ntext copied link copied\n\n## 7:10\n\n# Ron\n\nThis fairness issue is one that you\u2019ve seen from the inside working on early\nexchange infrastructure at Island and Instinet, which eventually became the\ntechnology that NASDAQ is built on. Early on, you guys built an infrastructure\nthat I think didn\u2019t have all the fairness guarantees that modern exchanges\nhappen today. Can you say more about how that actually plays out in practice?\n\ntext copied link copied\n\n## 7:29\n\n# Brian\n\nWhen working on the Island system, it was very close, originally, to sort of\n\u201cfair\u201d in that you had the same physical machines, you had an underlying\ndelivery mechanism, which we\u2019ll talk about, that was very fair at getting it\nto those individual machines. And then you were sending copies of orders or\ninstructions after going through one application to everyone. So you were all\npassing through about the exact same amount of work and the about the exact\nsame number of devices. But it was actually very inefficient, we were using\nthousands of machines that were mostly idle.\n\nSo once we started trying to handle multiple clients on a single machine, it\nexposed some obvious and silly problems. The naive implementation where people\nwould connect, we would collect all of those connections. And then when we had\na message, we would send them on the connections serially, often in the order\nin which people connected. Well, that immediately led to thousands of messages\nper second before the exchange open where somebody tried to be the very first\nconnection in that line.\n\nThen you start sort of round-robining, so you know, you start from one and\nthen the next time around, you start from two etc, etc, to try to randomize\nthis. And then you had people who were connecting to as many different ports\nas they could and taking the fastest of each one. And so these incentives are\nvery, very strong, and we\u2019d like to use machines to their fullest, but to\nliterally provide each participant their own unique machine for each\nconnection starts to get ridiculous as well.\n\ntext copied link copied\n\n## 9:04\n\n# Ron\n\nWhere did that lead you? How did you end up resolving that problem?\n\ntext copied link copied\n\n## 9:07\n\n# Brian\n\nA lot of these were TCP protocols. In those days, we actually had a decent\nnumber of people connecting over the open Internet. I don\u2019t think we provided\ntrading services directly over the open Internet, but we did actually provide\nmarket data that way. And TCP, probably your only reasonable option over\nsomething like the Internet. But once you started moving towards colocation,\nand towards more private networks, where people\u2019s machines were in the same\ndata center, and really only two or three network devices away from the\npublishing machine, it became a lot more feasible to start using different\nforms of networking, unreliable networking, UDP, and that leads you to\nsomething called multicast where rather than you sending multiple copies of\nthe message to n people, you send one copy that you allow the network\ninfrastructure to copy and deliver electrically much more deterministically\nand quickly.\n\ntext copied link copied\n\n## 10:03\n\n# Ron\n\nFor someone who\u2019s less familiar with the low level networking stories give a\nquick tour of the different options you have in terms of how you want to get a\npacket of data from one computer to another.\n\ntext copied link copied\n\n## 10:12\n\n# Brian\n\nThe Internet and Ethernet protocols are generally a series of layers. And at\nthe lowest layer, we have an unreliable best effort service to deliver a\npacket\u2019s worth of data. And it\u2019s sort of a one-shot thing more or less point-\nto-point from this machine to some destination address. Then we build services\non top of that that make it reliable by sequencing the data attaching sequence\nnumbers so we know the original order that was intended and having a system of\nretransmissions, measuring the average round trip time, probabilistically\nguessing whether packets are lost, etc, etc. That all gets built up into a\nfairly complex protocol that most of the Internet uses, TCP. Maybe not all and\nthere are some people pushing for future extensions to that, but by and large,\nI\u2019d say that the vast majority of reliable, in-order, connected data over the\nInternet is sent via TCP.\n\nTCP assumes that there\u2019s one sender, and one receiver and it has unique\nsequence numbers for each of those connections. So I really can\u2019t show the\nsame data to multiple participants, I actually have to write a unique copy to\neach participant. UDP is a much lighter layer on top of the underlying raw\ntransport, still unreliable but with a little bit of routing information. That\nprotocol has some features where you can say I want to direct this to a\nspecific participant, or a virtual participant, which the network could\ninterpret as a group. Machines can join that group and then that same message\ncan be delivered by network hardware to all the interested parties.\n\ntext copied link copied\n\n## 11:53\n\n# Ron\n\nOne of the key features there which I think is maybe not obvious \u2013 why would I\nprefer multicast over unicast? Why is it better for me to send one copy to the\nswitch that then sends a bunch of copies to a bunch of different recipients,\nversus me just sending a bunch of individual copies on my own. What\u2019s the\nadvantage of baking this into the actual switch infrastructure?\n\ntext copied link copied\n\n## 12:13\n\n# Brian\n\nThe switches are very fast and deterministic about how they do that. And\nbecause of, I think their usage in the industry, they\u2019ve gotten faster and\nmore deterministic. So they can just electrically repeat those bits\nsimultaneously to 48 ports, or whatever, that that switch might have. And\nthat\u2019s just going to be much faster and more regular than you trying to do it\non a general purpose server, where you might be writing multiple copies\nwriting to multiple places, you really can\u2019t compare the two.\n\ntext copied link copied\n\n## 12:56\n\n# Ron\n\nOne of the key advantages of using switches is that the switches are doing the\ncopying in specialized hardware, which is just fundamentally faster than you\ncan do on your own machine.\n\nAlso there\u2019s a distributed component of this. When you make available\nmulticast stream, there\u2019s this distributed algorithm that runs on the switches\nwhere it learns essentially what we call the \u201cmulticast tree.\u201d At each layer,\neach switch knows to what other switches it needs to forward packets, and then\nthose switches know which port they need to forward packets to. That gives you\nthe ability to kind of distribute the job of doing the copying. So the if you\nhave like 12 recipients in some distant network, you can send one to the local\nswitch, and then the final copying happens at the last layer, at the place\nwhere it\u2019s most efficient. That\u2019s the fundamental magic trick that multicast\nis providing for you.\n\ntext copied link copied\n\n## 13:33\n\n# Brian\n\nAs the network\u2019s gets simpler, the very first versions we were using weren\u2019t\neven using multicast. We were using something called broadcast, which just\nbasically said, \u201canything you get, I want you to repeat everywhere.\u201d It\u2019s\nfunny because you could imagine that you could certainly overwhelm a network\nthat way. And a large part of the uncertainty and the variation that comes\nfrom TCP are these self-learning algorithms that are very concerned about\nnetwork health. When we would work with the Linux kernel maintainers and have\nquestions about variability that we saw, then they would say, \u201cWell, you\nshouldn\u2019t be using TCP. If you care about latency, you shouldn\u2019t be using TCP.\nTCP makes trade offs all the time, for network health, and so on and so forth.\nAnd for the Internet that is absolutely necessary. And if you really have\nthese super tight requirements, and you really want them to get there fast,\nand you have a controlled network with very little packet loss, and very few\nlayers between participants, you should be using UDP.\u201d And they were probably\nright. We mostly do nowadays for this stuff, but it took a while to get there.\n\ntext copied link copied\n\n## 14:37\n\n# Ron\n\nAnd they were right in a way that was kind of totally unactionable. Which is\nto say, there are a bunch of standardized protocols, about how you communicate\nwhen you\u2019re sending orders. Another thing to say about the trading world is if\nyou step back and look at how the protocols we use are broken up, there are\ntwo kinds of primary data flows that a trading firm encounters, at least when\nwe\u2019re talking to an exchange. There is: the order flow connection, where we\nsend our specific orders and our specific cancels and see the specific\nresponses to those, and that is almost always done on a TCP connection; then\nthere is the receipt of market data, and that\u2019s where you\u2019re sending the data\nthat everyone needs to see exactly the same anonymized stream of data, and\nthat\u2019s almost always done through multicast.\n\nSo there is part of the data which is done via UDP in the way the Linux kernel\ndevelopers would recommend, and there\u2019s part of the data flow that\u2019s still to\nthis day, done under TCP. I think the difference is, we no longer use the open\nInternet in the way that we once did. I think there\u2019s been this transformation\nwhere instead of sending things up to the trunk and having things routed\naround the big bucket of the open Internet, trading firms will typically have\nlots in the way of colocation sites where they will put some of their servers\nvery near to the switches that the exchange manages, and they will have what\nwe call cross-connects: We will connect our switch to their switch and then\nbridge between the two networks and deliver multicast across these kind of\nlocal area networks that are very tightly controlled, that have very low rates\nof message loss. So in some sense, we\u2019re running these things over a very\ndifferent network environment than the one that most of the world uses.\n\ntext copied link copied\n\n## 16:15\n\n# Brian\n\nYeah, a couple interesting observations: It means that colocation makes\ncompetition between professional participants more fair \u2013 it enables us to use\nthese kinds of technologies, whereas, without colocation, you have less\ncontrol over how people are reaching you, and you end up with probably more\nvariation between participants.\n\nI think it\u2019s also worth saying that a lot of things we\u2019re talking about are a\nlittle bit skewed towards US equities and equities generally. There\u2019s lots of\nother trading protocols that are a little bit more bilateral, there isn\u2019t like\na single price that everybody observes. You know, in currencies often people\nshow different prices to different people, there\u2019s RFQ workflows in fixed\nincome and somewhat in equities and ETFs. But by and large, probably the vast\nmajority of the messages generated, look a bit like this, with shared public\nmarket data that\u2019s anonymized but viewable by everyone, and then the private\nstream, as you say, of your specific transactions and your specific\ninvolvement.\n\ntext copied link copied\n\n## 17:13\n\n# Ron\n\nI think from a perspective of what was going on, you know, 15 years ago, I\nfeel like the obvious feeling was, Well, yeah, the equity markets are becoming\nmore electronic, and more uniform and operating this way with this kind of\ncentral open exchange, and not much in the way of bilateral trading\nrelationships, and surely this is the way the future and everything else is\ngoing to become like this. No, actually, the world is way more complicated\nthan that. And currencies and fixed income and various other parts of the\nworld just have not become that same thing.\n\ntext copied link copied\n\n## 17:4\n\n# Brian\n\nYeah, and I think that\u2019s partly because those products are just legitimately\ndifferent, and the participants have different needs. Sometimes it\u2019s because\nthe equity markets happened so \u2013 I think relatively rapidly \u2013 a lot of the\ntransformation happened there. And so other markets that were a little bit\nbehind saw the playbook, they saw how it changed, and they sort of positioned\nand controlled some of that change to maintain their current business models,\netc.\n\nI wanted to go back to one thing \u2013 you said, we mostly use TCP. It\u2019s\ninteresting because there were attempts (I know of at least one off the top of\nmy head, probably there are more) to use UDP for order entry. Specifically,\nsomebody had a protocol called UFO, UDP for Orders. There wasn\u2019t a ton of\nuptake because look, if you\u2019re a trading firm connecting to 60 exchanges, and\n59 of them require you to be really good at managing a TCP connection, and one\nof you offers a unique UDP way, like that\u2019s great. But that\u2019s one out of 60,\nso I kind of have to be good at the other thing anyway, so there just wasn\u2019t\nas much adoption because there\u2019s just enough critical mass and momentum that\nthe industry kind of hovers around a certain set of conventions.\n\ntext copied link copied\n\n## 18:49\n\n# Ron\n\nThe place where you see other kinds of technologies really taking hold, are\nwhere there\u2019s a much bigger advantage to using it. I think when distributing\nmarket data, it\u2019s just kind of, obviously almost grotesquely wasteful, to send\nthings in unicast form where you send one message per recipient. And so\nmulticast is a huge simplifier. It makes the overall architecture simpler,\nmore efficient, fairer. There\u2019s a big win, that really got adopted pretty\nbroadly.\n\nWe\u2019ve kind of touched on half of the reason people use multicast. Which is, I\nthink, one of the core things I\u2019m kind of interested in in this whole story is\n\u201cwhy is trading so weird in this way?\u201d\n\nWhen I was a graduate student many years ago, multicast was going to be a big\nthing. It was going to be the way in the Internet that we delivered video to\neveryone. Totally dead. Multicast on the open Internet doesn\u2019t work. Multicast\nin the cloud basically doesn\u2019t work. But multicast in trading environments is\na dominant technology. And one of the reasons I think it\u2019s a dominant\ntechnology is because it turns out there are a small number of videos that we\nall want to watch at the same time. Unlike Netflix, where everybody watches a\ndifferent thing, we actually want in the trading world to all see what\u2019s going\non on NASDAQ and ARCA and NYSE and CBOE, and so on so forth, live in real\ntime, we\u2019re all stuck to the same cathode ray tube. But there\u2019s a whole\ndifferent way that people use multicast that has less to do with that, which\nis that multicast is used as a kind of internal coordination tool for building\ncertain kinds of highly performant highly scalable infrastructure. What is the\nrole that multicast plays on the inside of exchanges and also on the inside of\nlots of firms trading infrastructure?\n\ntext copied link copied\n\n## 20:32\n\n# Brian\n\nThe exchange, the primary thing it\u2019s doing is determining the order of the\nevents that are happening. And then the exchange wants to disseminate that\ninformation to as many participants as possible. So certain parts of this\ndon\u2019t parallelize very well, then the sequence has to pretty much be done in\none place for the same security. So you ended up where you were trying to\nfunnel a lot of traffic down into one place and then report those results\nback. In that one place, you want to do as little work as possible so that you\ncould be fast and deterministic. Then you were spreading that work out into\nlots of other applications that were sort of following along and provided\nvalue added information, value added services, and reporting what was\nhappening in whatever their specific protocol was. So the same execution that\ntells you that you bought the security you\u2019re interested in can also tell your\nclearing firm, maybe in a slightly different form, can tell the general public\nvia market data that\u2019s anonymized and takes your name off of it, etc, etc.\n\ntext copied link copied\n\n## 21:33\n\n# Ron\n\nLet me try and kind of sharpen the point you\u2019re making here. Because I think\nit\u2019s an interesting fact about how this kind architecture all comes together,\nwhich is: the kind of move you\u2019re talking about making is taking this very\nspecific and particular problem of \u201cwe want to manage a book of open orders on\nan exchange and distribute this and that kind of data\u201d and turning it into a\nfairly abstract CS problem of transaction processing. You\u2019re saying \u201cLook,\nthere\u2019s all these things that people want to do the actual logic and data\nstructure at the core of this thing is not incredibly complicated. So we want\nto do is just to simplify all of the work around it, we\u2019re just going to have\na system whose primary job is taking the events, you know, the request to add\norders and cancel and so forth. And choosing an ordering and then distributing\nthat ordering to all the different players on the system so that they can do\nthe concrete computations that need to be done to figure out what are the\nactual executions that happen, what are things need to be reported to\nregulators what needs to be reported on the public market data.\u201d\n\nThen, multicast becomes essentially the core fabric that you use for doing\nthis. You have one machine that sits in the middle, you can call it the\nmatching engine, but you could also reasonably just call it a sequencer\nbecause its primary role is getting all the different requests, and then\npublishing them back out in a well defined order. It\u2019s worth noting that\nmulticast gives you part of the story but not all of the story because it\ngives you getting messages out to everyone, but it misses two components. It\ndoesn\u2019t get it to them reliably, meaning messages can get lost, and it doesn\u2019t\nactually get them to each participant in order. Essentially, the sequencer\nkind of puts a counter on each message, so it\u2019s like, \u201cOh, I got messages\n1-2-4-3, well, okay, I\u2019ve got to reorder them and interpret them as 1-2-3-4.\u201d\nThen also that ordering lets you detect when you lose messages and then you\nhave another set of servers out there, whose job is to retransmit when things\nare lost, they can fill the gaps. Now this is a sort of specialized\nsupercomputer architecture, which gives you this very specialized bus for\nbuilding what you might call state machine style applications.\n\ntext copied link copied\n\n## 23:37\n\n# Brian\n\nRight, and I will say, I think I\u2019m aware of a number of exchanges that\nactually do have a model where they actually have just a sequencer piece that\ndoes no matching that really just determines the order. And then some of these\nsidecar pieces are the ones that are actually determining whether matches do\nindeed happen, and then sequencing them back, reporting them back, etc, etc.\nSo there\u2019s definitely examples of that.\n\nA couple of other the points: Yeah, the gap filling and recovery has been a\nproblem that I think is covered by other protocols. There are reliable\nmulticast RFCs and protocols out there and everywhere I\u2019ve been when we\u2019ve\nlooked at them, we\u2019ve run into the problem that they have the ability for\nreceivers to slow or stop publication. In those cases, if you scale up to\nhaving thousands of participants, there\u2019s sort of somebody somewhere who\nalways has a problem. So using any of these general purpose, reliable\nmulticast protocols never seemed to quite fit any of the problems that we had.\nAnd I think because of the lack of use for the other reasons you mentioned,\nthey were generally not super robust compared to what we had to build\nourselves. And so we ended up doing exactly that where we added sequencing and\nthe ability to retransmit missed messages in various specialized ways.\n\nIt\u2019s also worth noting that you get some domain-specific benefits that I think\nalso can generalize, where if you\u2019ve missed a sufficient amount of data, I\nguess you can always replay everything from the beginning. But it sort of\nturns out that if you know your domain really well, and you can compress that\ndata down to some fixed amount of state, you can have an application that\nstarts after 80% of the day is complete, and be immediately online because you\ncan give him just a smaller subset of the state. And a general purpose\nprotocol like TCP where you\u2019d have to sort of replay any missed data has a\nnumber of problems in trading: That can be buffered there for sort of\narbitrarily long and it assumes you still want it to get there. And it\u2019s\nbuffering it byte-by-byte. Whereas, you know, if you say, \u201cOh, I\u2019d like to\nplace an order, I\u2019d like to cancel an order, I\u2019d like to place an order, I\u2019d\nlike to cancel an order.\u201d If all of those are sitting in your buffers the\nideal thing to do would be well, if you know the domain, they cancel each\nother before even going out if they\u2019re waiting in the buffer, and you send\nnothing. So when we design those protocols ourselves optimized for the\nspecific domain, we can pick up a little bit more efficiency when we do it.\n\ntext copied link copied\n\n## 26:07\n\n# Ron\n\nThis is, in fact, in some ways, a general story about optimizing many\ndifferent kinds of systems: specialization, understanding the value system of\nyour domain, and being able to optimize for those values. I think the thing\nyou were just saying about not waiting for receivers, that\u2019s in some sense,\npart of the way in which people approach the business of trading. The people\nwho are participating in trading care about the latency and responsiveness of\ntheir systems. People who are running exchanges, who are disseminating data,\ncare about getting data out quickly and fairly, but they care more about\ngetting data to almost everyone in a clean way than they do making sure that\neveryone can keep up. So you\u2019d much rather just kind of pile forward\nobliviously and keep on pushing the data out, and then if people are behind,\nwell, you know, they need to think about how to engineer their systems\ndifferently so they\u2019re going to be able to keep up. You worry about the bulk\nof the herd, but not about everyone in the herd. You know, the stragglers in\nthe herd, they can catch up and get retransmissions later, and they\u2019re going\nto be slower, but we\u2019re not going to slow down. Understanding what\u2019s\nimportant, the applications can be massively simplified. A huge step you can\ntake in any technical design is figuring out what are the part of the problems\nyou don\u2019t have to solve.\n\ntext copied link copied\n\n## 27:11\n\n# Brian\n\nI think it\u2019s also worth saying that the problem is somewhat exacerbated by\nfragmentation. We said it\u2019s important for people to determine the order of\nevents, but you also need to report it back to them quickly. And, you know,\nreliably quickly, deterministically quickly, because that translates directly\ninto better prices. If I told you that you could submit an order and it would\nbe live for the next six or eight hours, you\u2019re going to enter probably a much\nmore conservative price. And let\u2019s say I\u2019m actually acting as an agent for\nyou, I\u2019m routing your order to one of these other 14 exchanges. Well, I may\nwant to check one and then go on to the next one. The faster and more reliable\nit is for me to check this one, the more frequently I\u2019ll do so. If I think\nthere\u2019s a good chance that the order will get held up there, well, that\u2019s\nopportunity cost, I may miss other places. So this is all kind of a rambling\nway of saying that speed and determinism translate directly into better prices\nwhen you have markets competing like this.\n\ntext copied link copied\n\n## 28:13\n\n# Ron\n\nPeople often don\u2019t appreciate some of the reasons that people care about\nperformance in exactly the way that you\u2019re kind of highlighting. To give\nexample in the same vein, like this fragmentation story, you might want to put\nout bids and offers at all of the different exchanges where someone might want\nto transact, right, there\u2019s a bunch of different marketplaces, you want to\nshow up on all of them. You may think, \u201cOh, I\u2019m willing to buy or sell 1000\nshares of the security. And I\u2019m happy to do it anywhere.\u201d But you may not be\nhappy to do it everywhere. There\u2019s like a missing abstraction in the market.\nSo they want to be able to express something like I would be willing to buy\nthe security at any one of these places, but they can\u2019t do it. So they try and\nsimulate that abstraction by being efficient, by being fast. So they\u2019ll put\nout their orders on lots of different exchanges, and then when they trade on\none of them, they\u2019ll say \u201cOkay, I\u2019m no longer interested,\u201d so they\u2019ll pull\ntheir orders from the others, and they\u2019re now worried about other\nprofessionals who are also very fast, who try to route quickly and in parallel\nto all the different places and take all the liquidity that shows up all at\nonce. This dynamic, that the speed and determinism of the markets now becomes\nsomething that essentially affects the trade offs between different\nprofessional participants in the market.\n\ntext copied link copied\n\n## 29:22\n\n# Brian\n\nYeah, that\u2019s right.\n\ntext copied link copied\n\n## 29:23\n\n# Ron\n\nAnother thing I kind of want to talk about for a second is: what are some of\nthe trade offs that you walk into when you start building systems on\nmulticast? Like I remember a bunch of years ago, you were like, in the guts of\nsystems like Island and Instinet, and NASDAQ, and Chi-X, and all that,\nbuilding this infrastructure. Before you came to Jane Street, I was on the\nother side. At the time, I think Jane Street understood much less about this\npart of the system. I remember the first time we heard a description from\nNASDAQ about how their system worked, and I basically didn\u2019t believe them, for\ntwo reasons. One reason is, it seemed impossible. The basic description was,\nthe way NASDAQ works is, every single transaction on the entire exchange goes\nthrough a single machine on a single core. And on that core is running a more\nor less ordinary Java program that processes every single transaction, and\nthat single machine was the matching engine, the sequencer. I didn\u2019t really\nknow how you could make it go fast enough for this to work. There was\nessentially a bunch of optimization techniques that at the time, we just\ndidn\u2019t understand well enough. And also, it just seemed perverse, like what\nwas the point why to go to all that trouble? Maybe you could do it, but why?\n\ntext copied link copied\n\n## 30:35\n\n# Brian\n\nWell, a couple things. I mean, first, I want to say like on all the systems\nyou mentioned, I like to think I did some good engineering work, but I was\ncertainly part of many excellent teams, and worked with just a tremendous\nbunch people over the years.\n\nBut yes, from a performance perspective, you said, well, the fewer processes I\nhave, the simpler the system is and it gives you some superpowers there where\nyou just don\u2019t have to worry about splitting things up in various ways. There\nwere certainly some benefits to adding complexity, but a lot of that came\nabout as hardware itself started to change. And that should provide probably\nthe baseline for optimization. I think you want to understand the hardware and\nthe machines you\u2019re using, the machines that are available, the hardware\nthat\u2019s available to you deeply. And you want to basically model out what the\ntheoretical bounds are. Then when you look at what you\u2019re doing in software,\nand you look at the kind of performance you\u2019re getting, if you can\u2019t really\nexplain where that is relative to what\u2019s capable, you\u2019re leaving some\nperformance on the floor. And so we were trying very, very hard to understand\nwhat the machine could theoretically do, and really utilize it to its fullest.\n\ntext copied link copied\n\n## 31:4\n\n# Ron\n\nPart of what you\u2019re saying is instead of thinking about having systems where\nyou fan out and distribute and break up the work into pieces, you stop and you\nthink if we can just optimize to the point where we can handle the entire\nproblem in a single core, a bunch of things get simpler, right? We\u2019re just\ngoing to keep everything going through this one stream. There\u2019s a lot of work\nthat goes into making things uniformly fast enough for this to make sense. But\nit simplifies the overall architecture in a dramatic way.\n\ntext copied link copied\n\n## 32:17\n\n# Brian\n\nIt definitely does. And it\u2019s been pretty powerful. I mean, not every exchange\noperates exactly on these principles, there\u2019s certainly lots of unique\nvariations that people have put out there. But I do think that it is pretty\nubiquitous. Certainly the idea that exchanges want some kind of multicast\nfunctionality, I think is universal at this stage. I\u2019m sure there may be an\nexception here or there. But amongst high-performance exchanges with\nprofessional participants like this, I think it\u2019s pretty universal.\n\ntext copied link copied\n\n## 32:47\n\n# Ron\n\nWhen you\u2019re talking about publication of market data, we can see that directly\nsince we\u2019re actually subscribing to multicast in order to receive the data\nourselves, but their internal infrastructure often depends on multicast as\nwell, right?\n\ntext copied link copied\n\n## 32:59\n\n# Brian\n\nTrue, although, you know, I\u2019m not as familiar with the crypto side of the\nworld. But since a lot of that is happening over the open Internet, UDP is\nprobably not one of the options. And so you have people using more, you know,\nWebSockets, and JSON API and things like that. But it is kind of the exception\nthat proves the rule, right? Because of that focus on the open Internet and\neverything, you\u2019ve got a totally different set of tools.\n\ntext copied link copied\n\n## 33:25\n\n# Ron\n\nIt highlights the fact that the technical choices are in some sense\nconditional on the background of the people building it. There\u2019s two sides of\nthe question we were just talking about, there\u2019s a question of what\u2019s the\nadvantage of doing all this performance engineering and the other question is,\nhow do you do it? How do you go about it?\n\ntext copied link copied\n\n## 33:44\n\n# Brian\n\nIt has moved around over the years. You know, many years ago, I remember that\nwe had interrupt driven I/O \u2013 packets would come into the network card where\nthey would essentially wait for some period of time and if it had waited there\nlong enough or enough data had accumulated, then the network card would\nrequest an interrupt for the CPU to come back and service the network card.\nHow frequently should we allow interrupts? If we allow them anytime a packet\narrives, that\u2019d be way too much CPU overhead. So there were trade offs of\nthroughput and latency.\n\nBut once you end up with the sheer number of cores that we do nowadays, we can\nessentially do away with interrupts and just wait for the network card by\npolling and checking \u201cDo you have data? Do you have data? Do you have data? Do\nyou have data?\u201d and the APIs have shifted a bit away from general purpose\nsockets. The sockets APIs require lots of copies of the data. There\u2019s an\nownership change there: When you read, you give the API a buffer that you own,\nthe data is filled in from the network and then given back to you. So this\nbasically implies a copy on all of the data that comes in. If you start to\nlook at say 25 gigabit networking, that means you basically have to copy 25\ngigabits a second to do anything at a baseline. The alternative is you try to\nreduce those copies as much as possible, and you have the network card just\ndelivering data into memory, the application polling, waiting for that data to\nchange, seeing the change, showing it to the application logic, and then\ntelling the network card, he\u2019s free to overwrite that data, you\u2019re done with\nit. When you get down to that level, you really are getting very close to the\nraw performance of what the machine is capable of.\n\nSo eliminating the copies and the unnecessary work in the system, that\u2019s\ncertainly one. Trying to make your service times for every packet in every\nevent as reliable and deterministic as possible so that you have very smooth\nsort of behavior when you queue. You don\u2019t end up having to do that\neverywhere. The critical path tends to be pretty small when it\u2019s all said and\ndone. I think one of the guys who had built the Island system really kind of\nhad the attitude that if any piece of the system is so complicated that you\ncan\u2019t rewrite it correctly and perfectly in a weekend, it\u2019s wrong. And so I\nthink that, you know, probably the average length of an application there was,\nyou know, 2000 lines or something like that, and the whole exchange probably\nwas maybe four or five applications this together.\n\ntext copied link copied\n\n## 36:11\n\n# Ron\n\nSad to say, I think we do not follow that rule in our engineering: I think we\ncould not rewrite all of our components in a weekend, I\u2019m afraid.\n\ntext copied link copied\n\n## 36:1\n\n# Brian\n\nThe world has gotten more complicated, but it\u2019s not a bad goal! To often ask\npeople, and I think it\u2019s consistent with reliability and performance, to\nconstantly ask yourself, \u201cyes, but can it be simpler?\u201d We want it to be as\nsimple as possible \u2013 no simpler \u2013 but as simple as possible. And it really is\na mark of you deeply understanding the problem when you can get it down to\nsomething that seems trivial to the next person who looks at it. It\u2019s a little\ndepressing because you kill yourself to get to that point. And then the next\nperson that sees it is like, \u201cOh, that makes sense. That seems obvious. What\ndid you spend all your time on?\u201d Like, if only you knew what was on the\ncutting room floor.\n\ntext copied link copied\n\n## 36:57\n\n# Ron\n\nOne thing that strikes me about this conversation is that just talking with\nyou about software is pretty different than lots of the other software\nengineers that I talk to because you almost immediately in talking about these\nquestions go to the question of what does the machine do? And how do you map\nthe way you\u2019re thinking about the program onto the actual physical hardware?\nCan you just talk for a minute about the role of mechanical sympathy (which is\na term I know you like for this) in the process of designing the software\nwhere you really care about performance?\n\ntext copied link copied\n\n## 37:2\n\n# Brian\n\nI do love that term. I did not coin that term. I think there was a blog by a\nnumber of people in the UK who ran a currency exchange called LMAX, and a\ngentleman ran a blog under that name. But it comes from a racecar driver, who\nwas talking about how drivers with mechanical sympathy, who really had a deep\nunderstanding of the car itself, were better drivers, in some way. And I think\nthat that translates to performance in that if you have some appreciation for\nthe physical, mechanical aspects, just the next layer of abstraction in how\nour computers are built, you can design solutions that are really much closer\nto the edge of what they\u2019re capable of. And it helps you a lot, I think in\nterms of thinking about performance when you know where those bounds are. So I\nthink what\u2019s important there is it gives you a yardstick.\n\nWithout that, without knowing what the machine is capable of, you can\u2019t\nquickly back of the envelope say, \u201cdoes the system even hang together? Can\nthis work at all?\u201d If you don\u2019t know what the machine is capable of you can\u2019t\neven answer that question. Then when you look at where you\u2019re at, you say,\n\u201cWell, how far am I from optimal?\u201d Without knowing what the tools you have are\ncapable of, I just don\u2019t know how you answer that question and when you stop\ndigging, so to speak. If you\u2019re observing that, the market, be it from a\ncompetitive perspective, or just the demands of the customer, are much higher\nthan what you think is possible, well, you\u2019ve probably got the wrong\narchitecture, you\u2019ve probably got the wrong hardware. It\u2019s kind of hard for me\nto not consider that.\n\ntext copied link copied\n\n## 39:01\n\n# Ron\n\nAs a practical matter as a software engineer, how do you get a good feel for\nthat? I feel like lots of software engineers, in some sense, operate most of\nthe time at an incredible remove from the hardware they work on. There\u2019s like,\nthe programming language they\u2019re in, and that compiles down to whatever\nrepresentation and maybe it\u2019s a dynamic programming language, and maybe it\u2019s a\nstatic one. There\u2019s like several different layers of infrastructure and\nframeworks they\u2019re using, and there\u2019s operating system, and you know, they\ndon\u2019t even know what hardware they\u2019re running on. They\u2019re running on\nvirtualized hardware in lots of different environments for lots of software\nengineers... A kind of concrete and detailed understanding of what the\nhardware can do feels kind of unachievable. How do you go about building\nintuition trying to understand better what the actual machine is capable of?\n\ntext copied link copied\n\n## 39:48\n\n# Brian\n\nI think you\u2019re separating programmers into people who get a lot of things\ndone, and people like myself \u2013 is that fair to say?\n\nIt\u2019s a good question. I think part of it is interest and I think you really\nneed to construct a lot of experiments. And you have to have a decent amount\nof curiosity and you have to be blessed with either a problem that demands it,\nor the freedom to be curious and to dig because you are going to waste some\ntime constructing experiments and your judgment, initially, is probably not\ngoing to be great. The machines nowadays are getting more and more\ncomplicated. They\u2019re trying to guess and anticipate a lot of what your\nprograms do. So very simple sorts of benchmarks, simple sorts of experiments\ndon\u2019t actually give you the insight you think you\u2019re getting from them. So I\ndo think it is a hard thing to develop. But certainly a good understanding of\ncomputer architecture or grounding in computer architecture helps. There are\nnow a decent number of tools that give you this visibility, but you do have to\ndevelop an intuition for what are the key experiments? What are the kinds of\nthings that are measurable? Do they correlate with what I\u2019m trying to\ndiscover, etc, etc. And I think it requires a lot of work, of staying current\nwith the technology and following the industry solutions, as well as what\u2019s\nhappening in the industry, generally of computing technology.\n\nYou\u2019ve gotta kind of love it, right? Gotta spend enough time to develop the\nright kind of intuition and judgment, to pick your spots when you do your\nexperiments.\n\ntext copied link copied\n\n## 41:39\n\n# Ron\n\nI think in lots of cases, people approach problems with a kind of, in some\nsense, fuzzy notion of scalability. There are some problems where if you\u2019re\nlike \u201cNo, actually I can write this one piece,\u201d it admits simpler solutions\nsome of the time, then they do if you try and make it scalable in a general\nway. You can make a thing that is scalable. But the question of being scalable\nisn\u2019t the same as being efficient. So when you think about scalability and\nthink about performance, it\u2019s useful to think about it in concrete numerical\nterms, and in terms that areat least, dimly aware of what the machine is\ncapable of.\n\ntext copied link copied\n\n## 42:15\n\n# Brian\n\nI think it\u2019s actually easy to get programmers to focus on this sort of thing:\nIf you just stop hardware people from innovating, they will have no choice.\nRight? So many programming paradigms and layers of complexity have been\nempowered by the good work of hardware folks who have continued to provide us\nwith increasing amounts of power. If that stops, and it does seem like in a\ncouple key areas that is slowing, I don\u2019t know about stopping but certainly\nslowing, then yeah, people will pay a lot more attention to efficiency.\n\ntext copied link copied\n\n## 42:46\n\n# Ron\n\nSo this is maybe a good transition to talking about some of the work that you\ndo now. You, these days, spend a bunch of your time thinking about a lot of\nthe kind of lowest level work that we do and some of that has to do with\nbuilding abstractions over network operations that give us the ability to more\nsimply and more efficiently do the kind of things that we want to do. And part\nof it has to do with hardware. So I wanted to just talk for a minute about the\nrole that you think custom hardware plays in trading infrastructures, and some\nof the work that we\u2019ve done around that.\n\ntext copied link copied\n\n## 43:17\n\n# Brian\n\nJane Street has always had a large and diversified business and for lots of\nour business, it\u2019s just not super relevant. But in the areas where message\nrates and competitiveness are a little extreme, it becomes a lot more\nefficient for us to take some of these programmable pieces of hardware and\nreally specialize for our domain. Like a network card is actually very good at\nfiltering for multicast data, it can compare these addresses bit by bit, but\nthere\u2019s really nothing that stops us from going deeper into the data and\nfiltering based on content, looking for specific securities, things like that,\nand there aren\u2019t a lot of general purpose solutions out there to do that at\nhardware speeds. But we can get programmable network cards, custom pieces of\nhardware, where we can stitch together solutions ourselves and I think that\u2019s\ngoing to become increasingly relevant and maybe even necessary, as we start to\nmove up in terms of data rates.\n\nI think earlier I mentioned that we have, I didn\u2019t get the exact number, maybe\nthere\u2019s 12 now going up to something like 15, 16, 17 different US equity\nexchanges, if each one of those can provide us data at something close to 10\ngigabits per second, and the rule set requires that we consolidate and\naggregate all that information in one place, well, we have something of a\nfundamental mismatch if we only have 10 gigabit network cards, right? So, for\nus to do that quickly and reliably in a relatively flat architecture we\u2019re\ngoing to need some magic, and the closest thing I think we have to magic is\nsome of the custom hardware.\n\ntext copied link copied\n\n## 45:02\n\n# Ron\n\nThis feels to me like the evolution of the multicast story. If you step back\nfrom it, you can think of the use of multicast in these systems as a way of\nusing specialized hardware to solve problems that are associated with trading.\nBut in this case, it\u2019s specialized networking hardware. So it\u2019s general\npurpose at the level of networking, but it\u2019s not a general purpose programming\nframework for doing all sorts of things. It\u2019s specialized to copying packets\nefficiently. Is there anything else at the level of switching and networking\nworth talking about?\n\ntext copied link copied\n\n## 45:34\n\n# Brian\n\nYeah, I think that it\u2019s funny. I don\u2019t know if I\u2019ve ever come across these\nLayer 1 crosspoint devices outside of our industry. I think certainly some use\nthem, maybe in the cybersecurity field. But within our industry, there\u2019s been\na couple of pioneering folks that have built devices that allow us to, with no\nswitching or intermediate analysis of the packet just merely replicate things\nelectrically everywhere according to a fixed set of instructions. It turns out\nthat that actually covers a tremendous number of our use cases when we\u2019re\ndistributing things like market data.\n\nSo, you know, the more traditional, very general switch will take in the\npacket, look at it, think about it, lookup in some memory where it should go\nand then route it to the next spot. That got sped up with slightly more\nspecialized switches, based on concepts from InfiniBand, that would do what\nwas known as cut-through. They would look at the early part of the packet,\nbegin to make a routing decision while the rest of the bytes were coming in,\nstart setting up that flow, send that data out, and then forward the rest of\nthe bytes as they arrive. Those were maybe an order of magnitude or even two\nfaster than the first generation. Well, these actually do no work whatsoever,\nbut just mechanically electrically replicate this data. They\u2019re another order\nof magnitude or two faster than that.\n\nA store-and-forward switch, the first kind I was describing, I don\u2019t know,\nmaybe that was seven to 10 microseconds, a cut-through switch, looking at part\nof the packet and moving it forward, maybe that\u2019s 300 to 500 nanoseconds. And\nnow these switches, these Layer 1 crosspoints, maybe they\u2019re more like three\nto five nanoseconds themselves. And so now we can take the same packet and\nmake it available in maybe hundreds of machines with two layers of switching\nlike that, and we\u2019re talking about a low single to double digit number of\nnanoseconds in terms of overhead from the network itself.\n\ntext copied link copied\n\n## 47:31\n\n# Ron\n\nI think it\u2019s an interesting point in general that having incredibly fast\nnetworking, changes your feelings about what kind of things need to be put on\na single box and what kind of things can be distributed across boxes. Computer\nscientists like to solve problems by adding layers of indirection. The\nincreasing availability of very cheap layers of indirection suddenly means\nthat you can do certain kinds of distribution of your computation that\notherwise wouldn\u2019t be easy or natural to do. What are the latencies like\ninside of a single computer versus between computers these days?\n\ntext copied link copied\n\n## 48:05\n\n# Brian\n\nIt\u2019s starting to vary quite a bit, especially with, folks like AMD having\nslightly different structure than Intel, but it\u2019s true that moving between\ncores is starting to get fairly close to what we can do with individual\nnetwork cards. To throw out some numbers that somebody will then probably\ncorrect me on, maybe that\u2019s something on the order of 100 nanoseconds. It\u2019s\nnot that different when we\u2019re going across the PCI bus and going through a\nhighly optimized network card. That might be something like 300 to 600\nnanoseconds and this is one way to get the data in. It is not unreasonable for\nthe sorts of servers that we work with to get frames all the way up to the\nuser space into the application to do you know, very little work on but then\nturn around and get that out in something less than a microsecond. Moving, you\nknow, context switching, things like that in the OS can start to be on the\norder of a microsecond or two.\n\ntext copied link copied\n\n## 49:00\n\n# Ron\n\nYeah, and I think a thing that\u2019s shocking and counterintuitive about that is\nthe quoted number for going through an L1 crosspoint switch versus going over\nthe PCI Express bus... we\u2019re talking 300 nanos to go across the PCI Express\nbus, and two orders of magnitude faster to go in and out of a crosspoint\nswitch.\n\ntext copied link copied\n\n## 49:19\n\n# Brian\n\nWell you gotta add the wires in, the wire starts to...\n\ntext copied link copied\n\n## 49:22\n\n# Ron\n\nOh, yeah, yeah, that\u2019s right.\n\ntext copied link copied\n\n## 49:23\n\n# Brian\n\nThe physical wiring starts to matter.\n\ntext copied link copied\n\n## 49:23\n\n# Ron\n\nThe wiring absolutely starts to matter. To go back to the mechanical sympathy\npoint, when you think about the machines, we\u2019re not just talking about the\ncomputers. We\u2019re also talking about the networking fabric and things like\nthat. I think an aspect of the performance of things that people often don\u2019t\nthink about is serialization delay. Can you explain what serialization delay\nis and how it plays into that story?\n\ntext copied link copied\n\n## 49:46\n\n# Brian\n\nWe\u2019ve been talking about networking at specific speeds. I can send one gigabit\nper second, I can send 10 gigabits per second, 25 gigabits per second, 40\ngigabits per second, etc. I can\u2019t take data in at 10 gigabit and send it out\nat 25 gigabit, I have to have the data continuously available. I have to\nbuffer enough and wait for enough to come in before I start sending because I\ncan\u2019t underflow, I can\u2019t run out of data to deliver. Similarly, if I\u2019m taking\ndata in a 10 gigabit and trying to send it out at one gigabit, I can\u2019t really\ndo this bit for bit. I\u2019ve kind of gotta queue some up, and I\u2019ve gotta wait.\nThe lowest latency is happening at the same speeds where you can do that and\nand certainly the L1 crosspoints are operating at such a low level, as far as\nI understand, that certainly no speed conversions are happening at the\nlatencies that I described.\n\ntext copied link copied\n\n## 50:36\n\n# Ron\n\nJust to kind of clarify the terminology, by serialization delay, you were\nmaking this point that, oh, yeah, when you\u2019re in at 10 gigabit and out at 25,\nwell, you can\u2019t pause or anything, right? You have to have all of the data\navailable at the high rate, which means you have to queue it up; when you send\nout a packet, it kind of has to be emitted in real time from beginning to end\nat a particular fixed rate. That means that there\u2019s a translation between how\nbig the packet is, and temporally how long it takes to get emitted onto the\nwire, right? There\u2019s a kind of electrically-determined space to time\nconversion that\u2019s there. It means if you have a store-and-forward switch, and\nyou have, say, a full, what\u2019s called an MTU \u2013 which is the maximum\ntransmission unit of an Ethernet switch, which is typically 1500 bytes-ish \u2013\nthat just takes a fixed amount of time. On a 10 gigabit network, what does\nthat translation look like?\n\ntext copied link copied\n\n## 51:31\n\n# Brian\n\nI think it roughly works out to something like a nanosecond per byte. I think\nthis comes back to the thing we were talking about in the beginning and a\nlittle bit of appreciation for multicast. So imagine I have 600 customers and\nI have one network card, and I would like to write a message to all 600. Well,\nlet\u2019s say the message is 1000 bytes. Okay, so that\u2019s about a microsecond\nper... so the last person in line is going to be 600 microseconds, at a\nminimum, behind the first person in line. Whereas with multicast, if I can\nsend one copy of that, and have the switch replicate that in parallel, with\none of these Layer 1 crosspoints, I\u2019m getting that to everybody, in something\nclose to a microsecond.\n\ntext copied link copied\n\n## 52:13\n\n# Ron\n\nThat affects latency, but it also affects throughput. If it takes you a half a\nmillisecond of wire time to just get the packets out the door, well, you could\ndo at most 2000 messages per second over that network card and that\u2019s that,\nright? Again, this goes back to... there are real physical limits imposed by\nthe hardware, that it can be as clever as you want, but there\u2019s just a limit\nto how much stuff you can emit over that one wire, and that\u2019s a hard\nconstraint that\u2019s worth understanding. Multicast is a story of \u201cthe technology\nthat could,\u201d it\u2019s incredibly successful in this niche. There\u2019s other\nnetworking technology that had a more complicated story and I\u2019m in particular\nthinking about things like InfiniBand and RoCE. What is RDMA? What is\nInfiniBand?\n\ntext copied link copied\n\n## 53:01\n\n# Brian\n\nInfiniBand is a networking technology that was very ahead of its time. I think\nit\u2019s still used in supercomputing areas and a lot of high-performance Ethernet\nhas begged borrowed and stolen ideas from InfiniBand. InfiniBand provided\nthings like reliable delivery at the hardware layer. They had APIs that\nallowed for zero-copy I/O. They had the concept of remote direct memory\naccess. So, direct memory access is something that peripherals, devices on\nyour computer, can use to sort of move memory around without involving the\nCPU. The CPU doesn\u2019t have to stop what it\u2019s doing, and copy a little bit over\nhere, from here to there, from here to there, the device itself can say \u201cokay,\nthat memory over there, I just want you to put this data right there.\u201d Remote\nDMA extends that concept and says \u201cI\u2019d like to take this data and I\u2019d like to\nput it on your machine over there in memory without your CPU being involved.\u201d\nThis is obviously powerful, but requires different APIs to interact with.\n\nA number of the places I\u2019ve been at used InfiniBand, some very much in\nproduction, some a little more experimentally. There are some bumps in the\nroad there: InfiniBand had some of this problem where, by default, it\nessentially had some flow control in hardware, meaning that it was concerned\nabout network bandwidth and could slow down the sender. So we\u2019d have servers\nthat didn\u2019t seem to be doing anything, but their network cards were sort of\noversubscribed, they had more multicast groups, then they could realistically\nfilter and so they were pushing back on the sender. So when we scaled it up to\nbig infrastructures, we\u2019d have market data slow down and it was very difficult\nto figure out why and to track down who was slowing that down. So the Ethernet\nmodel of like best-effort and sort of \u201cfail fast and throw things away\nquickly\u201d is in some cases a little bit easier to to get your head around and\nto debug.\n\ntext copied link copied\n\n## 54:54\n\n# Ron\n\nYou mentioned that when we talk about multicast one of the key issues of\nmulticast is it\u2019s not reliable. We don\u2019t worry about dealing with people who\ncan\u2019t keep up, right? People who can\u2019t keep up fall behind and have a separate\npath to recover, and that\u2019s that. You just mentioned that InfiniBand had a\nnotion of reliability, and reliability is a two-edged sword, right? The way\nyou make things reliable is in part by constraining what can be done, and so\nthe pushback on senders of data is kind of part and parcel of these\nreliability guarantees, I\u2019m assuming. Is that the right way of thinking about\nit?\n\ntext copied link copied\n\n## 55:39\n\n# Brian\n\nYeah, I think that\u2019s a good way to think about it. But certainly the the\nvisibility and the debugability could have been improved as well. And you\nmentioned RoCE, I never worked with it personally. But it was a way to sort of\nextend Ethernet to support the RDMA concept from InfiniBand. But I don\u2019t\nbelieve it... it involves some proprietary technology still, so it was a\nlittle bit of like the \u201cembrace and extend\u201d approach applied to Ethernet. So\nwhen you look at the kinds of custom hardware that was being developed, I\nthink there were sort of more interesting things happening in the commodity\nworld than RoCE.\n\ntext copied link copied\n\n## 56:13\n\n# Ron\n\nWe spent a lot of time talking about the value of customizing and doing just\nexactly the right thing and understanding the hardware. I guess the Ethernet\nversus InfiniBand story is in some sense about the value of not customizing of\nusing the commodity thing.\n\ntext copied link copied\n\n## 56:26\n\n# Brian\n\nThere is a strong lesson there. I mean, I had a couple of instances over my\ncareer where I was very surprised at the power of commodity technologies. I\nwas at a place that did telecommunications equipment, and they were doing\nspecial purpose devices for processing, phone calls, phone number recognition,\nwhat number did you press sorts of menus... and these had very special cards\nwith digital signal processors and algorithms to do all of this detection,\nsome basic voice recognition. This is in the 90s, and these were complex\ndevices. And it turned out that somebody in the research office in California\nbuilt a pure software version of the API that could use like a $14 card that\nwas sufficient to be able to generate ring voltages, and could emulate like\n80% of the product line in software. When I saw that, I was like, I\u2019m not\nreally sure I want to work on custom hardware. I don\u2019t know that I want to\nswim upstream against the relentless advance of x86 hardware and commodity\nvendors. Just the price/performance... you\u2019ve got a million people helping\nyou. Whereas in the other direction, you\u2019ve got basically yourself and it took\na lot to get me convinced to consider some alternative things.\n\nBut I do think that trends around the way processors and memory latency are\nimproving certainly make it clear that, just, you know, looking at things like\ndeep learning and GPUs, it\u2019s pretty clear that we\u2019re starting to see some\ngains from specializing again, even though I\u2019d say the first 10 or 15 years of\nmy career, it was pretty clear that commodity hardware was relentless.\n\ntext copied link copied\n\n## 58:17\n\n# Ron\n\nAnd it\u2019s worth saying, I think, in some sense, the question of what is\ncommodity hardware shifts over time: I think a standard joke in the networking\nworld is \u201calways bet on Ethernet.\u201d You have no idea what it is, but the thing\nthat\u2019s called Ethernet is the thing that\u2019s going to win. And I think that has\nplayed out over multiple generations of networking hardware, where it\u2019s, like\nyou said, stealing lots of ideas from other places, InfiniBand and whatever,\nbut, you know, there is like the chosen commodity thing and learning how to\nuse that, and how to identify what that thing is going to be is valuable.\n\nThe work that we\u2019re doing now in custom hardware is also still sensitive to\nthe fact that FPGAs themselves are a new kind of commodity hardware, but it\u2019s\nnot the case that we actually have to go out and actually get fabricated a big\ncollection of chips on one of those awesome reflective discs, we get to use a\ncertain kind of commodity hardware that lots of big manufacturers are actually\ngetting better and better at producing bigger and more powerful and easier to\nuse versions of these systems. Is there anything else that you see coming down\nthe line in the world of networking that you think is going to be increasingly\nrelevant and important, say over the next two to five years?\n\ntext copied link copied\n\n## 59:25\n\n# Brian\n\nI think what we\u2019re going to see is a little bit more of the things we\u2019ve been\ndoing already standardized and more common. So this sort of like userspace\npolling, and that form of I/O, I think you\u2019re seeing some of that start to hit\nLinux with io_uring. So these are very, very, very similar models to what\nwe\u2019ve been already doing with a lot of our own cards, but now they\u2019re going to\nbecome a bit more standardized, you\u2019re going to see more I/O devices meet that\ndesign, and then you\u2019re going to see more efficient, zero-copy, polling sorts\nof things come down the line. Some of the newer networking technologies like\n25 gigabit, I do think is going to have a decent amount of applicability; it\nis waiting for things like an L1 crosspoint. It\u2019s not always a clear net win;\nsome of the latency has gone up as we\u2019ve gone to these higher signaling rates.\nWith large quantities of data, the gain in serialization delay will overcome\nsome of the baseline latency as the data gets big enough, but, it\u2019s\ncomplicated.\n\ntext copied link copied\n\n## 1:00:30\n\n# Ron\n\nCan you say why it is? Why do switches that run at faster rates sometimes have\nhigher latency than switches that run at lower rates?\n\ntext copied link copied\n\n## 1:00:38\n\n# Brian\n\nPart of it is decisions by the vendor where they\u2019re sort of finding the right\nmarket for the mix of features and the sensitivity to latency. I do think that\nwe are at the mercy so to speak of some of the major buyers of hardware, which\nis probably cloud providers. That\u2019s just an enormous market. And so I do think\nthat the requirements hew a little closer to that than they do for our\nspecific industry, so we\u2019ve got to contend with that. As the signaling rates\ngo up, and again I\u2019m no expert here, but I think that you start to have to\nrely more on error correction, and forward error correction is built into 25\ngigabit and eats up a decent amount of latency if you have runs of any length.\nSo that\u2019s also a thing that we have to contend with and an added complexity.\n\nI think it\u2019s going to be important, I think it\u2019s going to be something that\ndoes come to our industry. And maybe quickly. I think at this point, there\u2019s a\ndecent amount of 25 gigabit outside of the finance industry, but not quite as\nmuch in the trading space. Once you start to see a little bit of it, you\u2019ll\nsee a lot very quickly.\n\ntext copied link copied\n\n## 1:01:43\n\n# Ron\n\nAll right, well, thanks a lot. This has been super fun. I\u2019ve really enjoyed\nwalking through some of the history and some of the low level details of how\nthis all works.\n\ntext copied link copied\n\n## 1:01:51\n\n# Brian\n\nI think we do this basically all the time, you and I, it\u2019s just kind of like\nnow we\u2019re doing it for somebody else!\n\ntext copied link copied\n\n## 1:02:00\n\n# Ron\n\nYou can find links to more information about some of the topics we discussed,\nas well as a full transcript of the episode on signalsandthreads.com. And\nwhile you\u2019re at it, please rate us and review us on Apple Podcasts. Thanks for\njoining us, and see you next week.\n\ntext copied link copied\n\n  * #### colocation\n\nPutting servers that run the exchange and the servers of market participants\nin the same datacenter. Usually, these machines will operate on the same,\nprivate, network. This helps reduce latency for network participants.\n\n  * #### exchange\n\nA place where orders to buy and sell stocks or other securities can be placed\nand executed.\n\n  * #### Instinet\n\nAn early electronic trading platform.\n\n  * #### Island\n\nAn early electronic equities exchange. Island was later acquired by Instinet.\n\n  * #### market data\n\nInformation about orders and executions, subscribed to by exchange\nparticipants so they can make trading decisions and satisfy regulatory\nreporting obligations.\n\n  * #### matching engine\n\nThe piece of an exchange that matches compatible buy and sell orders with each\nother and records that a trade has happened.\n\n  * #### multicast\n\nA network protocol that allows messages to be efficiently duplicated to all\nmembers of a group that have subscribed to that particular stream of messages.\n\n  * #### NASDAQ\n\nThe first electronic stock exchange. The NASDAQ is still one of the major US\nexchanges.\n\n  * #### network switch\n\nA hardware device that takes in packets and forwards them to the correct\nrecipients.\n\n  * #### order\n\nA message sent to an exchange offering to buy or sell a security, often\nincluding other parameters like a price and quantity.\n\n  * #### order book\n\nA data structure representing the open (active) orders on an exchange.\n\n  * #### UDP\n\nOr, User Datagram Protocol. A network protocol that sacrificies fault\ntolerance and congestion control for improved performance.\n\n# TECH TALKS\n\n## Watch Tech Talks\n\n# TECH BLOG\n\n## Read The Tech Blog\n\n# JANE STREET TECH\n\n## Learn More\n\n# JOIN OUR TEAM\n\n## Apply\n\n\u00a9 Copyright 2020-2024 Jane Street Group, LLC. All rights reserved.\n\nJane Street and the concentric circle mark are registered trademarks of Jane\nStreet.\n\nAd and Cookie Policy | Privacy Policy\n\nJane Street is an Equal Opportunity Employer\n\n", "frontpage": false}
