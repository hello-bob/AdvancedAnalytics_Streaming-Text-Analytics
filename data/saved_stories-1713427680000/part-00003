{"aid": "40069583", "title": "Why SQLite Performance Tuning Made Bencher 1200x Faster", "url": "https://bencher.dev/learn/engineering/sqlite-performance-tuning/", "domain": "bencher.dev", "votes": 2, "user": "homarp", "posted_at": "2024-04-17 20:15:16", "comments": 0, "source_title": "\ud83c\udfd7\ufe0f Why SQLite Performance Tuning made Bencher 1200x Faster", "source_text": "SQLite Performance Tuning | Bencher - Continuous Benchmarking\n\n  * Docs\n  * API\n  * Learn\n\n# Why SQLite Performance Tuning made Bencher 1200x Faster\n\nLast week, I got feedback from a user that their Bencher Perf Page was taking\na while to load. So I decided to check it out, and oh, man were they being\nnice. It took sooo long to load! Embarrassingly long. Especially for the\nleading Continuous Benchmarking tool.\n\nIn the past, I\u2019ve used the Rustls Perf Page as my litmus test. They have 112\nbenchmarks and one of the most impressive Continuous Benchmarking setups out\nthere. It used to take about 5 seconds load. This time around it took... \u23f3\ud83d\udc40\n... 38.8 seconds! With that sort of latency, I had to dig in. Performance bugs\nare bugs, after all!\n\nThe worst HTTP response time hit 2 minutes!\n\n## Background\n\nFrom the very start, I knew that the Bencher Perf API was going to be one of\nthe most demanding endpoints performance wise. I believe the main reason that\nso many folks have had to reinvent the benchmark tracking wheel is that the\nexisting off-the-shelf tools don\u2019t handle the high dimensionality required. By\n\u201chigh dimensionality\u201d, I mean being able to track performance over time and\nacross multiple dimensions: Branches, Testbeds, Benchmarks, and Measures. This\nability to slice and dice across five different dimensions leads to a very\ncomplex model.\n\nBecause of this inherent complexity and the nature of the data, I considered\nusing a time series database for Bencher. In the end though, I settled on\nusing SQLite instead. I figured it was better to do things that don\u2019t scale\nthan to spend the extra time learning an entirely new database architecture\nthat may or may not actually help.\n\nOver time, the demands on the Bencher Perf API have also increased.\nOriginally, you had to select all of the dimensions that you wanted to plot\nmanually. This created a lot of friction for users to get to a useful plot. To\nsolve this, I added a list of the most recent Reports to the Perf Pages, and\nby default, the most recent Report was selected and plotted. This means that\nif there were 112 benchmarks in the most recent Report, then all 112 would be\nplotted. The model also got even more complicated with the ability to track\nand visualize Threshold Boundaries.\n\nWith this in mind, I made a few performance related improvements. Since the\nPerf Plot needs the most recent Report to start plotting, I refactored the\nReports API to get a Report\u2019s result data in a single call to the database\ninstead of iterating. The time window for the default Report query was set to\nfour weeks, instead of being unbounded. I also drastically limited the scope\nof all database handles, reducing lock contention. To help communicate to\nusers, I added a status bar spinner for both the Perf Plot and the dimension\ntabs.\n\nI also had a failed attempt last fall at using a composite query to get all\nPerf results into a single query, instead of using a quadruple nested for\nloop. This lead to me hitting the Rust type system recursion limit, repeatedly\noverflowing the stack, suffering through insane (much longer than 38 seconds)\ncompile times, and finally dead ending at SQLite\u2019s max number of terms in a\ncompound select statement.\n\nWith all of that under my belt, I knew that I really needed to dig in here and\nput my performance engineer pants on. I had never profiled a SQLite database\nbefore, and honestly, I had never really profiled any database before. Now\nwait a minute you might might be thinking. My LinkedIn profile says I was a\n\u201cDatabase Administrator\u201d for almost two years. And I never profiled a\ndatabase\u203d Yep. That\u2019s a story for another time I suppose.\n\n## From ORM to SQL Query\n\nThe first hurdle I ran into was getting the SQL query out of my Rust code. I\nuse Diesel as the object\u2013relational mapper (ORM) for Bencher.\n\n> \ud83d\udc30 Fun Fact: Diesel uses Bencher for their Relative Continuous Benchmarking.\n> Check out the Diesel Perf Page!\n\nDiesel creates parameterized queries. It sends the SQL query and its bind\nparameters separately to the database. That is, the substitution is done by\nthe database. Therefore, Diesel cannot provide a complete query to the user.\nThe best method that I found was using the diesel::debug_query function to\noutput the parameterized query:\n\n    \n    \n    Query { sql: \"SELECT `branch`.`id`, `branch`.`uuid`, `branch`.`project_id`, `branch`.`name`, `branch`.`slug`, `branch`.`start_point_id`, `branch`.`created`, `branch`.`modified`, `testbed`.`id`, `testbed`.`uuid`, `testbed`.`project_id`, `testbed`.`name`, `testbed`.`slug`, `testbed`.`created`, `testbed`.`modified`, `benchmark`.`id`, `benchmark`.`uuid`, `benchmark`.`project_id`, `benchmark`.`name`, `benchmark`.`slug`, `benchmark`.`created`, `benchmark`.`modified`, `measure`.`id`, `measure`.`uuid`, `measure`.`project_id`, `measure`.`name`, `measure`.`slug`, `measure`.`units`, `measure`.`created`, `measure`.`modified`, `report`.`uuid`, `report_benchmark`.`iteration`, `report`.`start_time`, `report`.`end_time`, `version`.`number`, `version`.`hash`, `threshold`.`id`, `threshold`.`uuid`, `threshold`.`project_id`, `threshold`.`measure_id`, `threshold`.`branch_id`, `threshold`.`testbed_id`, `threshold`.`model_id`, `threshold`.`created`, `threshold`.`modified`, `model`.`id`, `model`.`uuid`, `model`.`threshold_id`, `model`.`test`, `model`.`min_sample_size`, `model`.`max_sample_size`, `model`.`window`, `model`.`lower_boundary`, `model`.`upper_boundary`, `model`.`created`, `model`.`replaced`, `boundary`.`id`, `boundary`.`uuid`, `boundary`.`threshold_id`, `boundary`.`model_id`, `boundary`.`metric_id`, `boundary`.`baseline`, `boundary`.`lower_limit`, `boundary`.`upper_limit`, `alert`.`id`, `alert`.`uuid`, `alert`.`boundary_id`, `alert`.`boundary_limit`, `alert`.`status`, `alert`.`modified`, `metric`.`id`, `metric`.`uuid`, `metric`.`report_benchmark_id`, `metric`.`measure_id`, `metric`.`value`, `metric`.`lower_value`, `metric`.`upper_value` FROM (((`metric` INNER JOIN ((`report_benchmark` INNER JOIN ((`report` INNER JOIN (`version` INNER JOIN (`branch_version` INNER JOIN `branch` ON (`branch_version`.`branch_id` = `branch`.`id`)) ON (`branch_version`.`version_id` = `version`.`id`)) ON (`report`.`version_id` = `version`.`id`)) INNER JOIN `testbed` ON (`report`.`testbed_id` = `testbed`.`id`)) ON (`report_benchmark`.`report_id` = `report`.`id`)) INNER JOIN `benchmark` ON (`report_benchmark`.`benchmark_id` = `benchmark`.`id`)) ON (`metric`.`report_benchmark_id` = `report_benchmark`.`id`)) INNER JOIN `measure` ON (`metric`.`measure_id` = `measure`.`id`)) LEFT OUTER JOIN (((`boundary` INNER JOIN `threshold` ON (`boundary`.`threshold_id` = `threshold`.`id`)) INNER JOIN `model` ON (`boundary`.`model_id` = `model`.`id`)) LEFT OUTER JOIN `alert` ON (`alert`.`boundary_id` = `boundary`.`id`)) ON (`boundary`.`metric_id` = `metric`.`id`)) WHERE ((((((`branch`.`uuid` = ?) AND (`testbed`.`uuid` = ?)) AND (`benchmark`.`uuid` = ?)) AND (`measure`.`uuid` = ?)) AND (`report`.`start_time` >= ?)) AND (`report`.`end_time` <= ?)) ORDER BY `version`.`number`, `report`.`start_time`, `report_benchmark`.`iteration`\", binds: [BranchUuid(a7d8366a-4f9b-452e-987e-2ae56e4bf4a3), TestbedUuid(5b4a6f3e-a27d-4cc3-a2ce-851dc6421e6e), BenchmarkUuid(88375e7c-f1e0-4cbb-bde1-bdb7773022ae), MeasureUuid(b2275bbc-2044-4f8e-aecd-3c739bd861b9), DateTime(2024-03-12T12:23:38Z), DateTime(2024-04-11T12:23:38Z)] }\n\nAnd then hand cleaning and parameterizing the query into valid SQL:\n\n    \n    \n    SELECT branch.id, branch.uuid, branch.project_id, branch.name, branch.slug, branch.start_point_id, branch.created, branch.modified, testbed.id, testbed.uuid, testbed.project_id, testbed.name, testbed.slug, testbed.created, testbed.modified, benchmark.id, benchmark.uuid, benchmark.project_id, benchmark.name, benchmark.slug, benchmark.created, benchmark.modified, measure.id, measure.uuid, measure.project_id, measure.name, measure.slug, measure.units, measure.created, measure.modified, report.uuid, report_benchmark.iteration, report.start_time, report.end_time, version.number, version.hash, threshold.id, threshold.uuid, threshold.project_id, threshold.measure_id, threshold.branch_id, threshold.testbed_id, threshold.model_id, threshold.created, threshold.modified, model.id, model.uuid, model.threshold_id, model.test, model.min_sample_size, model.max_sample_size, model.window, model.lower_boundary, model.upper_boundary, model.created, model.replaced, boundary.id, boundary.uuid, boundary.threshold_id, boundary.model_id, boundary.metric_id, boundary.baseline, boundary.lower_limit, boundary.upper_limit, alert.id, alert.uuid, alert.boundary_id, alert.boundary_limit, alert.status, alert.modified, metric.id, metric.uuid, metric.report_benchmark_id, metric.measure_id, metric.value, metric.lower_value, metric.upper_value FROM (((metric INNER JOIN ((report_benchmark INNER JOIN ((report INNER JOIN (version INNER JOIN (branch_version INNER JOIN branch ON (branch_version.branch_id = branch.id)) ON (branch_version.version_id = version.id)) ON (report.version_id = version.id)) INNER JOIN testbed ON (report.testbed_id = testbed.id)) ON (report_benchmark.report_id = report.id)) INNER JOIN benchmark ON (report_benchmark.benchmark_id = benchmark.id)) ON (metric.report_benchmark_id = report_benchmark.id)) INNER JOIN measure ON (metric.measure_id = measure.id)) LEFT OUTER JOIN (((boundary INNER JOIN threshold ON (boundary.threshold_id = threshold.id)) INNER JOIN model ON (boundary.model_id = model.id)) LEFT OUTER JOIN alert ON (alert.boundary_id = boundary.id)) ON (boundary.metric_id = metric.id)) WHERE ((((((branch.uuid = 'a7d8366a-4f9b-452e-987e-2ae56e4bf4a3') AND (testbed.uuid = '5b4a6f3e-a27d-4cc3-a2ce-851dc6421e6e')) AND (benchmark.uuid = '88375e7c-f1e0-4cbb-bde1-bdb7773022ae')) AND (measure.uuid = 'b2275bbc-2044-4f8e-aecd-3c739bd861b9')) AND (report.start_time >= 0)) AND (report.end_time <= 1712838648197)) ORDER BY version.number, report.start_time, report_benchmark.iteration;\n\nIf you know of a better way, please let me know! This is the way that the\nmaintainer of the project suggested though, so I just went with it. Now that I\nhad a SQL query, I was finally ready to... read a whole lot of documentation.\n\n## SQLite Query Planner\n\nThe SQLite website has great documentation for its Query Planner. It explains\nexactly how SQLite goes about executing your SQL query, and it teaches you\nwhich indexes are useful and what operations to look out for, like full table\nscans.\n\nIn order to see how the Query Planner would execute my Perf query, I needed to\nadd a new tool to my tool belt: EXPLAIN QUERY PLAN You can either prefix your\nSQL query with EXPLAIN QUERY PLAN or run the .eqp on dot command before your\nquery. Either way, I got a result that looks like this:\n\n    \n    \n    QUERY PLAN\n    \n    |--MATERIALIZE (join-5)\n    \n    | |--SCAN boundary\n    \n    | |--SEARCH threshold USING INTEGER PRIMARY KEY (rowid=?)\n    \n    | |--SEARCH model USING INTEGER PRIMARY KEY (rowid=?)\n    \n    | |--BLOOM FILTER ON alert (boundary_id=?)\n    \n    | `--SEARCH alert USING AUTOMATIC COVERING INDEX (boundary_id=?) LEFT-JOIN\n    \n    |--SEARCH branch USING INDEX sqlite_autoindex_branch_1 (uuid=?)\n    \n    |--SEARCH measure USING INDEX sqlite_autoindex_measure_1 (uuid=?)\n    \n    |--SEARCH benchmark USING INDEX sqlite_autoindex_benchmark_1 (uuid=?)\n    \n    |--SEARCH testbed USING INDEX sqlite_autoindex_testbed_1 (uuid=?)\n    \n    |--SCAN metric\n    \n    |--SEARCH report_benchmark USING INTEGER PRIMARY KEY (rowid=?)\n    \n    |--SEARCH report USING INTEGER PRIMARY KEY (rowid=?)\n    \n    |--SEARCH version USING INTEGER PRIMARY KEY (rowid=?)\n    \n    |--SEARCH branch_version USING COVERING INDEX sqlite_autoindex_branch_version_1 (branch_id=? AND version_id=?)\n    \n    |--BLOOM FILTER ON (join-5) (metric_id=?)\n    \n    |--SEARCH (join-5) USING AUTOMATIC COVERING INDEX (metric_id=?) LEFT-JOIN\n    \n    `--USE TEMP B-TREE FOR ORDER BY\n\nOh, boy! There is a lot here. But the three big things that jumped out to me\nwhere:\n\n  1. SQLite is creating a materialized view on-the-fly that scans the entire boundary table\n  2. SQLite is then scanning the entire metric table\n  3. SQLite is creating two on the fly indexes\n\nAnd just how big are the metric and boundary tables? Well they just so happen\nto be the two largest tables, as they are where all the Metrics and Boundaries\nare stored.\n\nSince this was my first SQLite performance tuning rodeo, I wanted to consult\nan expert before making any changes.\n\n# SQLite Expert\n\nSQLite has an experimental \u201cexpert\u201d mode that can be enabled with the .expert\non command. It suggests indexes for queries, so I decided to give it a try.\nThis is what it suggested:\n\n    \n    \n    CREATE INDEX report_benchmark_idx_fc6f3e5b ON report_benchmark(report_id, benchmark_id);\n    \n    CREATE INDEX report_idx_55aae6d8 ON report(testbed_id, end_time);\n    \n    CREATE INDEX alert_idx_e1882f70 ON alert(boundary_id);\n    \n    MATERIALIZE (join-5)\n    \n    SCAN boundary\n    \n    SEARCH threshold USING INTEGER PRIMARY KEY (rowid=?)\n    \n    SEARCH model USING INTEGER PRIMARY KEY (rowid=?)\n    \n    SEARCH alert USING INDEX alert_idx_e1882f70 (boundary_id=?) LEFT-JOIN\n    \n    SEARCH branch USING INDEX sqlite_autoindex_branch_1 (uuid=?)\n    \n    SEARCH benchmark USING INDEX sqlite_autoindex_benchmark_1 (uuid=?)\n    \n    SEARCH testbed USING INDEX sqlite_autoindex_testbed_1 (uuid=?)\n    \n    SEARCH measure USING INDEX sqlite_autoindex_measure_1 (uuid=?)\n    \n    SEARCH report USING INDEX report_idx_55aae6d8 (testbed_id=? AND end_time<?)\n    \n    SEARCH version USING INTEGER PRIMARY KEY (rowid=?)\n    \n    SEARCH branch_version USING COVERING INDEX sqlite_autoindex_branch_version_1 (branch_id=? AND version_id=?)\n    \n    SEARCH report_benchmark USING INDEX report_benchmark_idx_fc6f3e5b (report_id=? AND benchmark_id=?)\n    \n    SEARCH metric USING INDEX sqlite_autoindex_metric_2 (report_benchmark_id=? AND measure_id=?)\n    \n    BLOOM FILTER ON (join-5) (metric_id=?)\n    \n    SEARCH (join-5) USING AUTOMATIC COVERING INDEX (metric_id=?) LEFT-JOIN\n    \n    USE TEMP B-TREE FOR ORDER BY\n\nThis is definitely an improvement! It got rid of the scan on the metric table\nand both of the on-the-fly indexes. Honestly, I wouldn\u2019t have gotten the first\ntwo indexes on my own. Thank you, SQLite Expert!\n\n    \n    \n    CREATE INDEX index_report_testbed_end_time ON report(testbed_id, end_time);\n    \n    CREATE INDEX index_report_benchmark ON report_benchmark(report_id, benchmark_id);\n    \n    CREATE INDEX index_alert_boundary ON alert(boundary_id);\n\nNow the only thing left to get rid of is that darn on-the-fly materialized\nview.\n\n## Materialized View\n\nWhen I added the ability to track and visualize Threshold Boundaries last\nyear, I had a decision to make in the database model. There is a 1-to-0/1\nrelationship between a Metric and its corresponding Boundary. That is a Metric\ncan relate to zero or one Boundary, and a Boundary can only ever relate to one\nMetric. So I could have just expanded the metric table to include all of the\nboundary data with every boundary related field being nullable. Or I could\ncreate a separate boundary table with a UNIQUE foreign key to metric table. To\nme the latter option felt a lot cleaner, and I figured I could always deal\nwith any performance implications later.\n\nThese were the effective queries used to create the metric and boundary\ntables:\n\n    \n    \n    CREATE TABLE metric (\n    \n    id INTEGER PRIMARY KEY NOT NULL,\n    \n    uuid TEXT NOT NULL UNIQUE,\n    \n    report_benchmark_id INTEGER NOT NULL,\n    \n    measure_id INTEGER NOT NULL,\n    \n    value DOUBLE NOT NULL,\n    \n    lower_value DOUBLE,\n    \n    upper_value DOUBLE,\n    \n    FOREIGN KEY (report_benchmark_id) REFERENCES report_benchmark (id) ON DELETE CASCADE,\n    \n    FOREIGN KEY (measure_id) REFERENCES measure (id),\n    \n    UNIQUE(report_benchmark_id, measure_id)\n    \n    );\n    \n    \n    CREATE TABLE boundary (\n    \n    id INTEGER PRIMARY KEY NOT NULL,\n    \n    uuid TEXT NOT NULL UNIQUE,\n    \n    threshold_id INTEGER NOT NULL,\n    \n    statistic_id INTEGER NOT NULL,\n    \n    metric_id INTEGER NOT NULL UNIQUE,\n    \n    baseline DOUBLE NOT NULL,\n    \n    lower_limit DOUBLE,\n    \n    upper_limit DOUBLE,\n    \n    FOREIGN KEY (threshold_id) REFERENCES threshold (id),\n    \n    FOREIGN KEY (statistic_id) REFERENCES statistic (id),\n    \n    FOREIGN KEY (metric_id) REFERENCES metric (id) ON DELETE CASCADE\n    \n    );\n\nAnd it turns out \u201clater\u201d had arrived. I tried to simply add an index for\nboundary(metric_id) but that did not help. I believe the reason has to do with\nthe fact that the Perf query is originating from the metric table and because\nthat relationship is 0/1 or put another way, nullable it has to be scanned\n(O(n)) and cannot be searched (O(log(n))).\n\nThis left me with one clear option. I needed to create a materialized view\nthat flattened the metric and boundary relationship to keep SQLite from having\nto create an on-the-fly materialized view.\n\nThis is the query I used to create the new metric_boundary materialized view:\n\n    \n    \n    CREATE VIEW metric_boundary AS\n    \n    SELECT metric.id AS metric_id,\n    \n    metric.uuid AS metric_uuid,\n    \n    metric.report_benchmark_id,\n    \n    metric.measure_id,\n    \n    metric.value,\n    \n    metric.lower_value,\n    \n    metric.upper_value,\n    \n    boundary.id,\n    \n    boundary.uuid AS boundary_uuid,\n    \n    boundary.threshold_id AS threshold_id,\n    \n    boundary.model_id,\n    \n    boundary.baseline,\n    \n    boundary.lower_limit,\n    \n    boundary.upper_limit\n    \n    FROM metric\n    \n    LEFT OUTER JOIN boundary ON (boundary.metric_id = metric.id);\n\nWith this solution, I\u2019m trading off space for runtime performance. How much\nspace? Surprisingly only about a 4% increase, even though this view is for the\ntwo largest tables in the database. Best of all, it lets me have my cake and\neat it too in my source code.\n\nCreating a materialized view with Diesel was surprisingly easy. I just had to\nuse the exact same macros that Diesel uses when generating my normal schema.\nWith that said, I learned to appreciate Diesel a lot more throughout this\nexperience. See Bonus Bug for all the juicy details.\n\n## Wrap Up\n\nWith the three new indexes and a materialized view added, this is what the\nQuery Planner now shows:\n\n    \n    \n    QUERY PLAN\n    \n    |--SEARCH branch USING INDEX sqlite_autoindex_branch_1 (uuid=?)\n    \n    |--SEARCH testbed USING INDEX sqlite_autoindex_testbed_1 (uuid=?)\n    \n    |--SEARCH benchmark USING INDEX sqlite_autoindex_benchmark_1 (uuid=?)\n    \n    |--SEARCH measure USING INDEX sqlite_autoindex_measure_1 (uuid=?)\n    \n    |--SEARCH report USING INDEX index_report_testbed_end_time (testbed_id=? AND end_time<?)\n    \n    |--SEARCH version USING INTEGER PRIMARY KEY (rowid=?)\n    \n    |--SEARCH branch_version USING COVERING INDEX sqlite_autoindex_branch_version_1 (branch_id=? AND version_id=?)\n    \n    |--SEARCH report_benchmark USING INDEX index_report_benchmark (report_id=? AND benchmark_id=?)\n    \n    |--SEARCH metric USING INDEX sqlite_autoindex_metric_2 (report_benchmark_id=? AND measure_id=?)\n    \n    |--SEARCH boundary USING INDEX sqlite_autoindex_boundary_2 (metric_id=?) LEFT-JOIN\n    \n    |--SEARCH threshold USING INTEGER PRIMARY KEY (rowid=?) LEFT-JOIN\n    \n    |--SEARCH model USING INTEGER PRIMARY KEY (rowid=?) LEFT-JOIN\n    \n    |--SEARCH alert USING INDEX index_alert_boundary (boundary_id=?) LEFT-JOIN\n    \n    `--USE TEMP B-TREE FOR ORDER BY\n\nLook at all of those beautify SEARCHes all with existing indexes! \ud83e\udd72\n\nAnd after deploying my changes to production:\n\nThe new HTTP response times don\u2019t even register on the old scale!\n\nNow it was time for the final test. How fast does that Rustls Perf page load?\n\nHere I\u2019ll even give you anchor tag. Click it and then refresh the page.\n\n### Performance Matters\n\n## Bencher: Continuous Benchmarking\n\nBencher is a suite of continuous benchmarking tools. Have you ever had a\nperformance regression impact your users? Bencher could have prevented that\nfrom happening. Bencher allows you to detect and prevent performance\nregressions before they make it to production.\n\n  * Run: Run your benchmarks locally or in CI using your favorite benchmarking tools. The bencher CLI simply wraps your existing benchmark harness and stores its results.\n  * Track: Track the results of your benchmarks over time. Monitor, query, and graph the results using the Bencher web console based on the source branch, testbed, and measure.\n  * Catch: Catch performance regressions in CI. Bencher uses state of the art, customizable analytics to detect performance regressions before they make it to production.\n\nFor the same reasons that unit tests are run in CI to prevent feature\nregressions, benchmarks should be run in CI with Bencher to prevent\nperformance regressions. Performance bugs are bugs!\n\nStart catching performance regressions in CI \u2014 try Bencher Cloud for free.\n\n## Addendum on Dogfooding\n\nI\u2019m already dogfooding Bencher with Bencher, but all of the existing benchmark\nharness adapters are for micro-benchmarking harnesses. Most HTTP harnesses are\nreally load testing harnesses, and load testing is different than\nbenchmarking. Further, I\u2019m not looking to expand Bencher into load testing\nanytime soon. That is a very different use case that would require very\ndifferent design considerations, like that time series database for instance.\nEven if I did have load testing in place, I would really need to be running\nagainst a fresh pull of production data for this to have been caught. The\nperformance differences for these changes were negligible with my test\ndatabase.\n\nAll of this leads me to believe that I should create a micro-benchmark that\nruns against the Perf API endpoint and dogfood the results with Bencher. This\nwill require a sizable test database to make sure that these sort of\nperformance regressions get caught in CI. I have created a tracking issue for\nthis work, if you would like to follow along.\n\nThis has all got me thinking though: What if you could do snapshot testing of\nyour SQL database query plan? That is, you could compare your current vs\ncandidate SQL database query plans. SQL query plan testing would sort of be\nlike instruction count based benchmarking for databases. The query plan helps\nto indicate that there may be an issue with the runtime performance, without\nhaving to actually benchmark the database query. I have created a tracking\nissue for this as well. Please, feel free to add a comment with thoughts or\nany prior art that you are aware of!\n\n## Bonus Bug\n\nI originally had a bug in my materialized view code. This is what the SQL\nquery looked like:\n\n    \n    \n    SELECT branch.id, branch.uuid, branch.project_id, branch.name, branch.slug, branch.start_point_id, branch.created, branch.modified, testbed.id, testbed.uuid, testbed.project_id, testbed.name, testbed.slug, testbed.created, testbed.modified, benchmark.id, benchmark.uuid, benchmark.project_id, benchmark.name, benchmark.slug, benchmark.created, benchmark.modified, measure.id, measure.uuid, measure.project_id, measure.name, measure.slug, measure.units, measure.created, measure.modified, report.uuid, report_benchmark.iteration, report.start_time, report.end_time, version.number, version.hash, threshold.id, threshold.uuid, threshold.project_id, threshold.measure_id, threshold.branch_id, threshold.testbed_id, threshold.model_id, threshold.created, threshold.modified, model.id, model.uuid, model.threshold_id, model.test, model.min_sample_size, model.max_sample_size, model.window, model.lower_boundary, model.upper_boundary, model.created, model.replaced, alert.id, alert.uuid, alert.boundary_id, alert.boundary_limit, alert.status, alert.modified, metric_boundary.metric_id, metric_boundary.metric_uuid, metric_boundary.report_benchmark_id, metric_boundary.measure_id, metric_boundary.value, metric_boundary.lower_value, metric_boundary.upper_value, metric_boundary.boundary_id, metric_boundary.boundary_uuid, metric_boundary.threshold_id, metric_boundary.model_id, metric_boundary.baseline, metric_boundary.lower_limit, metric_boundary.upper_limit FROM (((((metric_boundary INNER JOIN ((report_benchmark INNER JOIN ((report INNER JOIN (version INNER JOIN (branch_version INNER JOIN branch ON (branch_version.branch_id = branch.id)) ON (branch_version.version_id = version.id)) ON (report.version_id = version.id)) INNER JOIN testbed ON (report.testbed_id = testbed.id)) ON (report_benchmark.report_id = report.id)) INNER JOIN benchmark ON (report_benchmark.benchmark_id = benchmark.id)) ON (metric_boundary.report_benchmark_id = report_benchmark.id)) INNER JOIN measure ON (metric_boundary.measure_id = measure.id)) LEFT OUTER JOIN threshold ON (metric_boundary.threshold_id = threshold.id)) LEFT OUTER JOIN model ON (metric_boundary.model_id = model.id)) LEFT OUTER JOIN alert ON (alert.boundary_id = metric_boundary.metric_id)) WHERE ((((((branch.uuid = 'a7d8366a-4f9b-452e-987e-2ae56e4bf4a3') AND (testbed.uuid = '5b4a6f3e-a27d-4cc3-a2ce-851dc6421e6e')) AND (benchmark.uuid = '88375e7c-f1e0-4cbb-bde1-bdb7773022ae')) AND (measure.uuid = 'b2275bbc-2044-4f8e-aecd-3c739bd861b9')) AND (report.start_time >= 0)) AND (report.end_time <= 1712838648197)) ORDER BY version.number, report.start_time, report_benchmark.iteration;\n\nDo you see the problem? Nope. Neither did I!\n\nThe issue is right here:\n\n    \n    \n    LEFT OUTER JOIN alert ON (alert.boundary_id = metric_boundary.metric_id)\n\nIt should actually be:\n\n    \n    \n    LEFT OUTER JOIN alert ON (alert.boundary_id = metric_boundary.boundary_id)\n\nI was trying to be too clever, and in my Diesel materialized view schema I had\nallowed this join:\n\n    \n    \n    diesel::joinable!(alert -> metric_boundary (boundary_id));\n\nI assumed that this macro was somehow smart enough to relate the\nalert.boundary_id to the metric_boundary.boundary_id. But alas, it was not. It\nseems to have just picked the first column of metric_boundary (metric_id) to\nrelate to alert.\n\nOnce I discovered the bug, it was easy to fix. I just had to use an explicit\njoin in the Perf query:\n\n    \n    \n    .left_join(schema::alert::table.on(view::metric_boundary::boundary_id.eq(schema::alert::boundary_id.nullable())))\n\n> \ud83d\udc30 That\u2019s all folks!\n\nPublished: Wed, April 17, 2024 at 10:11:00 AM UTC\n\n##### Continuous Benchmarking\n\n\u00a9 2024 Bencher\n\n", "frontpage": false}
