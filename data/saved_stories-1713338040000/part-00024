{"aid": "40056774", "title": "Vector Database are the new hype you want to avoid as a CTO", "url": "https://gen-ai.fr/en/large-language-models/vector-databases-chronicle-foretold-death/", "domain": "gen-ai.fr", "votes": 1, "user": "aschen", "posted_at": "2024-04-16 20:29:43", "comments": 0, "source_title": "Vector databases: chronicle of a foretold death - Generative AI France", "source_text": "Vector databases: chronicle of a foretold death - Generative AI France\n\ngen-ai.fr\n\n  * Home\n\n  * Large-Language-Models\n\n  * Vector databases: chronicle of a foretold death\n\n12 April 2024\n\nVector databases: chronicle of a foretold death\n\nFor several months now, we have heard a lot about these new \"Vector Databases\"\nwhich would be the \"memory\" of Large-Language-Models.\n\nPinecone ($128M), Qdrant ($28M), Croma ($18M), there are several dozen\nstartups raising millions and fighting in the hypothetical vector search\nmarket.\n\nIn this article, we'll look at how vector databases are more of a feature than\na full-fledged product and why they're already outdated.\n\n## What is vector search?\n\nTransformer type AI models used by LLMs (but also for the generation of\nimages, sounds, etc.) have an internal representation of the semantic field of\na text in the form of a vector.\n\nThis semantic field represents the \"meaning\" of the text and it is represented\nin the form of an N-dimensional vector which is also called Embedding.\n\nFor example, for OpenAI's text-embedding-3-small model, the vector has 1536\ndimensions. It is therefore an array of 1536 numbers between -1 and 1.\n\nWhat is practical about vectors is that we know how to manipulate them very\nwell. Most of the time, we manipulate vectors in 2 or 3 dimensions but\nmathematical operations can also be applied to vectors of 1536 dimensions.\n\nFor example, there are methods for calculating the distance between two\nvectors. In the case of an embedding from an LLM, this will indicate the\nproximity of the semantic fields.\n\nExtrapolation of a 2D representation of an LLM embedding\n\nIf we summarize, vector search is quite simply semantic search.\n\nWell, on the other hand, we have to be honest, it is by far the best semantic\nsearch technique that we have had until now, but ultimately it simply remains\nthe last iteration of this research field.\n\n## Vector Search and LLMs usecase\n\nOne of the most common uses of LLMs is Retrieval Augmented Generation, which\nconsists of overcoming the main drawback of LLMs, that is to say a prohibitive\nre-training cost on recent or private data.\n\nAppropriate re-training would allow the model to ingest new knowledge but the\nGPU time cost forces most actors to use a RAG pattern instead.\n\nThe first step of RAG consists of retrieving documents likely to contain the\nanswer to a question asked by a user.\n\nThis is where vector search comes in. This type of semantic search will make\nit possible to find relevant documents in a documentary database such as the\ninternal documentation of a company for example.\n\n(See also A RAG with the HyDE method: Integration of hypothetical documents)\n\nOnce the documents relevant to a question are retrieved, they will be injected\ninto a prompt with the question to allow the LLM to provide a specific answer\nin natural language.\n\n## Building a real search engine\n\nVector search offers good results for retrieving relevant documents but it is\nfar from sufficient\n\nIn the real world, documents also have metadata that will influence the\nretrieval process.\n\nExample: title, author, permission group, source, document type, etc.\n\n### Permissions and filtering\n\nThe first concrete example is permission management. In most companies, access\nto information is compartmentalized for reasons of confidentiality and\nsecurity.\n\nIt is then unthinkable that the RAG could happily dig into confidential\ndocuments reserved for C levels to answer a question asked by a trainee.\n\nA filtering step is then essential to exclude documents to which we do not\nhave access from the results and prevent them from ending up in the response\ngeneration prompt.\n\nThe vector databases on the market have understood this well and we see this\ntype of functionality appearing at Pinecone and Qdrant for example.\n\n### Scoring\n\nThose who have already had to build a search engine know that not all search\nconditions are equal.\n\nIn an advanced example, we could decide to store the document title embeddings\nin addition to those representing the content.\n\nExample of a question: How to use Didask's educational AI?\n\nLet's then imagine two documents\n\n  * Create granules with Didask AI: a document explaining how educational AI works\n\n  * Comparison of educational AI: a document citing the different educational AI on the market as well as that of Didask\n\nThe content of both documents will be semantically close to the question but\nif we consider the semantic content of the titles, then that of document 1 is\nmuch closer.\n\nIn a real search engine, documents each have a score assigned based on the\nquery and one can naturally configure a greater importance for the title than\nfor the content and adjust the score accordingly.\n\nFor example in Elasticsearch this is done using the boost keyword which allows\nthe score to be weighted according to a sub-part of the query as in our\nexample.\n\n### Query Language\n\nBeyond vector search, we may want to apply scoring functions for a whole range\nof use cases.\n\nFor example, it is possible to assign a higher score to more recent documents\nor to documents that have been referenced many times within other documents.\n\nIn addition to vector search, we can also use simpler techniques such as\nkeyword search to once again give an additional boost to documents containing\nthe desired keywords.\n\nFinally, the combination of operators with \"AND\" and \"OR\" is necessary to\nconstruct complete queries.\n\n## Search Engine at scale with Elasticsearch\n\nWhen we talk about search engines, it is difficult to ignore the undisputed\nleader in the field: Elasticsearch.\n\nLike all vector databases, Elasticsearch supports vector type fields and\nproximity queries (knn or nearest neighbors) allowing the adjacent semantic\nfields of LLM embedding to be highlighted.\n\nElasticsearch supports scoring and also a whole range of queries.\n\n### Performances\n\nRegarding performance, specialized vector databases are not even necessarily\nthe most efficient.\n\nIn a benchmark of the vector search functionality of Postgres vs Pinecone,\nPostgres significantly outperforms Pinecone for an equivalent infrastructure\ncost.\n\nFor its part, Elasticsearch is a horizontal scalability model with clusters\nreaching up to 600 Petabytes and several million requests per day in some\ncases.\n\n### Industry standard\n\nChoosing a product to include in your infrastructure is not a decision to take\nlightly.\n\nSeveral factors must be taken into account, such as the richness of the\necosystem in terms of SDKs, documentation and host offering the product off\nthe shelf or the sustainability of the company developing the product.\n\nToday, startups building their entire business model on a simple functionality\nare not sure that they will still exist in 5 years while your business will\nstill exist (at least I hope so).\n\n## Conclusion\n\nVector databases are built around the sole functionality of semantic search,\nwhereas the creation of a search engine requires going much further.\n\nAlmost all databases on the market already have their semantic search\nfunctionality built-in:\n\n  * Elasticsearch with knn\n\n  * Postgres with pg_vector\n\n  * Mongo with Atlas Vector Search (only available in the cloud)\n\nMoreover, the real challenges of a database often lie elsewhere than in a\nsimple vector search.\n\nQuery planner, cluster scalability, data integrity, the real challenges are\nstill to be met for new vector databases while existing databases have long\nproven themselves\n\nIn a future series of articles, I will explain how to use Elasticsearch and\nGPT4 to create a real knowledge engine, stay tuned \ud83d\ude09\n\nPr\u00e9c\u00e9dent\n\nThe real revolution brought by Google\u2019s Gemini models\n\nTAGS:\n\nAdrien Maret\n\n### Leave a Reply Cancel reply\n\nDERNIERS POSTS\n\nVector databases: chronicle of a foretold death\n\nLire\n\nBases de donn\u00e9es vectorielles: chronique d\u2019une mort annonc\u00e9e\n\nLire\n\nLe futur du m\u00e9tier de Copywriter avec l\u2019IA\n\nLire\n\nCATEGORIES\n\nTAGS\n\nGenAI est un site fran\u00e7ais \u00e0 propos des avanc\u00e9es technologiques dans le monde\nde l'intelligence artificielle.\n\nNewsletter\n\n\u00a9 Copyright CC-BY-SA 2023 gen-ai.fr\n\nClose menu\n\n", "frontpage": false}
