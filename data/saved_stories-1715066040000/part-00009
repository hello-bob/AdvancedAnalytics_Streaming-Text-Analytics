{"aid": "40279774", "title": "Using DuckDB to seamlessly query a large parquet file over HTTP", "url": "https://www.marginalia.nu/log/a_105_duckdb_parquet/", "domain": "marginalia.nu", "votes": 6, "user": "latexr", "posted_at": "2024-05-06 21:36:01", "comments": 3, "source_title": "Using DuckDB to seamlessly query a large parquet file over HTTP", "source_text": "Using DuckDB to seamlessly query a large parquet file over HTTP @\nmarginalia.nu\n\n# Using DuckDB to seamlessly query a large parquet file over HTTP\n\nPosted: 2024-05-05\n\nA neat property of the parquet file format is that it\u2019s designed with block\nI/O in mind, so that when you are interested in only parts of the contents of\na file, it\u2019s possible to some extent to only read that data. Many tools are\naware of this property, and DuckDB is one of them. Depending on which circles\nyou run in, a lesser known aspect of HTTP is range requests, where you specify\nwhich bytes in a file to be retrieved. It\u2019s possible to combine this trio of\nproperties to read remote parquet files directly in DuckDB.\n\nI\u2019ve had these facts things in the back of my head for a while, but saw the\nopportunity to put them in to practice recently when extending the admin GUI\nfor marginalia search to add the option to inspect remote crawl data.\n\nNormally I\u2019d ssh into the server and use DuckDB to interrogate the relevant\nparquet files, but this is a bit unwieldy. At the same time I wasn\u2019t\nparticularly keen on building an internal API for pushing the crawl data to\nthe admin GUI. That sort of plumbing is just annoying to maintain and makes\nthe system push back against future change efforts, it\u2019s bad enough the data\nneeds modelling to be presented in the GUI itself.\n\nThere was already an HTTP endpoint for moving files between services, and\namending it to understand range requests turned out to be very quick.\n\nA longer outline of what the range header does is available on MDN, but in\nbrief, an HTTP server can announce support for range requests by sending the\nresponse header \u2018Accept-Ranges: bytes\u2019. When this is seen by the client, it\nmay send a request header \u2018Range: bytes=nn-mm\u2019, and then the server should\nrespond a HTTP 206 and only send those octets. Only major caveat in\nimplementing this is that the range is inclusive.\n\nWith that in place, it\u2019s possible to query the URL in DuckDB as though it was\na database table:\n\n    \n    \n    select count(*) from 'http://remote-server/foo'\n\nYou can do this with other formats than parquet, but in many of those cases it\nwill need to download the entire file in order to perform the query.\n\nIn practice, DuckDB seems to probe the endpoint with a HEAD first, presumably\nto sniff out the Accept-Ranges header, and then it proceed with a series of\nrange requests to fetch the data itself.\n\nDuckDB integrates very easily into many programming languages. In Java you can\nuse JDBC to query it like any SQL data source, python has a very good\nintegration as well.\n\nWhile I wouldn\u2019t recommend using this method to build anything user-facing,\nit\u2019s definitely a bit slow where doing larger queries on parquet files in the\n100MB+ range may take a few hundred milliseconds, it does a remarkable job for\nthis particular task.\n\nPart of why I wanted to share this is that it\u2019s pretty rare that technologies\nclick together this effortlessly. DuckDB in general keeps surprising me with\nthese unobtrusive low friction integrations.\n\nIt\u2019s in stark contrast to the other ways of reading parquet files in Java,\nwhich typically requires you to pull in half the Hadoop ecosystem before the\ncode will even compile.\n\nSomeone built a very limited compatibility layer called \u2018parquet-floor\u2019 that\ncuts most of the Hadoop ties, and but it\u2019s very limited. I\u2019ve been slowly\nextending it with new capabilities, but it\u2019s still a very high friction\nenvironment.\n\nThe most reliable way I know of making ChatGPT look incompetent is to ask it\nhow to read a parquet file in Java without Hadoop. This is arguably mostly a\nreflection on the quality of information available regarding this task.\n\nThis isn\u2019t to fling crap on the Hadoop people, whom absolutely nobody has put\nin charge of maintaining universal parquet support for Java, even though\ntheirs seems to have become the de facto default implementation somehow.\n\nmarginalia.nu \u00a9 2024 Viktor Lofgren <kontakt@marginalia.nu>\n\nConsider donating to support the development effort of Marginalia Search and\nthe other services!\n\n", "frontpage": true}
