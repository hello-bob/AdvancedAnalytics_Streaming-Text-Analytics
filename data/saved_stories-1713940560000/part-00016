{"aid": "40136306", "title": "Python Big O: Time complexities of different data structures in Python", "url": "https://www.pythonmorsels.com/time-complexities/", "domain": "pythonmorsels.com", "votes": 15, "user": "rbanffy", "posted_at": "2024-04-23 19:41:38", "comments": 2, "source_title": "Python Big O: the time complexities of different data structures in Python", "source_text": "Python Big O: the time complexities of different data structures in Python -\nPython Morsels\n\n# Python Big O: the time complexities of different data structures in Python\n\nTrey Hunner 9 min. read \u2022 Python 3.8\u20143.12 \u2022 April 16, 2024\n\nShare\n\nCopied https://pym.dev/time-complexities/ to clipboard.\n\nLet's look at the time complexity of different Python data structures and\nalgorithms.\n\nThis article is primarily meant to act as a Python time complexity cheat sheet\nfor those who already understand what time complexity is and how the time\ncomplexity of an operation might affect your code. For a more thorough\nexplanation of time complexity see Ned Batchelder's article/talk on this\nsubject.\n\n## Time Complexity \u23f1\ufe0f\n\nTime complexity is one of those Computer Science concepts that's scary in its\npurest form, but often fairly practical as a rough \"am I doing this right\"\nmeasurement.\n\nIn the words of Ned Batchelder, time complexity is all about \"how your code\nslows as your data grows\".\n\nTime complexity is usually discussed in terms of \"Big O\" notation. This is\nbasically a way to discuss the order of magnitude for a given operation while\nignoring the exact number of computations it needs. In \"Big O\" land, we don't\ncare if something is twice as slow, but we do care whether it's n times slower\nwhere n is the length of our list/set/slice/etc.\n\nHere's a graph of the common time complexity curves:\n\nRemember that these lines are simply about orders of magnitude. If an\noperation is on the order of n, that means 100 times more data will slow\nthings down about 100 times. If an operation is on the order of n2 (that's\nn*n), that means 100 times more data will slow things down 100*100 times.\n\nI usually think about those curves in terms of what would happen if we\nsuddenly had 1,000 times more data to work with:\n\n  * O(1): no change in time (constant time!)\n  * O(log n): ~10 times slow down\n  * O(n): 1,000 times slow down\n  * O(n log n): 10,000 times slow down\n  * O(n2): 1,000,000 times slow down! \ud83d\ude32\n\nWith that very quick recap behind us, let's take a look at the relative speeds\nof all common operations on each of Python's data structures.\n\n## List \ud83d\udccb\n\nPython's lists are similar to arrays or array lists in some other languages.\n\nHere are the time complexities of some common list operations:\n\nBig O| Operation| Notably  \n---|---|---  \nO(1)| sequence.append(item)| Fast!  \nO(1)| sequence.pop(item)| Fast!  \nO(n)| sequence.insert(0, item)| Slow!  \nO(n)| sequence.pop(item, 0)| Slow!  \nO(1)| sequence[index]  \nO(1)| sequence[index] = value  \nO(n)| item in sequence| Slow!  \n  \nI've called out append, pop, and insert above because new Python programmers\nare sometimes surprised by the relative speeds of those operations.\n\nAdding or removing items from the end of a list are both very fast operations\nregardless of how large the list is. On the other hand, adding or removing\nitems from the beginning of a list is very slow (it requires shifting all\nitems after the change).\n\nNote that indexing and assigning to indexes is fast regardless of the index.\nAlso note that the in operator requires looping over the list, unlike sets (as\nwe'll see below).\n\nIn case you're curious, here are even more list operations:\n\nBig O| Operation| Notably  \n---|---|---  \nO(1)| len(sequence)  \nO(k)| sequence.extend(iterable)  \nO(k)| sequence[index:index+k]  \nO(n)| sequence.index(item)| Slow!  \nO(n)| sequence.count(item)| Slow!  \nO(n)| for item in sequence:  \n  \nFor the extend method, k represents the length of the given iterable. For\nslicing, k represents the length of the slice.\n\n## Double-Ended Queue \u2194\ufe0f\n\nLists are stack-like. That means it's inexpensive to perform operations on the\nmost-recently added items (at the end of a list).\n\nFor inexpensive operations involving the least-recently added item (the\nbeginning of a list), we'd need a queue-like structure. That's what Python's\ncollections.deque data structure is for.\n\n    \n    \n    >>> from collections import deque >>> queue = deque([2, 1, 3, 4])\n\nHere are the time complexities of common deque operations:\n\nBig O| Operation| Notably  \n---|---|---  \nO(1)| queue.append(item)  \nO(1)| queue.pop(item)  \nO(1)| queue.appendleft(item)| Fast!  \nO(1)| queue.popleft(item)| Fast!  \nO(n)| item in queue  \nO(n)| for item in queue:  \n  \nNote that we can efficiently add and remove items from the beginning of a\ndeque with the appendleft and popleft methods.\n\nIf you find yourself calling the insert or pop methods on a list with an index\nof 0, you could probably speed your code up by using a deque instead.\n\n## Dictionary \ud83d\udddd\ufe0f\n\nDictionaries are meant for grouping or accumulating values based on a key. Our\n\"dictionaries\" in Python are called hash maps (or sometimes \"associative\narrays\") in many other programming languages.\n\nHere are the time complexities of some common dictionary operations:\n\nBig O| Operation| Notably  \n---|---|---  \nO(1)| mapping[key] = value| Fast!  \nO(1)| mapping[key]  \nO(1)| mapping.get(key)  \nO(1)| mapping.pop(key)| Fast!  \nO(1)| key in mapping| Fast!  \nO(n)| for k, v in mapping.items():  \n  \nNote that the only expensive operation on a dictionary involves explicitly\nlooping over the dictionary.\n\nThanks to the power of hashing, dictionaries are very fast at all operations\nrelated to key lookups. Checking for containment, inserting a new item,\nupdating the value of an item, and removing an item are all constant time\noperations (that's O(1) in big O).\n\nHere are time complexities of slightly less common dictionary operations:\n\nBig O| Operation| Explanation  \n---|---|---  \nO(1)| next(iter(mapping))| Get first item  \nO(1)| next(reversed(mapping))| Get last item  \nO(n)| value in mapping.values()| Value containment  \nO(k)| mapping.update(iterable)| Add many items  \n  \nThe k in O(k) for the update method represents the number of items in the\ngiven iterable.\n\nNote that getting the first and last items is a bit awkward, but very fast.\n\nAlso note that checking whether a dictionary contains a particular value is\nslow! Dictionaries are optimized for fast key lookups, but not fast value\nlookups. Key containment checks are fast, but value containment checks require\nlooping over the whole dictionary.\n\n## Set \ud83c\udfa8\n\nSets store distinct items.\n\nUnlike lists, sets don't maintain the order of their items. Instead, they're\noptimized for quick containment checks.\n\nHere are the time complexities of some common set operations:\n\nBig O| Operation| Notably  \n---|---|---  \nO(1)| my_set.add(item)| Fast!  \nO(1)| my_set.remove(item)| Fast!  \nO(1)| item in my_set| Fast!  \nO(n)| for item in my_set:  \n  \nLike dictionaries, the only expensive operation on a set involves explicitly\nlooping over the set.\n\nMost importantly, asking whether a set contains an item (item in my_set) is\nfast, unlike lists.\n\nSets also support various operations between multiple sets:\n\nBig O| Operation| Explanation  \n---|---|---  \nO(n)| set1 & set2| Intersection  \nO(n)| set1 | set2| Union  \nO(n)| set1 ^ set2| Symmetric difference  \nO(n)| set1 - set2| Asymmetric difference  \n  \nI'm assuming the sets are the same size here. If the sets are different sizes,\nsome of those operations will be on the order of either the smallest or\nlargest set size (depending on the operation).\n\nAlso note that all of those operations work the same way between dictionary\nkeys as well! For example, if you wanted to efficiently find the common keys\nbetween two dictionaries, you can use the & operator:\n\n    \n    \n    >>> colors1 = {\"purple\": 1, \"blue\": 2, \"green\": 4} >>> colors2 = {\"red\": 2, \"purple\": 3, \"blue\": 1} >>> colors1.keys() & colors2.keys() {'blue', 'purple'}\n\n## Counter \ud83e\uddee\n\nPython's collections module includes a Counter class which can efficiently\ncount the number of times each item occurs within a given iterable.\n\nThis collections.Counter class is really just a specialized dictionary with\nsome extra operations.\n\nHere are the time complexities of some common Counter operations:\n\nBig O| Operation  \n---|---  \nO(1)| counter[item]  \nO(1)| counter.pop(item)  \nO(n)| for k, v in counter.items():  \nO(n log n)| for k, v in counter.most_common():  \nO(n log k)| for k, v in counter.most_common(k):  \n  \nNote that the most_common method does the same thing as the dictionary items\nmethod, except it sorts the items first. Although, if a number is passed to\nmost_common, it will efficiently lookup the k most common items instead\n(similar to the heapq.nlargest function noted in traversal techniques below).\n\nHere are a few more somewhat common Counter operations:\n\nBig O| Operation  \n---|---  \nO(k)| counter.update(iterable)  \nO(k)| counter.subtract(iterable)  \nO(n)| counter.total()  \n  \nThe k in O(k) above represents the length of the given iterable to the update\nand subtract methods.\n\n## Heap / Priority Queue \u26f0\ufe0f\n\nNeed a heap, possibly for the sake of implementing your own priority queue?\nPython's heapq module has you covered.\n\nHere are the time complexities of various heap-related operations provided by\nthe heapq module:\n\nBig O| Operation| Notably  \n---|---|---  \nO(n)| heapq.heapify(sequence)  \nO(log n)| heapq.heappop(sequence)| Fast!  \nO(log n)| heapq.heappush(sequence, item)| Fast!  \nO(1)| sequence[0]  \n  \nThe heapq module really just performs operations on a list to treat it like a\nheap.\n\nIt's pretty unusual to implement long-running daemon processes that add items\nto and remove items from a custom priority queue, so you're unlikely to need a\nheap directly within your own code.\n\nThe heapq module does have some handy helper utilities that are heap-powered\nthough (see traversal techniques below).\n\n## Sorted List \ud83d\udd24\n\nNeed to find items or ranges of items within a sorted list? The bisect module\nhas an implementation of binary search for you.\n\nHere are the time complexities of the bisect module's various binary search\noperations:\n\nBig O| Operation| Context / Notably  \n---|---|---  \nO(n log n)| sorted_sequence = sorted(sequence)| If not yet sorted  \nO(n)| sorted_sequence.index(item)| (For comparison's sake)  \nO(log n)| bisect.bisect(sorted_sequence, item)| Fast!  \nO(n)| bisect.insort(sorted_sequence, item)  \n  \nNote that you can combine bisect.bisect_left and bisect.bisect_right to\nefficiently find all items in a sorted list that are within a certain upper\nand lower bound.\n\nKeep in mind that the act of sorting a list takes more time than traversing,\nso unless you're working with already sorted data or you're repeatedly\nbisecting your sorted list, it may be more efficient to simply loop over an\nunsorted list.\n\nAlso keep in mind that adding a new value to a sorted list is slow for the\nsame reason that the list insert method is slow: all values after the\ninsertion index will need to be shuffled around.\n\n## Traversal Techniques \ud83d\udd0d\n\nLastly, let's look at a few common lookup/traversal techniques.\n\nBig O| Operation  \n---|---  \nO(n)| min(iterable)  \nO(n)| max(iterable)  \nO(n log n)| sorted(iterable)  \nO(n log k)| heapq.nsmallest(k, iterable)  \nO(n)| statistics.multimode(iterable)  \n  \nMost traversal techniques require looping over the given iterable, so they're\nO(n) at minimum.\n\nThe traversals that require more time are the ones that involve comparisons\nbetween more than just two values (like sorting every item).\n\nEfficient sorting is O(n log n) in time complexity terms. Whether you're using\nthe list sort method or the built-in sorted function, Python attempts to sort\nas efficiently as it can.\n\nIf you don't really care about sorting every value, but instead you just need\nthe k largest or smallest values, the heapq module has some heap-powered\nutilities that are even faster than sorting.\n\n## Other Data Structures? \ud83d\udcda\n\nComputer Science includes a number of other classical structures, including\nbut not limited to:\n\n  * Lists (CS lists, not Python lists): linked lists, skip lists\n  * Trees: binary trees, B-trees, red-black trees, tries\n  * Graphs: directed, undirected, and weighted\n  * Probabilistic: Bloom filters, locality-sensitive hashing\n\nWhy aren't these structures included in the Python standard library?\n\nWell, as Brandon Rhodes noted in his PyCon 2014 talk, many of the classic CS\ndata structures don't really make sense in Python because data structures in\nPython don't contain actually data but instead contain references to data (see\nvariables and objects in Python).\n\nWhen you do need a data structure that's optimized for specific operations,\nyou can always lookup an implementation online or find a PyPI module (such as\nsortedcollections).\n\n## Beware of Loops-in-Loops! \ud83e\udd2f\n\nNote that time complexity can really compound when you're performing\noperations within a loop.\n\nFor example, this code has an O(n2) time complexity because it contains a loop\ninside a loop:\n\n    \n    \n    counts = {} for item in my_list: counts[item] = my_list.count(item)\n\nThe for loop looks like a loop, but where's the other loop?\n\nThe list count method actually performs an implicit loop because it needs to\nloop over the list to perform its counting!\n\nSince we're performing an O(n) operation for each iteration of our loop, this\ncode is O(n) * O(n), which is usually written as O(n*n) or O(n2). Remember\nthat really steep line in the time complexity plot above? That's O(n2)!\n\nSometimes it's impossible to avoid an O(n2) operation. But it's often possible\nto change your algorithm or your data structures to greatly alter your code's\ntime complexity. In our case we could avoid the count method call in our loop\nby incrementing an item count for each item we see:\n\n    \n    \n    counts = {} for item in my_list: if item not in counts: counts[item] = 0 counts[item] = += 1\n\nDictionary containment checks, key lookups, and item assignments are all O(1)\noperations. So this new for loop now has an O(n) time complexity!\n\nThis code doesn't look any faster at a quick glance. But it will be much\nfaster for large amounts of data. For 1,000 times more data, our code will\nonly be 1,000 times slower, whereas the previous loop would have been\n1,000,000 times slower!\n\nYou can play with different list sizes for each of the above loops in this\ncode snippet.\n\nNote: for readability's sake, our whole loop could also just be one line with\ncollections.Counter.\n\n## Mind Your Data Structures \ud83d\uddc3\ufe0f\n\nChoosing between data structures involves a trade-off between features, speed,\nand memory usage.\n\nFor example, sets are faster at key lookups than lists, but they have no\nordering. Dictionaries are just as fast at key lookups as sets and they\nmaintain item insertion order, but they require more memory.\n\nIn day-to-day Python usage, time complexity tends to matter most for avoiding\nloops within loops.\n\nIf you take away just two things from this article, they should be:\n\n  1. Refactor O(n2) time complexity code to O(n) whenever possible\n  2. When performance really matters, avoid O(n) whenever O(1) or O(log n) are possible\n\nThe next time you're worried about slow code, consider your code's time\ncomplexity. The biggest code speed ups often come from thinking in orders of\nmagnitude.\n\n## A Python tip every week\n\nNeed to fill-in gaps in your Python skills?\n\nSign up for my Python newsletter where I share one of my favorite Python tips\nevery week.\n\n\u2715\n\n\u2191\n\nA Python Tip Every Week\n\nNeed to fill-in gaps in your Python skills? I send weekly emails designed to\ndo just that.\n\n\u00a9 2024\n\nNew User\n\n  * Python Tips\n  * Testimonials\n  * How It Works\n  * Redeem Code\n\nDetails\n\n  * Team Plans\n  * Skill Levels\n  * Pricing\n  * Discounts\n\nAbout\n\n  * FAQ / Help\n  * Privacy Policy\n  * Feature History\n  * About Us\n\nRelated Resources\n\n  * Python Terminology\n  * Python Resources\n  * Python Team Training\n  * Trey's Blog\n\n", "frontpage": true}
