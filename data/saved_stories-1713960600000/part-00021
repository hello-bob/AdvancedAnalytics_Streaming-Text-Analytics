{"aid": "40140036", "title": "Safely repairing broken builds with ML", "url": "https://research.google/blog/safely-repairing-broken-builds-with-ml/", "domain": "research.google", "votes": 1, "user": "transpute", "posted_at": "2024-04-24 03:06:01", "comments": 0, "source_title": "Safely repairing broken builds with ML", "source_text": "Safely repairing broken builds with ML\n\nresearch.google uses cookies from Google to deliver and enhance the quality of\nits services and to analyze traffic. Learn more.\n\nJump to Content\n\nResearch\n\nResearch\n\n# Safely repairing broken builds with ML\n\nApril 23, 2024\n\nEmily Johnston and Stephanie Tang, Software Engineers, Core Systems &\nExperiences\n\nAutomatically repairing non-building code increases productivity as measured\nby overall task completion and appears to introduce no detectable negative\nimpact on code safety, provided that high quality training data and\nresponsible monitoring are employed.\n\n## Quick links\n\n  *     * \u00d7\n\nSoftware development is a cyclical process of designing, writing, building,\ntesting, deploying, and debugging. If you\u2019ve ever resolved a build error only\nto introduce 20 more, you\u2019re familiar with the frustration of hunting down a\ntype mismatch in unfamiliar code, or diagnosing where you went wrong using\nsome new API.\n\nAt Google, every time a source code file is saved, a snapshot of the code is\nsaved to version control. Every time a build runs, build logs are saved. This\nis a data treasure trove! We can determine when a build broke, what error\nmessages were present, when the build succeeded, and what code changed \u2014\nessentially, exactly how the developer fixed the build.\n\nToday we describe how we trained a DIDACT ML model to predict build fixes.\nDIDACT, as we previously discussed, is a methodology that uses the software\ndevelopment process as a whole as training data. Using this dedicated build\nfix ML model, we then surfaced fixes in an IDE and ran a controlled experiment\non this feature. This experiment showed statistically-significant gains across\nseveral measures of productivity, including a 2% increase in the number of\ncode changes. These gains are not only good for their own sake, but they also\nremove developer toil through automation, allowing developers more time for\ncreative problem solving. This promotes focus by removing obstacles, thus\nkeeping developers in their flow state longer. Indeed, we now observe that\nabout 34% of users experiencing a build breakage in a given month end up\napplying such ML-suggested fixes. We also detect no observable increase in\nsafety risks or bugs when ML-generated fixes are applied. ML-powered build\nrepair is now enabled for all Google developers and was recently featured at\nGoogle Cloud Next.\n\nplay silent looping video pause silent looping video\n\nAn overview of our build repair model being trained and suggesting fixes to\ncode. A pre-trained DIDACT model is fine-tuned on records of developers\u2019 build\nerrors and their fixes. This model is then used to suggest repairs to\ndevelopers\u2019 builds in real time in the IDE.\n\n## Problem and motivation\n\nOur ambition originated with the desire to improve developers\u2019 experience\nfixing Java build errors. Could we make error messages more actionable, or\neven fix the errors automatically? We harnessed Google\u2019s comprehensive record\nof its developers\u2019 code and Google\u2019s research expertise to repair broken\nbuilds with ML.\n\nBuild errors are not all simple missing parentheses, typos, or oops-I-forgot-\nmy-dependency-agains. Errors from generics or templates can be convoluted, and\nerror messages can be cryptic. At Google, a build is also more than just\ncompilation. Over the years, there have been many efforts to \u201cshift left\u201d\nseveral types of errors that may occur in the course of the development\nlifecycle, detecting them as early as possible, frequently at build time\n(i.e., while the code is in the early stages of being written). For example, a\ncurated set of Error Prone static analysis checkers are run on every build.\nBuild errors and some static-analysis checks block submission of code changes.\nWe saw potential in our approach for these more complex issues, so we expanded\nthe scope of our repairs to other kinds of errors, then to other languages\nincluding C++, and to more environments.\n\n## Training and input data\n\nTo generate training data from Google\u2019s development history, we assembled\n\u201cresolution sessions\u201d. These are chronological sequences of code snapshots and\nbuild logs occurring in the same workspace, capturing the change in code from\nthe moment a breakage first occurred to the moment it was resolved.\n\nDIDACT is trained on the build errors in our internal codebases that appear at\nthe first snapshot of the resolution session, the state of the code (i.e., the\nfile contents) at that same broken snapshot, and the code diff between the\nbroken and the fixed snapshot.\n\nAt serving time, the DIDACT input is the current code state and the build\nerrors found at that state. DIDACT then predicts a patch to be applied to the\ncode (with a confidence score) as a suggested fix.\n\n## Filtering suggested fixes for quality and safety\n\nThe industry has seen that code-generation ML models may introduce bugs that\ncan slip past developers, so there is a real risk of making code worse with\nML-generated repairs. In order to maintain the quality and safety of Google\u2019s\ncodebase (and credibility with our developers), we apply post-processing to\nthe ML-generated fixes: auto-formatting, then heuristic filters we devised\nusing a combination of expert knowledge and user feedback to avoid common\nquality and safety pitfalls, such as code vulnerabilities.\n\n## What it looks like\n\nA developer sees a fix inline as soon as it is available, previews it, and can\nchoose to accept or reject it.\n\nplay silent looping video pause silent looping video\n\nA recording of a build repair being applied in an IDE. When the developer\nencounters a build error while coding, they are presented with a \u201cReview ML-\nsuggested fix\u201d button. Upon clicking the button, they are shown a preview of\nthe suggested fix, which they can then apply or discard. In this case, the\ndeveloper applied the fix, and the new code built successfully.\n\nFixes that pass quality and safety filtering are surfaced to the user in the\nIDE, and if they are accepted, development continues as normal \u2014 the standard\nprocess of building, testing, static and dynamic analysis, and code review\ncontinues.\n\n## The experiment\n\nWe randomly assigned 50% of all Google developers to the treatment group: they\nreceived access to ML build fixes in the IDE. The other 50% were assigned to\ncontrol. We monitored the two groups for 11 weeks, then compared outcomes.\n\n## Productivity improvements\n\nWe investigated speed- and efficiency-related metrics when evaluating\nproductivity impact. Our results show statistically significant results:\n\n  * ~2% reduction in active coding time per changelist (CL): Average time spent \u201cfingers on keyboard\u201d working on a CL, including coding and closely related activities, before sending for review.\n  * ~2% reduction in shepherding time per CL: Average time spent in the development of a CL after sending for review, including addressing code-review feedback.\n  * ~2% increase in CL throughput: Average number of CLs submitted per week.\n\nThese findings suggest that ML build fixes help developers more quickly\naddress development obstacles like build failures, so they can focus on the\nholistic goal of each CL. In other words, ML build fixes can help developers\nstay in the flow. Furthermore, the 2% increase in CL throughput shows that\ndevelopers don\u2019t just type more code; in fact, they now complete more\nsubmitted, tested units of work.\n\nWe suspect that developers realize increased productivity from more than just\ntrivial build fixes like missing semicolons, or else they wouldn't invest the\nextra time needed to review a suggestion. Instead, developers are likely\nsolving more complex build failures or static analysis failures more\nefficiently. Examples include complex C++ template issues and lambdas, or\ncomplex concurrency Java APIs. We hypothesize that developers end up reviewing\nand perhaps modifying the ML suggestions for such failures, rather than having\nto leave the IDE to search for possible answers, thus interrupting their\ncoding flow.\n\n## Maintaining safety\n\nIncreasing development velocity alone isn\u2019t necessarily better. We must also\nensure that ML-generated code is high-quality and safe.\n\nTo evaluate the risk of introducing incorrect or even dangerous code via our\nfixes, we examined retrospective safety- and reliability-related metrics,\nincluding:\n\n  * Rate of CL rollbacks: Issues in production are typically resolved by rolling back the culprit CL that introduced the bug.\n  * Rate of new sanitizer failures: Google runs sanitizers that can detect and flag issues like memory corruption or leaks on unit tests and fuzz targets; examples are AddressSanitizer, MemorySanitizer, and GWP-Asan.\n\nWe monitored these metrics between CLs authored with the help of build repairs\nvs. without, and found no detectable difference.\n\n## Conclusions\n\nSurfacing ML build fixes in the IDE, guarded by automated safety checks and\nhuman review, significantly improved developer productivity without negatively\nimpacting code safety. This supports our intuition that leveraging ML during\nsoftware development, even on well-understood problems, reduces toil and frees\ndevelopers to solve higher-order problems more efficiently. Moreover, similar\napproaches may be effective in addressing other kinds of error and engineering\ntasks throughout the development cycle. These results demonstrate the\npotential of ML to improve developer productivity, focus, and code quality,\nboth effectively and safely.\n\n## Acknowledgements\n\nThis work is the result of a multi-year collaboration between Google DeepMind\nand Google Core Systems & Experiences team. We would like to acknowledge our\ncolleagues Matt Frazier, Franjo Ivan\u010di\u0107, Jessica Ko, Markus Kusano, Pascal\nLamblin, Pierre-Antoine Manzagol, Sara Qu, and Vaibhav Tulsyan. We would also\nlike to thank our collaborators and leadership, including Hassan Abolhassani,\nEdward Aftandilian, Jacob Austin, Paige Bailey, Kevin Bierhoff, Boris\nBokowski, Juanjo Carin, Satish Chandra, Zimin Chen, Iris Chu, Cristopher\nClaeys, Elvira Djuraeva, Madhura Dudhgaonkar, Alberto Elizondo, Zoubin\nGhahramani, Eli Gild, Nick Glorioso, Chris Gorgolewski, Evgeny Gryaznov,\nDaniel Jasper, Manuel Klimek, Matthew F. Kulukundis, Hugo Larochelle, Darla\nLouis, Quinn Madison, Petros Maniatis, Vahid Meimand, Ali Mesbah, Subhodeep\nMoitra, Krist\u00f3f Moln\u00e1r, Ambar Murillo, Stoyan Nikolov, Amit Patel, Jim Plotts,\nAndy Qin, Marcus Revaj, Andrew Rice, Ballie Sandhu, Sergei Shmulyian, Tom\nSmall, Charles Sutton, Pavel Sychev, Maxim Tabachnyk, Danny Tarlow, David\nTattersall, Niranjan Tulpule, Cornelius Weig, Donald Duo Zhao, and Daniel\nZheng. Thank you!\n\nLabels:\n\n  * Machine Intelligence\n\n  * Software Systems & Engineering\n\n## Quick links\n\n  *     * \u00d7\n\n### Other posts of interest\n\n  * April 12, 2024\n\nContrastive neural audio separation\n\n    * Machine Intelligence \u00b7\n    * Sound & Accoustics\n\n  * April 11, 2024\n\nPatchscopes: A unifying framework for inspecting hidden representations of\nlanguage models\n\n    * Machine Intelligence \u00b7\n    * Natural Language Processing \u00b7\n    * Responsible AI\n\n  * March 28, 2024\n\nAutoBNN: Probabilistic time series forecasting with compositional bayesian\nneural networks\n\n    * Algorithms & Theory \u00b7\n    * Machine Intelligence \u00b7\n    * Open Source Models & Datasets\n\nFollow us\n\n", "frontpage": false}
