{"aid": "40199345", "title": "1000x the Telemetry at 0.01x the Cost", "url": "https://mattklein123.dev/2024/04/17/1000x-the-telemetry/", "domain": "mattklein123.dev", "votes": 8, "user": "fyshotshot", "posted_at": "2024-04-29 15:11:19", "comments": 0, "source_title": "1000x the telemetry at 0.01x the cost", "source_text": "1000x the telemetry at 0.01x the cost\n\n  * Home\n  * About\n  * Appearances\n  * Writing\n\nPrevious post Next post Back to top Share post\n\n  1. 1\\. How can I store nothing, but have access to everything?\n  2. 2\\. What does cost mean when talking about telemetry?\n  3. 3\\. Debug and trace logging\n  4. 4\\. Per-instance metrics\n  5. 5\\. Network protocol traces\n  6. 6\\. Hardware instruction/sensor traces\n  7. 7\\. Hardware performance profiles\n  8. 8\\. Mobile/web session UX tracing\n  9. 9\\. What 1000x case will you think of?\n\n# 1000x the telemetry at 0.01x the cost\n\nMatt Klein\n\n2024-04-17\n\nThis is my third post in a series on a different take on modern observability.\nIn my first post I talked about why modern observability is so expensive. In\nmy second post I talked about whether we really need to store every piece of\ntelemetry that we generate, and a new way of thinking about observability\nusing a control plane and local storage.\n\nIn this post, I\u2019m going to take you through a journey where we explore a world\nin which observability is flipped on its head: we store nothing by default,\nbut have access to anything we would ever want on the fly. What does life look\nlike when you can get any telemetry you want, any time, without thinking about\ncost or storage? What would it mean for our ability to debug and operate\nsystems?\n\n## How can I store nothing, but have access to everything?\n\nThe sad, or maybe just ugly, reality is that the vast majority of telemetry in\ntraditional systems is never read, either by human or machine. Even as\nstorage, compute, and network have gotten cheaper over the years, the\ncomplexity of our systems (microservices, CaaS, FaaS, etc.) and their ability\nto generate large volumes of telemetry data has kept pace, leading to\negregious waste, high relative costs, and unhappy operators.\n\nIn my previous post, I argued that we can apply a dynamic control plane, plus\nuse local storage, to intelligently decide what telemetry to store and use. If\nwe conservatively estimate that overall monthly telemetry volumes and costs\ncan be decreased by 90% using a fully dynamic control plane and local storage\n(I think the real volume decrease could approach 99% or more in some systems),\nwe now have slack to reuse some of this excess capacity for more telemetry\nwhen it matters. Effectively we are trading a steady state of dubiously useful\ntelemetry for bursts of extremely high fidelity telemetry that are targeted\ntowards solving specific problems at hand.\n\nFor the rest of this post I am going to go into some specific examples across\nboth mobile and server observability where getting bursts of 1000x the\ntelemetry will fundamentally change how engineers debug and operate systems.\nThese examples are by no means exhaustive and I fully expect that once dynamic\ntelemetry systems are widely deployed we will continue to see new use cases,\nlimited only by the imagination of engineers around the world.\n\n## What does cost mean when talking about telemetry?\n\nBefore talking about different examples of what 1000x the data can do, let\u2019s\nfirst discuss the definition of cost in the telemetry context.\n\n  1. The most obvious definition of cost is of course in terms of dollars and cents: what is the bill to transport, store, and make queryable telemetry data? This one doesn\u2019t need further explanation.\n  2. Another critical definition of cost, though less discussed, is what is the overhead of collecting telemetry data. Nothing is free in computing. Generating telemetry in and of itself uses CPU time, increases RAM usage, and possibly increases disk usage. In highly concurrent systems, the production of telemetry can change timings enough to alter program behavior. The more telemetry collected the higher the cost. Thus for the most in-depth telemetry collection at scale we want to amortize the overhead cost by limiting the data capture either by sampling across a very large population, limiting the collection time, or both.\n  3. Another definition of cost is what I would call cognitive cost. The larger the overall volume of data, the more of it there is to sift through in order to find the signal within the noise. If we generate large amounts of telemetry data, optimally we would like it to be targeted and obviously useful in understanding the problem currently at hand.\n  4. A final definition of cost is what I would call opportunity cost. These are all of the things that we can\u2019t and don\u2019t do because of the other three types of costs.\n\nAs an aside, on the topic of overhead I recall one particular bug I was\nchasing early in my career inside a mobile phone cellular stack \u2013 if we turned\non all logging the bug wouldn\u2019t repro \u2013 we had to add a single log line at a\ntime and spend hours trying to repro the issue! The problem ended up being a\nsuper rare race condition deep in the telephony stack which was tickled by a\nparticular phone model on a particular network carrier. What a fun bug!\n\nFor the remainder of the post when I talk about cost I am talking about all\nfour: the financial cost, the performance overhead of telemetry collection,\nthe cognitive cost of having too much irrelevant data to sift through, and the\nopportunity cost of not being able to get all of the data we need. Adding a\ncontrol plane and local storage allows us to amortize the cost of extremely\nhigh fidelity telemetry collection across short collection intervals, large\npopulations, and very specific collection triggers (finite state machine\nmatchers sent from control plane to data plane), thus making all four costs\nlow.\n\n## Debug and trace logging\n\nNearly every logging/event framework in existence has a way to categorize logs\ninto relative levels of severity. For example, error, warn, info, debug, and\ntrace. While there is no standard on how to categorize logs, and no standard\non which severity level of logs to emit by default to traditional\nobservability systems, most engineers have some rubric that they follow,\ntypically based on personal experience and/or company culture.\n\nIn order to reduce volume and cost, most organizations emit info and higher\nseverity logs to be stored by a traditional log storage system. Because of\nthis, most engineers have been trained to categorize the severity of their\nlogs, and are generally told to be judicious about info and higher severity,\nwhile debug/trace have no restrictions due to not being sent anywhere by\ndefault.\n\nThus, it is an extremely common occurrence that info logs do not have\nsufficient data to fully understand the path of execution that a program took\nprior to arriving at the info log in question. The number of times in my\ncareer that I have wished to easily see debug/trace logs on a production\nsystem are innumerable!\n\nAdding a control plane and local storage on the other hand allows us to\ndynamically enable debug and trace logs for a limited period of time (and in\nresponse to a specific sequence of events) and get access to the entire volume\nof data that is invaluable in understanding the full set of events that led up\nto a particular application event.\n\n## Per-instance metrics\n\nWhile metrics are largely considered to be more cost efficient than logs, they\nstill can create enormous bills in large organizations. The largest\norganizations typically perform metric aggregation, essentially taking a set\nof related metrics and collapsing them into a single aggregate time series, in\norder to both reduce cost and increase query performance (since many queries\ntypically end up asking for an aggregate average, max, etc. on the many\nunderlying time series). A concrete example of this approach would be taking a\ncounter generated by all Kubernetes pods in a service, dropping the pod label,\nand aggregating the remainder into a per-service count.\n\nUsing this technique can drop overall metric volume by 1-2 orders of magnitude\ndepending on the deployment. Most users don\u2019t notice any difference because as\nI mentioned above, queries tend to ask for aggregates anyway of the per-pod\ndata.\n\nHowever, there are cases in which having per-instance (per-pod) metrics is\nuseful during debugging. For example, looking for outliers that are easy to\nlose in a large batch of aggregate data. A control plane driven observability\narchitecture makes it trivial to temporarily turn on per-instance metrics, and\neven stream them to a live viewer if live debugging is all that is needed.\nThis avoids permanently storing orders of magnitude more metrics when they are\nrarely needed.\n\n## Network protocol traces\n\nNetworking is pervasive in modern systems, both within server-side\ninfrastructure as well as spanning the internet out to large fleets of\nmobile/IoT/web devices. Due to the highly concurrent and inherently fallible\nnature of networking, debugging problems that crop up can be very difficult\nwithout detailed tracing. Example low level protocols used in modern\ndistributed systems include: IP, TCP, UDP, QUIC, and TLS. High level protocols\ninclude HTTP, gRPC, and many others specific to individual databases, caches,\nand so on.\n\nSimilar to logging severity levels discussed in the previous section, it is\npossible to emit variable levels of network protocol tracing, up to and\nincluding \u201ctrace\u201d logging which might include the actual network payloads to\ngreatly aid debugging.\n\nClearly, continuously sending extremely detailed network payload information\nis infeasible due to overhead and cost, but if it could be accessed on-demand\nwhen needed imagine how much simpler it would be to debug hard to understand\nissues!\n\nHaving worked in the application networking world for many years, I cannot\ncount the number of times I have been asked to help capture HTTP REST/gRPC\nrequest and response bodies to aid debugging. A control plane and local\nstorage makes this easily possible at a reasonable cost.\n\nAs an aside, collecting deeply detailed telemetry, especially of this nature,\nhas substantial security implications. For example, network payloads are very\nlikely to have PII, credit card numbers, passwords, etc. Future highly dynamic\nobservability systems will have to very carefully consider data access\nsecurity and encryption of telemetry data. One nice property of the control\nplane / data plane split is that some cases can be solved by matching only\nwithin the data plane and not capturing any data at all with its inherent\nsecurity implications (for example emitting a synthetic metric when a match\ncondition happens). This will be the topic of a future post!\n\n## Hardware instruction/sensor traces\n\nUsing software technologies like eBPF or functionality built in to hardware\nitself, it\u2019s possible to produce extremely high volume tracing data to aid in\nproblem analysis. Like networking data, it\u2019s not practical to send this level\nof data all the time, but when required for in-depth analysis of hard to\nreproduce problems, being able to dynamically enable hardware level tracing\nfor a short period of time can be invaluable in root cause analysis. Note that\nthis category applies to all types of high volume hardware signals including\ninstruction traces, accelerometer readings, GPS/location readings, virtual\nreality camera/sensor recordings, etc.\n\n## Hardware performance profiles\n\nSimilar to hardware instruction/sensor tracing, in-depth profiling using tools\nlike Linux perf can be invaluable in understanding performance bottlenecks\nduring application execution. However, collecting performance profiles has\nnon-zero overhead and also generates large volumes of data which are\nimpractical to collect. Modern continuous profiling tools get around this\nlimitation by using sampling which is a fantastic approach but can make it\ndifficult to drill down into very specific cases and get accurate real-time\ndata specific to a particular program sequence. By allowing the observability\ncontrol plane to engage the profiler when a specific sequence of events\noccurs, very accurate performance data can be retrieved on demand and at low\noverall cost.\n\n## Mobile/web session UX tracing\n\nSession replay is generally defined as recording a user\u2019s journey through a\nmobile or web application. Some variant of session replay is included in many\ndifferent Real User Monitoring (RUM) and general observability tools. Typical\nsession replays capture some variant of the following UX data:\n\n  1. Screen captures\n  2. Taps and clicks\n  3. \u201cRage events\u201d such as shaking and aggressive tapping\n  4. Rotation / general orientation changes\n\nThis is in addition to general observability data capture including logs,\nnetworking events, etc.\n\nThe important thing to understand about session replay is that cost increases\nlinearly along with the amount of data stored (the fidelity). For example,\nrecording screen wire-frames is far more efficient (and privacy conscious)\nthan doing pixel perfect recordings, but has less overall fidelity. Similarly,\nthe more additional UX data is captured (up to and including phone gyroscope\nreadings) the higher the cost both in terms of data volume and capture\noverhead.\n\nBecause of this, especially on mobile, very high fidelity session replay is\nrarely deployed to production due to overall cost. However, when using a\ncontrol plane and local storage it becomes possible to on-demand capture very\nhigh fidelity data when a specific sequence of events occurs, making it\nsubstantially easier to understand real user journeys even for applications\nthat are deployed to millions of active users.\n\n## What 1000x case will you think of?\n\nIn this post I\u2019ve covered a small number of cases where using a control plane\nand local storage to dynamically enable very high fidelity telemetry can aid\nin root cause analysis of customer issues. Fundamentally, this type of system\nprovides a massively better ROI on telemetry cost (both financial, overhead,\nand cognitive), because the generated data is highly detailed when needed and\nabsent when not. 1000x telemetry at 0.01x the cost sounds too good to be true,\nbut in the future I firmly believe that we will look back on traditional\nobservability systems and wonder how we were ever able to debug anything!\n\nAt bitdrift we are just beginning to scratch the surface of what is possible\nwhen reimagining observability with a control plane / data plane split, and I\ncan\u2019t wait to see what other systems and use cases the industry comes up with\ncollectively over the coming years!\n\n  * Home\n  * About\n  * Appearances\n  * Writing\n\n  1. 1\\. How can I store nothing, but have access to everything?\n  2. 2\\. What does cost mean when talking about telemetry?\n  3. 3\\. Debug and trace logging\n  4. 4\\. Per-instance metrics\n  5. 5\\. Network protocol traces\n  6. 6\\. Hardware instruction/sensor traces\n  7. 7\\. Hardware performance profiles\n  8. 8\\. Mobile/web session UX tracing\n  9. 9\\. What 1000x case will you think of?\n\nMenu TOC Share Top\n\nCopyright \u00a9 2020-2024 Matt Klein\n\n", "frontpage": false}
