{"aid": "40153541", "title": "Mind the Patch Gap: Exploiting an Io_uring Vulnerability in Ubuntu", "url": "https://blog.exodusintel.com/2024/03/27/mind-the-patch-gap-exploiting-an-io_uring-vulnerability-in-ubuntu/", "domain": "exodusintel.com", "votes": 1, "user": "todsacerdoti", "posted_at": "2024-04-25 04:49:51", "comments": 0, "source_title": "Mind the Patch Gap: Exploiting an io_uring Vulnerability in Ubuntu - Exodus Intelligence", "source_text": "Mind the Patch Gap: Exploiting an io_uring Vulnerability in Ubuntu - Exodus\nIntelligence\n\nSkip to content\n\nExodus Blog\n\n# Mind the Patch Gap: Exploiting an io_uring Vulnerability in Ubuntu\n\n  * March 27, 2024\n  * Vulnerability Analysis, Exploit Techniques, General Research\n\nBy Oriol Castej\u00f3n\n\n## Overview\n\nThis post discusses a use-after-free vulnerability, CVE-2024-0582, in io_uring\nin the Linux kernel. Despite the vulnerability being patched in the stable\nkernel in December 2023, it wasn\u2019t ported to Ubuntu kernels for over two\nmonths, making it an easy 0day vector in Ubuntu during that time.\n\nIn early January 2024, a Project Zero issue for a recently fixed io_uring use-\nafter-free (UAF) vulnerability (CVE-2024-0582) was made public. It was\napparent that the vulnerability allowed an attacker to obtain read and write\naccess to a number of previously freed pages. This seemed to be a very\npowerful primitive: usually a UAF gets you access to a freed kernel object,\nnot a whole page \u2013 or even better, multiple pages. As the Project Zero issue\nalso described, it was clear that this vulnerability should be easily\nexploitable: if an attacker has total access to free pages, once these pages\nare returned to a slab cache to be reused, they will be able to modify any\ncontents of any object allocated within these pages. In the more common\nsituation, the attacker can modify only a certain type of object, and possibly\nonly at certain offsets or with certain values.\n\nMoreover, this fact also suggests that a data-only exploit should be possible.\nIn general terms, such an exploit does not rely on modifying the code\nexecution flow, by building for instance a ROP chain or using similar\ntechniques. Instead, it focuses on modifying certain data that ultimately\ngrants the attacker root privileges, such as making read-only files writable\nby the attacker. This approach makes exploitation more reliable, stable, and\nallows bypassing some exploit mitigations such as Control-Flow Integrity\n(CFI), as the instructions executed by the kernel are not altered in any way.\n\nFinally, according to the Project Zero issue, this vulnerability was present\nin the Linux kernel from versions starting at 6.4 and prior to 6.7. At that\nmoment, Ubuntu 23.10 was running a vulnerable verison of 6.5 (and somewhat\nlater so was Ubuntu 22.04 LTS), so it was a good opportunity to exploit the\npatch gap, understand how easy it would be for an attacker to do that, and how\nlong they might possess an 0day exploit based on an Nday.\n\nMore precisely:\n\n  * The vulnerability was patched in stable release 6.6.5 on December 8, 2023.\n  * The Project Zero issue was made public one month later, January 8, 2024.\n  * The issue was patched in the Ubuntu kernel 6.5.0-21 which was released on February 22, 2024, for both Ubuntu 22.04 LTS Jammy and Ubuntu 23.10 Mantic .\n\nThis post describes the data-only exploit strategy that we implemented,\nallowing a non-privileged user (and without the need of unprivileged user\nnamespaces) to achieve root privileges on affected systems. First, a general\noverview of the io_uring interface is given, as well as some more specific\ndetails of the interface relevant to this vulnerability. Next, an analysis of\nthe vulnerability is provided. Finally, a strategy for a data-only exploit is\npresented.\n\n## Preliminaries\n\nThe io_uring interface is an asynchronous I/O API for Linux created by Jens\nAxboe and introduced in the Linux kernel version 5.1. Its goal is to improve\nperformance of applications with a high number of I/O operations. It provides\ninterfaces similar to functions like read() and write(), for example, but\nrequests are satisfied in an asynchronous manner to avoid the context\nswitching overhead caused by blocking system calls.\n\nThe io_uring interface has been a bountiful target for a lot of vulnerability\nresearch; it was disabled in ChromeOS, production Google servers, and\nrestricted in Android. As such, there are many blog posts that explain it with\na lot of detail. Some relevant references are the following:\n\n  * Put an io_uring on it \u2013 Exploiting the Linux Kernel, a writeup for an exploit targeting an io_uring operation that provides the same functionality (IORING_OP_PROVIDE_BUFFERS) as the vulnerability discussed here (IORING_REGISTER_PBUF_RING), and that has also a broad overview of this subsystem.\n  * CVE-2022-29582 An io_uring vulnerability, where a cross-cache exploit is described. While the exploit described in our blog post is not strictly speaking cross-cache, there is some similarity between the two exploit strategies. It also provides an explanation of slab caches and the page allocator relevant to our exploit strategy.\n  * Escaping the Google kCTF Container with a Data-Only Exploit, where a different strategy for data-only exploit of an io_uring vulnerability is described.\n  * Conquering the memory through io_uring \u2013 Analysis of CVE-2023-2598, a writeup of a vulnerability that yields a very similar exploit primitive to ours. In this case, however, the exploit strategy relies on manipulating a structure associated with a socket, instead of manipulating file structures.\n\nIn the next subsections we give an overview of the io_uring interface. We pay\nspecial attention to the Provided Buffer Ring functionality, which is relevant\nto the vulnerability discussed in this post. The reader can also check \u201cWhat\nis io_uring?\u201d, as well as the above references for alternative overviews of\nthis subsystem.\n\n### The io_uring Interface\n\nThe basis of io_uring is a set of two ring buffers used for communication\nbetween user and kernel space. These are:\n\n  * The submission queue (SQ), which contains submission queue entries (SQEs) describing a request for an I/O operation, such as reading or writing to a file, etc.\n  * The completion queue (CQ), which contains completion queue entries (CQEs) that correspond to SQEs that have been processed and completed.\n\nThis model allows executing a number of I/O requests to be performed\nasynchronously using a single system call, while in a synchronous manner each\nrequest would have typically corresponded to a single system call. This\nreduces the overhead caused by blocking system calls, thus improving\nperformance. Moreover, the use of shared buffers also reduces the overhead as\nno data between user and kernelspace has to be transferred.\n\nThe io_uring API consists of three system calls:\n\n  * io_uring_setup()\n  * io_uring_register()\n  * io_uring_enter()\n\n#### The io_uring_setup() System Call\n\nThe io_uring_setup() system call sets up a context for an io_uring instance,\nthat is, a submission and a completion queue with the indicated number of\nentries each one. Its prototype is the following:\n\n    \n    \n    int io_uring_setup(u32 entries, struct io_uring_params *p);\n\nIts arguments are:\n\n  * entries: It determines how many elements the SQ and CQ must have at the minimum.\n  * params: It can be used by the application to pass options to the kernel, and by the kernel to pass information to the application about the ring buffers.\n\nOn success, the return value of this system call is a file descriptor that can\nbe later used to perform operation on the io_uring instance.\n\n#### The io_uring_register() System Call\n\nThe io_uring_register() system call allows registering resources, such as user\nbuffers, files, etc., for use in an io_uring instance. Registering such\nresources makes the kernel map them, avoiding future copies to and from\nuserspace, thus improving performance. Its prototype is the following:\n\n    \n    \n    int io_uring_register(unsigned int fd, unsigned int opcode, void *arg, unsigned int nr_args);\n\nIts arguments are:\n\n  * fd: The file io_uring file descriptor returned by the io_uring_setup() system call.\n  * opcode: The specific operation to be executed. It can have certain values such as IORING_REGISTER_BUFFERS, to register user buffers, or IORING_UNREGISTER_BUFFERS, to release the previously registered buffers.\n  * arg: Arguments passed to the operation being executed. Their type depends on the specific opcode being passed.\n  * nr_args: Number of arguments in arg being passed.\n\nOn success, the return value of this system call is either zero or a positive\nvalue, depending on the opcode used.\n\n##### Provided Buffer Rings\n\nAn application might need to have different types of registered buffers for\ndifferent I/O requests. Since kernel version 5.7, to facilitate managing these\ndifferent sets of buffers, io_uring allows the application to register a pool\nof buffers that are identified by a group ID. This is done using the\nIORING_REGISTER_PBUF_RING opcode in the io_uring_register() system call.\n\nMore precisely, the application starts by allocating a set of buffers that it\nwants to register. Then, it makes the io_uring_register() system call with\nopcode IORING_REGISTER_PBUF_RING, specifying a group ID with which these\nbuffers should be associated, a start address of the buffers, the length of\neach buffer, the number of buffers, and a starting buffer ID. This can be done\nfor multiple sets of buffers, each one having a different group ID.\n\nFinally, when submitting a request, the application can use the\nIOSQE_BUFFER_SELECT flag and provide the desired group ID to indicate that a\nprovided buffer ring from the corresponding set should be used. When the\noperation has been completed, the buffer ID of the buffer used for the\noperation is passed to the application via the corresponding CQE.\n\nProvided buffer rings can be unregistered via the io_uring_register() system\ncall using the IORING_UNREGISTER_PBUF_RING opcode.\n\n##### User-mapped Provided Buffer Rings\n\nIn addition to the buffers allocated by the application, since kernel version\n6.4, io_uring allows a user to delegate the allocation of provided buffer\nrings to the kernel. This is done using the IOU_PBUF_RING_MMAP flag passed as\nan argument to io_uring_register(). In this case, the application does not\nneed to previously allocate these buffers, and therefore the start address of\nthe buffers does not have to be passed to the system call. Then, after\nio_uring_register() returns, the application can mmap() the buffers into\nuserspace with the offset set as:\n\n    \n    \n    IORING_OFF_PBUF_RING | (bgid >> IORING_OFF_PBUF_SHIFT)\n\nwhere bgid is the corresponding group ID. These offsets, as well as others\nused to mmap() the io_uring data, are defined in\ninclude/uapi/linux/io_uring.h:\n\n    \n    \n    /* * Magic offsets for the application to mmap the data it needs */ #define IORING_OFF_SQ_RING 0ULL #define IORING_OFF_CQ_RING 0x8000000ULL #define IORING_OFF_SQES 0x10000000ULL #define IORING_OFF_PBUF_RING 0x80000000ULL #define IORING_OFF_PBUF_SHIFT 16 #define IORING_OFF_MMAP_MASK 0xf8000000ULL\n\nThe function that handles such an mmap() call is io_uring_mmap():\n\n    \n    \n    // Source: https://elixir.bootlin.com/linux/v6.5.3/source/io_uring/io_uring.c#L3439 static __cold int io_uring_mmap(struct file *file, struct vm_area_struct *vma) { size_t sz = vma->vm_end - vma->vm_start; unsigned long pfn; void *ptr; ptr = io_uring_validate_mmap_request(file, vma->vm_pgoff, sz); if (IS_ERR(ptr)) return PTR_ERR(ptr); pfn = virt_to_phys(ptr) >> PAGE_SHIFT; return remap_pfn_range(vma, vma->vm_start, pfn, sz, vma->vm_page_prot); }\n\nNote that remap_pfn_range() ultimately creates a mapping with the VM_PFNMAP\nflag set, which means that the MM subsystem will treat the base pages as raw\npage frame number mappings wihout an associated page structure. In particular,\nthe core kernel will not keep reference counts of these pages, and keeping\ntrack of it is the responsability of the calling code (in this case, the\nio_uring subsystem).\n\n#### The io_uring_enter() System Call\n\nThe io_uring_enter() system call is used to initiate and complete I/O using\nthe SQ and CQ that have been previously set up via the io_uring_setup() system\ncall. Its prototype is the following:\n\n    \n    \n    int io_uring_enter(unsigned int fd, unsigned int to_submit, unsigned int min_complete, unsigned int flags, sigset_t *sig);\n\nIts arguments are:\n\n  * fd: The io_uring file descriptor returned by the io_uring_setup() system call.\n  * to_submit: Specifies the number of I/Os to submit from the SQ.\n  * flags: A bitmask value that allows specifying certain options, such as IORING_ENTER_GETEVENTS, IORING_ENTER_SQ_WAKEUP, IORING_ENTER_SQ_WAIT, etc.\n  * sig: A pointer to a signal mask. If it is not NULL, the system call replaces the current signal mask by the one pointed to by sig, and when events become available in the CQ restores the original signal mask.\n\n## Vulnerability\n\nThe vulnerability can be triggered when an application registers a provided\nbuffer ring with the IOU_PBUF_RING_MMAP flag. In this case, the kernel\nallocates the memory for the provided buffer ring, instead of it being done by\nthe application. To access the buffers, the application has to mmap() them to\nget a virtual mapping. If the application later unregisters the provided\nbuffer ring using the IORING_UNREGISTER_PBUF_RING opcode, the kernel frees\nthis memory and returns it to the page allocator. However, it does not have\nany mechanism to check whether the memory has been previously unmapped in\nuserspace. If this has not been done, the application has a valid memory\nmapping to freed pages that can be reallocated by the kernel for other\npurposes. From this point, reading or writing to these pages will trigger a\nuse-after-free.\n\nThe following code blocks show the affected parts of functions relevant to\nthis vulnerability. Code snippets are demarcated by reference markers denoted\nby [N]. Lines not relevant to this vulnerability are replaced by a [Truncated]\nmarker. The code corresponds to the Linux kernel version 6.5.3, which\ncorresponds to the version used in the Ubuntu kernel 6.5.0-15-generic.\n\n### Registering User-mapped Provided Buffer Rings\n\nThe handler of the IORING_REGISTER_PBUF_RING opcode for the\nio_uring_register() system call is the io_register_pbuf_ring() function, shown\nin the next listing.\n\n    \n    \n    // Source: https://elixir.bootlin.com/linux/v6.5.3/source/io_uring/kbuf.c#L537 int io_register_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg) { struct io_uring_buf_reg reg; struct io_buffer_list *bl, *free_bl = NULL; int ret; [1] if (copy_from_user(&reg, arg, sizeof(reg))) return -EFAULT; [Truncated] if (!is_power_of_2(reg.ring_entries)) return -EINVAL; [2] /* cannot disambiguate full vs empty due to head/tail size */ if (reg.ring_entries >= 65536) return -EINVAL; if (unlikely(reg.bgid io_bl)) { int ret = io_init_bl_list(ctx); if (ret) return ret; } bl = io_buffer_get_list(ctx, reg.bgid); if (bl) { /* if mapped buffer ring OR classic exists, don't allow */ if (bl->is_mapped || !list_empty(&bl->buf_list)) return -EEXIST; } else { [3] free_bl = bl = kzalloc(sizeof(*bl), GFP_KERNEL); if (!bl) return -ENOMEM; } [4] if (!(reg.flags & IOU_PBUF_RING_MMAP)) ret = io_pin_pbuf_ring(&reg, bl); else ret = io_alloc_pbuf_ring(&reg, bl); [Truncated] return ret; }\n\nThe function starts by copying the provided arguments into an io_uring_buf_reg\nstructure reg [1]. Then, it checks that the desired number of entries is a\npower of two and is strictly less than 65536 [2]. Note that this implies that\nthe maximum number of allowed entries is 32768.\n\nNext, it checks whether a provided buffer list with the specified group ID\nreg.bgid exists and, in case it does not, an io_buffer_list structure is\nallocated and its address is stored in the variable bl [3]. Finally, if the\nprovided arguments have the flag IOU_PBUF_RING_MMAP set, the\nio_alloc_pbuf_ring() function is called [4], passing in the address of the\nstructure reg, which contains the arguments passed to the system call, and the\npointer to the allocated buffer list structure bl.\n\n    \n    \n    // Source: https://elixir.bootlin.com/linux/v6.5.3/source/io_uring/kbuf.c#L519 static int io_alloc_pbuf_ring(struct io_uring_buf_reg *reg, struct io_buffer_list *bl) { gfp_t gfp = GFP_KERNEL_ACCOUNT | __GFP_ZERO | __GFP_NOWARN | __GFP_COMP; size_t ring_size; void *ptr; [5] ring_size = reg->ring_entries * sizeof(struct io_uring_buf_ring); [6] ptr = (void *) __get_free_pages(gfp, get_order(ring_size)); if (!ptr) return -ENOMEM; [7] bl->buf_ring = ptr; bl->is_mapped = 1; bl->is_mmap = 1; return 0; }\n\nThe io_alloc_pbuf_ring() function takes the number of ring entries specified\nin reg->ring_entries and computes the resulting size ring_size by multiplying\nit by the size of the io_uring_buf_ring structure [5], which is 16 bytes.\nThen, it requests a number of pages from the page allocator that can fit this\nsize via a call to __get_free_pages() [6]. Note that for the maximum number of\nallowed ring entries, 32768, ring_size is 524288 and thus the maximum number\nof 4096-byte pages that can be retrieved is 128. The address of the first page\nis then stored in the io_buffer_list structure, more precisely in bl->buf_ring\n[7]. Also, bl->is_mapped and bl->is_mmap are set to 1.\n\n### Unregistering Provided Buffer Rings\n\nThe handler of the IORING_UNREGISTER_PBUF_RING opcode for the\nio_uring_register() system call is the io_unregister_pbuf_ring() function,\nshown in the next listing.\n\n    \n    \n    // Source: https://elixir.bootlin.com/linux/v6.5.3/source/io_uring/kbuf.c#L601 int io_unregister_pbuf_ring(struct io_ring_ctx *ctx, void __user *arg) { struct io_uring_buf_reg reg; struct io_buffer_list *bl; [8] if (copy_from_user(&reg, arg, sizeof(reg))) return -EFAULT; if (reg.resv[0] || reg.resv[1] || reg.resv[2]) return -EINVAL; if (reg.flags) return -EINVAL; [9] bl = io_buffer_get_list(ctx, reg.bgid); if (!bl) return -ENOENT; if (!bl->is_mapped) return -EINVAL; [10] __io_remove_buffers(ctx, bl, -1U); if (bl->bgid >= BGID_ARRAY) { xa_erase(&ctx->io_bl_xa, bl->bgid); kfree(bl); } return 0; }\n\nAgain, the function starts by copying the provided arguments into a\nio_uring_buf_reg structure reg [8]. Then, it retrieves the provided buffer\nlist corresponding to the group ID specified in reg.bgid and stores its\naddress in the variable bl [9]. Finally, it passes bl to the function\n__io_remove_buffers() [10].\n\n    \n    \n    // Source: https://elixir.bootlin.com/linux/v6.5.3/source/io_uring/kbuf.c#L209 static int __io_remove_buffers(struct io_ring_ctx *ctx, struct io_buffer_list *bl, unsigned nbufs) { unsigned i = 0; /* shouldn't happen */ if (!nbufs) return 0; if (bl->is_mapped) { i = bl->buf_ring->tail - bl->head; if (bl->is_mmap) { struct page *page; [11] page = virt_to_head_page(bl->buf_ring); [12] if (put_page_testzero(page)) free_compound_page(page); bl->buf_ring = NULL; bl->is_mmap = 0; } else if (bl->buf_nr_pages) { [Truncated]\n\nIn case the buffer list structure has the is_mapped and is_mmap flags set,\nwhich is the case when the buffer ring was registered with the\nIOU_PBUF_RING_MMAP flag [7], the function reaches [11]. Then, the page\nstructure of the head page corresponding to the virtual address of the buffer\nring bl->buf_ring is obtained. Finally, all the pages forming the compound\npage with head page are freed at [12], thus returning them to the page\nallocator.\n\nNote that if the provided buffer ring is set up with IOU_PBUF_RING_MMAP, that\nis, it has been allocated by the kernel and not the application, the userspace\napplication is expected to have previously mmap()ed this memory. Moreover,\nrecall that since the memory mapping was created with the VM_PFNMAP flag, the\nreference count of the page structure was not modified during this operation.\nIn other words, in the code above there is no way for the kernel to know\nwhether the application has unmapped the memory before freeing it via the call\nto free_compound_page(). If this has not happened, a use-after-free can be\ntriggered by the application by just reading or writing to this memory.\n\n## Exploitation\n\nThe exploitation mechanism presented in this post relies on how memory\nallocation works on Linux, so the reader is expected to have some familiarity\nwith it. As a refresher, we highlight the following facts:\n\n  * The page allocator is in charge of managing memory pages, which are usually 4096 bytes. It keeps lists of free pages of order n, that is, memory chunks of page size multiplied by 2^n. These pages are served in a first-in-first-out basis.\n  * The slab allocator sits on top of the buddy allocator and keeps caches of commonly used objects (dedicated caches) or fixed-size objects (generic caches), called slab caches, available for allocation in the kernel. There are several implementations of slab allocators, but for the purpose of this post only the SLUB allocator, the default in modern versions of the kernel, is relevant.\n  * Slab caches are formed by multiple slabs, which are sets of one or more contiguous pages of memory. When a slab cache runs out of free slabs, which can happen if a large number of objects of the same type or size are allocated and not freed during a period of time, the operating system allocates a new slab by requesting free pages to the page allocator.\n\nOne of such cache slabs is the filp, which contains file structures. A\nfilestructure, shown in the next listing, represents an open file.\n\n    \n    \n    // Source: https://elixir.bootlin.com/linux/v6.5.3/source/include/linux/fs.h#L961 struct file { union { struct llist_node f_llist; struct rcu_head f_rcuhead; unsigned int f_iocb_flags; }; /* * Protects f_ep, f_flags. * Must not be taken from IRQ context. */ spinlock_t f_lock; fmode_t f_mode; atomic_long_t f_count; struct mutex f_pos_lock; loff_t f_pos; unsigned int f_flags; struct fown_struct f_owner; const struct cred *f_cred; struct file_ra_state f_ra; struct path f_path; struct inode *f_inode; /* cached value */ const struct file_operations *f_op; u64 f_version; #ifdef CONFIG_SECURITY void *f_security; #endif /* needed for tty driver, and maybe others */ void *private_data; #ifdef CONFIG_EPOLL /* Used by fs/eventpoll.c to link all the hooks to this file */ struct hlist_head *f_ep; #endif /* #ifdef CONFIG_EPOLL */ struct address_space *f_mapping; errseq_t f_wb_err; errseq_t f_sb_err; /* for syncfs */ } __randomize_layout __attribute__((aligned(4))); /* lest something weird decides that 2 is OK */\n\nThe most relevant fields for this exploit are the following:\n\n  * f_mode: Determines whether the file is readable or writable.\n  * f_pos: Determines the current reading or writing position.\n  * f_op: The operations associated with the file. It determines the functions to be executed when certain system calls such as read(), write(), etc., are issued on the file. For files in ext4 filesystems, this is equal to the ext4_file_operations variable.\n\n### Strategy for a Data-Only Exploit\n\nThe exploit primitive provides an attacker with read and write access to a\ncertain number of free pages that have been returned to the page allocator. By\nopening a file a large number of times, the attacker can force the exhaustion\nof all the slabs in the filp cache, so that free pages are requested to the\npage allocator to create a new slab in this cache. In this case, further\nallocations of file structures will happen in the pages on which the attacker\nhas read and write access, thus being able to modify them. In particular, for\nexample, by modifying the f_mode field, the attacker can make a file that has\nbeen opened with read-only permissions to be writable.\n\nThis strategy was implemented to successfully exploit the following versions\nof Ubuntu:\n\n  * Ubuntu 22.04 Jammy Jellyfish LTS with kernel 6.5.0-15-generic.\n  * Ubuntu 22.04 Jammy Jellyfish LTS with kernel 6.5.0-17-generic.\n  * Ubuntu 23.10 Mantic Minotaur with kernel 6.5.0-15-generic.\n  * Ubuntu 23.10 Mantic Minotaur with kernel 6.5.0-17-generic.\n\nThe next subsections give more details on how this strategy can be carried\nout.\n\n#### Triggering the Vulnerability\n\nThe strategy begins by triggering the vulnerability to obtain read and write\naccess to freed pages. This can be done by executing the following steps:\n\n  * Making an io_uring_setup() system call to set up the io_uring instance.\n  * Making an io_uring_register() system call with opcode IORING_REGISTER_PBUF_RING and the IOU_PBUF_RING_MMAP flag, so that the kernel itself allocates the memory for the provided buffer ring.\n\nRegistering a provided buffer ring\n\n  * mmap()ing the memory of the provided buffer ring with read and write permissions, using the io_uring file descriptor and the offset IORING_OFF_PBUF_RING.\n\nMMap the buffer ring\n\n  * Unregistering the provided buffer ring by making an io_uring_register()system call with opcode IORING_UNREGISTER_PBUF_RING.\n\nUnregistering the buffer ring\n\nAt this point, the pages corresponding to the provided buffer ring have been\nreturned to the page allocator, while the attacker still has a valid reference\nto them.\n\n#### Spraying File Structures\n\nThe next step is spawning a large number of child processes, each one opening\nthe file /etc/passwd many times with read-only permissions. This forces the\nallocation of corresponding file structures in the kernel.\n\nSpraying file structures\n\nBy opening a large number of files, the attacker can force the exhaustion of\nthe slabs in the filp cache. After that, new slabs will be allocated by\nrequesting free pages from the page allocator. At some point, the pages that\npreviously corresponded to the provided buffer ring, and to which the attacker\nstill has read and write access, will be returned by the page allocator.\n\nRequesting free pages from the page allocator\n\nHence, all of the file structures created after this point will be allocated\nin the attacker-controlled memory region, giving them the possibility to\nmodify the structures.\n\nAllocating file structures within a controlled page\n\nNote that these child processes have to wait until indicated to proceed in the\nlast stage of the exploit, so that the files are kept open and their\ncorresponding structures are not freed.\n\n#### Locating a File Structure in Memory\n\nAlthough the attacker may have access to some slabs belonging to the filp\ncache, they don\u2019t know where they are within the memory region. To identify\nthese slabs, however, the attacker can search for the ext4_file_operations\naddress at the offset of the file.f_op field within the file structure. When\none is found, it can be safely assumed that it corresponds to the file\nstructure of one instance of the previously opened /etc/passwd file.\n\nNote that even when Kernel Address Space Layout Randomization (KASLR) is\nenabled, to identify the ext4_file_operations address in memory it is only\nnecessary to know the offset of this symbol with respect to the _text symbol,\nso there is no need for a KASLR bypass. Indeed, given a value val of an\nunsigned integer found in memory at the corresponding offset, one can safely\nassume that it is the address of ext4_file_operations if:\n\n  * (val >> 32 & 0xffffffff) == 0xffffffff, i.e. the 32 most significant bits are all 1.\n  * (val & 0xfffff) == (ext4_fops_offset & 0xfffff), i.e. the 20 least significant bits of val and ext4_fops_offset, the offset of ext4_file_operations with respect to _text, are the same.\n\n#### Changing File Permissions and Adding a Backdoor Account\n\nOnce a file structure corresponding to the /etc/passwd file is located in the\nmemory region accessible by the attacker, it can be modified at will. In\nparticular, setting the FMODE_WRITE and FMODE_CAN_WRITE flags in the\nfile.f_mode field of the found structure will make the /etc/passwd file\nwritable when using the corresponding file descriptor.\n\nMoreover, setting the file.f_pos field of the found file structure to the\ncurrent size of the /etc/passwd/ file, the attacker can ensure that any data\nwritten to it is appended at the end of the file.\n\nTo finish, the attacker can signal all the child processes spawned in the\nsecond stage to try to write to the opened /etc/passwd file. While most of all\nof such attempts will fail, as the file was opened with read-only permissions,\nthe one corresponding to the modified file structure, which has write\npermissions enabled due to the modification of the file->f_mode field, will\nsucceed.\n\n## Conclusion\n\nTo sum up, in this post we described a use-after-free vulnerability that was\nrecently disclosed in the io_uring subsystem of the Linux kernel, and a data-\nonly exploit strategy was presented. This strategy proved to be realitvely\nsimple to implement. During our tests it proved to be very reliable and, when\nit failed, it did not affect the stability of the system. This strategy\nallowed us to exploit up-to-date versions of Ubuntu during the patch gap\nwindow of about two months.\n\n## About Exodus Intelligence\n\nOur world class team of vulnerability researchers discover hundreds of\nexclusive Zero-Day vulnerabilities, providing our clients with proprietary\nknowledge before the adversaries find them. We also conduct N-Day research,\nwhere we select critical N-Day vulnerabilities and complete research to prove\nwhether these vulnerabilities are truly exploitable in the wild.\n\nFor more information on our products and how we can help your vulnerability\nefforts, visit www.exodusintel.com or contact info@exodusintel.com for further\ndiscussion.\n\n## Intelligence\n\n  * Zero-Day\n  * N-Day\n  * Training\n\n## Support\n\n  * Resources\n  * Blog\n  * FAQ\n\n## Company\n\n  * Why Exodus?\n  * About\n  * Careers\n\nCopyright 2021 Exodus Intelligence\n\n", "frontpage": false}
