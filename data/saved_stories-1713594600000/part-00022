{"aid": "40093009", "title": "Nand to Tetris: Building a Modern Computer System from First Principles", "url": "https://cacm.acm.org/research/nand-to-tetris-building-a-modern-computer-system-from-first-principles/", "domain": "acm.org", "votes": 2, "user": "sohkamyung", "posted_at": "2024-04-19 23:27:27", "comments": 0, "source_title": "Nand to Tetris: Building a Modern Computer System from First Principles \u2013 Communications of the ACM", "source_text": "Nand to Tetris: Building a Modern Computer System from First Principles \u2013\nCommunications of the ACM\n\nSkip to content\n\nSearch Sign In\n\nJoin ACM\n\nResearch and Advances\n\nArchitecture and Hardware\n\n# Nand to Tetris: Building a Modern Computer System from First Principles\n\nCS course walks students through a step-by-step construction of a complete,\ngeneral-purpose computer system\u2014hardware and software\u2014in one semester.\n\nBy Shimon Schocken\n\nPosted Apr 19 2024\n\n  * Share\n\n    * Twitter\n    * Reddit\n    * Hacker News\n  * Download PDF\n  * Print\n  * Join the Discussion\n  * View in the ACM Digital Library\n\n     * From Nand to Tetris\n     * Discussion: Engineering\n     * Discussion: Pedagogy\n     * Extensions\n     * Conclusion\n     * Acknowledgments\n     * References\n\nSuppose you were asked to design an abridged computer science (CS) program\nconsisting of just three courses. How would you go about it? The first course\nwould probably be an introduction to computer science, exposing students to\ncomputational thinking and equipping them with basic programming skills. The\nsecond course would most likely be algorithms and data structures. But what\nshould the third course be?\n\nSeveral reasonable options come to mind. One is a hands-on overview of applied\nCS, building on the programming skills and theoretical knowledge acquired in\nthe first two courses. Such a course could survey key topics in computer\narchitecture, compilation, operating systems, and software engineering,\npresented in one cohesive framework. Ideally, the course would engage students\nin significant programming assignments, have them implement classical\nalgorithms and widely used data structures, and expose them to a range of\noptimization and complexity issues. This hands-on synthesis could benefit\nstudents who seek an overarching understanding of computing systems, as well\nas self-learners and non-majors who cannot commit to more than a few core CS\ncourses.\n\n## Key Insights\n\n  * In the early days of computers, any curious person could gain a gestalt understanding of how the machine works. As digital technologies became increasingly more complex, this clarity is all but lost: the most fundamental ideas and techniques in applied computer science are now hidden under many layers of obscure interfaces and proprietary implementations.\n\n  * Starting from Nand gates only, students build a hardware platform comprising a CPU, RAM, datapath, and a software hierarchy consisting of an assembler, a virtual machine, a basic OS, and a compiler for a simple, Java-like object-based language.\n\n  * The result is a synthesis course that combines key topics from traditional systems courses in one hands-on framework. The course is self-contained, the only prerequisite being introduction to computer science.\n\nThis article describes one such a course, called Nand to Tetris, which walks\nstudents through a step-by-step construction of a complete, general-purpose\ncomputer system\u2014hardware and software\u2014in one semester. As it turns out,\nconstruction of the computer system built during the course requires exposure\nto, and application of, some of the most pertinent and beautiful ideas and\ntechniques in applied CS. The computer\u2019s hardware platform (CPU, RAM,\ndatapath) is built using a simple hardware description language and a supplied\nhardware simulator. The computer\u2019s software hierarchy (assembler, virtual\nmachine, compiler) can be built in any programming language, following\nsupplied specifications. The resulting computer is equipped with a simple\nJava-like, object-based language that lends itself well to interactive\napplications using graphics and animation. Thousands of computer games have\nalready been developed on this computer, and many of them are illustrated in\nYouTube.\n\nFollowing early versions of the course^6 that underwent many improvements and\nextensions, the complete Nand to Tetris approach was described in the book The\nElements of Computing Systems, by Noam Nisan and Shimon Schocken.^3 By\nchoosing a book title that nods to Strunk and White\u2019s masterpiece,^9 we sought\nto allude to the concise and principled nature of our approach. All course\nmaterials\u2014lectures, projects, specifications, and software tools\u2014are freely\navailable in open source.^5 Versions of the course are now offered in many\neducational settings, including academic departments, high schools, bootcamps,\nand online platforms. A typical course syllabus is available.^2 About half of\nthe course\u2019s online learners are developers who wish to acquire a deep, hands-\non understanding of the hardware and software infrastructures that enable\ntheir work. And the best way to understand something deeply is to build it\nfrom the ground up.\n\n## From Nand to Tetris\n\nThe explicit goal of Nand to Tetris courses is building a general-purpose\ncomputer system from elementary logic gates. The implicit goals are providing\na hands-on exposition of key concepts and techniques in applied computer\nscience, and a compelling synthesis of core topics from digital architectures,\ncompilation, operating systems, and software engineering. We make this\nsynthesis concrete by walking students through 12 hands-on projects. Each\nproject presents and motivates an important hardware or software abstraction,\nand then it provides guidelines for implementing the abstraction using\nexecutable modules developed in previous, lower-level projects. The computer\nsystem that emerges from this effort is built gradually and from the bottom up\n(see Figure 1).\n\nFigure 1. Overall course plan. Each project p1, p2, ..., p12 lasts one to two\ncourse-weeks. Project numbers indicate the normal sequence, although they can\nbe done in any desired order.\n\nThe first five projects in the course focus on constructing the chipset and\narchitecture of a simple von Neumann computer. The remaining seven projects\nrevolve around the design and implementation of a typical software hierarchy.\nIn particular, we motivate and build an assembler; a virtual machine; a two-\ntier compiler for a high-level, object-based programming language; a basic\noperating system (OS), and an application\u2014typically a simple computer game\ninvolving animation and interaction. The overall course consists of two parts,\nas we now turn to describe.\n\nPart I: Hardware. The course starts with a focused overview of Boolean\nalgebra. We use disjunctive normal forms and reductive reasoning to show that\nevery Boolean function can be realized using no more than NAND operators. This\nprovides a theoretical yet impractical demonstration that the course\ngoal\u2014building a general-purpose computer system from just NAND gates\u2014is indeed\nfeasible. We then discuss gate logic and chip specification, and present a\nsimple hardware description language (HDL) that can be learned in a few hours.\n\nIn project 1, students use this HDL and a supplied hardware simulator to build\nand unit-test elementary logic gates such as AND, OR, NOT, multiplexors, and\ntheir 16-bit extensions. We then discuss Boolean arithmetic, two\u2019s complement,\nand arithmetic-logic operations. This background sets the stage for project 2,\nin which students use the elementary gates built in project 1 to implement a\nfamily of combinational chips, leading up to an ALU. We then discuss how\nsequential logic and finite-state automata can be used to implement chips that\nmaintain state. In project 3, students apply this knowledge to gradually build\nand unit-test 1bit and 16bit registers, as well as a family of direct-access\nmemory units (in which addressing and storage are realized using combinational\nand sequential logic, respectively), leading up to a RAM.\n\nAt this stage, we have all the basic building blocks necessary for\nsynthesizing a simple 16bit von Neumann machine, which we call \u201cHack.\u201d Before\ndoing so, we present the instruction set of this target computer (viewed\nabstractly), in both its symbolic and binary versions. In project 4, students\nuse the symbolic Hack machine language to write assembly programs that perform\nbasic algebraic, graphical, and user-interaction tasks (the Hack computer\nspecification includes input and output drivers that use memory bitmaps for\nrendering pixels on a connected screen and for reading 16bit character codes\nfrom a connected keyboard). The students test and execute their assembly\nprograms on a supplied emulator that simulates the Hack computer along with\nits screen and keyboard devices.\n\nNext, we present possible skeletal architectures of the Hack CPU and datapath.\nThis is done abstractly, by discussing how an architecture can be functionally\nplanned to fetch, decode, and execute binary instructions written in the Hack\ninstruction set. We then discuss how the ALU, registers, and RAM chips built\nin projects 1-3 can be integrated into a hardware platform that realizes the\nHack computer specification and machine language. Construction of this topmost\ncomputer-on-a-chip is completed in project 5.\n\nAltogether, in Part I of the course, students build 35 combinational and\nsequential chips, which are developed in an HDL and tested on a supplied\nhardware simulator. For each chip, we provide a skeletal HDL program (listing\nthe chip name, I/O pin names, and functional documentation comprising the chip\nAPI); a test script, which is a sequence of set/eval/compare steps that walk\nthe chip simulation through representative test cases; and a compare file,\nlisting the outputs that a correctly implemented chip should generate when\ntested on the supplied test script (see Figure 2). For each chip developed in\nthe course, the contract is identical: Complete the given HDL skeletal program\nand test it on the hardware simulator using the supplied test script. If the\noutputs generated by your chip implementation are not identical to the\nsupplied compare file, keep working; otherwise, your chip behaves to\nspecification, but perhaps you want to optimize it for efficiency. The chip\nlogic is evaluated and tested on our hardware simulator (see Figure 3).\n\nFigure 2. The specification of each chip consists of a stub HDL file\n(containing the chip signature and an empty PARTS section), a test script, and\na compare file. When evaluated by the hardware simulator, the output file\nproduced by a correctly implemented HDL program should be identical to the\ngiven compare file.\n\nFigure 3. The Hardware Simulator, running/evaluating the HDL program shown in\nFigure 2 (the roles of the various panels are explained in the text in\nparentheses). This particular XOR implementation, which is readable but not\nnecessarily efficient, is based on two NOT, two AND, and one OR chip parts,\neach implemented as a standalone HDL program. When evaluating a chip, the\nsimulator evaluates recursively all its chip parts, all the way down to\nevaluating NAND gates, which have a primitive (built-in) implementation.\n\nPart II: Software. The barebones computer that emerges from Part I of the\ncourse can be viewed as an abstraction that has a well-defined interface: the\nHack instruction set. Using this machine language as a point of departure, in\nPart II of the course we construct a software hierarchy that empowers the Hack\ncomputer to execute code written in high-level programming languages. This\neffort entails six projects that build a compiler and a basic operating system\non top of the hardware platform built in Part I. Specifically, we implement a\nsimple object-based, Java-like language called \u201cJack.\u201d We start this journey\nby introducing the Jack language and the OS (abstractly) and discussing the\ntrade-offs of one-tier and two-tier compilation models. We also emphasize the\nrole that intermediate bytecode plays in modern programming frameworks.\n\nFollowing this general overview, we introduce a stack-based virtual machine\nand a VM language that features push/pop, stack arithmetic, branching, and\nfunction call-and-return primitives. This abstraction is realized in projects\n7 and 8, in which students write a program that translates each VM command\ninto a sequence of Hack instructions. This translator serves two purposes.\nFirst, it implements our virtual machine abstraction. Second, it functions as\nthe back-end module of two-tier compilers. For example, instead of writing a\nmonolithic compiler that translates Jack programs into the target machine\nlanguage, one can write a simple and elegant frontend translator that parses\nJack programs and generates intermediate VM code, just like Java and C#\ncompilers do. Before developing such a compiler, we give a complete\nspecification of the Jack language, and illustrate Jack programs involving\narrays, objects, list processing, recursion, graphics, and animation (the\nbasic Jack language is augmented by a standard class library that extends it\nwith string operations, I/O support, graphics rendering, memory management,\nand more). These OS services are used by Jack programs abstractly and\nimplemented in the last project in the course. In project 9, students use Jack\nto build a simple computer game of their choosing. The purpose of this project\nis not learning Jack, but rather setting the stage for writing a Jack compiler\nand a Jack-based OS.\n\nDevelopment of the compiler spans two projects. We start with a general\ndiscussion of lexicons, grammars, parse trees, and recursive-descent parsing\nalgorithms. We then present an XML mark-up representation designed to capture\nthe syntax of Jack programs. In project 10, students build a program that\nparses Jack programs as input and generates their XML mark-up representations\nas output. An inspection of the resulting XML code allows verifying that the\nparser\u2019s logic can correctly tokenize and decode programs. Next, we discuss\nalgorithms for translating parsed statements, expressions, objects, arrays,\nmethods, and constructors into VM commands that realize the program\u2019s\nsemantics on the virtual machine built in projects 7\u20138. In project 11,\nstudents apply these algorithms to morph the parser built in project 10 into a\nfull-scale compiler. Specifically, we replace the logic that generated passive\nXML code with logic that generates executable VM code. The resulting code can\nbe executed on the supplied VM emulator (see Figure 4) or translated further\ninto machine language and executed on the hardware simulator.\n\nFigure 4. A typical computer game, developed in Jack. The compiled VM code is\nloaded into, and executed by, the VM Simulator shown here. The simulator\ndisplays the VM code, the simulated computer screen, the VM stack and the\nvirtual memory segments, and the host RAM in which they are realized. For\nexample, RAM[0] stores the stack pointer, RAM[1] stores the base address of\nthe local variables segment, and so on.\n\nThe software hierarchy is summarized in Figure 5. The final task in the course\nis developing a basic operating system. The OS is minimal, lacking many\ntypical services, such as process and file management. Rather, our OS serves\ntwo purposes. First, it extends the basic Jack language with added\nfunctionality, like mathematical and string operations. Second, the OS is\ndesigned to close gaps between the software hierarchy built in Part II and the\nhardware platform built in Part I. Examples include a heap-management system\nfor storing and disposing arrays and objects, an input driver for reading\ncharacters and strings from the keyboard, and output drivers for rendering\ntext and graphics on the screen. For each such OS service, we discuss its\nabstraction and API, as well as relevant algorithms and data structures for\nrealizing them. For example, we use bitwise algorithms for efficient\nimplementation of algebraic operations, first-fit/best-fit and linked list\nalgorithms for memory management, and Bresenham\u2019s algorithm for drawing lines\nand circles. In project 12, students use these CS gems to develop the OS,\nusing Jack and supplied API\u2019s. And with that, the Nand to Tetris journey comes\nto an end.\n\nFigure 5. The software hierarchy built in Part II of the course (projects\n7\u201312): A Jack program, consisting of one or more class files, and the OS\n(implemented as a library of Jack classes) are compiled into a set of VM\nfiles. The VM files are compiled further into assembly code, which is\ntranslated by the assembler into binary code. The target code can be executed\nby the computer built in Part I of the course (projects 1\u20135).\n\n## Discussion: Engineering\n\nAbstraction-implementation. A hallmark of sound system engineering is\nseparating the abstract specification of what a system does from the\nimplementation details of how it does it. In Patterson and Hennessy\u2019s \u201cSeven\nGreat Ideas in Computer Architecture,\u201d abstraction is listed at the top of the\nlist.^4 Likewise, Dijkstra describes abstraction as an essential mental tool\nin programming.^1 In Nand to Tetris, the discussion of every hardware or\nsoftware module begins with an abstract specification of its intended\nfunctionality. This is followed by a proposed implementation plan that hints,\nin outline form, how the abstraction can be realized using abstract building\nblocks from the level below (see Figure 1). Here we mean \u201cabstract\u201d in a very\nconcrete way: Before tasking students to develop a hardware or software\nmodule\u2014any module\u2014we guide them to experiment with a supplied executable\nsolution that entails precisely what the module seeks to do.\n\n> In Nand to Tetris, the discussion of every hardware or software module\n> begins with an abstract specification of its intended functionality, and a\n> tool that realizes the abstraction, hands-on.\n\nThese experiments are facilitated by the Nand to Tetris online IDE,^8\ndeveloped by David Souther and Neta London. This set of tools includes a\nhardware simulator, a CPU emulator, a Hack assembler, a Jack compiler, and a\nVM emulator/runtime system that implements our virtual machine and OS. Before\nimplementing a chip, or, when teaching or learning its intended behavior, one\ncan load a built-in chip implementation into the hardware simulator and\nexperiment with it (we elaborate on this \u201cbehavioral simulation\u201d practice\nlater in this article). Before implementing the assembler, one can load\nassembly programs into the supplied assembler and visually inspect how\nsymbolic instructions are translated into binary codes. Prior to implementing\nthe Jack compiler, one can use the supplied compiler to translate\nrepresentative Jack programs, inspect the compiled VM code, and observe its\nexecution on the supplied VM emulator. And before implementing any OS\nfunction, one can call the function from a compiled Jack test program and\ninvestigate its input-output behavior.\n\nThe central role of abstraction is also inherent in all the project materials:\nOne cannot start implementing a module before carefully studying its intended\nfunctionality. Every chip is specified abstractly by a stub HDL file\ncontaining the chip signature and documentation, a test script, and a compare\nfile. Every software module\u2014for example, the assembler\u2019s symbol table or the\ncompiler\u2019s parser\u2014is specified by an API that documents the module along with\nstaged test programs and compare files. These specifications leave no room for\ndesign uncertainty: Before setting out to implement a module, students have an\nexact, hands-on understanding of its intended functionality.\n\nThe ability to experiment with executable solutions has subtle educational\nvirtues. In addition to actively understanding the abstraction\u2014a rich world in\nitself\u2014students are encouraged to discuss and question the merits and\nlimitations of the abstraction\u2019s design. We describe these explorations in the\nlast section of this paper, where we discuss the course\u2019s pedagogy.\n\nModularity. A system architecture is said to be modular when it consists of\n(recursively) a set of relatively small and standalone modules, so that each\nmodule can be independently developed and unit tested. Like abstraction,\nmodularity is a key element of sound system engineering: The ability to work\non each module in isolation, and often in parallel, allows developers to\ncompartmentalize and manage complexity.\n\n> The ability to work on each module in isolation, and often in parallel,\n> allows developers to compartmentalize and manage complexity.\n\nThe computer system built in Nand to Tetris courses comprises many hardware\nand software modules. Each module is accompanied by an abstract specification\nand a proposed architecture that outlines how it can be built from lower-level\nmodules. Individual modules are relatively small, so developing each one is a\nmanageable and self-contained activity. Specifically, the HDL construction of\na typical chip in Part I of the course includes an average of seven lower-\nlevel chip-parts, and the proposed API of a typical software module in Part II\nof the course consists of an average of ten methods.\n\nThis modularity impacts the project work as well as the learning experience.\nFor example, in project 2, students build several chips that carry out Boolean\narithmetic, including a \u201cHalf Adder.\u201d Given two input bits x and y, the half-\nadder computes a two-bit output consisting of the \u201csum bit\u201d and the \u201ccarry\nbit\u201d of x + y. As it turns out, these bits can be computed, respectively, by\nAND-ing and XOR-ing x and y. But what if, for some reason, the student did not\nimplement the requisite AND or XOR chip-parts in the previous project? Or, for\nthat matter, the instructor has chosen to skip this part of the course?\nBlissfully, it does not matter, as we now turn to explain.\n\nBehavioral simulation. When our hardware simulator evaluates a program like\nHalfAdder.hdl that uses lower-level chip-parts, the simulator proceeds as\nfollows: If the chipPart.hdl file (like And.hdl and Xor.hdl) exists in the\nproject directory, the simulator recurses to parse and evaluate these lower-\nlevel HDL programs, all the way down to the terminal Nand.hdl leaves, which\nhave a primitive/built-in implementation. If, however, a chipPart.hdl file is\nmissing in the project directory, the simulator invokes and evaluates a built-\nin chip implementation instead. This contract implies that all the chips in\nthe course can be implemented in any desired order, and failure to implement a\nchip does not prevent the implementation of other chips that depend on it.\n\nUsing another example, a one-bit register can be realized using a data flip-\nflop and a multiplexor. Implementing a flip-flop gate is an intricate art, and\ninstructors may wish to use it abstractly. With that in mind, HDL programs\nthat use DFF chip-parts can be implemented as is, without requiring students\nto implement a DFF.hdl program first. The built-in chip library, which is part\nof our open source hardware simulator, includes Java implementations of all\nthe chips built in the course. Instructors who wish to modify or extend the\nHack computer or build other hardware platforms can edit the existing built-in\nchips library or create new libraries.\n\nBehavioral simulation plays a prominent role in the software projects as well.\nFor example, when developing the Jack compiler, there is no need to worry\nabout how the resulting VM code is executed: The supplied VM emulator can be\nused to test the code\u2019s correctness. And when writing the native VM\nimplementation, there is no need to worry about the execution of the resulting\nassembly code, since the latter can be loaded into, and executed, on the\nsupplied CPU emulator. In general, although we recommend building the projects\nfrom the bottom up in their natural order (see Figure 1), any project in the\ncourse represents a standalone building block that can be developed\nindependently of all the other projects, in any desired order. The only\nrequisite is the API of the level below\u2014that is, its abstract interface.\n\n## Discussion: Pedagogy\n\nA modular architecture and a system specification are static artifacts, not\nplans of action. To turn them into a working system, we provide staged\nimplementation plans. The general staging strategy is based on sequential\ndecomposition: Instead of realizing a complex abstraction in one sweep, the\nsystem architect can specify a basic version, which is implemented first. Once\nthe basic version is developed and tested, one proceeds to extend it to a\ncomplete solution. Ideally, the API of the basic version should be a subset of\nthe complete API, and the basic version should be morphed into, rather than\nreplaced by, the complete version. Such staged implementations must be\ncarefully articulated and supported by staged scaffolding.\n\nStaging is informed by, but is not identical to, modularity. In some cases,\nthe architect simply recommends the order in which modules should be developed\nand tested. In other cases, the development of the module itself is staged.\nFor example, the hardware platform developed in Part I of the course consists\nof 35 modules (standalone chips) that are developed and unit-tested\nseparately, according to staged plans given in each project. In complex chips\nsuch as the ALU, CPU, and the RAM, the implementation of the module itself is\nexplicitly staged. For example, the Hack ALU is designed to compute a family\nof arithmetic/logic functions f(x, y) on two 16bit inputs x and y. In\naddition, the ALU computes two 1bit outputs, indicating that its output is\nzero or negative. The computations of these flag bits are orthogonal to the\nALU\u2019s main logic and can be realized separately, by independent blocks of HDL\nstatements. With that in mind, our project 2 guidelines recommend building and\ntesting a basic ALU that computes the f(x, y) output only, and then extending\nthe basic implementation to handle the two flag bits as well. The staged\nimplementation is supported by two separate sets of ALU stub files, test\nscripts, and compare files.\n\nStaged implementations are also inherent in the software projects in Part II\nof the course. For example, consider the assembler\u2019s development: In stage I,\nstudents are guided to develop a basic assembler that handles assembly\nprograms containing no symbolic addresses. This is a fairly straightforward\ntask: One writes a program that translates symbolic mnemonics into their\nbinary codes, following the Hack machine-language specification. In stage II,\nstudents are guided to implement and unit-test a symbol table, following a\nproposed API. Finally, and using this added functionality, in stage III\nstudents morph the basic assembler into a final translator capable of handling\nassembly code with or without symbolic addresses. Here, too, the separation to\nstages is supported by customized and separate test files: Assembly programs\nin which all variables and jump destinations are physical memory addresses for\nstage I, and assembly programs with symbolic labels for stages II and III.\n\nModularity and staging play a key role in the compiler\u2019s implementation,\nbeginning with the separation into a back-end module (the bytecode-to-assembly\ntranslator developed in projects 7\u20138) and a front-end module (the Jack-to-\nbytecode compiler developed in projects 10\u201311). The implementation of each\nmodule is staged further into two separate projects. In project 7, students\nimplement and test a basic virtual machine that features push/pop and\narithmetic commands only. In project 8, they extend the machine to also handle\nbranching and function calling. In project 10, students implement a basic\ncompilation engine that uses a tokenizer and a parser to analyze the source\ncode\u2019s syntax. In project 11, the compilation engine is extended to generate\ncode. In each of these projects, students are guided to first handle source\ncode that contains constants only, then variables, then expressions, and\nfinally arrays and objects, each accompanied with customized test programs and\ncompare files. For example, when writing the tokenizer and the parser,\nstudents use test programs that process the entire source code and print token\nlists and parse trees. These test programs are unsuitable for later stages,\nsince the fully developed compiler gets the next token on the fly and builds\nthe parse tree dynamically. However, the staged scaffolding is essential for\nturning the compiler\u2019s development from a daunting assignment into a sequence\nof relatively small tasks that can be localized, tested, and graded\nseparately. In general, staged development is one of the key enablers of the\naccelerated pace of Nand to Tetris courses.\n\nDesign. In Nand to Tetris courses, instructors and students play the\nrespective roles of system architects and junior developers. It is unsettling\nto see how, in many non-trivial programming assignments, computer science\nstudents are often left to their own devices, expected to figure out three\nvery different things: how to design a system, how to implement it, and how to\ntest it. As system architects, we eliminate two-thirds of this uncertainty:\nFor each hardware and software module, we supply detailed design\nspecifications, staged implementation plans, and test programs. Students are\nallowed to deviate from our proposed implementation and develop their own\ntests, but they are not permitted to modify the given specifications.\n\n> In Nand to Tetris courses, instructors and students play the respective\n> roles of system architects and junior developers.\n\nClearly, students must learn how to architect and specify systems. We believe,\nthough, that a crucial element of mastering the art of design is seeing many\ngood examples, as done consciously in architecture, law, medicine, and many\nother professional disciplines. In writing workshops, for example, significant\nlearning time is spent reading works of great masters and evaluating\ncritically those of other workshop participants. Why not do the same when\nteaching systems building? In Nand to Tetris courses, engage in dozens of\nmeticulously planned architectures, specifications, and staged implementation\nplans. For many students, this may well be the most well-designed and well-\nmanaged development experience in their careers. Another reason for factoring\nout design and specification requirements to other courses is pragmatic: It\nallows completing the Nand to Tetris journey in one course, giving students a\nunique sense of closure and accomplishment.\n\nFocus. Even when detailed designs and specifications are given, developing a\ngeneral-purpose computer system in one academic course is a tall order. To\nrender it feasible, we make two major concessions. First, we require that the\nconstructed computer system will be fast enough, but no faster. By \u201cfast\nenough\u201d we mean that the computer must deliver a satisfying user experience.\nFor example, if the computer\u2019s graphics are sufficiently smooth to support the\nanimation required by simple computer games, then there is no need to optimize\nrelevant hardware or software modules. In general, the performance of each\nbuilt module is viewed pragmatically: As long as the module passes a set of\noperational tests supplied by us, there is no need to optimize it further. One\nexception is the OS, which is based on highly efficient and elegant\nalgorithms.\n\nIn any hardware or software implementation project, much work is spent on\nhandling exceptions such as edge cases and erroneous inputs. Our second\nconcession is downplaying the former and ignoring the latter. For example,\nwhen students implement a chip that computes an n-bit arithmetic operation,\nthey are allowed to ignore overflow and settle for computed values that are\ncorrect up to n bits. And, when they develop the assembler and the compiler,\nthey are allowed to assume that the source programs contain no syntax errors.\nAlthough learning to handle exceptions is an important educational objective,\nwe believe that it is equally important to assume, at least provisionally, an\nerror-free world. This allows focusing on fundamental ideas and core concepts,\nrather than spending much time on handling exceptions, as required by\nindustrial-strength applications.\n\nThe rationale for these concessions is pragmatic. First, without them, there\nwould be no way to complete the computer\u2019s construction in one semester.\nSecond, Nand to Tetris is a synthesis course that leaves many details to\nother, more specific CS courses. Third, any one of the limitations inherent in\nour computer system (and there are many, to be sure) provides a rich and well-\nmotivated opportunity for extension projects, as described in this article\u2019s\nfinal section.\n\nExploration. Before implementing a hardware or software abstraction, we\nencourage playing with executable solutions. As students engage in these\nexperiments, questions abound. We use these questions to motivate and explain\nour design decisions. For example, how can we rely on the ALU\u2019s calculations\nif it takes a while before they produce correct answers? Answer: When we will\nintroduce sequential logic in the next project, we will set the clock cycle\nsufficiently long to allow time for the ALU circuits to stabilize on correct\nresults. How can we use goto label instructions in assembly programs before\nthe labels are declared? Answer: When we will write the assembler later in the\ncourse, we will present a two-pass translation algorithm that addresses this\nvery issue. When a class Foo method creates a new object of class Bar, and\ngiven that each class is a separate compilation unit, how does Foo\u2019s code know\nhow much memory to allocate for the Bar object without having access to its\nfield declarations? Answer: It does not know, but as you will see when we\nwrite the compiler, the compiled code of the Bar class constructor includes a\ncall to an OS routine that allocates the required memory. Why are assignment\nstatements in the Jack language preceded by a let prefix, as in let x = 1?\nAnswer: This is one of the grammatical features that turns Jack into an LL(1)\nlanguage, which is easier to compile using recursive descent algorithms. And\nwhy does Jack not have a switch statement? Answer: Indeed, this could be a\nnice touch; why not extend the language specification and implement switch in\nyour compiler? And so it goes: Students are invited to question every design\naspect of the architectures and languages presented in the course, and\ninstructors are invited to discuss them critically and propose possible\nextensions.\n\n## Extensions\n\nOptimization. With the exception of the OS, the computer system built in the\ncourse is largely unoptimized, and improving its efficiency is a fertile\nplayground for aspiring hardware and software engineers. We give two examples,\nfocusing on hardware and software optimizations. The n-bit ripple array adder\nchip built in Part I of the course (n = 16) is based on n lower-level full-\nadder chip-parts, each adding up two input bits and a carry bit. In the worst\ncase, carry bits propagate from the least- to the most-significant full-\nadders, resulting in a computation delay that is proportional to n. To boost\nperformance, we can augment the basic adder logic with Carry Look Ahead (CLA)\nlogic. The CLA logic uses AND/OR operations to compute carry bits up the carry\nchain, enabling various degrees of parallel addition, depending on how far we\nare willing to look ahead. Alas, for large n values, the CLA logic becomes\ncomplex, and the efficiency gain of parallel addition may not justify the cost\nof the supplementary look-ahead logic. Cost-benefit analyses of various CLA\nschemes can help yield an optimized adder which is demonstratively faster than\nthe basic one. This optimization is \u201cnice to have,\u201d since the basic design of\nthe adder is sufficiently fast for the course purposes. That said, every\nhardware module built in the course offers improvement opportunities that can\nbe turned into follow-up, bonus assignments that go beyond the basic project\nrequirements. Other examples include instructions requiring different clock\ntimes (IMUL / IDIV), pipelining, cache hierarchy, and more. Built-in versions\nof these extensions can be implemented in our open-ended hardware simulator,\nand then realized by students in HDL.\n\nOne of the software modules built in Part II of the course is a virtual\nmachine. In projects 7\u20138, we guide students to realize this abstraction by\nwriting a program that translates each VM command into several machine-\nlanguage instructions. For example, consider the VM code sequence push a, push\nb, add. The semantics of the latter add primitive is \u201cpop the two topmost\nvalues from the stack, add them up, and push the result onto the stack.\u201d In\nthe standard VM implementation, the translation of each such VM command yields\na separate chunk of binary instructions. Yet, an optimized translator could\ninfer from the VM code that the first two push operations are superfluous,\nreplacing the whole sequence with binary code that implements the single\nsemantic operation push(a + b). Similar optimizations were made by Robert\nWoodhead, at the Hack assembly language level.^10 Such optimizations yield\ndramatic efficiency gains as well as valuable hands-on system-building\nlessons.\n\nThese are just two examples of the numerous opportunities to improve the\nefficiency of the hardware and software platforms built in Nand to Tetris\ncourses. The simplicity of the platforms and the ubiquity of the software\ntools that support the coursework make such analyses and improvements a\nnatural sequel of every lecture and project. Quite simply, once an improvement\nhas been articulated algorithmically or technically, learners have what it\ntakes to realize the extension and appreciate the resulting gains by\nexperimenting with the optimized design in the relevant simulator.\n\nFPGA. In typical Nand to Tetris courses, students build chips by writing HDL\nprograms and executing them on the supplied hardware simulator. Committing the\nHack computer to silicon requires two additional steps. First, one must\nrewrite the HDL programs of the main Hack chips using an industrial-strength\nlanguage, such as Verilog or VHDL. This is not a difficult task, but one must\nlearn the language\u2019s basics, which may well be one of the goals of this\nextension project. Next, using a low-cost FPGA board and open source FPGA\nsynthesis tools, one can translate the HDL programs into an optimized\nconfiguration file that can be then loaded into the board, which becomes a\nphysical implementation of the Hack computer. Examples of such extension\nprojects, including step-by-step guidelines, are publicly available.^8\n\nInput/output. The Hack computer built in the course uses two memory bitmaps to\nconnect to a black-and-white screen and to a standard keyboard. It would be\nnice to extend the basic Hack platform to accommodate a flexible and open-\nended set of sensors, motors, relays, and displays, like those found on\nArduino and Raspberry Pi platforms. This extension can be done as follows.\nFirst, allocate additional maps in the Hack memory for representing the\nvarious peripheral devices. Second, specify and implement an interrupt\ncontroller chip that stores the states of the individual interrupts triggered\nby the various I/O devices. Third, extend the Hack CPU to probe and handle the\noutput of the interrupt controller. Finally, extend the operating system to\nmask, clean, and handle interrupts. We have started working on such\nextensions, but readers may well come up with better implementations.\n\n## Conclusion\n\nWe described Nand to Tetris, an infrastructure for courses that teach applied\ncomputer science by building a general-purpose computer system\u2014hardware and\nsoftware\u2014from the ground up. Nand to Tetris demystifies how computers work and\nhow they are built, engaging students in 12 hands-on projects. Different\ncourses can use different subsets of these projects and implement them in any\ndesired order. Nand to Tetris courses are offered in academic settings that\nseek to combine key lessons from computer architecture and compilation in one\ncourse, and as popular MOOCS taken by many self-learners and developers. Part\nI of Nand to Tetris (hardware) is also suitable for high school CS programs.\nAll Nand to Tetris course materials (lectures, projects, software tools) are\navailable freely in open source^8^,^5 and instructors are welcome to use and\nextend them.\n\n## Acknowledgments\n\nThe chief contributors to the software suite that preceded the online Nand to\nTetris IDE were Yaron Ukrainitz, Nir Rozen, and Yannai Gonczarowski. Mark\nArmbrust, William Bahn, Ran Navok, Yong Bakos, Tali Gutman, Rudolf Adamkovi\u010d,\nand Eytan Lifshitz made other significant contributions. Most of the key ideas\nand techniques underlying Nand to Tetris came from the brilliant mind of my\nfriend and colleague, Noam Nisan.\n\n## References\n\n    * 1\\. Dijkstra, E.W. The humble programmer. Commun. ACM 15, 3 (Oct. 1972), 859\u2013866.\n\n    * 2\\. Nand to Tetris Course Syllabus. Computer Science Dept., Princeton University; https://bit.ly/3raALBk.\n\n    * 3\\. Nisan, N. and Schocken, S. The Elements of Computing Systems. 2nd ed., MIT Press (2021).\n\n    * 4\\. Patterson, D.A. and Hennessy, J.L. Computer Organization and Design RISC-V Edition. 2nd ed., Morgan Kauffman, Cambridge, MA (2021), 11\u201313.\n\n    * 5\\. Schocken, S. and Nisan, N. Nand to Tetris website; https://bit.ly/3XD0Rt4.\n\n    * 6\\. Schocken, S., Nisan, N., and Armoni, M. A synthesis course in hardware architecture, compilers, and software engineering. In Proceedings of the ACM SIGCSE. ACM (Mar. 2009), 443\u2013447.\n\n    * 7\\. Schr\u00f6der, M. FPGA implementations of the Hack Computer; https://bit.ly/3puLCpp, https://bit.ly/3NZIiMu.\n\n    * 8\\. Souther, D. and London, N. Nand to Tetris IDE Online; bit.ly/3wNjeSu.\n\n    * 9\\. Strunk, Jr., W. and White, E.B. The Elements of Style, Macmillan (1959).\n\n    * 10\\. Woodhead, R.J. Optimizing Nand2Tetris assembly code. Medium (Dec. 2023); bit.ly/4acMJfc\n\nShimon Schocken (schocken@runi.ac.il) is a professor at the Efi Arazi School\nof Computer Science, Reichman University, Israel.\n\n  * Share\n\n    * Twitter\n    * Reddit\n    * Hacker News\n  * Download PDF\n  * Print\n  * Join the Discussion\n\nSubmit an Article to CACM\n\nCACM welcomes unsolicited submissions on topics of relevance and value to the\ncomputing community.\n\nYou Just Read\n\n#### Nand to Tetris: Building a Modern Computer System from First Principles\n\nView in the ACM Digital Library\n\n\u00a9 Copyright 2023 held by owner/author.\n\n### DOI\n\n10.1145/3626513\n\nAdvertisement\n\nAdvertisement\n\n### Join the Discussion (0)\n\n#### Become a Member or Sign In to Post a Comment\n\nSign In Sign Up\n\n### The Latest from CACM\n\nExplore More\n\nNews Apr 18 2024\n\nKeeping AI Out of Elections\n\nBennie Mols\n\nArtificial Intelligence and Machine Learning\n\nBLOG@CACM Apr 17 2024\n\nTechnical Marvels\n\nHerbert Bruderer\n\nComputer History\n\nBLOG@CACM Apr 16 2024\n\nThe Value of Data in Embodied Artificial Intelligence\n\nShaoshan Liu\n\nArtificial Intelligence and Machine Learning\n\n### Shape the Future of Computing\n\nACM encourages its members to take a direct hand in shaping the future of the\nassociation. There are more ways than ever to get involved.\n\nGet Involved\n\n### Communications of the ACM (CACM) is now a fully Open Access publication.\n\nBy opening CACM to the world, we hope to increase engagement among the broader\ncomputer science community and encourage non-members to discover the rich\nresources ACM has to offer.\n\nLearn More\n\nTopics\n\n  * Architecture and Hardware\n  * Artificial Intelligence and Machine Learning\n  * Computer History\n  * Computing Applications\n  * Computing Profession\n  * Data and Information\n  * Education\n  * HCI\n  * Philosophy of Computing\n  * Security and Privacy\n  * Society\n  * Software Engineering and Programming Languages\n  * Systems and Networking\n  * Theory\n\nMagazine\n\n  * Latest Issue\n  * Magazine Archive\n  * Editorial Staff and Board\n  * Submit an Article\n  * Alerts & Feeds\n  * Author Guidelines\n\nCommunications of the ACM\n\n  * About Us\n  * Frequently Asked Questions\n  * Contact Us\n  * For Advertisers\n  * Join ACM\n\n\u00a9 2024 Communications of the ACM. All Rights Reserved.\n\n  * Cookie Notice\n  * Privacy Policy\n\nBy continuing to use our website, you are agreeing to our use of cookies. To\nfind out more, please see our Privacy Policy.\n\n", "frontpage": false}
