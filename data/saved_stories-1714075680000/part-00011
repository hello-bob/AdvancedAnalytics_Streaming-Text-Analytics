{"aid": "40156010", "title": "Using Vectors Without a Vector Database", "url": "https://swirlaiconnect.com/blog/using-vectors-without-a-vector-database", "domain": "swirlaiconnect.com", "votes": 1, "user": "srbhr", "posted_at": "2024-04-25 11:14:04", "comments": 0, "source_title": "Using Vectors without a Vector Database", "source_text": "Using Vectors without a Vector Database\n\nSWIRL\n\n# Using Vectors without a Vector Database\n\nSimson Garfinkel - Feb 27, 2024\n\n\u201cYou Do Not Need a Vector Database\u201d is the provocative title of a recent blog\npost (with code) by Dr. Yucheng Low, co-founder of XetHub. In this post, I\u2019ll\nexplain what a vector database is, why Dr. Low says you don\u2019t need one, and\nprovide context to Dr. Low\u2019s answer.\n\n## Background\n\nTo make sense of Dr. Low\u2019s article, it\u2019s important to understand why vectors\nof numbers have become such an important tool for search systems and why that\npopularity can translate into unnecessary expense for organizations that\ndeploy vector-based systems.\n\nVectors of Numbers\n\nThe word \u201cvector\u201d in computer programming means \u201ca sequence of numbers.\u201d For\nexample, this vector represents the location of The White House in Washington\nDC:\n\n[38.897957, -77.036560]\n\nWhile this vector represents characteristics of an electric vehicle:\n\n[38990, 175, 341, 2]\n\nBoth of these vectors represent a point in some mathematical space. The first\nvector represents the location of the White House in a 2-dimensional space\nwhere the first dimension is the latitude and the second dimension is the\nlongitude. The second vector represents the price, range improvement with a\n15-minute charge, the total range, and the number of motors for an electric\nvehicle (in this case, a Tesla Model 3).\n\nClearly, context is required to understand the content and meaning of a\nvector.\n\nBags of Words\n\nFor the first few decades of text processing, one common way to represent the\ncontents of a document was with a vector where each number represented the\nnumber of times a specific word appeared in the text: the first element in the\nvector might count the word \u201cthe\u201d (the most common word), the second \u201cbe\u201d (the\nsecond most common word), and so on.\n\nThis is sometimes called a \u201cbag of words\u201d approach because the vector doesn\u2019t\nrepresent the order in which the words occur: it\u2019s as if you wrote each of the\ndocument\u2019s words onto a note card and put them all into a bag. The sentences\n\u201cI quite like chocolate ice cream\u201d and \u201cI cream the ice like chocolate\u201d each\nhave the same bag-of-words representation, even though they have very\ndifferent meanings.\n\nOne of the curious discoveries about the bag-of-words approach was that even\nthough the vectors had no semantic content, simple mathematical operations on\nthe vectors were semantically relevant. For example, if you created a\ndictionary of the 10,000 most popular words in the English language and used\nit to represent a bunch of documents, each with their own vector, then you\ncould find similar documents by clustering the points in 10,000-dimensional\nspace. Of course, it took decades of research to figure out how to make this\nwork. The ultimate result was the popular search engines and text retrieval\nsystems of the 1990s and early 2000s.\n\nWord Embeddings\n\nOver the past decade, researchers have developed new approaches for\nrepresenting the semantic content of phrases, sentences, and documents. These\napproaches use various techniques to create multidimensional vector spaces\nrepresenting concepts and find where a word, phrase, or entire document\nresides or embeds into that space. The resulting vectors are called\nembeddings.\n\nEnterprise software can easily create word embeddings entirely on enterprise\nservers using high-performance open-source tools like the spaCy open source\nnatural language processing system. There are commercial word embedding\nservices, such as the OpenAI text embedding API. Each system offers multiple\nlanguage models to create embeddings, and the embeddings created with one are\nincompatible with others.\n\nWith word embeddings, you can score the similarity between two documents\nsimply by computing the distance between their embeddings. That \u201cdistance\u201d is\nin a multidimensional space, although the math is similar to computing the\ndistance in two or three dimensions. This makes it possible to find similar\ndocuments similar to an exemplar document by computing the distance between\nyour exemplar\u2019s word embedding and the word embedding for every other document\nin your corpus. Likewise, you can find documents responsive to a question by\nfinding the document with an embedding closest to the embedding of the\nquestion.\n\nVector Databases\n\nVector databases implement this process of storing and operating on\nmultidimensional vectors. For example, you can store a million vectors in a\ndatabase (representing a million documents in your Google Drive, perhaps) and\nhave the database find the ten vectors closest to your vector for your search.\nOr you can give the vector two documents you think are similar (on the same\ntopic) and ask how many other similar vectors (documents) it has stored.\n\nThese are straightforward queries to resolve if the vector has only a single\nelement \u2014 that is if a single number indexes documents. But as vectors get\nlarger, these queries become exponentially more difficult. It\u2019s possible to\nanswer these questions by having the query examine every vector in the\ndatabase, but that is computationally slow. Vector databases like Chroma,\nFaiss, Pinecone, Weaviate, and Qdrant all implement various tricks and\nheuristics to improve performance, although their accuracy can suffer in the\nprocess.\n\n## Low: You don\u2019t need a vector database (for RAG)\n\nWe are now in a position to understand Low\u2019s article!\n\nYucheng Low is the co-founder and CEO of XetHub. This Seattle-area company has\ndata management tools that make it easy for data scientists to manage datasets\nin the gigabyte to petabyte range. He earned his PhD at Carnegie Mellon\nUniversity, where he worked on GraphLab.\n\nThe big idea in Low\u2019s article is that a hybrid approach of combining\ntraditional text retrieval algorithms with vector technology can produce\nbetter search results for a popular AI application called \u201cretrieval augmented\ngeneration\u201d (RAG) than simply using a vector database.\n\nLow\u2019s Demonstration\n\nLow\u2019s article compares three text retrieval approaches:\n\nApproach #1 \u2013 Traditional keyword search with heuristics \u2014 the \u201cBest Match 25\u201d\n(BM25) algorithm developed in the 1980s.\n\nApproach #2 \u2013 Retrieval using just vector embeddings \u2014what you would get with\na high-performance vector database.\n\nApproach #3 \u2013 First using BM25 to retrieve 1000 results, and then picking the\nbest results using vector embeddings.\n\nLow finds that the hybrid approach of using BM25 and then picking the best\nresults (\u201creranking\u201d) using vector embeddings produces better results than\neither BM25 or ranked vector embeddings on their own.\n\nThese findings are important, Low argues because you don\u2019t need a vector\ndatabase if you are only working with a thousand vectors. Modern computers are\nfast enough to sort through a few thousand vectors in multidimensional space\nand find the one that\u2019s the closest match to the question that you\u2019re asking.\nYou only need a vector database for sorting through millions or billions of\nvectors.\n\nWhat\u2019s RAG, Anyway?\n\nRetrieval augmented generation (RAG) is an approach that\u2019s been developed over\nthe past year to improve the usefulness of large language model (LLM) AI\nsystems such as ChatGPT.\n\nRAG solves two important problems with these chatbot systems. First, systems\nimplementing RAG can provide users with the \u201creferences\u201d that back up the\nLLM\u2019s answers. Second, RAG can help keep the ChatBot systems on target and\naligned with the goals of the organization deploying the LLM.\n\nWith RAG, the search engine first takes the user\u2019s question and searches\nthrough a large set of documents, finding parts of each document that might be\nresponsive. These document parts and the user\u2019s question are then provided to\nthe LLM with a prompt that says something along the lines of, \u201cGiven the\nfollowing documents, please answer this question.\u201d\n\n(Remember, it\u2019s important to say \u2018please\u2019 and \u2018thank you\u2019 to your ChatBot \u2014 it\nwill give you better answers if you are polite.)\n\nThe Copilot feature of Microsoft\u2019s Bing search engine uses RAG to improve\nresults and provide links and context. For example, when I ask CoPilot \u201cDo I\nneed a vector database to do RAG?,\u201d the user interface shows me that it first\nuses Bing to search the Internet, then uses a LLM to formulate its answer. At\nthe bottom of the answer are links to websites with more\ninformation\u2014presumably some of the websites that Bing uses as part of the RAG\nprocess.\n\nThe Surprising Conclusion\n\nBecause today\u2019s computers are fast enough to search through thousands of\nvectors, it\u2019s significantly more efficient to do an initial search using\ntraditional search technology and then use vector technology to rank the\nresults.\n\nWhat some people may find surprising about Low\u2019s demonstration is that he also\nshowed that he got better RAG results when using this hybrid approach. But\nthis isn\u2019t surprising to me: hybrid approaches combining multiple AI\ntechniques generally do better than a single, generalized approach.\n\nThis doesn\u2019t mean there is no need for vector databases; you just don\u2019t need\nthem to do RAG.\n\nSWIRL is advanced AI infrastructure software that serves as a crucial link for\nthe enterprise, enabling seamless integration between various data sources,\ngenerative AI technologies, and applications while prioritizing security.\n\n###### Product\n\n  * SWIRL AI Connect\n  * SWIRL Unified Search\n  * Documentation\n\n###### Resources\n\n  * Blog\n  * Connectors\n  * GitHub\n  * Releases\n  * Slack\n  * News\n\n###### Company\n\n  * About\n  * Careers\n  * FAQ\n  * Partners\n\n###### Contact Us\n\n  * Contact Us\n  * Locate Us\n\n\u00a9 SWIRL. All rights reserved.\n\n", "frontpage": false}
