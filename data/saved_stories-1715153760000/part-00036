{"aid": "40292572", "title": "Syllabus: Large Language Models, Content Moderation, and Political Communication", "url": "https://www.techpolicy.press/syllabus-large-language-models-content-moderation-and-political-communication/", "domain": "techpolicy.press", "votes": 2, "user": "MetaMonk", "posted_at": "2024-05-07 23:10:02", "comments": 0, "source_title": "Syllabus: Large Language Models, Content Moderation, and Political Communication | TechPolicy.Press", "source_text": "Syllabus: Large Language Models, Content Moderation, and Political Communication | TechPolicy.Press\n\nHome\n\n# Syllabus: Large Language Models, Content Moderation, and Political\nCommunication\n\nPrithvi Iyer, Justin Hendrix / May 7, 2024\n\nThis piece will be updated sporadically with additional resources. While we\ncannot post every link we receive, we encourage the Tech Policy Press\ncommunity to share material that may be relevant.\n\nShutterstock\n\nWith the advent of generative AI systems built on large language models, a\nvariety of actors are experimenting with how to deploy the technology in ways\nthat affect political discourse. This includes the moderation of user-\ngenerated content on social media platforms and the use of LLM-powered bots to\nengage users in discussion for various purposes, from advancing certain\npolitical agendas to mitigating conspiracy theories and disinformation. It\nalso includes the political effects of so-called AI assistants, which are\ncurrently in various stages of development and deployment by AI companies.\nThese various phenomena may have a significant impact on political discourse\nover time.\n\nFor instance, content moderation, the process of monitoring and regulating\nuser-generated content on digital platforms, is a notoriously complex and\nchallenging issue. As social media platforms continue to grow, the volume and\nvariety of content that needs to be moderated have also increased\ndramatically. This has led to significant human costs, with content moderators\noften exposed to disturbing and traumatic material, which can have severe\npsychological consequences. Moreover, content moderation is a highly\ncontentious issue, driving debates around free speech, censorship, and the\nrole of platforms in shaping public discourse. Critics argue that content\nmoderation can be inconsistent, biased, and detrimental to open dialogue,\nwhile proponents of better moderation emphasize the need to protect users from\nharmful content and maintain the integrity of online spaces. With various\ncompanies and platforms experimenting with how to apply LLMs to the problem of\ncontent moderation, what are the benefits? What are the downsides? And what\nare the open questions that researchers and journalists should grapple with?\n\nIn this syllabus, we examine some of what is known about the use of large\nlanguage models (LLMs) to assist with content moderation tasks, engage in\nvarious forms of political discourse, and deliver political content to users\nwhile also considering the ethical implications and limitations of relying on\nartificial intelligence in this context, and how bad actors may abuse these\ntechnologies.\n\nThis syllabus is a first draft; it will be periodically updated. If you would\nlike to recommend relevant resources to include, do reach out via email.\n\n## AI and Political Communication\n\nIn this section, we track academic research examining the use of generative AI\nfor counterspeech, hate-speech detection, political communication, and to\ncreate and mitigate disinformation campaigns.\n\n### Counterspeech and hate speech detection\n\n  * Ben-Porat, C. S., & Lehman-Wilzig, S. (2020). Political discourse through artificial intelligence: Parliamentary practices and public perceptions of chatbot communication in social media. In The Rhetoric of Political Leadership (pp. 230-245). Edward Elgar Publishing.\n  * Argyle, L. P., Bail, C. A., Busby, E. C., Gubler, J. R., Howe, T., Rytting, C., ... & Wingate, D. (2023). Leveraging AI for democratic discourse: Chat interventions can improve online political conversations at scale. Proceedings of the National Academy of Sciences, 120(41), e2311627120.\n  * Kumar, A. (2024). Behind the Counter: Exploring the Motivations and Perceived Effectiveness of Online Counterspeech Writing and the Potential for AI-Mediated Assistance (Doctoral dissertation, Virginia Tech).\n  * Zhu, W., & Bhat, S. (2021). Generate, prune, select: A pipeline for counterspeech generation against online hate speech. arXiv preprint arXiv:2106.01625.\n  * Cypris, N. F., Engelmann, S., Sasse, J., Grossklags, J., & Baumert, A. (2022). Intervening against online hate speech: A case for automated Counterspeech. IEAI Research Brief, 1-8.\n  * Gabriel, I., Manzini, A., Keeling, G., Hendricks, L. A., Rieser, V., Iqbal, H., ... & Manyika, J. (2024). The Ethics of Advanced AI Assistants. arXiv preprint arXiv:2404.16244.\n  * Tomalin, M., Roy, J., & Weisz, S. (2023). Automating Counterspeech. In Counterspeech (pp. 147-170). Routledge.\n  * Do\u011fan\u00e7, M., & Markov, I. (2023). From Generic to Personalized: Investigating Strategies for Generating Targeted Counter Narratives against Hate Speech. In Proceedings of the 1st Workshop on CounterSpeech for Online Abuse (CS4OA) (pp. 1-12).\n  * Costello, T. H., Pennycook, G., & Rand, D. (2024). Durably reducing conspiracy beliefs through dialogues with AI.\n  * Saha, P., Agrawal, A., Jana, A., Biemann, C., & Mukherjee, A. (2024). On Zero-Shot Counterspeech Generation by LLMs. arXiv preprint arXiv:2403.14938.\n  * Hong, L., Luo, P., Blanco, E., & Song, X. (2024). Outcome-Constrained Large Language Models for Countering Hate Speech. arXiv preprint arXiv:2403.17146.\n  * Jin, Y., Wanner, L., & Shvets, A. (2024). GPT-HateCheck: Can LLMs Write Better Functional Tests for Hate Speech Detection? arXiv preprint arXiv:2402.15238.\n  * Mun, J., Allaway, E., Yerukola, A., Vianna, L., Leslie, S. J., & Sap, M. (2023, December). Beyond Denouncing Hate: Strategies for Countering Implied Biases and Stereotypes in Language. In Findings of the Association for Computational Linguistics: EMNLP 2023 (pp. 9759-9777).\n  * Agarwal, V., Chen, Y., & Sastry, N. (2023). Haterephrase: Zero-and few-shot reduction of hate intensity in online posts using large language models. arXiv preprint arXiv:2310.13985.\n  * Zheng, Y., Ross, B., & Magdy, W. (2023, September). What makes good counterspeech? A comparison of generation approaches and evaluation metrics. In Proceedings of the 1st Workshop on CounterSpeech for Online Abuse (CS4OA) (pp. 62-71).\n  * Gligoric, K., Cheng, M., Zheng, L., Durmus, E., & Jurafsky, D. (2024). NLP Systems That Can't Tell Use from Mention Censor Counterspeech, but Teaching the Distinction Helps. arXiv preprint arXiv:2404.01651.\n  * Jahan, M. S., Oussalah, M., Beddia, D. R., & Arhab, N. (2024). A Comprehensive Study on NLP Data Augmentation for Hate Speech Detection: Legacy Methods, BERT, and LLMs. arXiv preprint arXiv:2404.00303.\n  * Zhang, M., He, J., Ji, T., & Lu, C. T. (2024). Don't Go To Extremes: Revealing the Excessive Sensitivity and Calibration Limitations of LLMs in Implicit Hate Speech Detection. arXiv preprint arXiv:2402.11406.\n  * Nirmal, A., Bhattacharjee, A., Sheth, P., & Liu, H. (2024). Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales. arXiv preprint arXiv:2403.12403.\n  * Roy, S., Harshavardhan, A., Mukherjee, A., & Saha, P. (2023). Probing LLMs for hate speech detection: strengths and vulnerabilities. arXiv preprint arXiv:2310.12860.\n  * Morbidoni, C., & Sarra, A. (2022). Can LLMs assist humans in assessing online misogyny? Experiments with GPT-3.5.\n  * Roberts, E. (2024). Automated hate speech detection in a low-resource environment. Journal of the Digital Humanities Association of Southern Africa, 5(1).\n  * Aldjanabi, W., Dahou, A., Al-qaness, M. A., Elaziz, M. A., Helmi, A. M., & Dama\u0161evi\u010dius, R. (2021, October). Arabic offensive and hate speech detection using a cross-corpora multi-task learning model. In Informatics (Vol. 8, No. 4, p. 69). MDPI.\n  * Guo, K., Hu, A., Mu, J., Shi, Z., Zhao, Z., Vishwamitra, N., & Hu, H. (2023, December). An Investigation of Large Language Models for Real-World Hate Speech Detection. In 2023 International Conference on Machine Learning and Applications (ICMLA) (pp. 1568-1573). IEEE.\n\n### Political campaigns and disinformation\n\n  * Ezzeddine, F., Ayoub, O., Giordano, S., Nogara, G., Sbeity, I., Ferrara, E., & Luceri, L. (2023). Exposing influence campaigns in the age of LLMs: a behavioral-based AI approach to detecting state-sponsored trolls. EPJ Data Science, 12(1), 46.\n  * Bai, H., Voelkel, J., Eichstaedt, J., & Willer, R. (2023). Artificial intelligence can persuade humans on political issues.\n  * Bang, Y., Chen, D., Lee, N., & Fung, P. (2024). Measuring Political Bias in Large Language Models: What Is Said and How It Is Said. arXiv preprint arXiv:2403.18932.\n  * T\u00f6rnberg, P. (2023). Chatgpt-4 outperforms experts and crowd workers in annotating political Twitter messages with zero-shot learning. arXiv preprint arXiv:2304.06588.\n  * Barman, D., Guo, Z., & Conlan, O. (2024). The Dark Side of Language Models: Exploring the Potential of LLMs in Multimedia Disinformation Generation and Dissemination. Machine Learning with Applications, 100545.\n  * Urman, A., & Makhortykh, M. (2023). The Silence of the LLMs: Cross-Lingual Analysis of Political Bias and False Information Prevalence in ChatGPT, Google Bard, and Bing Chat.\n  * Lucas, J., Uchendu, A., Yamashita, M., Lee, J., Rohatgi, S., & Lee, D. (2023). Fighting fire with fire: The dual role of llms in crafting and detecting elusive disinformation. arXiv preprint arXiv:2310.15515.\n  * Sun, Y., He, J., Cui, L., Lei, S., & Lu, C. T. (2024). Exploring the Deceptive Power of LLM-Generated Fake News: A Study of Real-World Detection Challenges. arXiv preprint arXiv:2403.18249.\n  * Mirza, S., Coelho, B., Cui, Y., P\u00f6pper, C., & McCoy, D. (2024). Global-liar: Factuality of LLMs over time and geographic regions. arXiv preprint arXiv:2401.17839.\n\n## LLMs and Content Moderation\n\nIn this section, we provide resources on the opportunities and risks of using\nlarge language models for content moderation. Research on this topic examines\nthe ability of LLMs to classify posts based on a platform\u2019s safety policies at\nscale.\n\n### Tech Policy Press coverage\n\n  * Willner, D., & Chakrabarti, S. (2024, January 29). Using LLMs for policy-driven content classification. Tech Policy Press.\n  * Barrett, P. M., & Hendrix, J. (2024, April 3). Is Generative AI the Answer for the Failures of Content Moderation? Tech Policy Press.\n  * Boicel, A. (2024, April 4). Using LLMs to Moderate Content: Are They Ready for Commercial Use? Tech Policy Press\n  * Iyer, P. (2024, April 3). Transcript: Dave Willner on Moderating with AI at the Institute for Rebooting Social Media. Tech Policy Press.\n\n### Blogs\n\n  * Weng, L., Goel , V., & Vallone , A. (2023, August 15). Using GPT-4 for content moderation.\n  * Leveraging LLMs in Social Media Content Moderation & Analysis | tome01. (n.d.).\n  * Spectrum Labs. (n.d.). AI-Based content moderation: Improving trust & safety online.\n  * Faieq, Z., Sartori, T., & Woodruff, M. (2024, February 15). Supervisor LLM Moderation: Using LLMs to Moderate LLMs.\n  * WhyLabs. (n.d.). Content Moderation with Large Language Models (LLMs).\n  * Cheparukhin. (2023, September 1). How OpenAI\u2019s GPT-4 LLM promises to Reshape Content Moderation. HackerNoon.\n\n### Technical papers released by AI companies\n\n  * Wang, G., Cui, Y., & Li, M. (2023, September 5). Build a generative AI-based content moderation solution on Amazon SageMaker JumpStart | AWS machine learning blog.\n  * Palangi, H., & Ray, D. (2022, May 23). (DE)ToxiGen: Leveraging large language models to build more robust hate speech detection tools. Microsoft Research.\n  * Anthropic. (n.d.). Content moderation. Claude.\n\n### Events\n\n  * Moderating AI and Moderating with AI.\n  * The Future of Content Moderation and its Implications for Governance\n\n### Academic papers\n\n  * Inan, H., Upasani, K., Chi, J., Rungta, R., Iyer, K., Mao, Y., ... & Khabsa, M. (2023). Llama guard: Llm-based input-output safeguard for human-ai conversations. arXiv preprint arXiv:2312.06674.\n  * Shah, Chirag, and Emily M. Bender. \"Envisioning Information Access Systems: What Makes for Good Tools and a Healthy Web?.\" ACM Transactions on the Web (2023).\n  * Gomez, J. F., Machado, C. V., Paes, L. M., & Calmon, F. P. (2024). Algorithmic Arbitrariness in Content Moderation. arXiv preprint arXiv:2402.16979.\n  * Kumar, D., AbuHashem, Y., & Durumeric, Z. (2024). Watch Your Language: Investigating Content Moderation with Large Language Models. arXiv preprint arXiv:2309.14517.\n  * He, Z., Guo, S., Rao, A., & Lerman, K. (2024). Whose Emotions and Moral Sentiments Do Language Models Reflect?. arXiv preprint arXiv:2402.11114.\n  * Qiao, W., Dogra, T., Stretcu, O., Lyu, Y. H., Fang, T., Kwon, D., ... & Tek, M. (2024, March). Scaling Up LLM Reviews for Google Ads Content Moderation. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining (pp. 1174-1175).\n  * Ma, H., Zhang, C., Fu, H., Zhao, P., & Wu, B. (2023). Adapting Large Language Models for Content Moderation: Pitfalls in Data Engineering and Supervised Fine-tuning. arXiv preprint arXiv:2310.03400.\n  * Franco, M., Gaggi, O., & Palazzi, C. E. (2023, September). Analyzing the use of large language models for content moderation with chatgpt examples. In Proceedings of the 3rd International Workshop on Open Challenges in Online Social Networks (pp. 1-8).\n  * Kwon, T., & Kim, C. (2023). Efficacy of Utilizing Large Language Models to Detect Public Threat Posted Online. arXiv preprint arXiv:2401.02974.\n  * Axelsen, H., Jensen, J. R., Axelsen, S., Licht, V., & Ross, O. (2023). Can AI Moderate Online Communities?. arXiv preprint arXiv:2306.05122.\n  * Kolla, M., Salunkhe, S., Chandrasekharan, E., & Saha, K. (2024). LLM-Mod: Can Large Language Models Assist Content Moderation?.\n  * Zhou, X., Sharma, A., Zhang, A. X., & Althoff, T. (2024). Correcting misinformation on social media with a large language model. arXiv preprint arXiv:2403.11169.\n  * Udupa, S., Maronikolakis, A., & Wisiorek, A. (2023). Ethical scaling for content moderation: Extreme speech and the (in) significance of artificial intelligence. Big Data & Society, 10(1), 20539517231172424.\n  * Nicholas, G., & Bhatia, A. (2023). Toward Better Automated Content Moderation in Low-Resource Languages. Journal of Online Trust and Safety, 2(1).\n  * Mullick, S. S., Bhambhani, M., Sinha, S., Mathur, A., Gupta, S., & Shah, J. (2023, July). Content moderation for evolving policies using binary question answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 5: Industry Track) (pp. 561-573).\n  * Murthy, S. A. N. Text Content Moderation Model to Detect Sexually Explicit Content.\n\n## Authors\n\nPrithvi Iyer\n\nPrithvi Iyer is a Program Manager at Tech Policy Press. He completed a masters\nof Global Affairs from the University of Notre Dame where he also served as\nAssistant Director of the Peacetech and Polarization Lab. Prior to his\ngraduate studies, he worked as a research assistant for the Observer Resea...\n\nJustin Hendrix\n\nJustin Hendrix is CEO and Editor of Tech Policy Press, a new nonprofit media\nventure concerned with the intersection of technology and democracy.\nPreviously, he was Executive Director of NYC Media Lab. He spent over a decade\nat The Economist in roles including Vice President, Business Development & ...\n\n## Topics\n\nArtificial IntelligenceContent ModerationDisinformationDemocracy\n\n#### Our content. Delivered.\n\nJoin our newsletter on issues and ideas at the intersection of tech &\ndemocracy\n\n#### Thank you!\n\nYou have successfully joined our subscriber list.\n\nHome\n\nA nonprofit media and community venture intended to provoke new ideas, debate\nand discussion at the intersection of technology and democracy.\n\nTPP RSS feedEmail TPP\n\nAbout\n\nDonate\n\nPrivacy Policy\n\nFellows\n\nContributors\n\nSubmissions\n\nArticles\n\nPodcast\n\nResearch Library\n\nTech Policy Press \u00a9 2023 - a 501(c)(3) organization\n\n", "frontpage": false}
