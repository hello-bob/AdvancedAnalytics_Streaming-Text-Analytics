{"aid": "40274849", "title": "Artificial General Intelligence Is a Red Herring", "url": "https://sibylline.dev/2024/05/06/agi-is-a-red-herring/", "domain": "sibylline.dev", "votes": 2, "user": "CuriouslyC", "posted_at": "2024-05-06 14:06:29", "comments": 0, "source_title": "Artificial General Intelligence is a Red Herring \u00b7 Sibylline.dev", "source_text": "Artificial General Intelligence is a Red Herring \u00b7 Sibylline.dev\n\nAI-powered applications that foster personal growth and well-being\n\n\u00a9 2024. All rights reserved.\n\n### Sibylline.dev Crafting AI for the human journey\n\n# Artificial General Intelligence is a Red Herring\n\n06 May 2024\n\n## Introduction\n\nIf you are into AI or futurism, it\u2019s pretty hard to avoid conversations about\nAGI. When will we achieve AGI? What will AGI be like? Will AGI end the human\nrace? This is a little frustrating, because I think a lot of the energy being\nput into thinking about and working towards AGI is being wasted. To be clear,\nI\u2019m not an AI doomer, I\u2019m very much on the \u201cAI will change the world\u201d\nbandwagon, I just think people are approaching the problem the wrong way.\n\n## The Problem with AGI\n\nPeople throw the term AGI around, but it lacks a clear definition that we can\nrally around. It could range from \u201csmarter than the average human at most\nthings\u201d all the way to \u201csmarter than the smartest humans at everything we can\nmeasure.\u201d If we use the weak definition, our current language models are\narguably AGI. If we use the strongest definition, we might never achieve AGI\nsince the smartest humans will be able to use the AI as a tool and stand on\nits shoulders for better results than the AI could achieve alone in most\ncases. Additionally, I think the intelligence of the model creators biases the\nintelligence of the training corpus in an unavoidable and limiting way.\n\nTo be clear, I\u2019m not saying that model creators can\u2019t create data sets that\nare more intelligent than they are, but it is unavoidable that creating such a\ndata set will be a trial and error process as the model creators lack the\nability to accurately assess superhuman intelligence a priori. That means that\nas AI approaches peak human intelligence we will have to resort to large scale\ndata generation and a global optimization algorithm to select the best data\nset over time in order to continue progressing. That will probably keep things\nmoving slowly, however I expect the progress drop off to be brutal as using\noptimization to explore the space of superintelligent models is definitely\nmuch slower than bootstrapping from human abilities which are themselves the\nresult of genetic algorithms over geological time scales. I imagine that some\nof the cost of running these data set experiments could be mitigated by the\nright architecture and training techniques, so if you want to get ahead of the\ncurve consider how you could reduce the cost of trying and comparing various\ndifferent combinations of data during training.\n\nThere are cases where models learn to be \u201csuperhuman\u201d such as AlphaGo and the\nother reinforcement learning projects from DeepMind, but these are specialized\ntools and the most \u201csuperhuman\u201d thing about them is the amount of energy they\nconsume to outperform humans. Reinforcement learning is no different than a\nperson experimenting while practicing a skill, I suspect AlphaGo has probably\nplayed several orders of magnitude more games than any living human go player,\nand since it wasn\u2019t stuck in the local optimum of human go strategies it was\nable to come up with some very original tricks. A human could have come up\nwith those tricks, but we have limited time to invest in experimentation so we\ntend to select safe experiments, whereas a bot with reinforcement learning and\na trillion dollar company footing the compute bill can try all sorts of stupid\nshit on the small chance that it might be accidentally brilliant.\n\nThese reinforcement learning systems aren\u2019t so much a path to AGI as\nspecialized knowledge creation engines for domains with a succinctly\ndescribable objective. You might argue that this approach could be scaled to\ncreate AGI, but it has the same problem as language models - the intelligence\nof the model is limited by the intelligence of the model\u2019s creators. In\nlanguage models, the problem is assembling a superhuman corpus of training\ndata, with reinforcement learning the problem is creating a superhuman\nobjective. You might suggest a variation of the same trick I talked about\nbefore - using another model to come up with the AGI objective, but I suspect\nthat is just shuffling the complexity around rather than actually solving the\nproblem. Perhaps we could throw enough compute at the problem to solve it\nusing the \u201cinfinite monkeys on infinite typewriters produce the complete works\nof Shakespeare\u201d strategy, but there\u2019s a reason people use that meme\npejoratively.\n\n## AGI Maybe never?\n\nIn fact, to strengthen the argument that if AGI means \u201cbetter than the\nsmartest humans at literally everything,\u201d there\u2019s a good chance we will never\nachieve it, I want to call attention to something that futurists tend to miss\n- specifically that progress isn\u2019t exponential, but sigmoidal. Exponential\ngrowth is impossible in any finite system - as the system approaches the\nlimits of its capacity, growth becomes logarithmic. That means you can get to\n80% of the limit of a sigmoidal function in pretty short order, but getting to\n99% or 99.999% is a very different feat. The people who\u2019ve worked on self-\ndriving cars know what I\u2019m talking about - the core technology has been\nfunctional for ~15 years but we\u2019re still getting crushed by the long tail\ncomplexity of the real world task. We\u2019re going to run into this phenomenon\nagain and again as we create tools to automate various aspects of human\nbehavior.\n\nFurthermore, the \u201cno free lunch theorem\u201d (NFLT) becomes more of an issue the\nmore general your model becomes. In simple terms, the NFLT states that there\nis no single model or algorithm that can outperform all others across all\npossible tasks or datasets. Think of it like a restaurant menu - just as there\nis no single dish that is the best choice for every person, every time, there\nis no single model that is the best choice for every problem, every time.\nInductive biases that improve inference for one set of problems will\nnecessarily make it worse for others. Eventually trying to make one model good\nat everything is going to become a game of whack-a-mole with endless\nperformance regressions unless you submit to parameter explosion.\n\nIf you want to see the no free lunch theorem in action, just look at how\nOpenAI \u201clobotomized\u201d ChatGPT - they optimized it to be better at logic and\nquestion answering and it caused a significant deterioration in its creative\nwriting skills. Likewise, if you look at the current generation of Llama3 fine\ntunes, they also tend to show deterioration in high level logic and reasoning\nin exchange for better role play abilities. That sort of \u201crobbing Peter to pay\nPaul\u201d effect is going to become more common as models become more optimized,\nwhich means monolithic models will have to get much larger to be superhuman at\nmore things. That isn\u2019t efficiently scalable even if we assume better\narchitectures, so specialization is the likely reality. Finally, model\nintelligence seems to scale logarithmically with additional parameters serving\nmore to bake in knowledge than increase intelligence, so it\u2019s questionable how\nviable larger monolithic models are as a strategy anyhow.\n\n## A Task-Centric Approach to AI\n\nSo, if AGI is a red herring and trying to build more and more intelligent\nmonolithic models to reach AGI is a fool\u2019s errand, what should we be doing\ninstead? I\u2019d like to propose that we stick to a task-centric view of AI -\ngiven a specific task, how well does the model perform the task compared to\naverage and optimal human performance? This is a very straightforward problem\nto understand, and achieving near optimal human performance is much easier\nwhen the bounds of the task are constrained. If there\u2019s a task current models\ndon\u2019t do well, we can funnel resources into creating new models that can\nperform that task.\n\nI can imagine a lot of readers are bristling at this suggestion, but there\u2019s a\nvery good reason for it. If there is a specialized tool for every known task a\nhuman might undertake that achieves near human optimal performance, the task\nof creating \u201cAGI\u201d becomes the task of creating an agent that is just smart\nenough to select the right tools for the problem at hand from a model catalog\nand large language models are pretty close to that level already. This\napproach also leaves the tools accessible to humans rather than hiding them\naway in a giant model, and if new tasks emerge or existing tasks change you\ndon\u2019t need to retrain that giant model to be good at them - just update the\nmodel catalog with a new specialized tool.\n\nBeyond the benefits I just mentioned, this path has the benefit of being\nclearly evaluable and more tractable to solve. If we managed to create AGI in\na monolithic manner, we would still need a vast suite of benchmarks\nencapsulating all the tasks humans are capable of in order to prove that it\nwas \u201cAGI.\u201d Since we need this exhaustive suite of benchmarks anyhow, why not\njust build to that in the first place? Any engineer worth their salt can tell\nyou that a system with thousands of functions that each do one thing well is\neasier to build and maintain than a system with one function that can do\nthousands of things when invoked with the correct arguments.\n\n## The Path Forward\n\nSo, if we\u2019re really serious about building AGI, what should we be doing today?\n\n  1. The first thing we need is an improved suite of truly exhaustive benchmarks. If our definition of AGI is \u201cbetter than most humans at everything,\u201d we can\u2019t even make coherent statements about that until we have the ability to measure our progress, and a deep benchmark suite is the starting point. This will entail moving beyond simple logical problems to complex world simulating scenarios, adversarial challenges and gamification of a variety of common tasks.\n\n  2. Secondly, stop trying to train supermodels that know everything, pick a problem domain that AI isn\u2019t good at yet and really hammer it with specialized models until we have tools that match or exceed the abilities of the best humans in that area. If you really want to push the boundaries of general foundation models, consider working on making them more efficient or easily tuneable/moddable. I also think there is a lot of low hanging fruit in transfer learning between languages, and any improvements there will make models better at coding and math as a side effect.\n\n  3. Finally, we need to design asynchronous agents that are capable of out of band thought/querying and online learning (in the optimization sense) and have been trained to identify sub-problems, select tools from a catalog to answer them, then synthesize the final answer from those partial answers. The agent could convert sub-problems into embeddings then use their positions in embedding space to select the models to process them with. I suspect this agent could be created with a multimodal model having a \u201cthought loop\u201d which can trigger actions based on the evolution of its objectives over time, with most actions being tool invocations producing output that further informs those objectives.\n\nThere is very little uncertainty in this path, no model regressions, no\nmeasurement problem and no need for supergenius engineers, only the steady\nratcheting of human cultural progress. Beyond that, this method will put the\nwhole \u201cwhen will we have AGI\u201d debate to bed because we\u2019ll be able to track the\ncoverage of tasks where AI is better than human, we\u2019ll be able to track the\ncreation of new tasks and the rate at which existing tasks fall to AI and use\na simple machine learning predictor to get a solid estimate for the exact date\nwhen AI will be better than humans at \u201ceverything.\u201d\n\n## The Sibylline Circle\n\nJust to give you an idea of how we can track this progress in a way that\u2019s\neasy for humans to quickly grok, here\u2019s an example visualization. Imagine that\neach bar is a benchmark for a specific task:\n\nOver time the number of individual benchmarks is likely to get so large that\nthe chart needs to be rendered as a line chart rather than bars, but to avoid\nlosing information, benchmarks could be clustered and sorted based on\ntopic/type of task/etc with coloring for each cluster so that the chart is\nstill easily interpretable.\n\nSince I\u2019ve always wanted to name something in public discourse, I\u2019m going to\ncall this measure of AGI progress the \u201csibylline circle.\u201d Since AI is oracular\nin both senses and sibylline can also indicate a double meaning, I think it\nfits and it\u2019s a fair bit of self promotion.\n\nNote that the sibylline circle could also be a useful tool for people using\nAI, as it would make identifying the areas where AI is unreliable easier.\n\n## Related posts\n\n  * ### Introducing Jung, your AI powered personality analyst! 29 Apr 2024\n\n", "frontpage": false}
