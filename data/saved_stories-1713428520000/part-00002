{"aid": "40071898", "title": "Parsing and All That", "url": "https://blog.jeffsmits.net/compsci/2024/04/07/parsing-and-all-that/", "domain": "jeffsmits.net", "votes": 1, "user": "foresterre", "posted_at": "2024-04-18 01:01:28", "comments": 0, "source_title": "Parsing and all that", "source_text": "Parsing and all that\n\nWhatever\n\n# Parsing and all that\n\nApr 7, 2024\n\nHello again! I\u2019m picking up my series on Automata, with this post that goes\ninto what I had always meant to get to: parsers. We\u2019ll check out the old-\nschool linear time parsing algorithms, which only need to go over the input\nonce, without backtracking or caching. We\u2019ll check out LL and LR, parse\ntables, recursive descent and recursive ascent. Welcome to the world of\ndeterministic parsing...\n\n# Refresher from Pushy Automata\n\nWe\u2019ll start with a brief refresher from the previous post of the series, pushy\nautomata, since that was a little while back.\n\n## Push-Down Automata\n\nPush-down automata (PDAs) are automata with a stack. Normal finite automata\njust consume input and have fixed memory via their states. PDAs can remember\nthings on a single stack too, by pushing onto it and popping from it. Here\u2019s a\ndeterministic PDA for recognising the language of words that start with\nzeroes, followed by an equal number of ones:\n\nSo we start at q0, see if there is a 0 as input, ignore the top of the stack,\nand put a $ on the stack as a marker for the end of the stack. Now we\u2019re in\nstate q1, in which we can consume more zeroes from the input and put those on\nthe stack. If we find a one as input, we remove a zero from the stack by not\npushing anything new on the stack. Now we\u2019re in state q2 where we remove\nzeroes from the stack for every one in the input, until we consume the final\none by removing the $ from the stack.\n\n> Aside: This is one of the examples from the old blog post, and I now see\n> that it is missing a transition! This automaton rejects the input 01,\n> because there is no transition q11, $ \u2192 \u03b5q3. Oops ^_^\n\n## Context-Free Grammars, Derivations, and a naive PDA translation\n\nA context-free grammar that describes the above language is:\n\nS=0S1| (step)  \n---|---  \nS=\u03b5| (\u03b5)  \n  \nSort S is the start symbol, the starting point in the grammar. If we\u2019re using\nthe grammar productively we start from the start symbol and use the rules\nleft-to-right to replace sorts until we get the sentence in the corresponding\ncontext-free language that we want. Something like: S\u21920S1\u219200S11\u21920011. This is\ncalled a derivation.\n\nThe most general, naive way to derive a push-down automaton for any context-\nfree grammar is by pushing the end-of-stack and start symbol at the start,\nhaving a \u201cmain\u201d state that uses the grammar rules with the body reversed (to\ndeal with the stack order), and an accept state that pops the end-of-stack:\n\nHere the stack grows left-to-right, so the lowest symbol on the stack is $\n(end of stack), followed by S (the grammar start symbol). By the rules of the\ngrammar we can manipulate the top of the stack and rewrite it to the body. If\nthe input lines up with what we have on the stack, we can eliminate both. It\u2019s\nsimple, but inefficient because of all the nondeterminism.\n\n## Derivations, Parse Trees and Ambiguity\n\nLet\u2019s look at a slightly more interesting grammar from a parser perspective:\n\nS=S+S| (add)  \n---|---  \nS=S\u2217S| (mul)  \nS=1| (\u03b5)  \n  \nWhen you want to derive 1+1\u22171, you can do this in all manner of ways. The\nfollowing derivation picks just an arbitrary sort on which to apply a rule\nfrom the grammar:\n\n  1. S (first S)\n  2. S+S (first S)\n  3. 1+S (first S)\n  4. 1+S\u2217S (second S)\n  5. 1+S\u22171 (first S)\n  6. 1+1\u22171\n\nNotice how in some steps the leftmost S was replaced, while in others the\nrightmost was replaced. Generally speaking, you\u2019ll want either a leftmost or a\nrightmost derivation for parsers, which is to say: a grammar rule is always\napplied to the leftmost or rightmost sort. There are three reasons for this.\nThe first is that you want a parser to be predictable in when it applies\ngrammar rules, as you may connect so-called semantic actions to each rule.\nThese are pieces of code that are run when the parser applies the particular\nrule. (A typical example is a simple calculator). Such actions could perform\nside-effects, therefore order matters. For this reason, leftmost vs rightmost\ncan also be observed. Two other reasons you to want this predictable\nderivation order is ease of implementation, and ease of proving things about\nyour algorithm. These last two care less for whether it\u2019s leftmost or\nrightmost.\n\nThe most common semantic actions I\u2019m aware of is to build a syntax tree with a\nparser. This builds a tree structure out of the parsed text. A parse tree, or\nconcrete syntax tree, contains all the rule applications as seen in the\ngrammar. An abstract syntax tree abstracts over some parts of the syntax tree,\nsuch as leaving out whitespace, or parentheticals (the shape of the tree\ncaptures the precedence anyway), or injections (grammars rules of the form\nS1=S2). Let\u2019s look at some parse trees of the last example, 1+1\u22171:\n\nNotice how the leaves of the two trees are in the same order left-to-right as\nthe input, but for the left tree the plus is higher up in the tree while in\nthe right tree the star is higher up. If we want to interpreter the input as\nsimple arithmetic, where multiplication binds tighter than addition, the left\ntree is the one we want. This is the predecedence of the operators, \u2217>+.\n\nWhen you can get multiple trees like this, the grammar is called ambiguous.\nMore formally, if you use only leftmost derivations (or only rightmost) and\nstill find two distinct derivations that give the same sentence, the grammar\nis ambiguous. So to be clear: the above trees can be created with only\nleftmost derivations, it\u2019s not a matter of choosing one or the other for the\ntwo trees. Derivation order (leftmost or rightmost) has to do with side-effect\norder of semantic actions only. When you build trees you don\u2019t really need\nside-effects, so the derivation order has no effect on it.\n\nWith that recap out of the way: For the purposes of this blog post, we\u2019ll look\nat unambiguous grammars for the languages we want to parse. Still, whether you\nuse leftmost derivation or rightmost derivation in a parser that parses\nunambiguous grammars matters quite a lot in terms of what languages you can\ndescribe deterministically. It also influences how easily you can write a\nparser by hand for such a grammar, and how easily you can (programmatically)\nexplain why your parser doesn\u2019t accept certain inputs (parser error messages).\nSo let\u2019s have a look at LL and LR parsing techniques, where the first L in\nthose abbreviations stands for Left-to-right (as in reading direction in\ntext), and the second letters are respectively leftmost and rightmost\nderivation.\n\n# Topdown, (Strong) LL parsing\n\nTo take a good look at LL parsing, we will first work with a grammar that is\nnot ambiguous or left-recursive:\n\nS=F| (1)  \n---|---  \nS=(S+F)| (2)  \nF=a| (3)  \n  \nSo sort S is the start symbol, we also have sort F, and we have round\nbrackets, plusses, and a\u2019s. This is enough information to create a table that,\nbased on (1) the next sort to be parsed and (2) the next symbol in the input,\npredicts which rule from the grammar to use to parse the input further. In\nother words, if you know where you are in the input and grammar, you can look\nahead at the next symbol of input and tell which unique grammar rule predicts\nthe next bit of input (assuming the input fits the grammar). The table for the\nabove grammar looks like so:\n\n(| a  \n---|---  \nS| 2| 1  \nF| 3  \n  \nA table like the above is an LL(1) parse table, because it uses only 1 symbol\nof \u201clook-ahead\u201d in the columns. LL(1) grammars are always strong LL grammars,\nwhich means that they only need the combination of the sort to be parsed and\nthe next symbol(s) to decide on a unique grammar rule to apply. In general,\nLL(k) grammars do not have to be strong, and if they are not, you also need to\nknow what was already parsed from the input (the \u201cleft context\u201d) in order to\nchoose a unique grammar rule^1. For example, the following grammar is LL(2),\nand not strong:\n\nS=A a b A b a| (1)  \n---|---  \nA=a| (2)  \nA=| (3)  \n  \nYou can see this if you try to write an LL(2) parse table for it:\n\na a| a b| b a  \n---|---|---  \nS| 1| 1  \nA| 2| 2,3| 3  \n  \nIf you look ahead to a b on the input, and the next sort is A, then it really\ndepends on whether you are at the start of the input or in the middle of rule\n1. If you\u2019re at the start, you must choose rule 3 so you can parse a b as part\nof the rule 1, but if you\u2019re already in the middle of rule 1, you must choose\nrule 2 for A so you can continue to parse b a of rule 1.\n\nIf you mark A in rule 1 with where you are in rule 1 (S=A1 a b A2 b a), you\nget an LL(2) grammar that is strong, although the table for it is larger^2:\n\na a| a b| b a  \n---|---|---  \nS| 1| 1  \nA1| 2| 3  \nA2| 2| 3  \n  \nIn general, you can always use this trick to construct a strong, structurally\nequivalent LL grammar with the same look-ahead. This is quite useful for\nconstructing simple LL parsers. However, the downside of these parsers is that\non wrong input they can fail later than a more complicated LL(k) parser that\nworks for the non-strong grammar. And that matters if you want to give nice\nerror messages.\n\n### An intuition for table construction by automaton\n\nBuilding the above tables was a matter of keeping in mind what they mean, and\nsquinting a little. But in the case of a larger grammar, or a parsetable\ngenerator, of course you want an exact process. Before I dive into First and\nFollow sets that are the traditional method for building these tables, let me\ngive you a method that is less practical but in my opinion more intuitive.\n\nStep 1: Let\u2019s build a simple automaton for each rule of the grammar, where we\nassume both sorts and terminals are on the input.\n\nNote how each node of a rule automaton has the number of the rule followed by\nthe offset into the body of the rule. The state represents where we are in the\nrule while parsing by that rule. The last node doesn\u2019t have this offset so you\ncan easily identify it, even when it\u2019s no longer a final state.\n\nTypically you\u2019ll find texts on parsers display the position in a rule more\nvisually with \u201cLR item\u201d notation. This uses the actual rule and a dot to\nindicate where we are in the rule. While this makes individual automata states\nmore readable, it makes the automata large and therefore harder to display in\na readable way as a whole. That\u2019s why you won\u2019t find that notation in this\npost\u2019s automata. But just to illustrate an example of the notation:\n\nShorthand in this blog| LR Item notation  \n---|---  \nS10| S=. A a b A b a  \nS11| S=A . a b A b a  \nS15| S=A a b A b . a  \nS1| S=A a b A b a .  \n  \nStep 2: Now instead of consuming a sort with an automaton, we\u2019ll use \u03b5 rules\nto jump to the automata of the rules for that sort instead. We\u2019ll use the PDA\nstack with unique labels to get back to where you would be after consuming the\nsort.\n\nThe \u2193X is an abbreviation for an \u03b5,\u03b5\u2192X edge that pushes a symbol on the stack\nunconditionally, it was hard to get graphviz to cooperate on node placement of\nthis graph otherwise... Now at every node that had a sort transition you have\nmultiple transition options, these are where you need to look ahead. Soooo...\n\nStep 3: at a sort transition node, for each \u2193 transition, follow transitions\nuntil you\u2019ve consumed k terminals (2 in this example) from the input. These\nare the terminals of the column in the parse table, and the rule of the \u2193\ntransition gets put into that cell. You can also put the look-ahead into the\nautomaton:\n\n### Building LL tables for strong LL grammars by traditional method\n\nWhile the above building of automata gives a visual intuition, it\u2019s not the\nmost efficient way to express how we can build LL tables. The traditional\nmethod does the same thing in essence, but using some pre-computed sets to\ncalculate the cells in the table.\n\nA cell at the row labeled with sort A and the column labeled with terminal(s)\nv should have the grammar rule A=w (where w is a mix of terminals and sorts or\n\u03b5), under the following condition: v is in the First set of w, or \u03b5 is in the\nFirst set of w and v is in the Follow set of A. In other words:\nv\u2208First(w)\u22c5Follow(A)\n\nLet\u2019s unpack that. The First set of a terminal is a singleton set with that\nterminal. The First set of a sort is the set of first non-terminals that the\nsort can expand to, directly or indirectly. So a rule A=a[...] causes a to\nappear in the First set of A, A=B[...] causes the First set of B to be\nincluded in the First set of A, and A=\u03b5 causes \u03b5 to appear in the First set of\nA. This last rule says A can be expanded to \u201cnothing\u201d, so if that\u2019s an option\nwe need to check the Follow set of A.\n\nThe Follow set is basically every non-terminal that can follow A in the\ngrammar. So when you have B=[...]A a[...], a is in the Follow set of A. A rule\nB=[...]A causes the Follow set of B to be included in the Follow set of A. And\nthe Follow set of the start symbol has the end-of-file \u2018terminal\u2019 $.\n\nThe Follow set is basically every non-terminal that can follow A in the\ngrammar. So when you have B=[...]A a[...], a is in the Follow set of A. A rule\nB=[...]A causes the Follow set of B to be included in the Follow set of A. And\nthe Follow set of the start symbol has the end-of-file \u2018terminal\u2019 $.\n\nFinally, there is the dot operator between the First and Follow sets: this is\na truncated product, that takes every combination of the two sets, sticks them\ntogether (in order), and truncates to length k. That\u2019s a bit of an abstraction\nover the k in LL(k), which I didn\u2019t take into account in the explanation of\nFirst and Follow sets. The First sets should have length k strings of course,\nand so you may need to take more First/Follow sets into account when computing\nthese. Another thing I glossed over is that we actually use the First set of\nw, a mix of terminals and sorts on the right-hand side of our grammar rules.\nIf w is a A B b, then its First set is {a}\u22c5First(A)\u22c5First(B)\u22c5{b}.\n\nOk, with that all done, we can use those tables. But before we do, a quick\nword about expressive power, because LL is not particularly powerful...\n\n### Limitations and Expressive power\n\nThere are always languages that cannot be captured by an LL(k) grammar that\ncan be captured by an LL(k+1) grammar. In other words, look-ahead size is\nimportant in the expressivity of an LL grammar, and LL(k) for any specific k\ndoes not capture all deterministic context-free languages^3.\n\nWe\u2019ve already seen an example of an LL(2) grammar, but what would be a\nlanguage that cannot be captured by any LL(k)? Take the language of a\u2019s\nfollowed by b\u2019s, where the number of a\u2019s \u2265 the number of b\u2019s. Or as a grammar:\n\nS=aS| (1)  \n---|---  \nS=A| (2)  \nA=aAb| (3)  \nA=\u03b5| (4)  \n  \nThe problem for LL here is that we would have to look ahead in the input until\nwe read the entire input before we could decide whether we can start consuming\nthe input with Rule 1 or Rule 2 (and then Rule 3).\n\nThere is a class of grammars called LL-regular (LLR) grammars captures all\nLL(k) grammars for any k and slightly more. These LLR grammars are cool in\nthat they are still parseable in linear time, as long as you have something\ncalled a \u201cregular partition\u201d of your grammar. Getting that is an undecidable\nproblem though. And since there is an LR(1) grammar that is not in LLR, this\nstuff is the trifecta of confusing, impractical, and less powerful^4 than a\nmuch more useful technique that we will cover later in this post: LR. But\nfirst, going from tables to parsers!\n\n## Predictive Parsing\n\nSince we already know what the tables mean, we can write a simple parse table\ninterpreter to finish our predictive parser. The parser is called predictive\nbecause based on the k look-ahead terminals, we decide the grammar rule to use\nto continue parsing, which typically predicts some of the structure of the\ninput well beyond the part we peeked at for the look-ahead.\n\nOk, let\u2019s write a quick parse table interpreter for our LL(2) example. We\u2019ll\nstart with some definitions.\n\n    \n    \n    use std::collections::HashMap; use std::env; use lazy_static::lazy_static; use peekmore::PeekMore; type Terminal = char; #[derive(Clone, Copy, Debug, Eq, Hash, PartialEq)] enum Sort { S, A1, A2, } enum Symbol { Sort(Sort), Terminal(Terminal), } #[derive(Debug, Eq, PartialEq)] enum Rule { S, Aa, AEpsilon, }\n\nThe imports become useful in a second, for now we have our terminals, sorts, a\ncombination type Symbol, and the names of our grammar rules. Assuming we keep\naround a proper PDA stack of symbols, we can write our grammar rules now:\n\n    \n    \n    impl Rule { fn apply(&self, stack: &mut Vec<Symbol>) { match self { Rule::S => Self::s(stack), Rule::Aa => Self::aa(stack), Rule::AEpsilon => Self::a_epsilon(stack), } } fn s(stack: &mut Vec<Symbol>) { stack.push(Symbol::Terminal('a')); stack.push(Symbol::Terminal('b')); stack.push(Symbol::Sort(Sort::A2)); stack.push(Symbol::Terminal('b')); stack.push(Symbol::Terminal('a')); stack.push(Symbol::Sort(Sort::A1)); } fn aa(stack: &mut Vec<Symbol>) { stack.push(Symbol::Terminal('a')); } #[allow(clippy::ptr_arg)] fn a_epsilon(_stack: &mut Vec<Symbol>) {} }\n\nClippy is great for catching all kinds of poor code, but for consistency I\u2019ve\nchosen to #[allow] this time. Note that to effectively run a context-free\ngrammar on a PDA, you need to push the symbols in your rules on the stack in\nreverse, as mentioned in the recap.\n\n    \n    \n    lazy_static! { static ref TABLE: HashMap<(Sort, Terminal, Terminal), Rule> = { let mut table = HashMap::new(); assert_eq!(None, table.insert((Sort::S, 'a', 'a'), Rule::S)); assert_eq!(None, table.insert((Sort::S, 'a', 'b'), Rule::S)); assert_eq!(None, table.insert((Sort::A1, 'a', 'a'), Rule::Aa)); assert_eq!(None, table.insert((Sort::A1, 'a', 'b'), Rule::AEpsilon)); assert_eq!(None, table.insert((Sort::A2, 'a', 'b'), Rule::Aa)); assert_eq!(None, table.insert((Sort::A2, 'b', 'a'), Rule::AEpsilon)); table }; }\n\nNothing very special really, just encoding what we had already. The main parse\nloop is also very unexciting now that we have implemented most of the logic of\nthe grammar already. We basically manage the stack, eliminating terminals on\nthe stack with those from the input and applying rules from the table based on\nsort and look-ahead, and give errors if we get unexpected input:\n\n    \n    \n    pub fn lex(input: String) -> Vec<Terminal> { input.chars().collect() } pub fn main() -> Result<(), String> { let input = env::args().next().expect(\"Argument string to parse\"); let input = lex(input); let mut input = input.iter().peekmore(); let mut stack = Vec::new(); stack.push(Symbol::Sort(Sort::S)); while let Some(symbol) = stack.pop() { return match symbol { Symbol::Terminal(predicted) => { if let Some(&&actual) = input.next() { if predicted == actual { continue; } Err(format!( \"Expected terminal {predicted:?}, but got {actual:?}.\" )) } else { Err(format!(\"Expected terminal {predicted:?}, but got EOF.\")) } } Symbol::Sort(sort) => { if let &[Some(&term1), Some(&term2)] = input.peek_amount(2) { if let Some(r) = TABLE.get(&(sort, term1, term2)) { r.apply(&mut stack); continue; } else { Err(format!( \"Unexpected {term1:?} {term2:?} while parsing {sort:?}\" )) } } else { Err(\"Unexpected end of input.\".to_owned()) } } }; } Ok(()) }\n\n## Recursive Descent\n\nBy encoding the parse table in data, we get some amount of interpretive\noverhead. We have a parse table interpreter with a stack we manage ourselves,\nbut the stack is not really used any different from a call stack. So what if\nwe use function calls instead? That\u2019s the idea of recursive descent parsing.\nIt actually makes our code smaller and more straight-forward, which is why\nit\u2019s so popular as a technique for hand-written parsers.\n\n    \n    \n    use std::env; use peekmore::PeekMore; use peekmore::PeekMoreIterator; type Iter<'a> = PeekMoreIterator<std::slice::Iter<'a, Terminal>>; type Terminal = char; fn consume(input: &mut Iter, predicted: Terminal) -> Result<(), String> { if let Some(&actual) = input.next() { if actual == predicted { Ok(()) } else { Err(format!( \"Expected terminal {predicted:?}, but got {actual:?}.\" )) } } else { Err(\"Unexpected end of file.\".to_owned()) } }\n\nThis time we only need terminals as a type, the rest is gone, and so is the\nhashmap import for the parsetable. We will need the input, and be able to\nremove predicted terminals from it, so consume comes in handy.\n\n    \n    \n    fn sort_s(input: &mut Iter) -> Result<(), String> { // S match input.peek_amount(2) { &[Some('a'), Some('a')] => s(input), &[Some('a'), Some('b')] => s(input), &[term1, term2] => Err(format!(\"Unexpected {term1:?} {term2:?} while parsing S\")), _ => Err(\"Unexpected end of file.\".to_owned()), } } fn sort_A1(input: &mut Iter) -> Result<(), String> { // A1 match input.peek_amount(2) { &[Some('a'), Some('a')] => a_a(input), &[Some('a'), Some('b')] => a_epsilon(input), &[term1, term2] => Err(format!(\"Unexpected {term1:?} {term2:?} while parsing A\")), _ => Err(\"Unexpected end of file.\".to_owned()), } } fn sort_A2(input: &mut Iter) -> Result<(), String> { // A2 match input.peek_amount(2) { &[Some('a'), Some('b')] => a_a(input), &[Some('b'), Some('a')] => a_epsilon(input), &[term1, term2] => Err(format!(\"Unexpected {term1:?} {term2:?} while parsing A\")), _ => Err(\"Unexpected end of file.\".to_owned()), } } fn s(input: &mut Iter) -> Result<(), String> { sort_A1(input)?; consume(input, 'a')?; consume(input, 'b')?; sort_A2(input)?; consume(input, 'b')?; consume(input, 'a') } fn a_a(input: &mut Iter) -> Result<(), String> { consume(input, 'a') } fn a_epsilon(_input: &mut Iter) -> Result<(), String> { Ok(()) }\n\nOur parse table has now become code directly, with these functions named after\nthe sorts of the rows. They call rules that are also functions, which in turn\nuse the sort functions. Those rules also use consume, this time without having\nto reverse the order of the rule body.\n\n    \n    \n    pub fn lex(input: String) -> Vec<Terminal> { input.chars().collect() } pub fn main() -> Result<(), String> { let input = env::args().next().expect(\"Argument string to parse\"); let input = lex(input); let mut input = input.iter().peekmore(); sort_s(&mut input) }\n\nFinally, our main function just calls the right sort function instead of\nputting that sort on the stack. And the loop is gone, since we now use\nrecursion.\n\n## Summary of LL, and an insight from the automaton\n\nWe\u2019ve now seen LL(k) parsing, left-to-right leftmost derivation. This leftmost\nderivation directly corresponds to walking through the parse tree topdown,\ndepth-first, leftmost child first. Whenever we expand a leftmost sort by a\nrule for that sort, we have to choose a rule, therefore we use the look-ahead\n(with a length of k) to see ahead and choose based on this.\n\nWe\u2019ve seen an LL(1) and an LL(2) grammar, and in general more look-ahead\nallows us to parse more grammars and more languages. Both are important:\ncertain languages cannot be expressed in LL(1) or LL(2), and some LL(1)\ngrammars are harder to read and write than the LL(2) grammar of the same\nlanguage.\n\nWe\u2019ve seen how we can construct simple DFAs for each rule in our grammar, and\nthen replace the sort transitions N1AN2 by a (PDA) push transition (\u2193A) from\nN1 to all starts of DFAs corresponding to rules of A, and a pop transition\n(\u2191A) from the ends of those DFAs to N2. Then the LL table, the decision table\nof sort + look-ahead = rule, naturally follows from this PDA by finding what\ninput will be consumed if a certain rule is chosen, and using that as the\nlook-ahead to make the decision for that rule.\n\nThe recursive descent way of writing a parser directly as code is nice and\nsimple, it really just follows the grammar. Since you\u2019re writing plain old\ncode with function calls, you can imagine people have found nice ways to\nextend and adapt the pattern of recursive descent parsers. For one, it\u2019s quite\neasy to reason about where you are in the parse when hitting an error state,\nwhich makes it fairly easy to give friendly error messages when the parser\ndoesn\u2019t accept an input. You can also use a trick to fix up direct left-\nrecursion called node reparenting, where you use a loop or tail-recursion\nlocally construct the tree bottom-up. You could argue that such a parser is a\nhybrid between recursive descent and ascent, a \u201crecursive descent-ascent\nparser\u201d.\n\nFinally, if we look back at the automaton, we can see that the PDAs we build\nhave a very strict shape. We either have a non-deterministic choice due to\nmultiple push transitions for a sort, or we have predicted input, a single\npath of terminals to consume from the input. If we think back to the NFAs and\nDFAs from early on in this blog post series, those used the input to chose\nwhat state to go to next. Now we have single-path DFAs that just consume\ninput, and a separate table on a look-ahead to resolve non-determinism from\nthe pushes and pops. The strict shape here indicated that we\u2019re not really\nmaking full use of the power of automata. This will change with the next\nparsing technique.\n\n# Bottomup, LR parsing\n\nLR stands for left-to-right, rightmost derivation in reverse. If you think\nabout it, left-to-right and rightmost derivation are incompatible: The\nrightmost derivation chooses the rule for the rightmost sort first every time,\nbut that means skipping over some unknown amount of input if you read left-to-\nright to even get to that point. However, the reverse of the rightmost\nderivation is a left-to-right form of parsing. This reverse derivation\ndescribes going bottomup, left-to-right through the parse tree.\n\n## Expressive power and relation to LL\n\nOne of the biggest upsides of LR(k) parsing is its expressivity. The set of\nall LL(k) languages of any k is a strict subset of all LR(1) languages. Note\nthat this is speaking of languages, not grammars. For grammars it holds that\nany LL(k) grammar for a specific k is also an LR(k) grammar, and not\nnecessarily the other way around.\n\nAn LR(k) grammar of any k greater than 1 can be automatically transformed into\nan LR(1) grammar that is not necessarily structurally equivalent. This is\nhighlights the difference between grammar and language level equivalence. We\ncan basically capture any LR language in an LR(1) grammar, but LR with larger\nk may be able to describe the language in a nicer way (smaller grammar).\n\nA good overview of how LL and LR relate to each other on the grammar and\nlanguage level is summarised on the Computer Science Stack Exchange. In the\ncomments someone suggests making a list of examples for each of these\nrelationships, which seems like a great idea, but not something I have the\npatience for right now. This blog post has enough scope creep already.\n\n## How LR works\n\nIn order to give a reverse rightmost derivation, we need to figure what sorts\ncan be at the leftmost leaf of the parse tree for our LR grammar. Then we try\nto apply the rules for those sorts all simultaneously. And to do so we can\u2019t\njust use the automaton build we\u2019ve used for LL.\n\nRemember that the automata we\u2019ve used previously mapped well on recursive\ndescent, and showed us where to use an LL parse table with look-ahead to\nresolve ambiguity. Crucially, those automata observe every rule we go into.\nBut for LR we need to explore down all the rules simultaneously. Let\u2019s see if\nwe can\u2019t get somewhere with that idea and the example grammar of the language\nthat wasn\u2019t LL:\n\nS=aS| (1)  \n---|---  \nS=A| (2)  \nA=aAb| (3)  \nA=\u03b5| (4)  \n  \nWe start again with the separate automata for each rule:\n\nNow in order to explore to the bottom-left of the parse tree, we need to be\nfree to go into any rule. So we will connect the rules again to the nodes that\nexpect a certain sort, but with epsilon transitions so we don\u2019t observe how\nfar down we are or with what rule in particular we got there. We\u2019ll need that\nlater, but let\u2019s not worry about that until we have the downward exploration.\n\nObviously this is not a full automaton model of a parser yet, but it allows us\nto always go down to the next leaf of the parse tree without using the stack.\nLet\u2019s clean up the automaton with an NFA-to-DFA conversion:\n\nThis is almost exactly how an LR(0) automaton would be drawn. Instead of S10\nand S11, you write out the \u201cLR item\u201ds S = . a S and S = a . S. But otherwise\nit would be pretty much this. This is considered a PDA, though what\u2019s\nhappening on the stack is left implicit. That\u2019s because what\u2019s actually\nhappening on the stack of LR automata is very structured, but a little\ninvolved. That makes the PDA harder to read and draw, but I\u2019ll demonstrate it\nonce:\n\nThis should look quite familiar. We\u2019re pushing inputs on the stack as we\nconsume them, paired with the state we\u2019re in at that point. And then we\u2019re\npopping whole bodies of rules off the stack and replacing them with the sort\nof that rule. The first thing is called a shift action, the second is called a\nreduce action. We\u2019ve seen this trick before in the naive PDA built from a CFG,\nall the way at the start of this post in the refresher. But this time we get\nan automaton with more states.\n\nNotice that where a reduce action goes depends on originating state of the\nlast thing that\u2019s popped. That\u2019s why we keep track of those on the stack. When\nwe reduce by rule 3 (state A3), depending on whether the a came from box 1 or\nbox 0, we go to different places. This is easier to see in our proper LR(0)\nautomaton, where box 1 points to state S1 with a transition labeled A. This is\na goto action. In an LR parse table interpreter, the goto is a separate action\nthat immediately follows a reduce action, which merely returns to the last\npopped state. When a reduce just returns that\u2019s also more like a function call\nand return, so that\u2019s nice. Anyway, that\u2019s also why a reduce transition in the\nabove automaton always labels the originating state of the pushed sort the\nsame as the last thing popped from the stack.\n\nSomething worth repeating now that we\u2019re looking at the details: LL decides\nwhat rule to take before consuming the input for that rule, whereas LR decides\nwhat rule to take after consuming all the input for that rule. In other words,\nwe only reduce by a rule when we\u2019ve seen the entire body of the rule, that why\nthere\u2019s less trouble with look-ahead.\n\nSpeaking of look-ahead: we have some shift-reduce problems in our automaton.\nAnd by that I mean: how do we choose when to shift and when to reduce when\nboth are an option? This is a determinism issue in our current automaton, and\njust like in our LL automaton, we solve it with look-ahead (and yes, that can\nand will later be summarised in a parse table). Our latest automaton gives a\nclear view of what we will be able to do if we reduce, so the look-ahead\nfollows from what input can be consumed next after each reduce:\n\nAs you can see, we need at most 1 look-ahead to deterministically parse this\ngrammar. We\u2019re sometimes looking ahead to the end-of-input represented with $.\nThe look-ahead makes this an LALR(1) grammar; what that is and why it\u2019s\ndifferent from normal LR(1) is what we\u2019ll see in the next section.\n\n## LR parsetable construction and expressivity\n\nLet\u2019s look at some example grammars, how to construct their tables, and when\nyou need a better parsetable construction method.\n\n### LR(0)\n\nLR(0) does not look ahead but just reduces whenever possible. If there are\nmultiple options, you have a shift-reduce or a reduce-reduce conflict. Shift-\nshift conflicts don\u2019t exist in LR since the NFA-to-DFA process would have\nmerged the two states such conflicting transitions would point to. Let\u2019s look\nat an LR(0) grammar:\n\nS=E2| (1)  \n---|---  \nE=E1| (2)  \nE=1| (3)  \n  \nThe LR automaton for this grammar is:\n\nThe corresponding parse table follows this automaton:\n\n1| 2| $| E  \n---|---|---|---  \nBox0| s E3| accept| Box1  \nBox1| s E2| s S1  \nE3| r 3| r 3| r 3  \nE2| r 2| r 2| r 2  \nS1| r 1| r 1| r 1  \n  \nThe transition from box 0 to E3 that shifts 1 becomes a shift action to E3 in\nthe row of box 0 and the column of 1. The transition from box 0 to box 1 with\nE becomes a goto to box 1 in the row of box 0 and column of E. Finally a state\nthat\u2019s at the end of a rule will get all reduce actions by that rule\n(indicated by its number) in the column for input. Accepting the input is\ntypically based on look-ahead of the end-of-input.\n\n### Simple LR (SLR)\n\nThe smallest motivating example for Simple LR is the following grammar that\nparses the same language as before:\n\nS=E2| (1)  \n---|---  \nE=1E| (2)  \nE=1| (3)  \n  \nNotice how rule 2 is now right-recursive instead of left-recursive. It\u2019s a\nnice symmetry how left-recursive rules give you trouble in LL, and right-\nrecursive rules (can) give you trouble in LR^5.\n\n1| 2| $| E  \n---|---|---|---  \nBox0| s Box1| accept| S11  \nBox1| s Box1 / r 3| r 3| r 3| E2  \nS11| s S1  \nS1| r 1| r 1| r 1  \nE2| r 2| r 2| r 2  \n  \nYay, we have a shift-reduce conflict. How do we solve it? By not blindly\nputting a reduce in the entire row of a state that could reduce. If we check\nthe Follow set of the sort we\u2019re reducing to (we defined that when we built LL\nparse tables, remember?), we can put the reduce action in only the column of\nthe terminals that are in that follow set. If we look at the grammar, we can\nsee that only 2 can follow E. So the SLR table for this grammar is:\n\n1| 2| $| E  \n---|---|---|---  \nBox0| s Box1| accept| S11  \nBox1| s Box1| r 3| E2  \nS11| s S1  \nS1| r 1  \nE2| r 2  \n  \n### Look-Ahead LR (LALR)\n\nFrom now on we\u2019ll be looking at reduce-reduce conflicts only. While you can\nget shift-reduce conflicts with the following algorithms through grammars that\ndon\u2019t fit (due to ambiguity or requiring more look-ahead than you\u2019re taking\ninto account), when you give an LALR(k) grammar to an SLR(k) algorithm you can\nonly get reduce-reduce conflicts. Same with an LR(k) grammar put through the\nLALR(k) algorithm.\n\nHere our example grammar that just barely doesn\u2019t work with SLR:\n\nS=aEc| (1)  \n---|---  \nS=aFd| (2)  \nS=bFc| (3)  \nE=e| (4)  \nF=e| (5)  \n  \nSee how rules 4 and 5 are the same except they have different sort names?\nYeah, that\u2019s going to be fun if they\u2019re used with the same prefix like in\nrules 1 and 2. Let\u2019s have a look at the automaton and SLR parse table.\n\na| b| c| d| e| $| E| F  \n---|---|---|---|---|---|---|---  \nBox0| s Box1| s Box2| accept  \nBox1| s Box3| S12| S22  \nBox2| s F5| S32  \nBox3| r 4 / r 5| r 5  \nS12| s S1  \nS1| r 1  \nS22| s S2  \nS2| r 2  \nS32| s S3  \nS3| r 3  \nF5| r 5| r 5  \n  \nThe reduce-reduce conflict, as promised. It\u2019s in box 3, where we can reduce by\nE4 or F5, when the look-ahead is c. This is because the look-ahead sets of\nboth E and F contain c due to rules 1 and 3. If we look at the automaton\nthough, we can clearly see that if we reduce and we have a c next, we should\nreduce by E.\n\nLook-Ahead LR parsing uses basically this method, analysing what shifts can\nhappen after certain reduces. Putting it is algorithmic terms, LALR doesn\u2019t\nuse LL Follow sets, but defines more accurate Follow sets based on the\nautomaton. Each instance of the start of a rule in the automaton (F50 in boxes\n1 and 2) gets a separate Follow set computed. That\u2019s how we resolve the\nconflict with LALR:\n\na| b| c| d| e| $| E| F  \n---|---|---|---|---|---|---|---  \nBox3| r 4| r 5  \n  \nNote that since the LALR Follow sets follow directly from the automaton, this\nis basically the same as the intuition given at the end of the previous\nsection.\n\n### LR(1)\n\nI like this LALR parsing story. It\u2019s so intuitive with the NFA-to-DFA\nconversion, just looking at the automaton to see the follow sets. But, it\u2019s\ndoesn\u2019t give you the complete power of deterministic push-down automata. I\npresent to you the previous example grammar with one more rule:\n\nS=aEc| (1)  \n---|---  \nS=aFd| (2)  \nS=bFc| (3)  \nE=e| (4)  \nF=e| (5)  \nS=bEd| (6)  \n  \nThis results in an automaton that\u2019s almost the same as before:\n\nWe now have a reduce-reduce conflict in box 3 again. With look-ahead a you can\nreduce to both E and F. Same for look-ahead b by the way. It is\ndeterministically decidable which one we should reduce to, but it basically\nnow depends on which state we came from.\n\nWith LALR we build an automaton for each rule, and try to reuse that rule\nindependent of the context in which it is used. That\u2019s keeps our process\nsimple, our automaton small, but it also causes us to lose exactly the\ninformation we need to resolve the reduce-reduce conflict in box 3 above: the\nleft context. I know the information is technically on the stack, but our\nparsing process decides on the rule to reduce by based on the state and look-\nahead only.\n\nLR(k) automata/parsers keep the same parsing process still, they just have\nlarger automata in which their states summarise the left context. We\u2019re\nbasically going to distinguish almost every occurrence of a sort in the\ngrammar, similar to when we made our LL(2) grammar strong:\n\nHow do we do this? We duplicate each rule for each terminal in the LL follow\nset of the sort of that rule. We annotate each of those rules with that\nterminal. Now we do our usual thing: rule to automaton, epsilons, NFA-to-DFA.\nBut when wiring the epsilons, extra terminal annotations should now match up\nwith the LALR follow set of the occurrence of the sort.\n\nWith this particular example, the automaton looks almost the same. There\u2019s a\nbit more fluff with the annotations, but they basically propagate the look-\nahead for each rule. Which means we can distinguish the context in which E and\nF are used differently! In general though, duplicating each rule for each\nterminal in the LL follow set leads to a very large amount of rules, and\nplenty of the time this isn\u2019t necessary... LR(1) automata have lots of\nredundant states that do basically the same thing and would have been merged\nin LALR without any reduce-reduce conflicts.\n\n### Parse table construction algorithm\n\nYou\u2019ve already seen parse table construction by automaton for both LL and the\nmany flavours of LR now. And you\u2019ve seen parse table construction by First and\nFollow set for LL. Parse table construction for LR will of course also require\nFirst and Follow sets, sometimes including more accurate Follow sets for\nparticular occurrences of sorts. It\u2019s mostly an iterative build-up of the NFA-\nto-DFA (powerset construction) though. I\u2019m not going to detail that in this\npost.\n\nWhile researching the material, I found some claims for minimal LR(1)\nalgorithms, which create LALR(1) tables when possible, and slightly larger\ntables when necessary. They look interesting, but quite something to figure\nout, and I haven\u2019t gotten to what I wanted to write about most yet, so that\nwill have to wait until some other time. Perhaps I\u2019ll include the normal LR\nparse table construction algorithm there too as a start.\n\n## Recursive Ascent\n\nWe finally get to the original impetus for this blog post: recursive ascent\nparsing. As you might be able to guess, this is the LR analogue to recursive\ndescent for LL. So we\u2019re going to write code that directly executes the LR\nautomaton instead of simulating it by parse table interpretation.\n\nBefore, in recursive descent parsing, we saw that rules and sorts become\nfunctions. Rules call sort functions to parse a sort, and sorts check the\nlook-ahead to choose a rule by which to parse the alternative of the sort.\nBasically grammar rules became functions, and the parse table was split into\nfunctions for each sort.\n\nIn recursive ascent parsing we will turn states of the LR automaton into\nfunctions. Each function will shift or reduce based on the input and call the\ncorresponding state for that edge. Let\u2019s expand our LR(1) example a little\nbit, and then take a look at the recursive ascent parsing:\n\nS=aEc| (1)  \n---|---  \nS=aFd| (2)  \nS=bFc| (3)  \nE=ee| (4)  \nF=ee| (5)  \nS=bEd| (6)  \nS=beea| (7)  \n  \nThe reason for the extra es in rules 3 and 4 is to show how that increases the\nLR(1) automaton size. We\u2019ll now have 4 states instead of 2 + an LALR reduce-\nreduce conflict. The reason for adding rule 7 is so we have a state where we\nmight shift or reduce depending on the look-ahead, which influences the code\nwe generate. Let\u2019s check out the automaton first:\n\nPerhaps making both changes at the same time makes this a bad example to show\noff LR(1) automaton size... If you imagine the automaton without rule 7 you\u2019ll\nsee that boxes 3 and 4 are the same except for their ingoing and outgoing\nedges. This is what happens with longer rules and having to distinguish the\nfinal state of the rules for a different look-ahead (boxes 5 and 6 here).\n\nThe other notable difference is that we now have a box 6 that can both shift\nand reduce. This will make the code for the recursive ascent more interesting.\nLet\u2019s start with the basics:\n\n    \n    \n    use std::env; use std::iter::Peekable; type Iter<'a> = Peekable<std::slice::Iter<'a, Terminal>>; type Terminal = char; #[derive(Clone, Copy, Debug, Eq, Hash, PartialEq)] enum Sort { S, E, F, } /// Box0, the starting state of our automaton. /// Itemset: /// S = . a E c /// S = . a F d /// S = . b F c /// S = . b E d /// S = . b e e a fn box0(input: &mut Iter) -> Result<(), String> { match input.next() { None => Err(String::from(\"Unexpected end of input: S = .\")), Some('a') => match box1(input)? { Sort::S => Ok(()), s => unreachable!(\"Unexpected sort: S = a {s:?}\"), }, Some('b') => match box2(input)? { Sort::S => Ok(()), s => unreachable!(\"Unexpected sort: S = b {s:?}\"), }, Some(c) => Err(format!(\"Unexpected input: S = . {c}\")), } }\n\nThis should look familiar from the recursive descent parser code. The notable\ndifference is that we now have a function name less connected to the grammar,\nand more to the LR automaton. This makes it harder to understand the code,\nstack traces, etc.\n\n    \n    \n    /// Box1 /// Itemset: /// S = a . E c /// S = a . F d /// E = . e /// F = . e fn box1(input: &mut Iter) -> Result<Sort, String> { match input.next() { None => Err(String::from( \"Unexpected end of input while parsing S = a . E; S = a . F\", )), Some('e') => match box3(input)? { Sort::E => s12(input), Sort::F => s22(input), s => unreachable!(\"Unexpected sort: S = a {s:?}\"), }, Some(c) => Err(format!(\"Unexpected input: S = a . {c}\")), } } /// S = a E . c fn s12(input: &mut Iter) -> Result<Sort, String> { match input.next() { None => Err(String::from( \"Unexpected end of input while parsing S = a E . c\", )), Some('c') => s1(input), Some(c) => Err(format!(\"Unexpected input: S = a E . {c}\")), } } /// S = a E c . fn s1(input: &mut Iter) -> Result<Sort, String> { match input.next() { None => Ok(Sort::S), Some(c) => Err(format!(\"Unexpected input: S = a E c . {c}\")), } } /// S = a F . d fn s22(input: &mut Iter) -> Result<Sort, String> { match input.next() { None => Err(String::from( \"Unexpected end of input while parsing S = a F . d\", )), Some('d') => s2(input), Some(c) => Err(format!(\"Unexpected input: S = a F . {c}\")), } } /// S = a F d . fn s2(input: &mut Iter) -> Result<Sort, String> { match input.next() { None => Ok(Sort::S), Some(c) => Err(format!(\"Unexpected input: S = a F d . {c}\")), } } /// Box3 /// Itemset: /// E = e . e (c) /// F = e . e (d) fn box3(input: &mut Iter) -> Result<Sort, String> { match input.next() { None => Err(String::from( \"Unexpected end of input while parsing E or F.\", )), Some('e') => box5(input), Some(c) => Err(format!(\"Unexpected input: E = e . {c} ; F = e . {c}\")), } } /// Box5 /// Itemset: /// E = e e . (c) /// F = e e . (d) fn box5(input: &mut Iter) -> Result<Sort, String> { match input.peek() { None => Err(String::from( \"Unexpected end of input while parsing E or F.\", )), Some('c') => Ok(Sort::E), Some('d') => Ok(Sort::F), Some(c) => Err(format!(\"Unexpected input: E = e e . {c} ; F = e e . {c}\")), } }\n\nThis bit of code should give you an idea of the code pattern in the \u201ceasy\ncase\u201d. Each state either shifts in one-or-more rules it\u2019s in (e.g. s12, box3),\nshifts into a new rule expecting a sort back to use for the goto (e.g. box1),\nor reduces (e.g. s1, box5).\n\n    \n    \n    /// Box2 /// Itemset: /// S = b . F c /// S = b . E d /// S = b . e e a /// E = . e /// F = . e fn box2(input: &mut Iter) -> Result<Sort, String> { match input.next() { None => Err(String::from( \"Unexpected end of input while parsing E or F.\", )), Some('e') => match box4(input)? { (0, Sort::F) => s32(input), (0, Sort::E) => s62(input), (1, Sort::S) => Ok(Sort::S), s => unreachable!(\"Unexpected return/sort: S = b {s:?}\"), }, Some(c) => Err(format!(\"Unexpected input: S = b . {c}\")), } }\n\nThis is the point where things start looking different. In box 2 we might\nshift e because we\u2019ve entered rules 4 or 5 which will reduce to E or F. But we\ncould also be in rule 7. If the result from box 4 is that we were in rule 7,\nwe need to go back to the previous caller. So function box4 returns a pair of\nthe number of returns left to go and the sort we\u2019re reducing to. This way we\ncan distinguish the two cases and take the appropriate action.\n\nIf you want to keep a recursive ascent code generator simpler you can of\ncourse always return a pair. You could also generate the code in continuation\npassing style, where you pass a function that takes the sort and does the goto\naction instead of accepting a pair as a result. But because the Rust compiler\nis not very good at tail call optimisation, so I\u2019m not doing that pattern\nhere.\n\n    \n    \n    /// S = b F . c fn s32(input: &mut Iter) -> Result<Sort, String> { match input.next() { None => Err(String::from( \"Unexpected end of input while parsing S = b F . c\", )), Some('c') => s3(input), Some(c) => Err(format!(\"Unexpected input: S = b F . {c}\")), } } /// S = b F c . fn s3(input: &mut Iter) -> Result<Sort, String> { match input.next() { None => Ok(Sort::S), Some(c) => Err(format!(\"Unexpected input: S = b F c . {c}\")), } } /// S = b E . d fn s62(input: &mut Iter) -> Result<Sort, String> { match input.next() { None => Err(String::from( \"Unexpected end of input while parsing S = b E . d\", )), Some('d') => s6(input), Some(c) => Err(format!(\"Unexpected input: S = b E . {c}\")), } } /// S = b E d . fn s6(input: &mut Iter) -> Result<Sort, String> { match input.next() { None => Ok(Sort::S), Some(c) => Err(format!(\"Unexpected input: S = b E d . {c}\")), } } /// Box4 /// Itemset: /// S = b e . e a /// E = e . e (d) /// F = e . e (c) fn box4(input: &mut Iter) -> Result<(usize, Sort), String> { match input.next() { None => Err(String::from( \"Unexpected end of input while parsing E or F.\", )), Some('e') => box6(input).map(decr), Some(c) => Err(format!( \"Unexpected input: S = b e . {c}; E = e . {c} ; F = e . {c}\" )), } } /// helper fn decr((c, s): (usize, Sort)) -> (usize, Sort) { (c - 1, s) }\n\nNote how in box4 we\u2019re now calling the decrement helper function after the\ncall to box6 to count one return we\u2019re going to do immediately after.\n\n    \n    \n    /// Box6 /// Itemset: /// S = b e e . a /// E = e e . (d) /// F = e e . (c) fn box6(input: &mut Iter) -> Result<(usize, Sort), String> { match input.peek() { None => Err(String::from( \"Unexpected end of input while parsing E or F.\", )), Some('c') => Ok((2, Sort::F)).map(decr), Some('d') => Ok((2, Sort::E)).map(decr), Some('a') => { input.next(); s7(input).map(decr) } Some(c) => Err(format!(\"Unexpected input: E = e . {c} ; F = e . {c}\")), } }\n\nThe number of returns to do is equal to the size of the body of the rule we\nare reducing. Of course we immediately decrement because we are going to\nimmediately return, hence the map(decr).\n\n    \n    \n    /// S = b e e a . fn s7(_input: &mut Iter) -> Result<(usize, Sort), String> { Ok((4, Sort::S)).map(decr) } fn lex(input: String) -> Vec<Terminal> { input.chars().collect() } pub fn main() -> Result<(), String> { let input = env::args().next().expect(\"Argument string to parse\"); let input = lex(input); let mut input = input.iter().peekable(); box0(&mut input) }\n\nIn our main function we can call box0 with the input. Since this is LR(1) we\nonly need a peekable iterator, that can look ahead 1 terminal.\n\n### Table size = Code size\n\nWith both recursive descent and recursive ascent parsing, we\u2019re representing\nthe parsing logic directly in code, not as an explicit data representation of\na parse table. As such, if you have a larger parse table, you get more code.\nIn LR, when LALR doesn\u2019t suffice, parse tables can potentially grow quite\nlarge, as we saw to a limited extent with the last example.\n\n## Recursive Ascent-Descent Parsing\n\nHave you noticed that in the recursive ascent code there are some pretty\nboring and tiny looking functions? I\u2019m talking about s12, s1, s22, s2, s32,\ns3, s62, s6. These will likely be targeted by the inliner of the Rust\ncompiler^6, but aren\u2019t they a bit much to even generate?\n\nThe common denominator of these functions, and the states of the LR automaton\nthey correspond to, is that they have already honed in on a single rule from\nthe grammar and are only parsing that. Kind of like in an LL parser, except we\nused the LR automaton mechanism to select the rule instead of an LL look-\nahead. If we follow that idea to its logical conclusion, we can do LL parsing\nfrom any point where we know there\u2019s only one rule left (or equivalently,\ninline those simple functions). This means we only have box functions left:\n\n    \n    \n    fn box1(input: &mut Iter) -> Result<Sort, String> { match input.next() { None => Err(String::from( \"Unexpected end of input while parsing E or F.\", )), Some('e') => match box3(input)? { Sort::E => { consume(input, 'c')?; Ok(Sort::S) } Sort::F => { consume(input, 'd')?; Ok(Sort::S) } s => unreachable!(\"Unexpected sort: S = a {s:?}\"), }, Some(c) => Err(format!(\"Unexpected input: S = a . {c}\")), } }\n\nThis is using the consume function from the recursive descent parser example\nfrom before.\n\n    \n    \n    /// Box6 /// Itemset: /// S = b e e . a /// E = e e . (d) /// F = e e . (c) fn box6(input: &mut Iter) -> Result<(usize, Sort), String> { match input.peek() { None => Err(String::from( \"Unexpected end of input while parsing E or F.\", )), Some('c') => Ok((2, Sort::F)).map(decr), Some('d') => Ok((2, Sort::E)).map(decr), Some('a') => { input.next(); // consume 'a' Ok((3, Sort::S)).map(decr) } Some(c) => Err(format!(\"Unexpected input: E = e . {c} ; F = e . {c}\")), } }\n\nNote that in box 6 we now count the number of symbols in the body of the rule\nbefore the dot to come up with the number of returns.\n\n### Left Corners?\n\nThe left corner of a rule in the grammar is the left-most symbol in the body\nof the rule, plus the left corners of any sorts in left corner. So it\u2019s\nbasically a First set with the sorts included. I found this is some of the\nolder literature, and figured I\u2019d add a note for myself in here.\n\nThere is/was such a thing as left-corner parsing, seemingly mostly used in\nnatural language processing (NLP). NLP mostly uses ambiguous context-free\ngrammars, and typically uses (used?) a backtracking parser to deal with that.\nThese can be slow of course. And it turns out left corners helped with this,\nby adding some \u201cfiltering\u201d that allows the parser to backtrack less. This is\nconnected to recursive ascent-descent parsing, which you could also see as\nfiltering with LR to finish parsing with LL. In our case we just don\u2019t do\nbacktracking.\n\n# Fin\n\nI really need to stop working on this blog post and publish it already. It\u2019s\nbeen over a year since I started working on it (on and off, during holidays\nwhen I had enough focus time)^7. I already had an idea of where to go to next\n(generalised parsers), but now I also want to study minimal LR(1)\nautomaton/parse table algorithms, and look at continuation passing style again\nbecause I think you can pass the left-context as a function argument. This\nwould give you an LALR automaton structure with LR parsing power. Is that a\ngood idea? Don\u2019t know, needs testing (or reading papers/blog posts, probably\nsomeone else already tried this before).\n\nI usually have a pithy remark or sneak the Kaomoji into the footnotes, but I\nmust be out of practice, because I can\u2019t think of a good way to do that...\n\nEhh, whatever \u0304\\\\_(\u30c4)_/ \u0304\n\n# Footnotes\n\n  1. I\u2019m fairly sure my prose description there is the same as a formal definition, and it feel a bit nicer to think about than the ones you can find on Wikipedia. \u21a9\n\n  2. Technically you\u2019d need to see A1 and A2 as separate symbols and duplicate the rules for A, resulting in a larger grammar in correspondence with the larger table. But I couldn\u2019t be bothered, and the parse table as shown works just as well. This is relevant to the code size of a recursive descent parser too, since you can just reuse the code for rules 2 and 3 instead of having duplicate code for the two extra rules. What\u2019s a recursive descent parser? That comes just a little later in the post, so keep reading ;) \u21a9\n\n  3. Yes, there are non-deterministic context-free languages. Those are the context-free languages that can only be parsed with a non-deterministic PDA. Since this post is about deterministic parsers, we\u2019ll ignore the non-deterministic languages. \u21a9\n\n  4. While I find the Wikipedia article on LLR confusing, and it makes a good case for why it\u2019s not really used, I\u2019m still somewhat intrigued. This is one of those things that will stay on my reading list for a while I think, something I still want to understand further... \u21a9\n\n  5. Indirect left recursion is even worse in LL. At least the direct version can still be dealt with by an automatic grammar rewrite algorithm. That\u2019s more or less what the node-reparenting trick mentioned at the end of the LL section does. Similarly, there are automatic grammar rewrites for direct right-recursion for LR, and indirect right recursion can be more problematic... \u21a9\n\n  6. Actually, I checked in Compiler Explorer how this turns out, and while s7 is inlined and compiled away entirely, adapting box1 to consume directly will make the assembly at opt-level=3 smaller. Adding an #[inline] hint on consume helps as well. Though I may just be seeing the effect of uniform error messages through consume. Actually following and understanding the optimised assembly code is a pain, so I just clicked around a bit to verify that the example code is reduced to a state machine with jumps and labels instead of using function call instructions. So that\u2019s neat, basically what I was hoping for :) \u21a9\n\n  7. I hope you appreciate how much time it took to find example grammars to steal (or occasionally develop myself) and especially how much time it took to get GraphViz to output somewhat decent automata of those examples! \u21a9\n\n## Whatever\n\n  * mail [at] this domain\n\n  * Apanatshka\n\nA web log. Mostly about computer science-y stuff.\n\n", "frontpage": false}
