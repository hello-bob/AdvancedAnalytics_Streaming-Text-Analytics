{"aid": "40292738", "title": "Just because you can do it yourself (synth monitoring), doesn't mean you should", "url": "https://www.checklyhq.com/blog/the-real-costs-of-aws-synthetics-are-operational/", "domain": "checklyhq.com", "votes": 1, "user": "serverlessmom", "posted_at": "2024-05-07 23:34:48", "comments": 0, "source_title": "The Real Cost of Synthetic User Testing with AWS", "source_text": "The Real Cost of Synthetic User Testing with AWS\n\n/Blog/AWS Costs\n\nFebruary 14, 2024\n\n(Updated: February 13, 2024)\n\n# The Real Cost of Synthetic User Testing with AWS\n\nNo\u010dnica Mellifera\n\nDeveloper Advocate\n\nAWS\n\ndevops\n\nShare on social\n\nEvery time I share a project using SaaS tools, someone inevitably responds\nthat they could do the same thing on their own home server \u2018for free.\u2019 I\nmention this not because it is annoying, since I would never go on social\nmedia at all if annoying responses were allowed to change my behavior, but\nbecause I think it points to a basic misconception that still affects DevOps\npractitioners today: the refusal to accurately estimate the real costs of\nself-managed solutions. Sure, we\u2019re probably not going to run the company\nemail server on a Raspberry Pi clone in the closet, and we don\u2019t see DIY\nsolutions as totally free, but there\u2019s still an unwillingness to write down\nand consider the real costs. Wherever you run your service, a DIY option will\nalways entail much higher operational and maintenance costs than\ninfrastructure costs\n\nCheckly is a good example of an \u2018out-of-the-box\u2019 SaaS tool that leaves you\ndoing as little work as possible to set up heartbeat checks, synthetic user\ntests, and API pingers. Checkly is all about site reliability: letting you\nknow that your service is working as expected for all users, anywhere in the\nworld. I wanted to take some time to compare Checkly to creating your own tool\nset and running these checks from Amazon Web Services (AWS). AWS has some\ntooling to perform a type of synthetics check from CloudWatch, called\nCanaries, though in practice either route to synthetics checks on AWS involve\na good amount of DIY work, and as you\u2019ll see in this article, the real costs\ninvolved are similar.\n\n## The real costs of AWS Synthetics are Operational\n\nWhen we consider the costs of AWS, the real differentiator between the public\ncloud and a dedicated service like Checkly is the Operational cost. For the\nmoment we won\u2019t discuss Infrastructure costs on AWS deeply, except to note\nthat the cost of a DIY solution made with the most readily available tools\ndoes have a considerable costs. As a single example, you\u2019ll quickly find that\nto run synthetic user tests effectively, you\u2019ll want to send requests from\nstatic IP addresses. This will add to the bare-bones pricing of a simple\ncontainer or AWS Lambda\n\nTo start this discussion, check out the build vs. buy calculator from the\nincident.io team. I\u2019ll use this calculator for a quick back-of-the-envelope\nfigure for the first year cost of an AWS Synthetics solution.\n\n### Estimating the cost of AWS Synthetics\n\nI want to make generous assumptions about the initial build time and\nmaintenance of an AWS-hosted Synthetics tool. First off let\u2019s assume our\nDevOps, SRE, or testing specialists are familiar with the language they\u2019ll be\nusing to write tests. It seems like a number of users of CloudWatch canaries\nare hitting that roadblock, but I don\u2019t want to take an extreme case. This\ngets us an initial development time of just a few weeks, and only a few days a\nmonth of maintenance work.\n\nThe low number of maintenance days per month may well be an issue. In the\nexample of working with CloudWatch canaries, the test code must either be part\nof a CloudFormation template or stored as a code file in a bucket somewhere.\nThat means its very unlikely that every engineer can easily be empowered to\nwrite and deploy their own synthetics test. That means our monthly maintenance\nbudget is the only time that tests will be added or modified. More on some\nother drawbacks to this limitation in the next section.\n\nFinally, no solution is going to include a UI out of the box. To make an\ninterface as nice as Checkly\u2019s, we\u2019re really talking about a large engineering\nproject. However for this example we\u2019ll only require an integration with a\npaging system, Slack, and an issue tracker. With these, our estimation is for\na \u201cQuick and dirty\u201d solution that will tell you what tests are failing where,\nbut not a nice plot of the failure timeline. We also may well end up digging\nthrough logs for details on the failure.\n\nPlugging these values into the Build vs. Buy calculator (and assuming that our\nestimates need no margin for error) we come up with a two month launch\ntimeline and a first year cost of $120k\n\nAgain this system has the following limitations:\n\n  * Only a basic web interface\n  * Doesn\u2019t let every developer add or modify tests\n  * Doesn\u2019t have sophisticated sharing and access control\n\nLet\u2019s not even mention the things like Checkly\u2019s visual testing recording that\nshow the exact way the page broke as a small video. Those reach features will\nnever be worked on if there are only a few days a month to even update tests!\n\n### Operational costs are about more than engineer pay\n\nBeyond the direct expense of work on a testing and monitoring system, it\nreally stands to consider the cost of tests that are not of the highest\nquality. When we use either CloudWatch canaries or from-scratch DIY tests,\nwe\u2019re using a system with its own rules that will be fairly opaque to most of\nthe developers on your team.\n\nWhen using a SaaS option like checkly, you\u2019re empowering each developer to\nwrite tests in javascript, and even to commit their tests write next to their\ncode repository with Monitoring as Code. This has a number of advantages:\n\n  * When the developer who worked on a feature writes the test, they\u2019re not working on a black box. They\u2019re testing the most important part of the system since they know its internals.\n  * Testing should take lest time to write\n  * When looking at early feedback from synthetics tests, the developers can modify tests as needed without working through a DevOps or SRE specialist to modify the tests\n  * When tests fail, either right away or months later, the developer being woken up to work on an incident will be the same one who wrote the feature and built the test. As developers we\u2019re never going to remember everything about a feature we worked on weeks ago, but it will still help our response time!\n\n(quick sidebar does anyone else feel weird using unordered lists in your\nwriting now because ChatGPT always uses them? I wrote every word of this\narticle. Here\u2019s a sentence only a real human could write: purple monkey\ndishwasher)\n\n## And now for the REAL-real costs: the costs of downtime\n\nWhat\u2019s downtime worth? Recent research points says that \u201cEvery minute of\ndowntime is thousands to millions of dollars in revenue losses and costly\ndamage to brand reputation.\u201d\n\nWill using a DIY heartbeat monitor on AWS take your site down? Absolutely not.\nWill make downtime worse? Absolutely.\n\nThe first step to fixing a systemic failure is recognizing its existence, and\nif you\u2019re running your own system with fractional engineering resources,\nthere\u2019s a good chance that when you hit real outages your system won\u2019t warn\nyou before your users do.\n\n### \u201cThat failure took the site down, unfortunately it also broke\nstatusPageUpdaterService\u201d\n\nOne of the great dangers of any DIY solution for uptime monitoring is a shared\nfailure: the same service that should warn us of downtime fails when the site\ndoes, so we slumber on blissfully unaware until our outage is a hashtag on the\nwebsite formerly known as Twitter. Not even the mighty AWS is immune to this\neffect. In 2020 a detailed postmortem mentioned that the status pages\ncontinued to show green for sometime since the Service Health Dashboard was\nnormally updated with Cognito, which was down.\n\nShared failures are something that you can control for with AWS, with its\nregions and availability zones, and hosting your synthetics checks on a public\ncloud means you can completely isolate that service from the rest of your\nproduction environment. You can. You can. But will you? Will you actually take\nthe time to stand up a health check monitor, a data store for those health\nchecks, a dashboard, a notification service, and a pager integration service,\nall completely separate from your actual production service? If you do, bravo!\nBut interdependency is always a risk, and a completely separate system adds\nsignificant development time.\n\n### Shared expertise improves response time\n\nIt\u2019s important not to forget how knowledge sharing and decentralization\nbenefit the overall technical response of a team. I discussed above the shared\nfailure of a service and its status page at AWS. From that same postmortem,\none detail from the same paragraph is worth highlighting:\n\nWe have a back-up means of updating the Service Health Dashboard that has\nminimal service dependencies. While this worked as expected, we encountered\nseveral delays during the earlier part of the event in posting to the Service\nHealth Dashboard with this tool, as it is a more manual and less familiar tool\nfor our support operators.\n\nWhen we DIY a solution, we are often siloing knowledge of that tool within our\nteam: without a tool that can readily be used by every engineer, a DIY tool is\noften useful only to a few DevOps people. The result is a dashboard that,\nwhile functional, only makes sense to a few people. This is a problem if\nyou\u2019re not sure who will be directed to this dashboard during an outage.\n\nWhile any tool, Checkly included, can be the victim of siloing, with its easy\ndashboards and ability to create accounts for anyone on your team who needs\naccess, Checkly makes it a lot easier to share monitoring data from your\nentire organization\n\n## Conclusions: Operational Costs Matter\n\nAll of us who work in Operations know that infrastructure and service costs\nare only a small part of the story. Without considering the costs of setting\nup and maintaining services, we can easily get deluded by the sticker price,\nand end up with a massive workload for SREs and Operations engineers who are\noverworked as it is.\n\nThe allure of a \"do-it-yourself\" approach with AWS might seem appealing at\nfirst glance, due to perceived flexibility and control. However, when we\naccount for the significant operational overhead\u2014development time,\nmaintenance, and the expertise required to manage and update tests\u2014the cost\nadvantage begins to diminish.\n\nThe comparison laid out in this post underscores a fundamental truth in\nsoftware development and operations: efficiency and reliability often come at\na cost that's not immediately apparent. While AWS provides powerful tools for\nbuilding custom synthetic testing solutions, the investment in time and\nresources can quickly outpace the initial estimates. This is especially true\nwhen considering the need for a robust, user-friendly interface and the\nability to empower every developer to write and modify tests without\nbottlenecking through a few specialized engineers.\n\nCheckly's lets engineers throughout your organization write tests and run them\nwith zero friction, significantly reduces the operational burden on teams. It\nenables developers to focus on what they do best\u2014building and deploying great\nsoftware\u2014rather than getting bogged down in the intricacies of maintaining a\ncustom testing framework.\n\nNo\u010dnica Mellifera\n\nDeveloper Advocate\n\nNo\u010dnica Mellifera (She/Her) was a developer for seven years before moving into\ndeveloper relations. She specializes in containerized workloads, serverless,\nand public cloud engineering. No\u010dnica has long been an advocate for open\nstandards, and has given talks and workshops on OpenTelemetry and Kubernetes\narchitecture. She enjoys making music that friends have described as\n\"unlistenable.\" In her spare time she cares for her two children and hundreds\nof houseplants.\n\nShare on social\n\n## Related Articles\n\n### New Relic vs Checkly: The Real Costs of Synthetics\n\nJanuary 16, 2024\n\n### The real costs of Datadog Synthetics monitoring\n\nDecember 21, 2023\n\nProduct\n\nMonitoring as codeSynthetic monitoringAPI monitoringAlertingPrivate\nlocationsIntegrationsDashboardsLive Checkly dashboardChangelogPricingStatus\n\nCompany\n\nAboutCareersBlogSecurityTerms of usePrivacy\n\nMonitoring as code\n\nCheckly CLIPulumi ProviderTerraform Provider\n\nConnect\n\nContact UsSupportSlack CommunityTwitter @ChecklyHQYouTubeLinkedInPublic\nroadmap\n\nLearn\n\nDocsCheckly GuidesPlaywright Tips\n\nHow we compare\n\nAlternative to DatadogAlternative to New RelicAlternative to\nDynatraceAlternative to Pingdom\n\nArticles\n\nWhat is Synthetic Monitoring?What is API monitoring?What is Playwright?A guide\nto Monitoring as CodeWhy Monitoring as Code?Cypress vs Selenium vs Playwright\nspeed comparison\n\nCopyright \u00a9 2024 Checkly Inc. All rights reserved.\n\n", "frontpage": false}
