{"aid": "40285489", "title": "Prompt Efficiency \u2013 Getting 8-shot LLM performance from 1-shot", "url": "http://blog.dottxt.co/prompt-efficiency.html", "domain": "dottxt.co", "votes": 1, "user": "Homunculiheaded", "posted_at": "2024-05-07 13:54:26", "comments": 0, "source_title": "Prompt Efficiency - Using Structured Generation to get 8-shot performance from 1-shot.", "source_text": "Prompt Efficiency - Using Structured Generation to get 8-shot performance from\n1-shot.\n\nHOME | .TXT WEBSITE\n\n# Prompt Efficiency - Using Structured Generation to get 8-shot performance\nfrom 1-shot.\n\nIn this post we\u2019re going to explore a surprising benefit of structured\ngeneration that we\u2019ve recently come across here at .txt we call \u201cprompt\nefficiency\u201d: For few-shot tasks, structured generation with Outlines is able\nto achieve superior performance in as little as one example than unstructured\nis with up to 8. Additionally we observed that 1-shot structured performance\nremains similar to higher shot structured generation, meaning 1-shot is all\nthat is necessary in many cases for high quality performance. This is useful\nfor a variety of practical reasons:\n\n  * convenience: For few-shot problems, examples can be difficult to come by and annotating examples that include a \u201cChain-of-Thought\u201d reasoning step can be very time consuming and challenging.\n  * speed: Longer prompts mean more computation, so keeping prompt size smaller means faster inference.\n  * context conservation: Examples easily eat up a lot of context for models with limited context length.\n\nWe\u2019ll walk through the experiments we\u2019ve run to show this property of\nstructured generation.\n\n## Few Shot Learning with LLMs\n\n\u201cFew Shot\u201d learning refers to the practice of providing the model with several\nexamples of question/reason/answer data from a given benchmark data set in the\nprompt. Here is an example from the GSM8K task using a \u201c3-shot\u201d prompt:\n\nTypically these examples are small in number (nearly always less than 10)\ncompared to traditional machine learning approaches which can easily use\nthousands to millions of examples to learn the required task.\n\nIn nearly all tasks LLMs benefit from being shown a few examples (as opposed\nto being prompted to complete the task with only instructions, aka 0-shot).\nWhile it is incredible that LLMs are able to learn tasks so quickly, it\u2019s also\na requirement that LLMs work with few shot cases since limited context length\nmeans they can\u2019t process as many examples as traditional machine learning\nmodels. Even for models with large (or even potentially unbounded) context\nlengths, longer context means more time consuming generation.\n\n## Prompt Efficiency\n\nWhile working with Hugging Face\u2019s Leaderboards & Evals research team on an\narticle about prompt consistency, we stumbled across an interesting finding.\nNot only did structured generation provide better performance something we had\nexplored previously and reduced variance in performance across shots (what we\nexplored with \ud83e\udd17), but also seemed to dramatically reduce the penalty we\nobserved for 1-shot prompting.\n\nIn the image below we can see this in the case of Mistral-7B-v0.1 on the GSM8K\ndata set.\n\nThe chart on the left shows the performance across different n-shots for\nunstructured generation. What we see is that 1-shot performance is abysmal.\nWhen looking at chart on the right showing the result for the same n-shots\nusing structured generation we noticed that performance for the 1-shot prompt\nsuffered a much smaller penalty, performing nearly as well as the unstructured\n8-shot.\n\nWhen looking at Zephyr we see an even more dramatic penalty for 1-shot\nprompting, but once again major improvements when using structured generation:\n\nThese results led us to explore the question:\n\n\u201cCan structured generation consistently provide 1-shot performance on par with\nhigher-shot unstructured performance?\u201d\n\nIf so, it means you can potentially save a lot of context in your prompt and\nneed far fewer examples to get high quality performance.\n\nTo explore this we looked at 6 different models across both the GSM8K\nevaluation set and the GPQA evaluation set and compared the performance\nbetween 1-shot and n-shot performance unstructured with 1-shot performance\nstructured.\n\n### GSM8K\n\nThe GSM8K data set consists of grade school mathematics questions. The answer\nto the questions are open response (meaning they are not multiple choice)\nintegers. Here is an example of what an individual \u201cshot\u201d will look like,\nalong with annotations about which parts the model will be responsible for\nfilling in in the end.\n\nThe chart below shows 1-shot and 8-shot performance unstructured, then\ncompares with 1-shot performance using structured generation:\n\nIn this chart the blue stars show us the performance of structured 1-shot,\nwhile the up and down arrows show us the 8-shot and 1-shot unstructured\nperformance. For example, in the case Pearl-7B-slerp we see that 1-shot\nunstructured is around 25% accuracy while the 8-shot unstructured performs\nmuch better at just shy of 80% accuracy. Looking at the 1-shot structured\nperformance we see that it is just a hair better performing than the 8-shot\nunstructured! As we can see from this chart, in most cases 1-shot structured\ngeneration performs at least as well as 8-shot unstructured generation.\n\nLet\u2019s take a look at a different benchmark and see if these results hold up.\n\n### GPQA\n\nThe GPQA task is a set of graduate level questions with multiple choice\nanswers. The data set is designed to be \u201cGoogle-Proof\u201d (that\u2019s what the GP\nstands for) meaning that, in theory, you cannot easily answer these questions\nusing Google search.\n\nHere is an example question with annotations about each section:\n\nThere are two major differences between GPQA and GSM8K. The first is that the\nquestions are notably more difficult, and the second is that the answers are\nmultiple-choice, meaning that the model only needs to select the correct\nanswer of four.\n\nThe GPQA model has longer questions and reasoning steps as well as higher\nvariance between different n-shots which lead us to use only up to 5 shots,\nbut rather than comparing just 1-shot to 5-shot we explored all possible shots\nand compared structured 1-shot to the best-shot for each model on the\nevaluation. Here are the results.\n\nAs we can see structured generation provides a major improvement in 1-shot\nlearning. Not only does structured 1-shot consistently match or beat the best\nperforming n-shot, but it is the only case to beat the baseline expectation of\na random guess (4 questions meaning you should get 25% by just guessing). This\nprovides evidence that even a single example can be enough to get excellent\nperformance out of a model using structured generation.\n\n## What does this tell us about examples in prompts?\n\nFor anyone coming from a background in machine learning, it's common to assume\nthat the number of examples provided give the model more semantic information\nabout the problem. That is, by providing examples the LLM will learn better to\ndo the task, similar to how a person might need a few examples of a problem to\nunderstand the task. However the fact that structured generation converges to\nnearly the same performance as other structured n-shots and dramatically out-\nperforms unstructured 1-shot prompts suggests something else might be at work\nhere.\n\nEven though we often think that the power of LLMs is their ability to model\nthe semantics of language, it appears that the structure/syntax is perhaps\njust as, if not more, important when prompting. It appears that providing\nadditional shots is helping the model to understand the structure of the\nproblem, and providing only 1 example is not enough in most cases. However,\nwhen we force the model to adhere to a the required structure it immediately\nappears to perform well, regardless of the number of shots provided. This\nsuggests that structure alone is doing a fair bit of the work in generating\nthe correct answers.\n\nOur findings here are reminisance of the findings in the Mixtral paper when\nanalyzing the routing of experts. The Mixtral LLM works by having 8 \"experts\"\nthat the model may chose from when generating tokens. This helps dramatically\nreduce the total memory required during generation since each expert is only\n7B parameters. It was assumed that experts were likely chosen by the model\nbased on the semantic content of the task. However the paper did not find this\nto be the case: \"Surprisingly, we do not observe obvious patterns in the\nassignment of experts based on the topic.\" Instead it appeared that the\nexperts seemed to be selected based on \"structured syntactic behavior\".\n\nThe exact role the structure plays in LLM generation is still in the early\nstages of exploration, but it increasingly appears to be a major component in\nthe performance of these models and plays a much larger role than simply\nproviding consistent format to the output.\n\n## Conclusion\n\nNeeding to provide only a single example of a problem to be solved provides a\nmajor advantage when working with LLMs. For many of the tasks that make a few\nshot classifier powerful, the fewer the shots required the better. Though LLMs\nare increasingly expanding their context length, for smaller (i.e. faster and\nmore affordable) models it\u2019s still important to limit the amount of context\nrequired for a given task for efficient inference. More important, for the\nkinds of problems where you only have a few labeled examples, there is often a\nmajor gap between having a single example and having multiple in terms of\neffort to build an application. Given the importance of structure to model\nperformance, it increasingly appears that structured generation is a must have\nfor anyone working with LLMs.\n\n", "frontpage": false}
