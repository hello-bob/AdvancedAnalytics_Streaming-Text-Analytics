{"aid": "40220940", "title": "Hierarchical 3D Gaussians for Real-Time Rendering of Large Datasets", "url": "https://repo-sam.inria.fr/fungraph/hierarchical-3d-gaussians/", "domain": "inria.fr", "votes": 1, "user": "jasondavies", "posted_at": "2024-05-01 08:46:04", "comments": 0, "source_title": "A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets", "source_text": "A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very\nLarge Datasets\n\n# A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very\nLarge Datasets\n\n# SIGGRAPH 2024 (ACM Transactions on Graphics)\n\nBernhard Kerbl^* 1, 2, 3 Andreas Meuleman^* 1,2 Georgios Kopanas^ 1,2 Michael\nWimmer^ 3 Alexandre Lanvin^1,2 George Drettakis^ 1,2\n\n^* Both authors contributed equally to the paper.\n\n^1Inria ^2Universit\u00e9 C\u00f4te d'Azur ^3TU Wien\n\n^1 ^2 ^3\n\nPaper - 74MB Paper - 10MB\n\n## Abstract\n\nNovel view synthesis has seen major advances in recent years, with 3D Gaussian\nsplatting offering an excellent level of visual quality, fast training and\nreal-time rendering. However, the resources needed for training and rendering\ninevitably limit the size of the captured scenes that can be represented with\ngood visual quality.\n\nWe introduce a hierarchy of 3D Gaussians that preserves visual quality for\nvery large scenes, while offering an efficient Level-of-Detail (LOD) solution\nfor efficient rendering of distant content with effective level selection and\nsmooth transitions between levels. We introduce a divide-and-conquer approach\nthat allows us to train very large scenes in independent chunks. We\nconsolidate the chunks into a hierarchy that can be optimized to further\nimprove visual quality of Gaussians merged into intermediate nodes.\n\nVery large captures typically have sparse coverage of the scene, presenting\nmany challenges to the original 3D Gaussian splatting training method; we\nadapt and regularize training to account for these is- sues. We present a\ncomplete solution, that enables real-time rendering of very large scenes and\ncan adapt to available resources thanks to our LOD method. We show results for\ncaptured scenes with up to tens of thousands of images with a simple and\naffordable rig, covering trajectories of up to several kilometers and lasting\nup to one hour\n\n## Video\n\n## BibTeX\n\n    \n    \n    @Article{hierarchicalgaussians24, author = {Kerbl, Bernhard and Meuleman, Andreas and Kopanas, Georgios and Wimmer, Michael and Lanvin, Alexandre and Drettakis, George}, title = {A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets}, journal = {ACM Transactions on Graphics}, number = {4}, volume = {43}, month = {July}, year = {2024}, url = {https://repo-sam.inria.fr/fungraph/hierarchical-3d-gaussians/} }\n\n## Acknowledgments and Funding\n\nThis research was funded by the ERC Advanced grant FUNGRAPH No 788065; B.K.\nand M.W. acknowledge funding from WWTF (project ICT22-055: Instant\nVisualization and Interaction for Large Point Clouds). The authors are\ngrateful to Adobe for generous donations, the OPAL infrastructure from\nUniversit\u00e9 C\u00f4te d\u2019Azur and for the HPC resources from GENCI\u2013IDRIS (Grant\n2022-AD011013409). The authors thank the anonymous reviewers for their\nvaluable feedback, Fr\u00e9do Durand and Adrien Bousseau for proof reading and\ninsightful comments, Sebastian Viscay for capturing SmallCity and Nikhil Mohan\nand colleagues at Wayve for the dataset and overall help.\n\n## References\n\n[M\u00fcller 2022] M\u00fcller, T., Evans, A., Schied, C. and Keller, A., 2022. Instant\nneural graphics primitives with a multiresolution hash encoding\n\n[Hedman 2018] Hedman, P., Philip, J., Price, T., Frahm, J.M., Drettakis, G.\nand Brostow, G., 2018. Deep blending for free-viewpoint image-based rendering.\nACM Transactions on Graphics (TOG), 37(6), pp.1-15.\n\n[Barron 2022] Barron, Jonathan T., et al. \"Mip-nerf 360: Unbounded anti-\naliased neural radiance fields.\" Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 2022.\n\n[Wang et al. 2023] Wang, P., Liu, Y., Chen, Z., Liu, L., Liu, Z., Komura, T.,\nTheobalt, C. & Wang, W. (2023). F2-nerf: Fast neural radiance field training\nwith free camera trajectories. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition.\n\n[Turki et al. 2022] Turki, H., Ramanan, D., & Satyanarayanan, M. (2022). Mega-\nnerf: Scalable construction of large-scale nerfs for virtual fly-throughs. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition.\n\n[Kerbl and Kopanas et al. 2023] Kerbl, B., Kopanas, G., Leimk\u00fchler, T., &\nDrettakis, G. (2023). 3d gaussian splatting for real-time radiance field\nrendering. ACM Transactions on Graphics, 42(4), 1-14.\n\nWe thank the authors of Nerfies that kindly open sourced the template of this\nwebsite.\n\n", "frontpage": false}
