{"aid": "40157985", "title": "Tokens for LLMs: Byte Pair Encoding in Go", "url": "https://eli.thegreenplace.net/2024/tokens-for-llms-byte-pair-encoding-in-go/", "domain": "thegreenplace.net", "votes": 4, "user": "ingve", "posted_at": "2024-04-25 14:24:51", "comments": 0, "source_title": "Tokens for LLMs: Byte Pair Encoding in Go", "source_text": "Tokens for LLMs: Byte Pair Encoding in Go - Eli Bendersky's website\n\nEli Bendersky's website\n\n  * About\n  * Projects\n  * Archives\n\n# Tokens for LLMs: Byte Pair Encoding in Go\n\nApril 25, 2024 at 06:34 Tags Go , Machine Learning , WebAssembly\n\nA basic unit of currency in modern LLMs is the token; exciting new models have\nlong context windows of millions of tokens. API pricing for the large\nproviders is per-token. We're even seeing the invention of new, derived units\nlike TPM (tokens per minute).\n\nBut what are tokens?\n\nThis OpenAI help article tells us that tokens are pieces of words, and gives\nsome useful rules of thumb like a token being equivalent to approximately 4\ncharacters or 3/4 of a word for the English language.\n\nIn this post I want to review the most commonly used algorithm for splitting\ntext into tokens, provide a complete implementation in Go, and show a\nplayground for experimenting with it. While my implementation isn't tuned for\nspeed, it aims to be complete, readable and compatible with OpenAI's tiktoken\nlibrary, generating identical results and working with the same vocabulary\nfiles.\n\n## Byte pair encoding - introduction\n\nByte pair encoding (BPE) is an algorithm originally designed for data\ncompression. A 2016 paper suggested re-purposing it for \"word segmentation\"\nfor machine learning tasks. The colloquial term for word segmentation is\ntokenization.\n\n  * Input: arbitrary text with words, numbers, whitespace and punctuation.\n  * Output: list of tokens representing the same text. Each token is an integer identifier which can be looked up in a vocabulary to reproduce the input text [1].\n\nThe BPE algorithm has an important pre-processing step: splitting the input\ntext into words. The splitting is customizable and different models /\nvocabularies use different regexps for splitting (more on this later). The\nmain idea is some sort of whitespace-based splitting (though whitespace itself\nis preserved) because we typically don't want inter-word tokens [2].\n\nWe'll be using this line from a catchy 1990s song as an example:\n\n    \n    \n    i'm blue dabadee dabadam\n\nA word splitter will produce something like the following list, where spaces\nare replaced by underscores _ for the sake of presentation (they remain as\nspaces in the actual implementation of the algorithm and its trained\nvocabulary):\n\n    \n    \n    i 'm _blue _dabadee _dabadam\n\nA few things to note:\n\n  * The contraction 'm is split from i - this is common for English language splitters, which want things like 'm, 'll, 're as separate words.\n  * Whitespace is preserved and attached at the start of a word. Whitespace is important because tokens at the beginning of words sometimes have different semantic meaning from tokens not at the beginning of words. The choice of where it's attached is arbitrary. From this point on, whitespace bytes are considered like any other bytes in the BPE algorithm.\n\nNow is a good time for some terminology we'll be using while talking about\nBPE:\n\n  * Word: produced by the splitter in pre-processing, like the list shown above.\n  * Token: typically a sub-word sequence of bytes; the output of the tokenizer is a list of tokens, by ID.\n  * Token ID: unique numerical identifier for a token.\n  * Vocabulary: a mapping of token IDs --> token values learned by the tokenizer during the training process.\n  * Training: the process in which BPE learns a vocabulary from a corpus of text.\n  * Splitter regexp: regular expression used to split text into words during pre-processing. Given an algorithm (in this case BPE), the pair vocabulary + splitter regexp unambiguously defines how a given text will be tokenized.\n  * Encoder: given a vocabulary and a splitter regexp, tokenizes any text into a list of IDs from the vocabulary.\n  * Decoder: given a list of IDs and the vocabulary, reconstructs the original text.\n\n## Training\n\nBPE training proceeds by first assuming each byte is its own token, and then\nsuccessively merging pairs of tokens into longer tokens and adding these to\nthe vocabulary, until the desired vocabulary size is achieved.\n\nLet's reuse our example, starting with these words:\n\n    \n    \n    i 'm _blue _dabadee _dabadam\n\nThe BPE process starts by creating a token for each byte in the inclusive\nrange [0..255]. So the minimal vocabulary size is 256; this guarantees that\nfrom the very start, there's a valid encoded representation of any text.\n\nThen, the following process is repeated:\n\n  * Count how many times each ordered pair of bytes appears in the input. Ordered pair here means two bytes right next to each other. In our example, some such pairs are \"bl\", \"da\", \"de\", \"ee\" etc.\n  * Find the pair with the highest count, and create a new token from it (create a new token ID, mapping it to the concatenation of the most common pair).\n  * Replace this most common pair with the combined token in the input set.\n\nIn our example, we start by splitting input words to bytes, so it's a list of\nsingle-byte token lists. This is our working list:\n\n    \n    \n    [i] [' m] [_ b l u e] [_ d a b a d e e] [_ d a b a d a m]\n\nNext, we count the frequency of appearance of each ordered pair:\n\n    \n    \n    [d a] --> 3 [a b] --> 2 [b a] --> 2 [' m] --> 1 [_ b] --> 1 [l u] --> 1 [u e] --> 1 [_ d] --> 2 [a d] --> 2 [d e] --> 1 [e e] --> 1 [b l] --> 1 [a m] --> 1\n\nThe pair \"da\" is the most common one, so we're creating a new token for it,\nand substituting it everywhere in the working list:\n\n    \n    \n    [i] [' m] [_ b l u e] [_ da b a d e e] [_ da b a da m]\n\nAs you can see, in every instance \"d\" followed by \"a\" was combined into \"da\".\nNow repeat the process; finding the most common pairs in this new working\nlist:\n\n    \n    \n    [e e] --> 1 [a da] --> 1 [l u] --> 1 [_ da] --> 2 [da b] --> 2 [a d] --> 1 [d e] --> 1 [da m] --> 1 [' m] --> 1 [_ b] --> 1 [b l] --> 1 [u e] --> 1 [b a] --> 2\n\nSeveral pairs have a count of 2, so we pick one arbitrarily. Let's say it's\n_da (a space followed by \"da\"). We add _da as a new token and make\nreplacements in the working list:\n\n    \n    \n    [i] [' m] [_ b l u e] [_da b a d e e] [_da b a da m]\n\nAnd so on. When does this process stop? When we either run out of pairs (every\nword consists of a single token) or - more realistically for an actual\ntraining corpus - when we reach our desired vocabulary size. For example the\nvocabulary used for GPT-4 has around 100,000 tokens (more on this later).\n\nThe output of the training process is a vocabulary; let's say we've only run\ntwo cycles on our input text as described. The vocabulary will have 258 tokens\nin it: 256 for the single bytes, one for da and another for _da. Each of these\nwould have a unique integer ID.\n\nIn our Go sample code, the training is implemented in this file. You can set\nthe debugTrain variable to true to follow the process on some sample text.\n\n## Encoding\n\nHaving learned a vocabulary, the process of encoding is what happens every\ntime we feed text into an LLM and it needs to be tokenized. The input is\narbitrary text, a splitting regexp and a vocabulary. For example, let's take\nthe input text \"yada daba\". Splitting is performed as before, and the input is\nbroken into individual bytes:\n\n    \n    \n    [y a d a] [_ d a b a]\n\nBPE encoding takes the vocabulary and tries to apply learned tokens to the\ninput text, word by word. The process is greedy - tokens are applied in the\nsame order they've been learned (this is easy to accomplish by assigning\nmonotonically increasing integer IDs to new tokens in the vocabulary, and then\nprioritizing lower-numbered tokens for encoding).\n\nThe first token we learned was da, so let's apply that:\n\n    \n    \n    [y a da] [_ da b a]\n\nThe next token we learned was _da:\n\n    \n    \n    [y a da] [_da b a]\n\nThis is the final stage; there are no more learned tokens to apply. The result\nwill consist of 6 tokens.\n\nIn our sample code, the encoder is in this file.\n\n## Realistic vocabulary and splitting\n\nThe examples shown so far have been toys, but the algorithms are real and work\nwith the actual vocabularies and splitters used in modern models. As a case\nstudy, the tokenizer used for OpenAI's GPT-4 uses a vocabulary called\ncl100k_base, which contains 100k tokens in addition to the 256 byte-sized\nones. This is also the vocabulary (encoding) the tiktoken library uses. It can\nbe freely downloaded from OpenAI - a copy is available in my sample\nrepository. The file is base64 encoded, which is easy to unravel and we'll see\ntokens like:\n\n    \n    \n    \" Fritz\" 91083 \"Initially\" 91084 \"nodeValue\" 91085 \"_TRIANGLES\" 91086 \"-backend\" 91087\n\nThe token string value is to the left, and the numerical token ID is to the\nright. As you can see, the algorithm is not particularly discerning about what\nit learns - names, pieces of code - whatever works!\n\nThe other important data needed to reproduce OpenAI's tokenization is the\nsplitting regexp, which is this:\n\n    \n    \n    (?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\n\nIt's just a combination of several alternatives. You could use one of the many\n\"regexp explainer\" websites out there to study it, or ask a modern LLM, but\nthe gist of it is: this regexp splits space-delimited words, leaving spaces in\nfront of the words, with some special provisions like English contractions\n(being separate words) and long numbers being split to groups of 3. For Go\nprogrammers, it's important to note that this pattern uses ?! - negative\nlookahead - which the standard regexp package doesn't support. Therefore,\nwe'll have to reach for the 3rd party regexp2 to implement this [3].\n\nIn our sample repository, take a look at this test that ties everything\ntogether - it loads the cl100k_base encoding and uses it alongside the\nsplitting regexp to tokenize some real text.\n\n## Full online demo with a web UI and WebAssembly\n\nMy goal with this project wasn't only to understand the BPE algorithm, but to\nalso try reproducing the actual tokenizer used by OpenAI for its most modern\nmodels. And this goal was accomplished!\n\nOpenAI has a nice website here that lets you enter text and see how it's\ntokenized. I've managed to reproduce this UI - see the cmd/wasm directory in\nthe repository. I've also placed it online - it can ran in your browser from\nhere. Here's a screenshot [4]:\n\nHow it works: the Go implementation of BPE is compiled to a WebAssembly binary\nthat's loaded from a bit of glue JavaScript embedded in a simple HTML page.\nThe JavaScript watches the text box as you type and sends the string to a Go\nfunction exported from the WASM, which tokenizes it on the fly. So we get a\nnice effect of \"tokens updated as we type\". The selection button at the bottom\nalso lets us see the numerical IDs for these tokens - they should be\nequivalent to what tiktoken is producing.\n\n[1]| For simplicity, this post will focus on English. As you'll see, however,\nthe BPE algorithm is language-agnostic.  \n---|---  \n[2]| There's also a performance implication: if we make tokenization word-\noriented, we can easily implement streaming tokenization without depending on\nprevious words.  \n---|---  \n[3]| I think it would be possible - with a bit of effort - to work around this\nlimitation and stick to the standard library, but just using regexp2 is\nsimpler, and it's also what tiktoken-go is doing.  \n---|---  \n[4]| You'll notice that in this example every word (except contractions) is a\nseparate token; this shouldn't be surprising, since these are all very common\nwords and the vocabulary is large! Try playing with it a bit though, giving it\nlonger words (like \"discombobulated\") or non-trivial variable names from a\nprogramming language.  \n---|---  \n  \nFor comments, please send me an email.\n\n\u00a9 2003-2024 Eli Bendersky\n\nBack to top\n\n", "frontpage": false}
