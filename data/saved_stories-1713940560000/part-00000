{"aid": "40136076", "title": "GPU Compute in the Browser at the Speed of Native: WebGPU Marching Cubes", "url": "https://www.willusher.io/graphics/2024/04/22/webgpu-marching-cubes", "domain": "willusher.io", "votes": 12, "user": "Twinklebear", "posted_at": "2024-04-23 19:22:22", "comments": 0, "source_title": "GPU Compute in the Browser at the Speed of Native: WebGPU Marching Cubes", "source_text": "GPU Compute in the Browser at the Speed of Native: WebGPU Marching Cubes\n\n# GPU Compute in the Browser at the Speed of Native: WebGPU Marching Cubes\n\nApril 22, 2024\n\nWebGPU is a powerful GPU-API for the web, providing support for advanced low-\noverhead rendering pipelines and GPU compute pipelines. WebGPU\u2019s support for\nGPU compute shaders and storage buffers are a key distinction from WebGL,\nwhich lacks these features, and make it possible to bring powerful GPU-\nparallel applications to run entirely in the browser. These applications can\nrange from GPGPU (e.g., simulations, data processing/analysis, machine\nlearning, etc.) to GPU compute driven rendering pipelines, and applications\nacross the spectrum.\n\nIn this post, we\u2019ll evaluate WebGPU compute performance against native Vulkan\nby implementing the classic Marching Cubes algorithm in WebGPU. Marching Cubes\nis a nearly embarrassingly parallel algorithm, with two global reduction steps\nthat must take place to synchronize work items and thread output locations.\nThis makes it a great first GPU-parallel algorithm to try out on a new\nplatform, as it has enough complexity to stress the API in a few different\nways beyond simple parallel kernel dispatches but isn\u2019t so complex as to take\nan substantial amount of time to implement or be bottlenecked by CPU\nperformance.\n\n# Introduction to Marching Cubes\n\nMarching Cubes is a classic algorithm in computer graphics for 3D surface\nreconstruction that was first introduced in 1987 by Lorensen and Cline. Given\na scalar field defined over a 3D grid, Marching Cubes can be used to compute a\ntriangle mesh of the surface at a given scalar value. The initial motivation\nfor Marching Cubes was medical visualization, where given a 3D volume from a\nCT or MRI scan, the algorithm was used to compute surfaces representing bone\nor tissue for display. Marching Cubes and its follow-on algorithms are widely\nused in scientific visualization to compute surfaces on simulation data sets\nand medical imaging volumes; and can also be applied to rendering mathematical\nimplicit surfaces defined over a 3D grid domain.\n\nThe algorithm is a bit easier to first understand in 2D, where it turns into\nthe Marching Squares algorithm. Given a grid with values defined in each cell,\nwe want to compute contour lines on the grid placed at a given value. For\nexample, on a map we may want to draw elevation lines at a certain elevation.\nThe Marching Squares algorithm proceeds as shown in the figure below. Marching\nCubes can be seen as generalization of Marching Squares into 3D, although the\nMarching Cubes algorithm was actually published first and Marching Squares was\na simplification of it into 2D.\n\nFigure 1: Given the cell-centered grid (a), we shift to the Dual Grid (b, in\nblue), where values are defined at the vertices instead of the cell centers.\nWithin a cell (c) we can now classify it into a contour case based on the\nvalues at its vertices (d). We compute a bitmask of 4 bits (one per vertex),\nwhere the bit is set if the vertex value is below the contour value, and 0 if\nit's above or equal to it. This bitmask is then used as an index value into a\ncase table that defines the edges that the contour crosses of the cell (e).\n\nAlthough we\u2019ll focus on a standard GPU-parallel implementation of Marching\nCubes here, we need some understanding of how the algorithm processes the data\nand interdependencies in the algorithm (if any) to see how it can be\nparallelized. For a more thorough discussion of Marching Cubes in a scalar\nexecution context, see Paul Bourke\u2019s excellent write-up. Another cool write up\nthat explains the basics of Marching Cubes starting from 2D is this write up\nby 42yeah.\n\nTo construct the triangle mesh representing the entire surface, Marching Cubes\nbreaks the problem down to the task of computing the individual pieces of the\nsurface that pass through each voxel. There are a number of possibilities for\nhow the surface may pass through a voxel: it could be that the surface cuts\nthe cell in half, clips a corner of it, passes through at some angle, or does\nnot pass through at all. Each voxel has 8 vertices, each of which can be\neither inside or outside the surface, giving us a total of 2^8 = 256 possible\nconfigurations for a voxel. For each voxel we compute an 8-bit mask, where a 1\nis marked for vertices below the desired surface value. The voxel bit mask is\nused to index into a triangulation case table that stores the list of edges\nthat are intersected by each triangle in the voxel\u2019s local surface. Each\nvoxel\u2019s surface can contain up to 5 triangles, following from the\ntriangulation cases defined in the original Marching Cubes paper.\n\nFigure 2: Case 1 in the Marching Cubes triangle table: The isosurface crosses\nedges 0, 3, and 8, defining a single triangle within the cell. The case index\nis a bitmask where each bit marks 1 if the vertex's value is below the\nisovalue and 0 otherwise. This produces an index into the case table, which\nstores a triangulation for each case as the list of edges where the triangle's\nvertices should be placed.\n\nA serial version of the algorithm can be implemented with the following\npseudo-code\n\n    \n    \n    output_vertices = [] for v in voxels: vertex_values = load_voxel_vertices(v) case_index = compute_case_index(vertex_values, isovalue) # No triangles in this voxel, skip it if case_index == 0 || case_index == 255: continue triangulation = case_table[case_index] for tri in triangulation: vertices = compute_triangle_vertices(tri, vertex_values, isovalue) output_vertices += vertices\n\n## Working on the Dual Grid\n\nMarching Cubes requires each voxel\u2019s values to be defined at its vertices,\ni.e. that the grid is \u201cvertex-centered\u201d. However, when we think about images\nor volumes (essentially 3D images), we typically think of them as cell-\ncentered, where the values are defined at the center of each pixel or voxel.\nIn order to run Marching Cubes on a cell-centered grid, we shift on to its\n\u201cDual Grid\u201d. The Dual Grid places vertices at the cell centers of the cell-\ncentered grid, producing a vertex-centered grid that is 1 cell smaller along\neach axis, and shifted up by half a cell:\n\nFigure 3: Cell-centered grids are widely used in simulation and data scanning\nsystems; however, Marching Cubes requires values defined at the vertices. For\nregular grids we can simply shift computation on to the Dual Grid, which is\ndefined by placing a vertex at each cell's center in the cell-centered grid.\nThe resulting Dual Grid has 1 fewer cells on each axis and is shifted up\nspatially by 0.5 on each axis.\n\n# GPU-Parallel Marching Cubes\n\nFrom the pseudo-code above, we can see that Marching Cubes is well-suited to\nparallel execution, as each voxel can be processed independently. However,\nthere is one array that is written by all voxels and which will be a write\nconflict without some care: the output_vertices array. Each voxel can output a\nvarying number of vertices depending on its vertex values. We know that each\nvoxel can output at most 5 triangles, so we could conservatively allocate\nspace for 5 triangles per voxel and fill unused entries with degenerate\ntriangles; however, this would require a prohibitive amount of memory for\nlarge surfaces and impact rendering performance. Instead, we would like to\noutput a compact buffer containing just the valid output triangles.\n\nTo write a compact vertex buffer while preventing voxels from trampling each\nother\u2019s output, we will split up the triangulation process into two kernels,\nand leverage the classic parallel primitive \u201cPrefix Sum\u201d (also known as\nExclusive Scan). First, we compute the number of vertices that each voxel will\noutput, then we use an exclusive scan to turn this list into a sequence of\noutput offsets for each voxel. Finally, we run a second kernel that writes\neach voxel\u2019s triangles to its assigned subregion of the array using the offset\ncomputed by the exclusive scan.\n\nWe\u2019ll also apply another optimization that requires a global synchronization\nstep to filter down the list of voxels that we run the vertex count and\ncomputation kernels over. This optimization is similar to the early out in the\npseudo-code. In a GPU-parallel context we will do this by computing a list of\nactive voxel IDs. Our subsequent vertex computation kernels then only operate\non the active voxels. The main benefit of this step is to increase GPU\nutilization and coherence for our vertex computation kernels. Rather than\ndispatching thousands of threads that immediately early-exit due to their\nvoxel not containing triangles, we can efficiently filter them out ahead of\ntime so that our vertex computation kernels only operate on a much smaller\nlist of active voxels. Isosurfaces are typically very sparse, with just\n4.5%-45% of the voxels may be active (check out my paper for some stats).\n\n## Exclusive Scan\n\nWe\u2019ll use a classic work-efficient parallel scan implementation, described for\nCUDA by Harris et al. in GPU Gems 3. Harris et al.\u2019s write-up is excellent,\nand the translation of it to WebGPU and WGSL is a direct translation of the\nCUDA code, so here I\u2019ll just cover some details of how scan works to see why\nit\u2019s useful in a GPU parallel Marching Cubes implementation.\n\nExclusive (and inclusive) scans form a common building block of data-parallel\nalgorithms, one common use for it is computing memory offsets for outputting\ndata in parallel, as we need to do for Marching Cubes. This works by first\noutputting a counts buffer that records how many outputs each input will have.\nFor example, after computing the number of triangles to be output by each\nvoxel we could have a buffer like below:\n\n    \n    \n    counts = [0, 3, 2, 0, 0, 5, 4]\n\nNow we know how many triangles each voxel will output, but we don\u2019t know where\nto write them in the output buffer without trampling outputs from other\nvoxels. Exclusive scan to the rescue! The exclusive scan computes the output y\nsum over the input x as:\n\n    \n    \n    y_0 = 0 y_1 = x_0 y_2 = x_0 + x_1\n\nApplying this to our counts buffer would produce:\n\n    \n    \n    out = [0, 0, 3, 5, 5, 5, 10]\n\nTo get the total number of outputs we can make our output buffer 1 element\nlarger, resulting in:\n\n    \n    \n    out = [0, 0, 3, 5, 5, 5, 10, 14]\n\nGiven our counts buffer, we\u2019ve now been able to compute that we have 14 total\noutput triangles that we need to allocate room for, and the output addresses\nfor each voxel that will output triangles. Voxel 1 will write to offset 0,\nvoxel 2 will write to offset 3, voxel 5 to offset 5, and voxel 6 to offset 10.\n\nThe full implementation for this project can be found in: exclusive_scan.ts,\nand the shaders: exclusive_scan_prefix_sum.wgsl,\nexclusive_scan_prefix_sum_blocks.wgsl, and exclusive_scan_add_block_sums.wgsl.\nThe implementation must chunk up dispatches to fit within the WebGPU max\ncompute dispatch size limit, and makes use of bind group dynamic buffer\noffsets to have shaders access the right subregion of the input buffer when\nscanning through it in blocks.\n\n## Stream Compaction\n\nThe operation we stated simply when describing the parallel algorithm, to just\nrun the triangle count and computation kernels on the active voxels, requires\na stream compaction operation to be implemented to run in a data parallel\ncontext. Similar to how we need to compute a buffer of the addresses to write\neach voxel\u2019s outputs to, we need to compute a compact list of the voxel IDs\nthat may contain the isosurface. This operation is typically called \u201cstream\ncompaction\u201d, where we have a set of elements as input along with an array of\nmasks specifying if we want to keep or discard an element. The output will\neither be the elements themselves, or in our case, the indices of the\nelements. Fortunately, we can perform this compaction by re-using the\nexclusive scan operation we just discussed above.\n\nOur masks buffer will have the form:\n\n    \n    \n    masks = [1, 0, 0, 1, 1, 0]\n\nAnd computing an exclusive scan on the masks will produce a list of offsets to\nwrite the compacted results out to, along with the total number of items\n(again offsets is 1 element bigger than the masks array)\n\n    \n    \n    offsets = [0, 1, 1, 1, 2, 2, 3]\n\nWith that, all that\u2019s left to do is write a kernel that runs over each input\nelement and writes its index to the specified offset if its mask value was 1!\n\nA more memory efficient implementation is also possible by running the scan in\nplace on the masks buffer and checking if an element is active by checking if:\n\n    \n    \n    offset[current] < offset[current + 1]\n\nIf this is true, it means the current element was active, as it caused the\noutput offset for elements following it to be increased to make room for its\noutput.\n\nThe implementation of the stream compaction that compacts the active voxel IDs\ndown based on the offsets buffer is in: stream_compact_ids.ts and\nstream_compact_ids.wgsl. The implementation is exactly as described here, with\nsome additional handling to deal with chunking up the scan kernel dispatches\nwhen we exceed the max dispatch size.\n\n# Putting it all Together in WebGPU\n\nNow that we\u2019ve implemented our data-parallel primitives: exclusive scan and\nstream compaction, we\u2019re ready to fill in the Marching Cubes specific parts of\nthe computation to implement our parallel algorithm. There are three stages to\nthe algorithm:\n\n  1. Compute active voxel IDs\n  2. Compute vertex output offsets\n  3. Compute vertices\n\nAfter which we can render the computed surface.\n\n## Compute Active Voxel IDs\n\nComputing the list of active voxel IDs consists of three steps:\n\n  1. We run a kernel to mark the active voxels, producing the mask buffer\n  2. Exclusive Scan the mask buffer to produce output offsets for active voxels\n  3. Compact the active voxel IDs down using the offsets via Stream Compaction\n\nWe\u2019ve implemented the primitives for steps 2 and 3, so the last thing to write\nis a kernel to run over the dual grid that computes if a voxel is active or\nnot. The kernel is a direct translation of the first bit of the pseudocode we\nsaw above performing the early out, except that instead of calling continue it\nwrites a 0 or 1 to the mask buffer.\n\nThe full kernel code can be found in mark_active_voxel.wgsl, and is written\nbelow in a briefer psuedo-shader form. This kernel is run for every voxel in\nthe Dual Grid to produce the mask buffer in parallel.\n\n    \n    \n    float values[8] = compute_voxel_values(voxel_id); uint case_index = compute_case_index(values, isovalue); voxel_active[voxel_idx] = case_index != 0 && case_index != 255 ? 1 : 0;\n\n## Compute Vertex Output Offsets and Count\n\nNow that we have a list of all the voxels that may output triangles, we can\ncompute how many triangles each one will output to determine the output\noffsets and total triangle count. This is done using a kernel\ncompute_num_verts.wgsl, that is run in parallel over the active voxel IDs and\noutputs the number of vertices that voxel will output. Since we\u2019re not using\nindexed rendering, every 3 vertices forms a triangle.\n\nThe compute num verts kernel computes the number of output vertices for a\nvoxel using the Marching Cube case table. As we saw in the first sketch\nshowing the calculation for a single voxel, the case table defines the edges\nof the voxel that the triangle\u2019s vertices lie on, and marks the end of the\nsequence with a -1. Thus, our kernel can load the voxel\u2019s values, compute the\ncase index, and then loop through the case table values for that case to\ncompute the number of vertices that will be output:\n\n    \n    \n    float values[8] = compute_voxel_values(voxel_id); uint case_index = compute_case_index(values, isovalue); int triangulation[16] = case_table[case_index]; uint num_verts = 0; for (uint i = 0; i < 16 && triangulation[i] != -1; ++i) { ++num_verts; } voxel_num_verts[work_item] = num_verts;\n\nWe then perform an exclusive scan on the voxel_num_verts output buffer to\ncompute the output offsets for each voxel and the total number of vertices to\nbe output.\n\n## Compute Vertices\n\nWith the output offsets ready and the output vertex buffer allocated, all\nthat\u2019s left to do is actually compute and output the vertices! This is done by\nthe kernel compute_vertices.wgsl. It takes the list of active voxel IDs and\nthe output offsets computed previously and computes the vertices for each\nvoxel and writes them to the output. This kernel is run in parallel over all\nactive voxels.\n\nThe vertices for each voxel are computed by looping through the triangulation\nfor its case and lerp\u2019ing between the vertices at the end of each edge by the\nfield value to compute the position where the isosurface intersects the edge.\nThe vertex is first computed in the unit voxel, then offset by the voxel\u2019s\nposition in the volume grid and the 0.5 offset of the Dual Grid. This is\nwritten below in pseudo-shader code, see the kernel code on Github for the\nfull code.\n\n    \n    \n    float values[8] = compute_voxel_values(voxel_id); uint case_index = compute_case_index(values, isovalue); int triangulation[16] = case_table[case_index]; uint output_offset = vertex_offsets[work_item]; for (uint i = 0; i < 16 && triangulation[i] != -1; ++i) { // Lerp the edge start/end vertices to find where the isosurface // intersects the edge uint v0 = edge_start_vertex(triangulation[i]); uint v1 = edge_end_vertex(triangulation[i]); float t = (isovalue - values[v0]) / (values[v1] - values[v0]); vec3 v = lerp(voxel_pos[v0], voxel_pos[v1], t); // Now offset v by the voxel position in the dual grid and output output_vertices[output_offset + i] = v + voxel_pos + 0.5; }\n\n## Render Surface\n\nComputing the surface took a bit of work and quite a few compute passes across\nall our kernels! Fortunately, rendering it is very simple. The output of our\ncompute vertices pass is a triangle soup with all the vertices for all voxels\nwritten out as an unindexed list of triangles that we can simply draw as a\n\u201ctriangle-list\u201d topology geometry without an index buffer.\n\n# Conclusion: WebGPU Performance vs. Vulkan\n\nWith our implementation done, it\u2019s time to see how it stacks up against a\nnative Vulkan implementation! I\u2019ve implemented the exact same code in Vulkan,\nwhich you can find here on Github. For these tests we\u2019ll use a few small\ndatasets from OpenScivisDatasets, the Skull, Bonsai, Foot and Aneurysm. Each\ndataset is 256x256x256 for a total of 16.77M cells (the dual grid has 16.58M\ncells).\n\nIn both methods I ran a sweep up the isovalue from 30 to 110, covering\nisosurfaces of interest to a typical user (i.e., not too much noise, not too\nsmall). The performance results are shown in the table below on a MBP with an\nM2 Max, and an RTX 3080. The RTX 3080 is used for the WebGPU vs. Vulkan\nperformance comparisons, where we find that performance on average is very\nclose to the native Vulkan version!\n\nDataset| WebGPU (M2 Max)| WebGPU (RTX 3080)| Vulkan (RTX 3080)  \n---|---|---|---  \nSkull| 43.5ms| 32ms| 30.92ms  \nBonsai| 42.5ms| 31ms| 29.3ms  \nFoot| 54.4ms| 32.5ms| 33.43ms  \nAneurysm| 40.8ms| 37.5ms| 27.45ms  \n  \nHaving access to a modern low-overhead GPU API in the browser is really\nexciting, and opens up a ton of new possibilities for games, content creation\ntools, scientific applications and more to run in the browser with near native\nperformance. Currently this application will only run in Chrome, but Safari\nand Firefox are working hard on WebGPU development. I\u2019m looking forward to the\nday that WebGPU support is widespread across vendors!\n\n## Live Demo and Code\n\nThe code for the WebGPU marching cubes implementation is available on Github,\nand should be runnable below in Chrome. Firefox Nightly and Safari Tech\nPreview aren\u2019t quite yet able to run it at the time of writing this post. You\ncan also try it out here: https://www.willusher.io/webgpu-marching-cubes/\n\nPrevious\n\n", "frontpage": true}
