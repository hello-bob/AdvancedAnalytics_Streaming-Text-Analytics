{"aid": "40206924", "title": "How to train diffusion for text from scratch", "url": "https://ghost.oxen.ai/how-to-train-diffusion-for-text-from-scratch/", "domain": "oxen.ai", "votes": 1, "user": "gregschoeninger", "posted_at": "2024-04-30 03:25:22", "comments": 0, "source_title": "How to Train Diffusion for Text from Scratch", "source_text": "How to Train Diffusion for Text from Scratch\n\nSign in Subscribe\n\nApr 29, 2024 16 min read Arxiv Dives\n\n# How to Train Diffusion for Text from Scratch\n\nThis is part two of a series on Diffusion for Text with Score Entropy Discrete\nDiffusion (SEDD) models. Today we will be diving into the code for diffusion\nmodels for text, and see how they work from the inputs to the outputs to the\nintermediate representations of the diffusion process itself. By the end of it\nyou will be able to train an end to end diffusion model for text and have a\nbetter understanding of the benefits and current limitations.\n\nWe do this series of ArXiv Dives live on Fridays with the Oxen.ai community.\nWe believe that it is not only important to read the papers, but dive into the\ncode that comes along with them to truly understand the implications, impact,\nand be able to apply the learnings to your own work.\n\nThe following are notes from our live session, feel free to follow along with\nthe video below.\n\n## Paper & Code\n\nDiscrete Diffusion Modeling by Estimating the Ratios of the Data Distribution\nby the teams at Stanford and Pika Labs.\n\nDiscrete Diffusion Modeling by Estimating the Ratios of the Data Distribution\n\nDespite their groundbreaking performance for many generative modeling tasks,\ndiffusion models have fallen short on discrete data domains such as natural\nlanguage. Crucially, standard diffusion models rely on the well-established\ntheory of score matching, but efforts to generalize this to discrete\nstructures have not yielded the same empirical gains. In this work, we bridge\nthis gap by proposing score entropy, a novel loss that naturally extends score\nmatching to discrete spaces, integrates seamlessly to build discrete diffusion\nmodels, and significantly boosts performance. Experimentally, we test our\nScore Entropy Discrete Diffusion models (SEDD) on standard language modeling\ntasks. For comparable model sizes, SEDD beats existing language diffusion\nparadigms (reducing perplexity by $25$-$75$\\%) and is competitive with\nautoregressive models, in particular outperforming GPT-2. Furthermore,\ncompared to autoregressive mdoels, SEDD generates faithful text without\nrequiring distribution annealing techniques like temperature scaling (around\n$6$-$8\\times$ better generative perplexity than un-annealed GPT-2), can trade\ncompute and quality (similar quality with $32\\times$ fewer network\nevaluations), and enables controllable infilling (matching nucleus sampling\nquality while enabling other strategies besides left to right prompting).\n\narXiv.orgAaron Lou\n\nBig shout out to @louaaron and team for the initial code implementation that\nwas released along side the paper. For this dive, we stripped down the\noriginal code base and simplified for optimized for my own clarity and\nunderstanding. The modified codebase can run on a single A10 GPU depending on\nthe model and batch sizes and tokenizer you use.\n\nGitHub - Oxen-AI/Score-Entropy-Discrete-Diffusion: Modified Score-Entropy-\nDiscrete-Diffusion to do a character level ml model and integrate with Oxen\n\nModified Score-Entropy-Discrete-Diffusion to do a character level ml model and\nintegrate with Oxen - Oxen-AI/Score-Entropy-Discrete-Diffusion\n\nGitHubOxen-AI\n\n## Quick Refresher on Diffusion for Text\n\nWe went over the math and paper in detail last week, but to set context the\nend goal is a generative LLM that instead of generating text token by token,\ncan generate the entire sequence at once.\n\nJust like diffusion for images or video, the idea is we start with a noisy\nrandom text sequence and slowly de-noise text into a coherent passage. You\nshould be able to trade off compute (time steps) for text quality and\ncoherence.\n\n## Why diffusion for text?\n\nThe paper cites a few reasons that diffusion is appealing for text. The first\nis pure performance when measured by perplexity (how \u201cperplexed\u201d is the model\nto see the next word given a dataset and the model's predicted probabilities).\n\nSEDD Outperforms GPT-2 model of same size trained on the same amount of data.\n\nYou may be thinking...but GPT-2 is old? Yes but we must start research\nsomewhere. What\u2019s nice is you don\u2019t need too much compute to reproduce at\nleast this size of a model, and you can trade off compute for quality during\ninference which is harder to do with standard auto-regressive modeling.\n\nThe second reason is that diffusion models are more optimized for parallel\ncompute, and can take advantage of modern hardware like GPUs better than auto-\nregressive models. Auto-regressive models predict the next token one by one\ngiven the previous context. This means we must process the text sequentially\ninstead of dividing and conquering in parallel.\n\nThe third reason is the fact that if an auto-regressive model makes a mistake,\nthe errors can cascade. Yann Lecun calls this a \u201cgeneration drifts\u201d from the\ndata distribution and causes the model to diverge during sampling. Diffusion\nhas the benefit of using the complete global context during generation,\nresulting in better overall generation.\n\nFinally, diffusion enables prompting from any position, or multiple positions,\nenabling use cases such as infilling or completing a prefix before a suffix.\n\nLet\u2019s put these reasons to the test...\n\n## Diving into training details \ud83e\udd3f\n\nBefore starting any model training, you need a dataset. To reproduce results\nsimilar to GPT-2, we need a dataset of text from the web. Luckily people have\nre-created such a dataset called OpenWebText. We took the time to index it\ninto Oxen.ai if you want to reproduce the full paper results. Feel free to\ndownload it from here the link below.\n\ndatasets/OpenWebText | Datasets at Oxen.ai\n\nOpen WebText \u2013 an open source effort to reproduce OpenAI\u2019s WebText dataset.\nThis distribution was created by Aaron Gokaslan and Vanya Cohen of Brown\nUniversity.. Contribute to the datasets/OpenWebText repository by creating an\naccount on Oxen.ai\n\nDatasets at Oxen.ai\n\nIf you are not familiar with Oxen.ai, it is a hub for large multi-modal\nmachine learning datasets. Files of the scale and size that you would never\ncheck into git seamlessly index into Oxen.\n\nThe models cited in the paper are the same sizes as GPT-2. They also have the\nsame tokenizer which contains a vocab of 50256 tokens.\n\n## Pre-Trained SEDD Demo\n\nThe first thing to do when trying out any model\u2019s codebase is to test out any\npre-trained models they have and see how it performs.\n\nYou can download the pre-trained model from Oxen.ai by using the oxen clone\ncommand.\n\n    \n    \n    oxen clone https://hub.oxen.ai/models/SEDD-large\n\nThen run the script from the GitHub Repo to sample from it.\n\n    \n    \n    python scripts/run_sample.py --model /path/to/SEDD-large/ --steps 1\n\nYou will see text that does not look very coherent with a single step. Some\nenglish words, some invalid UTF-8. But promising start given it is supposed to\nstart with noise. Let\u2019s crank up the number of diffusion steps and see if it\nimproves.\n\n    \n    \n    python scripts/run_sample.py --model /path/to/SEDD-large/ --steps 10\n\nA little better...\n\nWe can see we have more valid english words, and some phrases that are\ndefinitely coherent. Now crank it up to 1024 and see what the output looks\nlike.\n\n    \n    \n    python scripts/run_sample.py --model /path/to/SEDD-large/ --steps 1024\n\nThis time the entire context is rather coherent, and talking about the same\nsubject the whole time - race cars.\n\nEach time you run the script, it with sample randomly so it is hard to see how\nthe text actually changes step by step. You can pass in a --show_intermediate\nflag to the script to print out the intermediate outputs and see how it slowly\ndiffuses the text.\n\n    \n    \n    python scripts/run_sample.py --model /path/to/SEDD-large/ --steps 10 --show_intermediate\n\nWe can see we are progressively swapping tokens as we go along, to eventually\narrive at something more and more! That\u2019s great.\n\nNow how does it actually work under the hood?\n\n## Sampling with Model, Graph, and Noise\n\nIf you recall from our dive last week, it is infeasible to add/remove noise\nevery token at once within the sequence. Unlike images, text is represented by\ndiscrete values. Where small perturbations in pixel values result in an almost\nidentical image, this is not the case with text.\n\nTake for instance a simple Caesar Cipher. Remember, text is usually\nrepresented by tokens, which map to indices in a lookup table. If we change\nthe index by just adding +1 to each value, we turn a coherent sentence into\ngarbage in one fell swoop.\n\n    \n    \n    python scripts/caeser_cipher.py \"Uploading large datasets with Oxen.ai is a satisfying \ud83d\ude0c\"\n\n## The Graph\n\nThis means the diffusion process cannot add noise by simply adding a small\nnumber to the text indices. This paper addresses this problem by instead only\nmodifying a subset of token indices at each step, progressively flipping more\nand more. The number of tokens we flip is called the \u201chamming distance\u201d. If\nyou only flip one token in a phrase, this would be a hamming distance of one.\n\nIn theory, we can have a noise parameter that is passed into the model that\nrepresents roughly the hamming distance to the original text as context, then\nask the model to predict which tokens to flip to make the text more coherent.\n\nIn the code, the component that does the flipping is called the AbsorbingGraph\n. The component that generates the noise is called LogLinearNoise . Together\nthey can be used during model training and inference to corrupt text with\nnoise and then de-noise it.\n\nLet\u2019s demo the AbsorbingGraph in action with a script that flips a random\namount of tokens at given time step values.\n\n    \n    \n    python scripts/demo_perturb.py \"Applying diffusion to text is kind of crazy\" -t 0.35\n\nIn the case above, we passed in a time step value of 0.35 , tokenized the\ntext, and then flipped a subset of tokens to the value 50257. If you recall,\nthe tokenizer for GPT-2 has 50256 tokens, so this flipped value is an extra\ntoken that we can \"absorb\" and replace with '' (an empty string).\n\nThe text was perturbed from \u201cApplying diffusion to text is kind of crazy\u201d \u2192\n\u201cApplying diffusion text is kind\u201d. The idea is that at training time we will\nrandomly noise the training data with different known values and the models\njob is to figure out which tokens to flip back given the current text and the\nknown noise level.\n\nThe code that actually does the flipping is in sedd/models/graph.py under the\nAbsorbingGraph.sample_transition function. Let's add some print statements to\nsee how this works in practice.\n\nLet's run the same script again and see what these intermediate values are.\n\n    \n    \n    python scripts/demo_perturb.py \"Applying diffusion to text is kind of crazy\" -t 0.35\n\nFor one, you can see the randomness because we now perturbed the text to be\n\u201cApp diffusion to crazy\u201d. The move chance represents how likely we are to swap\nany individual token to 20257 and then we get our a mask to swap those tokens\nappropriately.\n\n## Loss Function\n\nIf you look at the loss function, we sample a random t and noise, perturb the\ntokens, have the model look at the perturbed sentence, the noise, and the\noriginal sentence to compute our loss value.\n\n    \n    \n    sedd/trainer/loss.py\n\nThe model\u2019s job is to assign high probability transitions to tokens that\nshould be flipped given the level of noise. During training we sample a random\ntime step each iteration, and the model has to guess which tokens were flipped\nand with what probability.\n\nThere is a script called scripts/demo_loss.py to make this more concrete.\nUncomment the log_score and loss values before running this script.\n\n    \n    \n    python scripts/demo_loss.py \"Oxen plows and maintains your fields of data\" -m /path/to/SEDD-large/ -t 0.5\n\nLook closely at the shape of the log_score and the loss tensors. In this case\nwe have 10 tokens. We then corrupt the tokens with a move probability of 0.5.\nThe score that comes out of the model is [batch_size, context_len,\nvocab_size]. This means for each token we have a probability mass function\nrepresenting which token from the vocab we should replace each token in our\nsequence with. If the probability of the current token is close to what the\nmodel predicts, our loss will be low.\n\nIf we do this over and over again during training, Monte Carlo sampling random\nlevels of noise and our training distribution, eventually we will be able to\nestimate the full probability mass function and flip the tokens to more and\nmore coherent values.\n\n## The Model\n\nUnder the hood, the model is simply a transformer with an output layer the\nshape of the log_score tensor above. The paper uses the same number of layers\nand block size as the GPT-2 paper. A transformer is able to use it's self\nattention mechanism to look at the entire context of the noised input to try\nto decide which tokens to flip.\n\nThey pass in the time step as an embedded value as well as use rotary\nembeddings for encoding the position of each token. Flash attention is used to\nspeed up compute. Full model code can be found in sedd/models/sedd.py.\n\n## Inference\n\nNow take a look at the inference code that reverses the text from random noise\nto coherent text.\n\nGiven the number of time steps, we generate a linear set of values from [1.0\n.. 0.0]. Then we sample from the model passing in the previous prediction, the\ntime step, and the delta in time step. The projector function is used for\nprompting simply overwriting tokens with fixed values if we provide them.\n\nWhat's cool is you can use the simple projector function to prompt from any\nlocation in your text.\n\n    \n    \n    def proj_fun(x): x[:, input_locs] = input_ids return x\n\nThis overwrites indices during inference and insure the model uses them for\nthe next time step.\n\nTo see the prefix and suffix prompting in action, you can run\nscripts/run_sample_cond.py\n\n    \n    \n    python scripts/run_sample_cond.py --model /path/to/SEDD-large/ --prefix \"Hi my name is\" --suffix \"and that's why I'm late\" --steps 1024\n\n> Hi my name is Sydney Jordan Brothers. I was a very good basketball player\n> when I got there. Petersen and Richie, when I had, well, they were not an\n> adopted brother but they were some similarities []. They both killed high\n> balls. They were much brotherlier than me and much calmer than me. And\n> they're both very strong guys. They're really\u2014they're really strong. I think\n> was what keeps them happy. When I got there, I didn't tell anybody because\n> we heard about the same thing because I was a nonpro player. When I looked\n> around and I was really happy, they no told what about me back then. But\n> there were stupid rumors about running down the street and the same way we\n> did things over there. But people you know why we're happy is there was\n> different environment and it's a different type of individual I guess.\n> Nobody was ready to workout like we grew up doing. It's just, I was a first\n> drop by teams. And it was only because the coaches had pushed me and I was\n> ready to do it. It was another thing to\u2014you know, because nobody knew about\n> nobody, man, I wasn't happy whatsoever. You was never after them for three\n> years. You like it for a little bit. There's a group of people that love\n> you. You time out your business, and people you met at work were like, a few\n> of them then come back and see your time. It's a bit like a motion shoot.\n> You know, it was the best part of my career. It was a lot and I was pretty\n> good at making it work. You think how they treated you as a basketball\n> player is a curse or curse? Just as I described it, that's the main thing\n> why I'm happy for a basketball player, I guess.\n\n> CW: Right. For most guys it's the same thing. Yeah, you go drive to the mall\n> going down there and you walk into the street and you drive by your family.\n> You drive at night because that's normally it. Those are your only customers\n> so you drive by your family a lot. Yeah. For me, these days, I drive by my\n> house. That's the same thing is you're a pro at this time at practice and\n> they can send you to immediately go in and go to the house, at night, to see\n> if you like the house. Until then, the house. If I'm sorry to tell you that\n> you were offended or saying something, it takes awhile and then just shut up\n> like me because you get used to it so much, you're not talking about\n> yourself, you're just about the house. Or the house, you're the housekeeper.\n> The core thing is, because of the way you are treated and compared to guys\n> the like of the leagues, perhaps you'd different and be done differently.\n> That, I think there should be. Most players think it's for the league to\n> give them better treatment because the players are\u2014you know, there are ten\n> million five thousand of them, when you don't know them, let's be honest\n> with you. That could get us killed. But they could be freaking out. Not\n> today. I don't think they want that. The other reason, for that to be a\n> tough play, is, these players get paid. If you have millions in sports, you\n> can be big, and if you can be little, you might not have so much. And they\n> are playing here. So, when they first came in, and they know you, and they\n> know the pay and you're sitting and they expect you to get up to the car and\n> pay up and drive home. You're not supposed to say anything. It's just like,\n> I paid. We don't have\u2014money was not as good as us. But they have a bit. How\n> is there money then? Yeah, they'd just have to use it more on our behalf.\n> Yeah, that's the way they've got to be careful, not the players, not the\n> NBA, the league. I think these years, until it's too late\u2014Mille Mason didn't\n> hang around when I was 10. I mean, yeah, I hung around at home.. He threw\n> perfect. I mean, we didn't know he played football until 8. So, I remember\n> him. I didn't get around when he was like 10. And he's just no slow mover\n> just. He was a [earner than the first 300 receivers] from when he was done\n> until he got released. So Nick comes along and they settle in, and he's my\n> friend, and I'm there. I had an introduction with him. We at, i think, hey,\n> this is you, we're talking to you, we are getting to know you and we know\n> you, it's life, it's not like you have to have to be a regular job and it's\n> supposed to be fun. So that's the real thing about it\u2014and that's why I'm\n> late\n\nIn theory this would be really nice to be able to prompt something like the\nthe beginning of a code snippet, and the end, and be able to fill in the\nmiddle. In practice we fill the entire context length of 1024, so would have\nto modify the training regiment to output '' in the middle of sequences so\nthat the infilling length of text is more dynamic.\n\n## Training from Scratch\n\nNow that you have seen the noising during training and the sampling process,\nlet\u2019s put it all together and watch this thing train on a character level\nmodel. For this script, we replace the full GPT-2 tokenizer with a simple\ncharacter level tokenizer, and reduce the model size significantly. This is\nsimply so that we can see results quicker and verify everything is working end\nto end.\n\n    \n    \n    python scripts/run_train.py --repo YOUR_OXEN_USERNAME/SEDD_baby_names\n\nThis script trains the model on baby names with a context length of 32 and\npadding of empty string at the end. The training data is from our\ndatasets/baby_names repository on Oxen.ai.\n\ndatasets/baby_names | Datasets at Oxen.ai\n\nBaby names, sex, count and year. Contribute to the datasets/baby_names\nrepository by creating an account on Oxen.ai\n\nDatasets at Oxen.ai\n\nTo run the script fully, you will need to create an Oxen.ai account because we\nwill be saving intermediate results there to compare over time.\n\nEven after minimal training on 1000 steps we are starting to get short text\nstrings that have capital letters at the start.\n\nAfter training for a significant amount of time, we start to get actual\nstrings that look like baby names out \ud83c\udf89.\n\n## Next Steps / Conclusion\n\nIf you have enough compute, it would be interesting to train the full model on\na larger dataset than even OpenWebText. It remains to be seen how well this\napproach scales up to models the size of GPT-3 and beyond.\n\nOverall I think this approach is cool in theory, but after playing with it the\ninfilling capabilities are still in early days and not practically usable as\nis out of the box.\n\nHopefully this post gave you a good understanding of how diffusion for text\nworks, how you can train it on your own data, and I hope you try some\nexperiments of your own!\n\nIf you like this kind of content, feel free to join our to Oxen.ai paper club\nor subscribe to our YouTube channel \ud83d\udc02\n\nOxen\n\nOxen.ai makes versioning your datasets as easy as versioning your code! Even\nis millions of unstructured images, we quickly handle any type of data so you\ncan build cutting-edge AI. Arxiv Dive: Each week we dive deep into a topic in\nmachine learning or general artificial intelligence research. The sessions are\nlive with a group of smart Oxen every Friday. Create an account: www.oxen.ai\nand join the discussion: https://lu.ma/oxenbookclub\n\nYouTube\n\n### Published by:\n\n### You might also like...\n\nApr\n\n15\n\n## ArXiv Dives: Text Diffusion with SEDD\n\nDiffusion models have been popular for computer vision tasks. Recently models\nsuch as Sora show how you can apply Diffusion\n\nApr 15, 2024\n\n11 min read\n\nApr\n\n08\n\n## ArXiv Dives: The Era of 1-bit LLMs, All Large Language Models are in 1.58\nBits\n\nThis paper presents BitNet b1.58 where every weight in a Transformer can be\nrepresented as a {-1, 0, 1}\n\nApr 8, 2024\n\n9 min read\n\nApr\n\n01\n\n## ArXiv Dives: Evolutionary Optimization of Model Merging Recipes\n\nToday, we\u2019re diving into a fun paper by the team at Sakana.ai called\n\u201cEvolutionary Optimization of Model Merging\n\nApr 1, 2024\n\n10 min read\n\nMar\n\n25\n\n## ArXiv Dives: I-JEPA\n\nToday, we\u2019re diving into the I-JEPA paper. JEPA stands for Joint-Embedding\nPredictive Architecture and if you have been following\n\nMar 25, 2024\n\n13 min read\n\nMar\n\n20\n\n## How to train Mistral 7B as a \"Self-Rewarding Language Model\"\n\nAbout a month ago we went over the \"Self-Rewarding Language Models\" paper by\nthe team at Meta AI\n\nMar 20, 2024\n\n17 min read\n\n### Member discussion\n\n1 comment\n\nOxen.ai \u00a9 2024\n\nPowered by Ghost\n\n", "frontpage": false}
