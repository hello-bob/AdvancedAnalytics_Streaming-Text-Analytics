{"aid": "40099681", "title": "Rust Stream API visualized and exposed", "url": "https://github.com/alexpusch/rust-magic-patterns/blob/master/rust-stream-visualized/Readme.md", "domain": "github.com/alexpusch", "votes": 1, "user": "todsacerdoti", "posted_at": "2024-04-20 18:35:41", "comments": 0, "source_title": "rust-magic-patterns/rust-stream-visualized/Readme.md at master \u00b7 alexpusch/rust-magic-patterns", "source_text": "rust-magic-patterns/rust-stream-visualized/Readme.md at master \u00b7\nalexpusch/rust-magic-patterns \u00b7 GitHub\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nalexpusch / rust-magic-patterns Public\n\n  * Notifications\n  * Fork 18\n  * Star 534\n\n/\n\n# Readme.md\n\n## Latest commit\n\nalexpusch\n\nAdd rust-stream-visualized\n\nApr 20, 2024\n\n0b201ca \u00b7 Apr 20, 2024Apr 20, 2024\n\n## History\n\nHistory\n\n282 lines (199 loc) \u00b7 16.5 KB\n\n/\n\n# Readme.md\n\n## File metadata and controls\n\n282 lines (199 loc) \u00b7 16.5 KB\n\nRaw\n\n# Rust Stream API visualized and exposed\n\nManaging concurrency in real-world applications can be quite tricky.\nDevelopers must grapple with concurrency control, back pressure, error\nhandling, and more. Thankfully, Rust provides us with the async/await\nmechanism, and on top of that, the Stream API. The Stream methods allow us to\nelegantly define a pipeline of asynchronous operations with a nifty\nabstraction addressing common use cases.\n\nUnfortunately, elegance sometimes masks complexity. Can you look at a\nstreaming pipeline and understand how many operations would run in parallel?\nWhat about the order? I found it trickier than it seems, so naturally, as a\ncomplete overkill, I wrote a Bevy visualization to investigate it. This\ninvestigation brought light to some truly unexpected results - so unexpected,\nthat in some cases, you might want to reconsider using this API.\n\n## Overview of Stream API\n\nLet's start with a brief overview of the Stream API. The following code\ndefines an async pipeline that iterates over integers from 0 to 10 and\nexecutes the async_work method with a concurrency limit of 3. The result is\nthen filtered using the async_predicate method. This is awesome! With just a\nfew lines of code, we've created a non-trivial async control flow.\n\n    \n    \n    async fn async_work(i32) -> i32 {...} async fn async_predicate(i32) -> Option<i32> {...} async fn buffered_filter_example() { let stream = stream::iter(0..10) .map(async_work) // async_work returns a future. The output of this stage is a stream of futures .buffered(3) // polls stream of futures and runs at most 3 concurrently .filter_map(async_predicate); // filters out the results of the previous stage using async_predicate function pin!(stream); while let Some(next) = stream.next().await { println!(\"finished working on: {}\", next); } }\n\nAmm, we can already see some complex elements. For instance, why did I use\nfilter_map instead of filter? What's this pesky pin!(stream) doing? I won't\ndigress into these questions. Instead, here are some informative links:\n\n  * Put a Pin on That\n  * How will futures::StreamExt::filter work with async closures?\n\nThe goal of this investigation is to get a better understanding of the\nexecution order, concurrency, and back pressure characteristics of such\npipelines. For example, in the code above, the map method executes 3\nasync_work concurrently, but what if async_predicate is a long operation? will\nit continue to concurrently run more async_work? Supposedly after it completes\n3 invocations, it should be able to run more while async_predicate runs in the\nbackground right? If so, will it take an unbounded amount of memory? What\nabout filter_map? it does not have a clear concurrency parameter. Does it runs\nthe provided method serially? or with unlimited concurrency?\n\nThe documentation leaves some of these questions unclear. We need to see it\nwith our own eyes.\n\n## Experiment tool - visualizing Rust streams\n\nI used Bevy to visualize the flow of data in a streaming pipeline. The idea\ninvolves defining a streaming pipeline with methods that report their progress\nvia a channel. I used Bevy's EventWriter to forward this information to a Bevy\nrendering system.\n\nHere's how it looks:\n\nIn the visualization, we see a representation of each streaming item\nnavigating through different stages of the pipeline. Units of work start from\nthe source and move to the map(..).buffered(..) stage. To simulate real world\nasync work I used a small loop of sleep() calls. This represents real world\nscenarios where async methods have multiple await calls and allows us to\nvisualize the future run progress.\n\n    \n    \n    for i in 0..5 { tokio::time::sleep(duration / 5).await; tx.send(/* update bevy rendering system */).unwrap(); }\n\nWe visualize future progress via a tiny progress bar on each item. After an\nitem completes the buffered stage, it proceeds to the sink and finishes its\njourney.\n\nIt is important to note that the visualization is sourced from actual running\nRust code. This isn't a simulation; it is a real-time visualization of the\nRust Stream pipeline.\n\nYou can find the source code here.\n\n## Experiment 1: buffered\n\n    \n    \n    stream::iter(0..10) .map(async_work) .buffered(5);\n\n> buffer up to at most n futures and then return the outputs in the same order\n> as the underlying stream. No more than n futures will be buffered at any\n> point in time\n\n### Experiment questions\n\n  * Will the buffered method fetch a new unit of work from the source stream as soon as any unit of work completes, or only when the earliest unit of work completes and sent as output to the next stage?\n\nAll right! look at it purr! As expected, each item goes through async_work.\nThe .buffered(5) step runs at most 5 futures concurrently, retaining completed\nfeatures until their predecessors completes as well.\n\n### Experiment result\n\nThe buffered method does not acquire a new unit of work once an arbitrary item\ncompletes. Instead, it only does so once the earliest item is completed and\nadvances to the next stage. This makes sense. A different behavior would\nrequire the buffered method to store the results of an unbounded number of\nfutures, which could lead to excessive memory usage.\n\nI wonder if there's a case to be made for a buffered_with_back_pressure(n:\nusize, b: usize) method that will allow some items to be taken from the source\nstream, up to b times.\n\n## Experiment 2: buffer_unordered\n\n    \n    \n    stream::iter(0..10) .map(async_work) .buffer_unordered(5);\n\n> buffer up to n futures and then return the outputs in the order in which\n> they complete. No more than n futures will be buffered at any point in time,\n> and less than n may also be buffered\n\n### Experiment questions\n\n  * Will the buffer_unordered method take a new unit of work from the source stream as soon as any unit of work completes, or only when the earliest unit of work is completed and sent to the next stage?\n\nUnlike buffered, buffer_unordered does not retain completed futures and\nimmediately makes them available to the next stage upon completion.\n\n### Experiment result\n\nThe buffer_unordered method does fetch a new unit of work as soon as any unit\nof work completes. Contrary to buffered, the unordered version does not need\nto retain completed future results to maintain output order. This allows it to\nprocess the stream with higher throughput.\n\n## Experiment 3: filter_map\n\n    \n    \n    stream::iter(0..10) .filter_map(async_predicate);\n\n> Filters the values produced by this stream while simultaneously mapping them\n> to a different type according to the provided asynchronous closure. As\n> values of this stream are made available, the provided function will be run\n> on them.\n\n### Experiment questions\n\n  * Does the filter method executes features in parallel or in series?\n\n### Experiment result\n\nNo surprises here. The filter operator processes each future in series.\n\nIf we want to accomplish async filtering with concurrency we can use a blend\nof map, buffered, and filter_map(future::ready). The map().buffered() duo\nwould calculate the predicate concurrently while filter_map remove failed\nitems from the stream\n\n    \n    \n    stream::iter(0..10) .map(async_predicate) .buffered(5) .filter_map(future::ready); // the ready function will return the predicate result wrapped in a ready future\n\n## Experiment 4: buffered + filter_map\n\n    \n    \n    stream::iter(0..10) .map(async_work) .buffered(3) .filter_map(async_predicate);\n\n### Experiment question\n\n  * How will a long-running filter_map step affect the concurrency of the buffered step?\n\nOk, this is unexpected! The stream does not function as I initially thought.\nWhile async_predicate is being executed, no async_work future is progressing.\nEven further, no new future starts to run until the first batch of five is\ncomplete. What's going on?\n\nLet's see what happens when we replace buffered with buffer_unordered.\n\nThe situation is pretty much identical. Again, the async_work futures are\nsuspended until async_predicate is completed.\n\nCould it be something to do with filter_map? Let's attempt to stick two\nbuffered steps sequentially:\n\nNope, the behavior remains the same.\n\n### What's going on?\n\nTurns out I'm not the first that encounters this difficulty. This is the same\nissue Barbara battled with.\n\nTo truly grasp what's happening, we need a solid understanding of Futurus,\nasync executors, and the stream API. Resources such as The async book and\nperhaps fasterthanlime's Understanding Rust futures by going way too deep can\nserve as good starting points.\n\nI'll attempt to give you some intuition.\n\nThe first clue comes from the question - when does Rust run two futures\nconcurrently? There's the join! and select! macros, and the ability to spawn\nnew async tasks. However, the Stream API neither join nor select over futures\ncreated by different pipeline steps, nor does it spawn new tasks each time it\nexecutes a future.\n\n### A Deeper Dive\n\nLet's take a closer look at our example and try to analyze the control flow.\n\n    \n    \n    let stream = stream::iter(0..10) .map(async_work) .buffered(5) .filter_map(async_predicate); pin!(stream); while let Some(next) = stream.next().await { println!(\"finished working on: {}\", next); }\n\nFirst we create the stream instance. Futures in Rust aren't executed until\nthey are awaited. Therefore, the first line of the example has no standalone\neffect. Lets look at the type definition of the stream variable:\n\n    \n    \n    FilterMap< Buffered<Map<Iter<Range<i32>>, fn async_work(i32) -> impl Future<Output = i32>>>, impl Future<Output = Option<i32>>, fn async_predicate(i32) -> impl Future<Output = Option<i32> >\n\nAfter the initial shock we find five nested structs Range within Iter within\nMap within Buffered within Filter. These types of structs are referred to as\n\"adapters\". Each adapter holds state and data and implements some trait, in\nour case, Stream. They wrap their own logic around this trait.\n\nFor example, the Buffered adapter owns a source stream and a\nin_progress_queue: FuturesOrdered to manage the buffering.\n\nElegantly skip over pin!.\n\nSo, what happens on the first stream.next().await command? The Next future\ncallsstream.poll_next_unpin(cx), where stream is an instance of FilterMap.\n\nIn turn, the FilterMap::poll_next implementation polls its inner stream - the\nBuffered stream - and executes async_predicate on the result. The\nBuffered::poll_next method polls its inner stream at most max times, until the\ninner buffer is filled.\n\nFor each such poll, the Map stream fetches an item from its source stream and\nruns the async_work method that returns a future.\n\nNote that the only place where futures are executed concurrently is the\nFuturesOrdered instance in the Buffered::poll_next implementation.\n\nWe can loosely transform the example to follow this pseudo code:\n\n    \n    \n    let range_stream = stream::iter(0..10); let in_progress_queue = FuturesOrdered::new() loop { // buffer at most 5 items to queue while in_progress_queue.len() < 5 { // get value from source stream and run the map step on it let next = range_stream.next(); // note we're not `await`ing the returned future yet let future = async_work(next); in_progress_queue.push(future) } // execute buffered futures. Get the next completed future (respecting order) // here 5 futures run concurrently let next = in_progress_queue.poll_next().await; // filter the result // futures in `in_progress_queue` are not getting polled while this runs! let predicate_result = async_predicate(next).await; // yield result accordingly }\n\nWhen deconstructing the stream pipeline into this simple representation, the\nresults of our experiment become clearer. While async_predicate is being\nexecuted, we don't poll the in_progress_queue - hence the futures are \"stuck\".\nFurthermore, when async_predicate is completed, we return to poll new futures\nfrom the in_progress_queue. However, even if we succeed, the subsequent\nin_progress_queue.poll_next().await will only run for a short while - just\nuntil the ongoing futures are completed. This leaves very little time for the\nnewly polled futures to execute. In fact, based on the visualization, they\nmight not be polled at all. Once the initial batch of futures is completed,\nthe newly polled futures get a chance to execute.\n\nAt this point some of you might be suspicious of the results. Surly if you\nmade a 100ms network request it would still take 100ms regardless of the\nhosting async executor. This is of course correct. Once a future has been\npolled, the underlying implementation will run to completion and wait\npatiently to be polled again. The effect I descried cause this final poll to\nbe delayed.\n\nTo illustrate this effect the two following versions of async_work will have\nsurprisingly different run characteristics in a stream pipeline.\n\nThe first version has a single call to tokio::time:sleep(100ms). sleep()\nreturns Sleep which implements Future directly. This means that the first poll\nof async_work will in turn call Sleep::poll which will do the needed operation\nto sleep for 100ms. However late this future will be polled again, it will\nreport it is Ready and async_work will return.\n\n    \n    \n    async fn async_work(x: i32) -> i32 { sleep(Duration::from_millis(100)).await; x }\n\nThe second version has 5 calls to sleep(20ms). In this case each consequent\n.await might suffer from the delayed polling again and again. This is the case\nfor the futures visualized in this investigation, and probably a better\nsimulacrum for real world use cases.\n\n    \n    \n    async fn async_work(x: i32) -> i32 { sleep(Duration::from_millis(20)).await; sleep(Duration::from_millis(20)).await; sleep(Duration::from_millis(20)).await; sleep(Duration::from_millis(20)).await; sleep(Duration::from_millis(20)).await; x }\n\n## Experiment summary\n\nOur experiments revealed that the Stream API pipelines can be surprisingly\nsuboptimal. Looking naively at a pipeline, we might imagine everything running\nconcurrently. However, the reality doesn't meet these expectations.\n\nShould you use the Stream API? As with many other things in our profession,\nthis depends on the trade-offs. On one hand, this API allows us to quickly\nmeet our needs with a clear and elegant API. On the other hand, the pipeline\nthroughput will not be optimal.\n\nIn my opinion, in many cases, dropping this API might be considered a\npremature optimization. Nevertheless, these findings definitely worth your\nconsideration.\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
