{"aid": "40216710", "title": "Backup Strategies for SQLite in Production", "url": "https://oldmoe.blog/2024/04/30/backup-strategies-for-sqlite-in-production/", "domain": "oldmoe.blog", "votes": 1, "user": "petercooper", "posted_at": "2024-04-30 21:42:10", "comments": 0, "source_title": "Backup strategies for SQLite in production", "source_text": "Backup strategies for SQLite in production \u2013 Oldmoe's blog\n\n# Oldmoe's blog\n\nBlog at WordPress.com.\n\n# Backup strategies for SQLite in production\n\nWritten in\n\nApr 30, 2024\n\nby\n\noldmoe\n\n## Introduction\n\nIf you are relying on SQLite as your production database, then it makes sense\nto have some sort of a backup strategy. Those backups would come handy in a\nmultitude of situations. For example, if you face a catastrophic hardware\nfailure and lose all the data on your server. Or if you face a catastrophic\nadministrative failure (e.g. you type delete from <table>; in the console and\nhit enter before typing the where condition) and lose important data all of a\nsudden. Or a similar bug in your code. Or wrongfully overriding the database\nfile in your deployment script. For these situations, and others, having a\nbackup would come in handy, as you will hopefully be able to restore the\ndatabase to a known good state and not lose much data in the process. In this\nblog, we discuss multiple options you can use for creating those backups for\nyour SQLite database.\n\n## Q: Should backup be stored locally or remotely?\n\nThere are multiple reasons you would want to have remote backups, since if\nyour hardware fails, you might lose the backup data along with the original\ndatabase file. And even if that\u2019s not the case, it might take considerable\ntime to bring that drive back online after the server has failed and that\ncould greatly disrupt your application.\n\nThat said, local backups do serve a purpose and they can make recovering from\nadmin or programming mistakes very quick and less painful. Moreover, with the\nright setup, local backups can have the resilience of remote ones. They\nactually become remote backups as we will see in a bit. This is generally my\nfavorite approach as it pushes the complexity down to the infrastructure layer\nand keeps my applications simpler to manage and reason about.\n\n## A (not so) quick infrastructure primer\n\nNote: this section deals a bit with server infrastructure and doesn\u2019t have\nmuch to do with SQLite itself, skip it to the next section if you are not\ninterested in this particular discussion.\n\nOne of the best things about computing is that we are very good at building\nabstractions. Those abstractions mean that we are able to hide complexity and\nimplementation details and deliver a consistent interface to other layers that\nrely on these abstractions. One very good example is block storage.\n\n### A distributed system in a disk drive\n\nAlmost all storage systems can be represented as sequences of data blocks that\nare called block devices. These block devices can then be formatted by\ndifferent filesystems so the OS and applications can use the POSIX interface\nto write, read, copy and delete files, directories or even portions of files\non the disk drive.\n\nGenerally, block devices physically reside on the same machine that runs the\nfilesystem. The OS would connect to the device over a PCI or NVMe connection\nor similar. But that\u2019s not the only options, block devices can represent\nstorage devices residing elsewhere on the network. And with awesome\ntechnologies like Ceph clusters, that can be an interface for networked,\nreplicated, distributed, self managed and self healing storage systems. This\nis exactly what solutions like EBS (Elastic Block Storage) from AWS and other\nsimilar solutions from other providers are using under the hood.\n\nThis means that these distributed data stores are exposed to you locally as\nblock devices. They can then be formatted by any local filesystem of your\nchoice, and they will behave like a local storage device. Except they will be\nsomewhat slower and a lot more durable (triple replicated over the network is\nthe normal level of redundancy for these).\n\nAs we further discuss backup strategies, please remember, that having a remote\nvolume mounted as a primary or a secondary disk in your system means you have\nfilesystem access to a distributed data store that you use by simply doing\nfile operations (e.g. cat, tail, cp, touch, mkdir, etc.).\n\n### Not just durability but potentially also high availability\n\nWith the backups and/or the original database files residing on replicated,\nremote block storage, if the server hosting that storage fails, it is possible\nto detach the remote storage from that server and attach it to a new, healthy\none relatively quickly, usually under a minute. It is worth noting that this\nis exactly how some cloud providers deliver high availability (HA) for their\nmanaged database services.\n\n### The cherry on top, CoW filesystems/operations\n\nAs we mentioned, the block device can be formatted using any filesystem you\ndesire. But there are certain filesystems that come with features that would\nmake taking backup potentially a lot more efficient. I won\u2019t get into much\ndetail at this point as we will touch on these later, but I would simply\nmention that features that exist in ZFS, Btrfs or XFS (not a CoW fs but has\nsome CoW features) can help a lot to optimize the backup process as we will\nsee later.\n\nAs an added bonus, both ZFS and Btrfs offer transparent filesystem\ncompression, meaning even the stored, deduplicated pages can be further\nreduced in size.\n\nEnough with the infra talk, back to SQLite.\n\n## Multiple options for things to backup\n\nWhen backing up a database like SQLite, there are multiple options for what\nexactly to backup.\n\n### Backup data pages in the file verbatim\n\nIn this mode, you copy each page of the source database files (or the changed\npages only if you are doing incremental backups) to the destination. This\neventually creates an exact replica of the original database file at a certain\npoint in time.\n\n### Pack data pages as you copy them\n\nDoing a byte by byte copy of existing database pages means you also copy along\nspace inefficiencies resulting from prior data updates/deletions or suboptimal\ninsertion order for data. By not staying true to the original page layout and\npacking the data in a more optimized format you end up with a backup that\nholds the same data as the original but hopefully uses less space. This\noperation requires more CPU power over just copying though.\n\n### Dump the data as SQL commands\n\nIn this mode you don\u2019t copy the actual data, but create the SQL commands that\nwould generate this data. Later you can create your backup copy by running\nthose statements and reconstructing your database. This mode usually results\nin a considerably larger backup file as the SQL commands\u2019 text adds\nconsiderable overhead per row of data. Though it can be very useful if you\nwant to have the option to restore your database in a different system, say\nPostgreSQL.\n\n## Multiple options for how to backup\n\nWe talked about what things you could backup, now we talk about how to perform\nthese backups.\n\nAs we discussed in the infra section, we will touch on the options of having\neither your backups or both your database and your backups residing on a\nremote replicated storage that is being used as a local filesystem.\n\nHere are some of the options you can use to backup your database files\n\n### Litestream\n\nLitestream is an interesting tool that basically does the following:\n\n  * It creates an initial copy of the database file\n  * The copy is shipped to S3 or an S3 compatible store\n  * It listens to WAL file changes\n  * Once a change is detected, new WAL pages are copied. This can happen every second\n  * The copy is shipped to S3 or the S3 compatible store\n  * You can later query the different versions on the remote object storage and restore any of them locally\n\nThis way, Litestream relies on cloning data pages as they show up in the\ndatabase WAL file.\n\n#### Pros\n\n  * Straight forward to use, there is even a gem that hides most of the complexity if you are using Ruby.\n  * Efficient storage wise, as only new pages written are shipped to remote storage\n  * Delivers point-in-time recovery and resilience to not just machine loss, but even complete datacenter loss.\n\n#### Cons\n\n  * Another piece of software to run and monitor.\n  * Can be a bit complicated to use.\n  * Potentially not compatible with the more advanced WAL2 mode.\n  * Requires subscribing to an S3 like object storage.\n  * Restoring might require downloading a large base file and multiple increments.\n\n### SQLite\u2019s backup tool\n\nSQLite has a backup API, which is also compiled into the sqlite3 CLI in the\nform of the .backup command. This creates an exact page by page replica of the\ndatabase file at the point of invoking the command. The database can be\nwritten to by other connections during the backup process, but new writes will\nnot reflect in the resulting backup.\n\nIf the target folder resides on a remote storage, then you are effectively\ncreating a remote backup. But furthermore, if the target filesystem is ZFS or\nBtrfs, then you can use deduplication to ensure that any shared data between\nbackups are stored only once on the filesystem. And if both the source and the\nbackups reside on the same remote storage, then the main database will be\nincluded in the deduplication process, meaning very little space is used for\nthe backups.\n\n#### Pros\n\n  * The backup API is rock solid (a SQLite component).\n  * With remote storage, you transparently get remote replication.\n  * With deduplication, backups are incremental even if they appear to be complete copies.\n  * Much faster to restore a backup.\n\n#### Cons\n\n  * Doesn\u2019t help with the whole datacenter going down (the paranoia is high on this one!).\n  * Setting up deduplication can be some effort, especially if you want to go fancy and use something like dm-vdo.\n  * The backups can compete for space with your original database files.\n\n### SQLite\u2019s VACUUM INTO\n\nSQLite has a VACUUM command, that is used to optimize the structure of the\ndatabase and shrink its size. There is a variant called VACUUM INTO that\nspecifies a target for this optimized version of the database, rather than\ndoing it in place. This process doesn\u2019t interfere with other writes, but it\nrequires more CPU cycles due to the optimization process\n\nIf the target for the vacuum process is on a remote drive, then you will be\ncreating this as a remote backup. And if the target uses deduplication then it\nwill not duplicate any shared pages between backups. There isn\u2019t a great\nbenefit here by having the original database on the same remote drive, as most\npages will changes during the VACUUM INTO process, negating any benefit from\ndeduplication with the original database file\n\n#### Pros\n\n  * Resulting backup is usually smaller and more efficient than the original database file.\n  * Can be stored on a remote storage, delivering replicated durability.\n  * Very fast to restore.\n\n#### Cons\n\n  * Can use a lot more space if not deduplicated (setting up deduplication is some effort).\n  * Cannot benefit from deduplication with the original database file.\n  * Competes for space with the original database file if they reside on the same drive.\n\n### SQLite CLI .dump command\n\nUsing the sqlite3 CLI .dump command creates a text file, that contains the SQL\ncommands that can be used to reconstruct the whole database. Both to create\nthe schema and to insert the data into it.\n\n#### Pros\n\n  * Resulting text files compress well.\n  * The database can be reconstructed using another RDBMS, not just SQLite.\n  * If it is written to a remote storage, you create a remote backup.\n\n#### Cons\n\n  * Uses a lot more space than other back methods.\n  * Deduplication is less effective than other methods.\n  * Restoring is the slowest of all mentioned methods.\n\n### Good old cp\n\nThis is my personal favorite as a Btrfs user, here\u2019s why:\n\ncp is the Linux/Posix copy command, instructing the filesystem to create a\ncopy of the file. When the copy is a page by page replica of the original file\nit takes up exactly as much space as the original.\n\nOne important thing to note is that you need to copy both the main database\nand the WAL file(s). In order to do so cleanly you will need to also start a\ndeferred transaction before issuing the cp command. This will allow other\nwriters to proceed, but will prevent the deletion of the WAL file during the\nbackup process.\n\n1234| db.transaction(:deferred) do`cp <src> <dist>``cp <src>-wal\n<dist>-wal'end  \n---|---  \n  \nAs with the other approaches, writing that to a remote storage ensures\nreplicated durability. But the goodness doesn\u2019t stop here. If both the main\ndatabase and the backup reside on the same remote storage, then you can do\nshallow copies. On Btrfs or XFS (and recently on ZFS) you can issue the cp\n--reflink=always variant of cp, which instructs the file system to create a\ncopy-on-write version of the file. Meaning, no blocks will be copied unless,\nthey get overwritten or deleted by the original database file. No\ndeduplication setup is needed in this case as only the changed pages are ever\nwritten to the filesystem.\n\n1234| db.transaction(:deferred) do`cp --reflink=always <src> <dist>``cp\n--reflink=always <src>-wal <dist>-wal'end  \n---|---  \n  \nOn a VM with attached block storage hosting both the source and destination\ndatabases this operation takes ~2ms to complete for a 440MB database file. The\nsame ~2ms are required to copy a 4.4GB database file as well.\n\nBefore creating the copy of the 4.7GB file, this was the result of running df\n-h.\n\n123| #df -hFilesystem Size Used Avail Use% Mounted\non/dev/mapper/vg01-lv_data_remote 99G 4.9G 93G 5% /mnt/remote  \n---|---  \n  \nAfter the copy, the results were as follows:\n\n123| #df -hFilesystem Size Used Avail Use% Mounted\non/dev/mapper/vg01-lv_data_remote 99G 4.9G 93G 5% /mnt/remote  \n---|---  \n  \nNo extra space was used (actually a tiny bit of space for the file metadata).\n\nWith this method I can aggressively create multiple copies per second of my\ndatabase file (if I detect data changes) that I can delete later on, if they\nend up not being needed, minimizing the potential of losing data due to a\nmistake or a bug to a few milliseconds.\n\nIt is important to note though, that if you copy this backup file to another\nremote or local drive then you will be copying the whole 4.4GBs.\n\n#### Pros\n\n  * cp is tried and true.\n  * With CoW, the copy process is extremely fast, since no pages are ever checked for deduplication.\n  * With CoW + remote block storage you get very fast remote backups.\n  * Restore is very fast too.\n\n### Cons\n\n  * Needs the database and the backup to reside on the same remote drive for the best results.\n  * Requires the use of a specific type of filesystem.\n  * Backups compete with the original database file for drive space (but they already consume too little).\n\n## Summary\n\nThere are multiple solid solutions for backing up your SQLite database in\nproduction. We have reviewed a couple of them and here\u2019s a summary\n\nMethod| Durability| Space efficiency| Restore Performance| Complexity  \n---|---|---|---|---  \nLitestream| Very High| High (only changes are shipped) but needs to send base\nfile once in a while| Slowish, requires downloading the base file and WAL\nsnapshot from S3| Easy to setup, requires setting up S3 and ensuring it is\nstill up  \n.backup| High if remote target| Very High if target supports deduplication|\nVery fast if on the same storage (local or remote)| Very easy to use, needs\nremote storage for durability. Deduplication setup can be complex  \nVACUUM INTO| High if remote target| High if target supports deduplication|\nVery fast if on the same storage (local or remote)| Very easy to use, needs\nremote storage for durability. Deduplication setup can be complex  \n.dump| High if remote target| Consumes space| Slowest, need to download the\nSQL dump and reconstruct the database| Very easy to use, needs remote storage\nfor durability.  \ncp (--reflink=always)| High if remote target| Very high if source and target\non the same filesystem with shallow copy support| Very fast if on the same\nstorage (local or remote)| Very easy to use, needs remote storage for\ndurability. Needs using XFS or Btrfs for space efficiency  \nSummary of SQLite backup options\n\n## Conclusion\n\nThese are some of the most common ways to manage backups for SQLite in\nproduction. Personally, if I have full control on my machine and I have access\nto remote block storage, I always opt for Btrfs with cp --reflink=always, but\nif such a facility is not an option and you have access to object storage\ninstead, then Litestream would be a very sensible solution.\n\nGo ahead and use SQLite in production, with the right backup strategy, you\nwill ensure your data safety with a lot less complexity overall compared to\nother RDMBS solutions.\n\n### Share this:\n\n  * Facebook\n  * X\n  * Reddit\n\nLike Loading...\n\nTags\n\nbackup, database, sql, sqlite, sqlite_in_production\n\nCategories\n\nUncategorized\n\n### Leave a comment Cancel reply\n\n# Oldmoe's blog\n\n  * GitHub\n  * Twitter\n\n  * Comment\n  * Reblog\n  * Subscribe Subscribed\n\n    * Oldmoe's blog\n    * Already have a WordPress.com account? Log in now.\n\n  * Privacy\n  *     * Oldmoe's blog\n    * Edit Site\n    * Subscribe Subscribed\n    * Sign up\n    * Log in\n    * Copy shortlink\n    * Report this content\n    * View post in Reader\n    * Manage subscriptions\n    * Collapse this bar\n\n%d\n\n", "frontpage": false}
