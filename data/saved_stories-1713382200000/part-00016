{"aid": "40063843", "title": "Roast My Docs", "url": "https://langfuse.com/docs", "domain": "langfuse.com", "votes": 7, "user": "clemo_ra", "posted_at": "2024-04-17 12:44:46", "comments": 0, "source_title": "Overview - Langfuse", "source_text": "Overview - Langfuse\n\nCTRL K\n\n  * Docs\n\nGuides\n\n  * Overview\n  * Interactive Demo\n  *     * Local (docker compose)\n    * Self-host (docker)\n\n  * Tracing\n  * Introduction\n  * Quickstart\n  *     * Sessions\n    * Users\n    * Metadata\n    * Tags\n    * Trace URL\n\n  *     * Overview\n    *       *         * Get Started\n        * Track Errors\n        * Example Notebook\n\n      *         * Get Started\n\n    *       * Tracing\n      * Example Python\n      * Example JS\n      * Upgrade Paths\n\n    *       * Get Started\n      * Example (Python)\n\n    * LiteLLM (Proxy)\n    * Flowise\n    * Instructor\n    * Langflow\n    * Superagent\n    * AI SDK by Vercel \u2197\n\n  *     * Overview\n    *       * Decorators\n      * Example Notebook\n      * Low-level SDK\n      * Reference \u2197 (opens in a new tab)\n\n    *       * Guide\n      * Guide (Web)\n      * Example (Vercel AI)\n      * Reference \u2197 (opens in a new tab)\n\n  * Develop\n  *     * Get Started\n    * Example OpenAI Functions\n    * Example Langchain (Py)\n    * Example Langchain (JS)\n\n  * Export & Fine-tuning\n  * Monitor\n  *     * Overview\n    * PostHog Integration\n\n  * Model Usage & Cost\n  *     * Overview\n    * Manually in Langfuse UI\n    * User Feedback\n    * Model-based Evaluation\n    * Custom via SDKs/API\n\n  * Test\n  * Experimentation\n  *     * Overview\n    * Cookbook\n\n  * References\n  * API \u2197 (opens in a new tab)\n  * Python SDK \u2197 (opens in a new tab)\n  * JS SDK \u2197 (opens in a new tab)\n  * More\n  * Project Sharing (RBAC)\n  * Data Security & Privacy\n  * Open Source\n  * Roadmap\n  * Support \u2197 (opens in a new tab)\n\nDocs\n\nOverview\n\n# Overview\n\n> Langfuse is an open-source LLM engineering platform that helps teams\n> collaboratively debug, analyze, and iterate on their LLM applications.\n\n## Core platform features\n\n### Develop\n\n  * Observability: Instrument your app and start ingesting traces to Langfuse (Quickstart, Tracing)\n\n    * Track all LLM calls and all other relevant logics in your app\n    * Async SDKs for Python and JS/TS\n    * @observe() decorator for Python\n    * Integrations for OpenAI SDK, Langchain, LlamaIndex, LiteLLM, Flowise and Langflow\n    * API (opens in a new tab)\n  * Langfuse UI: Inspect and debug complex logs and user sessions (Demo, Tracing, Sessions)\n  * Prompt Management: Manage, version and deploy prompts from within Langfuse (Prompt Management)\n\n### Monitor\n\n  * Analytics: Track metrics (LLM cost, latency, quality) and gain insights from dashboards & data exports (Analytics)\n  * Evals: Collect and calculate scores for your LLM completions (Scores & Evaluations)\n\n    * Run model-based evaluations\n    * Collect user feedback\n    * Manually score observations in Langfuse\n\n### Test\n\n  * Experiments: Track and test app behaviour before deploying a new version\n\n    * Datasets let you test expected in and output pairs and benchmark performance before deployiong\n    * Track versions and releases in your application (Experimentation, Prompt Management)\n\n## Get started\n\nHow tracing works\u2192\n\nQuickstart\u2192\n\nInteractive demo\u2192\n\n## Why Langfuse?\n\nWe wrote a concise manifesto on this: Why Langfuse?\n\n  * Open-source\n  * Model and framework agnostic\n  * Built for production\n  * Incrementally adoptable - start with a single LLM call or integration, then expand to full tracing of complex chains/agents\n  * Use the GET API to build downstream use cases\n\n###\n\nChallenges of building LLM applications and how Langfuse helps\n\nIn implementing popular LLM use cases \u2013 such as retrieval augmented\ngeneration, agents using internal tools & APIs, or background\nextraction/classification jobs \u2013 developers face a unique set of challenges\nthat is different from traditional software engineering:\n\nTracing & Control Flow: Many valuable LLM apps rely on complex, repeated,\nchained or agentic calls to a foundation model. This makes debugging these\napplications hard as it is difficult to pinpoint the root cause of an issue in\nan extended control flow.\n\nWith Langfuse, it is simple to capture the full context of an LLM application.\nOur client SDKs and integrations are model and framework agnostic and able to\ncapture the full context of an execution. Users commonly track LLM inference,\nembedding retrieval, API usage and any other interaction with internal systems\nthat helps pinpoint problems. Users of frameworks such as Langchain benefit\nfrom automated instrumentation, otherwise the SDKs offer an ergonomic way to\ndefine the steps to be tracked by Langfuse.\n\nOutput quality: In traditional software engineering, developers are used to\ntesting for the absence of exceptions and compliance with test cases. LLM-\nbased applications are non-deterministic and there rarely is a hard-and-fast\nstandard to assess quality. Understanding the quality of an application,\nespecially at scale, and what \u2018good\u2019 evaluation looks like is a main\nchallenge. This problem is accelerated by changes to hosted models that are\noutside of the user\u2019s control.\n\nWith Langfuse, users can attach scores to production traces (or even sub-steps\nof them) to move closer to measuring quality. Depending on the use case, these\ncan be based on model-based evaluations, user feedback, manual labeling or\nother e.g. implicit data signals. These metrics can then be used to monitor\nquality over time, by specific users, and versions/releases of the application\nwhen wanting to understand the impact of changes deployed to production.\n\nMixed intent: Many LLM apps do not tightly constrain user input.\nConversational and agentic applications often contend with wildly varying\ninputs and user intent. This poses a challenge: teams build and test their app\nwith their own mental model but real world users often have different goals\nand lead to many surprising and unexpected results.\n\nWith Langfuse, users can classify inputs as part of their application and\ningest this additional context to later analyze their users behavior in-depth.\n\nLangfuse features along the development lifecycle\n\n## Updates\n\nLangfuse evolves quickly, check out the changelog for the latest updates.\n\nSubscribe to the mailing list to get notified about new major features:\n\n## Get in touch\n\nWe actively develop Langfuse in open source. Join our Discord, provide\nfeedback, report bugs, or request features via GitHub issues.\n\nLearn more about ways to get in touch on our Support page.\n\nLast updated on April 16, 2024\n\nInteractive Demo\n\n### Was this page useful?\n\n### Questions? We're here to help\n\nGitHub Q&AEmailTalk to sales\n\n### Subscribe to updates\n\nPlatform\n\n  * LLM Tracing\n  * Prompt Management\n  * Evaluation\n  * Datasets\n  * Metrics\n\nIntegrations\n\n  * Python SDK\n  * JS/TS SDK\n  * OpenAI SDK\n  * Langchain\n  * Llama-Index\n  * Litellm\n  * Flowise\n  * Langflow\n  * API\n\nResources\n\n  * Documentation\n  * Interactive Demo\n  * Changelog\n  * Roadmap\n  * Pricing\n  * Status\n  * Self-hosting\n  * Open Source\n\nAbout\n\n  * Blog\n  * Careers\n  * About us\n  * Support\n  * Why Langfuse?\n  * Schedule Demo\n  * OSS Friends\n\nLegal\n\n  * Security\n  * Imprint\n  * Terms\n  * Privacy\n\n\u00a9 2022-2024 Finto Technologies\n\n", "frontpage": false}
