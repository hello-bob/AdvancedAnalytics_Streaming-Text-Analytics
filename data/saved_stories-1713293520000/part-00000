{"aid": "40051263", "title": "Operationalizing Vector Databases on Postgres", "url": "https://tembo.io/blog/operationalizing-vectordbs-on-postgres", "domain": "tembo.io", "votes": 1, "user": "chuckhend", "posted_at": "2024-04-16 12:49:47", "comments": 0, "source_title": "Operationalizing Vector Databases on Postgres | Tembo", "source_text": "Operationalizing Vector Databases on Postgres | Tembo\n\n# Operationalizing Vector Databases on Postgres\n\nApr 15, 2024 \u2022 7 min read\n\n# Adam Hendel\n\nFounding Engineer\n\nWhy do we need vector databases? The proliferation of embeddings immediately\nbrought forth the need to efficiently store, index, and search these arrays of\nfloats. However, these steps are just a small piece of the overall technology\nstack required to make use of embeddings. The task of transforming source data\nto embeddings and the serving of the transformer models that make this happen\nis often left as a task to the application developer. If that developer is\npart of a large organization, they might have a machine learning or data\nengineering team to help them. But in any case, the generation of embeddings\nis not a one-time task, but a lifecycle that needs to be maintained.\nEmbeddings need to be transformed on every search request, and inevitably the\nnew source data is generated or updated, requiring a re-compute of embeddings.\n\n## Consistency between model training and inference\n\nTraditionally, machine learning projects have two distinct phases: training\nand inference. In training, a model is generated from a historical dataset.\nThe data that go into the model training are called features, and typically\nundergo transformations.\n\nAt inference, the model is used to make predictions on new data. The data\nincoming into the model for inference requires precisely the same\ntransformations that were conducted at training. For example in classical ML,\nimagine you have a text classification model trained on TF-IDF vectors. At\ninference, any new text must undergo the same preprocessing (tokenization,\nstop word removal) and then be transformed into a TF-IDF vector using the same\nvocabulary as during training. If there\u2019s a discrepancy in this\ntransformation, the model\u2019s output will be unreliable.\n\nSimilarly, in a vector database used for embedding search, if you\u2019re dealing\nwith text embeddings, a new text query must be converted into an embedding\nusing the same model and preprocessing steps that were used to create the\nembeddings in the database. Embeddings stored in the database using OpenAI\u2019s\ntext-embedding-ada-002 model must also be searched using the same text-\nembedding-ada-002 model in order to produce a comparable embedding vector for\nsearch.\n\nIt is not enough to just store embeddings in a database. To operationalize the\nvector database, you must also have a process to transform new data into\nembeddings and to serve the embeddings for search.\n\n### Generating and searching embeddings\n\npg_vectorize solves this problem by keeping track of the transformer model was\nused to generate embeddings.\n\nTo follow along with code, you can start a VectorDB instance for free on Tembo\nCloud or run the docker-compose example locally. Refer to our detailed guide\nfor a more in-depth walkthrough of pg_vectorize and the VectorDB Stack\n\nIf you want to generate embeddings using an open source the Apache-2.0\nlicensed all-MiniLM-L6-v2 sentence transformer, you can initialize your\nproject like so:\n\n    \n    \n    SELECT vectorize.table( job_name => 'my_product_search_project', \"table\" => 'products', primary_key => 'product_id', columns => ARRAY['product_name', 'description'], transformer => 'sentence-transformers/all-MiniLM-L6-v2', schedule => 'realtime' );\n\nThis will create a new table in the vectorize schema and immediately begin to\nfill it with the embeddings generated by the all-MiniLM-L6-v2 model.\n\nTo search the table using the embeddings, you can use the vectorize.search\nfunction:\n\n    \n    \n    SELECT * FROM vectorize.search( job_name => 'my_product_search_project', query => 'accessories for mobile devices', return_columns => ARRAY['product_id', 'product_name'], num_results => 3 );\n    \n    \n    search_results --------------------------------------------------------------------------------------------- {\"product_id\": 13, \"product_name\": \"Phone Charger\", \"similarity_score\": 0.8147814132322894} {\"product_id\": 6, \"product_name\": \"Backpack\", \"similarity_score\": 0.7743061352550308} {\"product_id\": 11, \"product_name\": \"Stylus Pen\", \"similarity_score\": 0.7709902653575383}\n\npg_vectorize keeps track of which transformer is used for each job_name, so\nwhen we want to search my_product_search_project, it will use the same\ntransformer to generate embeddings for the query.\n\nIf you want to view all your source data and the embeddings side-by-side,\npg_vectorize creates a view for you.\n\n    \n    \n    SELECT * FROM vectorize.my_product_search_project\n\nThe view contains everything from your original table, plus the embeddings and\nthe last time they were updated.\n\n    \n    \n    \\d vectorize.my_product_search_project; View \"vectorize.my_product_search_project\" Column | Type | Collation | Nullable | Default -----------------------+--------------------------+-----------+----------+--------- product_id | integer | | | product_name | text | | | description | text | | | last_updated_at | timestamp with time zone | | | embeddings | vector(384) | | | embeddings_updated_at | timestamp with time zone | | |\n\nAs an application developer, you can move the vectorize.table call into your\nSQL migration scripts, then call vectorize.search in your application code to\nsearch the embeddings.\n\n## Day 2 operations for embeddings\n\nInevitably, the source data will change. New data will be added, old data will\nbe updated, and the embeddings will need to be re-computed. The process of\ntransforming source data into embeddings is not a one-time task, but a\ncontinuous cycle that needs to be managed.\n\npg_vectorize provides two ways of managing transformations; an interval-based\ncron-like schedule via pg_cron, or a real-time trigger-based approach. In both\nmethods, pg_vectorize enqueues jobs into a queue using pgmq and a background\nworker handles the transformations. In the case of OpenAI embeddings, that\nmeans HTTP requests to OpenAI\u2019s public API. For Hugging Face Sentence\nTransformers, HTTP requests are made to a container that hosts the sentence-\ntransformer models. On Tembo Cloud, this container runs in a pod in the same\nnamespace as your Postgres instance. If you\u2019re running the open-source\npg_vectorize project on your own, the docker-compose file will start this\ncontainer next to Postgres for you. In both cases, pg_vectorize is pre-\nconfigured to reach these endpoints.\n\nBelow is the process of updating embeddings with a trigger-based flow. The\ncron based update flow is identical, except that a pg_cron job checks for\nupdates rather than a trigger.\n\nMany thanks to Gunnar Morling who recently demonstrated this trigger based\nupdate flow on X.\n\n## Flexibility and rapid iteration\n\nThe ability to quickly change the transformer model used to generate\nembeddings is highly valuable. New transformer models are constantly being\ndeveloped embeddings need to be consistent between initial project set up and\nsearch time. As of this writing, OpenAI has three embedding models, and\nHugging Face supplies hundreds of open source text embedding models.\n\n### Text Embedding Model Sources\n\npg_vectorize supports all of OpenAI\u2019s transformer models and all Sentence\nTransformer models available on Hugging Face \u2014 including any private models\nyou may have trained and updated to the model hub.\n\nTo take our example above and instead use one of OpenAI\u2019s embedding models, we\nsimply change the transformer parameter. Note, OpenAI also requires an API\nkey, so we will set that first:\n\n    \n    \n    ALTER SYSTEM SET vectorize.openai_key TO '<your api key>'; SELECT pg_reload_conf();\n    \n    \n    SELECT vectorize.table( job_name => 'product_search_openai', \"table\" => 'products', primary_key => 'product_id', columns => ARRAY['product_name', 'description'], transformer => 'openai/text-embedding-ada-002', schedule => 'realtime' );\n\nNow we can search our data using the same vectorize.search function call as\nbefore, but specifying the new job_name.\n\n    \n    \n    SELECT * FROM vectorize.search( job_name => 'product_search_openai', query => 'accessories for mobile devices', return_columns => ARRAY['product_id', 'product_name'], num_results => 3 );\n\nMeanwhile, our original embeddings are still available.\n\n    \n    \n    SELECT * FROM vectorize.search( job_name => 'my_product_search_project', query => 'accessories for mobile devices', return_columns => ARRAY['product_id', 'product_name'], num_results => 3 );\n\n### Transforming text to embeddings directly\n\nWe can also transform text directly by calling vectorize.transform_embeddings.\n\n    \n    \n    select vectorize.transform_embeddings( input => 'the quick brown fox jumped over the lazy dogs', model_name => 'sentence-transformers/all-MiniLM-L6-v2' );\n    \n    \n    {0.032561734318733215,0.09281901270151138, ... ( omitted for formatting), 0.07227665930986404}\n\nUsing transform_embeddings directly is useful if you want to do any complex\njoins or further transformations on your query. For example, we can manually\nre-create the underlying SQL that is executed when we call vectorize.search:\n\n    \n    \n    SELECT product_name, description, 1 - ( product_search_all_MiniLM_L6_v2 <=> vectorize.transform_embeddings('mobile electronic devices', 'sentence-transformers/all-MiniLM-L12-v2')::vector ) as similarity FROM products ORDER by similarity DESC LIMIT 3;\n\n## What\u2019s next?\n\nWe are continuing to work on improving pg_vectorize and the VectorDB Stack at\nTembo. Additional embedding model sources (such as Ollama) and a more complete\nRAG API are in the works. The pg_vectorize extension is open source and\navailable on GitHub. If you find any issues or just want to talk to us, open\nan issue, pull request, or join our community Slack.\n\nJoin us at the Silicon Valley Postgres Meetup on April 30th, 2024 if you\u2019re\ninterested in learning more on this topic.\n\n### What's next?\n\nTry Tembo Cloud for free\n\nStar on Github\n\nSubscribe on Youtube\n\nView our RSS feed\n\nFollow on X\n\nNext post\n\n#### Introducing Tembo\n\nJan 18, 2023\n\npostgres vectordb pg_vectorize\n\n## On this page\n\nConsistency between model training and inferenceGenerating and searching\nembeddingsDay 2 operations for embeddingsFlexibility and rapid iterationText\nEmbedding Model SourcesTransforming text to embeddings directlyWhat\u2019s next?\n\n## Share this article\n\n###### Company\n\nDocs Blog Pricing Product Cloud Trunk Roadmap Changelog\n\n###### Connect\n\nGithub Twitter LinkedIn Youtube Tembo Slack Trunk Slack\n\n###### Resources\n\nCareers\n\nPrivacy policy Terms of service\n\nSOC2 Type 1 compliant\n\n", "frontpage": false}
