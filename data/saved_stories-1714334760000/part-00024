{"aid": "40187975", "title": "So what's the point of linear algebra, anyway?", "url": "https://chittur.dev/math/2024/04/21/linear-algebra.html", "domain": "chittur.dev", "votes": 2, "user": "3PS", "posted_at": "2024-04-28 12:01:07", "comments": 2, "source_title": "So what\u2019s the point of linear algebra, anyway?", "source_text": "So what\u2019s the point of linear algebra, anyway? | Krishna\u2019s Blog\n\nLoading [MathJax]/jax/output/HTML-CSS/jax.js\n\n# Krishna's Blog\n\n## The perpetual work-in-progress.\n\n2024 April 21\n\n# So what's the point of linear algebra, anyway?\n\n### Or: let's try to invent linear algebra, but backwards\n\nAsking someone in a STEM field to justify linear algebra concepts is a bit\nlike asking a Haskell programmer about monads. The best case scenario is that\nyou\u2019ll get a shrug and a \u201cdon\u2019t worry about it, just learn when to use it\u201d.\nThe more common scenario is an hour-long explanation that leaves you more\nconfused than when you started. In keeping with my lifelong passion for\nconfusing and upsetting as many people as possible, today I would like to talk\nabout linear algebra. (I\u2019ll save the monads for a later date. Don\u2019t worry,\nthey\u2019re just burritos.)\n\nTarget audience for this post: someone with at least a little exposure to\nlinear algebra. If you\u2019re starting from complete scratch and want a good high-\nlevel picture, I highly recommend 3Blue1Brown\u2019s Essence of Linear Algebra\nseries on YouTube. Though of course, even that is no substitute for an actual\ntextbook.\n\n## Why all the vague definitions?\n\nOne of the reasons that linear algebra can be frustrating to the average high\nschooler or undergrad is that it\u2019s often the first time in your life that you\nget exposed to the Mathematician Way of Doing ThingsTM. That is, rather than\nworking with relatively concrete definitions, like \u201ca vector is a list of real\nnumbers\u201d (a.k.a. the physicist\u2019s vector), you get far less helpful and less\nintuitive definitions, like \u201ca vector is an element of a vector space\u201d, and \u201ca\nvector space is a set that satisfies all of these different properties that\nyou definitely won\u2019t remember and will definitely just end up visualizing as\nRn anyway\u201d.\n\nThere are basically two main benefits to doing things this way:\n\n  1. Minimal assumptions. Structures like Rn have a LOT going on - they have a notion of distance, they have a notion of an inner product, you can even do calculus in them! Why make so many assumptions if you don\u2019t need them to prove the theorem at hand?\n  2. Broad applicability. This goes hand in hand with the above. By assuming as little as possible, you can find potentially unexpected applications for existing ideas with minimal extra effort.\n\nIn short: the math way of doing things is a programmer\u2019s dream come true.^1\nMaximum code reuse! But a little more confusing than necessary for someone who\njust wants to think of vectors as arrows and spaces as grids.\n\n## Okay but seriously though what\u2019s up with all of these arrays of numbers and\ntedious operations?\n\nNow for the contentious part. I\u2019m no pedagogy expert, but my ideal linear\nalgebra course would change up the order of things a bit from the usual\napproach. In particular, the typical course goes something like this (e.g. in\nGilbert Strang\u2019s book):\n\n  1. Vectors, matrices\n  2. Lots and lots of number crunching: matrix-vector products, matrix multiplication, Gauss-Jordan elimination, determinants, etc.\n  3. Even more number crunching, projections, various kinds of decompositions\n  4. Nullspaces, bases, dimensions, kernels, etc.\n  5. Eigenvalues and eigenvectors\n  6. OH LOOK WE CAN FINALLY TALK ABOUT LINEAR TRANSFORMATIONS\n\nBluntly, while this ordering may make sense for someone whose idea of linear\nalgebra starts and ends with BLAS and numpy.linalg, I think this approach\ntends to miss the forest for the trees. Don\u2019t get me wrong: drilling the\nfundamentals is absolutely essential, and having concrete numbers to work with\ncan make new math more approachable. But linear algebra is in the unfortunate\nsituation of being a particularly useful subject that is also particularly\nobtuse from a purely numerical standpoint. And so in my ideal course, we would\nstart from the other end and teach in the following order:\n\n  1. Linearity and linear transformations\n  2. Invent the concepts of a basis, vector, and matrix\n  3. Invent matrix multiplication, eigenvalues and eigenvectors\n  4. Invent the rest of linear algebra now that we have what we need to do so\n\nI\u2019m not sure if this would actually hold up in a real classroom, but\nregardless, I\u2019d like to sketch out how this approach would work with the rest\nof this post.^2 So let\u2019s start with:\n\n## Linearity\n\nLet\u2019s finally try to answer the question in the title of the post. What is the\npoint of linear algebra? My answer is simple:\n\nLinear algebra is the study of linear transformations.\n\nOh boy, if that isn\u2019t circular. But I mean this sincerely. And to illustrate\nwhat I mean, let\u2019s do something really mathematically inappropriate: I\u2019ll show\nyou a formula, and I won\u2019t define any of the symbols in it. (Except \u2200, which\nmeans \u201cfor all\u201d.)\n\n\u2200a,b,\u2192x,\u2192y:T(a\u2192x+b\u2192y)=aT(\u2192x)+bT(\u2192y)\n\nTo me, that equation, and its many generalizations, are the absolute core of\nlinear algebra. It doesn\u2019t matter what T and \u2192x and a are. It doesn\u2019t even\nmatter what addition and multiplication mean (within reason). You can shuffle\nthe definitions around however you like, as mathematicians are wont to do.\nWhat matters is the structure of the equation itself. Because when you view T\nas a sort of function, and you view the above equation as a constraint on\n\u201clinear\u201d functions, you are basically saying the following:\n\nA linear transformation is a function that you can understand just by\nunderstanding how it acts on smaller, simpler inputs.\n\nIn other words, to understand how T acts on the complicated input a\u2192x+b\u2192y, we\njust need to know how it acts on \u2192x and \u2192y. I argue that this one concept is\nso immensely powerful that it motivates all of the other concepts in linear\nalgebra: from matrices to eigenvectors to everything else. The notion of a\ntransformation that can be studied just by breaking down how it acts on\nspecific inputs encompasses concepts as disparate as 3D rotations, the growth\nof the Fibonacci sequence, differentiation and integration, and the evolution\nof quantum states over time. For whatever reason, the universe is just full of\ntransformations that can be broken down like this. And so correspondingly, we\nwant our conception of these transformations to be broad enough to adequately\ncapture this beautiful phenomenon in its myriad forms.^3\n\n## Bases\n\nTo recap, we have now the vague half-concept of a \u201clinear transformation\u201d as\nsome kind of operation that can be understood by examining how it acts on\ncomponents of its input. But for a concept like this to be useful, we need to\nactually be able to break said input down into components in the first place.\nHence the notion of a basis.\n\nA basis is a collection of building blocks that you can use to express the\ninputs to a linear transformation.\n\nHow delightfully backwards! How pleasantly vague! Let\u2019s try to strip away some\nof this vagueness and pin down something more useful and algebraic. \u201cInput to\na linear transformation\u201d is a mouthful, so let\u2019s abbreviate that to the\ncurrently-meaningless term \u201cvector\u201d. Let\u2019s assume our basis, our building\nblocks, consists of some set of mathematical objects b1,...,bn. Let\u2019s assume\nthat \u2192x is some complicated object (\u201cvector\u201d) and we want to understand how\nour lovely linear transformation T behaves when you feed it \u2192x.\n\nThis is quite abstract, so as a motivating example, let's examine derivatives,\nparticularly derivatives of real-valued polynomials of degree 2 and lower. In\nan elementary calculus class, one learns that you can compute the derivative\nof a function by breaking it up into pieces and then adding up the derivative\nof each piece. So intuitively, differentiation seems like the sort of\noperation that we want our concept of \"linearity\" to apply to. In this case,\nour transformation T is the map D which takes as input a function f:R\u2192R of the\nform f(z)=a+bz+cz2 and spits out the function D(f)=ddzf which is the\nderivative of f with respect to z. (I'm using z here instead of x to avoid\nconfusing this with our general notation of \u2192x for abstract vectors.) In this\ncase, our linear transformation T is the derivative operator D, and our vector\n\u2192x is the function f. We want to gain a better understand of T(\u2192x) (i.e. D(f))\nby examining how T (i.e. D) acts on the building blocks that we used to build\n\u2192x (i.e. f).\n\nTo compute T(\u2192x), then, we need to understand two things:\n\n  1. How can we build our vector \u2192x out of our presumably useful building blocks b1,...,bn?\n  2. How can we express how T behaves on each of these building blocks?\n\nOur one question has grown to two, but these seem more tractable. In\nparticular, we can solve the former by inventing the coordinate vector, and\nthe latter by inventing the dreaded matrix.\n\n## Coordinate vectors\n\nWe\u2019ve got our building blocks B={b1,...,bn}, and we\u2019ve got our complicated\nobject \u2192x, so let\u2019s take a first crack at building our \u2192x out of said building\nblocks.\n\nAttempt 1: we can try to express \u2192x by listing off which building blocks it\ncontains and which ones it doesn't. Like, \u2192x={b1,b3,b5}.\n\nNot a crazy idea. But we immediately run into a wall: if we have only a finite\nnumber of basis elements, then we can only express a finite number of vectors.\nIn particular, if we\u2019re only adding basis elements, then we can express at\nmost 2n vectors with this scheme. Even if we invent a symbol for subtraction,\nlike \u2192x=b1\u2212b2, that still only gives us 3n possibilities - way too finite. How\nwill we ever express all polynomials or all of the possible rotations of a 3D\nshape with just a finite number of vectors? No, this won\u2019t do.\n\nLet\u2019s try a different approach. In addition to \u201cadding\u201d and \u201csubtracting\u201d\nbasis elements (whatever that means), we\u2019ll also allow for \u201cscaling\u201d. That is,\nwe\u2019ll throw some coefficients into the mix, and assume they behave reasonably\nlike numbers.^4 Since we use these things for \u201cscaling\u201d, we can call them\n\u201cscalers\u201d, sorry, scalars.\n\nThis gives us the axiom we want:\n\nAttempt 2: Any vector \u2192x can be expressed as a unique sum of scaled basis\nelements for some basis B. For a basis B={b1,...,bn} and corresponding scaling\nfactors x1,...,xn, we denote this as \u2192x=\u2211ni=1xibi.\n\n(Yes, I did sneak the extra \u201cunique\u201d in there; without it, we can have\nmultiple ways to represent something with our basis, which is kind of\nannoying, as we\u2019ll see in a moment.)\n\nRevisiting our example of real-valued polynomials of degree 2 and lower, we\ncan define the notion of \"addition\" of two functions (vectors) to be pointwise\naddition, i.e. if f,g:R\u2192R then we can define f+g:R\u2192R as the map that sends z\nto f(z)+g(z). We can then define scalar multiplication a similar way, where we\nrequire that any scalar s is a real number and define sf to be the function\nthat sends z to s\u00d7f(z). Feel free to check that our notion of linearity as\ndefined above applies to the derivative operator when we use these definitions\nof addition and scaling. That is, D(af+bg)=aD(f)+bD(g).\n\nWe can then define our basis Q (for \"quadratic basis\") as the set of functions\n{q0,q1,q2} where q0:z\u21a61, q1:z\u21a6z, and q2:z\u21a6z2. Our function f:z\u21a6az2+bz+c can\nthen be rewritten using our new function-level addition and scaling operations\nas f=aq0+bq1+cq2, which satisfies our axiom above. Proving that this\nrepresentation is unique is left as an exercise, but should hopefully be self-\nevident.\n\nNow that we have a nice notion of breaking a vector down into a basis, we\nnotice something else. Since our basis is so great and reusable, there\u2019s\nprobably no need to keep writing it down. In fact, if we have lots of vectors,\nwe can distinguish them from each other solely by breaking them down into this\nbasis and examining the scaling factors. (This is why uniqueness matters - you\ndon\u2019t want two sets of scaling factors to give you the same vector!) So then\nwe can simply hide away the bi and identify \u2192x with its coordinates x1,...,xn.\nThe basis is still there, invisible, like the air you breathe. But there\u2019s no\nneed to talk about it unless you ever want to switch to a different basis.\n\nHenceforth, then, we\u2019ll refer to these scaling factors xi as the \u201ccoordinates\u201d\nof \u2192x with respect to our basis b1,...,bn, and we\u2019ll use the term \u201ccoordinate\nvector\u201d to refer to the list of these coordinates [x1...xn]. We can go from \u2192x\nto its coordinates by breaking it down along the basis elements, and we can go\nfrom the coordinate vector back to \u2192x just by using the handy sum\n\u2192x=\u2211ni=1xibi. So far so good! We\u2019ll use the following notation for coordinate\nvectors^5:\n\n[x]B:=[x1...xn]\n\nIn the case of our example, this becomes [f]Q=[abc], with the basis elements\nordered q0,q1,q2. Verify this for yourself!\n\n## Refining our notion of a vector\n\nEarlier, we defined \u201cvector\u201d as \u201cthe input to a linear transformation\u201d. But\nnow that we have this clean concept of a basis, we can refine our informal\ndefinition a bit. Namely:\n\nA vector is anything that can be expressed as a scaled summation of basis\nelements for any given basis B, i.e. in the form \u2192x=\u2211ni=1xibi. Words like\n\"scale\" and \"sum\" can be redefined as needed, within reason. In other words, a\nvector is anything that can be broken down into understandable pieces. We\nhenceforth refer to scaled summations of this form as \"linear combinations\".\n\nNote that this means that basis elements bi\u2208B are also vectors over B, because\nany basis element bi can be expressed as bi=0\u00d7b1+...+1\u00d7bi+...+0\u00d7bn. So we can\nnow just call them basis vectors.\n\nIt\u2019s worth noting that, modulo rigor, this definition of a vector is pretty\nmuch always equivalent to the traditional \u201ca vector is an element of a vector\nspace\u201d definition, since every vector space has a basis.^6 Speaking of which,\nlet\u2019s go ahead and define a vector space as well:\n\nA vector space is the set of all vectors for some basis. In other words, it's\na collection of objects than can be broken down into the same underlying\ncomponents.\n\nNote the careful phrasing; this basis isn\u2019t necessarily unique for said vector\nspace, but unlike the traditional definition, you do need to pick a basis to\nstart with when you define a vector space.\n\nIn the case of our example, the vector space spawned from the basis Q is the\nset of all real-valued polynomials of degree 2 and lower. Take a moment to\ncheck this!\n\n## Matrices\n\nRecall that to understand T(\u2192x), we needed to answer two questions:\n\n  1. How can we build our vector \u2192x out of our presumably useful building blocks basis vectors b1,...,bn?\n  2. How can we express how our linear transformation T behaves on each of these basis vectors?\n\nThe notion of a coordinate vector solves our first problem, but we still need\nto answer the second. And to do this, we\u2019ll need to add a long-overdue\nrestriction to our notion of a linear transformation:\n\nIt's not just the inputs to a linear transformation that are vectors, but the\noutputs too! All of the inputs share the same vector space, and all of the\noutputs likewise share a vector space. (If the input and output vector spaces\nare the same, then we refer to this transformation as a linear operator.)\n\nIn other words, we want to be able to understand both the input and output of\nT by breaking them down into components, even if those components are\ncompletely different. Let\u2019s just feed in one basis element to start, b1. By\nour definition of a vector, we know that we can express T(b1) as a scaled sum\nof elements of some basis C={c1,...,cm} and get some coefficients, which we\u2019ll\ncall t1,1,...,tm,1. We can use our handy concept of a coordinate vector to\nexpress the coordinates of T(b1):\n\nT(b1)=m\u2211i=1ti,1ciBy the definition of a vector[T(b1)]C=[t1,1...tm,1]Re-\nexpressing as a coordinate vector\n\nBut there are quite a few more input basis vectors than just b1, so we may as\nwell write out all of these components as a grid of numbers:\n\n[T(bi)]C=[t1,i...tm,i]The coordinates when we feed in just one basis\nvector[T]B,C:=[t1,1...t1,nt2,1...t2,n\u22ee\u22f1\u22eetm,1...tm,n]Making a big grid with\neach value above as a separate column\n\nWe\u2019ll call this thing we just invented a matrix. There are two things to note\nhere:\n\n  1. Reifying T into a concrete grid of numbers required us to use two bases, not just one. Hence the notation [T]B,C.\n  2. We\u2019ve written down all of our components, but that doesn\u2019t actually help us do anything yet. We\u2019re still not sure what T(\u2192x) is.\n\nRecall that we were interested in the derivative operator D as it applies to\nreal-valued polynomials of degree 2 and lower. The derivative of a degree-2\npolynomial is a degree-1 polynomial, so we could just reuse our basis Q or\ndefine a slightly smaller basis for the output of D. For convenience, I'll\njust reuse Q. We can apply D individually to our basis vectors using some very\nbasic calculus to build our matrix:\n\n  0. D(q0)=D(z\u21a61)=z\u21a60.\n  1. D(q1)=D(z\u21a6z)=z\u21a61.\n  2. D(q2)=D(z\u21a6z2)=z\u21a62z.\n\nThen we can express all three of the above results as coordinate vectors in\nthe basis Q and organize these coordinate vectors into columns, which gives us\nthe following matrix. I highly encourage working this out for yourself!\n[D]Q,Q=[010002000]\n\nThe second point above segues nicely into our next topic:\n\n## Matrix-vector multiplication as an efficient shorthand for function\napplication\n\nAt this point we have all of the puzzle pieces. We have clean, concrete\nrepresentations of \u2192x and T relative to our well-understood basis vectors. Now\nall we need to do is find a good representation of T(\u2192x). This representation\nwill invariably be in the form of coordinates with respect to the basis C.\nWith this, we\u2019ll finally have a full, concrete understanding of T(\u2192x). In\nessence, we\u2019re replicating the green arrow below with the three white arrows:\n\nAs the diagram suggests, the missing piece here is the notion of matrix-vector\nmultiplication. We simply take the dot product of each row of [T]B,C with [x]B\nto get the coordinates of T(\u2192x) in the basis C. The proof for this is quite\nelegant:\n\n[T(\u2192x)]C=[T(n\u2211i=1xibi)]CRewriting x in the basis B=[n\u2211i=1xiT(bi)]CUsing the\nfact that T is linear=[n\u2211i=1(xim\u2211j=1tj,icj)]CBy definition of the coordinates\nof T(bi)=[m\u2211j=1n\u2211i=1tj,ixicj]CSwapping\nsummations=[\u2211ni=1t1,ixi...\u2211ni=1tm,ixi]By the definition of a coordinate vector\n\nWe can now read off the j-th element of our result above, \u2211ni=1tj,ixi, and\ncall it the \u201cdot product\u201d of the coordinate vectors [x1...xn] (our input) and\n[tj,1...tj,n] (the j-th row of our original matrix [T]B,C). Feel free to\nquickly check that this is, indeed, the same thing as the traditional\ndefinition of a dot product.\n\nAnd now that we have this compact way to compute [T(\u2192x)]C as a coordinate\nvector of dot products, we can abbreviate this computation as our definition\nof a matrix-vector product.\n\n[T]B,C[x]B:=[\u2211ni=1t1,ixi...\u2211ni=1tm,ixi]\n\nAnd that\u2019s it! It\u2019s worth meditating on that proof for a little bit. Matrix-\nvector multiplication isn\u2019t just some meaningless symbol shuffling: we\u2019ve\nderived exactly the computations necessary to go from a representation of \u2192x\nto a representation of T(\u2192x). Of course, now that we\u2019ve worked through a\njustification for this notation instead of just having a definition presented\nto us, we know that \u201cproduct\u201d is a bit of a misnomer, and this is really just\nan efficient method of function application.\n\nOne last time, let's pull up our example and put it all together. This time we\ncan use our shiny new definition of matrix-vector multiplication to compute\nthe derivative of the function f where f(z)=1+2z+3z2. First, we'll rewrite f\nin our basis Q to get f=q0+2q1+3q2. Then, we'll rephrase that as the\ncoordinate vector [f]Q=[123]. Finally, we'll apply the matrix for our\nderivative operator, which we computed above:\n[D]Q,Q[f]Q=[010002000][123]=[2\u00d712\u00d730]=[260]=[z\u21a62+6z]Q And indeed,\nddz(1+2z+3z2)=2+6z. Success!\n\nA similar process can be used to invent matrix-matrix multiplication: it\u2019s\njust function composition, carried out numerically. This one is left as an\nexercise for the reader :D\n\n## Some conclusions\n\nOriginally, I had planned to go further with this post, inventing eigenvectors\nas \u201cparticularly nice basis vectors\u201d which allow you to just think of your\ntransformation as scalings, and then introducing the Fourier transform as an\neigendecomposition for the second derivative operator. But I think I\u2019ve made\nthe high-level idea clear enough now, so let\u2019s skip to conclusions.\n\nIn short, I don\u2019t think it\u2019s necessarily optimal to teach linear algebra the\nway it is taught traditionally, with a deluge of mechanical rigmarole\npreceding any conceptual justification for why we do things the way we do.\nWorse, I think that obfuscating the centrality of linear transformations\nactually undersells the broad applicability of linear algebra, since priming\nstudents to think of the subject as just manipulating grids of numbers makes\nthem less equipped to grasp other kinds of vector spaces, especially function\nspaces.\n\nHeck, linear algebra is so broadly useful that we will go out of our way to\nmake things linear when we can - whether we\u2019re linearizing differential\nequations or using representation theory to convert all kinds of algebraic\nstructures into linear transformations.\n\nDoes that mean you should ditch Gilbert Strang and actually teach things the\nway this blog post does? Well, probably not. For starters, the more\ntraditional, axiomatized definition of a vector space is better suited to\npreparing a student for future mathematics courses. But I think there is a\npedagogical middle ground here that doesn\u2019t leave linear transformations as an\nafterthought, and I hope that I\u2019ve at least shown that the idea to front-load\nlinear transformations isn\u2019t without merit. In particular, I think that\nemphasizing the centrality of linear transformations also primes students to\nbetter understand other structure-preserving maps in the future, such as\ngroup/ring/etc homomorphisms, continuous functions between topological spaces,\nand so on.\n\nOne thing I have not mentioned yet is that part of linear algebra\u2019s usefulness\ncomes not just from its mathematical universality, but from how amenable it is\nto being automated on modern hardware. The natural parallelizability of matrix\nmultiplication has allowed us to build larger and faster GPUs to crunch\nnumbers in quantities that have a quality all of their own. The entire modern\nfield of deep learning hinges on this, as do other kinds of numerical methods\nand physical simulation.\n\nEven quantum computers are really just chains of matrix multiplications when\nit comes down to it.\n\nIn a way, trying to explain how useful linear algebra is before actually\nteaching it may be a fool\u2019s errand; like a prisoner staring at dancing shadows\non the wall of Plato\u2019s cave, sometimes you just have to understand something\nfor yourself before it can really click. But that doesn\u2019t mean that we can\u2019t\nhelp people turn around so they can see the vast world that lay unnoticed\nbehind them.\n\n## Footnotes\n\n  1. An aside for programmers: the math way of defining structures can be thought of as defining interfaces or protocols, like VectorSpace or TopologicalSpace or Ring, and then letting any arbitrary concrete structure implement said interface if it satisfies the necessary properties (i.e. a structural type system). In a sense, the structuralist point of view on math is that it\u2019s only these interfaces that really matter, not the underlying concrete objects that implement them. So the natural numbers, for example, could just as well be any sequence of things with the right properties to be assigned the labels of \u201cone\u201d and \u201ctwo\u201d and so on, not just one particular canonical set of mathematical objects. This is a bit like programming while only paying attention to your type system and contracts and never once thinking about the actual bytes that you\u2019re shuffling around. Which, well, is exactly what math is, from the Curry-Howard perspective. But now we\u2019re getting REALLY off topic. \u21a9\n\n  2. In particular, what I\u2019m aiming for here is a little reverse mathematics, where rather than presenting definitions as a fait accompli and deriving theorems, I want to work backwards from our desired properties to figure out what definitions would get us there. \u21a9\n\n  3. For the sake of clarity, though, I will cheat a little bit and stick to finite-dimensional reasoning for most of this post. Pretty much everything should be generalizable to the infinite-dimensional case. \u21a9\n\n  4. More specifically, we assume they come from a field, i.e. have reasonable definitions of addition, subtraction, multiplication, and division with reasonable concepts of 0 and 1. If you give up on division you still have a ring, which means that we end up defining modules instead of vector spaces, which are still pretty nice but way beyond the scope of this post. \u21a9\n\n  5. At the expense of being confusingly nonstandard, I\u2019m going to write out coordinate vectors as rows instead of columns, because that makes them easier to write inline and I don\u2019t want to justify the concept of transposing just to write ^T everywhere for no real benefit. \u21a9\n\n  6. The caveat here is that the proof that every vector space has a basis requires the axiom of choice, but that axiom\u2019s basically a given if you want to have any fun anyway. \u21a9\n\ntags: math - musings\n\n", "frontpage": false}
