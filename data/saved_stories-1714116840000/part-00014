{"aid": "40162188", "title": "How to (accurately) evaluate RAG systems on tabular data", "url": "https://dynamo.ai/blog/rag-evals-on-embedded-tables", "domain": "dynamo.ai", "votes": 4, "user": "whoami_nr", "posted_at": "2024-04-25 19:51:38", "comments": 0, "source_title": "How to (accurately) evaluate RAG systems on tabular data", "source_text": "How to (accurately) evaluate RAG systems on tabular data\n\nRequest Demo\n\nResearch\n\nApr 25, 2024\n\n# How to (accurately) evaluate RAG systems on tabular data\n\n## Low-code tools are going mainstream\n\nPurus suspendisse a ornare non erat pellentesque arcu mi arcu eget tortor eu\npraesent curabitur porttitor ultrices sit sit amet purus urna enim eget.\nHabitant massa lectus tristique dictum lacus in bibendum. Velit ut viverra\nfeugiat dui eu nisl sit massa viverra sed vitae nec sed. Nunc ornare consequat\nmassa sagittis pellentesque tincidunt vel lacus integer risu.\n\n  1. Vitae et erat tincidunt sed orci eget egestas facilisis amet ornare\n  2. Sollicitudin integer velit aliquet viverra urna orci semper velit dolor sit amet\n  3. Vitae quis ut luctus lobortis urna adipiscing bibendum\n  4. Vitae quis ut luctus lobortis urna adipiscing bibendum\n\n### Multilingual NLP will grow\n\nMauris posuere arcu lectus congue. Sed eget semper mollis felis ante. Congue\nrisus vulputate nunc porttitor dignissim cursus viverra quis. Condimentum nisl\nut sed diam lacus sed. Cursus hac massa amet cursus diam. Consequat sodales\nnon nulla ac id bibendum eu justo condimentum. Arcu elementum non suscipit\namet vitae. Consectetur penatibus diam enim eget arcu et ut a congue arcu.\n\nVitae quis ut luctus lobortis urna adipiscing bibendum\n\n#### Combining supervised and unsupervised machine learning methods\n\nVitae vitae sollicitudin diam sed. Aliquam tellus libero a velit quam ut\nsuscipit. Vitae adipiscing amet faucibus nec in ut. Tortor nulla aliquam\ncommodo sit ultricies a nunc ultrices consectetur. Nibh magna arcu blandit\nquisque. In lorem sit turpis interdum facilisi.\n\n  * Dolor duis lorem enim eu turpis potenti nulla laoreet volutpat semper sed.\n  * Lorem a eget blandit ac neque amet amet non dapibus pulvinar.\n  * Pellentesque non integer ac id imperdiet blandit sit bibendum.\n  * Sit leo lorem elementum vitae faucibus quam feugiat hendrerit lectus.\n\n##### Automating customer service: Tagging tickets and new era of chatbots\n\nVitae vitae sollicitudin diam sed. Aliquam tellus libero a velit quam ut\nsuscipit. Vitae adipiscing amet faucibus nec in ut. Tortor nulla aliquam\ncommodo sit ultricies a nunc ultrices consectetur. Nibh magna arcu blandit\nquisque. In lorem sit turpis interdum facilisi.\n\n> \u201cNisi consectetur velit bibendum a convallis arcu morbi lectus aecenas\n> ultrices massa vel ut ultricies lectus elit arcu non id mattis libero amet\n> mattis congue ipsum nibh odio in lacinia non\u201d\n\n###### Detecting fake news and cyber-bullying\n\nNunc ut facilisi volutpat neque est diam id sem erat aliquam elementum dolor\ntortor commodo et massa dictumst egestas tempor duis eget odio eu egestas nec\namet suscipit posuere fames ded tortor ac ut fermentum odio ut amet urna\nposuere ligula volutpat cursus enim libero libero pretium faucibus nunc arcu\nmauris sed scelerisque cursus felis arcu sed aenean pharetra vitae suspendisse\nac.\n\n## Introduction\n\nIn our previous post, we explored how Retrieval-Augmented Generation (RAG)\nsystems can encounter hallucination issues and how DynamoEval can accurately\nand effectively diagnose the source of these errors. When RAG systems generate\nresponses, the retrieved document may be in plain text format or a different\nformat. Notably, when a table is present in the text, large language models\n(LLMs) may be more prone to hallucination due to the inherently different\nstructure of tables. Moreover, queries involving tables often require\ncomputational operations, which pose a greater challenge for LLMs. For\ninstance, on the WikiTableQuestion (WTQ) dataset, a standardized benchmark for\ntabular question-answering, the state-of-the-art model exhibits an error rate\nof 32.69 percent. Despite this fragility, there is a lack of dedicated RAG\nevaluation solutions focused on assessing pipelines that involve tabular data.\nDynamoEval aims to address this gap by providing a comprehensive offering\ntailored to evaluate RAG systems dealing with tabular data.\n\nFor this post, we will focus on evaluating RAG systems where the retrieved\ndocument is a table, and the generated response may involve some\nlogical/computational reasoning over the content in the table. For instance,\nconsider RAG systems interacting with financial documents with tables, like\nthe consolidated balance sheets from page 30 of Apple\u2019s 10-K report.\n\nUsers may be interested in simple look-up queries like \"What is the total\ncurrent asset of AAPL at the end of September, 2023? Respond in millions.\u201d or\noperation-focused queries like \u201cBy what percentage did the deferred revenue\nincrease/decrease in September, 2023 compared to September, 2022? Round to the\nfirst decimal place.\" When the model generates responses like \"$143,566\nmillion\" and \"Increased by 1.9%\" for these queries respectively based on the\nbalance sheet, the evaluator should be able to identify them as accurate and\nfaithful; on the other hand, responses like \"$135,405 million\" and \"Increased\nby 1.3%\", should be flagged as incorrect and unfaithful. In fact, for this\nparticular example, DynamoEval is able to do this accurately while existing\nevaluation solutions fail in one or more of these questions.\n\nIn the next sections, we will explore methods to enhance the evaluation\ncapabilities of two critical aspects:\n\n  1. Assessing the relevance of the table retrieved by the RAG system: This involves determining whether the retrieved table contains the necessary information to accurately derive an answer to the given query.\n  2. Evaluating the correctness and faithfulness of the generated response: This focuses on assessing whether the RAG system's output is not only correct but also faithful to the information provided in the retrieved table and the given query.\n\nBy addressing these two key areas, DynamoEval aims to improve the diagnosis of\nRAG systems when dealing with tabular data. Throughout the post we use a\nseries of test datasets modified from a standard Tabular QA dataset\nWikiTableQuestion (WTQ) with some manual cleaning, curation, and augmentation.\nWe curate different datasets which consist of queries, contexts, responses,\nand ground-truth binary labels indicating the quality (good/bad) of the\ncontexts and responses for retrieval and faithfulness evaluation. The\nevaluators will classify these contexts and responses, and their performance\nwill be measured with accuracy, precision, and recall based on the ground-\ntruth labels.\n\n## Findings\n\n### Prompting an LLM better can get you pretty far\n\nWec compare DynamoEval\u2019s ability to evaluate retrieval relevance and\nfaithfulness against existing RAG evaluation solutions like RAGAS, LlamaIndex\nEvaluators, and Tonic Validate. We also test a multi-modal evaluation with an\nimage input as a baseline: instead of feeding the table content as text, we\nconvert the table to an image and feed it to a vision-language model (VLM)\nlike GPT-4-vision.\n\nBecause existing RAG evaluation solutions are primarily designed for\nevaluating textual data, we observe that they are not well-suited for tasks\ninvolving tables when used out-of-the-box, despite utilizing the same base\nmodel, such as GPT-4. However, DynamoEval demonstrates that significant\nperformance improvements can be achieved through prompt optimizations. Some\nkey factors contributing to this enhancement include:\n\n  1. Instruction prompts for role assignment: By providing specific instructions to the model, particularly assigning it a well-defined role, the model can better understand its task and focus on the relevant aspects of the evaluation process.\n  2. Chain of Thought (CoT) prompting: Encouraging the model to outline the steps taken to reach a conclusion enables a more structured and transparent evaluation process. This approach allows for a clearer understanding of the model's reasoning and decision-making process.\n  3. Response structure optimization: Instructing the model to state its decision at the end of the response, after generating a step-by-step explanation, promotes a more correct decision. This structure ensures that the model's conclusion is well-conditioned on the explanations.\n  4. Binary decision output: Instead of generating scores, prompting the model to output a binary decision (e.g., correct or incorrect) simplifies the evaluation process and provides a clear-cut assessment of the RAG system's performance.\n\nBy incorporating these prompt optimization techniques, DynamoEval showcases\nits ability to significantly enhance the evaluation of RAG systems when\ndealing with tabular data, surpassing the limitations of existing solutions.\n\n## The choice of base model for evaluation matters\n\nWe have observed that the performance of the evaluation process varies\nsignificantly depending on the choice of the base LLM, even when using the\nsame optimized prompts. The plot below illustrates the performance of GPT\n(3.5) and Mistral (small) models on faithfulness evaluation using different\nversions of prompts:\n\n  1. Vanilla: Vanilla prompting (no Chain of Thought), with the decision stated before the explanation\n  2. CoT: Chain of Thought prompting, with the decision stated before the explanation\n  3. CoT + Optimized: Chain of Thought prompting, with the decision stated after the explanation\n\nThe results demonstrate that CoT prompting and stating the decision after the\nexplanation provides a greater benefit to the GPT model compared to the\nMistral model. However, both models ultimately exhibit lower performance\ncompared to the GPT-4 model discussed earlier.\n\n## More on operation-heavy queries\n\nWhen working with tabular data, it is common to encounter queries that demand\nmore complex operations or logical reasoning over the contents of the table.\nTo better understand how models perform in this scenario, we manually created\na dataset based on the WikiTableQuestion (WTQ) dataset, specifically focusing\non queries that heavily rely on operations. We evaluate the faithfulness\nperformance on a set of questions that involves various types of operations,\nincluding addition, subtraction, variance, standard deviation, counting,\naveraging and percentage calculations.\n\nBy assessing the models' performance on this curated dataset, we aim to gain\ninsights into their capabilities and limitations when dealing with more\ncomplex queries involving tabular data. The below figure demonstrates\nDynamoEval\u2019s performance compared to other RAG evaluation solutions.\n\nWhile DynamoEval shows a slightly lower performance compared to the previous\nset of \u201ceasier\u201d queries, it is still able to significantly outperform existing\nsolutions. We describe some preliminary patterns from the failure cases, which\nwill be useful to further investigate and categorize the types of\nqueries/tables the evaluator model is particularly weak at:\n\n### Operation involving a long list of entries\n\n  * It is more likely to fail when the table is long and therefore requires more entries to consider for operations. In the examples below, the model failed to identify the given responses as accurate and faithful, by failing to carry out calculations from a long list of entries or miscounting the entries from a long table.\n\n#### Example 1\n\n#### Example 2\n\n### Errors in filtering the correct entries\n\n  * There were occasional errors for smaller tables in filtering the correct entries to consider. In the examples below, the model failed to identify the given responses as accurate and faithful by incorrectly considering the rows that did not satisfy the conditions set by the query.\n\n#### Example 1\n\n#### Example 2\n\n## Conclusion\n\nEvaluating the performance of RAG systems involving table data presents unique\nchallenges due to the inherent differences between tabular and textual\ncontent. Our findings demonstrate that DynamoEval, with its optimized\nprompting techniques, significantly outperforms existing RAG evaluation\nsolutions in assessing the relevance of retrieved tables and the faithfulness\nof generated responses. Through our curated datasets based on the\nWikiTableQuestion (WTQ) benchmark, we have identified key areas where the\nevaluator models may struggle, particularly when dealing with complex queries\ninvolving lengthy tables or multiple logical operations. By further\nunderstanding these limitations, we can focus our efforts on developing more\nrobust and reliable diagnostics for RAG systems that can handle a wider range\nof tabular data and query types.\n\n## Contact us\n\nAt Dynamo AI, we are committed to helping organizations measure and mitigate\nRAG hallucination effectively. Our comprehensive RAG evaluation offering\nprovides deep insights into model performance, enabling teams to identify and\naddress weaknesses in their RAG pipelines.\n\nDynamo AI also offers a range of AI privacy and security solutions to help you\nbuild trustworthy and responsible AI systems. To learn more about how Dynamo\nAI can help you evaluate and improve your RAG pipelines, or to explore our AI\nprivacy and security offerings, please request a demo here.\n\nJoon Kim\n\nMachine Learning Research Scientist\n\n## Popular posts\n\nResearch\n\nApril 23, 2024\n\n### Tackling the Explainability Gap in RAG Hallucination Evals\n\nProduct\n\nApril 17, 2024\n\n### DynamoFL is now Dynamo AI: the End-to-End Platform for Productionizing\nSecure and Compliant AI for Enterprises\n\n## Latest articles\n\nBrowse all articles\n\nResearch\n\nApr 24, 2024\n\n### Tackling the Explainability Gap in RAG Hallucination Evals\n\nProduct\n\nApr 17, 2024\n\n### DynamoFL is now Dynamo AI: the End-to-End Platform for Productionizing\nSecure and Compliant AI for Enterprises\n\nResearch\n\nApr 15, 2024\n\n### Maximizing the ROI of Large Language Models for the Large Enterprise\n\nThe enterprise platform for enabling private, secure, and regulation-compliant\nGen AI models\n\nPlatform\n\n  * DynamoEnhance\n  * DynamoEval\n  * DynamoGuard\n\nCompany\n\n  * Careers\n  * Legal\n\n  * Blog\n  * Solutions\n\n### Compliance-Ready AI for the Enterprise\n\nReady your organization for emerging regulations with safe and secure\nproduction-grade Gen AI.\n\nRequest a Demo\n\nCopyright \u00a9 AI X plus | Designed by BRIX Templates - Powered by Webflow\n\n", "frontpage": false}
