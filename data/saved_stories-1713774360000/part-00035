{"aid": "40109474", "title": "Stripe adopts Airbnb's Chronon for ML features", "url": "https://stripe.com/blog/shepherd-how-stripe-adapted-chronon-to-scale-ml-feature-development", "domain": "stripe.com", "votes": 2, "user": "acossta", "posted_at": "2024-04-21 21:39:50", "comments": 0, "source_title": "Shepherd: How Stripe adapted Chronon to scale ML feature development", "source_text": "Shepherd: How Stripe adapted Chronon to scale ML feature development\n\n# Global Payments\n\n  * Payments\n\nOnline payments\n\n  * Checkout\n\nPrebuilt payment form\n\n  * Elements\n\nCustomizable payments UIs\n\n  * Payment Links\n\nNo-code payments\n\n  * Radar\n\nFraud & risk management\n\n  * Connect\n\nPayments for platforms\n\n  * Billing\n\nSubscription management\n\n  * Invoicing\n\nOnline invoices\n\n  * Terminal\n\nIn-person payments\n\n  * Financial Connections\n\nLinked financial account data\n\n  * Identity\n\nOnline identity verification\n\n  * Climate\n\nCarbon removal\n\n# Revenue and Finance Automation\n\n  * Billing\n\nSubscription management\n\n  * Invoicing\n\nOnline invoices\n\n  * Tax\n\nSales tax & VAT automation\n\n  * Revenue Recognition\n\nAccounting automation\n\n  * Sigma\n\nCustom reports\n\n  * Data Pipeline\n\nData warehouse sync\n\n  * Atlas\n\nStartup incorporation\n\nAutomate revenue and finance on Stripe\n\n# Banking-as-a-Service\n\n  * Connect\n\nPayments for platforms\n\n  * Capital\n\nBusiness financing\n\n  * Issuing\n\nCard creation\n\n  * Treasury\n\nBanking-as-a-service\n\n# By stage\n\n  * Startups\n  * Enterprises\n\n# By use case\n\n  * SaaS\n  * Platforms\n  * Ecommerce\n  * Marketplaces\n  * Crypto\n  * Creator Economy\n  * Embedded Finance\n  * Global Businesses\n  * Finance Automation\n\n# Integrations & Custom Solutions\n\n  * App Marketplace\n  * Professional Services\n  * Partner Ecosystem\n\n  * Documentation\n\n# Get started\n\n  * Prebuilt checkout\n  * Libraries and SDKs\n  * App integrations\n\n# Guides\n\n  * Accept Online Payments\n  * Manage Subscriptions\n  * Send Payments\n\n  * Full API Reference\n  * API Status\n  * API Changelog\n  * Build on Stripe Apps\n\n  * Support Center\n  * Support Plans\n  * Guides\n  * Customer Stories\n  * Blog\n  * Annual Conference\n  * Contact Sales\n\n  * Jobs\n  * Newsroom\n  * Stripe Press\n  * Become a Partner\n\nSign in\n\n  * Payments\n\nOnline payments\n\n  * Checkout\n\nPrebuilt payment form\n\n  * Elements\n\nCustomizable payments UIs\n\n  * Payment Links\n\nNo-code payments\n\n  * Radar\n\nFraud & risk management\n\n  * Connect\n\nPayments for platforms\n\n  * Billing\n\nSubscription management\n\n  * Invoicing\n\nOnline invoices\n\n  * Terminal\n\nIn-person payments\n\n  * Financial Connections\n\nLinked financial account data\n\n  * Identity\n\nOnline identity verification\n\n  * Climate\n\nCarbon removal\n\n  * Billing\n\nSubscription management\n\n  * Invoicing\n\nOnline invoices\n\n  * Tax\n\nSales tax & VAT automation\n\n  * Revenue Recognition\n\nAccounting automation\n\n  * Sigma\n\nCustom reports\n\n  * Data Pipeline\n\nData warehouse sync\n\n  * Atlas\n\nStartup incorporation\n\nAutomate revenue and finance on Stripe\n\n  * Connect\n\nPayments for platforms\n\n  * Capital\n\nBusiness financing\n\n  * Issuing\n\nCard creation\n\n  * Treasury\n\nBanking-as-a-service\n\n# By stage\n\n  * Startups\n\n  * Enterprises\n\n# By Use Case\n\n  * SaaS\n  * Platforms\n  * Ecommerce\n  * Marketplaces\n  * Crypto\n\n  * Creator Economy\n  * Embedded Finance\n  * Global Businesses\n  * Finance Automation\n\n# Integrations & Custom Solutions\n\n  * App Marketplace\n  * Professional Services\n\n  * Partner Ecosystem\n\n  * Documentation\n\nStart integrating Stripe\u2019s products and tools\n\n# Get started\n\n  * Prebuilt checkout\n  * Libraries and SDKs\n  * App integrations\n  * Code samples\n\n# Guides\n\n  * Accept online payments\n  * Manage subscriptions\n  * Send payments\n  * Set up in-person payments\n\n  * Full API Reference\n  * API Status\n\n  * API Changelog\n  * Build on Stripe Apps\n\n  * Support Center\n  * Support Plans\n  * Guides\n  * Customer Stories\n\n  * Blog\n  * Annual Conference\n  * Contact Sales\n\n  * Jobs\n  * Newsroom\n\n  * Stripe Press\n  * Become a Partner\n\n# Shepherd: How Stripe adapted Chronon to scale ML feature development\n\nApril 15, 2024\n\nBen Mears ML Features\n\nMachine learning (ML) is a foundation underlying nearly every facet of\nStripe\u2019s global operations, optimizing everything from backend processing to\nuser interfaces. Applications of ML at Stripe add hundreds of millions of\ndollars to the internet economy each year, benefiting millions of businesses\nand customers worldwide. Developing and deploying ML models is a complex\nmultistage process, and one of the hardest steps is feature engineering.\n\nBefore a feature\u2014an input to an ML model\u2014can be deployed into production, it\ntypically goes through multiple iterations of ideation, prototyping, and\nevaluation. This is particularly challenging at Stripe\u2019s scale, where features\nhave to be identified among hundreds of terabytes of raw data. As an engineer\non the ML Features team, my goal is to build infrastructure and tooling to\nstreamline ML feature development. The ideal platform needs to power ML\nfeature development across huge datasets while meeting strict latency and\nfreshness requirements.\n\nIn 2022 we began a partnership with Airbnb to adapt and implement its\nplatform, Chronon, as the foundation for Shepherd\u2014our next-generation ML\nfeature engineering platform\u2014with a view to open sourcing it. We\u2019ve already\nused it to build a new production model for fraud detection with over 200\nfeatures, and so far the Shepherd-enabled model has outperformed our previous\nmodel, blocking tens of millions of dollars of additional fraud per year.\nWhile our work building Shepherd was specific to Stripe, we are generalizing\nthe approach by contributing optimizations and new functionality to Chronon\nthat anyone can use.\n\nThis blog discusses the technical details of how we built Shepherd and how we\nare expanding the capabilities of Chronon to meet Stripe\u2019s scale.\n\n## ML feature engineering at Stripe scale\n\nIn a previous blog post, we described how ML powers Stripe Radar, which allows\ngood charges through while blocking bad ones. Fraud detection is adversarial,\nand Stripe needs to improve models quickly\u2014fraud patterns change as malicious\nactors evolve their attacks, and Stripe needs to move even faster.\n\nML feature development is the process of defining the inputs (features) that a\nmodel uses to make its predictions. For example, a feature for a fraud\nprediction model could be the total number of charges processed by a business\non Stripe over the last seven days.\n\nTo identify and deploy new features that would address rapidly changing fraud\ntrends, we needed a feature engineering platform that would allow us to move\nquickly through the lifecycle of feature development.\n\nEffectively deploying ML models in the Stripe environment also requires\nmeeting strict latency and feature freshness requirements.\n\n  * Latency: A measure of the time required to retrieve features during model inference. This is important because models such as the ones powering Radar are also used in processing payments, and the time required to retrieve features directly impacts the overall payment API latency\u2014lower latency means faster payments and a better overall customer experience for businesses.\n  * Feature freshness: A measure of the time required to update the value of features. This is important because Stripe needs to react quickly to changes in fraud patterns. For example, if there is an unusual spike in transactions for one business, feature values must quickly be updated to reflect the pattern so models can incorporate the new information in their predictions for other businesses.\n\nThere are trade-offs between latency and feature freshness. For example, we\ncan improve latency at the expense of freshness by performing more\nprecomputation when new data arrives, while we can prioritize freshness over\nlatency by performing more of the feature computation during serving. Stripe\u2019s\nstrict requirements for both low latency and feature freshness across the\nbillions of transactions we process create a unique set of constraints on our\nfeature platform.\n\n## Shepherd: Stripe\u2019s next-generation ML feature platform\n\nAs Stripe grew, so did our ambitions for applying ML to hard problems. To\naccelerate our feature engineering work, we evaluated several options,\nincluding revamping our existing platform, building from scratch, and\nimplementing proprietary or open-source options. One particularly appealing\noption was an invitation we received from Airbnb to become early external\nadopters of Chronon, which Airbnb had developed to power its ML use cases.\n\nAirbnb wanted to integrate the platform with an external partner prior to open\nsourcing, and Chronon met all of our requirements: an intuitive Python- and\nSQL-based API, efficient windowed aggregations, support for online and offline\ncomputation of features, and built-in consistency monitoring. At the same\ntime, we couldn\u2019t just use it off-the-shelf. We knew we would need to adapt\nChronon to Stripe\u2019s unique scale, where training data can include thousands of\nfeatures and billions of rows. It was going to be a significant engineering\nchallenge, but we were confident that it was a strong foundational building\nblock.\n\n### Adapting Chronon\n\nChronon supports batch and streaming features in both online and offline\ncontexts. To be able to use Chronon as the foundation for Shepherd, we needed\nto make sure the offline, online, and streaming components could all meet\nStripe\u2019s scale.\n\nML engineers use Chronon to define their features with a Python- and SQL-based\nAPI, and Chronon provides the offline, streaming, and online components to\ncompute and serve the features. Integrating with Chronon involves setting up\neach of these components and providing an implementation for the key-value\n(KV) store used to store feature data for serving. When integrating with\nChronon, we needed to make sure each of the components could meet our feature\nfreshness and latency requirements.\n\n### KV store implementation\n\nThe KV store is responsible for storing data required to serve features.\nOffline jobs compute and write historical feature data to the store, and\nstreaming jobs write feature updates. To cost-efficiently scale our KV store,\nwe split it into two implementations: a lower-cost store optimized for bulk\nuploads that is write-once and read-many, and a higher-cost distributed\nmemcache-based store that is optimized for write-many and read-many. With this\ndual KV store implementation, we lowered the cost of storing and serving data\nwhile still meeting our latency and feature freshness requirements.\n\n### Streaming jobs\n\nChronon streaming jobs consume event streams and write the events to the KV\nstore. The events can be thought of as updates to features. The default\nChronon implementation writes events into the KV store with no preaggregation.\nStoring individual events into the KV store would not allow us to meet our\nlatency requirements for features with a large number of events. We needed to\nchoose a streaming platform that could achieve low latency updates and allow\nus to implement a more scalable write pattern.\n\nWe chose Flink as the streaming platform because of its low latency stateful\nprocessing. Since the Chronon API is a combination of Python and Spark SQL,\nmaintaining consistency between offline and online computation meant we needed\na way to run Spark SQL expressions in Flink. Fortunately, the Spark SQL\nexpressions used in Chronon\u2019s feature definitions only require maps and\nfilters. These are narrow transformations\u2014with no shuffling of data\u2014and can be\napplied to individual rows.\n\nWe implemented support for Spark SQL expressions applied to Flink rows. With\nFlink now powering our feature updates, we achieved p99 feature freshness of\n150ms.\n\nUntiled Architecture\n\nTiled Architecture\n\nFlink-based streaming architecture allowed us to meet our feature freshness\nrequirements; that left latency targets. To achieve those, we needed to modify\nhow Chronon stores events in the KV store. When events are stored\nindividually, computing features requires retrieving events for the feature\nand aggregating them together. If there are a large number of events for the\nfeature, this is time-consuming and increases latency.\n\nRather than store individual events, we decided to maintain the state of\npreaggregated feature values in the Flink app, and periodically flush those\nvalues out to the KV store. We call each of these preaggregated values a\n\u201ctile.\u201d With tiling, computing a feature only requires retrieving and\naggregating the tiles for the feature rather than all the individual events.\nFor features with a large number of events, this is a much smaller amount of\ndata and significantly decreases latency. We contributed both the Flink and\ntiling implementations back to Chronon, along with documentation on how to get\nstarted with them.\n\n### Meeting Stripe\u2019s offline requirements\n\nThe Chronon offline algorithm produces both offline training data for models\nand batch-only use cases. Offline jobs are also required to compute historical\ndata used for serving GroupBys. The offline jobs are configured using the same\nPython- and Spark SQL-based API as the online jobs, allowing developers to\ndefine their features once and compute both online and offline features.\n\nStripe\u2019s scale for offline jobs is larger than previous use cases of Chronon,\njust as it was for streaming and online components. Although the offline\nalgorithm is designed to be robust, with support for handling skewed data, we\nneeded to verify that it would scale to the size of Stripe\u2019s training sets. As\na first step to integrating with Chronon\u2019s offline jobs, we performed\nbenchmarks of training dataset generation and found the algorithm to be\nscalable with predictable tuning knobs.\n\nAfter verifying its scalability, we needed to integrate Chronon\u2019s offline jobs\nwith Stripe\u2019s data orchestration system. We built a custom integration for\nscheduling and running jobs that worked with our highly customized Airflow\nsetup. We designed the integration so users only need to mark their GroupBys\nas online or set an offline schedule in their Join definitions, after which\nthe required offline jobs are automatically scheduled.\n\nWe also needed to integrate Chronon with Stripe\u2019s data warehouse. Chronon\nassumes data sources are all partitioned Hive tables. Not all data sources at\nStripe meet these requirements. For example, many of the data sources required\nfor batch features are unpartitioned snapshot tables.\n\nWe built support into our Chronon integration for defining features with a\nwider variety of data sources, and for writing features to Stripe\u2019s data\nwarehouse using customized Iceberg writers. Fully integrating with our data\nwarehouse provides feature engineers the flexibility to define features using\nany data source, and to consume features in downstream batch jobs for use\ncases including model training and batch scoring.\n\nOur implementation for more flexible data source support was Stripe-specific,\nbut we plan to generalize the approach and contribute it to Chronon.\n\n## Building a SEPA fraud model on Shepherd\n\nOur first use case for Shepherd was a partnership with our Local Payment\nMethods (LPM) team to create an updated ML model for detecting SEPA fraud.\nSEPA, which stands for Single Euro Payments Area, enables people and\nbusinesses to make cashless euro payments\u2014via credit transfer and direct\ndebit\u2014anywhere in the European Union. The LPM team initially planned on\ncombining new Shepherd-created features with existing features from our legacy\nfeature platform, but found development on Shepherd so easy that they created\nall new features and launched a Shepherd-only model.\n\nOur new SEPA fraud model consists of over 200 features, including a\ncombination of batch-only and streaming features. As we built the model, we\nalso developed support for modeling delay in the offline training data so we\ncould accurately represent the delay of batch data in training data to avoid\ntraining-serving skew\u2014when the feature values that a model is trained on are\nnot reflective of the feature values used to make predictions.\n\nAs part of the new SEPA fraud model, we also built monitoring and alerting for\nShepherd\u2014including integrating with Chronon\u2019s online offline consistency\nmonitoring. As we mentioned at the start of this post, the new model blocks\ntens of millions of dollars of additional fraud a year.\n\n## Supporting the Chronon community\n\nAs a co-maintainer of Chronon with Airbnb, we\u2019re excited to grow and support\nthis open-source community while continuing to expand the capabilities of the\nproject. We also designed the new Chronon logo, a subtle nod to the fabric of\ntime.\n\nOver the coming months, we\u2019ll contribute new functionality and additional\noptimizations to Chronon, and we\u2019ll share more details about how teams at\nStripe are adopting Shepherd.\n\nTo get started with Chronon, check out the GitHub repository, read the\ndocumentation at Chronon.ai, and drop into our community Discord channel.\n\nAnd if you\u2019re interested in building ML infrastructure at Stripe\u2014or developing\nML features for Stripe products\u2014consider joining our engineering team.\n\n# Stay connected\n\nSubscribe to receive new blog posts from Stripe in your RSS reader.\n\nSubscribe to RSS\n\n# Like this post? Join our team.\n\nStripe builds financial tools and economic infrastructure for the internet.\n\nView roles\n\n# Have any feedback or questions?\n\nWe\u2019d love to hear from you.\n\nContact us\n\n", "frontpage": false}
