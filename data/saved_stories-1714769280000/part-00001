{"aid": "40248113", "title": "Small Data, Big Compute", "url": "https://www.moderndescartes.com/essays/small_data_big_compute/", "domain": "moderndescartes.com", "votes": 1, "user": "brilee", "posted_at": "2024-05-03 14:32:14", "comments": 0, "source_title": "Small Data, Big Compute", "source_text": "Small Data, Big Compute\n\n  * Brian Kihoon Lee\n  * Essays\n\n# Small Data, Big Compute\n\nOriginally posted 2024-03-31\n\nTagged: software engineering, machine learning, strategy\n\nObligatory disclaimer: all opinions are mine and not of my employer\n\nLLMs are really expensive to run, computationally speaking. I think you may be\nsurprised by the order of magnitude difference.\n\nWhile working at Lilac, I coined a phrase \u201csmall data, big compute\u201d to\ndescribe this pattern, and used it to drive engineering decisions.\n\n## Arithmetic intensity\n\nArithmetic intensity is a concept popularized by NVidia that measures a very\nsimple ratio: how many arithmetic operations are executed per byte\ntransferred?\n\nConsider a basic business analyst query: SELECT SUM(sales_amount) FROM table\nWHERE time < end_range AND time >= start_range. This query executes 1 addition\nfor each 4-byte floating point number it processes, for an arithmetic\nintensity of 0.25. However, the bytes corresponding to sales_amount are\nusually interleaved with the bytes for time and row_id and everything else in\nthe table, so only 1-10% of the bits read from disk are actually relevant to\nthe calculation, for a net arithmetic intensity of 0.01.\n\nIs 0.01 good or bad? Well, computers can read data from disk at roughly 1 GiB\nper second, or 250M floats per second; they can compute at roughly 8-16 FLOPs\nper cycle, with 3GHz clock speed = 25-50B float ops per second. Computers\ntherefore have a 100:1 available ratio of compute to disk. Any code with an\narithmetic intensity of less than 100 is underutilizing the CPU.\n\nIn other words, your typical business analyst query is horrendously\nunderutilizing the computer, by a factor of about 10,000x. This mismatch is\nwhy there exists a $100B market for database companies and technologies that\ncan optimize these business queries (Spark, Parquet, Hadoop, MapReduce, Flume,\netc.). They do so by using columnar databases and on-the-fly compression\ntechniques like run-length encoding, bit-packing, and delta compression, which\ntrade increased compute for more effective use of bandwidth. The result is\nblazing fast analytics queries that actually fully utilize the 100:1 available\nratio of compute to disk.\n\nHow many FLOPs do we spend per byte of user data in an LLM? Well... consider\nthe popular 7B model size. As a rough approximation, let\u2019s say each parameter-\nbyte interaction results in 1 FLOP, for an arithmetic intensity of 1010\noperations per byte processed. Other larger LLMs can go to 1013. You could\nquibble about bytes vs. tokens or multiply vs. add and the cost of\nexponentiation. But does it really matter if it\u2019s 8 or 9 orders of magnitude\nmore expensive per byte than the business analyst query? Convnets for image\nprocessing have an arithmetic intensity of 104 - 105. It\u2019s large but not\nunreasonable, which is why they\u2019ve found many applications in factory QC,\nagriculture, satellite imagery processing, etc..\n\nNeedless to say, this insane arithmetic intensity breaks just about every\nassumption and expectation that\u2019s been baked into the way we think about\nsoftware for the past twenty years.\n\n## Technical implications\n\n### No need for distributed systems\n\nUnless you work at a handful of companies that train LLMs from scratch, you\nwill not have the budget to operate LLMs on \u201cbig data\u201d. A single 1TB harddrive\ncan store enough text data to burn 10 million dollars in GPT4 API calls!\n\nAs a result, most business use cases for LLMs will inevitably operate on small\ndata - say, <1 million rows.\n\nThe software industry has spent well over a decade learning how to build\nsystems that scale across trillions of rows and thousands of machines, with\nthe tradeoff that you would wait at least 30s per invocation. We got used to\nthis inconvenience because it let us turn a 10 day single-threaded job into a\n20 minute distributed job.\n\nNow, faced with the daunting prospect of a mere 1 million rows, all of that is\nunnecessary complexity. Users deserve sub-second overheads when doing non-LLM\ncomputations on such small data. Lilac utilizes DuckDB to blast all cores on a\nsingle machine to compute basic summary statistics for every column in the\nuser\u2019s dataset, in less than a second - a luxury that we can afford because of\nour increased budget for slop!\n\n### Massive budget for bloat\n\nOrdinarily, inefficiencies in per-item handling can add up to a significant\ncost. This includes things like network bandwidth/latency, preprocessing of\ndata in a slow language like Python, HTTP request overhead, unnecessary\ndependencies, and so on.\n\nLLMs are so expensive that everything else is peanuts. There is a lot more\nbudget for slop and I fully expect businesses to use this budget. I am sorry\nto the people who are frustrated with the increasing bloat of the modern\nsoftware stack - LLMs will bring on yet another expansionary era of bloat.\n\nAt Lilac, we ended up building a per-item progress saver into our dataset.map\ncall, because it was honestly a small cost, relative to the fees that our\nusers were incurring while making API calls to OpenAI. In comparison,\nHuggingFace\u2019s dataset.map doesn\u2019t implement checkpointing, because it would be\nan enormous waste of time and compute and disk space to checkpoint the result\nof a trivial arithmetic operation.\n\n### Latency-batching tradeoffs\n\nGPU cores have a compute-memory bandwidth ratio of around 100 - they are not\nfundamentally different from computers in this regard. Ironically, LLMs end up\nbandwidth-limited despite the insane arithmetic intensity quoted above. If you\nalso count the parameters of the model in the \u201cbytes transferred\u201d denominator,\nthen LLM arithmetic intensity is roughly n+mnm, with n = input bytes and m =\nmodel bytes. Since m\u226bn, arithmetic intensity is proportional to n. Increasing\nbatch size is thus a free win, up to the point where the GPU is compute-bound\nrather than bandwidth-bound.\n\nFor real-time use cases like chatbots, scale is king! When you have thousands\nof queries per second, it becomes easy to wait 50 milliseconds for a batch of\nuser queries to accumulate, and then execute them in a single batch. If you\nonly have one query per second, you are in a situation where you will either\nget poor GPU utilization (expensive hardware goes to waste), or users will\nhave to wait multiple seconds for enough accumulated queries to make a batch.\n\nFor offline use cases like document corpus embedding/transformation, we can\nautomatically get full utilization through internal batching of the corpus.\nBecause GPUs are the expensive part, I expect organizations to implement a\nqueueing system to maximize usage of GPUs around the clock, possibly even\nintermingling offline jobs with real-time computation.\n\n### Minimal viable fine-tune\n\nAs a corollary of \u201ccompute cost dominates all\u201d, any and all ways to optimize\ncompute cost will be utilized. We will almost certainly see a relentless drive\ntowards specialization of cheaper fine-tuned models for every conceivable use\ncase. Stuff like speculative decoding shows just how expensive the largest\nLLMs are - you can productively run a smaller LLM to try and predict the\nlarger LLM\u2019s output, in real time!\n\nIn between engineering optimizations, fine-tuning/research breakthroughs, and\nincreased availability of massively parallel hardware optimized for LLMs, the\ncost for any particular performance point will decrease significantly - some\npeople claim 4x every year, which sounds aggressive but not even that\nunreasonable - 1.5x each from hardware, research, and engineering\noptimizations gets you close to ~4x.\n\nI expect there to be a good business in drastically reducing compute costs by\nmaking it very easy to fine-tune a minimal viable model for a specific\npurpose.\n\n## Business implications\n\n### Data egress is not a moat\n\nCloud providers invest a lot of money into onboarding customers, with the\nknowledge that once they\u2019re inside, it becomes very expensive to unwind all of\nthe business integrations they\u2019ve built. Furthermore, it becomes very\nexpensive to even try to diversify into multiple clouds, because data egress\noutside of the cloud is stupidly expensive. This is all part of an intentional\nstrategy to make switching harder.\n\nYet, the insane cost of LLMs means that data egress costs are a relatively\nsmall deal. 1GB of egress costs ~$0.10, while embedding 1GB worth of text\nwould cost ~$50. As a result, I expect that...\n\n### A new GPU cloud will emerge\n\nBecause of the ease with which small data can flow between clouds, I expect a\nnew cloud competitor, focused on cheap GPU compute. Scale will be king here,\nbecause increased scale results in negotiating power for GPU purchase\ncontracts, investments into GPU reliability, investments into engineering\ntricks to maximize GPU utilization, and improved latency for realtime\napplications. Modal, Lambda, and NVidia seem like potential cloud winners\nhere, but the truth is that we\u2019re all winners, because relentless competition\nwill drive down GPU costs for everyone.\n\n### Attack > defense\n\nA certain class of user-generated content will become a Turing Arena of sorts,\nwhere LLMs will generate fake text (think Amazon product reviews or Google\nsearch result spam or Reddit commenter product/service endorsements), and LLMs\nwill try to detect LLM-generated text. I think it\u2019s a reasonable guess that\nLLMs will only be able to detect other LLMs of lesser quality.\n\nUnfortunately for the internet, I think attack will win over the defense. The\nreason is safety in numbers.\n\nA small number of attackers will have the resources to use the most expensive\nLLMs to generate the most realistic looking fake reviews, specifically in\ncategories where the profit margins are highest (think \u201cbest hotel in\nmanhattan\u201d or \u201cbest machu picchu tour\u201d). However, a much larger number of\nattackers will have moderate resources to use medium-sized LLMs to generate a\nmuch larger volume of semi-realistic fake reviews. The defense, on the other\nhand, has to scale up LLMs to run on all user-generated content, and\nrealistically they will only be able to afford running medium or small LLMs to\ndo so. Dan Luu\u2019s logorrhea on the diseconomies of scale is exactly the right\nway to think here.\n\nEventually, I think it will actually push some sort of in-person notarization\nor other reputation-based system to finally become a reality - the physical\nlogistics will eventually become cheaper than running that many LLMs at scale.\nI won\u2019t endorse anything cryptocurrency related, but it\u2019s clear that Sam\nAltman\u2019s Worldcoin saw this eventuality coming many years ago.\n\n## Conclusion\n\n\u201cSmall data, big compute\u201d allowed us to optimize for a certain class of\ndataset and take certain shortcuts. The Lilac team will be joining Databricks\nand I look forward to continuing to build systems tailored to the unusual\nneeds of LLMs!\n\nWant to become a better programmer? Join the Recurse Center!\n\n", "frontpage": false}
