{"aid": "40291673", "title": "Computer Vision and Apache Pinot Vector Index", "url": "https://hubertdulay.substack.com/p/computer-vision-apache-pinot-vector", "domain": "hubertdulay.substack.com", "votes": 1, "user": "PeterCorless", "posted_at": "2024-05-07 21:24:20", "comments": 0, "source_title": "Computer Vision + Apache Pinot Vector Index", "source_text": "Computer Vision + Apache Pinot Vector Index\n\n# SUP! Hubert\u2019s Substack\n\nShare this post\n\n#### Computer Vision + Apache Pinot Vector Index\n\nhubertdulay.substack.com\n\n#### Discover more from SUP! Hubert\u2019s Substack\n\n\"Streaming Data Mesh\" OReilly. Currently writing his second book \"Streaming\nDatabases - Supporting Monolithic Data Engineers https://linktr.ee/hkdulay\n\nContinue reading\n\nSign in\n\n# Computer Vision + Apache Pinot Vector Index\n\n### Booth Duty Demo at RTA Summit\n\nHubert Dulay\n\nMay 03, 2024\n\n1\n\nShare this post\n\n#### Computer Vision + Apache Pinot Vector Index\n\nhubertdulay.substack.com\n\nShare\n\nAt RTA Summit 2024, we will have a booth demonstration showcasing Apache\nPinot\u2019s ability to serve analytics and similarity search in real-time. There\nis actually a lot to make this successful. Let me unpack this by describing\nthe use case.\n\n#\n\nBooth Duty Demo\n\nAt conferences, you should always have someone at your booth to answer\nquestions, hand out swag, and scan attendees. This tends to require multiple\npeople. This demo will monitor your booth using your camera to identify people\nassigned to booth duty and measure their activity throughout the day.\n\nBelow is a screenshot of what the \u201cBooth Duty\u201d demo can look like.\n\nThe timeline chart at the top of the dashboard plots the occurrences of those\nassigned booth duty. The middle table is the same dataset but displayed as a\ntable. The bottom of the dashboard is a GenAI response using information\nobtained from the frames. It provides a summary of what has been happening at\nthe booth for the last 15 minutes while providing the frames that captured\nthis information.\n\nSo how are we doing this? I can break this down into these features.\n\n  * Video frame capturing using computer vision.\n\n  * Creating image embeddings from video frames.\n\n  * Image captioning to get an image description.\n\n  * Similarity search of booth duty assignees.\n\n  * Real-Time RAG (Retrieval-Augmented Generation).\n\n#\n\nComputer Vision\n\nTo capture a video feed from Python, you can use a computer vision module\ncalled OpenCV.\n\n    \n    \n    pip install opencv-python\n    \n    \n    import cv2 video = cv2.VideoCapture(0) while True: success, frame = video.read()\n\nTo convert the frame to an embedding, you can use the PIL module to read the\nframe and pass it to a sentence transformer using the clip-ViT-B-32 image\nembedding model.\n\n    \n    \n    from PIL import Image from sentence_transformers import SentenceTransformer iframe = Image.fromarray(frame) model = SentenceTransformer('clip-ViT-B-32') img_emb = model.encode(iframe).tolist()\n\nThe clip-ViT-B-32 model is multimodal in that it can create embeddings for\ntext and images, putting them in the same vector space. This means you can ask\nthe question, \u201cFind me images of a crowd of people,\u201d and the model can convert\nthis question to an embedding and search for image embeddings.\n\nThis model will help us search for images but will not tell us what is\nhappening in the image.\n\n#\n\nImage Captioning\n\nTo get a simple caption for the frames being captured we can use the\nSalesforce/blip-image-captioning-base model.\n\n    \n    \n    from transformers import pipeline captioner = pipeline(\"image-to-text\",model=\"Salesforce/blip-image-captioning-base\") caption = captioner(iframe)[0]['generated_text']\n\nThis model will give you a short description of what is happening in an image.\nIt does not know our booth duty assignees, so it will not help you count the\nnumber of instances a booth assignee appears in the video.\n\n#\n\nSimilarity Search in Pinot\n\nFirst, we need to pre-load Apache Pinot with image embeddings of the booth\nduty assignees. When frame images are captured, we create embeddings of the\nframe image and compare them to the embeddings of our assignees.\n\n    \n    \n    with DIST as ( SELECT name, cosine_distance(person_embedding, ARRAY{frame_embedding}) AS distance from people where VECTOR_SIMILARITY(person_embedding, ARRAY{frame_embedding}, 10) ) select * from DIST where distance < {threshold} order by distance asc\n\nIn the SQL above, Pinot leverages the vector index in the predicate with\nVECTOR_SIMILARITY which speeds up the vector search. We further filter the\nresults by providing a threshold, making sure the distance between the two\nembeddings is within a limit. The threshold is how we can tune the search\nresults. The smaller the threshold, the shorter the distance, and therefore,\nthe search is tuned more precisely, which can result in no assignees being\nfound. Conversely, the higher the threshold, the more lenient the search is,\nwhich may result in false positives. In this case, our threshold is set to .3.\n\nWe can be more precise with our search by picking the faces out of the frame\nand comparing facial features. We chose to simplify by only comparing the\nentire image, but the results are still good.\n\n#\n\nFrame Rate\n\nVideos create an average of about 24 frames per second, which is about 691,200\nframes for 8 hours of booth duty. Monitoring two booths at the same time\ncreates 1,382,400 frames for 8 hours. Monitoring two booths for 16 hours (two\ndays) creates 2,764,800 frames.\n\nIf we capture one frame every 5 seconds, we can reduce this to 23,040 frames\nto support 2 booths for 16 hours.\n\nFor every frame we capture, we perform a similarity search in Pinot against\nthe booth duty assignees table. If we find a person we recognize, we add it to\nthe message below including the frame number, the image caption, and\ntimestamp. If a person is not found, we still send it over to Pinot to capture\ninstances where the booth is empty.\n\n    \n    \n    { \"frame\": frame_number, \"person\": person, \"description\": captioner(iframe)[0]['generated_text'], \"embedding\":img_emb, \"ts\": ts }\n\n#\n\nReal-Time GenAI\n\nFor the dashboard, we are refreshing the dashboard every 5 seconds. This sends\ntwo queries to Pinot to populate the timeline chart and to pull video\ndescriptions for the past 15 minutes.\n\n    \n    \n    SELECT frame, person, description from video where ts > ago('PT15M') order by frame desc limit 50\n\nThen we assemble the prompt with the person and description for every frame\nentry.\n\n    \n    \n    frame: [100] - person [hubert]: a man on his phone wearing glasses frame: [200] - person [hubert]: a man wearing a hoddie\n\nWe add this to the context of the prompt below and answer a question:\n\n> Summarize what has been happening at the booths in two sentences\n    \n    \n    PROMPT_TEMPLATE = \"\"\" Below are video logs for the last 15 minutes. They contain descriptions of video frames and the name of a person that was found in the frame if one was identified. No logs indicates the video stream has just started. Answer the question based on this log: ---- {context} ---- Based on the above video frame descriptions, answer this question: {question} \"\"\"\n\nThe LLM will return an answer similar to the one below:\n\n> Based on the video frame descriptions, it appears that a man named Hubert\n> has been consistently present at the booths. He is described as wearing\n> glasses and a brown hoodie in most frames, occasionally standing in a room\n> or in front of a ceiling with lights. No other individuals have been\n> identified in the frames.\n\n#\n\nAI and Real-Time Analytics Together\n\nWith Pinot supporting real-time analytics with vector search provides a more\ncompelling real-time view of the business. Real-time analytics combined with\nreal-time GenAI can enable hyper-personalized experiences for users. By\nanalyzing real-time data streams and generating personalized content or\nrecommendations in real-time, businesses can tailor their offerings to\nindividual preferences and behaviors.\n\nLearn more about this demo by trying it out here.\n\nSUP! Hubert\u2019s Substack is a reader-supported publication. To receive new posts\nand support my work, consider becoming a free or paid subscriber.\n\n1 Like\n\n1\n\nShare this post\n\n#### Computer Vision + Apache Pinot Vector Index\n\nhubertdulay.substack.com\n\nShare\n\nComments\n\nStream Processing vs Real-time OLAP vs Streaming Database\n\nHow to use them and where they overlap\n\nFeb 19, 2023 \u2022\n\nHubert Dulay\n\n19\n\nShare this post\n\n#### Stream Processing vs Real-time OLAP vs Streaming Database\n\nhubertdulay.substack.com\n\nOne Big Table (OBT) vs Star Schema\n\nWhere do you execute the JOIN in real-time analytics?\n\nDec 20, 2023 \u2022\n\nHubert Dulay\n\n18\n\nShare this post\n\n#### One Big Table (OBT) vs Star Schema\n\nhubertdulay.substack.com\n\nReal-Time Streaming Ecosystem Part 1\n\nSetting up the use case\n\nApr 27, 2023 \u2022\n\nHubert Dulay\n\n12\n\nShare this post\n\n#### Real-Time Streaming Ecosystem Part 1\n\nhubertdulay.substack.com\n\nReady for more?\n\n\u00a9 2024 Hubert Dulay\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
