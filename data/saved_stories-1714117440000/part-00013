{"aid": "40163954", "title": "Mistral.rs: Fast LLM Engine", "url": "https://github.com/EricLBuehler/mistral.rs", "domain": "github.com/ericlbuehler", "votes": 4, "user": "atrudeau", "posted_at": "2024-04-25 22:38:49", "comments": 0, "source_title": "GitHub - EricLBuehler/mistral.rs: Blazingly fast LLM inference.", "source_text": "GitHub - EricLBuehler/mistral.rs: Blazingly fast LLM inference.\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nEricLBuehler / mistral.rs Public\n\n  * Notifications\n  * Fork 8\n  * Star 113\n\nBlazingly fast LLM inference.\n\n### License\n\nMIT license\n\n113 stars 8 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# EricLBuehler/mistral.rs\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n2 Branches\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nEricLBuehlerUpdate readmeApr 25, 2024b3d130d \u00b7 Apr 25, 2024Apr 25, 2024\n\n## History\n\n1,623 Commits  \n  \n### .cargo\n\n|\n\n### .cargo\n\n| Update the config.toml| Apr 15, 2024  \n  \n### .github/workflows\n\n|\n\n### .github/workflows\n\n| Build python docs in actions| Apr 24, 2024  \n  \n### chat_templates\n\n|\n\n### chat_templates\n\n| Remove eos tok from chat template specify and use template eos tok| Mar 9,\n2024  \n  \n### docs\n\n|\n\n### docs\n\n| Improve docs and update llamaindex docstring| Apr 24, 2024  \n  \n### examples\n\n|\n\n### examples\n\n| Add cookbook| Apr 25, 2024  \n  \n### integrations\n\n|\n\n### integrations\n\n| Add the system pyo3 role| Apr 24, 2024  \n  \n### mistralrs-bench\n\n|\n\n### mistralrs-bench\n\n| Add docs for mistralrs-bench| Apr 25, 2024  \n  \n### mistralrs-core\n\n|\n\n### mistralrs-core\n\n| Support no bos| Apr 25, 2024  \n  \n### mistralrs-lora\n\n|\n\n### mistralrs-lora\n\n| Merge lora for all models| Apr 25, 2024  \n  \n### mistralrs-pyo3\n\n|\n\n### mistralrs-pyo3\n\n| Merge branch 'master' into device_mapping| Apr 25, 2024  \n  \n### mistralrs-server\n\n|\n\n### mistralrs-server\n\n| Cast the final layer| Apr 25, 2024  \n  \n### mistralrs\n\n|\n\n### mistralrs\n\n| Merge branch 'master' into device_mapping| Apr 25, 2024  \n  \n### orderings\n\n|\n\n### orderings\n\n| Update all the examples| Apr 20, 2024  \n  \n### scripts\n\n|\n\n### scripts\n\n| Xlora sanity check| Apr 20, 2024  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| Use kvconcat kernel| Mar 14, 2024  \n  \n### .typos.toml\n\n|\n\n### .typos.toml\n\n| Add simple docs| Apr 23, 2024  \n  \n### Cargo.toml\n\n|\n\n### Cargo.toml\n\n| Add a features| Apr 21, 2024  \n  \n### Dockerfile\n\n|\n\n### Dockerfile\n\n| Set compute cap in dockerfile| Apr 2, 2024  \n  \n### Dockerfile-cuda-all\n\n|\n\n### Dockerfile-cuda-all\n\n| Exclude flash-attention for now| Apr 2, 2024  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Update| Feb 26, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update readme| Apr 25, 2024  \n  \n## Repository files navigation\n\n# mistral.rs\n\n### Blazingly fast LLM inference.\n\n| Rust Documentation | Python Documentation | Discord |\n\nMistral.rs is a fast LLM inference platform supporting inference on a variety\nof devices, quantization, and easy-to-use application with an Open-AI API\ncompatible HTTP server and Python bindings.\n\n## Upcoming features\n\n  * More models: please submit requests here.\n  * X-LoRA: Scalings topk and softmax topk (#48).\n  * Parallel linear layers (sharding) (#50).\n  * In situ quantization: download safetensor models and quantize them in place\n  * Speculative decoding: https://arxiv.org/pdf/2211.17192\n\nRunning the new Llama 3 model\n\ncargo run --release --features ... -- -i plain -m meta-llama/Meta-\nLlama-3-8B-Instruct -a llama\n\nRunning the new Phi 3 model with 128K context window\n\ncargo run --release --features ... -- -i plain -m\nmicrosoft/Phi-3-mini-128k-instruct -a phi3\n\n## Description\n\nFast:\n\n  * Quantized model support: 2-bit, 3-bit, 4-bit, 5-bit, 6-bit and 8-bit for faster inference and optimized memory usage.\n  * Continuous batching.\n  * Prefix caching.\n  * Device mapping: load and run some layers on the device and the reset on the CPU.\n\nAccelerator support:\n\n  * Apple silicon support with the Metal framework.\n  * CPU inference with mkl, accelerate support and optimized backend.\n  * CUDA support with flash attention and cuDNN.\n\nEasy:\n\n  * Lightweight OpenAI API compatible HTTP server.\n  * Python API.\n  * Grammar support with Regex and Yacc.\n\nPowerful:\n\n  * Fast LoRA support with weight merging.\n  * First X-LoRA inference platform with first class support.\n\nThis is a demo of interactive mode with streaming running Mistral GGUF:\n\nSupported models:\n\n  * Mistral 7B (v0.1 and v0.2)\n  * Gemma\n  * Llama, including Llama 3\n  * Mixtral 8x7B\n  * Phi 2\n  * Phi 3\n  * Qwen 2\n\nPlease see this section for details on quantization and LoRA support.\n\n## APIs and Integrations\n\nRust Library API\n\nRust multithreaded API for easy integration into any application.\n\n  * Docs\n  * Examples\n  * To install: Add mistralrs = { git = \"https://github.com/EricLBuehler/mistral.rs.git\" }\n\nPython API\n\nPython API for mistral.rs.\n\n  * Installation\n  * Docs\n  * Example\n  * Cookbook\n\n    \n    \n    from mistralrs import Runner, Which, ChatCompletionRequest, Message, Role runner = Runner( which=Which.GGUF( tok_model_id=\"mistralai/Mistral-7B-Instruct-v0.1\", quantized_model_id=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", quantized_filename=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", tokenizer_json=None, repeat_last_n=64, ) ) res = runner.send_chat_completion_request( ChatCompletionRequest( model=\"mistral\", messages=[Message(Role.User, \"Tell me a story about the Rust type system.\")], max_tokens=256, presence_penalty=1.0, top_p=0.1, temperature=0.1, ) ) print(res.choices[0].message.content) print(res.usage)\n\nHTTP Server\n\nOpenAI API compatible API server\n\n  * API Docs.\n  * Running\n  * Example\n\nLlama Index integration\n\n  * Source.\n  * Example\n  * Cookbook\n\n## Supported accelerators\n\n  * CUDA:\n\n    * Enable with cuda feature: --features cuda\n    * Flash attention support with flash-attn feature, only applicable to non-quantized models: --features flash-attn\n    * cuDNNsupport with cudnn feature: --features cudnn\n  * Metal:\n\n    * Enable with metal feature: --features metal\n  * CPU:\n\n    * Intel MKL with mkl feature: --features mkl\n    * Apple Accelerate with accelerate feature: --features accelerate\n\nEnabling features is done by passing --features ... to the build system. When\nusing cargo run or maturin develop, pass the --features flag before the --\nseparating build flags from runtime flags.\n\n  * To enable a single feature like metal: cargo build --release --features metal.\n  * To enable multiple features, specify them in quotes: cargo build --release --features \"cuda flash-attn cudnn\".\n\n## Benchmarks\n\nDevice| Mistral.rs Completion T/s| Llama.cpp Completion T/s| Model| Quant  \n---|---|---|---|---  \nA10 GPU, CUDA| 78| 78| mistral-7b| 4_K_M  \nIntel Xeon 8358 CPU, AVX| 6| 19| mistral-7b| 4_K_M  \nRaspberry Pi 5 (8GB), Neon| 2| segfault| mistral-7b| 2_K  \nA100 GPU, CUDA| 110| 119| mistral-7b| 4_K_M  \n  \nPlease submit more benchmarks via raising an issue!\n\n## Usage\n\n### Installation and Build\n\nTo install mistral.rs, one should ensure they have Rust installed by following\nthis link. Additionally, the Huggingface token should be provided in\n~/.cache/huggingface/token when using the server to enable automatic download\nof gated models.\n\n  1. Install required packages\n\n     * openssl (ex., sudo apt install libssl-dev)\n     * pkg-config (ex., sudo apt install pkg-config)\n  2. Install Rust: https://rustup.rs/\n    \n        curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh source $HOME/.cargo/env\n\n  3. Set HF token correctly (skip if already set or your model is not gated, or if you want to use the token_source parameters in Python or the command line.)\n    \n        mkdir ~/.cache/huggingface touch ~/.cache/huggingface/token echo <HF_TOKEN_HERE> > ~/.cache/huggingface/token\n\n  4. Download the code\n    \n        git clone https://github.com/EricLBuehler/mistral.rs.git cd mistral.rs\n\n  5. Build or install\n\n     * Base build command\n        \n                cargo build --release\n\n     * Build with CUDA support\n        \n                cargo build --release --features cuda\n\n     * Build with CUDA and Flash Attention V2 support\n        \n                cargo build --release --features \"cuda flash-attn\"\n\n     * Build with Metal support\n        \n                cargo build --release --features metal\n\n     * Build with Accelerate support\n        \n                cargo build --release --features accelerate\n\n     * Build with MKL support\n        \n                cargo build --release --features mkl\n\n     * Install with cargo install for easy command line usage\n\nPass the same values to --features as you would for cargo build\n\n        \n                cargo install --path mistralrs-server --features cuda\n\n  6. The build process will output a binary misralrs-server at ./target/release/mistralrs-server which may be copied into the working directory with the following command:\n    \n        cp ./target/release/mistralrs-server .\n\n  7. Installing Python support\n\nYou can install Python support by following the guide here.\n\n### Getting models\n\nMistral.rs will automatically download models from HF hub. To access gated\nmodels, you should provide a token source. They may be one of:\n\n  * literal:<value>: Load from a specified literal\n  * env:<value>: Load from a specified environment variable\n  * path:<value>: Load from a specified file\n  * cache: default: Load from the HF token at ~/.cache/huggingface/token or equivalent.\n  * none: Use no HF token\n\nThis is passed in the following ways:\n\n  * Command line:\n\n    \n    \n    ./mistralrs-server --port 1234 gguf -m mistralai/Mistral-7B-Instruct-v0.1\n\n  * Python:\n\nExample here.\n\nLoading locally will be added shortly.\n\n### Run\n\nTo start a server serving Mistral on localhost:1234,\n\n    \n    \n    ./mistralrs-server --port 1234 --log output.log plain -m TheBloke/Mistral-7B-Instruct-v0.1-GGUF\n\nMistral.rs uses subcommands to control the model type. They are generally of\nformat <XLORA/LORA>-<ARCHITECTURE/QUANTIZATION>. Please run ./mistralrs-server\n--help to see the subcommands.\n\nInteractive mode:\n\nYou can launch interactive mode, a simple chat application running in the\nterminal, by passing -i:\n\n    \n    \n    ./mistralrs-server -i gguf -t mistralai/Mistral-7B-Instruct-v0.1 -m TheBloke/Mistral-7B-Instruct-v0.1-GGUF -f mistral-7b-instruct-v0.1.Q4_K_M.gguf\n\n### Quick examples:\n\n  * X-LoRA with no quantization\n\nTo start an X-LoRA server with the exactly as presented in the paper:\n\n    \n    \n    ./mistralrs-server --port 1234 x-lora-plain -o orderings/xlora-paper-ordering.json -x lamm-mit/x-lora\n\n  * LoRA with a model from GGUF\n\nTo start an LoRA server with adapters from the X-LoRA paper (you should modify\nthe ordering file to use only one adapter, as the adapter static scalings are\nall 1 and so the signal will become distorted):\n\n    \n    \n    ./mistralrs-server --port 1234 lora-gguf -o orderings/xlora-paper-ordering.json -m TheBloke/zephyr-7B-beta-GGUF -f zephyr-7b-beta.Q8_0.gguf -x lamm-mit/x-lora\n\nNormally with a LoRA model you would use a custom ordering file. However, for\nthis example we use the ordering from the X-LoRA paper because we are using\nthe adapters from the X-LoRA paper.\n\n  * With a model from GGUF\n\nTo start a server running Mistral from GGUF:\n\n    \n    \n    ./mistralrs-server --port 1234 gguf -t mistralai/Mistral-7B-Instruct-v0.1 -m TheBloke/Mistral-7B-Instruct-v0.1-GGUF -f mistral-7b-instruct-v0.1.Q4_K_M.gguf\n\n  * With a model from GGML\n\nTo start a server running Llama from GGML:\n\n    \n    \n    ./mistralrs-server --port 1234 ggml -t meta-llama/Llama-2-13b-chat-hf -m TheBloke/Llama-2-13B-chat-GGML -f llama-2-13b-chat.ggmlv3.q4_K_M.bin\n\n  * Plain model from safetensors\n\nTo start a server running Mistral from safetensors.\n\n    \n    \n    ./mistralrs-server --port 1234 gguf -m mistralai/Mistral-7B-Instruct-v0.1\n\nCommand line docs\n\nCommand line docs here\n\n## Supported models\n\nQuantization support\n\nModel| GGUF| GGML  \n---|---|---  \nMistral 7B| \u2705  \nGemma  \nLlama| \u2705| \u2705  \nMixtral 8x7B| \u2705  \nPhi 2| \u2705  \nPhi 3| \u2705  \nQwen 2  \n  \nDevice mapping support\n\nModel| Supported  \n---|---  \nNormal| \u2705  \nGGUF| \u2705  \nGGML  \n  \nX-LoRA and LoRA support\n\nModel| X-LoRA| X-LoRA+GGUF| X-LoRA+GGML  \n---|---|---|---  \nMistral 7B| \u2705| \u2705  \nGemma| \u2705  \nLlama| \u2705| \u2705| \u2705  \nMixtral 8x7B| \u2705| \u2705  \nPhi 2| \u2705  \nPhi 3| \u2705| \u2705  \nQwen 2  \n  \nUsing derivative models\n\nTo use a derivative model, select the model architecture using the correct\nsubcommand. To see what can be passed for the architecture, pass --help after\nthe subcommand. For example, when using a different model than the default,\nspecify the following for the following types of models:\n\n  * Normal: Model id\n  * Quantized: Quantized model id, quantized filename, and tokenizer id\n  * X-LoRA: Model id, X-LoRA ordering\n  * X-LoRA quantized: Quantized model id, quantized filename, tokenizer id, and X-LoRA ordering\n  * LoRA: Model id, LoRA ordering\n  * LoRA quantized: Quantized model id, quantized filename, tokenizer id, and LoRA ordering\n\nSee this section to determine if it is necessary to prepare an X-LoRA/LoRA\nordering file, it is always necessary if the target modules or architecture\nchanged, or if the adapter order changed.\n\nIt is also important to check the chat template style of the model. If the HF\nhub repo has a tokenizer_config.json file, it is not necessary to specify.\nOtherwise, templates can be found in chat_templates and should be passed\nbefore the subcommand. If the model is not instruction tuned, no chat template\nwill be found and the APIs will only accept a prompt, no messages.\n\nFor example, when using a Zephyr model:\n\n./mistralrs-server --port 1234 --log output.txt gguf -t\nHuggingFaceH4/zephyr-7b-beta -m TheBloke/zephyr-7B-beta-GGUF -f\nzephyr-7b-beta.Q5_0.gguf\n\n### Adapter model support: X-LoRA and LoRA\n\nAn adapter model is a model with X-LoRA or LoRA. X-LoRA support is provided by\nselecting the x-lora-* architecture, and LoRA support by selecting the lora-*\narchitecture. Please find docs for adapter models here\n\n### Chat Templates and Tokenizer\n\nMistral.rs will attempt to automatically load a chat template and tokenizer.\nThis enables high flexibility across models and ensures accurate and flexible\nchat templating. However, this behavior can be customized. Please find\ndetailed documentation here.\n\n## Contributing\n\nIf you have any problems or want to contribute something, please raise an\nissue or pull request!\n\nConsider enabling RUST_LOG=debug environment variable.\n\nIf you want to add a new model, please see our guide.\n\n## Credits\n\nThis project would not be possible without the excellent work at candle.\nAdditionally, thank you to all contributors! Contributing can range from\nraising an issue or suggesting a feature to adding some new functionality.\n\n## About\n\nBlazingly fast LLM inference.\n\n### Topics\n\nrust llm\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\nActivity\n\n### Stars\n\n113 stars\n\n### Watchers\n\n6 watching\n\n### Forks\n\n8 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 1\n\n  * mistral.rs\n\n## Contributors 4\n\n  * EricLBuehler Eric Buehler\n  * lucasavila00 Lucas de \u00c1vila Martins\n  * LLukas22 Lukas Kreussel\n  * hugoabonizio Hugo Abonizio\n\n## Languages\n\n  * Rust 97.0%\n  * Python 2.9%\n  * Dockerfile 0.1%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
