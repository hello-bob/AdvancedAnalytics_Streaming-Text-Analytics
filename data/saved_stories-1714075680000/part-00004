{"aid": "40155926", "title": "Desperately Trying to Fathom the Coffeepocalypse Argument", "url": "https://www.astralcodexten.com/p/desperately-trying-to-fathom-the", "domain": "astralcodexten.com", "votes": 5, "user": "feross", "posted_at": "2024-04-25 11:02:02", "comments": 0, "source_title": "Desperately Trying To Fathom The Coffeepocalypse Argument", "source_text": "Desperately Trying To Fathom The Coffeepocalypse Argument\n\n# Astral Codex Ten\n\nShare this post\n\n#### Desperately Trying To Fathom The Coffeepocalypse Argument\n\nwww.astralcodexten.com\n\n# Desperately Trying To Fathom The Coffeepocalypse Argument\n\n### ...\n\nApr 25, 2024\n\n28\n\nShare this post\n\n#### Desperately Trying To Fathom The Coffeepocalypse Argument\n\nwww.astralcodexten.com\n\n50\n\nShare\n\nOne of the most common arguments against AI safety is:\n\n> Here\u2019s an example of a time someone was worried about something, but it\n> didn\u2019t happen. Therefore, AI, which you are worried about, also won\u2019t\n> happen.\n\nI always give the obvious answer: \u201cOkay, but there are other examples of times\nsomeone was worried about something, and it did happen, right? How do we know\nAI isn\u2019t more like those?\u201d The people I\u2019m arguing with always seem so\nsurprised by this response, as if I\u2019m committing some sort of betrayal by\ndestroying their beautiful argument.\n\nThe first hundred times this happened, I thought I must be misunderstanding\nsomething. Surely \u201cI can think of one thing that didn\u2019t happen, therefore\nnothing happens\u201d is such a dramatic logical fallacy that no human is dumb\nenough to fall for it. But people keep bringing it up, again and again. Very\nsmart people, people who I otherwise respect, make this argument and genuinely\nexpect it to convince people!\n\nUsually the thing that didn\u2019t happen is overpopulation, global cooling, etc.\nBut most recently it was some kind of coffeepocalypse:\n\nYou can read the full thread here, but I\u2019m warning you, it\u2019s just going to be\n\u201conce people were worried about coffee, but now we know coffee is safe.\nTherefore AI will also be safe.\u201d1\n\nI keep trying to steelman this argument, and it keeps resisting my\nsteelmanning. For example:\n\n  * Maybe the argument is a failed attempt to gesture at a principle of \u201cmost technologies don\u2019t go wrong\u201d? But people make the same argument with things that aren\u2019t technologies, like global cooling or overpopulation.\n\n  * Maybe the argument is a failed attempt to gesture at a principle of \u201cthe world is never destroyed, so doomsday prophecies have an abysmal track record\u201d? But overpopulation and global cooling don\u2019t claim that everyone will die - just that a lot of people will. And plenty of prophecies about mass death events have come true (eg Black Plague, WWII, AIDS). And none of this explains coffee!\n\nSo my literal, non-rhetorical question, is \u201chow can anyone be stupid enough to\nthink this makes sense?\u201d I\u2019m not (just) trying to insult the people who say\nthis; I consider their existence a genuine philosophical mystery. Isn\u2019t this,\nin some sense, no different from saying (for example):\n\n> I once heard about a dumb person who thought halibut weren\u2019t a kind of fish\n> - but boy, that person sure was wrong. Therefore, AI is also a kind of fish.\n\nThe coffee version is:\n\n> I once heard about a dumb person who thought coffee would cause lots of\n> problems - but boy, that person sure was wrong. Therefore, AI also won\u2019t\n> cause lots of problems.\n\nNobody would ever take it seriously in its halibut form. So what part of\nreskinning it as about coffee makes it more credible?\n\nWhenever I wonder how anyone can be so stupid, I start by asking if I myself\nam exactly this stupid in some other situation. This time, I remembered an\nargument from one of Stuart Russell\u2019s pro-AI-risk arguments. He pointed out\nthat physicist Ernest Rutherford declared nuclear chain reactions impossible\nless than twenty-four hours before Fermi successfully achieved a nuclear chain\nreaction. At the time, I thought this was a cute and helpful warning against\nbeing too sure that superintelligence was impossible.\n\nBut isn\u2019t this the same argument as the coffeepocalypse? A hostile rephrasing\nmight be:\n\n> There is at least one thing that was possible. Therefore, superintelligent\n> AI is also possible.\n\nAnd an only slightly less hostile rephrasing:\n\n> People were wrong when they said nuclear reactions were impossible.\n> Therefore, they might also be wrong when they say superintelligent AI is\n> possible.\n\nHow is this better than the coffeepocalypse argument? In fact, how is it even\nbetter than the halibut argument? What are we doing when we make arguments\nlike these?\n\nSome thoughts:\n\nAs An Existence Proof?\n\nWhen I think of why I appreciated Prof. Russell\u2019s argument, it wasn\u2019t because\nit was a complete proof that superintelligence was possible. It was more like\nan argument for humility. \u201cYou may think it\u2019s impossible. But given that\nthere\u2019s at least one case where people thought that and were proven wrong, you\nshould believe it\u2019s at least possible.\u201d\n\nBut first of all, one case shouldn\u2019t prove anything. If you doubt you will win\nthe lottery, I can\u2019t prove you wrong - even in a weak, probabilistic way - by\nbringing up a case of someone who did. I can\u2019t even prove you should be humble\n- you are definitely allowed to be arrogant and very confident in your belief\nyou won\u2019t win!\n\nAnd second of all, existence proofs can only make you slightly more humble.\nThey can refute the claim \u201cI am absolutely, 100% certain that AI is/isn\u2019t\ndangerous\u201d. But not many people make this claim, and it\u2019s uncharitable to\nsuspect your opponent of doing so.\n\nMaybe this debate collapses into the debate around the Safe Uncertainty\nFallacy, where some people think if there\u2019s any uncertainty at all about\nsomething, you have to assume it will be totally safe and fine (no, I don\u2019t\nget it either), and other people think if there\u2019s even a 1% chance of\ndisaster, you have to multiply out by the size of the disaster and end up very\nconcerned (at the tails, this becomes Pascalian reasoning, but nobody has a\ngood theory of where the tails begin).\n\nI still don\u2019t think an existence proof that it\u2019s theoretically possible for\nyour opponent to be wrong goes very far. Still, this is sort of what I was\ntrying to do with the diphyllic dam example here - show that a line of\nargument can sometimes be wrong, in a way that forces people to try something\nmore sophisticated.\n\nAs An Attempt To Trigger A Heuristic?\n\nMaybe Prof. Russell\u2019s argument implicitly assumes that everyone has a large\nstore of knowledge about failed predictions - no heavier-than-air flying\nmachine is possible, there is a world market for maybe five computers. You\ncould think of this particular example of a prediction being false as trying\nto trigger people\u2019s existing stock of memories that very often people\u2019s\npredictions are false.\n\nYou could make the same argument about the coffeepocalypse. \u201cPeople worried\nabout coffee but it was fine\u201d is intended to activate a long list of stored\nmoral panics in your mind - the one around marijuana, the one around violent\nvideo games - enough to remind you that very often people worry about\nsomething and it\u2019s nothing.\n\nBut - even granting that there are many cases of both - are these useful?\nThere are many cases of moral panics turning out to be nothing. But there are\nmany other cases of moral panics proving true, or of people not worrying about\nthings they should worry about. People didn\u2019t worry enough about tobacco, and\nthen it killed lots of people. People didn\u2019t worry enough about lead in\ngasoline, and then it poisoned lots of children. People didn\u2019t worry enough\nabout global warming, OxyContin, al-Qaeda, growing international tension in\nthe pre-WWI European system, etc, until after those things had already gotten\nout of control and hurt lots of people. We even have words and idioms for this\nkind of failure to listen to warnings - like the ostrich burying its head in\nthe sand.\n\n(and there are many examples of people predicting that things were impossible,\nand they really were impossible, eg perpetual motion).\n\nIt would seem like in order to usefully invoke a heuristic (\u201cremember all\nthese cases of moral panic we all agree were bad? Then you should assume this\nis probably also a moral panic\u201d), you need to establish that moral panics are\nmore common than ostrich-head-burying. And in order to usefully invoke a\nheuristic against predicting something is impossible, you need to establish\nthat failed impossibility proofs are more common than accurate ones.\n\nThis seems somewhere between \u201cnobody has done it\u201d and \u201cimpossible in\nprinciple\u201d. Insisting on it would eliminate 90%+ of discourse.\n\nSee also Caution On Bias Arguments, where I try to make the same point. I\nthink you can rewrite this section to be about proposed bias arguments\n(\u201cPeople have a known bias to worry about things excessively, so we should\ncorrect for it\u201d). But as always, you can posit an opposite bias (\u201cPeople have\na known bias to put their heads in the sand and ignore problems that it would\nbe scary to think about or expensive to fix\u201d), and figuring out which of these\ndueling biases you need to correct for, is the same problem as figuring out\nwhich of the dueling heuristics you need to invoke.\n\nWhat Is Evidence, Anyway?\n\nSuppose someone\u2019s trying to argue for some specific point, like \u201cRussia will\nwin the war with Ukraine\u201d. They bring up some evidence, like \u201cRussia has some\nvery good tanks.\u201d\n\nObviously this on its own proves nothing. Russia could have good tanks, but\nUkraine could be better at other things.\n\nBut then how does any amount of evidence prove an argument? You could make a\nhundred similar statements: \u201cRussia has good tanks\u201d, \u201cRussia has good troop\ntransport ships\u201d, \u201cthe Russian general in the 4th District of the Western\nTheater is very skilled\u201d [...], and run into exactly the same problem. But an\nargument that Russia will win the war has to be made up of some number of\npieces of evidence. So how can it ever work?\n\nI think it has to carry an implicit assumption of \u201c...and you\u2019re pretty good\nat weighing how much evidence it would take to prove something, and everything\nelse is pretty equal, so this is enough evidence to push you over the edge\ninto believing my point.\u201d\n\nFor example, if someone said \u201cRussia will win because they outnumber Ukraine 3\nto 1 and have better generals\u201d (and then proved this was true), that at least\nseems like a plausible argument that shouldn\u2019t be immediately ignored.\nEveryone knows that having a 3:1 advantage, and having good generals, are both\nbig advantages in war. It carries an implied \u201cand surely Ukraine doesn\u2019t have\nsome other advantage that counterbalances both of those\u201d. But this could be so\nplausible that we accept it (it\u2019s hard to counterbalance a 3:1 manpower\nadvantage). Or it could be a challenge to pro-Ukraine people (if you can\u2019t\nname some advantage of your side that sounds as convincing as these, then we\nwin).\n\nAnd it\u2019s legitimate for someone who believes Russia will win, and has talked\nabout it at length, to write one article about the good tanks, without\nexplicitly saying \u201cObviously this is only one part of my case that Russia will\nwin, and won\u2019t convince anyone on its own; still, please update a little on\nthis one, and maybe as you keep going and run into other things, you\u2019ll update\nmore.\u201d\n\nIs this what the people talking about coffee are doing?\n\nAn argument against: you should at least update a little on the good tanks,\nright? But the coffee thing proves literally nothing. It proves that there was\none time when people worried about a bad thing, and then it didn\u2019t happen.\nSurely you already knew this must have happened at least once!\n\nAn argument in favor: suppose there are a hundred different facets of war as\nimportant as \u201chas good tanks\u201d. It would be very implausible if, of two\nrelatively evenly-matched competitors, one of them was better at all 100, and\nthe other at 0. So all that \u201cRussia has good tanks\u201d is telling you is that\nRussia is better on at least one axis, which you could have already predicted.\nIs this more of an update than the coffee situation?\n\nMy proposed answer: if you knew the person making the argument was\ndeliberately looking for pro-Russia arguments, then \u201chas good tanks\u201d updates\nyou almost zero - it would only convince you that Russia was better in at\nleast 1 of 100 domains. If you thought they were relatively unbiased and just\nhappened to stumble across this information, it would update you slightly (we\nhave chosen a randomly selected facet, and Russia is better).\n\nIf you thought the person making the coffee argument was doing an unbiased\nsurvey of all times people had been worried, then the coffee fact (in this\nparticular time people worried, it was unnecessary) might feel like sampling a\nrandom point. But we have so much more evidence about whether things are\ndangerous or safe that I don\u2019t think sampling a random point (even if we could\ndo so fairly) would mean much.\n\nConclusion: I Genuinely Don\u2019t Know What These People Are Thinking\n\nI would like to understand the mindset of people who make arguments like this,\nbut I\u2019m not sure I\u2019ve succeeded. The best I can say is that sometimes people\non my side make similar arguments (the nuclear chain reaction one) which I\ndon\u2019t immediately flag as dumb, and maybe I can follow this thread to figure\nout why they seem tempting sometimes.\n\nIf you see me making an argument that you think is like coffeepocalypse,\nplease let me know, so I can think about what factors led me to think it was a\nreasonable thing to do, and see if they also apply to the coffee case.\n\n. . . although I have to admit, I\u2019m a little nervous asking for this, though.\nDouglas Adams once said that if anyone ever understood the Universe, it would\nimmediately disappear and be replaced by something even more incomprehensible.\nI worry that if I ever understand why anti-AI-safety people think the things\nthey say count as good arguments, the same thing might happen.\n\n1\n\nAnd as some people on Twitter point out, it\u2019s wrong even in the case of\ncoffee! The claimed danger of coffee was that \u201cKings and queens saw coffee\nhouses as breeding grounds for revolution\u201d. But this absolutely happened -\ncoffeehouse organizing contributed to the Glorious Revolution and the French\nRevolution, among others. So not only is the argument \u201cFears about coffee were\ndumb, therefore fears about AI are dumb\u201d, but the fears about coffee weren\u2019t\neven dumb.\n\n### Subscribe to Astral Codex Ten\n\nBy Scott Alexander\n\nP(A|B) = [P(A)*P(B|A)]/P(B), all the rest is commentary.\n\n28 Likes\n\n\u00b7\n\n1 Restack\n\n28\n\nShare this post\n\n#### Desperately Trying To Fathom The Coffeepocalypse Argument\n\nwww.astralcodexten.com\n\n50\n\nShare\n\nShare this discussion\n\n#### Desperately Trying To Fathom The Coffeepocalypse Argument\n\nwww.astralcodexten.com\n\n50 Comments\n\n  * New First\n  * Chronological\n\nMike GThe Math Lab60 mins agoMotivated reasoning. Sub-conscious. 10% or\nwhatever chance of AI Doom is such a scary idea, more so even than nuclear war\netc, that I need to counter that fearful emotion. Hence coffee and\nhalibut.Expand full commentReply (1)Share  \n---  \n  \nScott SmythSecond Story54 mins agoYou beat me to it. You see this all the time\nin my field (teaching). I say, \u201cKids these days are not alright,\u201d and then\nsomeone pulls up a quote from Plato saying that the kids in his day weren\u2019t\nalright either, as if the fact that someone in the past was concerned about\nyoung people proves that any problems today are illusory.Expand full\ncommentReply (2)Share  \n---  \n  \nKristian47 mins agoYes, also some people think it is cool to be one of the\n\u201cfearless\u201d people.Expand full commentReplyShare  \n---  \n  \nPjohn36 mins ago\u00b7edited 32 mins agoI completely agree with your point - but\ndisagree with your example of it! The point of the Plato quote isn't to say\n\"kids in classical Greece weren't okay\" halibut-style; the point is that\nPlato's description of kids is *so similar* to present-day descriptions of\nkids that people literally can't tell whether the kids being described come\nfrom classical Greece or present-day Europe unless they're told the origin of\nthe quote - and therefore there probably isn't a downward trend.A more\ninteresting argument that can be made from the same observation is that the\nreason why Plato's description is so uncannily similar to ours is that \"how\nkids look to an elder\" is a sort of constant, and kids will always look that\nway to elders no matter what the kids are actually like, therefore we need\nproper metrics rather than just elders' impressions.(Not endorsing any of\nthese arguments myself, by-the-by, just setting them out because I think you\naren't doing them justice!)Expand full commentReply (1)Share  \n---  \n  \nBrandon FishbackThe Stuff You Get Wrong13 mins agoIt might sound similar\nbecause that quote isn\u2019t from Plato but is from someone\u2019s dissertation a\nhundred years ago.Expand full commentReply (1)Share  \n---  \n  \nPjohn2 mins agoOh wow, I never knew that. Well-caught!Expand full\ncommentReplyShare  \n---  \n  \nEdan Maor57 mins agoI think the entire argument is against expertise,\nespecially regarding \"new technologies\". It's \"Experts tell you something, but\nlook, here experts were wrong about something similar, so just distrust\nexperts in general\".And people more vividly remember \"experts were\nhumiliatingly wrong\" stories than \"experts warned us of something and managed\nto prevent it\" stories. Also more than \"experts didn't warn us and some\ndisaster happened\", those stories are least memorable cause there's no heroic\nprotagonist - there's a lack of one.Expand full commentReply (1)Share  \n---  \n  \nScott Alexander50 mins agoAuthorI feel like if this was the argument, they\nwould have made more of an effort to show that it was \"experts\" who were\nagainst coffee. Instead they blame kings, queens, and alcohol vendors.Expand\nfull commentReply (2)Share  \n---  \n  \nlec38 mins agoAt least to me these register as critiques not so much against\nexperts, but towards predictions in general. \u201dPeople are bad at predicting\nthings. Nothing ever happens, so you shouldn\u2019t worry.\u201dExpand full\ncommentReplyShare  \n---  \n  \nEdan Maor32 mins agoIn the coffee example, you're right. But the usual\nargument is usually framed around experts.Also, experts might be the wrong\nterm here. Maybe it's closer to \"elites\", which Kings and Queens definitely\nfall under. I could imagine someone saying \"President Clinton claimed X but\nactually Y happened\", about, say, the economy. And this would count as proof\nthat elites get things wrong, even if Clinton isn't actually an \"economic\nexpert\".Expand full commentReply (1)Share  \n---  \n  \nBrian Smith20 mins agoThere are much more concrete examples re. the economy.\nPresident Obama famously claimed that, without his proposed stimulus,\nunemployment would reach 8%, but the stimulus would prevent this result. We\ngot his stimulus, and unemployment went over 10%, and stayed above 8% for two\nyears. Now, I don't know whether any (let alone most) economists would have\nsupported his claim in 2009, so this doesn't exactly reflect on economists'\ncapabilities. But it sure does reflect on politicians' and other \"thought\nleaders'\" use of expertise claims to advance their agenda.Expand full\ncommentReplyShare  \n---  \n  \nThrowaway123455 mins ago\u00b7edited 40 mins agoI suggest that these are people\nexperiencing epistemic learned helplessness.cf.\nhttps://slatestarcodex.com/2019/06/03/repost-epistemic-learned-helplessness/>\n\"And there are people who can argue circles around me. Maybe not on every\ntopic, but on topics where they are experts and have spent their whole lives\nhoning their arguments. When I was young I used to read pseudohistory books;\nImmanuel Velikovsky\u2019s Ages in Chaos is a good example of the best this genre\nhas to offer. I read it and it seemed so obviously correct, so perfect, that I\ncould barely bring myself to bother to search out rebuttals.\"> \"And then I\nread the rebuttals, and they were so obviously correct, so devastating, that I\ncouldn\u2019t believe I had ever been so dumb as to believe Velikovsky.\"> \"And then\nI read the rebuttals to the rebuttals, and they were so obviously correct that\nI felt silly for ever doubting.\"Presented with a flood of logical steps by\nsomeone capable of arguing circles around me, adding up to a conclusion that\nis deeply counterintuitive; and presented perhaps also with a similarly\ndaunting flood of logical steps by someone else also capable of arguing\ncircles around me, adding up just as convincingly to the opposite\nconclusion... what is one to do?One can sway back and forth, like a reed in\nthe wind, according to which super convincing argument one last read.Or one\ncan throw one's hands up in the air and say: \"There are hundreds of incredibly\nspecific assumptions combined with many logical steps here. They all look\npretty convincing to me; and actually, so do the other guy's. So clearly\nsomeone made a mistake somewhere, possibly more than one person and more than\none mistake; certainly that's more likely than both the mutually contradictory\nyet convincing arguments being right simultaneously; but I just can't see it.\nWhat now? What other information do I have? Well, approximately ten gazillion\npeople have predicted world-ending events in the past, and yet here we all\nare, existing. So I conclude it's much more likely that the weirder conclusion\nis the one that's wrong.\"Condense that to a sentence or two, couple with an\naverage rather than expert ability to articulate, and you arrive at\ncoffeepocalypse.From the above essay:> \"Even the smartest people I know have a\ncommendable tendency not to take certain ideas seriously. Bostrom\u2019s simulation\nargument, the anthropic doomsday argument, Pascal\u2019s Mugging \u2013 I\u2019ve never heard\nanyone give a coherent argument against any of these, but I\u2019ve also never met\nanyone who fully accepts them and lives life according to their\nimplications.\"AIpocalypse is just another idea to add to that list.Expand full\ncommentReplyShare  \n---  \n  \nPerformative Bafflement54 mins agoI think the steelman for coffee or \"failed\npredictions of disaster\" arguments is based on Lindy Effect sort of things\n(well, humanity's been around a quarter million years, plagues, volcanoes,\nhurricanes, tidal waves, wars and genocides have all happened and we're still\nhere, sooo....), and maybe for some few better read and better epistemic-ed\narguers, pointing to nuclear-disaster style arguments about how hard it would\nREALLY be to actually kill every human.I personally don't buy either of these\narguments (for one thing, we've been through at least one and probably two\nsignificant \"close to extinction\" genetic bottlenecks during our time as\nhumans https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2842629/), but they're a\nbetter class of argument than \"we don't worry about halibuts\" or\nwhatever.Expand full commentReplyShare  \n---  \n  \nsmith52 mins agoI think the strongman of the argument they are making is, for\nalmost every new technology there have been people making dire warnings and\napocalyptic predictions about it (e.g. trains would kill everyone over a\ncertain speed, video games corrupt our youth, nuclear power etc).These\narguments are easy to make because new technologies are scary to many people\nand it is very hard to definitively show something that is new and unproven is\nsafe. Nearly all of these arguments have turned out to be mostly or completely\nwrong.Therefore when someone says AI could kill everyone, your prior should be\nthat it almost certainly won\u2019t.Whilst I can broadly accept that, it isn\u2019t an\nargument to just ignore evidence about AI, just that before you examine the\nsubject that should be your starting point.You could also make the bolder\nclaim that the burden of proof is on those claiming AI will cause calamity,\ngiven how often those sorts of arguments have been made and how beneficial new\ntechnologies have been on average. But I\u2019m not sure I would go that far.Expand\nfull commentReply (1)Share  \n---  \n  \nPeter15 mins agoI think its even broader than this. There is a market for\nconsuming doomsday prophesy. So, writers produce doomsday prophesy and are\nincentivized to make it as plausible as possible to gain market share. This\nhas shifted over my lifetime from nuclear war, ozone, global warming, climate\nchange, COVID, and AI. If you put my casual knowledge against any of the top\nwriters and say, \"respond to my specific arguments otherwise you are wrong\", I\nloose every time.In the paradigm, most doomsday authors will switch between\nthe most salient doomsday narrative over time. And, I think this is what we\nsaw with Scott, Yud, etc. They all switched to COVID for a bit, and now that\nthat has lost steam, they are back on AI. I think their COVID record is good\nevidence that they are overreacting to AI. The historical overreaction to\ncoffee shows that this is a normal thing to happen in societies.Expand full\ncommentReply (1)Share  \n---  \n  \nVitor5 mins ago> If you put my casual knowledge against any of the top writers\nand say, \"respond to my specific arguments otherwise you are wrong\", I loose\nevery time.+1. This is the core of it. I'm reminded of conspiracy theorists\nwho insist that you're not entitled to disagree with them until you've watched\n[some random 4h long youtube video], or read Marx cover to cover.Sneering\nabout epistemic learned helplessness is dishonest, when it just isn't\npractical for me to spend months looking into everything in detail. I do take\nAI risk seriously, and I've engaged (admittedly somewhat superficially) with\nmany of the arguments. What else do you want me to do?Expand full\ncommentReplyShare  \n---  \n  \nDavid Khoo51 mins agoI think the argument isn't against disaster specifically.\nIt's against our ability to predict anything accurately. The argument isn't\n\"The smart guys predicted a disaster but no disaster happened, so no disaster\nwill happen this time\". It's \"The smart guys predicted something and were\nwrong, so smart guys aren't as good at prediction as they think\".Bringing up\nexamples where no disaster was predicted but a disaster happened anyway\ndoesn't refute them. It supports them. Bringing up examples of accurate\npredictions refutes them. However, in practice this is hard to refute because\npredictions are rarely *completely* accurate. Even getting things\ndirectionally right is very hard, let alone getting the timing, magnitude and\nexact character of a big future change correct. Also, an isolated correct\nprediction can be written off as luck, or the \"stopped clock\" effect. You need\na long history of correct prediction to show that prediction works.I think the\neasiest way to gauge what side of this you fall on is the extent to which you\nbelieve in the Efficient Markets hypothesis.Expand full commentReplyShare  \n---  \n  \nPeter50 mins agoMost here will disagree, but \"Number of times the world has\nbeen destroyed by new technology\" seems like a reasonable reference class for\nAI risk.Expand full commentReply (1)Share  \n---  \n  \nJacob G-WJacob\u2019s Substack34 mins agoMaybe, but if the world were destroyed, we\nwouldn't be here to talk about it so this number will always be zero.Expand\nfull commentReply (2)Share  \n---  \n  \nBugmaster27 mins agoArguably, the world was indeed destroyed at least a couple\nof times, for certain values of \"world\" and \"destroyed\". The extinction of the\ndinosaurs is one obvious global example; the Black Death and any of the\nvarious genocides in human history are local examples. None of those cases\ndepended primarily on new technologies, however.Expand full commentReplyShare  \n---  \n  \nPjohn16 mins agoIn risk-analysis I believe they have a system for assessing\nthe probability of risks that must always turn up nil for observer-selection-\neffect reasons: I gather they look at the near-misses, then apply a correction\nfactor based on the average ratio of near-misses* to incidents within similar\nfields.(* presumably near-misses for less-observer-selection-effect-\nsusceptible risks..)For example: for Titanic-level maritime disasters, even\nthough they're hard to do statistics with about because the frequency is less\nthan once per lifetime, the incident:near-miss ratio is about 1:100 (source:\nvague half-remembered lecture I once attended, possibly whilst not entirely\nsober..) so if you see 10 near-misses in a given time-frame you can suppose a\nroughly 10% chance of an incident within the same time-frame.Can we do this\nwith AI doom? Probably not, because there probably aren't any incidents\nsimilar enough to AI doom with a known incident:near-miss ratio. However, to\naddress Peter's phrasing: we do have data on nuclear war near-misses - and\nthere are certainly more than enough of those to make one unwilling to just\nassume new technology won't destroy large parts of the world that one cares\nabout.Expand full commentReplyShare  \n---  \n  \nVictor Dordanski49 mins agoI think part of the problem is exactly defining\nwhat risks AI pose. There is a difference between a hand-wavy nanobot\napocalypse and something like \u201cIt could disrupt multiple industries completely\nand immediately, which will lead to instability in the economy\u201d.But big\npicture, absolutely none of this matters, because we can clutch our pearls\nhere in the United States, or even pass regulation, trying to slow down\ntechnical progress (good luck), but the reality is progress will just march\nforward in Russia and China and India.Seems to me the only safety we can get\nis to try to make the US be the leader of this technology, instead of de facto\nceding leadership to ChinaExpand full commentReply (1)Share  \n---  \n  \ncarateca1 min agoYes, this is a vital point that just gets whistled past. The\nclosest I've ever seen a doomer get to addressing it is \"the Chinese\ngovernment is notoriously pragmatic and careful and would never take such a\nrisk\" which is so disconnected from reality it leaves one staring in\nawe.Expand full commentReplyShare  \n---  \n  \nOmer49 mins agoI'm puzzled by your puzzlement. The (indeed silly)\n\"coffeepocalypse\" argument just highlights how new things instinctively and\ngenerically cause panic, even if unjustified\u2014and a fortiori when there's\nactually some inherent danger to them. It says: you should significantly lower\nyour prior regarding the risk associated with new developments.Expand full\ncommentReplyShare  \n---  \n  \nTatu AhponenTatu Ahponen45 mins ago\u00b7edited 42 mins agoI think it would be best\nrephrased as \"There are people who have a reflexive mindset against new ideas.\nIt's a good idea to get rid of such a mindset\". Global cooling and\noverpopulation may not be technologies but they are ideas, just as AI doom, AI\nparadise and pretty much every AI-related prediction and concept is.One\nproblem here is that it's probably a good idea that we do have at least *some*\npeople who are true curmudgeons, willing to challenge everything and anything,\nsimply because it's good for everything to go through a challenge. It would be\ngreat in general to acknowledge that the society benefits from a vast number\nof different mindsets and ways of reacting to things.Expand full\ncommentReplyShare  \n---  \n  \nAJKamper42 mins agoI appreciate the steelmanning, but isn't this really an\nargument aimed at fellow anti-AI-riskers to help them feel superior? I just\ncan't take this charitably at all.Expand full commentReplyShare  \n---  \n  \nFeral Finster41 mins agoIf humans insist on dopey analogies: if you play\nRussian Roulette often enough, the probability you will kill yourself\napproaches 100%Expand full commentReply (1)Share  \n---  \n  \nBugmaster30 mins agoWhile this is true, your decision to play or not to play\ndepends on the size of the revolver's drum. If the drum contains one trillion\nchambers, and you know that only one is loaded, then you might as well play at\nleast a few times (assuming there's a payoff involved).Expand full\ncommentReplyShare  \n---  \n  \nBugmaster40 mins ago> Here\u2019s an example of a time someone was worried about\nsomething, but it didn\u2019t happen. Therefore, AI, which you are worried about,\nalso won\u2019t happen.Right, that's an obvious strawman. The real argument goes\nmore like this:\"Here's an example of a time someone was worried about\nsomething due to employing a particular pattern of reasoning. The something\ndidn't happen, and the manner in which it didn't happen helped to expose the\nflaws in the pattern of reasoning. You are worried about AI due to employing\nthe same pattern of reasoning, riddled with the same flaws. Therefore, you are\nlikely to be just as wrong as that other guy.\"Expand full commentReply\n(1)Share  \n---  \n  \nMicah Zoltu34 mins agoI have been sitting here on the comments page trying to\nfigure out how to express what was in my head, and I think this does an\nexcellent job, thanks!Expand full commentReplyShare  \n---  \n  \nAS33 mins agoA maximally hostile and bad explanation of the prevalence of\nhalibut method: for any question that seems hard, most people won\u2019t even try\nto engage with it at an object level. What they do instead is look for any\nsimilar disagreements they remember and which side won them and try to pick\nthe winning side to maintain social status. They get helpfully halibuted into\nthe \u2018right\u2019 side of the argument by the nudge in the problem presentation and\nfrom then on they are invested into getting their side of the argument to win,\nspreading halibut arguments further. Note that at no point did they reflect on\nwhat the actual answer would be - that would be waste of resources.Expand full\ncommentReplyShare  \n---  \n  \nTasDeBoisVert33 mins agoContra the coffepocalypse argument: the coffepocalypse\nis real and true. It's a drug that infected the entire world, and even people\nwho don't drink coffee are mostly addicted through other sources of caffeine\n(tea, coke, energy drinks, etc). We can't fathom living without it, we got\ndispensers in every company, college, rest rooms. We feel like shit waking up\nuntil we get our first dose and getting a hit together is one of our main way\nof socializing. Our world runs on caffeine.And the case against AI is the\nsame: we're facing a potential future where the entire human civilization is\ndependent on it. Of course the same case can be made of cheap energy, of cars,\nor something else (insert that picture of a 26-lanes highway for impact).\nWe're living on layers upon layers of technology & infrastructures we can't\nlive without. Can the pile eventually crumble under it's own weight?I admit,\nit's not exactly (at all?) the original coffepocalypse argument. But I stand\nby it.Expand full commentReplyShare  \n---  \n  \nRobert Horn33 mins agoFor me the best version of coffeepocalypse is as a\ndemonstration that no matter how obviously harmless a new technology is,\npeople will be extremely worried about it and say it will destroy everything.\nThis is such a consistent rule that it even applies to *coffee* (but NB also\nall other new things) so you shouldn't take any information from people being\nextremely worried about a particular new technology. I don't think this is\nactually true or valid, but it's the most optimistic interpretation I can\nthink of.Expand full commentReplyShare  \n---  \n  \nRob Miles31 mins ago\u00b7edited 18 mins agoIt's a social argument! It's not meant\nto be deployed against \"AI is dangerous\", but against \"Various people say AI\nis dangerous, therefore AI is dangerous\". If you don't trust your ability to\nevaluate the technical arguments, all you have is what various people are\nsaying, and this kind of thing becomes a critical consideration. The news says\n\"Geoffrey Hinton, very respected AI expert, says AI is very dangerous, so\nmaybe you should think so too\". In that situation \"A lot of authoritative\npeople have thought things were dangerous in the past and been wrong\" is a\npoint very worth considering.Even technically minded people can do this. If\nyou've never thought deeply about the technical details of AI risk, but heard\nsome bad version once and dismissed it from that point forward, then when you\nfind someone suggesting AI is dangerous, you may assume they got there by\nadopting the conclusions of others, and try to convince them that this way of\nreasoning is error-prone.Edit: If this is the case, the most productive\nresponse may be \"I don't think AI is dangerous because others believe it, I\nthink it's dangerous for the following technical reasons that have nothing to\ndo with deferring to others' conclusions...\"Edit2: In the same way, I have\noften used the Rutherford and Szilard argument *as a response to* the then-\ncommon argument \"experts predict AGI is a long way away, therefore it's a long\nway away\", to which saying \"sometimes experts predict things are a long way\naway when they aren't at all\" is perfectly reasonableExpand full\ncommentReplyShare  \n---  \n  \nWilliam H Stoddard29 mins agoIf we are going to use the ostrich as a metaphor,\nI think it would be symmetrical to have a fable for the other side also.\nPerhaps Chicken Little saying the sky is falling? That actually turns out to\nbe very old; one of the jatakas has Buddha, in one of his past lives, calming\nthe fears raised by a rabbit after a similar minor incident . . .Expand full\ncommentReplyShare  \n---  \n  \nBrian Smith26 mins agoI have to admit I started out puzzled over the whole AI\nSafety issue, for several reasons:1\\. Nothing I've heard of in the realm of AI\ncomes close to anything I'd regard as \"Intelligence\", and I haven't heard of\nany major steps in that direction.2\\. Since nothing we have, or are\ncontemplating, comes close to \"Intelligence\", we really can't plan an\neffective protection against the next-several-generations version that might\nconceivably be a threat.3\\. Who would ever give any automated system enough\npower to control important things in real life anyway? It makes some\ninteresting premises for movies (Like Colossus: The Forbin Project, or War\nGames, or 2001: A Space Odyssey), but really those are more ridiculous than\ninteresting.I mainly started to take the idea semi-seriously because Scott\ntakes it very seriously, and Scott is not only very smart, but very focused on\ngood reasoning and careful consideration of evidence in context. At least in\nsome areas. ;-)But I keep returning to something I saw in one of the\nSubstacks, which looked at the evolution of LLMs and GPT in particular. If I\nremember right, it extrapolated versions of GPT and concluded that GPT4 used\nsome enormous amount of training data and computing power more than GPT3, and\nGPT5 would need very much more, and GPT6 would need more computing power than\nthe world could provide, and still would be unlikely to approach anything like\nindependent intelligence. So I think the dangers from AI are still (almost)\nentirely theoretical, and far in the future, and so far away from the current\nworld that preparations are (probably) useless. But I'm nevertheless glad that\nsmart, conscientious people like Scott are devoting some thought and energy to\nthe issue, just in case it does turn out to be important.I was amused that\nScott mentioned my favorite current issue: global warming. I'm not sure of his\noverall stance, and maybe he was only listing issues that others might find\nrelevant, but \"People didn\u2019t worry enough about global warming\" seems like the\nopposite of his argument. Judging by media coverage and politicians' rhetoric,\npeople seem to be worrying a very great deal about global warming, even though\nthere's very little evidence that it has caused significant damage (at least\ncompared to other environmental issues of the past and present) or that it\nwill in the future. When I make this claim, I don't consider media reports to\nbe evidence, even the media reports that say \"Scientists say\". I trace to\nactual scientific claims in actual scientific publications, as compiled by the\nIPCC, and find some reasons for concern, but no current or projected problems\nthat would be anywhere near the magnitude of the problems caused by serious\nefforts to rapidly decarbonize the world's economy, which seems to be the only\nsolution on offer.But maybe my reasoning is no better than the head-in-the-\nsand example. I often point out that the sky-is-falling reasoning common in\npublic discussion is unsubstantiated, and the proposed solutions won't come\nclose to solving the alleged problem, but even if that's true, it doesn't\nprove that there's no apocalypse coming. So maybe it's good that people are\nwilling to spend a few hundred billion dollars on things that probably won't\nmake much difference. Except that I still think we could spend those hundreds\nof billions on other things that would make a difference in other areas.Expand\nfull commentReplyShare  \n---  \n  \nSeiWanderer in the Sea of Fog24 mins agoThe coffee argument attempts to\nbolster the idea that risks that seem ridiculous are ridiculous and that\nthings suggested to pose theoretical risks via novel predicted pathways are\nusually wrong. It differs from plague and war in that plague and war are bad\nin very predictable ways and that coffee and AI were suggested to be bad in\nheretofore unforeseen ways that sound stupid.\"Computers will make computers so\nsmart they could outsmart everyone at everything, at which point they'll kill\nus all\" is in fact a novel and stupid sounding argument. I just happen to\nthink it's probably true.Expand full commentReplyShare  \n---  \n  \nOzynever odd or even23 mins agoMy take is related to \"\"Twitter derangement\nsyndrome\"\". I often see smart people on twitter transition from reasonable\n(but ofc with biases etc) tweets, who avoid the dumb on the other side of any\nargument and focus on the interesting ideas of the other side of an argument.\nThey transition to meme broadcast. Musk for instance.I think this happens\nbecause (a) takes on issues aren't independent correlations, (b) the algo\npushes you to the pattern of reading the smart people on your correlation\nside, and dumb people on the other correlation side.So you get ego-massage (\"I\nalso think what this very smart person thinks - I could have said that if I\nhad more time ...\") and outrage (\"I can't believe whats in the heads of the\npeople who think the opposite to me.\")At this point you may start to become\ntribal, *you* change. Whats the point of reason if the other side don't accept\nit? Just firehose them with vibes.So my take on the coffeepocalypse argument\nis they are no longer trying to persuade you. They are trying to influence the\n99% of people that are tribal that they see also make dumb arguments.Could it\nbe that the more smart people act tribally, the more oneself does. TDS is\ninfectious.Perhaps the 'grey' tribe subconsciously operates with the world\nview analogous to trickle-down economics: The tribe will adopt what their own\nthought leaders think. And those thought-leaders are susceptible to argument\nas the redistribution of the wealth of the rich-in-reasoning. But social media\nisn't well-designed for that to work well?This is all black and white. I don't\nmean it so - we all have these elements running through us at different times\nof day and different issues. TLDR, it's not you the person is trying to\nconvince. That's not where the engagement reward comes from anymore.Not a new\nargument on reflection. Oh well.Expand full commentReplyShare  \n---  \n  \nArrk Mindmaster22 mins agoI don't understand why you feel the need to\nunderstand these arguments. Between this and the last one,\nhttps://www.astralcodexten.com/p/contra-hanson-on-medical-effectiveness, it\nalmost seems like you're beating up on the intellectual weaklings.Why not ask\nsomething like what is it about YouTube that attracts bad comments?Idiots\nabound. Someone is ALWAYS wrong on the internet. https://xkcd.com/386/Expand\nfull commentReplyShare  \n---  \n  \nTom Zimbardo18 mins agoIts an error of overgeneralisation:People worried\ncoffee would be dangerous, and were wrong - correctly leads to the conclusion\nthat *its possible to worry about new things being dangerous, and be wrong* -\nbut it absolutely *does not* lead to the conclusion that *its possible to\nworry about new things being dangerous, and be wrong, therefore any time\nanyone worries about new things being dangerous, they are wrong*.The Halibut\nexample is a little confusing because its reverse the negative (not) to are.\nSuppose it was this, leaving the NOT in place:A guy thought Halibut was a type\nof cow. He was wrong. Its not a type of cow. Therefore AI is not a type of\ncow.Now, its almost certainly true that AI is not a type of cow (unless the\nMootrix hypothesis is actually true), but the fact that AI is almost certainly\nnot a type of cow does not logically flow from the fact that Halibut is not a\ntype of cow. The two statements are unrelated.So too does the fact that coffee\nturned out not to be dangerous (except in edge cases where people drank over\nthe LD50 which is about 10grams or 100 cups of coffee and died of a heart\nattack). The fact that coffee is mostly not dangerous is completely\nindependent of the as yet unknown safety or danger of as yet uninvented AI.The\nbetter argument for me is to think about the Omohundro AI drives argument,\nwith relevance to human intelligence. Omohundro argued that without specific\nreasons not to, any AI with motivational drives would behave like a human\npsychopath in its pursuit of resources. AI at that level doesn't exist yet,\nbut there are plenty of examples of the Unfriendly Human Problem, aka human\npsychopaths, or just plain mean people. The fact that Unfriendly Humans *can\nexist* strikes me as far more relevant to the hypothesis that Unfriendly AIs\n*may be able to exist*.By analogy from humans with specific types of brain\ninjury (my field), most present AIs are analogous to being disabled in many\nways. ChatGPT is blind for example, you can't show it anything, it can only\nrespond to text. Its also pseudo in its theory of mind capabilities - you\ncould cry while typing a message to it and it wouldn't know, although it might\nrespond sympathetically if you typed about how sad you were. By extension, it\ncan't really empathise, no matter what it \"types\" in response. It is\npsychopathic by default because no one has solved the AI empathy problem yet.\nIf we solved the AI problem for general intelligence but didn't solve the\nempathy problem, we might be in trouble. The fact that high IQ psychopaths do\nexist means that psychopathic AI *might* be able to exist, but is not a proof\nor certainty.Expand full commentReplyShare  \n---  \n  \nGeoffrey Irving17 mins agoLikely this was said above and I missed it, but the\nRussell argument from nuclear fission is (arguably) from a much smaller\nreference class: the first time we invented a plausibly catastrophic\ntechnology, some experts confidently predicted it was impossible. Thus,\nRussell\u2019s should be more convincing than the coffee argument, which is\npresumably sampled/found from a much larger class of worried.Expand full\ncommentReply (1)Share  \n---  \n  \nGeoffrey Irving17 mins ago(larger class of *worries*, that is)Expand full\ncommentReplyShare  \n---  \n  \nVitor16 mins agoFor me, it's about remembering past moral panics. I'm not\nsaying that AI couldn't be really bad. I'm just saying that lots of ostensibly\nsmart people believing it's going to be bad is not, in itself, a strong\nargument. This is especially relevant considering the information cascades\nthat do happen in our community. \"Eliezer believes in AI doom and he's really\nsmart in [some other domain], therefore I'll update towards his position on AI\ndoom\".I'm pretty much an AI bear. While I agree that AGI is not impossible, I\ndon't see strong evidence that it's around the corner. In fact, I have the\nsame frustration as Scott in reverse. Lots of people gesturing wildly at stuff\nthat is clearly not AGI, then expecting me to freak out, without addressing\nthe core of my skepticism.P.S.: My favorite moral panic I remember from\nchildhood is the idea that we were going to bury ourselves in mountains of\ntrash. I looked up some old news clips from the late 80's in the archive of my\nlocal tv channel and it was quite funny to watch.Expand full commentReplyShare  \n---  \n  \nyesthisisdog15 mins ago\u00b7edited 12 mins ago\"I would like to understand the\nmindset of people who make arguments like this\"I think a lot of people who do\nthis aren't making logical arguments. They're signalling some combination\nof1\\. Relaxed and confident leader: \"don't be anxious about this stuff, be\nchill. Look, I'm chill.\"2\\. Be optimistic despite things in fact being bad:\n*artillery raining down on trenches \"We've always gotten through wars in the\npast, this time we'll make it too.\"3\\. Caution against the village weirdo,\nadversary, and/or \"boy who cried wolf.\"4\\. etc.These things are social signals\nand/or \"just-so\" stories (the heuristic arguments you allude to). Yes, on\nfurther inspection they aren't logically correct, but they do have utility,\nwhich is why they have survived as standard ways of communicating. I'm not\ndefending their behavior, just observing!In terms of Zvi's simulacra levels,\npeople are operating on different simulacra levels and jumping around the\nlevels. When you engage on level 1, suddenly it's a snap to reality instead of\nsocial signals and other goals, so it's a sudden incongruous incompatibility\nwith what they're saying. You're talking about different things. It's like the\nfearless leader getting annoyed \"Hey what are you doing saying we might lose\nthe war, I'm trying to calm everyone down so we can keep people showing up to\nwork.\"Then there's also stuff like \"do I want to think of myself or be\nperceived as the sort of person who would lie or be incorrect about something\"\nand/or true curiosity about the truth, which is why they don't have a response\nto you or are even surprised/annoyed that you've said what you've said.In\nterms of behavioral biology, you're snapping on the executive function/logical\nreasoning parts of the brain and asking them to allocate more compute here\nwhen someone was going more off vibes, social calculation, feels. This can be\nuncomfortable or even perceived as socially adversarial.Disclaimer: I made it\nall up and you probably know most of what I wrote. To be fair, I understand a\nlarge purpose of the post was to introspect about our own ways in which we\nmake these same mistakes, rather than point out that people make a certain\ncategory of mistakes.Expand full commentReplyShare  \n---  \n  \nMorgan Beatus11 mins agoPeople making these kinds of \u201cThing A didn\u2019t happen\ntherefore Thing B also won\u2019t happen\u201d assume that (or act as if) your worry\nabout Thing B is based solely on feeling the emotion of apocalyptic fear for\nthe very first time, and submitting that alleged novelty of emotion as your\nonly evidence that worrying about Thing B is rational. So, they believe that\nif they can attack the novelty of that feeling (\u201cpeople have felt apocalyptic\nfear before!\u201d) your argument has crumbled. They are not thinking of your\napocalyptic fear about Thing B as a downstream consequence of other evidence\nat all, so they don\u2019t think they have to address the upstream evidence.Expand\nfull commentReplyShare  \n---  \n  \nLorenzo FerroOdyssey of Buridasinus11 mins agoRhetorics. Shiny things owns us.\nHere's a nice similarity, there's a touching picture, there is a charismatic\nperson.Noticing these influences and fighting them to consider only the\nrational backbone of things is for very few... if any.Expand full\ncommentReplyShare  \n---  \n  \nstronghand14stronghand14\u2019s Substack6 mins agoCould it be that someone just\nwanted to write about coffee and knew that pretending his coffee-story was\nactually about AI would get more clicks for the coffee-story?Expand full\ncommentReplyShare  \n---  \n  \nMattRMattR\u2019s Substack6 mins agoI think people like Eliezer and Conor Leahy\npredict doom so confidently and occupy so much attention space here that they\ncause these arguments to look good as existence proofs.Even putting those\ncases aside, people typically ascribe high confidence to most public claims\npeople make and this implicitly lowers the bar for what counts as acceptable\ncriticism. Original claim-makers are trying to model the world; critics are\njust trying to raise considerations.Expand full commentReplyShare  \n---  \n  \nAlex Poterack6 mins agoI think this is a typical failure of thinking\nprobabilistically. A lot of people basically think all probabilities are zero\nor 1; they hear someone say the probability of AI killing us all is non-zero,\nand hear that it's 100%. The coffee argument is a perfectly fine argument that\nthe probability of AI killing us all is not 100%; having demonstrated it's not\n100%, they conclude it's zero.Expand full commentReplyShare  \n---  \n  \nLoris4 mins agoI'm not registered with twitter (and don't want to be). All I\nsee at the 'read the full thread here' link is the tweet with the coffeecup\nimage. I might be missing the link on the page, but I have looked and can't\nsee anything obvious. Is there a way for me to read it?Expand full\ncommentReplyShare  \n---  \n  \nIvermectin: Much More Than You Wanted To Know\n\n...\n\nNov 17, 2021\n\n403\n\nShare this post\n\n#### Ivermectin: Much More Than You Wanted To Know\n\nwww.astralcodexten.com\n\n2,155\n\nStill Alive\n\nYou just keep on trying till you run out of cake\n\nJan 21, 2021\n\n1,198\n\nShare this post\n\n#### Still Alive\n\nwww.astralcodexten.com\n\n512\n\nIn The Long Run, We're All Dad\n\n...\n\nDec 22, 2023\n\n1,045\n\nShare this post\n\n#### In The Long Run, We're All Dad\n\nwww.astralcodexten.com\n\n460\n\nReady for more?\n\n\u00a9 2024 Scott Alexander\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": true}
