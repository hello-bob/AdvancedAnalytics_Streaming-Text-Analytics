{"aid": "40101960", "title": "Open source \u2013 Unsupervised captioning getting closer to supervised captioning", "url": "https://github.com/DavidHuji/CapDec", "domain": "github.com/davidhuji", "votes": 2, "user": "capdecRon", "posted_at": "2024-04-20 23:35:09", "comments": 0, "source_title": "GitHub - DavidHuji/CapDec: CapDec: SOTA Zero Shot Image Captioning Using CLIP and GPT2, EMNLP 2022 (findings)", "source_text": "GitHub - DavidHuji/CapDec: CapDec: SOTA Zero Shot Image Captioning Using CLIP\nand GPT2, EMNLP 2022 (findings)\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nDavidHuji / CapDec Public\n\n  * Notifications\n  * Fork 18\n  * Star 167\n\nCapDec: SOTA Zero Shot Image Captioning Using CLIP and GPT2, EMNLP 2022\n(findings)\n\n167 stars 18 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# DavidHuji/CapDec\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n2 Branches\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nDavidHujiMerge pull request #18 from davenu/patch-3Jan 28, 20244451bfd \u00b7 Jan\n28, 2024Jan 28, 2024\n\n## History\n\n291 Commits  \n  \n### .github/workflows\n\n|\n\n### .github/workflows\n\n| Create python-package.yml| May 10, 2023  \n  \n### figures\n\n|\n\n### figures\n\n| cosmetics| Nov 2, 2022  \n  \n### others\n\n|\n\n### others\n\n| cosmetics| Nov 5, 2022  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update README.md| Jan 28, 2024  \n  \n### custom_types.py\n\n|\n\n### custom_types.py\n\n| cosmetics| Sep 15, 2022  \n  \n### embeddings_generator.py\n\n|\n\n### embeddings_generator.py\n\n| dont_norm 1 2 3| Oct 14, 2022  \n  \n### gpt2_prefix.py\n\n|\n\n### gpt2_prefix.py\n\n| cosmetics| Oct 22, 2022  \n  \n### gpt2_prefix_eval.py\n\n|\n\n### gpt2_prefix_eval.py\n\n| adding eval files from private git repo| Jun 8, 2022  \n  \n### parse_karpathy.py\n\n|\n\n### parse_karpathy.py\n\n| cosmetics| Nov 1, 2022  \n  \n### predictions_runner.py\n\n|\n\n### predictions_runner.py\n\n| comment| Nov 14, 2022  \n  \n### requirments.txt\n\n|\n\n### requirments.txt\n\n| citing| Nov 7, 2022  \n  \n### train.py\n\n|\n\n### train.py\n\n| cosmetics| Nov 2, 2022  \n  \n### transformer_mapper.py\n\n|\n\n### transformer_mapper.py\n\n| adding eval files from private git repo| Jun 8, 2022  \n  \n## Repository files navigation\n\nInference Notebook (a few different models):\n\n## Link to YouTube Presentation\n\n# \"CapDec: Text-Only Training for Image Captioning using Noise-Injected CLIP\",\nEMNLP 2022 (findings).\n\n## Official implementation of the paper\n\nAs shown in the paper, CapDec achieves SOTA image-captioning in the setting of\ntraining without even a single image. This is the formal repository for\nCapDec, in which you can easily reproduce the papers results. You can also\nplay with our inference notebook to see how the model works, and try it on\nyour OWN images with different CapDec-based models.\n\n## FlickrStyle7k Examples\n\nExamples for styled captions of CapDec on FlickrStyle10K dataset:\n\n## Training prerequisites\n\nClone, create environment and install dependencies:\n\n    \n    \n    git clone https://github.com/DavidHuji/CapDec && cd CapDec conda env create -f others/environment.yml conda activate CapDec\n\n# Datasets\n\n  1. Download the datasets using the following links: COCO, Flickr30K, FlickrStyle10k.\n  2. Parse the data to the correct format using our script parse_karpathy.py, just make sure to edit head the json paths inside the script.\n\n# Training\n\nMake sure to edit head the json or pkl paths inside the scripts.\n\n  1. Extract CLIP features using the following script:\n\n    \n    \n    python embeddings_generator.py -h\n\n  2. Training the model using the following script:\n\n    \n    \n    python train.py --data clip_embeddings_of_last_stage.pkl --out_dir ./coco_train/ --noise_variance 0.016\n\nThere are a few interesting configurable parameters for training as follows.\nYou can view it by running 'python train.py --help'\n\n    \n    \n    optional arguments: -h, --help show this help message and exit --data path to clip embeddings of captions generated by the attached embeddings_generator script --val_pt path to clip embeddings of validations set --pretrain_weights path to pretrained weights, if not specified, will train from scratch --out_dir path to output directory --add_modality_offset train with modality offset that was pre calculated at others/CLIP_embeddings_centers_info.pkl --prefix PREFIX prefix for saved filenames --noise_variance noise variance --uniform_noise use uniform noise instead of gaussian --dont_norm dont normalize CLIP embeddings --lr LR learning rate --epochs EPOCHS number of epochs --save_every save every n epochs --prefix_length prefix length --prefix_length_clip prefix length for clip --bs BS batch size --only_prefix train only the mapper between CLIP and GPT, while GPT is frozen --mapping_type type of architurctre between CLIP and GPT (mlp/transformer) --num_layers number of layers in the mapper --is_not_rn Choose the CLIP backbone: False for RN, True for ViT --use_image_embedding_as_clipcap use image embedding as ClipCap\n\n# Evaluation\n\nFor evaluation, we used a repository that adapts the COCO evaluation script to\npython 3 here. In order to evaluate the model, you need to first generate the\ncaptions using the following command (just edit the images_root path inside in\norder to direct it to the right ground truth annotations).\n\n    \n    \n    python predictions_runner.py --checkpoint path_to_checkpoints.pt --dataset_mode 0\n\n# Pre Trained Models\n\nWe upload the trained weights that we used for creating Fig.3 in the paper, so\nyou can download it if you do not want to wait for training. Here are the\ntrained weights of 9 different noise levels.\n\n# Open Text Training - Training on any corpus as Harry Potter Books,\nShakespeare Plays, or The New York Times (was NOT presented at the paper).\n\nA cool application of CapDec is to create captions in the style of a specific\ncorpus that was not even in the form of captions. Ideally, any given text can\nbe used to train CapDec's decoder to decode CLIP embeddings. It enables the\nelimination of the need to have any sort of captions textual data. Moreover,\nit enables captioning model that is in the specific style of the given text.\nfor that, we can first pre-train with images as regular ClipCap, then we fine-\ntune as in CapDec with text only when the text data is a combination of half\nCOCO captions and half sentences from the open text (HP or News) sentences in\nlength between 4 to 20 words.\n\nIn order to reproduce that, all you need is to create sentences out of the\nopen text, save them in the right format as the json we have for COCO and then\nrepeat the steps mentioned above for training. For that you can use the\nattached script at others/hp_to_coco_format.py. Although you can use any sort\nof text for that, you can download the data we used, from the following links:\nHarry Potter, Shakespeare, News You can see an example of the correct format\nfor training at others/parssed_sheikspir_alllines_111k.json\n\nHere are a few examples of the results of training on the Harry Potter books,\nShakespeare plays, and news articles:\n\n# Fairness.\n\nIn principle, CapDec could be useful for creating captions that are fairer by\nfixing biases in the data. For example, we can de-bias the textual data by\nchanging gender terms. That trick is possible only in our setting of text-only\ntraining (i.e. image editing is much more complex than text editing). More\ngenerally, any sort of bias in the data could be manipulated in the data by\nsimple text editing.\n\nIn order to examine this idea we implemented text-data bias editing for gender\nterms. You can use it by adding the flag --fix_gender_imbalance_mode when you\nrun the script of embeddings_generator.py. It has three modes: 0 - no fixing,\n1 for both genders, 2 for men only, 3 for women only. For example when\nrunning: python embeddings_generator.py --fix_gender_imbalance_mode 2 any\ngender term of male will be exchanged with a probability of 0.5 to a female\nterm, resulting in more balanced data (in COCO there are much more male\ncaptions than a woman as shown by 'woman also snowboard 2018').\n\n## Citation\n\nIf you use this code for your research, please cite:\n\n    \n    \n    @article{nukrai2022text, title={Text-Only Training for Image Captioning using Noise-Injected CLIP}, author={Nukrai, David and Mokady, Ron and Globerson, Amir}, journal={arXiv preprint arXiv:2211.00575}, year={2022} }\n\n## Acknowledgments\n\nThis repository is based on CLIP, ClipCap and pycocotools repositories.\n\n## Contact\n\nFor any issue please feel free to contact me at: nukraidavid@mail.tau.ac.il.\n\n## Star History\n\n## About\n\nCapDec: SOTA Zero Shot Image Captioning Using CLIP and GPT2, EMNLP 2022\n(findings)\n\n### Topics\n\nclip zero-shot-learning captioning multimodal-deep-learning gpt-2 clipcap\n\n### Resources\n\nReadme\n\nActivity\n\n### Stars\n\n167 stars\n\n### Watchers\n\n5 watching\n\n### Forks\n\n18 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Contributors 7\n\n## Languages\n\n  * Python 68.1%\n  * Jupyter Notebook 31.9%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
