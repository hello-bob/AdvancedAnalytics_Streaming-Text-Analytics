{"aid": "40070240", "title": "Effort Engine", "url": "https://kolinko.github.io/effort/index.html", "domain": "kolinko.github.io", "votes": 1, "user": "todsacerdoti", "posted_at": "2024-04-17 21:28:58", "comments": 0, "source_title": "Effort Engine", "source_text": "Effort Engine\n\nThe Basics Introducing bucketMul The GPU implementation MoE Pesky details\nAbout the Author(s) Download and Run\n\n# Effort\n\nA possibly new algorithm for LLM Inference\n\nWith Effort you can adjust smoothly - and in real time - how many calculations\nyou'd like to do during inference of an LLM model.\n\nAt 50% calculations it is as fast as regular matrix multiplications on Apple\nSilicon chips. At 25% effort it's twice as fast and still retains most of the\nquality.\n\nYou can also freely choose to skip loading the least important weights.\n\nIt is implemented for Mistral now, it should work for all the other models\njust as well. No retraining needed, just conversion to a different format and\nsome precomputation.\n\nYou can download the implementation here - from Github. It should run right\nafter fetching the converted weights.\n\nThe implementation is done for FP16 only for now. The multiplications are\nfast, but inference overall is still slightly lacking because some non-\nessential parts - softmax etc - need improvement.\n\nMixtral and Q8 are in the works.\n\nOh, and it also allows to dynamically decide how much of the model you want to\nload into the memory. You can skip the least important 10-20-30% weights while\nloading. It doesn't require any conversion steps - it just loads less data.\nSort of an ad-hoc distillation, you could say.\n\nLet's see it in action now.\n\n\\--:----:--\n\nThe actual speed is constrained by the implementation overhead. Even on 0%, it\nstill takes 15ms on my machine (and a few seconds on M1 Air) to produce a\nsingle token. These parts are done in <1ms in Llama.cpp/Ollama, and I could\nuse help from a Swift/Metal engineer to fix this.\n\nYou can download and test it yourself from Github.\n\nGetting back to benchmarks...\n\nThe pink line is actual speed, with a suboptimal implementation of the\noverhead (calculating norms, attn scores etc). There is a ~15ms overhead to\nevery token that I didn't manage to fix, but Llama.cpp/Ollama/MLX don't seem\nto have. Help would be appreciated here from someone proficient in\nMetal/Swift.\n\nLet's talk quality now.\n\nFirst, of the multiplication approximation itself.\n\nTracked by taking a sample input state vector, multiplying it by say wq, or\nw1. Some Matrixes seem to be easier to approximate using this method, some are\nslightly more difficult. But more or less the output looks like here.\n\nAnd now for the model itself.\n\nMeasured by generating a 500 token text first, and then comparing predictions\nof the tokens when this text would be given as an input. Perplexity score\nwould be nice here, see notes at the end why it's not yet done.\n\nAnd basic QA tests:\n\nBasicQ is a list of a few tricky QA prepared by GPT-4. Hopefully this and the\nworking demo are enough to show the potential. Performing HumanEval and\nHellaSWAG needs fixes in the implementation first - see below.\n\nIf you're still sceptical - as I would be - please head out to the Help\nNeeded! section to see what's needed for the better tests to be performed.\n\nThe initial results (and undocumented experiments with Mixtral) seem to be\nrobust enough to warrant publication. I hope though that the above is enough\nto convince you to play with the 0.0.1B version.\n\n# Deep dive into the algorithm.\n\n\\- The Basics\n\n\\- Introducing bucketMul\n\n\\- The GPU implementation\n\n\\- MoE, quantization and the others.\n\n\\- Pesky details (or: Help Needed!)\n\n## And of course...\n\n\\- About the Author(s)\n\n\\- Download and Run\n\n\\- Citations, notes and so on\n\n", "frontpage": false}
