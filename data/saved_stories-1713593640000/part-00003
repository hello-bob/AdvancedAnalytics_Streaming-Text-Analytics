{"aid": "40090264", "title": "Enhancing Low-Light Images Locally and Privately in the Browser", "url": "https://blog.mdturp.ch/posts/2024-01-24-low-light-image-enhancement.html", "domain": "mdturp.ch", "votes": 1, "user": "goranmoomin", "posted_at": "2024-04-19 18:19:12", "comments": 0, "source_title": "Enhancing Low-Light Images Locally and Privately in Your Browser | MDTURP", "source_text": "Enhancing Low-Light Images Locally and Privately in Your Browser | MDTURP\n\nSkip to content\n\nMDTURP\n\nAppearance\n\nOn this page\n\n# Low light image enhancement locally and privately in the browser\n\nThis week's blog post showcases how to perform low-light image enhancement\nentirely in the browser. I will demonstrate the steps required to transform a\nPyTorch model into an ONNX model and explain how to get it running in a Vue.js\napplication.\n\nThe final outcome of this process will enable us to execute transformations\nsuch as the one illustrated below:\n\nAll relevant resources can be found here:\n\nGithub\n\nDemo\n\n## The plan\n\nAfter last week's post about using the HuggingFace TransformersJS library for\nimage super-resolution in the browser, I've decided to delve deeper into the\nlibrary's underlying execution environment, specifically the ONNX runtime. The\ngoal is to convert a pretrained PyTorch model into the ONNX format and then\nseamlessly integrate it into a Vue.js application.\n\nFor this project, I've chosen the Self-Calibrated Illumination (SCI) model\nfrom the paper Toward Fast, Flexible, and Robust Low-Light Image Enhancement\nby Ma et al. he reasons are twofold: the authors have generously provided the\npretrained model and code on their Github Repository, and the model is\nremarkably compact, approximately 44KB in size, ensuring swift performance in\nthe browser.\n\n## Transforming the Model to ONNX\n\nIn the previous sections, we discussed how the HuggingFace TransformersJS\nlibrary leverages the ONNX runtime for executing models on the web. But what\nis the ONNX runtime exactly? According to the ONNX website, it is described\nas:\n\n> ONNX Runtime is a cross-platform machine-learning model accelerator, with a\n> flexible interface to integrate hardware-specific libraries.\n\nThis means the ONNX runtime facilitates the execution and porting of machine\nlearning models across various platforms. The typical workflow for utilizing\nthe ONNX runtime is as follows:\n\n  1. Train a model in your favorite framework (e.g. PyTorch, TensorFlow, etc.)\n  2. Export the model to ONNX format\n  3. Load the model into the ONNX runtime of your choice ( ONNX Runtime for C#, ONNX Runtime Web, etc.)\n\nWith the low light image enhancement model already trained and the weights\nalready made available by Ma et al. the only thing left for us to do was to\nconvert the model to the ONNX format.\n\npython\n\n    \n    \n    import onnxruntime as ort import torch import torch.onnx import model # Is a pytorch nn.Module model.eval() scripted_model = torch.jit.script(model) dummy_input = torch.randn(1, 3, 1000, 1000) dynamic_axes = { \"input\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}, \"output\": {0: \"batch_size\", 2: \"height\", 3: \"width\"}, } torch.onnx.export( scripted_model, dummy_input, \"model.onnx\", export_params=True, verbose=True, input_names=[\"input\"], output_names=[\"output\"], dynamic_axes=dynamic_axes, )\n\nLet's examine the code in detail. After importing all the necessary libraries,\nwe first prepare the model:\n\npython\n\n    \n    \n    model.eval() scripted_model = torch.jit.script(model)\n\nThe model.eval(), puts the model in evaluation mode, which is crucial for\nexporting and inference. It turns off certain layers like dropout and batch\nnormalization that behave differently during training. The\ntorch.jit.script(model) function converts the model to a TorchScript\nrepresentation. This step is particularly useful for complex models with\ndynamic control flows (like loops and conditionals) which might not be\ndirectly exportable to ONNX.\n\nOnce the model is prepared, we generate a dummy input and specify the axes in\nthe ONNX model that we want to be dynamic. For our purposes, these are the\nbatch size, height, and width. By doing this, we enable the model to process\nimages of any size and batch size.\n\nIn the last step we export the model to the ONNX format using the\ntorch.onnx.export function.\n\n## Integrating the ONNX Model into a Vue Application for Browser Execution\n\nWith the model now converted to the ONNX format, our next step is to integrate\nit into a Vue.js application for execution in the browser. While this process\nis straightforward, there are some potential pitfalls to be mindful of.\n\nThe first step involves installing the onnxruntime-web package, which will\nallow us to load and run the model.\n\njavascript\n\n    \n    \n    import * as ort from \"onnxruntime-web\"; const executionProviders = [\"wasm\"]; session = await ort.InferenceSession.create(\"./model.onnx\", { executionProviders, });\n\nNote\n\nA common problem that I and others have encountered is that the application\ncan not find the .wasm files that are needed to initialize the ONNX web\nruntime and execute the model when running the application with vite\napplication.\n\nThe solution that worked for me was to place the .wasm files from the\nonnxruntime-web package into the public folder of the vue application and then\nload the initialize the runtime with the following code:\n\njavascript\n\n    \n    \n    import * as ort from \"onnxruntime-web\"; ort.env.wasm.wasmPaths = { \"ort-wasm.wasm\": \"./ort-wasm.wasm\", \"ort-wasm-simd.wasm\": \"./ort-wasm-simd.wasm\", \"ort-wasm-threaded.wasm\": \"./ort-wasm-threaded.wasm\", };\n\nIf someone has a better solution to this problem please let me know.\n\n## Correctly Processing Input and Output for the Model\n\nOnce the model is successfully loaded, a crucial step remains: accurately\nprocessing and preparing the input. This involves normalizing the input data\nand transposing it into the format that the model expects, which is CHW\n(channel, height, width).\n\njavascript\n\n    \n    \n    export function getImageData(image) { const canvas = document.createElement(\"canvas\"); const ctx = canvas.getContext(\"2d\"); canvas.width = image.width; canvas.height = image.height; ctx.drawImage(image, 0, 0); return ctx.getImageData(0, 0, canvas.width, canvas.height); } export function normalizeAndTranspose(imageData, width, height) { const float32Data = new Float32Array(width * height * 3); const rArray = new Float32Array(width * height); const gArray = new Float32Array(width * height); const bArray = new Float32Array(width * height); for (let y = 0; y < height; y++) { for (let x = 0; x < width; x++) { const idx = (y * width + x) * 4; const i = y * width + x; rArray[i] = imageData.data[idx] / 255; gArray[i] = imageData.data[idx + 1] / 255; bArray[i] = imageData.data[idx + 2] / 255; } } float32Data.set(rArray); float32Data.set(gArray, rArray.length); float32Data.set(bArray, rArray.length + gArray.length); return float32Data; }\n\nAfter processing the input we can run our model using the ONNX runtime.\n\njavascript\n\n    \n    \n    const imageData = getImageData(image); const float32Data = normalizeAndTranspose(imageData, image.width, image.height); const tensor = new ort.Tensor(\"float32\", float32Data, [ 1, 3, image.height, image.width, ]); const output = await session.run({ input: tensor });\n\nAnd finally we need to transform the output back to the HWC format and de-\nnormalize it.\n\njavascript\n\n    \n    \n    export function transformAndTranspose(outputTensor, width, height, channels=3) { const transposedData = new Float32Array(width * height * channels) for (let h = 0; h < height; h++) { for (let w = 0; w < width; w++) { for (let c = 0; c < channels; c++) { transposedData[h * width * channels + w * channels + c] = outputTensor.data[c * height * width + h * width + w] } } } // Scale to [0, 255] and create Uint8ClampedArray for canvas const outimageData = new Uint8ClampedArray(width * height * 4) for (let i = 0; i < transposedData.length; i += channels) { for (let c = 0; c < channels; c++) { outimageData[(i * 4) / 3 + c] = Math.min( 255, Math.max(0, Math.round(transposedData[i + c] * 255)) ) } outimageData[(i * 4) / 3 + 3] = 255 // Alpha channel } return outimageData }\n\nThis was the last step and the only thing left for us to do is to display the\nresult. The full code can be found in the Github repository.\n\n## Conclusion\n\nIn this week's blog post, we explored the process of converting a PyTorch\nmodel into the ONNX format and demonstrated how to execute it in a browser\nusing the ONNX runtime. Additionally we have seen how to integrate the model\ninto a vue application and avoid the most common pitfalls.\n\n## References\n\n  * Toward Fast, Flexible, and Robust Low-Light Image Enhancement Github\n  * ONNX Runtime Docs\n  * ONNX Tutorial Web\n  * Pytorch Script\n\nCreated by M. Dennis Turp\n\n", "frontpage": false}
