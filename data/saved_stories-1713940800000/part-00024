{"aid": "40137124", "title": "The Serverless Illusion", "url": "https://architectelevator.com/cloud/serverless-illusion/", "domain": "architectelevator.com", "votes": 2, "user": "chriscbr", "posted_at": "2024-04-23 20:52:47", "comments": 0, "source_title": "The Serverless Illusion", "source_text": "The Serverless Illusion - The Architect Elevator\n\n# The Serverless Illusion\n\nAbstractions can become illusions. Is Serverless one of them?\n\n### Gregor Hohpe\n\nI help enterprises with their architecture strategy and cloud transformation\njourney by connecting the penthouse with the engine room. Ex-Google, Allianz,\nThoughtWorks, Deloitte.\n\n  * Singapore\n  * Feed\n  * Twitter\n  * LinkedIn\n\nUpdated: April 01, 2024 Updated: Cloud\n\nObviously, serverless technologies aren\u2019t an illusion. They are quite real.\nAWS kicked off serverless in 2014 with the launch of Lambda, which remains the\ncategory-defining service to date. Since then \u201cserverless\u201d has taken on a much\nbroader meaning and virtually every cloud provider offers serverless run-\ntimes, databases, and integration services. So where\u2019s the illusion?\n\n## Building Abstractions Instead of IllusionsPermalink\n\nAs I discussed in a prior post (which since expanded into a talk and a chapter\nin Platform Strategy), illusions happen when abstractions hide relevant\nconcepts and therefore set false expectations, for example, if a distributed\nsystem that is subject to latency and out-of-order delivery pretends to be\njust like a local system that doesn\u2019t exhibit these run-time challenges.\nDevelopers may enjoy their initial, seemingly simplified, experience until\ninevitably the illusion pops and things break without them having much of an\nidea why. In a way, software illusions are the antithesis of mechanical\nsympathy: they pretend that the system is something that it isn\u2019t and likely\ncan\u2019t be.\n\nSo, where does serverless fall into this trap? Doesn\u2019t it do a nice job\nabstracting away infrastructure, so you can focus on writing business logic\nand providing value instead of toiling in the operational engine room, doing\nundifferentiated heavy lifting? Before we reveal that part, let\u2019s take a tiny\nstep back and look at serverless tech in context.\n\n## Serverless is awesomePermalink\n\nI routinely pitch serverless as follows:\n\n> Serverless is the real cloud native way because it\u2019s how the cloud wants you\n> to build applications.\n\nServerless (specifically Functions-as-a-Service) provides the highest level of\nrun-abstraction: you write a function, deploy it, and invoke it. The\ninfrastructure figures out where your code (or image) should reside, which\nhost to run it on, and how to route requests to it. This process is so\nseamless that it\u2019s easy to forget how awesome it is (or perhaps how cumbersome\nit was before).\n\nMy first triumph with this kind of instant-deployment technology that allows\nyou to forget about servers was in 2009, a full 5 years before Lambda. During\na JavaOne panel on cloud computing, my fellow panelists showed slides\nexplaining how awesome the cloud is. I instead fired up an IDE, typed a few\nlines of code, and deployed them to a public URL, using Google App Engine at\nthe time. App Engine was a big step ahead (I still have my T-shirt), but it\nalso came with several constraints that restricted what kind of applications\nyou could run.\n\nSo, serverless is awesome. But we should also reflect how the higher-level\nrun-time abstraction influences the programming model, so we don\u2019t end up with\nillusions.\n\n## Run-Time IllusionsPermalink\n\nA function-as-a-service function isn\u2019t a like local method. It runs\ndistributed, invoked via a queue, under resource constraints that are managed\nby an auto-scaling mechanism. The best-known symptom of a cloud function is\ncold starts. Cold starts occur when no instances of the function are available\nand a new environment to execute the function has to start up. Cold starts\ncause latency spikes, increasing latency typically by a multiple despite\nclever approaches like Lambda SnapStart and GCP CPU Boost. If you use\nserverless functions for anything but demo apps, you should consider cold\nstarts. Yan Cui wrote an insightful post on Lambda cold starts several years\nago.\n\nCold start\u2019s ugly cousin is throttling, which happens when incoming requests\nexceed the number of available function instances. Invoking a function whose\nconcurrency is exhausted, results in a 429\u2013Too Many Requests exception. The\nexistence of a standard HTTP response status code hints that such an error\nisn\u2019t unique to serverless functions but fundamental behavior of distributed\nsystems. The mechanism behind that error, load shedding, is actually essential\nto handling high load situations gracefully. But it\u2019s also an error that local\nmethods don\u2019t have.\n\nAWS\u2019 recommended settings for managing cold starts and throttling are Reserved\nand Provisioned Concurrency, which roughly translates into the number of\ninstances a function can use under high traffic and the number of instances\nthat are always active even when traffic is low. The documentation explains in\ndetail how to manage these settings. Similarly, push subscriptions in GCP\nPub/Sub have a robust but also complex scaling behavior. Building such systems\nrequires a good amount of RTFM.\n\n## The Simplicity IllusionPermalink\n\nThe documentation on concurrency settings is excellent, but it\u2019s also dozens\nof pages. Polylith applications (my word creation of the day to denote\nsomething that\u2019s not a monolith but also not fine-grained until I found out\nthe term\u2019s already taken), perhaps deployed inside a container, don\u2019t require\nsuch run-time management (nor the associated documentation) to make a function\ncall.\n\nServerless applications favor fine-grained, distributed, and concurrent\narchitectures. While that gives them desirable operational characteristics\nlike resilience and auto-scaling, it also implies complex design and run-time\nconsiderations. Developers have to configure and integrate numerous components\nincluding access control at design-time, often with little language support.\nThey also have to account for out-of-order or duplicate message delivery and\nperform retry logic. At run-time, developers have to manage scaling, quotas,\nqueue sizes, dead-letter queues, poison messages, just to name a few.\n\nTo make things even more \u201cinteresting\u201d, not all the run-time elements are\ndirectly visible to the developer, for example invocation queues and retry\nlogic with exponential back-off. These \u201cbuilt-in\u201d elements increase system\nresilience but also influence system behavior like latency and therefore can\u2019t\nbe ignored.\n\nBuilding serverless solutions reduces operational effort to manage resources.\nBut coding such applications is complex as you\u2019re facing the realities of\nfine-grained, asynchronous systems. Now you could argue that without\nserverless you\u2019d have both: operational toil and the distributed complexities,\nbut in reality you\u2019d be less likely to build such a fine-grained architecture\nif you\u2019re managing infrastructure yourself.\n\nDoes serverless shift complexity from one layer to another?\n\nAs a reward for reading this far despite the click-bait title, you shall be\nrewarded with what I provocatively call the \u201cserverless illusion\u201d:\n\n> Serverless reduces the need for (readily available) ops skills but increases\n> the demand for (less readily available) distributed system design skills.\n\nVirtual machine and database operations isn\u2019t simple, but it\u2019s a task that\ntraditional IT teams have been carrying out for decades. Serverless\ndramatically reduces the need for such operational toil. But it requires deep\nknowledge of dynamic distributed system behavior and the related platform-\nspecific settings. That\u2019s a skill set that is much harder (or more pricey) to\ncome by.\n\n## Is Ops Really \u201cUndifferentiated\u201d?Permalink\n\nInfrastructure ops are \u201cundifferentiated\u201d because the time spent there could\nbe invested in building functionality that delivers satisfied users and\nbusiness value. That doesn\u2019t mean the ops heavy lifting isn\u2019t valuable\u2013if your\napp doesn\u2019t perform, users may never get to see that exciting functionality\nyou built. The main argument is that you can focus on application-level tasks\nby letting the cloud provider handle operations.\n\nBut here lies the catch. There are plenty of operational tasks left for\nserverless teams to tackle. They are just at a higher level of abstraction, so\nyou\u2019re dealing with a different set of problems. You could argue whether\ndealing with such distributed system complexity is more \u201cdifferentiated\u201d than\ndealing with infrastructure. Both are needed to maintain system availability,\nbut neither is building features.\n\n> A fine-grained, asynchronous application is likely more resilient and cost-\n> effective than a monolith running on VMs. But it may be harder to build and\n> operate.\n\n## What if I Want the Functionality but not the Distributedness?Permalink\n\nMost serverless setups provide additional functions, for example for event\nrouting and filtering, data streaming, APIs access, or higher-level services\nlike pre-trained machine learning models for image recognition. Having such an\necosystem is a strength of serverless cloud environments (and the reason I\nadvise to not evaluate cloud platforms by single features). But these\nfunctions are packaged in stand-alone services, requiring developers to form a\nvalid API request, handle error codes, and get IAM setup so that the call\nactually goes through. Once that\u2019s all done, they must configure logging and\nrelated functions.\n\nThis form of run-time composition has tremendous operational advantages, but\nit\u2019s more complex than design-time composition, which simply calls a library\nfunction. Sometimes I just want to run a set of predicates over a JSON object\nwithout having to configure an entire event router service and the associated\npermissions and limitations. Put another way:\n\n> What if I want the functionality but not the distributedness?\n\nThat might be the case because I am testing, or because my app has limited (or\npredictable) scalability needs. Using the mental model of an architect, I\u2019d\ncharacterize the challenge as follows:\n\n> Serverless marries functionality with the run-time model.\n\nIf I want to access specific behavior, I need to accept the associated run-\ntime model. This is a strong form of coupling between functionality and run-\ntime architecture that can cause complexity even in cases when it\u2019s not\ndelivering much value.\n\nThe inverse coupling ties the availability of functionality to specific run-\ntime characteristics, for example pull-vs-push control flow. For example, to\nuse an Enricher in AWS-land (I am using AWS as an example because I know it\nbetter), you have to use EventBridge Pipes , which implies a \u201cpull\u201d control\nflow. A \u201cpush\u201d control flow requires an EventBridge Bus, which doesn\u2019t support\nan Enrichment step. There are valid reasons underneath, but having to know the\ncontrol flow to use a piece of functionality doesn\u2019t make a developer\u2019s live\nsimpler.\n\nServerless makes you code to the run-time model, not the functionality\n\n## Can I Have my Serverless Cake and Eat it too?Permalink\n\nFolks have noticed that composing distributed systems in the cloud isn\u2019t\nparticularly easy, so we are starting to see efforts to improve the quality of\nlive of serverless developers. Approaches tend to fall into two categories:\n\n  * Simplify composition\n  * Reduce composition by allowing more coarse-grained components\n\nThe more fine-grained your system is, the more relevant the lines between the\ncomponents become. As explained in a previous post, making composition a\nfirst-class concern is paramount to reducing the complexity of coding\nserverless applications. This shift is at the core of my work on Architecture\nas Code.\n\n> If you want serverless to not be an illusion, code to the lines, not the\n> boxes!\n\nAzure Aspire is another approach to make describing the applications easier.\nDurable functions overcome the serverless run-time limits and look to simplify\nthe programming model. GCP increased the Cloud Functions run-time to 1h and\ninstance sizes to 16 GB to allow the serverless application architecture to be\na less fine-grained and hence easier to manage.\n\n## A monolithic programming model for distributed applicationsPermalink\n\nAn undesirable side-effect of programming to the run-time model is that you\nend up with a distributed programming model, which loads you up with a ton of\ncognitive load, aka \u201cmental heavy lifting\u201d. Detaching the programming model\nfrom the individual cloud services reduces the amount of infrastructure\ncomplexity that shifts to the programming model.\n\nThis approach was best summarized by Elad Ben-Israel (creator of CDK and\nfounder of Wing) during a Twitter/X discussion (that platform is apparently\nstill good for something):\n\n> I want a monolithic programming model for a distributed application\n\nFor fairness\u2019 sake, other folks are working in the same space, so I should\nmention Jeremy Daly\u2019s Ampt as an example. The core concept is to code your\napplication in a uniform code base that describes the application as a whole\nas opposed to a collection of individual services that are wired together\nthrough string values.\n\nI find this approach most promising with the key question being on what\nbaseline to build such a programming model. Winglang and Ampt start from a\nclean slate whereas my AaC efforts test how far we can get on top of existing\ntools like CDK.\n\nMost approaches utilize the type system of our automation languages to replace\nthe current stringly typed approach. Very little of the run-time behavior of\nthe current cloud services is represented in the type system. You can connect\nmost any resource to any other, and they will deploy fine, but they won\u2019t run,\nat least not in the sense that you\u2019d expect. Finding this out at run-time is\nthe antithesis of a delightful developer experience.\n\n## Simple vs. Intuitive vs. AIPermalink\n\nAside from the technical choices, we can utilize three different levers to\nmake serverless development delightful:\n\n  1. Make it simpler\n  2. Make it more intuitive\n  3. Let GenAI deal with it\n\nDistributed system development isn\u2019t simple. Most attempts to make it look\nsimple end up being illusions or forfeit essential properties such as\nconcurrent processing. But I feel that distributed system development in a\nserverless context can be much more intuitive. Intuitive programming models\ngenerate a system that behaves the way a developer would expect (without\nhaving to read reams of documentation.)\n\nOn the other end of the spectrum, GenAI can make us more productive in\novercoming these hurdles, but it\u2019s still largely up to us to define the\nunderlying model on which the code operates, e.g. a shift from resource\nhierarchies to control flow.\n\n## Who will lead the pack?Permalink\n\nIt can take time for platform companies to fully internalize what they have\nactually built. We are now seeing a lot of activity and innovation in this\nspace, driven by start-up companies as opposed to the traditional cloud\nproviders. From the latter, Azure appears to be most keen to simplify the\ndeveloper experience for serverless systems. Still, it\u2019s a valid question\nwhether cloud providers are better equipped to build robust and scalable run-\ntimes as opposed to stellar developer experiences:\n\n> A shift in run-time model implies a shift in programming model. Driving\n> innovation in both areas requires a diverse skill set and focus.\n\nIf major cloud providers don\u2019t drive innovation in this area, they may\nultimately be commoditized as run-time providers with developers choosing the\nbest programming model as opposed to choosing the best run-time.\n\nA strategy doesn't come from looking for easy answers but rather from asking\nthe right questions. Cloud Strategy, loaded with 330 pages of vendor-neutral\ninsight, is bound to trigger many questions as it lets you look at your cloud\njourney from a new angle.\n\nShare on Twitter Facebook LinkedIn\n\n### Make More Impact as an Architect\n\nMy book The Software Architect Elevator helps architects and IT professionals\nplay at the intersection of technology, organization, and transformation by\nsharing the real-life journey of a chief architect. Buy it on Amazon US,\nAmazon UK, Amazon Europe\n\n#### You may also enjoy\n\n## Spooky: Platform Strategy Awakens\n\nPosted: Oct 31, 2023\n\nWhat better day to launch The Grim Wrapper than Halloween?\n\n## IxC: Infrastructure as Code, from Code, with Code\n\nPosted: Aug 15, 2023\n\nToday\u2019s cloud automation goes far beyond provisioning servers. Let\u2019s apply\narchitecture models to the latest trends.\n\n  * Follow:\n  * Twitter\n  * Feed\n  * CONTACT:\n  * LinkedIn\n  * E-MAIL\n\n\u00a9 2024 Gregor Hohpe. All opinions my own.\n\n", "frontpage": false}
