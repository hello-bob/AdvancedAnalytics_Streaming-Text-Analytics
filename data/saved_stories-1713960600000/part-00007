{"aid": "40139824", "title": "Overcoming the Pitfalls of Vision-Language Model Finetuning \u2013 OOD Generalization", "url": "https://machinelearning.apple.com/research/overcoming-pitfalls-vision-language", "domain": "machinelearning.apple.com", "votes": 1, "user": "zerojames", "posted_at": "2024-04-24 02:32:29", "comments": 0, "source_title": "Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization", "source_text": "Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD\nGeneralization - Apple Machine Learning Research\n\nresearch areaComputer Vision, research areaMethods and Algorithms | conference ICLR\n\ncontent type paper | published April 2024\n\n# Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD\nGeneralization\n\nAuthorsYuhang Zang, Hanlin Goh, Josh Susskind, Chen Huang\n\nView publication\n\nView source code (GitHub)\n\nExisting vision-language models exhibit strong generalization on a variety of\nvisual domains and tasks. However, such models mainly perform zero-shot\nrecognition in a closed-set manner, and thus struggle to handle open-domain\nvisual concepts by design. There are recent finetuning methods, such as prompt\nlearning, that not only study the discrimination between in-distribution (ID)\nand out-of-distribution (OOD) samples, but also show some improvements in both\nID and OOD accuracies. In this paper, we first demonstrate that vision-\nlanguage models, after long enough finetuning but without proper\nregularization, tend to overfit the known classes in the given dataset, with\ndegraded performance on unknown classes. Then we propose a novel approach OGEN\nto address this pitfall, with the main focus on improving the OOD\nGENeralization of finetuned models. Specifically, a class-conditional feature\ngenerator is introduced to synthesize OOD features using just the class name\nof any unknown class. Such synthesized features will provide useful knowledge\nabout unknowns and help regularize the decision boundary between ID and OOD\ndata when optimized jointly. Equally important is our adaptive self-\ndistillation mechanism to regularize our feature generation model during joint\noptimization, i.e., adaptively transferring knowledge between model states to\nfurther prevent overfitting. Experiments validate that our method yields\nconvincing gains in OOD generalization performance in different settings.\n\n## Related readings and updates.\n\n### Uncertainty Weighted Actor-Critic for Offline Reinforcement Learning\n\nOffline Reinforcement Learning promises to learn effective policies from\npreviously-collected, static datasets without the need for exploration.\nHowever, existing Q-learning and actor-critic based off-policy RL algorithms\nfail when bootstrapping from out-of-distribution (OOD) actions or states. We\nhypothesize that a key missing ingredient from the existing methods is a\nproper treatment of uncertainty in the offline setting. We propose\nUncertainty...\n\nSee paper details\n\n### Generating Synthetic Images by Combining Pixel-level and Feature-level\nGeospatial Conditional Inputs\n\nTraining robust supervised deep learning models for many geospatial\napplications of computer vision is difficult due to dearth of class-balanced\nand diverse training data. Conversely, obtaining enough training data for many\napplications is financially prohibitive or may be infeasible, especially when\nthe application involves modeling rare or extreme events. Synthetically\ngenerating data (and labels) using a generative model that can sample from\na...\n\nSee paper details\n\n## Discover opportunities in Machine Learning.\n\nOur research in machine learning breaks new ground every day.\n\nWork with us\n\nPrivacy Policy Terms of Use Legal\n\nCopyright \u00a9 2024 Apple Inc. All rights reserved.\n\n", "frontpage": false}
