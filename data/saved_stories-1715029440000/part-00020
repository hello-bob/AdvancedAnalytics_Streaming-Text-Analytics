{"aid": "40274996", "title": "Mantis: Interleaved Multi-Image Instruction Tuning", "url": "https://tiger-ai-lab.github.io/Mantis/", "domain": "tiger-ai-lab.github.io", "votes": 1, "user": "Anon84", "posted_at": "2024-05-06 14:19:04", "comments": 0, "source_title": "Mantis", "source_text": "Mantis\n\n# Mantis: Interleaved Multi-Image Instruction Tuning\n\n### Balancing Multi-Image and Single-Image Abilities of Large Multimodal\nModels\n\nDongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, Wenhu Chen\n\n\u25b6 University of Waterloo \u25b6 Tsinghua University \u25b6 Sea AI Lab\n\nPaper Code Mantis-Instruct Mantis-Eval \ud83d\udd25 Models \ud83e\udd17 Spaces Twitter\n\n## Abstract\n\n\ud83e\udd14 The recent years have witnessed a great array of large multimodal models\n(LMMs) to effectively solve single-image vision language tasks. However, their\nabilities to solve multi-image visual language tasks is yet to be improved. \ud83d\ude26\nThe existing multi-image LMMs (e.g. OpenFlamingo, Emu, Idefics, etc) mostly\ngain their multi-image ability through pre-training on hundreds of millions of\nnoisy interleaved image-text data from web, which is neither efficient nor\neffective.\n\n  1. Mantis-Instruct Data. We present the first fully text-image interleaved multimodal instruction tuning dataset, containing 721K examples from 14 subsets and covering multi-image skills including co-reference, reasoning, comparing, temporal understanding\n  2. Mantis Model. We introduce Mantis, an LLaMA-3 based LMM with interleaved text and image as inputs, train on Mantis-Instruct under academic-level resources (i.e. 36 hours on 16xA100-40G)\n  3. Performance. Mantis reaches the state-of-the-art performance on five multi-image benchmarks (NLVR2, Q-Bench, BLINK, MVBench, Mantis-Eval), and also maintain a strong single-image performance on par with CogVLM and Emu2.\n  4. Open-source. Our curated Mantis-Instruct data, along with the training/evaluation codes, model checkpoints are all released to the public\n\n## Interleaved Multi-Image Instruction-Tuning data\n\n  * Mantis-Instruct has a total of 721K instances, consisting of 14 subsets to cover all the multi-image skills.\n  * Among the 14 subsets, 10 subsets are from the existing datasets. For example, NLVR2, IconQA, etc for reasoning skill; DreamSim, Birds-to-Words, etc for comparison skill; NExT-QA, STAR, for temporal understanding\n  * We additionally curate four new datasets LLaVA-665k-multi, LRV-multi to cover coref skill and Contrast-Caption, Multi-VQA to broaden reasoning skill, where Multi-VQA is generated by prompting GPT-4.\n  * Please check out \ud83e\udd17 Mantis-Instruct on hugging face datasets for usage.\n\nSubset| Multi-Image Skill| Sample Size  \n---|---|---  \nLLaVA-665k-multi| Coref| 313K  \nLRV-multi| Coref| 8K  \nNLVR2| Reason| 86K  \nIconQA| Reason| 64K  \nContrast-Caption| Reason| 36K  \nImageCoDe| Reason| 17K  \nMulti-VQA| Reason| 5K  \nCo-Instruct| Compare| 151K  \nDreamsim| Compare| 16K  \nSpot-the-Diff| Compare| 8K  \nBirds-to-Words| Compare| 3K  \nVIST| Temporal| 7K  \nNExT-QA| Temporal| 4K  \nSTAR| Temporal| 3K  \n  \n## Mantis:\n\nMantis applies the LLaVA's architecture, using CLIP/SigLIP as vision encoders\nand Meta-Llama-3-8B-Instruct as language model. To support super-resolution,\nwe also train a variant based on SigLIP and Fuyu-8B. We consider a two-stage\ninstruction-tuning procedure: connects pre-trained CLIP ViT-L/14 visual\nencoder and large language model Vicuna, using a simple projection matrix. We\nconsider a two-stage instruction-tuning procedure: Please check out our [Model\nZoo].\n\n## Performance\n\n## Multi-Image VQA: Towards GPT-4-level multi-image understanding\n\nBenchmark| Multi-Image Skill| Held-in/Held-out  \n---|---|---  \nNLVR2| Reason| Held-in  \nQ-bench| Reason| Held-in  \nMantis-Eval| Reason & Co-reference| Held-out  \nBLINK| Reason| Held-out  \nMVBench| Temporal| Held-out  \n  \nWe select 5 multi-image benchmarks that cover the four crucial multi-image\nskills: co-reference, reasoning, comparing, temporal understanding to evaluate\nMantis.\n\nEvaluation on 5 benchmarks, including NLVR2, Q-Bench, BLINK, MVBench, Mantis-\nEval, shows that Mantis achieves the state-of-the-art performance. It\ndemonstrates that Mantis effectively learn the 4 crucial multi-image skills\n(co-reference, reasoning, comparing, temporal understanding) from the\ninterleaved text-image instructions dataset, Mantis-Instruct. We have\nsurpassed the second best Idefics2-8B (pre-trained on 140M interleaved image-\ntext data) by an average of 9 absolute points, and is only behind GPT-4 by 2\npoints.\n\n## Single Image VQA: Maintain strong performance\n\nWe also evaluate Mantis-8B-CLIP and Mantis-8B-SigLIP on various single-image\ntasks, including TextVQA, VQA-v2, MMBench, MMMU, etc. Mantis models reach on-\npar average performance with CogVLM, and Emu2-Chat\n\n## Examples on Visual Instruction Following\n\n## BibTeX\n\n    \n    \n    @inproceedings{Jiang2024MANTISIM, title={MANTIS: Interleaved Multi-Image Instruction Tuning}, author={Dongfu Jiang and Xuan He and Huaye Zeng and Cong Wei and Max W.F. Ku and Qian Liu and Wenhu Chen}, publisher={arXiv2405.01483} year={2024}, }\n\n", "frontpage": false}
