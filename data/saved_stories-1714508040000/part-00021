{"aid": "40210237", "title": "DevOps \u2013 The Simple Version", "url": "https://tabaza.dev/Blog/DevOps---The-Simple-Version", "domain": "tabaza.dev", "votes": 1, "user": "Tabz98", "posted_at": "2024-04-30 12:20:32", "comments": 0, "source_title": "DevOps - The Simple Version", "source_text": "DevOps - The Simple Version\n\n# Mohammad Tabaza\n\nSearch\n\n  *     *       * Do Not Re-Invent the Wheel\n      * Mindless Learning\n      * How two WhatsApp Chat Bots had a Fight\n\n    *       * DevOps - The Simple Version\n      * Functional Programming - The Simple Version\n      * A Guide to Scala 3\n      * The Scala Collections Library\n      * Parsing The World with Rust and POM\n      * Machine Learning - From Zero to Slightly Less Confused\n      * RxJS From Scratch\n      * FeathersJS For The Beginner - Hello World!\n      * FeathersJS For The Beginner - Basic concepts\n\n# DevOps - The Simple Version\n\nApr 26, 202420 min read\n\nDevOps has gotten to the point where it\u2019s not even easy to define. The field\nhas a lot of history, and with it comes scary terms like \u201cContinuous\nIntegration\u201d, \u201cContainer Orchestration\u201d, and worse. I was completely lost when\nI dove into the subject a few years ago, so I thought I\u2019d make it easier for\nthose after me to wrap their head around what DevOps engineers do, and how.\n\nThis post gives an overview of DevOps, its common practices, problems it\nsolves, and the most popular tools used to solve these problems. Whether\nyou\u2019re a fresh grad pursuing a career in DevOps, a hacker looking for the\nright tool, or an expert in need of a refresher, this post is for you.\n\n# Development\n\nAny software engineer (I hope) should be able to write code, run it, version\nit with Git, and ultimately submit it for production release to be available\nto end-users.\n\nAfter the code is released, a bug is reported. It gets assigned to a hapless\nengineer who fixes it by writing code, testing it, committing it to Git, and\nopening a pull request to be merged and released. The engineer is then free to\nstart working on a new feature, which involves writing code, running it,\npushing the changes, and opening a PR. This is the development cycle.\n\nThis is what software developers do: write code to produce software\napplications that automate business. However, it turns out that developers\nalso need to automate their own work as well.\n\nMany software production tasks must be automated in order to shorten the time\nit takes to complete the development cycle. Not only to maintain the sanity of\ndevelopers, but also to maximize their productivity. The faster working code\ngets shipped to end-users, the better.\n\nOne such process that should be automated is building executable programs,\nwhich you, as a developer, do multiple times every day. A build tool is the\nprogram that compiles all your code files and external libraries and glues\nthem all together to produce an executable application, ready for you to run\nduring development, or send it to a user.\n\nNow, imagine that every time you wanted to run your code after editing it, you\nhad to manually compile every file or module, and then glue the results\ntogether to form an executable program. And of course, that wouldn\u2019t work the\nfirst few hundred times because you\u2019ve messed up the order in which you\ncompiled your modules, or you tried to skip files you\u2019ve already compiled only\nto realize that you\u2019ve modified one of their dependencies and you actually\nneeded to re-compile the code with the new dependencies.\n\nWithin a week, you would either go insane, or get fired for getting nothing\ndone.\n\nThankfully, developers before us have already walked this minefield and\ncleared the path for us. They created build tools like Maven for Java, or\nWebpack for JavaScript. They automated the process of correctly compiling and\ngluing things together to produce a runnable app.\n\nFast-forward a few decades and you\u2019ll find that developers didn\u2019t stop at\nautomating builds, but kept automating other tedious and error-prone tasks.\nHowever, these tasks significantly grew in complexity with the growing demand\nfor more scalable systems and processes.\n\nBusinesses now have more users. Users now demand more, so businesses hire more\ndevelopers. Developers need to keep up with the increasing needs of users, so\nthe needs of developers increase as well. More servers, more storage, more\ncomplex software architectures, and all of it needs to be reliable, fast,\ncost-effective, and secure.\n\nEven with all this added complexity, developers are still expected to deliver\nquickly. Developers needed to specialize. Some should handle the needs of\nusers, while some should handle the needs of their fellow developers.\n\nEnter the DevOps engineer: the engineer whose job it is to ensure that\noperations run smoothly. And I know \u201coperations\u201d and \u201csmoothly\u201d are broad\nterms, so hang on.\n\n# CI/CD\n\nSo, you\u2019ve built your app using your fancy build tool, you ran and tested it,\npushed your changes, and opened a PR. Now what? Well, in some companies, this\nis where you would wait for your changes to be approved by someone else. When\nyou get a green light, you merge and deploy the code, i.e. you run a script or\nclick a few buttons to upload your program to a server somewhere, where it\u2019ll\nbe accessible to end-users.\n\nBetter yet, in some companies, you wouldn\u2019t even need to run that script or\nclick any buttons; instead, your code is shipped to end-users automatically\nwhen it is approved and merged. This automation is called Continuous Delivery,\nor CD for short.\n\nSome also take the opportunity to run automated tests automatically when code\nis pushed, so that the reviewer (the one to approve the changes) would see\nthat the new code mostly works and doesn\u2019t break any existing code. It\u2019s also\nnice to keep checking changes against upstream code to make sure merge\nconflicts are resolved as soon as possible. This automation is called\nContinuous Integration, or CI.\n\nSome take automation even further and deploy a preview environment for every\npull request opened. The reviewer (or otherwise, any stakeholder) can go see\nthe new code in action, without having to build the app themselves to run it\non their machines. They just go to a link where the new version of the app is\nhosted, and play with it until they\u2019re satisfied with the work. Once the PR is\nmerged, the preview environment is safe to delete entirely in order to save\ncosts.\n\nIn such companies where all this automation is implemented, it\u2019s likely that\nyou\u2019ll find good DevOps engineers. A DevOps engineer\u2019s job is to help\ndevelopers focus on writing code. Part of the job is to completely automate\nthe deployment workflow. The developer isn\u2019t really required to learn how to\nget code from the Git repo into the hands of end-users.\n\nIt\u2019s enough that a developer knows how to correctly implement business\nrequirements. Developers focus on user-facing features, while DevOps focus on\ndeveloper-facing features. Developers don\u2019t need to waste time deploying code\nto test environments for stakeholders.\n\n## Tools\n\nCI/CD workflows can get complicated very quickly. You need to trigger them on\nspecific events, like merging a PR into the main branch. Probably, you\u2019ll need\nto talk to multiple cloud services and SaaS to get the app deployed. Most\ntasks you\u2019ll perform take a long time to run, and some of them depend on one\nanother to complete. Some tasks can be performed in parallel to speed up the\nworkflow. Some tasks are asynchronous, but you still need to make sure they\u2019ve\ncompleted successfully.\n\n> We\u2019ll talk about Docker in a bit.\n\nThe worst thing you can do when setting out to automate your CI/CD is to\ncreate an empty file and start writing a bash script. Please, don\u2019t re-invent\nthe wheel. Most people use workflow orchestration tools to solve these\nproblems.\n\nMany workflow orchestration tools let you specify your workflows in YAML\nfiles, and many have CI/CD-specific features. Simple, yet extremely valuable\nforms of CI/CD can be achieved for free using GitHub Actions. There are also\nopen source solutions like Jenkins or GitLab that you can self-host. And of\ncourse, giant cloud providers offer their own tools, like Google\u2019s Cloud\nBuild, AWS CodePipeline, or Azure Build.\n\n# Infrastructure\n\nYour server-side apps need to run on servers - actual machines with CPUs and\nmemory. You also need databases, file storage for uploads, and probably much\nmore. Most people use the cloud these days. Providers like Google, Amazon,\nMicrosoft and many others let you pay them in exchange for allowing you to use\ntheir computing resources.\n\nHowever, the services that these clouds provide do not stop at lending you\nbare metal machines. It\u2019s too much work for you to manage a bunch of machines\ndirectly when all you want is to deploy an application with a database and\nsome file storage. Instead, they provide managed services that take a lot of\nload off your back, especially if you barely know what you\u2019re doing.\n\nFor example, you can use a managed database service that automatically takes\ncare of data backups and encryption, and increasing disk sizes according to\nyour usage, without you having to worry about running out of space. A managed\ndatabase service might also take care of scaling the compute and memory of\nyour database according to load.\n\nYou should be careful when using managed services though. Some of them still\nrequire considerable knowledge to use effectively and efficiently, since these\nservices automatically increase the resources you\u2019ll end up paying for. Read\nthe docs and understand the pricing very well.\n\n## Tools\n\nThe list of cloud computing platforms is endless, but there\u2019s a few really big\nones:\n\n  * Google Cloud Platform (GCP)\n  * Amazon Web Services (AWS)\n  * Microsoft Azure\n\nThese provide everything you\u2019ll need to build almost anything, no matter how\nlarge or small. However, capitalism has its way, and many smaller cloud\nproviders compete with the giants:\n\n  * DigitalOcean\n  * Cloudflare\n  * Hetzner\n  * Linode\n  * Vercel\n\nAnd many, many others. Each has its own strengths and weaknesses. Pricing is a\nfactor, but the services each provider offers could be the primary reason to\npick one provider over the other.\n\nThe big providers have the advantage of offering a comprehensive, cohesive\nsuite of products and services that integrate well together, while smaller\nproviders compete with lower prices (Hetzner and DigitalOcean) or incredible\ndeveloper experience (Vercel). In the end, you\u2019ll need to study what each\nprovider offers to best fit your needs.\n\nCloud providers usually have a Web interface that you can use to ask for their\nservices. You\u2019ll think I\u2019m crazy at first, but I think you should not use the\nUI to provision your cloud infrastructure.\n\nI do agree that it\u2019s kind of magical to simply log into your cloud provider\u2019s\nUI, click a few buttons, and now you have a managed database up and running.\nIt\u2019s quite convenient, sure. But:\n\n  * What if you end up using multiple cloud providers?\n  * What if you end up having to provision lots of databases? Do you wish to click through the same wizard a hundred times?\n  * What if you click around and everything falls to pieces? How would you know what you did in order to undo it? Once you do know what you did wrong, is it easy to undo?\n  * What if the UI is AWS crap?\n  * What if you need to replicate your production infrastructure for staging? How do you keep them in sync?\n  * What if someone else needs to review the changes you want to make to the infrastructure?\n\nAll the above problems can be solved if you define all your infrastructure as\ncode. You write files that describe what you need from the cloud provider, and\nyou let a tool talk to the cloud provider and see what it needs to do to make\nwhat you\u2019ve specified in the code match what is actually provisioned.\n\nThe most popular and well-supported Infrastructure as Code tool is Terraform.\nAn open-source drop-in replacement to Terraform is OpenTofu.\n\n# Deployment\n\nSo how does your code get from the Git repository to the server where it runs?\nHow and when does it start running? How do you make sure it will restart if it\ncrashes? How do you make sure there\u2019s no downtime when deploying new versions\nof your app? Let\u2019s answer these questions one by one.\n\n## Containers\n\nIf you were thinking of SSH-ing into your server and cloning your repo, or\nuploading your code to the server via FTP, don\u2019t. There are much better ways\nthese days. You\u2019ve likely heard about Docker by now, but how does it help us\nget our code to the server?\n\nDocker is known as a container runtime, and it also provides tools for\nbuilding, storing, and distributing container images. Practically, a container\nis a process that can run on any machine as-is. A container image is a\ntemplate that describes a container. You can use an image to create as many\ncontainers as you wish, similarly to how you can have a class and create\nmultiple instances from it in object-oriented programming.\n\nWhat\u2019s nice about using Docker for deployments is that a Docker image contains\neverything your code needs to run, so you only need your server to have Docker\ninstalled, and then you can run any Docker image you want without installing\nanything else. For example, if you have a NodeJS app, you build an image that\ncontains both the NodeJS runtime and your application code that runs on top of\nthe NodeJS runtime..\n\nWhen you finally have an image for your app, you can upload it to one of many\ncontainer registries, which are services that store your images. Once the\nimage is stored in a registry, you can easily pull it to your server and run\nit.\n\nDocker has become the industry standard for packaging server-side apps. You\nshould really get comfortable with it. Not only to package your own apps, but\nalso to more easily run other people\u2019s apps, be they apps built by other teams\nin your company, or open-source apps built by the developer community.\n\n### Tools\n\nYou can upload your Docker images to one of these registries:\n\n  * DockerHub\n  * GitHub Packages\n  * Google Artifact Registry\n  * AWS ECR\n  * Azure Container Registry\n\n## Container Orchestration\n\nYou now have an easy way to get your apps to servers, but how do you run them?\nThe naive way would be to SSH into the server and run docker run to start your\napp, and when there\u2019s a new version of your app, you SSH into the server\nagain, kill the old container, and run the new image.\n\nThis way of doing things is simple, sure. But it has its issues:\n\n  * It introduces downtime when you kill the old container, and if the new container fails to start, there\u2019s even more downtime until you get the old one running again\n  * It becomes unmanageable if your app runs on multiple servers (more on this shortly)\n\nAnd to complicate things further, you shouldn\u2019t even run your apps directly\nwith docker run for many reasons, including the fact that Docker on its own is\nnot enough to ensure that your app stays alive - it has no notion of health\nchecks, and it\u2019s not enough for managing a distributed application (an app\nthat runs on multiple servers).\n\nNow here\u2019s where you (the DevOps engineer) need to make a choice: can the\nbusiness tolerate having a bit of downtime on every new deployment? Does the\nbusiness not have enough traffic to warrant having multiple servers to handle\nthe load? If the answer is yes, you can probably get away with a very simple\ndeployment setup. But it\u2019s too often the case that the needs of the business\nrequire elaborate setups.\n\n### Tools\n\nIn order to satisfy your business\u2019s availability and scalability requirements\nwhen it comes to managing deployments, you\u2019ll likely find yourself reaching\nfor a container orchestration tool. These tools manage your containers for\nyou.\n\nFor instance, there are tools that keep probing your app to see if it\u2019s still\nhealthy and responding as expected. If not, the tool kills the app, runs a new\ncontainer, and alerts you to the problem. The same tool can also manage\nmultiple instances of your app running on multiple servers, which can ensure\nthat there\u2019s always at least one server ready to serve requests at all times\nif all other servers crash. This tool would also help you with gradual\nrollouts, meaning that when you have a new version of your app, the tool stops\nsending traffic to the old version of the app only after it makes sure that\nthe new version is up and running, ensuring zero downtime on each deployment.\n\nThe simplest, but possibly the most costly solution for container\norchestration is to use a cloud service that simply takes your Docker images,\nand runs them. It\u2019s that easy. You don\u2019t need to know anything about the\nactual server where your container runs. These services are known as CaaS, or\nContainers as a Service. Because you don\u2019t need to know anything about servers\nto use these services, they\u2019re often called serverless. Most of these services\nhave a decent free tier.\n\nHere are some managed cloud services that can run your containers:\n\n  * Fly.io\n  * Render\n  * Cloudflare\n  * Heroku\n  * DigitalOcean Docker Hosting\n\nAnd of course, the giants have their own solutions:\n\n  * Google Cloud Run\n  * AWS AppRunner\n  * Azure Container Apps\n\n> Some serverless hosting services can host your applications without even\n> requiring you to package your app in a Docker image, such as AWS Lambda,\n> Google Cloud Functions, Azure Functions, Cloudflare Workers, and Vercel.\n> However, support for programming languages and frameworks is limited. With\n> containers, you have the freedom to use any language or framework, at the\n> cost of having to build the Docker image yourself.\n\nThere also exist many open source solutions out there, and you\u2019ve probably\nheard of them by now. Kubernetes (k8s for short) is currently the most popular\ncontainer orchestration framework, with a huge, vibrant ecosystem of tools\nbuilt around it. With that said, you probably shouldn\u2019t be using Kubernetes on\nits own, but rather leverage the tools built on top of it.\n\n> If you\u2019ve never deployed an app yourself before, don\u2019t use Kubernetes. Stick\n> to managed cloud services instead.\n\nHere are some tools and frameworks built on top of Kubernetes that can\ndramatically simplify deployments:\n\n  * Knative - the framework that Google Cloud Run uses under the hood\n  * OpenFunction for serverless functions\n  * Kubeflow for machine learning workflows\n\nKubernetes, and all projects mentioned above are in some way part of the Cloud\nNative Computing Foundation (CNCF). Any CNCF project is worth checking out,\nsince these projects are sure to be open-source, well documented, and well\nmaintained. These projects are also likely to be foundational and extremely\nvaluable.\n\nThe term \u201cCloud Native\u201d likely means that the thing works very well with\nKubernetes, since Kubernetes is synonymous with open cloud computing these\ndays. This is how important and foundational Kubernetes is.\n\nTo make working with Kubernetes more familiar for developers, there are many\ntools that help you manage Kubernetes using Git, like you would manage any\nother code. These tools follow GitOps principles, which basically aim to apply\nsoftware engineering best practices to DevOps. The two most popular GitOps\ntools are Argo CD and Flux.\n\n# Monitoring\n\nWhen your app is deployed and running in production, you aught to keep an eye\non it to see how it\u2019s doing. Is it throwing errors in response to specific\nuser inputs? Is it consuming too much memory? Is it still fast? And if my app\nruns on multiple servers, how do I trace requests between them?\n\nThe first thing we all learn as programmers is how to print \u201cHello World\u201d to\nstandard output. We also often use prints to debug our applications during\ndevelopment. Printing is sometimes very useful in production as well, error\nmessages and stack traces in particular. Logging things to standard output (or\nfiles) is one of the simplest ways for us to observe what happened inside our\napplication.\n\nIn a production environment, logs should be stored somewhere for us to revisit\nwhen something goes wrong, or to extract insights about our app. So it\u2019s\nuseful to think about logs as data that should be stored in a database.\n\nLogs are great, and everyone should use them, but they\u2019re not great for\neverything. They won\u2019t help you if you\u2019re trying to know how much memory your\napp is using. Many argue logs aren\u2019t even great for measuring any type of\nmetric, even though it\u2019s possible. You can measure the number of times an HTTP\nendpoint was called by counting the number of times a particular log appeared,\nbut you might soon find that things got complicated.\n\nIn any case, if logs alone serve your needs well, that\u2019s good. But it\u2019s often\nthe case that you need more precise types of data.\n\nThere are three common types of data that helps you know what\u2019s going on with\nyour deployed applications: logs, metrics, and traces. These three types of\ndata are often called telemetry data, and the process of making your apps\nproduce all this data is called instrumentation.\n\nLogs and metrics are widely known, but what are traces? In short, traces are\nsimilar to regular stack traces in programming, but they can be distributed\nacross different machines and applications. You can also store useful data in\ntraces to ease debugging, or to make certain traces easy to find.\n\n## Tools\n\nSo how do you collect, store, and access all your telemetry data? The good\nnews is that you\u2019re well covered by existing tools. The bad news is that you\nmight need to set it all up as a DevOps engineer.\n\nIf you\u2019re using a managed cloud service to host your app, you probably don\u2019t\nneed to do anything to get the essentials. Most serverless hosting services\nstore your logs and let you query them, and they also give you basic metrics\nfor HTTP requests and resource utilization.\n\nEven if you\u2019re not using a managed service to host your application, you can\nstill use managed cloud services to monitor your application. New Relic and\nGrafana Cloud are pretty popular APM (Application Performance Monitoring)\nplatforms. There are also open-source APMs like:\n\n  * Sentry\n  * SigNoz\n  * Highlight.io\n  * HyperDX\n\nYou\u2019re also covered with the giants:\n\n  * Google Cloud Monitoring and Logging\n  * AWS CloudWatch\n  * Azure Monitor\n\nHowever, shipping telemetry data to managed service could be annoying to set\nup. You may need to run and manage an agent that collects data from your\napplications and machines, and ships it to the telemetry service to be stored.\nYou also need to make sure your app exports telemetry data in the format that\nthe service provider expects.\n\nOpenTelemetry is a CNCF project that standardizes telemetry data formats and\ntransfer protocols, and it\u2019s widely supported by cloud service providers. The\nbest way to instrument your applications is to make them export OpenTelemetry\ndata using the OpenTelemetry SDK for your languages. This helps you avoid\nchanging code if you ever need to change your telemetry service provider, and\nsaves you a lot of time in the future when you want to integrate new\napplications with new providers, since you\u2019ve already learned how to use\nOpenTelemetry.\n\nIf you\u2019re planning on hosting your own application observability\ninfrastructure, there are many open source solutions available. The open-\nsource solutions mentioned above are all-in-one solutions that you can self-\nhost, but you can get more granular depending on your needs. Be warned though,\nmanaging these things in production is risky if you don\u2019t know what your\u2019re\ndoing:\n\n  * Elastic Stack or Grafana Loki for collecting logs\n  * Prometheus or Victoria Metrics for collecting metrics\n  * Grafana Tempo or Jaeger for collecting traces\n  * Grafana for querying and visualizing telemetry data from any of the above sources\n\nHosting any of these solutions on Kubernetes is made much easier for you with\nthe help of operators. However, there are solutions specific to Kubernetes\nthat can make life much simpler for you with fully automatic instrumentation,\nsuch as Pixie.\n\n# Conclusion\n\nWe\u2019ve covered a lot in this post, so don\u2019t feel like you need to understand\nall of it right away. Keep this post as a reference for the future. But if\nthere\u2019s one thing I\u2019d like you to take away, it\u2019d be that you should take the\ntime to study Kubernetes and its ecosystem if you\u2019re considering a career in\nDevOps.\n\nIt\u2019s also important to remember that we do what we do to satisfy business\nneeds, so keep things as simple as possible.\n\n  * Development\n  * CI/CD\n  * Tools\n  * Infrastructure\n  * Tools\n  * Deployment\n  * Containers\n  * Tools\n  * Container Orchestration\n  * Tools\n  * Monitoring\n  * Tools\n  * Conclusion\n\n  * GitHub\n  * LinkedIn\n  * RSS\n\n", "frontpage": false}
