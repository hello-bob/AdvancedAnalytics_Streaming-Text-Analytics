{"aid": "40247673", "title": "Acceleration is all you need (now)", "url": "https://octo.ai/blog/acceleration-is-all-you-need-techniques-powering-octostacks-10x-performance-boost/", "domain": "octo.ai", "votes": 1, "user": "bcarambio", "posted_at": "2024-05-03 13:52:01", "comments": 0, "source_title": "The techniques powering OctoStack's 10x performance boost | OctoAI", "source_text": "The techniques powering OctoStack's 10x performance boost | OctoAI\n\nNew Webinar May 7th: Bring GenAI to your datastore with OctoStack\n\nRegister Now\n\n  * Product\n\n  * Pricing\n\n  * Resources\n\n  * Company\n\nSign up\n\nLog in\n\n  * Product\n\n  * Pricing\n\n  * Resources\n\n  * Company\n\nSign up\n\nLog in\n\nHome\n\nBlog\n\nAcceleration is all you need (now): Techniques powering OctoStack's 10x\nperformance boost\n\n# Acceleration is all you need (now): Techniques powering OctoStack's 10x\nperformance boost\n\nLuis Ceze&\n\nJason Knight&\n\nTianqi Chen\n\nMay 2, 2024\n\n10 minutes\n\nIn this article\n\nApproaches to optimize your GenAI model inferences\n\nEffective use of the NVIDIA H100 FP8 precision\n\nOptimal tensor parallelism with the new NVIDIA CustomAllReduce kernel\n\nEfficient GPU compute utilization with CUDA graphs\n\nFaster output with Speculative decoding\n\nImproved GPU utilization with Dynamic SplitFuse\n\nOctoStack delivers exponential improvements with scale\n\nShare\n\nIn this article\n\nApproaches to optimize your GenAI model inferences\n\nEffective use of the NVIDIA H100 FP8 precision\n\nOptimal tensor parallelism with the new NVIDIA CustomAllReduce kernel\n\nEfficient GPU compute utilization with CUDA graphs\n\nFaster output with Speculative decoding\n\nImproved GPU utilization with Dynamic SplitFuse\n\nOctoStack delivers exponential improvements with scale\n\nGenAI apps are changing the world. We\u2019ve seen through our customers how new\napps are simplifying how users get new information, interact with services,\nand unlock creative potential. Even as organizations explore the newest models\nand latest capabilities, they are acutely aware of the resource impact of the\nsuccess of these applications. Builders want efficiency, customizability and\nreliability, as they build for this growing demand. And underpinning all of\nthese is the need to optimize inferences, a problem space that OctoAI has been\nfocused on since its founding.\n\nBuilding on our years of experience across the inference stack, we have built\na number of leading edge optimization technologies into the OctoAI systems\nstack. With OctoStack \u2013 our turnkey GenAI serving stack offering for the\nenterprise environment \u2013 these are available as levers or knobs that builders\ncan choose to turn on and off for their deployment. The chart below captures\nresults from preliminary benchmarking of a few optimizations we have added to\nthe OctoAI systems stack in the last few months. Our preliminary benchmarking\nshows 4x to 5x improvements in low concurrency use cases, and orders of\nmagnitude improvement for higher concurrency use cases.\n\nThroughput improvements: OctoStack evaluations with single user configurations\n\nMulti-user Throughput of vLLM compared to OctoStack chart\n\nLatency improvements from recently added optimizations\n\nThese are optional configurations available for OctoStack today. If you think\nOctoStack may be a fit for your organization\u2019s needs, reach out to our team\nfor an assessment.\n\nThe rest of this article is a brief overview of these optimization techniques,\nand new technologies that we are investigating and in the process of\nincorporating into our stack in the coming months.\n\n## Approaches to optimize your GenAI model inferences\n\nGenAI models, and large language models (LLMs) in particular, are among the\nmost resource-intensive workloads in the industry today. Running inferences\nrequires extensive coordination and orchestration across data storage, memory,\nand compute caches, all dealing with tens of gigabytes of data and billions of\ncoordinated computations. Techniques to improve these span the entire GenAI\nserving stack, from the model, to the serving layer, to the computational\nkernel and techniques used. The following is a discussion of six such\ntechniques. These are not in any particular order.\n\n## Effective use of the NVIDIA H100 FP8 precision\n\nGenAI models have larger memory requirements than typical data workloads. As\nan example, a database may have a minimum requirement of 1 to 2 GB of memory.\nIn contrast, even small LLMs like Meta\u2019s recent Llama 3 8B model requires\nnearly 20GB of memory. For such models, the primary factor determining\nresource requirements (ie - which GPUs and how many) is often the memory\nrequirement, especially for small volumes of inferences where the compute\nusage cannot be fully utilized alongside the memory.\n\nOne approach to reduce the memory footprint is quantization, reducing the\nspace needed to capture each number involved in the model\u2019s operation, like\nthe model\u2019s weights - which are usually in the 16 bit FP16 format. Moving from\na 16 bit floating point (FP16) representation to an 8 bit integer\nrepresentation (like INT8) results in halving the memory needed for weights.\nBut post training quantization from FP16 to INT8 results in a steep drop in\nquality. Part of the reason is that the training of the original model is\naccomplished with floating point numbers and floating point weights, and the\nquantization to a non floating point representation loses much of the fidelity\nachieved in the original training. This degradation takes away the key\nbenefits of moving to newer and larger models trained with bigger datasets.\nBecause of this, quantization to integer representation has not been broadly\nadopted for quality sensitive production use cases yet.\n\nAnother approach to quantization is the new FP8 format. FP8 is an 8 bit\nfloating point representation, introduced collaboratively by NVIDIA, Arm and\nIntel. It provides a higher dynamic range than integer based quantization\nschemes, making it suitable for quantizing more components of an LLM (in\nparticular activations and gradients). This also enables better alignment\nbetween the numbers used in inference computations and in training, even in\npost-training quantization. FP8 is also natively supported in the new NVIDIA\nH100 GPUs, enabling better computational efficiencies through native hardware\nsupport.\n\nWhile hardware support and floating point representation have benefits,\neffective use of FP8 to unlock its quality-performance capabilities requires\ncorrect implementation at multiple levels in the inference process. This needs\nto be done on a model by model basis, and at multiple components like the\nweights, the activation functions and the KV cache. This also requires the\nright choice of FP8 representation for each of these, based on the model and\nvalue ranges to be converted. Examples of this include selection of the two\nFP8 formats available on H100 hardware, e4m3 and e5m2, choice of per tensor or\nper channel scaling, the selection and configuration of calibration and\nsmoothing procedures and the downstream performance implications of all of\nthese choices. All of this requires deep expertise with the inference\noperations and the impact of these changes. Incorrect assumptions and\nimplementation at any of these can result in limited performance improvement\nand unpredictable quality impact.\n\nWhen correctly implemented, FP8 can have performance improvements with near\nzero quality degradation. NVIDIA\u2019s published results report that BERT model\ninferences with FP8 (post training quantization) have 99.9% the accuracy of\nFP16. This is also validated by internal testing at OctoAI. In tests with the\nMixtral 8x7B model, we observed that inferences served by the FP8 quantized\nversion delivered LongBench benchmark results within 0.5% of the FP16 results\n(713 versus 716), and a semantic match consistency of 94.7% between FP16 and\nFP8 results (evaluated using internal GTE-Large embeddings based semantic\nevaluation framework). With these, FP8 brings to most use cases the quality\nstrengths of larger new models, while reducing the resource needs.\n\n## Optimal tensor parallelism with the new NVIDIA CustomAllReduce kernel\n\nComputing language model outputs on a GPU is done by executing a sequence of\nCUDA kernels. These kernels execute the mathematical operation on the GPU.\nTraditionally, machine learning inference commonly leverages a single GPU per\ndeployment replica for ease of deployment and minimizing complexity. With the\ngrowth in model sizes, effective model execution today must span multiple\nGPUs, and requires coordination in operation across these GPUs. When sharing\ndata across GPUs, most developers will and should first use NVIDIA\u2019s\nCollective Communications Library (NCCL) to simplify inter-GPU communications\nin an effective way on NVIDIA GPUs. The OctoAI team continuously evaluates\nperformance and tradeoffs across kernel options. We have seen that the new\nCustomAllReduce kernel, released as part of the NVIDIA TRT LLM project and\nbuilding on the commonly used operations in the NCCL library, results in the\nbest performance for computations for forward passes in language models. We\nhave now added the ability to use this new kernel with the OctoAI stack,\nresulting in faster inference execution and lower inter token latency with no\nquality tradeoffs.\n\nTensor parallelism is the particular form of GPU compute parallelism we use to\ndistribute each computation across multiple GPUs, in order for each GPU to\nonly depend on a subset of the weights (this is in contrast to data\nparallelism, another parallelism strategy where the same weights are\nreplicated across multiple GPUs but the input data is segmented to be run in\nparallel across these replicas). Implementing tensor parallelism effectively\nincludes both the selection of the optimal parallelization strategy (or number\nof GPUs) for the desired use case and expected volume as well as the right\nselection of compute and communication kernels. As an example, OctoAI\u2019s\nbenchmarking has shown that going from 2 to 4 parallel GPUs is particularly\nvaluable for high concurrency use cases - which are the defacto when it comes\nto SaaS or managed services deployments while also balancing the communication\noverhead and serving flexibility with even more GPUs per serving replica. At\nmoderate concurrency levels of 50 or more, we were able to observe a\ncumulative improvement of more than 5x compared to alternative best in class\nDIY implementations on the same GPU footprint. The impact of this parallelism\nis not as apparent at smaller concurrency levels. When configuring your\nOctoStack deployment, we work with customers to choose the right tensor\nparallelism level based on anticipated usage and to tune this as they learn\nmore about traffic and utilization in pilots and production usage.\n\n## Efficient GPU compute utilization with CUDA graphs\n\nAs discussed in the previous segment, computations in the GPUs are executed\nwithin kernels. These GPU kernels are invoked by the driving server process on\nthe CPU. Each execution includes CPU and GPU dispatch overhead. These take a\nfinite amount of time depending on the kernel and launch parameters, in the\norder of microseconds. With the growing speed of GPU compute and the\nincreasing amount of data - this overhead time can grow to become significant\nfor certain models and parallelism strategies, especially at higher tensor\nparallelism sharding amounts and smaller models. As a result, sequential\nexecution of individual GPU kernels can become increasingly inefficient due to\nthis overhead leading to wasted GPU cycles.\n\nCUDA graphs were introduced by NVIDIA to address this issue. With CUDA graphs,\ninstead of launching individual GPU operations, the CUDA kernels are sequenced\nand grouped together as a dependency tree (or graph) and sent for execution in\none go. This results in an initially higher overhead for the CPU operation,\nbut cumulatively speeds up the execution as the graphic from NVIDIA below\nillustrates. OctoAI inference stack now implements CUDA graphs enabled\nscheduling to take advantage of these efficiencies.\n\nSource: https://developer.nvidia.com/blog/constructing-cuda-graphs-with-\ndynamic-parameters/\n\n## Faster output with Speculative decoding\n\nText generation in LLMs is an iterative process of generating the best next\ntoken (numeric representations of words or groups of words) to a given\nsequence of input tokens. The input prompt word sequence is converted to a\nsequence of tokens by the tokenizer and fed into the model. The model\nprocesses the input token sequence, creates a list of output token options and\nprobabilities, and selects the best next token from the group based on the\ninference parameters. This token is then appended to the input prompt, and fed\nback into the model to generate the next token. This process is repeated until\nthe generation is complete.\n\nSpeculative decoding (also referred to as speculative sampling or assisted\ngeneration) builds on the fact that execution of the computations in the\nforward pass of a large language model is not a compute constrained operation\nbut constrained by other resources limitations like memory bandwidth and\ncommunications bottlenecks (not different from what we observed earlier at the\nlower layer, with CUDA graphs and the kernel execution and overheads).\nBuilding on this and the availability of additional compute capacity with the\nloaded model, this approach accelerates the inference by increasing\nconcurrent/concurrent computing with \u201cspeculative execution\u201d. In brief, the\nidea is that multiple tokens are generated at the same time, based on some\nassumptions of what the generated tokens will be (also referred to as draft\ntokens). These draft tokens are in turn generated by a smaller draft model,\nwhich may have lower accuracy and may get them wrong.\n\nSource:\nhttps://www.semanticscholar.org/reader/b7d12aec8a0152ec4921dfa43ab525a63b334385\n\nThe parallel execution results in a validation of whether the initially\ngenerated draft tokens are correct or not, for the input prompt. Once an\nincorrect token is detected, the generation can start from that position using\nthe sequential token by token process - but this results in time savings\ncorresponding to the token creations until that position N. The researchers\nbehind the papers in this subfield built on an observation that many text\ngeneration steps are much easier than others, and can be generated much more\neasily by simpler models and techniques. For the same reason that you can\nlikely complete the last word of this phrase: \u201ctake your toothbrush and brush\nyour ____\u201d. This is how the draft tokens can be created with a high\nprobability of being correct without needing the full compute from the main\nLLM.\n\nMore recent advances, in particular the EAGLE approach, builds on this with\nfurther simplifications to generate the draft tokens from the same model -\nusing an additional layer (plugin) added to the model to do its own\nspeculation and create the draft tokens needed for the remainder of the\nprocess. This overcomes the limitations arising due to the difference in\nparameter counts and the capabilities when using a smaller draft model,\nresulting in overall higher accuracy in the initial draft tokens and larger\nsavings in overall generation. OctoAI has an internal preview implementation\nof speculative decoding using this approach, and we are validating this with\npreliminary quality and speed benchmarks.\n\n## Improved GPU utilization with Dynamic SplitFuse\n\nDynamic SplitFuse is an approach that improves the utilization of GPU\nresources, based on optimizing the cumulative number of prompts fed into each\nforward pass of the model. It builds on the fact that GPU utilization is\nmaximized if the number of tokens passed into the forward pass (generation) of\nthe model is in what is referred to as the \u201csaturation zone\u201d. To ensure that\nall forward passes stay in this optimal zone, the Dynamic SplitFuse technique\nsplits input prompts into smaller chunks to schedule them across multiple\nforward passes. This results in the model execution always staying in the\n\u201csweet spot\u201d of GPU compute usage in every run. As in the case of speculative\ndecoding, Dynamic SplitFuse implementation is in internal preview and being\nevaluated.\n\n## OctoStack delivers exponential improvements with scale\n\nCore to OctoStack is the MLC compilation and serving layer, based on the MLC\nLLM project \u2014 created by a group of leading researchers in the AI community\nincluding one the OctoAI co-founders and other Octonauts. The MLC architecture\nis designed to deliver efficiencies across a broad range of traffic and\ndeployment patterns, and these benefits get exponentially higher at higher\nlevels of concurrency and scale (typical to model GenAI application usage\npatterns).\n\nThe chart here captures the results from recent internal benchmarking\ncomparing OctoStack performance against vLLM (version 0.3.3:\n82091b864af105dbe373353655dc9d8c0a6ba66f) at different concurrency levels.\nLarger scale deployments with tens to hundreds of concurrent users can see an\norder of magnitude improvement in speed and utilization, of 10x or more, much\nbeyond the 4x to 5x improvements in smaller deployments. And as the plot\nshows, OctoStack customers have tremendous economies of scale benefits as they\nincrease usage (ability to increase throughput and concurrency with only a\nmarginal change latency). These scale benefits are complementary to the\ntechniques discussed through this article, and we anticipate it will continue\nto push the efficiency improvements delivered with OctoStack.\n\nSource: Benchmarking of OctoStack and vLLM at multiple levels of concurrent\nusers (load test concurrency labeled on plot), 1024 input tokens, 128 output\ntokens, 3 sigma deviation\n\nReach out if you\u2019d like to explore these further. Our team can help assess the\nbest approaches for your use case, and configure your pilot OctoStack\ndeployment.\n\n\u00a9 2024 OctoAI. All rights reserved.\n\n  * Acceptable Use Policy\n  * Terms of Use\n  * Privacy Policy\n  * Responsible Disclosure\n  * DPF Certification\n  * Safety Center\n\n", "frontpage": false}
