{"aid": "40217934", "title": "Productionizing Embeddings: Challenges and a Path Forward", "url": "https://www.tecton.ai/blog/productionizing-embeddings-challenges-and-a-path-forward/", "domain": "tecton.ai", "votes": 7, "user": "mihirmathur", "posted_at": "2024-04-30 23:56:38", "comments": 0, "source_title": "Productionizing Embeddings: Challenges and a Path Forward", "source_text": "Productionizing Embeddings: Challenges and a Path Forward | Tecton\n\nTecton\n\nReady to accelerate your ML initiatives? Dive into Tecton's interactive demo\n\nDismiss\n\nMenu\n\nRequest a Demo\n\nExplore Tecton\n\n# Productionizing Embeddings: Challenges and a Path Forward\n\nBy\n\nMihir Mathur Brian Hart Brian Seo\n\nPublished: April 30, 2024\n\nProduct UpdatesThought Leadership\n\nEmbeddings \u2013 condensed, rich representations of unstructured data \u2013 have\nemerged as a transformative technique for unlocking the full potential of both\npredictive and generative AI.\n\nIn business-critical predictive AI applications like fraud detection and\nrecommendation systems, embeddings enable models to identify complex patterns,\nleading to more accurate predictions. For generative AI, embeddings provide a\nsemantic bridge, allowing models to leverage the contextual meaning of data to\ncreate novel, valuable content.\n\nHowever, the path to productionizing embeddings at scale in business-critical\nAI systems is fraught with several technical hurdles. In this post, we\u2019ll\nfirst describe the major challenges organizations face in productionizing\nembeddings to drive business value. We\u2019ll then share a path to overcome these\nchallenges using Tecton\u2019s newly released product capability, Embeddings\nGeneration and Serving.\n\n## Production Challenges\n\nThere are two major engineering stages needed for operationalizing embeddings\nat scale: Inference & Serving. Inference refers to the process of generating\nembeddings from input data, while Serving involves efficiently storing and\nretrieving these embeddings for downstream applications.\n\nLet\u2019s take a look at the top challenges organizations face for each of these:\n\nInference Challenges:\n\n  * Compute Resource Management: Large scale inference of embeddings, such as processing millions of product descriptions nightly for a recommendation system, is computationally expensive and memory intensive. Such workloads need careful provisioning, scheduling, and tuning of resources such as GPUs to ensure cost-efficient performance.\n  * Data Pipeline Orchestration: Orchestrating the end-to-end lifecycle of structured and unstructured data living in different sources (S3, Snowflake, BigQuery, etc.), embedding models, training and inference layer, and diverse downstream AI pipelines requires robust data engineering practices and is tedious to setup and manage.\n  * Training Data Generation: For retraining models, generating point-in-time accurate embeddings training data efficiently at scale without the right set of tools can be an arduous process, requiring specialized data engineering expertise.\n  * Ease of Experimentation & Reproducibility: Finding the optimal balance between embedding model complexity, inference performance, and infrastructure costs is a delicate dance that demands deep technical understanding and trial-and-error. Further, reproducibility of embeddings is critical for ensuring offline experiments are consistent with production. ML practitioners need tooling to quickly test, evaluate, and refine their embeddings approach while ensuring production reproducibility.\n\nServing Challenges:\n\n  * Efficient storage and retrieval at scale: Real-world AI applications require storing and serving tens or even hundreds of millions of embeddings. These applications need different retrieval patterns: point-based lookups by unique identifiers and vector similarity searches for nearest matches to query vectors. Achieving retrieval latencies in the tens of milliseconds while optimizing storage footprint and cost requires specialized embedding storage and retrieval architectures, which are hard to build and maintain. For instance, most-frequently accessed embeddings should be cached to ensure they\u2019re available with minimal latency.\n  * Scalability: Handling irregular traffic patterns (eg. spikes or seasonality) which cause sudden increases in embeddings lookup volume without performance degradation while avoiding over-provisioning serving resources is hard to build and maintain.\n\nBeyond these Inference and Serving challenges, there are other operational\nchallenges around: collaboration on embeddings pipelines in a large\norganization, version control of embeddings, governance of embeddings, and\nadhering to safety standards while leveraging open-source models.\n\n## Embeddings Generation with Tecton\n\nOver the years, Tecton has been crafted into a platform that solves the\naforementioned challenges extremely well for ML features. Since embeddings are\nessentially model-generated features, we\u2019ve extended Tecton\u2019s capabilities to\nprovide first-class support for embeddings, harnessing our expertise in\neffective feature management. With our new Embeddings capability, we bring\nTecton\u2019s best-in-class compute, storage, serving, and systematic approach for\nproductionizing hand-engineered features to embeddings.\n\nHere\u2019s how easy it is to write production-ready embeddings pipelines with\nTecton\u2019s declarative interface:\n\n    \n    \n    from tecton import batch_feature_view, Embedding, RiftBatchConfig from datetime import timedelta @batch_feature_view( sources=[products], entities=[product], features=[ Embedding(column=\"PRODUCT_NAME\", model=\"Snowflake/snowflake-arctic-embed-l\"), Embedding(column=\"DESCRIPTION\", model=\"Snowflake/snowflake-arctic-embed-l\"), ], mode=\"pandas\", batch_schedule=timedelta(days=1), timestamp_field=\"TIMESTAMP\", batch_compute=RiftBatchConfig(instance_type=\"g5.xlarge\"), ) def product_info_embeddings(products): return products[[\"PRODUCT_ID\", \"PRODUCT_NAME\", \"DESCRIPTION\", \"TIMESTAMP\"]]\n\nThe above example shows an embeddings generation pipeline that runs daily for\nembedding product names and descriptions using the Snowflake/snowflake-arctic-\nembed-l model. Top performing open-source embeddings models are shipped out of\nthe box with Tecton; we\u2019ve optimized our inference for model families such as\nSnowflake Arctic embed, BGE, GTE, mxbai, and more. Further, the generated\nembeddings can be easily ingested to any vector database including Pinecone,\nMilvus, LanceDB, Weaviate, etc.\n\nHere\u2019s a Tecton screenshot showing the dataflow of the simple embeddings\npipeline we defined above. This pipeline reads product data from a Snowflake\ndata source, runs batch inference, stores the embeddings in the offline and\nonline store for training and realtime serving through a Feature Service.\n\n### Production Inference Challenges Solved\n\nHere\u2019s how Tecton solves the Inference challenges mentioned above:\n\n  * Compute Resource Management: We\u2019ve built numerous optimizations for efficient compute resource management for embeddings inference. Some of these include: dynamic batching based on sequence length, sequence length ordering per data chunk, GPU aware batch sizes that optimize throughput, column-wise inference, and a lot more.\n  * Data Pipeline Orchestration: Tecton\u2019s seamless data pipeline management enables teams to connect to diverse datasources such as Snowflake or S3 or BigQuery and generate and ingest embeddings to different online and offline stores on a schedule.\n  * Training Data Generation: Generating point-in-time correct training data offline is a breeze by leveraging Tecton framework methods such as get_features_for_events in a notebook.\n  * Ease of Experimentation & Reproducibility: Experimenting with different state-of-the-art open-source models is as simple as changing a parameter to automatically run performance optimized embeddings inference. Tecton\u2019s \u201cfeatures-as-code\u201d paradigm ensures reproducibility of embeddings and guarantees online-offline consistency.\n\nThe following chart compares the inference throughput of different models\nusing Tecton Embeddings on a single NVIDIA A10 GPU (AWS g5.xlarge), showcasing\nthe tradeoff of model size to inference speed. This was run using the MS Marco\nv1.1 dataset.\n\n### Production Serving Challenges Solved\n\nHere\u2019s how Tecton solves the Serving challenges mentioned above:\n\n  * Efficient storage and retrieval at scale: With Tecton, modelers get the flexibility to pre-generate and serve embeddings, saving costs and reducing latency, while also allowing real-time generation when needed (eg. in Retrieval-Augmented Generation usecases). Tecton\u2019s Serving Cache can store the most frequently accessed embeddings, ensuring minimal latency for those frequent requests. Tecton can export embeddings to any vector store, enabling organizations to leverage the unique cost-performance tradeoffs offered by various vector search solutions.\n  * Scalability: scaling resources to meet spiky and seasonal increases for embeddings lookup volume is effortless and cost-efficient with Feature Server Autoscaling. Further, Tecton provides critical aspects of an operationally excellent scalable embeddings serving system: best-in-class availability SLA, in-depth monitoring, and alerting.\n\n## What\u2019s next?\n\nAt Tecton, we are laser-focused on empowering organizations to unlock the full\npotential of their data in mission-critical AI applications. In the months\nahead, we will be rolling out significant enhancements to our Embeddings\noffering. Some of these include:\n\n  1. More flexibility: Ability to bring in first party custom models for embeddings generation\n  2. Better performance: Further optimizations for batch and real-time generation of embeddings\n\nWe can\u2019t wait to see Tecton\u2019s Embeddings capabilities power more production AI\nuse cases\u2014reach out to us to sign up for the private preview!\n\nProduct UpdatesThought Leadership\n\npredictive aigenerative aiembeddings\n\n## Related Posts\n\n## How Tecton Helps ML Teams Build Smarter Models, Faster\n\nApril 5, 2024\n\nJulia Brouillette\n\nIn the race to infuse intelligence into every product and application, the\nspeed at which machine learning (ML) teams can innovate is not just a metric\nof efficiency. It\u2019s what sets industry leaders apart, empowering them to\nconstantly improve ...\n\nThought LeadershipFeature Platforms\n\n## Introducing Tecton\u2019s Integration with ModelBit\n\nFebruary 27, 2024\n\nmodelbit\n\nThis post was originally published on the Modelbit blog and can be viewed\nhere. We are excited to announce that Tecton and Modelbit have partnered to\nrelease an integration to enable a more streamlined ML model deployment and\nfeature management ...\n\nProduct UpdatesPartners & Integrations\n\n## Production ML: 6 Key Challenges & Insights\u2014an MLOps Roundtable Discussion\n\nJanuary 24, 2024\n\nEvelyn Chea\n\nNavigating the journey from a promising ML concept to a robust, production-\nready application is filled with challenges. Teams need to establish efficient\ndata pipelines, understand and attribute their costs, and design\norganizational processes that ...\n\nThought Leadership\n\n## Let's keep in touch\n\nEnter your email to get the latest content from Tecton, including newsletters\nabout product updates, upcoming events, and industry news\n\n\u00a9 Tecton, Inc. All rights reserved. Various trademarks held by their\nrespective owners.\n\nPrivacy and Terms\n\n### Request a Demo\n\nUnfortunately, Tecton does not currently support these clouds. We\u2019ll make sure\nto let you know when this changes!\n\nHowever, we are currently looking to interview members of the machine learning\ncommunity to learn more about current trends.\n\nIf you\u2019d like to participate, please book a 30-min slot with us here and we\u2019ll\nsend you a $50 amazon gift card in appreciation for your time after the\ninterview.\n\nCTA link\n\nor\n\nCTA button\n\n### Contact Sales\n\nInterested in trying Tecton? Leave us your information below and we\u2019ll be in\ntouch.\n\nUnfortunately, Tecton does not currently support these clouds. We\u2019ll make sure\nto let you know when this changes!\n\nHowever, we are currently looking to interview members of the machine learning\ncommunity to learn more about current trends.\n\nIf you\u2019d like to participate, please book a 30-min slot with us here and we\u2019ll\nsend you a $50 amazon gift card in appreciation for your time after the\ninterview.\n\nCTA link\n\nor\n\nCTA button\n\n### Request a free trial\n\nInterested in trying Tecton? Leave us your information below and we\u2019ll be in\ntouch.\n\nUnfortunately, Tecton does not currently support these clouds. We\u2019ll make sure\nto let you know when this changes!\n\nHowever, we are currently looking to interview members of the machine learning\ncommunity to learn more about current trends.\n\nIf you\u2019d like to participate, please book a 30-min slot with us here and we\u2019ll\nsend you a $50 amazon gift card in appreciation for your time after the\ninterview.\n\nCTA link\n\nor\n\nCTA button\n\n", "frontpage": false}
