{"aid": "40131014", "title": "LLM2Vec: Large Language Models Are Powerful Text Encoders", "url": "https://github.com/McGill-NLP/llm2vec", "domain": "github.com/mcgill-nlp", "votes": 1, "user": "plurby", "posted_at": "2024-04-23 12:11:51", "comments": 0, "source_title": "GitHub - McGill-NLP/llm2vec: Code for 'LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders'", "source_text": "GitHub - McGill-NLP/llm2vec: Code for 'LLM2Vec: Large Language Models Are\nSecretly Powerful Text Encoders'\n\n## Navigation Menu\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nMcGill-NLP / llm2vec Public\n\n  * Notifications\n  * Fork 14\n  * Star 286\n\nCode for 'LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders'\n\nmcgill-nlp.github.io/llm2vec/\n\n### License\n\nMIT license\n\n286 stars 14 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# McGill-NLP/llm2vec\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n8 Branches\n\n5 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nvaibhavadAdd link to examplesApr 19, 2024a4742d3 \u00b7 Apr 19, 2024Apr 19, 2024\n\n## History\n\n82 Commits  \n  \n### docs\n\n|\n\n### docs\n\n| Setup github pages| Apr 5, 2024  \n  \n### examples\n\n|\n\n### examples\n\n| Added STS example| Apr 18, 2024  \n  \n### experiments\n\n|\n\n### experiments\n\n| remove create model card| Apr 17, 2024  \n  \n### images\n\n|\n\n### images\n\n| Rename unsupervised.png to images/unsupervised.png| Apr 8, 2024  \n  \n### llm2vec\n\n|\n\n### llm2vec\n\n| Remove show_progress_bar = True (#17)| Apr 18, 2024  \n  \n### train_configs/mntp\n\n|\n\n### train_configs/mntp\n\n| MNTP training (#15)| Apr 17, 2024  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| MNTP training (#15)| Apr 17, 2024  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Initial commit| Apr 3, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| Add link to examples| Apr 19, 2024  \n  \n### setup.cfg\n\n|\n\n### setup.cfg\n\n| Added setup files| Apr 3, 2024  \n  \n### setup.py\n\n|\n\n### setup.py\n\n| MNTP training (#15)| Apr 17, 2024  \n  \n## Repository files navigation\n\n# LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders\n\nLLM2Vec is a simple recipe to convert decoder-only LLMs into text encoders. It\nconsists of 3 simple steps: 1) enabling bidirectional attention, 2) training\nwith masked next token prediction, and 3) unsupervised contrastive learning.\nThe model can be further fine-tuned to achieve state-of-the-art performance.\n\n## Installation\n\nTo use LLM2Vec, first install the llm2vec package from PyPI, followed by\ninstalling flash-attention:\n\n    \n    \n    pip install llm2vec pip install flash-attn --no-build-isolation\n\nYou can also directly install the latest version of llm2vec by cloning the\nrepository:\n\n    \n    \n    pip install -e . pip install flash-attn --no-build-isolation\n\n## Getting Started\n\nLLM2Vec class is a wrapper on top of HuggingFace models to support sequence\nencoding and pooling operations. The steps below showcase an example on how to\nuse the library.\n\n### Preparing the model\n\nInitializing LLM2Vec model using pretrained LLMs is straightforward. The\nfrom_pretrained method of LLM2Vec takes a base model identifier/path and an\noptional PEFT model identifier/path. All HuggingFace model loading arguments\ncan be passed to from_pretrained method (make sure the llm2vec package version\nis >=0.1.3).\n\nHere, we first initialize the Mistral MNTP base model and load the\nunsupervised-trained LoRA weights (trained with SimCSE objective and wiki\ncorpus).\n\n    \n    \n    import torch from llm2vec import LLM2Vec l2v = LLM2Vec.from_pretrained( \"McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp\", peft_model_name_or_path=\"McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp-unsup-simcse\", device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\", torch_dtype=torch.bfloat16, )\n\nWe can also load the model with supervised-trained LoRA weights (trained with\ncontrastive learning and public E5 data) by changing the\npeft_model_name_or_path.\n\n    \n    \n    import torch from llm2vec import LLM2Vec l2v = LLM2Vec.from_pretrained( \"McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp\", peft_model_name_or_path=\"McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp-supervised\", device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\", torch_dtype=torch.bfloat16, )\n\nBy default the LLM2Vec model uses the mean pooling strategy. You can change\nthe pooling strategy by passing the pooling_mode argument to the\nfrom_pretrained method. Similarly, you can change the maximum sequence length\nby passing the max_length argument (default is 512).\n\n### Inference\n\nThis model now returns the text embedding for any input in the form of\n[[instruction1, text1], [instruction2, text2]] or [text1, text2]. While\ntraining, we provide instructions for both sentences in symmetric tasks, and\nonly for for queries in asymmetric tasks.\n\n    \n    \n    # Encoding queries using instructions instruction = ( \"Given a web search query, retrieve relevant passages that answer the query:\" ) queries = [ [instruction, \"how much protein should a female eat\"], [instruction, \"summit define\"], ] q_reps = l2v.encode(queries) # Encoding documents. Instruction are not required for documents documents = [ \"As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\", \"Definition of summit for English Language Learners. : 1 the highest point of a mountain : the top of a mountain. : 2 the highest level. : 3 a meeting or series of meetings between the leaders of two or more governments.\", ] d_reps = l2v.encode(documents) # Compute cosine similarity q_reps_norm = torch.nn.functional.normalize(q_reps, p=2, dim=1) d_reps_norm = torch.nn.functional.normalize(d_reps, p=2, dim=1) cos_sim = torch.mm(q_reps_norm, d_reps_norm.transpose(0, 1)) print(cos_sim) \"\"\" tensor([[0.5485, 0.0551], [0.0565, 0.5425]]) \"\"\"\n\nMore examples of classification, clustering, sentence similarity etc are\npresent in examples directory.\n\n## Model List\n\n  * ### Mistral-7B\n\n    * ### Bi + MNTP\n\n    * ### Bi + MNTP + SimCSE (Unsupervised state-of-the-art on MTEB)\n\n    * ### Bi + MNTP + Supervised (state-of-the-art on MTEB among models trained on public data)\n\n  * ### Llama-2-7B\n\n    * ### Bi + MNTP\n\n    * ### Bi + MNTP + SimCSE\n\n    * ### Bi + MNTP + Supervised\n\n  * ### Sheared-Llama-1.3B\n\n    * ### Bi + MNTP\n\n    * ### Bi + MNTP + SimCSE\n\n    * ### Bi + MNTP + Supervised\n\n## Training\n\n### MNTP training\n\nTo train the model with Masked Next Token Prediction (MNTP), you can use the\nexperiments/run_mntp.py script. It is adapted from HuggingFace Masked Language\nModeling (MLM) script. To train the Mistral-7B model with MNTP, run the\nfollowing command:\n\n    \n    \n    python experiments/run_mntp.py train_configs/mntp/Mistral.json\n\nThe Mistral training configuration file contains all the training\nhyperparameters and configurations used in our paper.\n\n    \n    \n    { \"model_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.2\", \"dataset_name\": \"wikitext\", \"dataset_config_name\": \"wikitext-103-raw-v1\", \"mask_token_type\": \"blank\", \"data_collator_type\": \"all_mask\", \"mlm_probability\": 0.8, \"lora_r\": 16, \"gradient_checkpointing\": true, \"torch_dtype\": \"bfloat16\", \"attn_implementation\": \"flash_attention_2\" // .... }\n\nSimilar configurations are also available for Llama-2-7B and Sheared-\nLlama-1.3B models.\n\n## Citation\n\nIf you find our work helpful, please cite us:\n\n    \n    \n    @article{llm2vec, title={{LLM2Vec}: {L}arge Language Models Are Secretly Powerful Text Encoders}, author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy}, year={2024}, journal={arXiv preprint}, url={https://arxiv.org/abs/2404.05961} }\n\n## Bugs or questions?\n\nIf you have any questions about the code, feel free to open an issue on the\nGitHub repository.\n\n## About\n\nCode for 'LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders'\n\nmcgill-nlp.github.io/llm2vec/\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\nActivity\n\nCustom properties\n\n### Stars\n\n286 stars\n\n### Watchers\n\n9 watching\n\n### Forks\n\n14 forks\n\nReport repository\n\n## Releases 5\n\n0.1.4 Latest\n\nApr 17, 2024\n\n\\+ 4 releases\n\n## Contributors 4\n\n  * vaibhavad Vaibhav Adlakha\n  * ParishadBehnam Parishad BehnamGhader\n  * sivareddyg Siva Reddy\n  * tma15 Takuya Makino\n\n## Languages\n\n  * Python 100.0%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
