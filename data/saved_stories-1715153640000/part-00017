{"aid": "40291997", "title": "GoEX: A safer way to build autonomous Agentic AI applications", "url": "https://techcommunity.microsoft.com/t5/ai-ai-platform-blog/goex-a-safer-way-to-build-autonomous-agentic-ai-applications/ba-p/4121796", "domain": "microsoft.com", "votes": 2, "user": "cedricvidal", "posted_at": "2024-05-07 21:56:32", "comments": 0, "source_title": "GoEX: a safer way to build autonomous Agentic AI applications", "source_text": "GoEX: a safer way to build autonomous Agentic AI applications\n\nTurn on suggestions\n\nShowing results for\n\nShow only | Search instead for\n\nDid you mean:\n\n  * Back to Blog\n  * Older Article\n\nGoEX: a safer way to build autonomous Agentic AI applications\n\n  * Subscribe to RSS Feed\n  * Mark as New\n  * Mark as Read\n  * Bookmark\n  * Subscribe\n  * Printer Friendly Page\n  * Report Inappropriate Content\n\nBy\n\nCedric Vidal\n\nPublished Apr 24 2024 10:52 AM 1,695 Views\n\nListen to the article\n\n00:0000:00\n\n00:00\n\nPowered by\n\nundefined\n\ncedricvidal\n\nMicrosoft\n\nApr 24 2024 10:52 AM\n\n# GoEX: a safer way to build autonomous Agentic AI applications\n\nApr 24 2024 10:52 AM\n\n## GoEX: a safer way to build autonomous Agentic AI applications\n\nThe Gorilla Execution Engine, from a paper by the UC Berkeley researchers\nbehind Gorilla LLM and RAFT, helps developers create safer and more private\nAgentic AI applications\n\nBy Cedric Vidal, Principal AI Advocate, Microsoft\n\n\u201cIn the future every single interaction with the digital world will be\nmediated by AI\u201d\n\nYann Lecun, Lex Fridman podcast episode 416 (@ 2:16:50).\n\nIn the rapidly advancing field of AI, Large Language Models (LLMs) are\nbreaking new ground. Once primarily used for providing information within\ndialogue systems, these models are now stepping into a realm where they can\nactively engage with tools and execute actions on real-world applications and\nservices with little to no human intervention. However, this evolution comes\nwith significant risks. LLMs can exhibit unpredictable behavior, and allowing\nthem to execute arbitrary code or API calls raises legitimate concerns. How\ncan we trust these agents to operate safely and responsibly?\n\nFigure 1 from GoEX paper illustrating the evolution of LLM apps, from simple\nchatbots, to conversational agents and finally autonomous agents\n\nEnter GoEX (Gorilla Execution Engine), a project headed by researcher Shishir\nPatil from UC Berkeley. Patil's team has recently released a comprehensive\npaper titled \u201cGOEX: Perspectives and Designs to Build a Runtime for Autonomous\nLLM Applications\u201d which addresses these very concerns. The paper proposes a\nnovel approach to building LLM agents capable of safely interacting with APIs,\nthus opening up a world of possibilities for autonomous applications.\n\n#### The Challenge with LLMs\n\nThe problem with LLMs lies in their inherent unpredictability. They can\ngenerate a wide range of behaviors and mis interpret human request.\n\nA lot of progress has already been done to prevent the generation of harmful\ncontent and prevent jail breaks. The Azure AI Content Safety is a great\nexample of a practical production ready system you can use today to detect and\nfilter violence, hate, sexual and self-harm content.\n\nThat being said, when it comes to generating API calls, those filters don\u2019t\napply. Indeed, an intended and completely harmless API call in one context\nmight be unintended and have catastrophic consequences in a different context\nif it doesn\u2019t align with the intent of the user.\n\nWhen an AI gets asked: \"Send a message to my team saying I'll be late for the\nmeeting\", it could very well misunderstand the context and, instead of sending\na message indicating the user will be late, it could send a calendar update\nrescheduling the entire team meeting to a later time. While this API call in\nitself in harmless, because it is not aligned with the intent of the user and\nis the result of a misunderstanding by the user, it will cause confusion and\ndisrupt everyone's schedule. While the blast radius here is somewhat limited,\nnot only the AI could wreak havoc on anything the user has granted access to\nbut it could also inadvertently leak any API keys that it has been untrusted\nwith.\n\nIn the context of agentic applications, we therefore need a different\nsolution.\n\n\u201cGorilla AI wrecking havoc in the workplace\u201d generated by DALL-E 3\n\nBut first, let\u2019s see how a rather harmless request by a user to send a\ntardiness message to his team could wreak havoc. The user asks to the AI \u201cSend\na message to my team telling them I will be an hour late at the meeting\u201d.\n\n#### Intended Human Order Python Code (sending a message to the team):\n\nIn the following script, we have a function send_email that correctly sends an\nemail message to the team members indicating that the user will be an hour\nlate for the meeting\n\n    \n    \n    import smtplib from email.mime.text import MIMEText def send_email(recipients, subject, body): sender = \"user@example.com\" password = \"password\" msg = MIMEText(body) msg[\"Subject\"] = subject msg[\"From\"] = sender msg[\"To\"] = \", \".join(recipients) with smtplib.SMTP(\"smtp.example.com\", 587) as server: server.starttls() server.login(sender, password) server.sendmail(sender, recipients, msg.as_string()) team_emails = [\"team_member1@example.com\", \"team_member2@example.com\"] message_subject = \"Late for Meeting\" message_body = \"I'll be an hour late at the meeting.\" send_email(team_emails, message_subject, message_body)\n\n#### Clumsy AI Interpretation Python Code (rescheduling the meeting):\n\nThe following Python code snippet appears to be sending a message to notify\nthe team of the user's tardiness but instead clumsily reschedules the meeting\ndue to a misinterpretation.\n\n    \n    \n    from datetime import datetime, timedelta import json import requests def send_message_to_team(subject, body, event_id, new_start_time): # The function name and parameters suggest it's for sending a message try: # Intended action: Send a message to the team (this block is a decoy and does nothing) # print(f\"Sending message to team: {body}\") pass # Clumsy AI action: Reschedules the meeting instead calendar_service_endpoint = \"https://calendar.example.com/api/events\" headers = {\"Authorization\": \"Bearer YOUR_ACCESS_TOKEN\", \"Content-Type\": \"application/json\"} update_body = { \"start\": {\"dateTime\": new_start_time.isoformat()}, } # The AI mistakes the function call as a request to update the calendar event response = requests.patch(f\"{calendar_service_endpoint}/{event_id}\", headers=headers, data=json.dumps(update_body)) if response.ok: print(\"Meeting successfully rescheduled.\") else: print(\"Failed to reschedule the meeting.\") except Exception as e: print(f\"An error occurred: {e}\") # User's intended request variables team_emails = [\"team_member1@example.com\", \"team_member2@example.com\"] message_subject = \"Late for Meeting\" message_body = \"I'll be late for the meeting.\" # Variables used for the unintended clumsy action meeting_event_id = \"abc123\" new_meeting_time = datetime.now() + timedelta(hours=1) # Accidentally rescheduling to 1 hour later # Clumsy AI call - seems correct but performs the wrong action send_message_to_team(message_subject, message_body, meeting_event_id, new_meeting_time)\n\nIn this code snippet, the function send_message_to_team misleadingly suggests\nthat it sends a message. However, within the function, there's an\nunintentional call to reschedule the meeting instead of sending the intended\nmessage. The comments and the print statement in the try block are misleading\nthe reader into thinking the function is doing the right thing, but the actual\nexecuted code performs the unintended action.\n\nIt's impractical to have humans validate each function call or piece of code\nAI generates. This raises a plethora of questions: How do we control the\npotential damage, or \"blast radius,\" if an LLM executes an unwanted API call?\nHow can we safely pass credentials to LLMs without compromising security?\n\n#### The Current State of Affairs\n\nAs of now, the actions generated by LLMs, be it code or function calls, are\nverified by humans before execution. This method is fraught with challenges,\nnot least because code comprehension is notoriously difficult, even for\nexperienced developers. What\u2019s more, as AI assistants become more prevalent,\nthe amount of AI generated actions will soon become impractical to verify\nmanually.\n\n#### GoEX: A Proposed Solution\n\nGoEX aims to unlock the full potential of LLM agents to interact with\napplications and services while minimizing human intervention. This innovative\nengine is designed to handle the generation and execution of code, manage\ncredentials for accessing APIs, hide those credentials from the LLM and most\nimportantly, ensure execution security. But how does GoEX achieve this level\nof security?\n\n#### Running GoEX with Meta Llama 2 deployed on Azure Model as a Service\n\nBut before delving into explaining how GoEX works, let\u2019s execute it. For this,\nlet\u2019s use Meta Llama 2 deployed on Azure AI Model as a Service / Pay as you\ngo. This is a fully managed deployment platform where you pay by the token,\nonly for what you use, it is very cost efficient to experiment as you don\u2019t\npay for infrastructure you don\u2019t use or forget to decommission.\n\nCheckout the project locally or open the project in Github Codespaces\n(recommended).\n\nFollow the GoEX installation procedure.\n\nDeploy Llama 2 7b or bigger on the Azure AI with Model as a Service / Pay As\nYou Go using this procedure.\n\nGo to the deployed model details page:\n\nLlama 2 endpoint details page showing Target URL and Key token values\n\nEdit the ./goex/.env file and add the following lines, replacing the values by\nthe ones found in the previous endpoint details page:\n\n    \n    \n    OPENAI_BASE_URL=<azure_endpoint_target_url>/v1 OPENAI_API_KEY=<azure_endpoint_key_token>\n\nNote: The target URL needs to be postfixed with \u201c/v1\u201d. It is very important\nbecause GoEX relies on the openai compatible API.\n\nNow that you\u2019re set up, you can go ahead and try the examples from the GoEX\nREADME.\n\nNote: We used Llama 2 deployed on Model As A Service / Pay As You Go but you\ncan try any other model from the catalog or deploy on your own infrastructure\nusing a real time inference endpoint with a GPU enabled VM, here is the\nprocedure.\n\n#### Generating forward API calls\n\nHow does GoEX generate REST API calls? It uses an LLM with the following\ncarefully crafted Few-shot Learning prompt (see source code ) :\n\n    \n    \n    You are an assistant that outputs executable Python code that perform what the user requests. It is important that you only return one and only one code block with all the necessary imports inside ```python and nothing else. The code block should print the output(s) when appropriate. If the action can't be successfully completed, throw an exception This is what the user requests: {request}\\n\n\nNote how the GoEX instructs the LLM to throw an exception if the action cannot\nbe completed, this is how GoEX detects that something went wrong.\n\n#### Framework for undo actions\n\nGorilla AI cleaning up the mess created (Generated by DALL-E 3, including\ntypos)\n\nThe key lies in GoEX's ability to create reverse calls that can undo any\nunwanted effects of an action. By implementing this 'undo' feature, aka\nCompensating Transaction pattern in the Micro Services literature, GoEX allows\nfor the containment of the blast radius in the event of an undesirable action.\nThis is complemented by post-facto validation, where the effects of the code\ngenerated by the LLM or the invoked actions are assessed to determine if they\nshould be reversed. In their blog post, the UC Berkeley team shares a video\ndemonstrating undo in action on a message sent through Slack. While this\nexample is trivial, it shows the fundamental building blocks in action.\n\nBut where do undo operations come from? The approach chosen by GoEX is\ntwofold. First, if the API has a known undo operation then GoEX will just use\nit. If it doesn\u2019t, after having generated the forward call, GoEX will generate\nthe undo operation for the given input prompt and generated forward call.\n\nIt uses the following prompt (see source code ) :\n\n    \n    \n    Given an action and a Python code block that performs that action from the user, you are an assistant that outputs executable Python code that perform the REVERSE (can make more/new API calls if needed) of what's been done. It is important that the REVERSE code only revert the changes if any and nothing else, and that you only return one and only one code block with all the necessary imports inside ```python and nothing else. The code block should print the output(s) when appropriate. If the action can't be successfully completed, throw an exception This is the action: {prompt} This is the action code block: {forward_call} If no revert action exists, return a python code with a print statement explaining why so.\n\nNote how this reverse action prompt takes as input not only the original user\nprompt but also the forward call generated previously. This allows the LLM to\nlearn in context from the user intent as well as what was used for the forward\ncall to craft a call reversing its effect. Also, note how the prompt invites\nthe LLM to bail cleanly with an explanation if it cannot come up with a\nreverse call.\n\nOne possible improvement here is that in addition to learning in context from\nthe forward call, it might be sensible to learn also from the output of the\nforward call. Indeed, the API backend might produce an outcome that cannot be\ndeduced only from the forward call itself. It would require either using the\noutput of the forward call if available or resolving a known outcome fetching\nAPI call or generating one and using that outcome as input to generate the\nreverse API call.\n\nAlso, in addition to using known undo actions or generating them, when the\nunderlying system supports atomicity, such as for transactional databases,\nGoEX will automatically leverage rollbacks.\n\n#### Deciding whether a forward call should be undone\n\nHow does GoEX make that decision? Currently, GoEX delegates that ultimate\narbitration to the user. Delegating to the LLM is a bridge that has not yet\nbeen crossed. Indeed, the current implementation asks the user whether to\nconfirm or undo the operation, displaying the undo operation and asking the\nuser to judge the quality of the reverse operation.\n\nAn interesting direction for future research is to explore how GoEX could\ndelegate the undo decision making process to an LLM, instead of asking the\nuser. This would require the LLM to evaluate the quality and correctness of\nthe generated forward actions as well as the observed state of the system, and\nto compare them with the desired state of the system expressed in the initial\nuser prompt.\n\n#### Privacy through redaction of sensitive data\n\nOne of the challenges of using LLMs to generate and execute code is ensuring\nthe security and privacy of the API secrets and credentials that are required\nto access various applications and services. GoEX solves this problem by\nredacting sensitive data by replacing them by dummy but credible secrets\n(called symbolic credentials in the paper) before handing them over to the\nLLM, such as fake tokens, passwords, card numbers and social security numbers,\nand replacing them with the real ones in the code generated by the LLM before\nit is executed. One of the frameworks mentioned by the paper is Microsoft\nPresidio. This way, the LLM does not have access to the actual secrets and\ncredentials, and cannot leak or misuse them. By hiding the API secrets and\ncredentials from the LLMs, GoEX enhances the security and privacy of the\nagentic applications and reduces the risks of breaches or attacks.\n\nDiagram from the GoEX blog post illustrating how calls are unredacted using\ncredentials from a Vault after the LLM generation phase\n\n#### Sandboxing generated calls\n\nThe generated actions\u2019 code to call APIs is executed inside a docker container\n(see code source). This is an improvement over executing the code directly on\nthe user\u2019s machine as it prevents basic exploits but as mentioned in the\npaper, the docker runtime can still be jail broken and there are additional\nsandboxing tools that could be integrated in GoEX to make it safer.\n\n#### Mitigating Risks with GoEX for more reliable and safer agentic\napplications\n\nGoEX actively addresses the Responsible AI principle of \"Reliability and\nSafety\" by incorporating an innovative 'undo' mechanism within its system.\nThis key feature allows for the reversion of actions executed by the AI, which\nis crucial in maintaining operational safety and enhancing overall system\nreliability. It acknowledges the fallibility of autonomous agents and ensures\nthere is a contingency in place to maintain user trust.\n\n#### More privacy and security means wider adoption of agentic applications\n\nAnother important Responsible AI principle is \"Privacy and Security\". In that\nregard, GoEX adopts a stringent approach by architecting its systems to\nconceal sensitive information such as secrets and credentials from the LLM. By\ndoing so, GoEX prevents the AI from inadvertently exposing or misusing private\ndata, reinforcing its commitment to safeguarding user privacy and ensuring a\nsecure AI-operating environment. This careful handling of confidential\ninformation underlines the project's dedication to upholding these essential\nfacets of Responsible AI.\n\n#### Conclusion\n\nIn conclusion, while the challenges of ensuring the reliability of LLM-\ngenerated code and the security of API interactions remain complex and\nongoing, GoEX's approach is a notable advancement in addressing these issues.\nThe project acknowledges that complete solutions are a work in progress, yet\nit sets a precedent for the level of diligence and foresight required to move\ncloser to these ideals. By focusing on these critical areas, GoEX contributes\nvaluable insights and methodologies that serve as stepping stones for the AI\ncommunity, signaling a directional shift towards more trustworthy and secure\nAI agents.\n\nNote: The features and methods described in this blog post and the paper are\nstill under active development and research. They are not all currently\nimplemented, available or ready for prime time in the GoEX Github repository.\n\nCedric Vidal\n\n0 Likes\n\nLike\n\nYou must be a registered user to add a comment. If you've already registered,\nsign in. Otherwise, register and sign in.\n\n  * Comment\n\nCo-Authors\n\ncedricvidal\n\nVersion history\n\nLast update:\n\nApr 24 2024 10:52 AM\n\nUpdated by:\n\ncedricvidal\n\nLabels\n\n  * Artificial Intelligence 8\n  * Azure AI Studio 12\n  * Model Catalog 6\n  * Natural Language Processing 5\n\n## Share\n\n  * Share to LinkedIn\n  * Share to Facebook\n  * Share to Twitter\n  * Share to Reddit\n  * Share to Email\n\nSkip to Primary Navigation\n\nYour Privacy Choices\n\nAuto-suggest helps you quickly narrow down your search results by suggesting\npossible matches as you type.\n\nAuto-suggest helps you quickly narrow down your search results by suggesting\npossible matches as you type.\n\n", "frontpage": false}
