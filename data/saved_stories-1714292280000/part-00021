{"aid": "40185346", "title": "Andes: Optimizing the QoE of LLM text streaming services", "url": "https://llm-qoe.github.io/", "domain": "llm-qoe.github.io", "votes": 1, "user": "jaywonchung", "posted_at": "2024-04-28 01:54:52", "comments": 0, "source_title": "Defining and Enhancing Quality-of-Experience in LLM-Based Text Streaming Services \u00b7 Andes Blog", "source_text": "Defining and Enhancing Quality-of-Experience in LLM-Based Text Streaming\nServices \u00b7 Andes Blog\n\n# Defining and Enhancing Quality-of-Experience in LLM-Based Text Streaming\nServices\n\n# Defining and Enhancing Quality-of-Experience in LLM-Based Text Streaming\nServices\n\nJiachen Liu, Zhiyu Wu, Jae-Won Chung University of Michigan, Fan Lai UIUC,\nMyungjin Lee Cisco Systems, Mosharaf Chowdhury University of Michigan.\n\nPaper | GitHub (Coming soon)\n\nTL;DR: Large language models (LLMs) have revolutionized text-based\ninteractions, enabling services from real-time translation to AI-driven\nchatbots. By streaming tokens to users, akin to video streaming, such text\nstreaming service allows users to digest the content incrementally, whether in\ntext or speech form. However, existing serving systems primarily focus on\noptimizing server-side aggregated metrics while ignoring individual user\nexperience, leading to unfavorable service quality or poor Quality-of-\nExperience (QoE) under high and/or bursty load.\n\nIn this project, we first formally define QoE in text streaming services by\nconsidering the end-to-end token delivery process. Thereafter, we propose\nAndes, a QoE-aware serving system that enhances user experience. Andes\nachieves this by strategically scheduling multiple requests on contended GPU\nresources, prioritizing them based on their resource demands and service\nacquired. Our evaluations demonstrate that, compared to the state-of-the-art\nLLM serving systems like vLLM, Andes improves the average QoE by up to 3.2\u00d7\nunder high request rate, or alternatively, it attains up to 1.6\u00d7 higher\nrequest rate while preserving high QoE.\n\n## A User-Side Story\n\nImagine three different scenarios where text is streamed to users. Despite all\nhaving the same efficiency in token generation throughput, their user\nexperiences vary dramatically:\n\nFigure 1 (a). Although all the responses complete within the same time, user\nexperiences vary significantly. Figure 1 (b). Number of accumulated tokens\ndisplayed over time.\n\nDespite generating the same response within the same time frame; even\nscenarios 1 and 3, which have identical average or P90 time per output token\nlatency, these scenarios deliver vastly different user experiences. The\nslowdown that happens in scenario 2 and 3 are common under high server loads,\nsuch as during bursty request periods or when managing requests with extensive\ncontext.\n\nIt is crucial that the initial response is prompt and that subsequent tokens\nare delivered at a pace aligned with the user\u2019s ability to digest them.\nHowever, the expected token delivery speed (TDS) differs from request to\nrequest. For instance, a chat service utilizing text-to-speech to deliver\nresponses may have different pacing requirements than a text-based chat\nservice, because a user\u2019s speaking speed is often slower than their reading\nspeed, but it may require smaller time to first token (TTFT) to better\nresemble real-life verbal conversations.\n\nIn sum, QoE in text streaming shouldn\u2019t be just another aggregated number to\ntrack; it needs to capture the entire user interaction from start to finish.\nPlease refer to Section 3.1 of our paper for more detailed QoE formulation.\n\n## System Imbalance and Opportunity\n\nCurrent first-come, first-served (FCFS) scheduling policy, commonly adopted in\nLLM serving systems, fails to account for the QoE requirements of individual\nrequests and cannot efficiently utilize resources, especially when the request\nload surpasses the server\u2019s capacity. As shown in Figure 2, they often lead to\nmisaligned user experiences, where the timing of token delivery doesn\u2019t\nnecessarily meet user needs.\n\nFigure 2 (a). Existing LLM serving systems are oblivious of QoE. User 2\nexperiences a long wait time(TTFT) and therefore lower QoE.\n\nFigure 2 (b). A QoE-aware LLM serving system can schedule token generation\nover time to enhance QoE. User 2\u2019s TTFT is drastically improved without\naffecting User 1\u2019s token delivery timeline.\n\nFigure 2. Server-side token generation timeline and user- side response\ndigestion progress. Even if the server generates tokens very fast, users\ncannot digest them at such a pace.\n\nWe notice that especially under high request load, uneven user experiences\narise as shown in Figure 3: (1) certain users may encounter extended time to\nfirst token, or TTFT; (2) conversely, other users might receive tokens at a\npace (TDS) surpassing their digestion ability.\n\nFigure 3 (a). 90th-p TTFT increases dramatically as the request rate surpasses\nthe server\u2019s capacity.\n\nFigure 3 (b). Token generation speed is much faster than the user expected\nspeed.\n\nUsually, in order to preserve good user experience, the service provider must\nprovision more compute resources proportional to the excess request load,\nleading to higher costs.\n\nHowever, we observe that there is an opportunity to optimize user experience\nby balancing prolonged TTFT and excessively fast token generation speed. By\ntemporarily pausing the response generation for requests with already\nsufficient tokens generated, we can spare the limited GPU resources to other\npending requests. This approach leverages the disparity between the expected\nand actual token generation speeds, optimizing both resource efficiency and\nuser satisfaction.\n\n## Introducing Andes: Towards a Better User Experience in Text Streaming\nServices\n\nWe propose Andes, an LLM serving system that optimizes the overall QoE of text\nstreaming services. Andes employs a dynamic priority-based preemptive\nscheduler that operates at the granularity of tokens. Andes strategically\nallocates resources to more urgent requests and preempts requests that have\nalready received sufficient service, all to enhance QoE. Additionally, Andes\ntakes the resource demand of each request into account while prioritizing\nresources. Together, we formulate the request scheduling problem as a knapsack\nvariant and propose a heuristic to solve it.\n\nBy satisfying more requests with high QoE using the same amount of resource,\nAndes eliminates the need for additional resource provisioning, thus reducing\nLLM serving cost. Andes also co-designs a client-side token buffer that\ntemporarily withholds excess tokens and displays them to the user at their\nexpected pace. This design ensures users experience smooth token delivery,\noblivious to the intricacies of server-side scheduling or network\nfluctuations.\n\nFigure 4. Average QoE under different request rates using the ShareGPT\ndataset. We set the threshold to 0.9 as the minimum acceptable average QoE.\n\nIn our evaluation of Andes, we show\n\n  1. Andes improves the average QoE up to 3.2\u00d7 when the system experiences high/bursty load. Specifically, Andes significantly improves TTFT, while maintaining TDS above user expected speed.\n  2. Andes can manage up to 1.6\u00d7 higher request rates while preserving high QoE without additional resources, significantly reducing the serving cost.\n\n# results matching \"\"\n\n# No results matching \"\"\n\n", "frontpage": false}
