{"aid": "40198763", "title": "Self-Playing Adversarial Language Game Enhances LLM Reasoning", "url": "https://arxiv.org/abs/2404.10642", "domain": "arxiv.org", "votes": 2, "user": "rootforce", "posted_at": "2024-04-29 14:28:57", "comments": 0, "source_title": "Self-playing Adversarial Language Game Enhances LLM Reasoning", "source_text": "[2404.10642] Self-playing Adversarial Language Game Enhances LLM Reasoning\n\nSkip to main content\n\nWe gratefully acknowledge support from the Simons Foundation, University at\nBuffalo (SUNY), and all contributors. Donate\n\n> cs > arXiv:2404.10642\n\n# Computer Science > Computation and Language\n\narXiv:2404.10642 (cs)\n\n[Submitted on 16 Apr 2024]\n\n# Title:Self-playing Adversarial Language Game Enhances LLM Reasoning\n\nAuthors:Pengyu Cheng, Tianhao Hu, Han Xu, Zhisong Zhang, Yong Dai, Lei Han,\nNan Du\n\nView a PDF of the paper titled Self-playing Adversarial Language Game Enhances\nLLM Reasoning, by Pengyu Cheng and 6 other authors\n\nView PDF HTML (experimental)\n\n> Abstract:We explore the self-play training procedure of large language\n> models (LLMs) in a two-player adversarial language game called Adversarial\n> Taboo. In this game, an attacker and a defender communicate with respect to\n> a target word only visible to the attacker. The attacker aims to induce the\n> defender to utter the target word unconsciously, while the defender tries to\n> infer the target word from the attacker's utterances. To win the game, both\n> players should have sufficient knowledge about the target word and high-\n> level reasoning ability to infer and express in this information-reserved\n> conversation. Hence, we are curious about whether LLMs' reasoning ability\n> can be further enhanced by Self-Play in this Adversarial language Game\n> (SPAG). With this goal, we let LLMs act as the attacker and play with a copy\n> of itself as the defender on an extensive range of target words. Through\n> reinforcement learning on the game outcomes, we observe that the LLMs'\n> performance uniformly improves on a broad range of reasoning benchmarks.\n> Furthermore, iteratively adopting this self-play process can continuously\n> promote LLM's reasoning ability. The code is at this https URL.\n\nComments:| Preprint  \n---|---  \nSubjects:| Computation and Language (cs.CL); Machine Learning (cs.LG)  \nCite as:| arXiv:2404.10642 [cs.CL]  \n(or arXiv:2404.10642v1 [cs.CL] for this version)  \nhttps://doi.org/10.48550/arXiv.2404.10642arXiv-issued DOI via DataCite  \n  \n## Submission history\n\nFrom: Pengyu Cheng [view email] [v1] Tue, 16 Apr 2024 15:16:22 UTC (2,136 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Self-playing Adversarial Language Game Enhances\nLLM Reasoning, by Pengyu Cheng and 6 other authors\n\n  * View PDF\n  * HTML (experimental)\n  * TeX Source\n  * Other Formats\n\nview license\n\nCurrent browse context:\n\ncs.CL\n\n< prev | next >\n\nnew | recent | 2404\n\nChange to browse by:\n\ncs cs.LG\n\n### References & Citations\n\n  * NASA ADS\n  * Google Scholar\n  * Semantic Scholar\n\na export BibTeX citation Loading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer (What is the Explorer?)\n\nLitmaps (What is Litmaps?)\n\nscite Smart Citations (What are Smart Citations?)\n\n# Code, Data and Media Associated with this Article\n\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\n\nDagsHub (What is DagsHub?)\n\nGotit.pub (What is GotitPub?)\n\nPapers with Code (What is Papers with Code?)\n\nScienceCast (What is ScienceCast?)\n\n# Demos\n\nReplicate (What is Replicate?)\n\nHugging Face Spaces (What is Spaces?)\n\nTXYZ.AI (What is TXYZ.AI?)\n\n# Recommenders and Search Tools\n\nInfluence Flower (What are Influence Flowers?)\n\nConnected Papers (What is Connected Papers?)\n\nCORE Recommender (What is CORE?)\n\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new\narXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and\naccepted our values of openness, community, excellence, and user data privacy.\narXiv is committed to these values and only works with partners that adhere to\nthem.\n\nHave an idea for a project that will add value for arXiv's community? Learn\nmore about arXivLabs.\n\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n\n  * About\n  * Help\n\n  * Contact\n  * Subscribe\n\n  * Copyright\n  * Privacy Policy\n\n  * Web Accessibility Assistance\n  * arXiv Operational Status Get status notifications via email or slack\n\n", "frontpage": false}
