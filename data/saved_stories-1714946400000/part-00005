{"aid": "40263675", "title": "Community \u2013 Navigating through failures, build resilient serverless systems", "url": "https://jimmydqv.com/navigating-failures/index.html", "domain": "jimmydqv.com", "votes": 1, "user": "kiyanwang", "posted_at": "2024-05-05 10:21:16", "comments": 0, "source_title": "Navigating through failures, build resilient serverless systems", "source_text": "Navigating through failures, build resilient serverless systems | Jimmy Dahlqvist\n\nJimmy Dahlqvist AWS Ambassador | AWS Community Builder\n\n# Navigating through failures, build resilient serverless systems\n\n2024-04-26\n\nVoice provided by Amazon Polly\n\nNavigating failures! Building resilient serverless workloads!\n\nI have been building and architecting serverless systems for almost a decade\nnow, for a variety of companies, from small start-ups to large enterprises.\nAnd I have seen many strange things over the years.\n\nServerless services from AWS come with high availability and resiliency built\nin. But that is on the service level, AWS Lambda, StepFunctions, EventBridge,\nand so on are all highly available and have high resiliency.\n\nIf all components in our systems were serverless that would be great. However,\nthat is not the case. In almost all systems that I have designed or worked on\nthere has been components that are not serverless. It can be the need for a\nrelational database, and yes, I do argue that not all data-models and search\nrequirements can't be designed for DynamoDB. It can be that we need to\nintegrate with a 3rd party and their API and connections come with quotas and\nthrottling. It could be that we as developers are unaware of certain traits of\nAWS services that make our serverless services break. And even sometimes our\nusers are even our worst enemy. When we are building our serverless systems we\nmust always remember that there are components involved that don't scale as\nfast or to that degree that serverless components does.\n\nIn this post I'm going to give some thoughts on navigating failures and\nbuilding a high resiliency serverless system. This post is not about\npreventing failures, instead it's about handling and recover from failures in\na good way. I'll start with a common serverless architecture and add\narchitecture concepts, that I often use, that can help enhance its resiliency.\n\nMany of the concepts in this post can also be found on My serverless Handbook\nall with different implementation examples and fully working and deployable\ninfrastructure using CloudFormation and SAM.\n\n## What is serverless\n\nMy definition of serverless is probably the same as for many of you and AWS as\nwell. Serverless services come with automatic and flexible scaling, scale down\nto zero and up to infinity (almost). Serverless services has little to none\ncapacity planning, I should not have to do much planning how much resources I\nneed before hand. Serverless services has a pay-for-use, if I don't use it I\ndon't pay for it.\n\nBut. this is not black or white, there is a grey zone I think. Where services\nlike Fargate and Kinesis Data Stream ends up.\n\n## Service categories\n\nLooking at services from AWS, in the red corner we have serverless services\nAPI gateway, Lambda, SQS, StepFunctions, DynamoDB and more. Services that we\ncan throw a ton of work on and they will just happily scale up and down, to\nhandle our traffic.\n\nIn the blue corner we have the managed services. This is services like Amazon\nAurora, ElasticCache, OpenSearch. This is services that can scale basically to\ninfinity, but they do require some capacity planning for that to happen, and\nif we plan incorrectly we can be throttled or even have failing requests. I\nwill also put the grey zone services, like Fargate and Kinesis Data Streams in\nthis blue corner.\n\nThen there is the server corner, that would be anything with EC2 instances. We\ndon't speak about them.....\n\n## What is resiliency\n\nSo, what is resiliency?\n\nSometimes it gets confusing, and people mix up resiliency with reliability. As\nmentioned in the beginning, resiliency is not about preventing failures, it's\nabout recovering from them. It\u2019s about making sure our system maintain an\nacceptable level of service even if when other parts of our system is not\nhealthy. It's about gracefully deal with failures.\n\nReliability focuses on the prevention of the failure happening in the first\nplace, while resiliency is about recovering from it.\n\n## Everything fails all the time\n\nThis is by far one of my favorite quotes by Dr. Werner Vogels. Because this is\nreal life! Running large distributed systems, then everything will eventually\nfail. We can have down stream services that is not responding as we expect,\nthey can be having health problems. Or we can be throttled by a 3rd party or\neven our own services.\n\n## Cracks\n\nIt's important that the cracks that form when components in our system fail,\ndoesn't spread. That they don't take down our entire system. We need ways to\nhandle and contain the cracks. That way we can isolate and protect our entire\nsystem.\n\nWhen our serverless systems integrate with non-serverless components. Which in\nsome cases it can be obvious, like when our system interacts with an Amazon\nAurora database. Other times it's not that clear, the system integrates with a\n3rd party API that can lead to throttling that can affect our system and start\nforming cracks if not handled properly.\n\nHow does our system handle a integration point that is not responding?\nSpecially under a period of high load. This can easily start creating cracks\nthat can bring our entire system to a halt or that we start loosing data.\n\n## Everything has a limit\n\nWhen we build serverless systems we must remember that every API in AWS has a\nlimit. We can store application properties in System Manager Parameter Store,\na few of them might be sensitive and encrypted with KMS. What now happens is\nthat we can get throttled by a different service without realizing it. SSM\nmight have a higher limit but getting an encrypted value would then be\nimpacted by the KMS limit. If we then don't design our functions correctly,\nand call SSM in the Lambda handler on every invocation we would quickly get\nthrottles. Instead we could load properties in the initialization phase.\n\nUnderstanding how AWS services work under the hood, to some extent, is\nextremely important, so our systems doesn't fail due to some unknown kink. For\nexample, consuming a Kinesis Data Stream with a Lambda function, if processing\nan item in a batch fails, the entire batch will fail. The batch would then be\nsent to the Lambda function over and over again.\n\nWhat we can do in this case is to bisect batches on Lambda function failures.\nThe processed batch will be split in half and sent to the function. Bisect\nwould continue to we only have the single failing item left.\n\n## Resiliency testing\n\nNow, I bet most of you run a multi-environment system, you have your dev,\ntest, pre-prod, and prod environments. Many would probably say that your QA,\nStaging, Pre-prod, or what ever you call it, has an identical setup with your\nprod environment. But now, let's make sure we consider data and integrations\nas well. The amount of data, the difference in user generated data, the\ncomplexity in data, difference in integrations with 3rd party. I have seen\nsystem been taken down on multiple occasions due to differences in data and\nintegration points. Everything works in staging but then fails in production.\n\nOne large system I worked on we had a new advanced feature that had been\ntested and prepared in all environments. But, when deploying to production,\nthe database went haywire on us. We used Amazon Aurora serverless and the\ndatabase suddenly scaled out to max and then could handle the load anymore.\nOur entire service was brought down. This was caused by a SQL query that due\nto the amount of data in production consumed all database resources, in a\nnasty join.\n\nIn a different system, I had a scenario where in production a 3rd party\nintegration had an IP-Allow list in place, so when we extended our system and\ngot some new IPs suddenly only one third of our calls was allowed and a\nsuccess. In the staging environment, the 3rd party didn't have any IP-blocks.\nIntermittent failures are always the most fun to debug.\n\nA good way to practice and prepare for failures are through Resiliency\ntesting, chaos engineering. AWS offers their service around this topic, AWS\nFault Injection Service, which you can use to simulate failures and see how\nyour components and system handles them. What I'm saying is that when you plan\nfor your Resiliency testing, start in your QA or staging environment. But,\ndon't forget about production and do plan to run test there as well.\n\n## Classic web application\n\nNow let's start off with a classic web application, single page application\nwith an API. SPA hosted from S3 and CloudFront, API in API Gateway and compute\nin Lambda, finally a database in DynamoDB. That is one scalable application!\n\nBut, maybe we can't model our data in DynamoDB? Not everything will fit in a\nkey/value model, we need an relational database like Aurora, or we need to\nintegrate with 3rd party. This could be an integration that still run on-prem,\nit could be running in a different cloud on servers. With any form of compute\nsolution that doesn\u2019t scale as fast and flexible as our serverless solution.\nThis application is setup as a classic synchronous request-response where our\nclient expect a response back immediate to the request.\n\nMost developers are very familiar with the synchronous request-response\npattern. We send an request and get a response back. We wait for this entire\nprocess to happen, with more complex integrations with chained calls and even\n3rd party integrations the time quickly adds up, and if one of the components\nis down and not responding we need to fail the entire operation, and we leave\nany form of retries to the calling application.\n\n## Do we need an immediate response?\n\nOne question we need to ask when building our APIs is does our write\noperations really need an immediate response? Can we make this an asynchronous\nprocess? Now, building with a synchronous request-response is less complex\nthan an asynchronous system. However, in a distributed system, do the calling\napplication need to know that we have stored the data already? Or can we just\nhand over the event and send a response back saying that \"Hey I got the\nmessage and I will do something with it\". In an event-driven system\nasynchronous requests are very common.\n\n### Buffer events\n\nWith an asynchronous system we can add an buffer between our calls and storage\nof our data. What this will do is protect us and the downstream services. The\ndownstream service will not be overwhelmed and by that we protect our own\nsystem as well from failures. This can however create an eventual consistency\nmodel, where read after write not always gives us the same data back.\n\n## Storage-first\n\nBuffers leads us to our first good practice when building a high resiliency\nsystem, storage-first. The idea behind the storage-first pattern is to not do\nimmediate processing of the data. Instead we directly integrate with a service\nfrom AWS, it could be SQS, EventBridge or any other service with a durable\nstorage mechanism. I use the storage-first pattern in basically all systems I\ndesign and build.\n\nSo, let's get rid of the Lambda integration and instead integrate directly to\nthe SQS. It doesn't have to be SQS it can be any service. The major benefits\nwith the storage first pattern is that the chance of us loosing any data is\nvery slim. We store it in a durable service and the process it as we see fit.\nEven if processing would fail we can still handle and process it later.\n\nFor more implementation examples check out My serverless Handbook and Storage-\nFirst Pattern\n\n## Queue Load leveling\n\nWhen using storage-first pattern with SQS we have the possibility to use one\nmore pattern, that I frequently use, the queue load leveling pattern, here we\ncan protect the downstream service, and by doing that our self, by only\nprocessing events in a pace that we know the service can handle. Other\nbenefits that come with this pattern, that might not be that obvious, is that\nit can help us control. We could run on subscriptions with lower throughput\nthat is lower in cost, when integrating with a 3rd party. We could also down-\nscale our database, as we don't need to run a huge instance to deal with\npeaks. Same goes if we don't process the queue with Lambda functions but\ninstead use containers in Fargate, we can set the scaling to fewer instances\nor even do a better auto-scaling solution.\n\nOne consideration with this pattern is that if our producers are always\ncreating more requests than we can process, we can end up in a situation where\nwe are always trailing behind. For that scenario we either need to scale up\nthe consumers, which might lead to unwanted downstream consequences or we need\nat some point evict and throw away messages. What we choose, and how we do it,\nof course come with the standard architect answer \"It depends....\"\n\n## Fan out\n\nA good practice in a distributed system is to make sure services can scale and\nfail independently. That mean that we can have more than one service that it\nis interested in the request. For a SQS queue we can only have one consumer,\ntwo consumers can't get the same message. In this case we need to create a fan\nout or multicast system. We can replace our queue with EventBridge that can\nroute the request or the message to many different services. It can be SQS\nqueues, StepFunctions, Lambda functions, other EventBridge buses. EventBridge\nis highly scalable with high availability and resiliency with a built in retry\nmechanism for 24 hours. With the archive feature we can also replay messages\nin case they failed. And if there is a problem delivering message to a target\nwe can set a DLQ to handle that scenario.\n\nThis is however one of the kinks that we need to make sure we are aware of.\nRemember that the DLQ only come into affect if there is a problem calling the\ntarget, lacking IAM permissions or similar. If the target it self has a\nproblem and fails processing, message will not end up in the DLQ.\n\n## Retry with backoff\n\nEven with a storage-first approach we are of course not protected against\nfailures. They will happen, remember \"Everything fails all the time\".\n\nIn the scenarios where our processing do fail we need to retry again. But,\nretries are selfish and what we don't want to do, in case it's a downstream\nservices that fail, or if we are throttled by the database, is to just retry\nagain. Instead we like to backoff and give the service som breathing room. We\nwould also like to apply exponential backoff, so if our second call also fails\nwe like to back off a bit more. So first retry we do after 1 second, then 2,\nthen 4, and so on till we either timeout and give up of have a success.\n\n### Retry with backoff and jitter\n\nThere is a study conducted by AWS, a couple of years ago, that show that in a\nhighly distributed system retries will eventually align. If all retries happen\nwith the same backoff, 1 second, 2 seconds, 4 seconds and so on they will\neventually line up and happen at the same time. This can then lead to the\ndownstream service crashing directly after becoming healthy just due to the\namount of job that has stacked up and now happen at the same time. It's like\nin an electric grid, after a power failure, all appliances turn on at the same\ntime creating such a load on the grid that it go out again, or we blow a fuse.\nWe change the fuse, everything turn on at the same time, and the fuse blow\nagain.\n\nTherefor we should also use some form of jitter in our backoff algorithm. This\ncould be that we add a random wait time to the backoff time. It would work\nthat we first wait 1 second + a random number of hundreds of milliseconds.\nSecond time we wait 2 second + 2x a random number, and so on. By doing that,\nour services will not line up the retries. How we add the jitter and how much,\nthat well depends on your system and implementation.\n\n### StepFunctions has it built in\n\nA good way to handle retries is to use StepFunctions as a wrapper around our\nLambda functions. StepFunctions has a built in retry mechanism which we can\nutilize in our solutions.\n\n## Dead Letter Queues\n\nIn some cases we just have to give up the processing. We have hit the max\nnumber of retries, we can't continue forever, this is where the DLQ come in.\nWe route the messages to a DLQ where we can use a different retry logic or\neven inspect the messages manually. The DLQ also create a good indicator that\nsomething might be wrong, and we can create alarms and alerts based on number\nof messages in it. One message might not be an problem but if the number of\nmessages start stacking up it's a clear indicator that something is wrong.\n\nIn case we are using SQS as our message buffer we can directly connect a DLQ\nto it. If we use StepFunctions as our processor we can send messages to a SQS\nqueue if we reach our retry limit.\n\nFor a implementation examples check out My serverless Handbook and retries\n\n## Circuit breaker\n\nFor the final part in our system we look at the circuit breaker. Retries are\nall good, but there is no point for us to send requests to an integration that\nwe know is not healthy, it will just keep failing over and over again. This is\nwhere this pattern comes in.\n\nIf you are not familiar with Circuit breakers it is a classic pattern, and\nwhat it does is make sure we don't send requests to API, services, or\nintegration that is not healthy and doesn't respond. This way we can both\nprotect the downstream service but also our self from doing work we know will\nfail. Because everything fails all the time, right.\n\nBefore we call the service we'll introduce a status check, if the service is\nall healthy we'll send the request this is a closed state of the circuit\nbreaker. Think of it as an electric circuit, when the circuit is closed\nelectricity can flow and the lights are on. As we do make calls to the service\nwe'll update the status, if we start to get error responses on our requests\nwe'll open the circuit and stop sending requests. In this state is where\nstorage-first shine, we can keep our messages in the storage queue until the\nintegration is back healthy again. What we need to consider is when to open\nthe circuit, if we have 1000 requests per minute we don't want to open the\ncircuit just because 10 fails. Also, checking the status before every\ninvocation might also be a bit inefficient, so here we need to find a good\nbalance when to check the status and when to open the circuit.\n\nBut! We just can't stop sending requests for ever. What we do is to\nperiodically place the circuit in a half-open state to send a few requests to\nit and update our status with the health from these requests.\n\nIn case of an SQS queue we can't just stop calling the Lambda integration,\ninstead we need to have some logic to add and remove the integration.\n\n## Putting it all together\n\nIf now put all of the different patterns together we'll have serverless system\nthat should be able to handle all kinds of strange problems.\n\n## Final Words\n\nIn this post we have looked at how I normally design and build systems to make\nthem handle and withstand failures. There is of course more to a ultra robust\nsystem and we always need to consider what happens if something fails. The\ndifferent patterns and implementations in this post should however halp you a\nbit on your journey to resiliency.\n\nCheck out My serverless Handbook for some of the concepts mentioned in this\npost.\n\nDon't forget to follow me on LinkedIn and X for more content, and read rest of\nmy Blogs\n\nAs Werner says! Now Go Build!\n\n\u00a9 2024 Jimmy Dahlqvist.\n\n", "frontpage": false}
