{"aid": "40049521", "title": "How Not to Run an A/B Test", "url": "https://www.evanmiller.org/how-not-to-run-an-ab-test.html", "domain": "evanmiller.org", "votes": 1, "user": "alexmolas", "posted_at": "2024-04-16 08:10:14", "comments": 0, "source_title": "How Not To Run an A/B Test", "source_text": "How Not To Run an A/B Test \u2013 Evan Miller\n\n# How Not To Run an A/B Test\n\nBy Evan Miller\n\nApril 18, 2010\n\nTranslations: Russian Spanish Uzbek\n\nIf you run A/B tests on your website and regularly check ongoing experiments\nfor significant results, you might be falling prey to what statisticians call\nrepeated significance testing errors. As a result, even though your dashboard\nsays a result is statistically significant, there\u2019s a good chance that it\u2019s\nactually insignificant. This note explains why.\n\n## Background\n\nWhen an A/B testing dashboard says there is a \u201c95% chance of beating original\u201d\nor \u201c90% probability of statistical significance,\u201d it\u2019s asking the following\nquestion: Assuming there is no underlying difference between A and B, how\noften will we see a difference like we do in the data just by chance? The\nanswer to that question is called the significance level, and \u201cstatistically\nsignificant results\u201d mean that the significance level is low, e.g. 5% or 1%.\nDashboards usually take the complement of this (e.g. 95% or 99%) and report it\nas a \u201cchance of beating the original\u201d or something like that.\n\nHowever, the significance calculation makes a critical assumption that you\nhave probably violated without even realizing it: that the sample size was\nfixed in advance. If instead of deciding ahead of time, \u201cthis experiment will\ncollect exactly 1,000 observations,\u201d you say, \u201cwe\u2019ll run it until we see a\nsignificant difference,\u201d all the reported significance levels become\nmeaningless. This result is completely counterintuitive and all the A/B\ntesting packages out there ignore it, but I\u2019ll try to explain the source of\nthe problem with a simple example.\n\n## Example\n\nSuppose you analyze an experiment after 200 and 500 observations. There are\nfour things that could happen:\n\nScenario 1| Scenario 2| Scenario 3| Scenario 4  \n---|---|---|---  \nAfter 200 observations| Insignificant| Insignificant| Significant!|\nSignificant!  \nAfter 500 observations| Insignificant| Significant!| Insignificant|\nSignificant!  \nEnd of experiment| Insignificant| Significant!| Insignificant| Significant!  \n  \nAssuming treatments A and B are the same and the significance level is 5%,\nthen at the end of the experiment, we\u2019ll have a significant result 5% of the\ntime.\n\nBut suppose we stop the experiment as soon as there is a significant result.\nNow look at the four things that could happen:\n\nScenario 1| Scenario 2| Scenario 3| Scenario 4  \n---|---|---|---  \nAfter 200 observations| Insignificant| Insignificant| Significant!|\nSignificant!  \nAfter 500 observations| Insignificant| Significant!| trial stopped| trial\nstopped  \nEnd of experiment| Insignificant| Significant!| Significant!| Significant!  \n  \nThe first row is the same as before, and the reported significance levels\nafter 200 observations are perfectly fine. But now look at the third row. At\nthe end of the experiment, assuming A and B are actually the same, we\u2019ve\nincreased the ratio of significant relative to insignificant results.\nTherefore, the reported significance level \u2013 the \u201cpercent of the time the\nobserved difference is due to chance\u201d \u2013 will be wrong.\n\n## How big of a problem is this?\n\nSuppose your conversion rate is 50% and you want to test to see if a new logo\ngives you a conversion rate of more than 50% (or less). You stop the\nexperiment as soon as there is 5% significance, or you call off the experiment\nafter 150 observations. Now suppose your new logo actually does nothing. What\npercent of the time will your experiment wrongly find a significant result? No\nmore than five percent, right? Maybe six percent, in light of the preceding\nanalysis?\n\nTry 26.1% \u2013 more than five times what you probably thought the significance\nlevel was. This is sort of a worst-case scenario, since we\u2019re running a\nsignificance test after every observation, but it\u2019s not unheard-of. At least\none A/B testing framework out there actually provides code for automatically\nstopping experiments after there is a significant result. That sounds like a\nneat trick until you realize it\u2019s a statistical abomination.\n\nRepeated significance testing always increases the rate of false positives,\nthat is, you\u2019ll think many insignificant results are significant (but not the\nother way around). The problem will be present if you ever find yourself\n\u201cpeeking\u201d at the data and stopping an experiment that seems to be giving a\nsignificant result. The more you peek, the more your significance levels will\nbe off. For example, if you peek at an ongoing experiment ten times, then what\nyou think is 1% significance is actually just 5% significance. Here are other\nreported significance values you need to see just to get an actual\nsignificance of 5%:\n\nYou peeked...| To get 5% actual significance you need...  \n---|---  \n1 time| 2.9% reported significance  \n2 times| 2.2% reported significance  \n3 times| 1.8% reported significance  \n5 times| 1.4% reported significance  \n10 times| 1.0% reported significance  \n  \nDecide for yourself how big a problem you have, but if you run your business\nby constantly checking the results of ongoing A/B tests and making quick\ndecisions, then this table should give you goosebumps.\n\n## What can be done?\n\nIf you run experiments: the best way to avoid repeated significance testing\nerrors is to not test significance repeatedly. Decide on a sample size in\nadvance and wait until the experiment is over before you start believing the\n\u201cchance of beating original\u201d figures that the A/B testing software gives you.\n\u201cPeeking\u201d at the data is OK as long as you can restrain yourself from stopping\nan experiment before it has run its course. I know this goes against something\nin human nature, so perhaps the best advice is: no peeking!\n\nSince you are going to fix the sample size in advance, what sample size should\nyou use? This formula is a good rule of thumb:\n\nn=16\u03c32\u03b42\n\nWhere \u03b4 is the minimum effect you wish to detect and \u03c32 is the sample variance\nyou expect. Of course you might not know the variance, but if it\u2019s just a\nbinomial proportion you\u2019re calculating (e.g. a percent conversion rate) the\nvariance is given by:\n\n\u03c32=p\u00d7(1\u2212p)\n\nCommitting to a sample size completely mitigates the problem described here.\n\nUPDATE, May 2013: You can see this formula in action with my new interactive\nSample Size Calculator. Enter the effect size you wish to detect, set the\npower and significance levels, and you'll get an easy-to-read number telling\nyou the sample size you need. END OF UPDATE\n\nIf you write A/B testing software: Don\u2019t report significance levels until an\nexperiment is over, and stop using significance levels to decide whether an\nexperiment should stop or continue. Instead of reporting significance of\nongoing experiments, report how large of an effect can be detected given the\ncurrent sample size. That can be calculated with:\n\n\u03b4=(t\u03b1/2+t\u03b2)\u03c3\u221a2/n\n\nWhere the two t\u2019s are the t-statistics for a given significance level \u03b1/2 and\npower (1\u2212\u03b2).\n\nPainful as it sounds, you may even consider excluding the \u201ccurrent estimate\u201d\nof the treatment effect until the experiment is over. If that information is\nused to stop experiments, then your reported significance levels are garbage.\n\nIf you really want to do this stuff right: Fixing a sample size in advance can\nbe frustrating. What if your change is a runaway hit, shouldn\u2019t you deploy it\nimmediately? This problem has haunted the medical world for a long time, since\nmedical researchers often want to stop clinical trials as soon as a new\ntreatment looks effective, but they also need to make valid statistical\ninferences on their data. Here are a couple of approaches used in medical\nexperiment design that someone really ought to adapt to the web:\n\n  * Sequential experiment design: Sequential experiment design lets you set up checkpoints in advance where you will decide whether or not to continue the experiment, and it gives you the correct significance levels.\n\nLearn more: \u201cSimple Sequential A/B Testing\u201d\n\n  * Bayesian experiment design: With Bayesian experiment design you can stop your experiment at any time and make perfectly valid inferences. Given the real-time nature of web experiments, Bayesian design seems like the way forward.\n\nLearn more: \u201cBayesian A/B Testing\u201d\n\n## Conclusion\n\nAlthough they seem powerful and convenient, dashboard views of ongoing A/B\nexperiments invite misuse. Any time they are used in conjunction with a manual\nor automatic \u201cstopping rule,\u201d the resulting significance tests are simply\ninvalid. Until sequential or Bayesian experiment designs are implemented in\nsoftware, anyone running web experiments should only run experiments where the\nsample size has been fixed in advance, and stick to that sample size with\nnear-religious discipline.\n\n## Further reading\n\n### Repeated Significance Tests\n\nP. Armitage, C. K. McPherson, and B. C. Rowe. \u201cSignificance Tests on\nAccumulating Data,\u201d Journal of the Royal Statistical Society. Series A\n(General), Vol. 132, No. 2 (1969), pp. 235-244\n\n### Optimal Sample Sizes\n\nJohn A. List, Sally Sadoff, and Mathis Wagner. \u201cSo you want to run an\nexperiment, now what? Some Simple Rules of Thumb for Optimal Experimental\nDesign.\u201d NBER Working Paper No. 15701\n\nWheeler, Robert E. \u201cPortable Power,\u201d Technometrics, Vol. 16, No. 2 (May,\n1974), pp. 193-201\n\n### Sequential Experiment Design\n\nPocock, Stuart J. \u201cGroup Sequential Methods in the Design and Analysis of\nClinical Trials,\u201d Biometrika, Vol. 64, No. 2 (Aug., 1977), pp. 191-199\n\nPocock, Stuart J. \u201cInterim Analyses for Randomized Clinical Trials: The Group\nSequential Approach,\u201d Biometrics, Vol. 38, No. 1 (Mar., 1982), pp. 153-162\n\n### Bayesian Experiment Design\n\nBerry, Donald A. \u201cBayesian Statistics and the Efficiency and Ethics of\nClinical Trials,\u201d Statistical Science, Vol. 19, No. 1 (Feb., 2004), pp.\n175-187\n\nYou\u2019re reading evanmiller.org, a random collection of math, tech, and musings.\nIf you liked this you might also enjoy:\n\n  * The Low Base Rate Problem\n  * Simple Sequential A/B Testing\n  * You Can\u2019t Spell CUPED Without Frisch-Waugh-Lovell\n\nGet new articles as they\u2019re published, via LinkedIn, Twitter, or RSS.\n\nWant to look for statistical patterns in your MySQL, PostgreSQL, or SQLite\ndatabase? My desktop statistics software Wizard can help you analyze more data\nin less time and communicate discoveries visually without spending days\nstruggling with pointless command syntax. Check it out!\n\nWizard Statistics the Mac way\n\nBack to Evan Miller\u2019s home page \u2013 Subscribe to RSS \u2013 LinkedIn \u2013 Twitter\n\n", "frontpage": false}
