{"aid": "40280600", "title": "Exporting YouTube Subscriptions to OPML and Watching via RSS", "url": "https://www.wezm.net/v2/posts/2024/youtube-subscriptions-opml/", "domain": "wezm.net", "votes": 1, "user": "wezm", "posted_at": "2024-05-06 23:20:35", "comments": 0, "source_title": "Exporting YouTube Subscriptions to OPML and Watching via RSS", "source_text": "Exporting YouTube Subscriptions to OPML and Watching via RSS - Wesley Moore\n\n# \ud83d\udc68\ud83d\udcbb Wesley Moore\n\n## Exporting YouTube Subscriptions to OPML and Watching via RSS\n\n06 May 2024\n\n\u00b7updated 06 May 2024\n\nThis post describes how I exported my 500+ YouTube subscriptions to an OPML\nfile so that I could import them into my RSS reader. I go into fine detail\nabout the scripts and tools I used. If you just want to see the end result the\ncode is in this repository, which describes the steps needed to run it.\n\nI was previously a YouTube Premium subscriber but I cancelled it when they\njacked up the already high prices. Since then I\u2019ve been watching videos in\nNewPipe on my Android tablet or via an Invidious instance on real computers.\n\nTo import my subscriptions into NewPipe I was able to use the\nsubscriptions.csv file included in the Google Takeout dump of my YouTube data.\nThis worked fine initially but imposed some friction when adding new\nsubscriptions.\n\nIf I only subscribed to new channels in NewPipe they were only accessible on\nmy tablet. If I added them to YouTube then I had to remember to also add them\nin NewPipe, which was inconvenient if I wasn\u2019t using the tablet at the time.\nInevitably the subscriptions would drift out of sync and I would have to\nperiodically re-import the subscriptions from YouTube into NewPipe. This was\ncumbersome as it doesn\u2019t seem to have a way to do this incrementally. Last\ntime I had to nuke all its data in order to re-import.\n\nTo solve these problems I wanted to manage my subscriptions in my RSS reader,\nFeedbin. This way Feedbin would track my subscriptions and new/viewed videos\nin a way that would sync between all my devices. Notably this is possible\nbecause Google actually publishes an RSS feed for each YouTube channel.\n\nTo do that I needed to export all my subscriptions to an OPML file that\nFeedbin could import. I opted to do that without requesting another Google\nTakeout dump as they take a long time to generate and also result in multiple\ngigabytes of archives I have to download (it includes all the videos I\u2019ve\nuploaded to my personal account) just to get at the subscriptions.csv file\nwithin.\n\n### Generating OPML\n\nI started by visiting my subscriptions page and using some JavaScript to\ngenerate a JSON array of all the channels I am subscribed to:\n\n    \n    \n    copy(JSON.stringify(Array.from(new Set(Array.prototype.map.call(document.querySelectorAll('a.channel-link'), (link) => link.href))).filter((x) => !x.includes('/channel/')), null, 2))\n\nThis snippet:\n\n  * queries the page for all channel links\n  * gets the link URL of each matching element\n  * Creates a Set from them to de-duplicate them\n  * Turns the set back into an Array\n  * filters out ones that contain /channel/ to exclude some links like Trending that also appear on that page\n  * Turns the Array into pretty printed JSON\n  * Copies it to the clipboard\n\nWith the list of channel URLs on my clipboard I pasted this into a\nsubscriptions.json file. The challenge now was that these URLs were of the\nchannel pages like:\n\nhttps://www.youtube.com/@mooretech\n\nbut the RSS URL of a channel is like:\n\nhttps://www.youtube.com/feeds/videos.xml?channel_id=<CHANNEL_ID>,\n\nwhich means I needed to determine the channel id for each page. To do that\nwithout futzing around with Google API keys and APIs I needed to download the\nHTML of each channel page.\n\nFirst I generated a config file for curl from the JSON file:\n\n    \n    \n    jaq --raw-output '.[] | (split(\"/\") | last) as $name | \"url \\(.)\\noutput \\($name).html\"' subscriptions.json > subscriptions.curl\n\njaq is an alternative implementation of jq that I use. This jaq expression\ndoes the following:\n\n  * .[] iterate over each element of the subscriptions.json array.\n  * (split(\"/\") | last) as $$name split the URL on / and take the last element, storing it in a variable called $name.\n\n    * for a URL like https://www.youtube.com/@mooretech this stores @mooretech in $name.\n  * \"url \\\\(.)\\noutput \\\\($$name).html\" generates the output text interpolating the channel page url and channel name.\n\nThis results in lines like this for each entry in subscriptions.json, output\nto subscriptions.curl:\n\n    \n    \n    url https://www.youtube.com/@mooretech output @mooretech.html\n\nI then ran curl against this file to download all the pages:\n\n    \n    \n    curl --location --output-dir html --create-dirs --rate 1/s --config subscriptions.curl\n\n  * \\--location tells curl to follow redirects, for some reason three of my subscriptions redirected to alternate names when accessed.\n  * \\--output-dir tells curl to output the files into the html directory.\n  * \\--create-dirs tells curl to create output directories if they don\u2019t exist (just the html one in this case).\n  * \\--rate 1/s tells curl to only download at a rate of 1 page per second\u2014I was concerned YouTube might block me if I requested the pages too quickly.\n  * \\--config subscriptions.curl tells curl to read additional command line arguments from the subscriptions.curl file generated above.\n\nNow that I had the HTML for each channel I needed to extract the channel id\nfrom it. While I was processing each HTML file I also extracted the channel\ntitle for use later. For each HTML file I ran this script on it. I called the\nscript generate-json-opml:\n\n    \n    \n    #!/bin/sh set -eu URL=\"$1\" NAME=$(echo \"$URL\" | awk -F / '{ print $NF }') HTML=\"html/${NAME}.html\" CHANNEL_ID=$(scraper -a content 'meta[property=\"og:url\"]' < \"$HTML\" | awk -F / '{ print $NF }') TITLE=$(scraper -a content 'meta[property=\"og:title\"]' < \"$HTML\") XML_URL=\"https://www.youtube.com/feeds/videos.xml?channel_id=${CHANNEL_ID}\" json_escape() { echo \"$1\" | jaq --raw-input . } JSON_TITLE=$(json_escape \"$TITLE\") JSON_XML_URL=$(json_escape \"$XML_URL\") JSON_URL=$(json_escape \"$URL\") printf '{\"title\": %s, \"xmlUrl\": %s, \"htmlUrl\": %s}\\n' \"$JSON_TITLE\" \"$JSON_XML_URL\" \"$JSON_URL\" > json/\"$NAME\".json\n\nLet\u2019s break that down:\n\n  * The channel URL is stored in URL.\n  * The channel name is determined by using awk to split the URL on / and take the last element.\n  * The path to the downloaded HTML page is stored in HTML.\n  * The channel id is determined by finding the <meta> tag in the html with a property attribute of og:url (the OpenGraph metadata URL property). This URL is again split on / and the last element stored in CHANNEL_ID.\n\n    * Querying the HTML is done with a tool called scraper that allows you to use CSS selectors to extract parts of a HTML document.\n  * The channel title is done similarly by extracting the value of the og:title metadata.\n  * The URL of the RSS feed for the channel is stored in XML_URL using CHANNEL_ID.\n  * A function to escape strings destined for JSON is defined. This makes use of jaq.\n  * TITLE, XML_URL, and URL are escaped.\n  * Finally we generate a JSON object with the title, URL, and RSS URL and write it into a json directory under the name of the channel.\n\nUpdate: Stephen pointed out on Mastodon that the HTML contains the usual <link\nrel=\"alternate\" tag for RSS auto-discovery. I did check for that initially but\nI think the Firefox dev tools where having a bad time with the large size of\nthe YouTube pages and didn\u2019t show me any matches at the time. Anyway, that\ncould have been used to find the feed URL directly instead of building it from\nthe og:url.\n\nOk, almost there. That script had to be run for each of the channel URLs.\nFirst I generated a file with just a plain text list of the channel URLs:\n\n    \n    \n    jaq --raw-output '.[]' subscriptions.json > subscriptions.txt\n\nThen I used xargs to process them in parallel:\n\n    \n    \n    xargs -n1 --max-procs=$(nproc) --arg-file subscriptions.txt --verbose ./generate-json-opml\n\nThis does the following:\n\n  * -n1 read one line from subscriptions.txt to be passed as the argument to generate-json-opml.\n  * \\--max-procs=$(nproc) run up the number of cores my machine has in parallel.\n  * \\--arg-file subscriptions.txt read arguments for generate-json-opml from subscriptions.txt.\n  * \\--verbose show the commands being run.\n  * ./generate-json-opml the command to run (this is the script above).\n\nFinally all those JSON files need to be turned into an OPML file. For this I\nused Python:\n\n    \n    \n    #!/usr/bin/env python import email.utils import glob import json import xml.etree.ElementTree as ET opml = ET.Element(\"opml\") head = ET.SubElement(opml, \"head\") title = ET.SubElement(head, \"title\") title.text = \"YouTube Subscriptions\" dateCreated = ET.SubElement(head, \"dateCreated\") dateCreated.text = email.utils.formatdate(timeval=None, localtime=True) body = ET.SubElement(opml, \"body\") youtube = ET.SubElement(body, \"outline\", {\"title\": \"YouTube\", \"text\": \"YouTube\"}) for path in glob.glob(\"json/*.json\"): with open(path) as f: info = json.load(f) ET.SubElement(youtube, \"outline\", info, type=\"rss\", text=info[\"title\"]) ET.indent(opml) print(ET.tostring(opml, encoding=\"unicode\", xml_declaration=True))\n\nThis generates an OPML file (which is XML) using the ElementTree library. The\nOPML file has this structure:\n\n    \n    \n    <?xml version='1.0' encoding='utf-8'?> <opml> <head> <title>YouTube Subscriptions</title> <dateCreated>Sun, 05 May 2024 15:57:23 +1000</dateCreated> </head> <body> <outline title=\"YouTube\" text=\"YouTube\"> <outline title=\"MooreTech\" xmlUrl=\"https://www.youtube.com/feeds/videos.xml?channel_id=UCLi0H57HGGpAdCkVOb_ykVg\" htmlUrl=\"https://www.youtube.com/@mooretech\" type=\"rss\" text=\"MooreTech\" /> </outline> </body> </opml>\n\nIt does the following:\n\n  * Generates the top level OPML structure.\n  * For each JSON file, read and parse the JSON and then use that to generate an outline entry for that channel.\n  * Indent the OPML document.\n  * Write it to stdout using a Unicode encoding with an XML declaration (<?xml version='1.0' encoding='utf-8'?>).\n\nWhew that was a lot! With the OPML file generated I was finally able to import\nall my subscriptions into Feedbin.\n\nAll the code is available in this repository. In practice I used a Makefile to\nrun the various commands so that I didn\u2019t have to remember them.\n\n### Watching videos from Feedbin\n\nNow that Feedbin is the source of truth for subscriptions, how do I actually\nwatch them? I set up the FeedMe app on my Android tablet. In the settings I\nenabled the NewPipe integration and set it to open the video page when tapped:\n\nScreenshot of the FeedMe integration settings\n\nNow when viewing an item in FeedMe there is a NewPipe button that I can tap to\nwatch it:\n\nScreenshot of FeedMe viewing a video item\n\n### Closing Thoughts & Future Work\n\nCould I have done all the processing to generate the OPML file with a single\nPython file? Yes, but I rarely write Python so I preferred to just cobble\nthings together from tools I already knew.\n\nShould I ever become a YouTube Premium subscriber again I can continue to use\nthis workflow and watch the videos from the YouTube embeds that Feedbin\ngenerates, or open the item in the YouTube app instead of NewPipe.\n\nAt some point I\u2019d like to work out how to get Feedbin to filter out YouTube\nShorts. It has the ability to automatically filter items matching any of the\nsupported search syntax but I\u2019m not sure if Shorts are easily identifiable.\n\nLastly, what about desktop usage? When I\u2019m on a real computer I read my RSS\nvia the Feedbin web app. It supports custom sharing integrations. In order to\nopen a video on an Invidious instance I need to rewrite it from a URL like:\n\nhttps://www.youtube.com/watch?v=u1wfCnRINkE\n\nto one like:\n\nhttps://invidious.perennialte.ch/watch?v=u1wfCnRINkE.\n\nI can\u2019t do that directly with a Feedbin custom sharing service definition but\nit would be trivial to set up a little redirector application to do it. I even\npublished a video on building a very similar thing last year. Alternatively I\ncould install a redirector browser plugin, although that would require set up\non each of the computers and OS installs I use so I prefer the former option.\n\n### Comments\n\n  * Fediverse\n  * Lobsters\n\n### Stay in touch!\n\nFollow me on the Fediverse, subscribe to the feed, or send me an email.\n\nAbout Archives \u2022 RSS Email Fediverse GitHub Support My Work\n\nCopyright \u00a9 2003 \u2013 2024 Wesley Moore \u2014 Website Source on GitHub\n\n", "frontpage": false}
