{"aid": "40163882", "title": "Apple releases eight small AI language models aimed at on-device use", "url": "https://arstechnica.com/information-technology/2024/04/apple-releases-eight-small-ai-language-models-aimed-at-on-device-use/", "domain": "arstechnica.com", "votes": 7, "user": "MBCook", "posted_at": "2024-04-25 22:30:51", "comments": 0, "source_title": "Apple releases eight small AI language models aimed at on-device use", "source_text": "Apple releases eight small AI language models aimed at on-device use | Ars Technica\n\nSkip to main content\n\nSubscribe\n\nClose\n\n### Navigate\n\n  * Store\n  * Subscribe\n  * Videos\n  * Features\n  * Reviews\n\n  * RSS Feeds\n  * Mobile Site\n\n  * About Ars\n  * Staff Directory\n  * Contact Us\n\n  * Advertise with Ars\n  * Reprints\n\n### Filter by topic\n\n  * Biz & IT\n  * Tech\n  * Science\n  * Policy\n  * Cars\n  * Gaming & Culture\n  * Store\n  * Forums\n\n### Settings\n\nFront page layout\n\nGrid\n\nList\n\nSite theme\n\nlight\n\ndark\n\nSign in\n\n#### Inside the Apple core \u2014\n\n# Apple releases eight small AI language models aimed at on-device use\n\n## OpenELM mirrors efforts by Microsoft to make useful small AI language\nmodels that run locally.\n\nBenj Edwards - 4/25/2024, 8:55 PM\n\nEnlarge\n\nGetty Images\n\n#### reader comments\n\n21\n\nIn the world of AI, what might be called \"small language models\" have been\ngrowing in popularity recently because they can be run on a local device\ninstead of requiring data center-grade computers in the cloud. On Wednesday,\nApple introduced a set of tiny source-available AI language models called\nOpenELM that are small enough to run directly on a smartphone. They're mostly\nproof-of-concept research models for now, but they could form the basis of\nfuture on-device AI offerings from Apple.\n\n### Further Reading\n\nApple aims to run AI models directly on iPhones, other devices\n\nApple's new AI models, collectively named OpenELM for \"Open-source Efficient\nLanguage Models,\" are currently available on the Hugging Face under an Apple\nSample Code License. Since there are some restrictions in the license, it may\nnot fit the commonly accepted definition of \"open source,\" but the source code\nfor OpenELM is available.\n\nOn Tuesday, we covered Microsoft's Phi-3 models, which aim to achieve\nsomething similar: a useful level of language understanding and processing\nperformance in small AI models that can run locally. Phi-3-mini features 3.8\nbillion parameters, but some of Apple's OpenELM models are much smaller,\nranging from 270 million to 3 billion parameters in eight distinct models.\n\nIn comparison, the largest model yet released in Meta's Llama 3 family\nincludes 70 billion parameters (with a 400 billion version on the way), and\nOpenAI's GPT-3 from 2020 shipped with 175 billion parameters. Parameter count\nserves as a rough measure of AI model capability and complexity, but recent\nresearch has focused on making smaller AI language models as capable as larger\nones were a few years ago.\n\nThe eight OpenELM models come in two flavors: four as \"pretrained\" (basically\na raw, next-token version of the model) and four as instruction-tuned (fine-\ntuned for instruction following, which is more ideal for developing AI\nassistants and chatbots):\n\nAdvertisement\n\n  * OpenELM-270M\n  * OpenELM-450M\n  * OpenELM-1_1B\n  * OpenELM-3B\n  * OpenELM-270M-Instruct\n  * OpenELM-450M-Instruct\n  * OpenELM-1_1B-Instruct\n  * OpenELM-3B-Instruct\n\nOpenELM features a 2048-token maximum context window. The models were trained\non the publicly available datasets RefinedWeb, a version of PILE with\nduplications removed, a subset of RedPajama, and a subset of Dolma v1.6, which\nApple says totals around 1.8 trillion tokens of data. Tokens are fragmented\nrepresentations of data used by AI language models for processing.\n\nApple says its approach with OpenELM includes a \"layer-wise scaling strategy\"\nthat reportedly allocates parameters more efficiently across each layer,\nsaving not only computational resources but also improving the model's\nperformance while being trained on fewer tokens. According to Apple's released\nwhite paper, this strategy has enabled OpenELM to achieve a 2.36 percent\nimprovement in accuracy over Allen AI's OLMo 1B (another small language model)\nwhile requiring half as many pre-training tokens.\n\nEnlarge / An table comparing OpenELM with other small AI language models in a\nsimilar class, taken from the OpenELM research paper by Apple.\n\nApple\n\nApple also released the code for CoreNet, a library it used to train\nOpenELM\u2014and it also included reproducible training recipes that allow the\nweights (neural network files) to be replicated, which is unusual for a major\ntech company so far. As Apple says in its OpenELM paper abstract, transparency\nis a key goal for the company: \"The reproducibility and transparency of large\nlanguage models are crucial for advancing open research, ensuring the\ntrustworthiness of results, and enabling investigations into data and model\nbiases, as well as potential risks.\"\n\n### Further Reading\n\nApple aims to run AI models directly on iPhones, other devices\n\nBy releasing the source code, model weights, and training materials, Apple\nsays it aims to \"empower and enrich the open research community.\" However, it\nalso cautions that since the models were trained on publicly sourced datasets,\n\"there exists the possibility of these models producing outputs that are\ninaccurate, harmful, biased, or objectionable in response to user prompts.\"\n\nWhile Apple has not yet integrated this new wave of AI language model\ncapabilities into its consumer devices, the upcoming iOS 18 update (expected\nto be revealed in June at WWDC) is rumored to include new AI features that\nutilize on-device processing to ensure user privacy\u2014though the company may\npotentially hire Google or OpenAI to handle more complex, off-device AI\nprocessing to give Siri a long-overdue boost.\n\n### Ars Video\n\n#### reader comments\n\n21\n\nBenj Edwards Benj Edwards is an AI and Machine Learning Reporter for Ars\nTechnica. In his free time, he writes and records music, collects vintage\ncomputers, and enjoys nature. He lives in Raleigh, NC.\n\nAdvertisement\n\n### Channel Ars Technica\n\n#### Unsolved Mysteries Of Quantum Leap With Donald P. Bellisario\n\nToday \"Quantum Leap\" series creator Donald P. Bellisario joins Ars Technica to\nanswer once and for all the lingering questions we have about his enduringly\npopular show. Was Dr. Sam Beckett really leaping between all those time\nperiods and people or did he simply imagine it all? What do people in the\nwaiting room do while Sam is in their bodies? What happens to Sam's loyal ally\nAl? 30 years following the series finale, answers to these mysteries and more\nawait.\n\n  * ##### Unsolved Mysteries Of Quantum Leap With Donald P. Bellisario\n\n  * ##### Unsolved Mysteries Of Warhammer 40K With Author Dan Abnett\n\n  * ##### SITREP: F-16 replacement search a signal of F-35 fail?\n\n  * ##### Sitrep: Boeing 707\n\n  * ##### Steve Burke of GamersNexus Reacts To Their Top 1000 Comments On YouTube\n\n  * ##### Modern Vintage Gamer Reacts To His Top 1000 Comments On YouTube\n\n  * ##### How The NES Conquered A Skeptical America In 1985\n\n  * ##### Scott Manley Reacts To His Top 1000 YouTube Comments\n\n  * ##### How Horror Works in Amnesia: Rebirth, Soma and Amnesia: The Dark Descent\n\n  * ##### LGR's Clint Basinger Reacts To His Top 1000 YouTube Comments\n\n  * ##### The F-35's next tech upgrade\n\n  * ##### How One Gameplay Decision Changed Diablo Forever\n\n  * ##### Unsolved Mortal Kombat Mysteries With Dominic Cianciolo From NetherRealm Studios\n\n  * ##### US Navy Gets an Italian Accent\n\n  * ##### How Amazon\u2019s \u201cUndone\u201d Animates Dreams With Rotoscoping And Oil Paints\n\n  * ##### Fighter Pilot Breaks Down Every Button in an F-15 Cockpit\n\n  * ##### How NBA JAM Became A Billion-Dollar Slam Dunk\n\n  * ##### Linus \"Tech Tips\" Sebastian Reacts to His Top 1000 YouTube Comments\n\n  * ##### How Alan Wake Was Rebuilt 3 Years Into Development\n\n  * ##### How Prince of Persia Defeated Apple II's Memory Limitations\n\n  * ##### How Crash Bandicoot Hacked The Original Playstation\n\n  * ##### Myst: The challenges of CD-ROM | War Stories\n\n  * ##### Markiplier Reacts To His Top 1000 YouTube Comments\n\n  * ##### How Mind Control Saved Oddworld: Abe's Oddysee\n\n  * ##### Bioware answers unsolved mysteries of the Mass Effect universe\n\n  * ##### Civilization: It's good to take turns | War Stories\n\n  * ##### SITREP: DOD Resets Ballistic Missile Interceptor program\n\n  * ##### Warframe's Rebecca Ford reviews your characters\n\n  * ##### Subnautica: A world without guns | War Stories\n\n  * ##### How Slay the Spire\u2019s Original Interface Almost Killed the Game | War Stories\n\n  * ##### Amnesia: The Dark Descent - The horror facade | War Stories\n\n  * ##### Command & Conquer: Tiberian Sun | War Stories\n\n  * ##### Blade Runner: Skinjobs, voxels, and future noir | War Stories\n\n  * ##### Dead Space: The Drag Tentacle | War Stories\n\n  * ##### Teach the Controversy: Flat Earthers\n\n  * ##### Delta V: The Burgeoning World of Small Rockets, Paul Allen's Huge Plane, and SpaceX Gets a Crucial Green-light\n\n  * ##### Chris Hadfield explains his 'Space Oddity' video\n\n  * ##### The Greatest Leap, Episode 1: Risk\n\n  * ##### Ultima Online: The Virtual Ecology | War Stories\n\nMore videos\n\n\u2190 Previous story Next story \u2192\n\n### Related Stories\n\n### Today on Ars\n\nCNMN Collection WIRED Media Group \u00a9 2024 Cond\u00e9 Nast. All rights reserved. Use of and/or registration on any portion of this site constitutes acceptance of our User Agreement (updated 1/1/20) and Privacy Policy and Cookie Statement (updated 1/1/20) and Ars Technica Addendum (effective 8/21/2018). Ars may earn compensation on sales from links on this site. Read our affiliate link policy. Your California Privacy Rights | Manage Preferences The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond\u00e9 Nast. Ad Choices\n\n## We Care About Your Privacy\n\nWe and our 167 partners store and/or access information on a device, such as\nunique IDs in cookies to process personal data. You may accept or manage your\nchoices by clicking below or at any time in the privacy policy page. These\nchoices will be signaled to our partners and will not affect browsing\ndata.More information about your privacy\n\n### We and our partners process data to provide:\n\nUse precise geolocation data. Actively scan device characteristics for\nidentification. Store and/or access information on a device. Personalised\nadvertising and content, advertising and content measurement, audience\nresearch and services development.\n\n", "frontpage": true}
