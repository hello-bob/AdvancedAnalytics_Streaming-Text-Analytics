{"aid": "40187722", "title": "Minimizing on-call burnout through alerts observability", "url": "https://blog.cloudflare.com/alerts-observability", "domain": "cloudflare.com", "votes": 1, "user": "kiyanwang", "posted_at": "2024-04-28 11:16:57", "comments": 0, "source_title": "Minimizing on-call burnout through alerts observability", "source_text": "Minimizing on-call burnout through alerts observability\n\nGet Started Free|Contact Sales\n\n## The Cloudflare Blog\n\nSubscribe to receive notifications of new posts:\n\n# Minimizing on-call burnout through alerts observability\n\n03/29/2024\n\n  * Monika Singh\n\n10 min read\n\n### Introduction\n\nMany people have probably come across the \u2018this is fine\u2019 meme or the original\ncomic. This is what a typical day for a lot of on-call personnel looks like.\nOn-calls get a lot of alerts, and dealing with too many alerts can result in\nalert fatigue \u2013 a feeling of exhaustion caused by responding to alerts that\nlack priority or clear actions. Ensuring the alerts are actionable and\naccurate, not false positives, is crucial because repeated false alarms can\ndesensitize on-call personnel. To this end, within Cloudflare, numerous teams\nconduct periodic alert analysis, with each team developing its own dashboards\nfor reporting. As members of the Observability team, we've encountered\nsituations where teams reported inaccuracies in alerts or instances where\nalerts failed to trigger, as well as provided assistance in dealing with\nnoisy/flapping alerts.\n\nObservability aims to enhance insight into the technology stack by gathering\nand analyzing a broader spectrum of data. In this blog post, we delve into\nalert observability, discussing its importance and Cloudflare's approach to\nachieving it. We'll also explore how we overcome shortcomings in alert\nreporting within our architecture to simplify troubleshooting using open-\nsource tools and best practices. Join us to understand how we use alerts\neffectively and use simple tools and practices to enhance our alerts\nobservability, resilience, and on-call personnel health.\n\nBeing on-call can disrupt sleep patterns, impact social life, and hinder\nleisure activities, potentially leading to burnout. While burnout can be\ncaused by several factors, one contributing factor can be excessively noisy\nalerts or receiving alerts that are neither important nor actionable.\nAnalyzing alerts can help mitigate the risk of such burnout by reducing\nunnecessary interruptions and improving the overall efficiency of the on-call\nprocess. It involves periodic review and feedback to the system for improving\nalert quality. Unfortunately, only some companies or teams do alert analysis,\neven though it is essential information that every on-call or manager should\nhave access to.\n\nAlert analysis is useful for on-call personnel, enabling them to easily see\nwhich alerts have fired during their shift to help draft handover notes and\nnot miss anything important. In addition, managers can generate reports from\nthese stats to see the improvements over time, as well as helping assess on-\ncall vulnerability to burnout. Alert analysis also helps with writing incident\nreports, to see if alerts were fired, or to determine when an incident\nstarted.\n\nLet\u2019s first understand the alerting stack and how we used open-source tools to\ngain greater visibility into it, which allowed us to analyze and optimize its\neffectiveness.\n\n### Prometheus architecture at Cloudflare\n\nAt Cloudflare, we rely heavily on Prometheus for monitoring. We have data\ncenters in more than 310 cities, and each has several Prometheis. In total, we\nhave over 1100 Prometheus servers. All alerts are sent to a central\nAlertmanager, where we have various integrations to route them. Additionally,\nusing an alertmanager webhook, we store all alerts in a datastore for\nanalysis.\n\n### Lifecycle of an alert\n\nPrometheus collects metrics from configured targets at given intervals,\nevaluates rule expressions, displays the results, and can trigger alerts when\nthe alerting conditions are met. Once an alert goes into firing state, it will\nbe sent to the alertmanager.\n\nDepending on the configuration, once Alertmanager receives an alert, it can\ninhibit, group, silence, or route the alerts to the correct receiver\nintegration, such as chat, PagerDuty, or ticketing system. When configured\nproperly, Alertmanager can mitigate a lot of alert noise. Unfortunately, that\nis not the case all the time, as not all alerts are optimally configured.\n\nIn Alertmanager, alerts initially enter the firing state, where they may be\ninhibited or silenced. They return to the firing state when the silence\nexpires or the inhibiting alert resolves, and eventually transition to the\nresolved state.\n\nAlertmanager sends notifications for firing and resolved alert events via\nwebhook integration. We were using alertmanager2es, which receives webhook\nalert notifications from Alertmanager and inserts them into an Elasticsearch\nindex for searching and analysis. Alertmanager2es has been a reliable tool for\nus over the years, offering ways to monitor alerting volume, noisy alerts and\ndo some kind of alert reporting. However, it had its limitations. The absence\nof silenced and inhibited alert states made troubleshooting issues\nchallenging. We often found ourselves guessing why an alert didn't trigger -\nwas it silenced by another alert or perhaps inhibited by one? Without concrete\ndata, we lacked the means to confirm what was truly happening.\n\nSince the Alertmanager doesn\u2019t provide notifications for silenced or inhibited\nalert events via webhook integration, the alert reporting we were doing was\nsomewhat lacking or incomplete. However, the Alertmanager API provides\nquerying capabilities and by querying the /api/alerts alertmanager endpoint,\nwe can get the silenced and inhibited alert states. Having all four states in\na datastore will enhance our ability to improve alert reporting and\ntroubleshoot Alertmanager issues.\n\nInterfaces for providing information about alert states\n\n## Solution\n\nWe opted to aggregate all states of the alerts (firing, silenced, inhibited,\nand resolved) into a datastore. Given that we're gathering data from two\ndistinct sources (the webhook and API) each in varying formats and potentially\nrepresenting different events, we correlate alerts from both sources using the\nfingerprint field. The fingerprint is a unique hash of the alert\u2019s label set\nwhich enables us to match alerts across responses from the Alertmanager\nwebhook and API.\n\nAlertmanager webhook and API response of same alert event\n\nThe Alertmanager API offers additional fields compared to the webhook\n(highlighted in pastel red on the right), such as silencedBy and inhibitedBy\nIDs, which aid in identifying silenced and inhibited alerts. We store both\nwebhook and API responses in the datastore as separate rows. While querying,\nwe match the alerts using the fingerprint field.\n\nWe decided to use a vector.dev instance to transform the data as necessary,\nand store it in a data store. Vector.dev (acquired by Datadog) is an open-\nsource, high-performance, observability data pipeline that supports a vast\nrange of sources to read data from and supports a lot of sinks for writing\ndata to, as well as a variety of data transformation operations.\n\nHere, we use one http_server vector instance to receive Alertmanager webhook\nnotifications, two http_client sources to query alerts and silence API\nendpoints, and two sinks for writing all of the state logs in ClickHouse into\nalerts and silences tables\n\nAlthough we use ClickHouse to store this data, any other database can be used\nhere. ClickHouse was chosen as a data store because it provides various data\nmanipulation options. It allows aggregating data during insertion using\nMaterialized Views, reduces duplicates with the replacingMergeTree table\nengine, and supports JOIN statements.\n\nIf we were to create individual columns for all the alert labels, the number\nof columns would grow exponentially with the addition of new alerts and unique\nlabels. Instead, we decided to create individual columns for a few common\nlabels like alert priority, instance, dashboard, alert-ref, alertname, etc.,\nwhich helps us analyze the data in general and keep all other labels in a\ncolumn of type Map(String, String). This was done because we wanted to keep\nall the labels in the datastore with minimal resource usage and allow users to\nquery specific labels or filter alerts based on particular labels. For\nexample, we can select all Prometheus alerts using labelsmap[\u2018service\u2019\u2019] =\n\u2018Prometheus\u2019.\n\n## Dashboards\n\nWe built multiple dashboards on top of this data:\n\n  * Alerts overview: To get insights into all the alerts the Alertmanager receives.\n  * Alertname overview: To drill down on a specific alert.\n  * Alerts overview by receiver: This is similar to alerts overview but specific to a team or receiver.\n  * Alerts state timeline: This dashboard shows a snapshot of alert volume at a glance.\n  * Jiralerts overview: To get insights into the alerts the ticket system receives.\n  * Silences overview: To get insights into the Alertmanager silences.\n\n### Alerts overview\n\nThe image is a screenshot of the collapsed alerts overview dashboard by\nreceiver. This dashboard comprises general stats, components, services, and\nalertname breakdown. The dashboard also highlights the number of P1 / P2\nalerts in the last one day / seven days / thirty days, top alerts for the\ncurrent quarter, and quarter-to-quarter comparison.\n\n### Component breakdown\n\nWe route alerts to teams and a team can have multiple services or components.\nThis panel shows firing alerts component counts over time for a receiver. For\nexample, the alerts are sent to the observability team, which owns multiple\ncomponents like logging, metrics, traces, and errors. This panel gives an\nalerting component count over time, and provides a good idea about which\ncomponent is noisy and at what time at a glance.\n\n### Timeline of alerts\n\nWe created this swimlane view using Grafana\u2019s state timeline panel for the\nreceivers. The panel shows how busy the on-call was and at what point. Red\nhere means the alert started firing, orange represents the alert is active and\ngreen means it has resolved. It displays the start time, active duration, and\nresolution of an alert. This highlighted alert is changing state too\nfrequently from firing to resolved - this looks like a flapping alert.\nFlapping occurs when an alert changes state too frequently. This can happen\nwhen alerts are not configured properly and need tweaking, such as adjusting\nthe alert threshold or increasing the for duration period in the alerting\nrule. The for duration field in the alerting rules adds time tolerance before\nan alert starts firing. In other words, the alert won\u2019t fire unless the\ncondition is met for \u2018X\u2019 minutes.\n\n## Findings\n\nThere were a few interesting findings within our analysis. We found a few\nalerts that were firing and did not have a notify label set, which means the\nalerts were firing but were not being sent or routed to any team, creating\nunnecessary load on the Alertmanager. We also found a few components\ngenerating a lot of alerts, and when we dug in, we found that they were for a\ncluster that was decommissioned where the alerts were not removed. These\ndashboards gave us excellent visibility and cleanup opportunities.\n\n### Alertmanager inhibitions\n\nAlertmanager inhibition allows suppressing a set of alerts or notifications\nbased on the presence of another set of alerts. We found that Alertmanager\ninhibitions were not working sometimes. Since there was no way to know about\nthis, we only learned about it when a user reported getting alerted for\ninhibited alerts. Imagine a Venn diagram of firing and inhibited alerts to\nunderstand failed inhibitions. Ideally, there should be no overlap because the\ninhibited alerts shouldn\u2019t be firing. But if there is an overlap, that means\ninhibited alerts are firing, and this overlap is considered a failed\ninhibition alert.\n\nFailed inhibition venn diagram\n\nAfter storing alert notifications in ClickHouse, we were able to come up with\na query to find the fingerprint of the `alertnames` where the inhibitions were\nfailing using the following query:\n\n    \n    \n    SELECT $rollup(timestamp) as t, count() as count FROM ( SELECT fingerprint, timestamp FROM alerts WHERE $timeFilter AND status.state = 'firing' GROUP BY fingerprint, timestamp ) AS firing ANY INNER JOIN ( SELECT fingerprint, timestamp FROM alerts WHERE $timeFilter AND status.state = 'suppressed' AND notEmpty(status.inhibitedBy) GROUP BY fingerprint, timestamp ) AS suppressed USING (fingerprint) GROUP BY t\n\nThe first panel in the image below is the total number of firing alerts, the\nsecond panel is the number of failed inhibitions.\n\nWe can also create breakdown for each failed inhibited alert\n\nBy looking up the fingerprint from the database, we could map the alert\ninhibitions and found that the failed inhibited alerts have an inhibition\nloop. For example, alert Service_XYZ_down is inhibited by alert server_OOR,\nalert server_OOR is inhibited by alert server_down, and server_down is\ninhibited by alert server_OOR.\n\nFailed inhibitions can be avoided if alert inhibitions are configured\ncarefully.\n\n### Silences\n\nAlertmanager provides a mechanism to silence an alert while it is being worked\non or during maintenance. Silence can mute the alerts for a given time and it\ncan be configured based on matchers, which can be an exact match, a regex, an\nalert name, or any other label. The silence matcher doesn\u2019t necessarily\ntranslate to the alertname. By doing alert analysis, we could map the alerts\nand the silence ID by doing a JOIN query on the alerts and silences tables. We\nalso discovered a lot of stale silences, where silence was created for a long\nduration and is not relevant anymore.\n\n## DIY Alert analysis\n\nThe directory contains a basic demo for implementing alerts observability.\nRunning `docker-compose up` spawns several containers, including Prometheus,\nAlertmanager, Vector, ClickHouse, and Grafana. The vector.dev container\nqueries the Alertmanager alerts API and writes the data into ClickHouse after\ntransforming it. The Grafana dashboard showcases a demo of Alerts and Silences\noverview.\n\nMake sure you have docker installed and run docker compose up to get started.\n\nVisit http://localhost:3000/dashboards to explore the prebuilt demo\ndashboards.\n\n## Conclusion\n\nAs part of the observability team, we manage the Alertmanager, which is a\nmulti-tenant system. It's crucial for us to have visibility to detect and\naddress system misuse, ensuring proper alerting. The use of alert analysis\ntools has significantly enhanced the experience for on-call personnel and our\nteam, offering swift access to the alert system. Alerts observability has\nfacilitated the troubleshooting of events such as why an alert did not fire,\nwhy an inhibited alert fired, or which alert silenced / inhibited another\nalert, providing valuable insights for improving alert management.\n\nMoreover, alerts overview dashboards facilitate rapid review and adjustment,\nstreamlining operations. Teams use these dashboards in the weekly alert\nreviews to provide tangible evidence of how an on-call shift went, identify\nwhich alerts fire most frequently, becoming candidates for cleanup or\naggregation thus curbing system misuse and bolstering overall alert\nmanagement. Additionally, we can pinpoint services that may require particular\nattention. Alerts observability has also empowered some teams to make informed\ndecisions about on-call configurations, such as transitioning to longer but\nless frequent shifts or integrating on-call and unplanned work shifts.\n\nIn conclusion, alert observability plays a crucial role in averting burnout by\nminimizing interruptions and enhancing on-call duties' efficiency. Offering\nalerts observability as a service benefits all teams by obviating the need for\nindividual dashboard development and fostering a proactive monitoring culture.\nIf you found this blog post interesting and want to work on observability,\nplease check out our job openings \u2013 we\u2019re hiring for Alerting and Logging!\n\nWe protect entire corporate networks, help customers build Internet-scale\napplications efficiently, accelerate any website or Internet application, ward\noff DDoS attacks, keep hackers at bay, and can help you on your journey to\nZero Trust.\n\nVisit 1.1.1.1 from any device to get started with our free app that makes your\nInternet faster and safer.\n\nTo learn more about our mission to help build a better Internet, start here.\nIf you're looking for a new career direction, check out our open positions.\n\nDiscuss on Hacker News\n\nObservabilityDevelopersDeveloper PlatformPrometheusAlertmanager\n\nFollow on X\n\nMonika Singh|@m0nikasingh\n\nCloudflare|@cloudflare\n\nRelated posts\n\nApril 05, 2024 3:50 PM\n\n## Cloudflare acquires Baselime to expand serverless application observability\ncapabilities\n\nToday, we\u2019re thrilled to announce that Cloudflare has acquired Baselime, a\nserverless observability company...\n\nBy\n\n  * Boris Tane,\n\n  * Rita Kozlov\n\nDeveloper Week, Developers, Developer Platform, Product News, Cloudflare\nWorkers, Observability, Acquisitions\n\nApril 04, 2024 1:05 PM\n\n## New tools for production safety \u2014 Gradual deployments, Source maps, Rate\nLimiting, and new SDKs\n\nToday we are announcing five updates that put more power in your hands \u2013\nGradual Deployments, Source mapped stack traces in Tail Workers, a new Rate\nLimiting API, brand-new API SDKs, and updates to Durable Objects \u2013 each built\nwith mission-critical production services in mind...\n\nBy\n\n  * Tanushree Sharma,\n\n  * Jacob Bednarz\n\nDeveloper Week, Cloudflare Workers, Rate Limiting, SDK, Observability\n\nJanuary 24, 2024 2:00 PM\n\n## Introducing Foundations - our open source Rust service foundation library\n\nFoundations is a foundational Rust library, designed to help scale programs\nfor distributed, production-grade systems...\n\nBy\n\n  * Ivan Nikulin\n\nOpen Source, Rust, Observability, Security, Oxy, Developers, Developer\nPlatform\n\nJanuary 08, 2024 2:00 PM\n\n## An overview of Cloudflare's logging pipeline\n\nIn this post, we\u2019re going to go over what that looks like, how we achieve high\navailability, and how we meet our Service Level Objectives (SLOs) while\nshipping close to a million log lines per second...\n\nBy\n\n  * Colin Douch\n\nObservability, Logs\n\n  * Getting Started\n  * Free plans\n  * For enterprises\n  * Compare plans\n  * Get a recommendation\n  * Request a demo\n  * Contact Sales\n\n  * Resources\n  * Learning Center\n  * Analyst reports\n  * Cloudflare Radar\n  * Cloudflare TV\n  * Case Studies\n  * Webinars\n  * White Papers\n  * Developer docs\n  * theNet\n\n  * Solutions\n  * Connectivity cloud\n  * SSE and SASE services\n  * Application services\n  * Network services\n  * Developer services\n\n  * Community\n  * Community Hub\n  * Project Galileo\n  * Athenian Project\n  * Cloudflare for Campaigns\n  * Critical Infrastructure Defense Project\n  * Connect 2024\n\n  * Support\n  * Help center\n  * Cloudflare Status\n  * Compliance\n  * GDPR\n  * Trust & Safety\n\n  * Company\n  * About Cloudflare\n  * Our team\n  * Investor relations\n  * Press\n  * Careers\n  * Diversity, equity & inclusion\n  * Impact/ESG\n  * Network Map\n  * Logos & press kit\n  * Become a partner\n\n\u00a9 2024 Cloudflare, Inc. | Privacy Policy | Terms of Use | Report Security Issues |Cookie Preferences | Trademark\n\n", "frontpage": false}
