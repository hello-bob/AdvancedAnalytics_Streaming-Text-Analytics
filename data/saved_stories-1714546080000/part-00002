{"aid": "40216086", "title": "Pareto-Optimal Compression (2021)", "url": "https://insanity.industries/post/pareto-optimal-compression/", "domain": "insanity.industries", "votes": 1, "user": "refset", "posted_at": "2024-04-30 20:42:33", "comments": 0, "source_title": "Pareto-optimal compression | Insanity Industries", "source_text": "Pareto-optimal compression | Insanity Industries\n\nSubscribe Mastodon Twitter\n\n# Pareto-optimal compression\n\n## 02\\. Mar 2021\n\n  * #funwithdata\n  * #tooling\n\nData compression is an incredibly useful tool in many situations, be it for\nbackups, archiving, or even filesystems. But of the many compressors that\nthere are, which of them is the one to be preferred? To properly answer this\nquestion, we first have to answer a different, but related one: What exactly\nmakes a compression algorithm good?\n\nTime for a closer investigation to find the best of the best of the best!\n\n# Pareto optimality or what makes a compression algorithm good\n\n> Pareto optimality is a situation where no [...] preference criterion can be\n> better off without making at least one [...] preference criterion worse off\n> [...].\n>\n> ~ Wikipedia\n\nThe concept of Pareto Optimality is tremendously useful, far beyond the scope\nof this blogpost, by acknowledging competing interests.\n\nIn our particular case of optimal compression, one of the obvious candidates\nis compression size. The smaller the resulting file is, the better the\ncompressor, right?\n\nBut what if we are just compressing so we can speed up upload to a remote\nlocation? At this point, it doesn\u2019t matter if the compression is supremely\nsuperior to everything else if we have to wait twice as long for it to happen\ncompared to simply upload the uncompressed file.\n\nSo for our algorithms, we have at least two criteria that come into play:\nactual achievable compression and compression cost, which we will measure as\n\u201chow long does it take to compress our content?\u201d^1. These two are the criteria\nthis blogpost is focused on.\n\n## A practical example\n\nPractically speaking, let\u2019s assume two compression tools A and B, having both\ntwo compression levels 1 and 2, which we use on a sample file. The results\nmight look like the following:\n\nalgorithm| level| file size| time  \n---|---|---|---  \nA| 1| 60%| 2s  \nA| 2| 50%| 4s  \nB| 1| 40%| 3s  \nB| 2| 30%| 11s  \n  \nWe find that while both algorithms increase their compression size with level,\nB.1 is unconditionally better than A.2, so there is no reason to ever use A.2\nif B.1 is available. Besides that, A.1, B.1 and B.2 continuously increase in\ncompression effectiveness as well as time taken for that. For easier\ndigestion, we can visualize these results:\n\nCompression results for different hypothetical compression algorithms,\nincluding the Pareto frontier indicated in blue.\n\nHere we clearly see what could already be taken from the table above, but\nsignificantly more intuitive. Remembering our definition of Pareto optimality\nfrom before, we see that A.2 is not Pareto optimal, as B.1 is both better in\ntime taken as well as in compression effect. This shows more intuitively that\nin this scenario there is no reason to use A.2 if B.1 is available. For A.1,\nB.1 and B.2 the choice is not so clear-cut, as the resulting file size can\nonly be reduced further by investing more time into the compression. Hence,\nall three of them are Pareto optimal and constitute the Pareto frontier or the\nPareto set.\n\nOne wants to always strive for choosing a Pareto optimal solution whenever\npossible, as non-Pareto-optimal solutions are always to some degree wasteful.\n\nWith this insight into Pareto optimality, we can now put our knowledge into\npractice and begin our journey to the Pareto frontiers of compression\nalgorithms.\n\n# Setup and test procedure for real-world measurements\n\nData was gathered on a Linux system with a given sample file for the resulting\nfilesize compared to the original as well as the time the process took for\ncompressing. First, the sample file was read from disk entirely, ensuring it\nwould be present in Linux\u2019 filesystem cache, then compressed with each\ncompression program at each level 15 times for a decent statistic, the\ncompressed result is routed directly into /dev/null. All presented compression\ntimes presented are the median of these 15 runs, compression size was only\nmeasured once after verifying that it was deterministic^2.\n\nUnless explicitly denoted otherwise, all compressors were run in their default\nconfiguration with a single compression thread. Furthermore, all applications\non the machine used for benchmarking except the terminal holding the\nbenchmarking process were closed to reduce interference by other processes as\nmuch as possible.\n\nThe tests for decompression were done analogously (with the obvious exception\nthat the sample file is compressed according to the testcase in play\nbeforehand), single-threaded decompression as well as routing the decompressed\nresult to /dev/null.\n\nAll tests were run on several different CPUs: Intel i7-8550U, Intel i5-3320M^3\n(both mobile CPUs), Intel i7-7700K and AMD Ryzen 7 3800X (both workstation\nCPUs). While the absolute compression times changed, the Pareto frontiers did\nnot, hence in this blogpost only the plots for the i7-8550U are shown\nexemplarily.\n\n# Case study: initramfs-compression\n\nJust recently, the current maintainer of mkinitcpio, Giancarlo Razzolini,\nannounced the transition from gzip to zstd as the default for initramfs-\ncompression. The initramfs is a little mini-system, whose only objective is to\nprepare the hardware and assemble all disks so that the main system is readily\nprepared to take over operation. The initramfs needs to be loaded (and hence\ndecompressed) at every boot as well as recreated occasionally, when components\ncontained in it are updated.\n\nmkinitcpio supports a variety of compression algorithms: none, gzip, bzip2,\nlzma, lzop, lz4 and most recently zstd.\n\nQuantifying these algorithms and plotting the Pareto frontier for compression\nyields:\n\nCompression results for a standard mkinitcpio-generated initramfs and the\ncorresponding Pareto frontier. The difficult-to-decipher black culmination at\nabout 34% resulting file size are the bzip2 results.\n\nWe find that the change of defaults from gzip to zstd was well justified, as\ngzip can no longer be considered Pareto optimal for this type of file.\nChoosing lzma as the default would make it even smaller, but this would be\npaid by a noticable higher resource usage for compression (which has to be\ninvested on every update affecting the initramfs), so from the data zstd is\ncertainly the wiser choice^4.\n\nThis can also be seen when we take a look of Pareto optimality of\ndecompression (after all, this decompression needs to happen on every single\nsystem boot):\n\nDecompression results for a standard mkinitcpio-generated initramfs and the\ncorresponding Pareto frontier.\n\nWe clearly see that zstd is blasting all other algorithms out of the water\nwhen it comes to decompression speed, making it even more of a good choice for\nthis use case. Given these numbers, it is even more of a good choice for an\ninitramfs, not only does it compress fast, it also decompresses impressively\nfast, six times faster than lzma which was previously known for its quick\ndecompression speed despite high compression factors.\n\nGiven the data for zstd, it cannot be ruled out completely that zstd simply\nhit a non-CPU-bound on decompression, but even if it did, the conclusion for\nthe choice of algorithm does not change.\n\n### dracut\n\nIf you happen to be on a Linux distribution that uses dracut for initramfs-\ngeneration, the conclusions that can be drawn for dracut-initramfs-\ncompressions are the almost identical given the compression and decompression\ndata for a dracut-initrd, the Pareto frontier remains mostly unchanged with\njust some more levels of zstd in it.\n\n## Real world usage recommendation\n\nTo use zstd in mkinitcpio, simply use version 30 and above. You can modify the\ncompression level (default is -3) by adding a specific level to\nCOMPRESSION_OPTIONS, but given the data, this doesn\u2019t seem to provide much of\na benefit.\n\nFor dracut, add compress=\"zstd\" to /etc/dracut.conf to get zstd compression at\nthe default level.\n\n# Case study: borgbackup\n\nIn the next scenario we will investigate the impact of compression when\nbacking up data with borgbackup. Borg comes with an integrated option to\ncompress the backupped data, with algorithms available being lz4, zstd,\nzlib/gzip and lzma. In addition to this, borg has an automated detection\nroutine to see if the files backupped do actually compress enough to spend CPU\ncycles on compressing (see borg help compression for details).\n\nFor this scenario, we do not only have one definite sample, but will consider\nthree different samples: A simple text file, being a dump of the Kernel log\nvia the dmesg command, representing textual data. A binary, in this particular\ncase the dockerd ELF binary, (rather arbitrarily) representing binary data.\nFinally, an mkinitcpio-image, intended as somewhat of a mixed-data sample. We\ndo not consider media files, as these are typically already stored in\ncompressed formats, hence unlikely to compress further and thus dealt with by\nborg\u2019s compressibility heuristics.\n\nThe resulting Pareto frontiers for compression are, starting with least\neffective compression:\n\ntext| binary| initramfs  \n---|---|---  \nlz4.2| lz4.1| lz4.1  \nzstd.1| zstd.1| zstd.1  \nzstd.2| zstd.2| zstd.2  \nzstd.4| zstd.3| zstd.3  \nzstd.3| zstd.4| zstd.4  \nzstd.5| zstd.5| zstd.5  \nzstd.6| zstd.6| zstd.6  \nzstd.7| zstd.7| zstd.7  \nzstd.8| zstd.8| zstd.8  \nzstd.9| zstd.9| zstd.9  \nzstd.10| lzma.0| lzma.0  \nzstd.11| lzma.1| lzma.1  \nzstd.12| lzma.2| lzma.2  \nlzma.2| lzma.3| lzma.3  \nlzma.3| lzma.4| lzma.4  \nlzma.6| lzma.5| lzma.5  \nzstd.19| lzma.6| lzma.6  \n.| lzma.7| lzma.7  \n.| lzma.8| lzma.8  \n.| lzma.9| lzma.9  \n  \nWe see that effectively, except for some brief occurrance of lz4 at the top,\nthe relevant choices are lzma and zstd. More details can be seen in the plots\nlinked in the column headers. Hence, as backups should be run often (with a\ntool as borg there is little reason for anything else but daily), zstd with a\nlevel slightly below 10 seems to be a good compromise of speed and resulting\ndata size.\n\n## Real world usage recommendation\n\nAdding --compression=auto,zstd,7 to the borg command used to create a backup\nwill use zstd on level 7 if borgs internal heuristics considers the file in\nquestion to compress well, otherwise no compression will be used.\n\nThis flag can be added on-the fly, without affecting existing repositories or\nborgs deduplication. Already backupped data is not recompressed, meaning that\nadding this flag for use with an existing repository does not require a\nreupload of everything. Consequentially, it also means that to recompress the\nentire repository with zstd one effectively has to start from scratch.\n\n# Case study: Archiving with tar\n\nCompression can not only be used for backup-tools like borg, it can also be\nused to archive files with tar. Some compressors have explicit flags in tar,\nsuch as gzip (-z), lzma (-J) or bzip2 (-j), but any compression algorithm can\nbe used via tar\u2019s -I flag.\n\nWorking with tar poses two challenges with regard to the compressor:\n\n  * streaming: tar concatenates data and streams the result through the compressor. Hence, to extract files at a certain position of a tarball, the entirety of the data before that file needs to be decompressed as well.\n  * compress-only: as a further consequence of this, tar lacks a feature testing if something is well compressible, so uncompressible data will also be sent through the compressor\n\nIf we want to investigate a good compressor without consideration for the\ninput data, we aim for picking a Pareto optimal compressor that takes two\nproperties into consideration:\n\n  * fast decompression, to be able to easily and quickly extract data from the archive again if need be\n  * good performance on non-compressible data (this particularly means that incompressible data should especially not increase in size).\n\n## Compression capabilities\n\nTo investigate the decompression capabilities of certain compressors, we can\nreuse the dataset used on the borg case study and add some incompressible data\nin form of a flac music file to the mix. As tar has a larger variety of usable\nalgorithms, we include lz4, gzip, lzma, zstd, lzop and brotli as well.\n\nWe can exemplarily see the effectiveness of these on the dockerd elf binary\n(other datasets can be found below) first:\n\nCompression results and the corresponding Pareto frontier for the dockerd elf\nbinary. The unreadable black cluster at 25% compressed size is again bzip2,\nthe one at 34% is predominantly lz4.\n\nDecompression results and the corresponding Pareto frontier for the dockerd\nelf binary. The clusters are lzma for 20% resulting size, zstd at 25%, brotli\nat 24% and lz4 at 33%.\n\nIn summary, the Pareto frontiers for the different types of data overall turn\nout to be (with c for compression and d for decompression):\n\ntext (c)| text (d)| binary (c)| binary (d)| initramfs (c)| initramfs (d)| flac\n(c)| flac (d)  \n---|---|---|---|---|---|---|---  \nlz4.2| lz4.9| lz4.1| lz4.8| lz4.1| lz4.7| lz4.2| zstd.1  \nzstd.1| zstd.13| lzop.1| lz4.9| lzop.6| lz4.9| brotli.1| zstd.5  \nzstd.2| zstd.11| lzop.3| zstd.15| zstd.1| zstd.1| brotli.2| zstd.7  \nzstd.4| zstd.12| zstd.1| zstd.16| zstd.2| zstd.9| brotli.3| zstd.19  \nzstd.3| zstd.14| zstd.2| zstd.17| zstd.3| zstd.14| zstd.18| .  \nzstd.5| zstd.15| zstd.3| zstd.19| zstd.4| zstd.15| zstd.19| .  \nzstd.6| zstd.19| zstd.4| lzma.8| zstd.5| zstd.16| .| .  \nzstd.7| .| zstd.5| lzma.9| zstd.6| zstd.17| .| .  \nzstd.8| .| zstd.6| .| zstd.7| zstd.18| .| .  \nzstd.9| .| zstd.7| .| zstd.8| zstd.19| .| .  \nzstd.10| .| zstd.8| .| zstd.9| lzma.8| .| .  \nbrotli.5| .| zstd.9| .| brotli.5| lzma.9| .| .  \nbrotli.6| .| brotli.5| .| lzma.0| .| .| .  \nbrotli.7| .| lzma.1| .| lzma.1| .| .| .  \nlzma.3| .| lzma.2| .| lzma.2| .| .| .  \nlzma.6| .| lzma.3| .| lzma.3| .| .| .  \nzstd.19| .| lzma.4| .| lzma.4| .| .| .  \n.| .| lzma.5| .| lzma.5| .| .| .  \n.| .| lzma.6| .| lzma.6| .| .| .  \n.| .| lzma.7| .| lzma.7| .| .| .  \n.| .| lzma.8| .| lzma.8| .| .| .  \n.| .| lzma.9| .| lzma.9| .| .| .  \n  \nAs can be seen in the linked plots, we again find that while lzma still\nachieves the highest absolute compression, zstd dominates the sweet spot right\nbefore computational cost skyrockets. We also find brotli to be an interesting\ncontender here, making it into the Pareto frontier as well. However, with only\nsometimes making it into the Pareto frontier, whereas lzma and zstd robustly\ndefend their inclusion in it, it seems more advisable to resort to either lzma\nor zstd as this only provides a sample binary and actual data might vary.\nFurthermore, when it comes to decompression brotli is not Pareto optimal\nanymore at all, also indicating lzma and zstd as being the better choice.\n\n## Impact on incompressible files in detail\n\nWe will take another closer look at the incompressible case, represented by a\nflac file and strip away bzip2 and lzma, as we could tell from the linked\nplots that these two clearly increase the size of the result and are hence not\nPareto optimal (as they are already beaten by the Pareto optimal case \u201cno\ncompression\u201d).\n\nThe results have a clear indication:\n\nCompression results on incompressible data.\n\nDecompression results on incompressible data. The unreadable black cluster at\n99.975% size is zstd, the one at 99.988% contains lzop and brotli.\n\nThe recommended choice of algorithm for compression is either brotli or zstd,\nbut when it comes to decompression, zstd takes the lead again. This is of\ncourse a cornercase, the details of this might change with the particular\nchoice of incompressible data. However, I do not expect the overall impression\nto significantly change.\n\n## Real world usage recommendation\n\nConcluding this section, the real-world recommendation resulting from this\nseems to simply use zstd for any tarball compression if available. To do so,\ntar \"-Izstd -10 -T0\" can be a good choice, with -T0 telling zstd to\nparallelize the compression onto all available CPU cores, speeding things up\neven more beyond our measurements. Depending on your particular usecase it\nmight be interesting to use an alias like\n\n    \n    \n    alias archive='tar \"-Izstd -19 -T0\" -cf'\n\nwhich allows quickly taring data into a compressed archive via archive\nmyarchive.tar.zst file1 file2 file3 ....\n\n# Case study: filesystems\n\nAnother usecase for compression is filesystem compression. Conceptually\nsimilar to what borg does, files are transparently compressing and\ndecompressed when written to or read from disk.\n\nAmong the filesystems capable of such inline-compression are ZFS and btrfs.\nbtrfs supports ZLIB (gzip), LZO (lzop) and zstd, whereas ZFS supports lz4,\nLZJB (which was not included in these benchmarks as no appropriate binary\ncompressor was found), gzip and ZLE (zero-length-encoding, only compressing\nzeroes, hence also not tested). zstd support for OpenZFS has been merged, but\napparently hasn\u2019t made it into any stable version yet at time of writing\naccording to the OpenZFS documentation.\n\nThis case is situated similar to the tar-case study, and as all compressors\navailable for ZFS and btrfs have already been covered in the section above,\nthere is no reason to reiterate these results here.\n\nIt shall, however, be noted, that at least for btrfs, the standard flag for\nfilesystem compression adopts a similar heuristic as borg and hence the case\nof incompressible data might not be so relevant for a btrfs installation. That\nbeing said, the conclusion here is a recommendation of zstd, and as we have\nseen in the last section, the question of incompressible files doesn\u2019t change\nthe overall recommendation.\n\n## Real world usage recommendation\n\nIf you want to save diskspace by using compression, the mount option compress\nin combination with zstd is generally a good choice for btrfs. This also\nincludes the compressibility-heuristics (compress-force would be the option\nthat compresses without this heuristics). For ZFS, the general recommendation\nis consequentially also zstd once it makes its way into a release.\n\n# Conclusion\n\nConcluding the experiments laid out in this blogpost, we can effectively state\nan almost unconditional and surprisingly clear recommendation to simply use\nzstd for everything. The exact level might depend on the usecase, but overall\nit has demonstrated to be the most versatile, yet effective compressor around\nwhen it comes to effectiveness and speed, both for compression and especially\ndecompression. Furthermore, it has, in contrast to most other contenders,\nflags for built-in parallelization, which not used in this blogpost at all,\nand yet zstd still stomped almost the entire competition.\n\nOnly if resulting filesize should be pushed down as much as possible, without\nany regard for computational cost, lzma retains an edge for most kinds of\ndata. In practice, however, the conclusion is to simply use zstd.\n\n  1. Which is a easy though little bit cheated way of asking \u201chow much CPU-time do I have to burn on this?\u201d. Obviously, there are plenty of other criteria that might be relevant, depending on the particular usecase, such as memory consumption. Furthermore, all these criteria also apply for decompression as well as compression, as we will investigate later, technically doubling the amount of criteria we can take into consideration. \u21a9\ufe0e\n\n  2. For algorithms that allow parallelized compressions, this might no longer necessarily be the case, but all data in this blogpost was gathered with non-parallelized compression for all tested algorithms. \u21a9\ufe0e\n\n  3. Fun fact on the site: the entire benchmarking suite (including some more data that is not included in this blogpost) runs 61 days straight on the i5-3320M. Fortunately it\u2019s a bit faster on newer CPUs. :D \u21a9\ufe0e\n\n  4. Furthermore, mkinitcpio runs zstd with -T0 by default, which parallelizes compression to all available cores. This accellerates compression even further, but was not tested in this particular scenario and hence not included in the plot, as most compressors do not support parallelization. But even without parallelization, zstd still makes it to the pareto frontier. There might be another blogpost upcoming to take a look at parallelization at some point, though... \u21a9\ufe0e\n\n  * About\n  * Contact\n  * Github\n  * Mastodon\n\n  * RSS-Feed\n  * Atom-Feed\n  * Mastodon-Feed\n  * Twitter-Feed\n\n  * Privacy Policy\n  * Imprint\n\n", "frontpage": false}
