{"aid": "40235080", "title": "How I dubbed a 16-second video with lip sync for $0.50 using open-source models", "url": "https://www.union.ai/blog-post/open-source-video-dubbing-using-whisper-m2m-coqui-xtts-and-sad-talker", "domain": "union.ai", "votes": 1, "user": "samhita-alla", "posted_at": "2024-05-02 11:54:28", "comments": 0, "source_title": "Open-Source Video Dubbing Using Whisper, M2M, Coqui XTTS, and Sad Talker \u2022 Union.ai", "source_text": "Open-Source Video Dubbing Using Whisper, M2M, Coqui XTTS, and Sad Talker \u2022\nUnion.ai\n\nLearn how Porch used Union to migrate off Airflow\n\nRead the case study\n\nGet a demo\n\nSamhita Alla\n\nhttps://www.union.ai/blog-post/open-source-video-dubbing-using-\nwhisper-m2m-coqui-xtts-and-sad-talker\n\n5\n\nMin Read\n\n\u2022\n\nMay 1, 2024\n\nArticle\n\n# Open-Source Video Dubbing Using Whisper, M2M, Coqui XTTS, and Sad Talker\n\nAI video dubbing or translation has surged in popularity, breaking down\nlanguage barriers and enabling communication across diverse cultures. While\nmany paid services offer video dubbing, they often rely on proprietary black-\nbox models outside your control. What if you could deploy a fully\ncustomizable, open-source video dubbing pipeline tailored to your needs?\n\nWith the right open-source models stitched together, you can translate videos\non your terms. This blog walks through how users of Union can tweak\nparameters, scale resources based on input, reproduce results if needed, and\nleverage caching to optimize costs. This flexible, transparent approach gives\nyou full command over quality, performance, and spend.\n\nI dubbed a 16-second video for an estimated $0.50 using an AWS T4 instance.\nEnabling caching can further reduce costs. For example, if you need to update\nlip sync parameters, only the lip sync task needs to be re-executed, while\nother task outputs, such as voice cloning and text translation, are read from\nthe cache. If you want to dub to a different language, the speech-to-text\ntranscription won\u2019t be re-executed as the task output for the previously\ndubbed language will remain the same.\n\nBuilding your own video dubbing pipeline is easier than you think. I\u2019ll show\nyou how to assemble best-in-class open-source models into a single pipeline,\nall ready to run on the Union platform.\n\nBefore diving into the details, let's take a sneak peek at the workflow.\n\nCopied to clipboard!\n\n    \n    \n    from flytekit import workflow @workflow def video_translation_wf(...) -> FlyteFile: values = fetch_audio_and_image(...) text = speech2text(...) translated_text = translate_text(...) cloned_voice = clone_voice(...) return lip_sync(...)\n\nWhen you run this workflow on Union, it triggers a sequence of steps to\ntranslate your video:\n\n  1. The pipeline starts by fetching the audio and image components from your input video file for further processing.\n  2. Using Whisper, the audio is transcribed into text, enabling translation.\n  3. The transcribed text is fed into the M2M100 model, translating it into your desired target language.\n  4. Coqui XTTS clones the original speaker's voice in the target language.\n  5. Finally, Sad Talker lip-syncs the translated audio to the original video, producing a complete translated clip with accurate lip movements.\n\n## Audio & image extraction\n\nTo enable transcription and translation, we need to extract audio from the\nvideo file, and for the lip sync model, we require a frame from the video.\nWhile the model typically selects at random, choosing the most representative\nkeyframe using the Katna library could yield better results.\n\nCopied to clipboard!\n\n    \n    \n    from flytekit import ImageSpec, task preprocessing_image = ImageSpec( name=\"fetch_audio_and_image\", builder=\"ucimage\", apt_packages=[\"ffmpeg\"], packages=[ \"moviepy==1.0.3\", \"katna==0.9.2\", \"unionai==0.1.5\", ], ) @task( container_image=preprocessing_image, requests=Resources(mem=\"5Gi\", cpu=\"1\"), ) def fetch_audio_and_image( video_file: FlyteFile, output_ext: str ) -> audio_and_image_values: from Katna.video import Video from Katna.writer import KeyFrameDiskWriter from moviepy.editor import VideoFileClip ...\n\nThe ImageSpec utility captures all the dependencies, while the ucimage builder\nautomatically builds the image remotely when you launch a remote execution.\n\nThe resources parameter in the task decorator allows you to specify the\nnecessary resources to run the task. You can also adjust the resources based\non the task's consumption, as observed in the Union UI.\n\nResource utilization for lip sync task as observed in the Union UI\n\n## Speech-to-text transcription\n\nThe audio must then be transcribed to enable translation in the subsequent\ntask. The automatic speech recognition (ASR) task enables transcribing speech\naudio recordings into text.\n\nCopied to clipboard!\n\n    \n    \n    @task( container_image=speech2text_image, requests=Resources(gpu=\"1\", mem=\"10Gi\", cpu=\"1\"), accelerator=T4, ) def speech2text( checkpoint: str, audio: FlyteFile, chunk_length: float, return_timestamps: bool, translate_from: str, ) -> str: ... pipe = pipeline( \"automatic-speech-recognition\", model=checkpoint, chunk_length_s=chunk_length, device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\", ) ...\n\nThe checkpoint can refer to the Whisper model (for example, Whisper Large v2),\nbut in reality, it can be any speech-to-text model. You can configure it to\nrun on a GPU to speed up execution, and you can use an accelerator to select\nthe GPU on which you want the transcription to run.\n\n## Text translation\n\nThe M2M100 1.2B model by Meta enables text translation. When executing the\nworkflow, both the source and target languages need to be provided as inputs.\n\nCopied to clipboard!\n\n    \n    \n    @task( container_image=language_translation_image, requests=Resources(mem=\"10Gi\", cpu=\"3\"), ) def translate_text(translate_from: str, translate_to: str, input: str) -> str: ... model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_1.2B\") tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_1.2B\") ...\n\nThis task doesn\u2019t require a GPU.\n\n## Voice cloning\n\nThe translated text is used to clone the speaker\u2019s voice. Coqui XTTS clones\nthe voice based on the provided text, target language, and the speaker\u2019s\naudio.\n\nCopied to clipboard!\n\n    \n    \n    @task( container_image=clone_voice_image, requests=Resources(gpu=\"1\", mem=\"15Gi\"), accelerator=T4, environment={\"COQUI_TOS_AGREED\": \"1\"}, ) def clone_voice(text: str, target_lang: str, speaker_wav: FlyteFile) -> FlyteFile: ... tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device) ...\n\nThe XTTS model supports the following languages for voice cloning, making them\npotential target languages for video dubbing as well:\n\nCopied to clipboard!\n\n    \n    \n    language_codes = { \"English\": \"en\", \"Spanish\": \"es\", \"French\": \"fr\", \"German\": \"de\", \"Italian\": \"it\", \"Portuguese\": \"pt\", \"Polish\": \"pl\", \"Turkish\": \"tr\", \"Russian\": \"ru\", \"Dutch\": \"nl\", \"Czech\": \"cs\", \"Arabic\": \"ar\", \"Chinese\": \"zh-cn\", \"Japanese\": \"ja\", \"Hungarian\": \"hu\", \"Korean\": \"ko\", \"Hindi\": \"hi\", }\n\nIf another voice cloning model supports a larger set of languages, you can\nalso use that.\n\n## Lip syncing\n\nSad Talker model generates head videos based on an image and audio input. The\nmodel allows for adjusting various parameters such as pose style, face\nenhancement, background enhancement, expression scale, and more. The following\ncode snippet outlines the inputs that the lip sync task accepts:\n\nCopied to clipboard!\n\n    \n    \n    @task( requests=(Resources(gpu=\"1\", mem=\"30Gi\")), container_image=lip_sync_image, accelerator=T4, ) def lip_sync( audio_path: FlyteFile, pic_path: FlyteFile, ref_pose: FlyteFile, ref_eyeblink: FlyteFile, pose_style: int, batch_size: int, expression_scale: float, input_yaw_list: Optional[list[int]], input_pitch_list: Optional[list[int]], input_roll_list: Optional[list[int]], enhancer: str, background_enhancer: str, device: str, still: bool, preprocess: str, checkpoint_dir: str, size: int, ) -> FlyteFile: ...\n\nYou can find the end-to-end video dubbing workflow on GitHub.\n\n## Running the pipeline\n\nNote: Starting with Union is simple. Explore the unionai SDK to run workflows\non the platform!\n\nYou can register the workflow on Union as follows:\n\nCopied to clipboard!\n\n    \n    \n    unionai --verbose register --project default src.\n\nOnce registered, you can trigger the workflow in the Union UI to translate\nyour videos!\n\nOutputs of our video dubbing application. The still parameter in the workflow\nis set to True because the Sad Talker model supports head motions. You can set\nit to False if preprocess is set to crop.\n\n## Key takeaways\n\nWhat exactly does this video dubbing pipeline unlock?\n\n  * Each task has an associated ImageSpec, eliminating the need for a single bloated image containing all dependencies. You can also use different Python versions or install CUDA libraries to run on GPUs, providing a new level of dependency isolation. This reduces the chances of dealing with \u201cdependency hell\u201d!\n  * Within a single workflow, tasks can run on both CPUs and GPUs, and you can adjust resources based on the requirements of each task.\n  * You can easily swap out existing libraries with other open-source alternatives. The transparent pipeline lets you fine-tune parameters for optimal performance.\n  * The versioning and caching features of Union enable you to roll back to a previous execution with ease and avoid re-running executions that have already been completed, respectively.\n  * Reproducibility is the low-hanging fruit of Union that accelerates the iteration velocity while developing workflows.\n  * If you have Flyte up and running, you can also \u201cself-host\u201d this pipeline without relying on third-party video dubbing libraries.\n\nContact the Union team if you\u2019re interested in implementing end-to-end AI\nsolutions!\n\nStar us\n\n## More from Union\n\nThomas Fan\n\nPerformance Tuning AI Models with NVIDIA DGX Cloud\n\nJohn Votta\n\nMove Fast and Don\u2019t Break Things: Introducing Artifacts Lineage and Reactive\nWorkflows\n\nSamhita Alla\n\nDeploy Segment Anything Model (SAM) for Inference on Amazon SageMaker\n\n< Back to Union Square\n\n## Table of Contents\n\nAudio & image extraction\n\nSpeech-to-text transcription\n\nText translation\n\nVoice cloning\n\nLip syncing\n\nRunning the pipeline\n\nKey takeaways\n\nhttps://www.union.ai/blog-post/open-source-video-dubbing-using-\nwhisper-m2m-coqui-xtts-and-sad-talker\n\nArticle\n\nGet a demo\n\n### Sign up for the latest updates from Union\n\n#### Products\n\nUnionPanderaUnionMLFlyteTM\u2197\n\n#### Modern AI\n\nModern AI OrchestrationFlyte vs. Airflow\u2197Flyte vs. Kubeflow\u2197\n\n#### Use Cases\n\nMachine LearningDataAnalyticsBioinformatics\n\n#### Learn\n\nBlogEventsVideos\u2197Docs\u2197Github\u2197Community\u2197\n\n#### About\n\nCompanyCareersContactSupportCase Studies\n\n\u00a9 2024 Union.ai. All rights reserved.\n\nPrivacy Policy\n\nBy clicking \u201cAccept All Cookies\u201d, you agree to the storing of cookies on your\ndevice to enhance site navigation, analyze site usage, and assist in our\nmarketing efforts. View our Privacy Policy for more information.\n\nPreferences\n\nDenyAccept\n\n", "frontpage": false}
