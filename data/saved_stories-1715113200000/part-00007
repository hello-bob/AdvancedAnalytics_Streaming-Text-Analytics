{"aid": "40284113", "title": "We Built an Open-Source Text-to-Image Evaluation Library for Clip Models", "url": "https://github.com/encord-team/text-to-image-eval", "domain": "github.com/encord-team", "votes": 1, "user": "Stephen_Oladele", "posted_at": "2024-05-07 10:48:35", "comments": 0, "source_title": "GitHub - encord-team/text-to-image-eval: Evaluate custom and HuggingFace text-to-image/zero-shot-image-classification models like CLIP, SigLIP, DFN5B, and EVA-CLIP. Metrics include Zero-shot accuracy, Linear Probe, Image retrieval, and KNN accuracy.", "source_text": "GitHub - encord-team/text-to-image-eval: Evaluate custom and HuggingFace text-\nto-image/zero-shot-image-classification models like CLIP, SigLIP, DFN5B, and\nEVA-CLIP. Metrics include Zero-shot accuracy, Linear Probe, Image retrieval,\nand KNN accuracy.\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nencord-team / text-to-image-eval Public\n\n  * Notifications\n  * Fork 0\n  * Star 13\n\nEvaluate custom and HuggingFace text-to-image/zero-shot-image-classification\nmodels like CLIP, SigLIP, DFN5B, and EVA-CLIP. Metrics include Zero-shot\naccuracy, Linear Probe, Image retrieval, and KNN accuracy.\n\nencord.com\n\n### License\n\nApache-2.0 license\n\n13 stars 0 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# encord-team/text-to-image-eval\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n3 Branches\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\neloy-encordmisc: add license tag and improved texts to readme (#70)May 7,\n2024df4a8ef \u00b7 May 7, 2024May 7, 2024\n\n## History\n\n76 Commits  \n  \n### .github\n\n|\n\n### .github\n\n| chore: add CTA (#62)| Apr 22, 2024  \n  \n### images\n\n|\n\n### images\n\n| refactor: rename tool and update README (#63)| Apr 22, 2024  \n  \n### sources\n\n|\n\n### sources\n\n| refactor: rename tool and update README (#63)| Apr 22, 2024  \n  \n### tests\n\n|\n\n### tests\n\n| refactor: rename tool and update README (#63)| Apr 22, 2024  \n  \n### tti_eval\n\n|\n\n### tti_eval\n\n| fix: error when Encord is the first downloaded dataset source (#71)| May 3,\n2024  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| feat: allow multiple dataset sources (#22)| Feb 21, 2024  \n  \n### .pre-commit-config.yaml\n\n|\n\n### .pre-commit-config.yaml\n\n| chore: enforce linting and type checks (#15)| Feb 15, 2024  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| add apache 2 license (#65)| Apr 24, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| misc: add license tag and improved texts to readme (#70)| May 7, 2024  \n  \n### dev-setup.sh\n\n|\n\n### dev-setup.sh\n\n| chore: enforce linting and type checks (#15)| Feb 15, 2024  \n  \n### poetry.lock\n\n|\n\n### poetry.lock\n\n| misc: remove unused packages (#55)| Apr 22, 2024  \n  \n### pyproject.toml\n\n|\n\n### pyproject.toml\n\n| refactor: rename tool and update README (#63)| Apr 22, 2024  \n  \n## Repository files navigation\n\nWelcome to tti-eval, a repository for benchmarking text-to-image models on\nyour own data!\n\n> Evaluate your (or HF) text-to-image embedding models like CLIP from OpenAI\n> against your (or HF) datasets to estimate how well the model will perform on\n> your classification dataset.\n\n## TLDR\n\nWith this library, you can take an embedding model intended for jointly\nembedding images and text (like CLIP) and compute metrics for how well such\nmodel performs on classifying your custom dataset. What you will do is\n\n  0. Install tti-eval\n  1. Compute embeddings of a dataset with a model\n  2. Do an evaluation of the model against the dataset\n\nYou can easily benchmark different models and datasets against each other.\nHere is an example:\n\n## Installation\n\n> tti-eval requires Python 3.11 and Poetry.\n\n  1. Clone the repository:\n    \n        git clone https://github.com/encord-team/text-to-image-eval.git\n\n  2. Navigate to the project directory:\n    \n        cd text-to-image-eval\n\n  3. Install the required dependencies:\n    \n        poetry shell poetry install\n\n  4. Add environment variables:\n    \n        export TTI_EVAL_CACHE_PATH=$PWD/.cache export TTI_EVAL_OUTPUT_PATH=$PWD/output export ENCORD_SSH_KEY_PATH=<path_to_the_encord_ssh_key_file>\n\n## CLI Usage\n\n### Embeddings Generation\n\nTo build embeddings, run the CLI command tti-eval build. This command allows\nto interactively select the model and dataset combinations on which to build\nthe embeddings.\n\nAlternatively, you can choose known (model, dataset) pairs using the --model-\ndataset option. For example:\n\n    \n    \n    tti-eval build --model-dataset clip/Alzheimer-MRI --model-dataset bioclip/Alzheimer-MRI\n\n### Model Evaluation\n\nTo evaluate models, use the CLI command tti-eval evaluate. This command\nenables interactive selection of model and dataset combinations for\nevaluation.\n\nAlternatively, you can specify known (model, dataset) pairs using the --model-\ndataset option. For example:\n\n    \n    \n    tti-eval evaluate --model-dataset clip/Alzheimer-MRI --model-dataset bioclip/Alzheimer-MRI\n\n### Embeddings Animation\n\nTo create 2D animations of the embeddings, use the CLI command tti-eval\nanimate. This command allows to visualise the reduction of embeddings from two\nmodels on the same dataset.\n\nYou have the option to interactively select two models and a dataset for\nvisualization. Alternatively, you can specify the models and dataset as\narguments. For example:\n\n    \n    \n    tti-eval animate clip bioclip Alzheimer-MRI\n\nThe animations will be saved at the location specified by the environment\nvariable TTI_EVAL_OUTPUT_PATH. By default, this path corresponds to the output\nfolder in the repository directory. To interactively explore the animation in\na temporary session, use the --interactive flag.\n\n## Some Example Results\n\nOne example of where this tti-eval is useful is to test different open-source\nmodels against different open-source datasets within a specific domain. Below,\nwe focused on the medical domain. We evaluate nine different models of which\nthree of them are domain specific. The models are evaluated against four\ndifferent medical datasets. Note, Further down this page, you will find links\nto all models and datasets.\n\nFigure 1: Linear probe accuracy across four different medical datasets.\nGeneral purpose models are colored green while models trained for the medical\ndomain are colored red.\n\n## Datasets\n\nThis repository contains classification datasets sourced from Hugging Face and\nEncord.\n\n> \u26a0\ufe0f Currently, only image and image groups datasets are supported, with\n> potential for future expansion to include video datasets.\n\nDataset Title| Implementation| HF Dataset  \n---|---|---  \nAlzheimer-MRI| Hugging Face| Falah/Alzheimer_MRI  \nchest-xray-classification| Hugging Face| trpakov/chest-xray-classification  \nLungCancer4Types| Hugging Face| Kabil007/LungCancer4Types  \nplants| Hugging Face| sampath017/plants  \nskin-cancer| Hugging Face| marmal88/skin_cancer  \nsports-classification| Hugging Face| HES-XPLAIN/SportsImageClassification  \nrsicd| Encord| * Requires ssh key and access to the Encord project  \n  \n### Add a Dataset from a Known Source\n\nTo register a dataset from a known source, you can include the dataset\ndefinition as a JSON file in the sources/datasets folder. The definition will\nbe validated against the schema defined by the\ntti_eval.dataset.base.DatasetDefinitionSpec Pydantic class to ensure that it\nadheres to the required structure. You can find the explicit schema in\nsources/dataset-definition-schema.json.\n\nCheck out the declarations of known sources at tti_eval.dataset.types and\nrefer to the existing dataset definitions in the sources/datasets folder for\nguidance. Below is an example of a dataset definition for the plants dataset\nsourced from Hugging Face:\n\n    \n    \n    { \"dataset_type\": \"HFDataset\", \"title\": \"plants\", \"title_in_source\": \"sampath017/plants\" }\n\nIn each dataset definition, the dataset_type and title fields are required.\nThe dataset_type indicates the name of the class that represents the source,\nwhile title serves as a reference for the dataset on this platform.\n\nFor Hugging Face datasets, the title_in_source field should store the title of\nthe dataset as it appears on the Hugging Face website.\n\nFor datasets sourced from Encord, other set of fields are required. These\ninclude project_hash, which contains the hash of the project, and\nclassification_hash, which contains the hash of the radio-button (multiclass)\nclassification used in the labels.\n\n### Add a Dataset Source\n\nExpanding the dataset sources involves two key steps:\n\n  1. Create a dataset class that inherits from tti_eval.dataset.Dataset and specifies the input requirements for extracting data from the new source. This class should encapsulate the necessary logic for fetching and processing dataset elements.\n  2. Generate a dataset definition in JSON format and save it in the sources/datasets folder, following the guidelines outlined in the previous section. Ensure that the definition includes essential fields such as dataset_type, title, and module_path, which points to the file containing the dataset class implementation.\n\n> It's recommended to store the file containing the dataset class\n> implementation in the tti_eval/dataset/types folder and add a reference to\n> the class in the __init__.py file in the same folder. This ensures that the\n> new dataset type is accessible by default for all dataset definitions,\n> eliminating the need to explicitly state the module_path field for datasets\n> from such source.\n\n### Programmatically Add a Dataset\n\nAlternatively, you can programmatically add a dataset, which will be available\nonly for the current session, using the register_dataset() method of the\ntti_eval.dataset.DatasetProvider class.\n\nHere is an example of how to register a dataset from Hugging Face using Python\ncode:\n\n    \n    \n    from tti_eval.dataset import DatasetProvider, Split from tti_eval.dataset.types import HFDataset DatasetProvider.register_dataset(HFDataset, \"plants\", title_in_source=\"sampath017/plants\") ds = DatasetProvider.get_dataset(\"plants\", split=Split.ALL) print(len(ds)) # Returns: 219\n\n### Remove a Dataset\n\nTo permanently remove a dataset, simply delete the corresponding JSON file\nstores in the sources/datasets folder. This action removes the dataset from\nthe list of available datasets in the CLI, disabling the option to create any\nfurther embedding using its data. However, all embeddings previously built on\nthat dataset will remain intact and available for other tasks such as\nevaluation and animation.\n\n## Models\n\nThis repository contains models sourced from Hugging Face, OpenCLIP and local\nimplementations based on OpenCLIP models.\n\nTODO: Some more prose about what's the difference between implementations.\n\n### Hugging Face Models\n\nModel Title| Implementation| HF Model  \n---|---|---  \napple| OpenCLIP| apple/DFN5B-CLIP-ViT-H-14  \napple| OpenCLIP| apple/DFN5B-CLIP-ViT-H-14  \nbioclip| OpenCLIP| imageomics/bioclip  \neva-clip| OpenCLIP| BAAI/EVA-CLIP-8B-448  \nvit-b-32-laion2b| OpenCLIP| laion/CLIP-ViT-B-32-laion2B-s34B-b79K  \nclip| Hugging Face| openai/clip-vit-large-patch14-336  \nfashion| Hugging Face| patrickjohncyh/fashion-clip  \nplip| Hugging Face| vinid/plip  \npubmed| Hugging Face| flaviagiammarino/pubmed-clip-vit-base-patch32  \nrsicd| Hugging Face| flax-community/clip-rsicd  \nsiglip_large| Hugging Face| google/siglip-large-patch16-256  \nsiglip_small| Hugging Face| google/siglip-base-patch16-224  \nstreet| Hugging Face| geolocal/StreetCLIP  \ntinyclip| Hugging Face| wkcn/TinyCLIP-ViT-40M-32-Text-19M-LAION400M  \n  \n### Locally Trained Models\n\nModel Title| Implementation| Weights  \n---|---|---  \nrsicd-encord| LocalOpenCLIP| -  \n  \n### Add a Model from a Known Source\n\nTo register a model from a known source, you can include the model definition\nas a JSON file in the sources/models folder. The definition will be validated\nagainst the schema defined by the tti_eval.model.base.ModelDefinitionSpec\nPydantic class to ensure that it adheres to the required structure. You can\nfind the explicit schema in sources/model-definition-schema.json.\n\nCheck out the declarations of known sources at tti_eval.model.types and refer\nto the existing model definitions in the sources/models folder for guidance.\nBelow is an example of a model definition for the clip model sourced from\nHugging Face:\n\n    \n    \n    { \"model_type\": \"ClosedCLIPModel\", \"title\": \"clip\", \"title_in_source\": \"openai/clip-vit-large-patch14-336\" }\n\nIn each model definition, the model_type and title fields are required. The\nmodel_type indicates the name of the class that represents the source, while\ntitle serves as a reference for the model on this platform.\n\nFor non-local models, the title_in_source field should store the title of the\nmodel as it appears in the source. For model checkpoints in local storage, the\ntitle_in_source field should store the title of the model used to train it.\nAdditionally, on models sourced from OpenCLIP the optional pretrained field\nmay be needed. See the list of OpenCLIP models here.\n\n### Add a Model Source\n\nExpanding the model sources involves two key steps:\n\n  1. Create a model class that inherits from tti_eval.model.Model and specifies the input requirements for loading models from the new source. This class should encapsulate the necessary logic for processing model elements and generating embeddings.\n  2. Generate a model definition in JSON format and save it in the sources/models folder, following the guidelines outlined in the previous section. Ensure that the definition includes essential fields such as model_type, title, and module_path, which points to the file containing the model class implementation.\n\n> It's recommended to store the file containing the model class implementation\n> in the tti_eval/model/types folder and add a reference to the class in the\n> __init__.py file in the same folder. This ensures that the new model type is\n> accessible by default for all model definitions, eliminating the need to\n> explicitly state the module_path field for models from such source.\n\n### Programmatically Add a Model\n\nAlternatively, you can programmatically add a model, which will be available\nonly for the current session, using the register_model() method of the\ntti_eval.model.ModelProvider class.\n\nHere is an example of how to register a model from Hugging Face using Python\ncode:\n\n    \n    \n    from tti_eval.model import ModelProvider from tti_eval.model.types import ClosedCLIPModel ModelProvider.register_model(ClosedCLIPModel, \"clip\", title_in_source=\"openai/clip-vit-large-patch14-336\") model = ModelProvider.get_model(\"clip\") print(model.title, model.title_in_source) # Returns: clip openai/clip-vit-large-patch14-336\n\n### Remove a Model\n\nTo permanently remove a model, simply delete the corresponding JSON file\nstores in the sources/models folder. This action removes the model from the\nlist of available models in the CLI, disabling the option to create any\nfurther embedding with it. However, all embeddings previously built with that\nmodel will remain intact and available for other tasks such as evaluation and\nanimation.\n\n## Set Up the Development Environment\n\n  1. Create the virtual environment, add dev dependencies and set up pre-commit hooks.\n    \n        ./dev-setup.sh\n\n  2. Add environment variables:\n    \n        export TTI_EVAL_CACHE_PATH=$PWD/.cache export TTI_EVAL_OUTPUT_PATH=$PWD/output export ENCORD_SSH_KEY_PATH=<path_to_the_encord_ssh_key_file>\n\n## Contributing\n\nContributions are welcome! Please feel free to open an issue or submit a pull\nrequest with your suggestions, bug fixes, or new features.\n\n### Adding Dataset Sources\n\nTo contribute by adding dataset sources, follow these steps:\n\n  1. Store the file containing the new dataset class implementation in the tti_eval/dataset/types folder. Don't forget to add a reference to the class in the __init__.py file in the same folder. This ensures that the new dataset type is accessible by default for all dataset definitions, eliminating the need to explicitly state the module_path field for datasets from such source.\n  2. Open a pull request with the necessary changes. Make sure to include tests validating that data retrieval, processing and usage are working as expected.\n  3. Document the addition of the dataset source, providing details on its structure, usage, and any specific considerations or instructions for integration. This ensures that users have clear guidance on how to leverage the new dataset source effectively.\n\n### Adding Model Sources\n\nTo contribute by adding model sources, follow these steps:\n\n  1. Store the file containing the new model class implementation in the tti_eval/model/types folder. Don't forget to add a reference to the class in the __init__.py file in the same folder. This ensures that the new model type is accessible by default for all model definitions, eliminating the need to explicitly state the module_path field for models from such source.\n  2. Open a pull request with the necessary changes. Make sure to include tests validating that model loading, processing and embedding generation are working as expected.\n  3. Document the addition of the model source, providing details on its structure, usage, and any specific considerations or instructions for integration. This ensures that users have clear guidance on how to leverage the new model source effectively.\n\n## Known Issues\n\n  1. autofaiss: The project depends on the autofaiss library which can give some trouble on Windows. Please reach out or raise an issue with as many system and version details as possible if you encounter it.\n\n## About\n\nEvaluate custom and HuggingFace text-to-image/zero-shot-image-classification\nmodels like CLIP, SigLIP, DFN5B, and EVA-CLIP. Metrics include Zero-shot\naccuracy, Linear Probe, Image retrieval, and KNN accuracy.\n\nencord.com\n\n### Topics\n\nknn-search evaluation-metrics evaluation-framework linear-probing embedding-\nevaluation zero-shot-retrieval zero-shot-classification model-evaluation-\nmetrics embeddings-extraction zero-shot-image-classification text-to-image-\nevaluation\n\n### Resources\n\nReadme\n\n### License\n\nApache-2.0 license\n\nActivity\n\nCustom properties\n\n### Stars\n\n13 stars\n\n### Watchers\n\n4 watching\n\n### Forks\n\n0 forks\n\nReport repository\n\n## Contributors 3\n\n  * eloy-encord Eloy P\u00e9rez Torres\n  * frederik-encord Frederik Hvilsh\u00f8j\n  * Jim-Encord\n\n## Languages\n\n  * Python 99.7%\n  * Shell 0.3%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
