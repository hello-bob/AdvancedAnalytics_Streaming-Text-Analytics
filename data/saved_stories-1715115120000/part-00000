{"aid": "40286220", "title": "Transforming LLMs into parallel decoders boosts inference speed by up to 3.5x", "url": "https://hao-ai-lab.github.io/blogs/cllm/", "domain": "hao-ai-lab.github.io", "votes": 3, "user": "snyhlxde", "posted_at": "2024-05-07 14:49:45", "comments": 0, "source_title": "Consistency Large Language Models: A Family of Efficient Parallel Decoders", "source_text": "Consistency Large Language Models: A Family of Efficient Parallel Decoders | Hao AI Lab @ UCSD\n\n# Consistency Large Language Models: A Family of Efficient Parallel Decoders\n\nMay 6, 2024 \u00b7 10 min \u00b7 Siqi Kou*, Lanxiang Hu*, Zhezhi He, Zhijie Deng, Hao\nZhang\n\nAn instance of Jacobi trajectory and an illustration of the global consistency\nloss learning objective.\n\nTL;DR: LLMs have been traditionally regarded as sequential decoders, decoding\none token after another. In this blog, we show pretrained LLMs can be easily\ntaught to operate as efficient parallel decoders. We introduce Consistency\nLarge Language Models (CLLMs), a new family of parallel decoders capable of\nreducing inference latency by efficiently decoding an -token sequence per\ninference step. Our research shows this process \u2013 mimicking human cognitive\nprocess of forming complete sentences in mind before articulating word by word\n\u2013 can be effectively learned by simply finetuning pretrained LLMs.\nSpecifically, CLLMs are trained to perform parallel decoding by mapping any\nrandomly initialized -token sequence to the same result yielded by\nautoregressive (AR) decoding in as few steps as possible. Experiment results\nshow CLLMs obtained using our proposed method are highly effective, showing to\nimprovements in generation speed, in par with or even beter than other fast\ninference techniques like Medusa2 and Eagle, yet require no additional memory\ncost to accomodate auxiliary model components at inference time.\n\nFigure 1: Demo of speedup by CLLM-ABEL-7B-001 in comparison with baseline\nABEL-7B-001 using Jacobi decoding on GSM8K.\n\n## Background: Jacobi Decoding#\n\nLarge language models (LLMs) are transforming the landscape of human lives,\nfrom programming to offering legal and health advice. However, during\ninference, LLMs generate responses token by token using AR decoding as shown\nin Figure 1, leading to high latency for longer responses. Using AR decoding,\nit often necessitates architectural modifications, auxiliary components, or\ndraft models, to speed up inference by generating more than one token at a\ntime.\n\nFigure 2: illustration of conventional AR decoding: one token is generated at\na time.\n\nJacobi decoding originates from the Jacobi and Gauss-Seidel fixed-point\niteration for solving nonlinear equations, and is proven identical to AR\ngeneration using greedy decoding. Jacobi decoding reformulates the sequential\ngeneration process into a system of non-linear equations with variables\nsolvable in parallel based on Jacobi iteration. Each iteration step might\npredict more than one correct token (By \u201ccorrect\u201d, we mean alignment with the\nAR decoding result under a greedy sampling strategy), thereby accelerating AR\ndecoding potentially.\n\nFigure 3: illustration of Jacobi decoding: -token sequence is fed into the LLM\nand iterates until convergence.\n\nTo be specific, Jacobi decoding method first randomly guesses the next tokens\nin a sequence (referred to as -token sequence hereinafter unless specified\notherwise) from an input prompt. The -token sequence, along with the prompt,\nis then fed to the LLM to iteratively update itself. This process continues\nuntil the -token sequence stabilizes and no further changes occur, reaching a\nfixed point. Notably, Jacobi decoding requires no more queries to the LLM than\nauto-regressive (AR) decoding. Eventually, the -token sequence converges to\nthe output that would be generated by AR decoding under a greedy strategy.\nThis progression from an initial random guess to the final AR generation\noutcome traces what is known as a Jacobi trajectory. An instance of Jacobi\ndecoding iteration process and Jacobi trajectory is illustrated in Figure 2.\n\n### Limitations of Jacobi Decoding#\n\nHowever, vanilla Jacobi decoding for LLMs shows only marginal speedup over AR\ndecoding in practice, e.g., an average of speedup. This is because an AR-\ntrained LLM can rarely yield a correct token when there are incorrections in\nits preceding tokens. Thereby, most Jacobi iterations gain only one correction\nfor the -token sequence, resulting in a longer trajectory as illustrated on\nthe left side of Figure 3.\n\nLookahead decoding and speculative decoding methods try to mitigate\ninefficiency in Jacobi decoding and conventional AR decoding, but incurs extra\nmemory cost during inference time. While CLLMs require none.\n\n## Consistency LLMs (CLLMs)#\n\n### Jacobi Decoding Preliminary#\n\nGiven a prompt and a pre-trained LLM , LLM typically generates with the\nstandard AR decoding method under the greedy strategy, i.e.\n\nJacobi decoding re-frames the LLM inference process as solving a system of\nnonlinear equations to transform the decoding process into a parallelizable\ncomputation. Consider, , we can rewrite the above equation as a system of\nnonlinear equations:\n\nNote that the process exits at some k such that and we define as the fixed\npoint, and as the Jacobi trajectory.\n\n### Training with Jacobi Trajectories#\n\nTo address this, we propose adapting pre-trained LLMs so that they can\nconsistently map any point on the Jacobi trajectory to the fixed point .\nSurprisingly, we find such an objective is analogous to that of consistency\nmodels, a leading acceleration approach for diffusion models. In our proposed\nmethod, we use Jacobi trajectories collected from a target model to train the\nmodel with a loss that encourages single-step convergence during Jacobi\niterations. For each target model to be adapted as a CLLM, the training\nconsists of two parts:\n\n  * Jacobi trajectory preparation: for each prompt, we sequentially perform Jacobi decoding for every truncation of tokens until the entire response sequence has been generated, which amounts to a concatenation of all consecutive fixed points. Each sequence generated along a trajectory counts as one data entry. Note that for a lengthy response of () tokens, such truncation avoids slow model evaluation on lengthy input.\n\n  * Training with consistency and AR loss: we jointly optimize two losses for tuning CLLMs, the consistency loss guarantees the prediction of multiple tokens at once and the AR loss prevents the CLLM from deviating from the target LLM so as to maintain generation quality.\n\nFigure 4: an illustration of consistency training for one-step convergence:\nrefining the target LLM to consistently predict the fixed point given any\nstate along Jacobi trajectory as input.\n\n### Consistency and AR Loss#\n\n#### Consistency Loss#\n\nLet denote the target LLM. Let denote the CLLM with parameters initialized\nwith those of . For a prompt and the corresponding Jacobi trajectory , let and\ndenote a random state and the fixed point on the trajectory, respectively.\n\nWe can encourage CLLM to output with as the input by minimizing the following\nloss, termed as the global consistency (GC) loss:\n\nwhere and we abuse notations to represent uniform sampling from the dataset,\nand we abuse notations to represent uniform sampling from the dataset. denotes\nthe distance between two distributions, choices are discussed in the GKD\nmethod and in this paper we primarily experiment with the forward KL.\n\nAlternatively, local consistency (LC) loss following the formulation in\nconsistency models, where the adjacent states in a Jacobi trajectory are\ndriven to yield the same outputs:\n\n#### AR Loss#\n\nTo avoid deviating from the distribution of the target LLM, we incorporate the\ntraditional AR loss based on the generation of the target LLM :\n\nPutting the two loss together, with some weight , the total loss for training\na CLLM is:\n\n## Experiments#\n\n### Results#\n\nOur experiments contain three domain-specific tasks, including Spider (text-\nto-SQL), Human-Eval (Python code completion), and GSM8k (math), and the\nbroader open-domain conversational challenge, MT-bench. Reported experiments\nwere conducted using either fine-tuned coder LLM, Deepseek-coder-7B-instruct,\nLLaMA-2-7B or ABEL-7B-001 as the target model depending on the task. Both\ntraining and evaluation are carried out on NVIDIA A100 40GB servers.\n\nFigure 5: CLLM speedup on different downstream tasks. CLLMs are significantly\nfaster than pre-trained models and achieve comparable speedups in comparison\nwith Medusa, yet with no extra cost at inference time.\n\nFigure 6: illustration of CLLM vs. other baselines on domain-specific tasks\n(Spider, CSN-Python, GSM8k), as well as on MT-bench. CLLMs achieve similar or\neven better speedup in comoparison with Medusa2 while introducing no extra\ninference cost (in terms FLOPS and memory consumption).\n\nSpecialized domains: From Figure 5, we can see that in comparison with other\nbaselines including the original target model, Medusa2, and speculative\ndecoding, CLLMs achieve the most significant speedup.\n\nOpen-domain conversational Challenge (MT-bench): CLLM trained from LLaMA2-7B\nusing ShareGPT dataset can achieve roughly the same speedup as Medusa2 when\ncombined with lookahead decoding, with comparable scores on MT-bench. However,\nCLLM offers higher adaptability and memory efficiency as it requires no\nmodifications to the target model\u2019s original architecture and no auxiliary\ncomponents.\n\n### Training Cost#\n\nThe fine-tuning cost of CLLMs is moderate, e.g., passing only around 1M tokens\nfor LLaMA-7B to achieve a speedup on the Spider dataset. In the cases where\nthe dataset size is large, for example, for CodeSearchNet-Python, only 10% of\nthe dataset is required to generate Jacobi trajectories in training CLLMs to\nobtain around speedup. The total number of tokens can be estimated by taking:\n\navg # of trajectories per prompt avg trajectory length # of prompts.\n\ndataset| estimated training cost (tokens)| of pre-training cost  \n---|---|---  \nSpider| 2M  \nCodeSearchNet-Python| 100M  \nGSM8K| 10M  \nShareGPT| 200M  \n  \n### Fast Forwarding and Stationary Tokens#\n\nFigure 7: Comparison of Jacobi trajectory between a target LLM and CLLMs on\nSpider. Each point along the Jacobi trajectory is a color-coded sequence: blue\nfor correct tokens matching with AR results, and red for inaccurate ones. CLLM\ndemonstrates enhanced efficiency, converging to the fixed point faster the\nTarget LLM. This increased efficiency in the CLLM can be attributed to the\nconsistency loss which facilitates the learning of the structure of each\n-token sequence given a prefix.\n\nThe left side of Figure 6 shows target LLMs typically generate only one\ncorrect token in one iteration. In contrast, in CLLMs, we identify fast\nforwarding phenomenon where multiple consecutive tokens are correctly\npredicted in a single Jacobi iteration.\n\nMoreover, tokens correctly generated in advance (e.g. \u201ccountry\u201d and \u201cH\u201d at\nindex 6 and 7 on the left side of Figure 6), are often replaced inaccurately\nin subsequent iterations in target LLMs. On the other hand, CLLMs exhibit the\ncapability of predicting correct tokens preemptively, even with preceding\nincorrect tokens, while ensuring the tokens remain unchanged. We term such\ntokens as stationary tokens. Both phenomena contribute to the fast convergence\nin Jacobi decoding of CLLMs, thereby leading to a considerable generation\nspeedup.\n\nWe observe that CLLMs acquire a crucial linguistic concept through training \u2013\ncollocations: a series of words or terms that co-occur more frequently than\none would expect by random chance. Language is not solely composed of isolated\nwords but also relies heavily on specific word pairings. Examples of\ncollocations are abundant in both natural and coding languages. They include\nverb + preposition combinations (e.g., \u2018\u2019talk to\u2019\u2019, \u2018\u2018remind ... of ...\u2019\u2019),\nverb + noun structures (e.g., \u2018\u2018make a decision\u2019\u2019, \u2018\u2018catch a cold\u2019\u2019), and many\nmore domain-specific syntactical structures (e.g., \u2018\u2018SELECT ... FROM ...\u2019\u2019,\n\u2018\u2018if ... else\u2019\u2019 for programming). The consistency generation objective allows\nCLLMs to infer such structures from any point in the Jacobi trajectory,\nencouraging CLLMs to acquire proficiency in numerous collocations and thereby\npredict multiple words simultaneously to minimize iteration steps.\n\n## Get started#\n\nPlease see our paper for more details. We also invite you to try out our\ncodebase and CLLM checkpoints!\n\n## Acknowledgement#\n\nWe would like to thank Yang Song, Canwen Xu, Yonghao Zhuang, Dacheng Li and\nYichao Fu for providing insightful feedback.\n\n## Citation#\n\n    \n    \n    @misc{kou2024cllms, title={CLLMs: Consistency Large Language Models}, author={Siqi Kou and Lanxiang Hu and Zhezhi He and Zhijie Deng and Hao Zhang}, year={2024}, eprint={2403.00835}, archivePrefix={arXiv}, primaryClass={cs.CL} }\n\n\u00a9 2024 Hao AI Lab @ UCSD Powered by Hugo & PaperMod , Adapted by Lanxiang Hu &\nHao Zhang\n\n", "frontpage": true}
