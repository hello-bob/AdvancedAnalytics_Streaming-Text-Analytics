{"aid": "40199444", "title": "Fighting LLMs to get the types of response you want", "url": "https://sourcery.ai/blog/dont-tell-me-what-not-to-do/", "domain": "sourcery.ai", "votes": 1, "user": "tim_sourcery", "posted_at": "2024-04-29 15:17:43", "comments": 0, "source_title": "Don\u2019t tell me what (not) to do!", "source_text": "Don\u2019t tell me what (not) to do!\n\n### Don\u2019t tell me what (not) to do!\n\nOn the surprising stubbornness of large language models\n\nAuthor\n\nNick Thapen\n\nDate\n\nApr 26, 2024\n\nAuthor\n\nNick Thapen\n\nDate\n\nApr 26, 2024\n\nBack to Blog Home\n\nPhoto by Phil Goodwin on Unsplash\n\n## Dealing with language models\n\nLarge language models (LLMs) are amazing pieces of technology, able to\n(fairly) reliably perform some advanced tasks which computers have not\npreviously been able to.\n\nA big issue though is that their behaviour cannot be reasoned about like\ntraditional software. If I\u2019m coding up a script and there\u2019s a failure case I\nwant to avoid it\u2019s straightforward to tell the computer not to do it. When I\nrun the script it will then do what I want every single time (threading\nconsiderations aside). An LLM is different since it behaves probabilistically.\nMaybe it will do what you want 90% of the time, maybe only 20% - which is a\nproblem.\n\n## Why won't you just do what I want?\n\nIt's often very easy to quickly spin up a great demo using LLMs, or get a\nproduct 70-80% of the way to being great. Pushing for that last 20% can be a\nhard slog, and a large part of that is the unreliability. When getting a\nsystem involving LLMs to work reliably you can use prompt engineering to get\nas close as possible, then add non-LLM based workarounds to handle the rest.\n\nOften a model will do something mostly right, but insist on handling some\ncases in a way that you don\u2019t want. It\u2019s then tempting to instruct the model\nnot to do that. Sometimes this works, but often it doesn\u2019t.\n\nIn fact mentioning something in a prompt negatively can draw the model\u2019s\nattention to it and perversely make it more likely to do it.\n\nAt this point it\u2019s tempting to ask the model NOT TO DO THE THING, or resorting\nto tips or threats, but whether this works is kind of inconclusive. A better\nway forward is to work with the grain of the model - if it really wants to do\nsomething, let it do it, but ask it to classify it\u2019s behaviour. You can then\nadd a later step that filters out the responses you don\u2019t want.\n\n## A concrete example - code reviews\n\nHere\u2019s an example of this that I\u2019ve been working on lately at Sourcery. We\u2019d\nlike to add a comment to our code reviews to remind users when they\u2019ve\nforgotten to update the docstring to a function or method. This would be\nreally nice - it\u2019s easy to forget to update docstrings, and then they\ngradually decay and become less and less useful over time.\n\nIt would be very hard to do this with traditional static analysis tools - how\ncan you tell if the functionality of a method has changed and now differs from\nwhat the docstring says? However translation between code and natural English\nis something an LLM does pretty well.\n\nHere\u2019s an example of such a comment:\n\n    \n    \n    **suggestion (docstrings):** Please update the docstring for function: `get_github_installation_by_id` Reason for update: Functionality has changed to include verification that the installation belongs to the authenticated user. Suggested new docstring: \"\"\"Fetches a GitHub installation by its ID from the database and verifies it belongs to the authenticated user. Returns None if the installation does not exist or does not belong to the user.\"\"\"\n\nTo generate these comments we give the model an extended diff of the changes\nmade, so that it can see which functions have been updated and the existing\ndocstrings. Here\u2019s where the stubbornness of the model comes in - it\u2019s\nextremely keen on adding docstrings for functions that don\u2019t already have\nthem. Whether to add a docstring to a function is very specific to each team\nand codebase, so we don\u2019t want to start annoying users by constantly\nsuggesting they add docstrings.\n\nWe experimented with various methods for getting the model to stop doing this:\n\n  * Asking it nicely not to do it\n  * Asking it not to do it in the system prompt\n  * Using a chain of thought approach to get it to carefully consider whether the functions have docstrings\n  * Stressing that it should only return updates for existing docstrings\n\n    * Here we went as far as asking it to return the existing docstring to check that there was one, but it just started hallucinating them where there wasn\u2019t one.\n\nSadly none of these approaches worked.\n\n## Models are better at classifying their behaviour than changing it\n\nAfter thinking about it for a while we decided to try and go around the\nproblem rather than tackling it directly. Rather than asking the model to stop\nproducing responses for functions with docstrings we ask it to classify\nwhether the function has an existing docstring.\n\nWe ask for the results in JSON - so here was the field we added to the schema:\n\n    \n    \n    // Does the function have an existing docstring that is present in the diff. Look back at the diff to determine this. has_existing_docstring: boolean;\n\nUnlike the previous approaches this actually worked pretty well!\n\n## Then you can filter things yourself\n\nWe then added a filtering step to remove responses from the model with\nhas_existing_docstring as False. This hasn\u2019t eliminated the problem\ncompletely, but it\u2019s close!\n\nThis isn't quite as good as if the model had never produced the responses -\nthere's still a cost and a performance implication to getting output tokens\nthat you then have to throw away. To us it's definitely worth it to eliminate\nnoise.\n\nWe\u2019ve had success applying this technique to various different aspects of our\ncode review process - it\u2019s definitely a useful tool in the prompt engineer\u2019s\narsenal.\n\n## A quick recap\n\nIf the model is showing an unwanted behaviour which negative prompting isn\u2019t\nresolving you can:\n\n  * Ask the model to classify its response as showing or not showing the behaviour\n  * Then filter out the responses that show the behaviour in code\n\nWe\u2019re working to build a better automated version of code reviews at Sourcery.\nTry it out on any of your GitHub repos or reach out to hello@sourcery.ai to\nlearn more.\n\n## See Sourcery in action\n\nPyatmo\n\n#495\n\nPR approved\n\n3 comments from Sourcery\n\nrtk-rnjn/algorithms\n\n#77\n\nPR approved\n\n3 comments from Sourcery\n\nCPUT-DEVS/devpost-hackathon\n\n#12\n\nPR approved\n\n2 comments from Sourcery\n\nTry Sourcery code reviews on all your PR's free for 14 days\n\n### Product\n\n  * Code Review\n  * Coding Assistant\n  * Pricing\n\n### Integrations\n\n  * VS Code\n  * PyCharm\n  * Vim\n  * Sublime\n\n### Resources\n\n  * Blog\n  * Changelog\n  * Documentation\n  * Contact us\n\n### Company\n\n  * About us\n  * Privacy policy\n  * Terms\n\n### COMMUNITY\n\nTwitterGithubLinkedinEmail\n\n(\u30ce\u2609\u30ee\u2686)\u30ce \u2312*:\u30fb\u309a\u2727 sOURcERy\n\n", "frontpage": false}
