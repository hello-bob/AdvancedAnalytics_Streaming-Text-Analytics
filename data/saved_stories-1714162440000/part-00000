{"aid": "40168905", "title": "Can Phi3 and Llama3 Generalise to Bio-Medical Tasks? An Experiment", "url": "https://brainsteam.co.uk/2024/04/26/can-phi-and-llama-do-biology/", "domain": "brainsteam.co.uk", "votes": 1, "user": "DrRavenstein", "posted_at": "2024-04-26 13:00:07", "comments": 0, "source_title": "Can Phi3 and Llama3 Do Biology? - Brainsteam", "source_text": "Can Phi3 and Llama3 Do Biology? - Brainsteam\n\nCan Phi3 and Llama3 Do Biology? - Brainsteam\n\nSkip to content\n\n# Brainsteam\n\n## Can Phi3 and Llama3 Do Biology?\n\n26 April 2024\n\n/\n\nAI and Machine Learning\n\n/\n\njamesravey\n\nSmall Large Language Model might sound like a bit of an oxymoron. However, I\nthink it perfectly describes the class of LLMs in the 1-10 billion parameter\nrange like Llama and Phi 3. In the last few days, Meta and Microsoft have both\nreleased these open(ish) models that can happily run on normal hardware. Both\nmodels perform surprisingly well for their size, competing with much larger\nmodels like GPT 3.5 and Mixtral. However, how well do they generalise to new\nunseen tasks? Can they do biology?\n\n## Introducing Llama and Phi\n\nMeta\u2019s offering, Llama 3 8B, is an 8 billion parameter model that can be run\non a modern laptop. It performs almost as well as Mixtral 8x22B mixture-of-\nexpert model, a model 22x bigger and compute intensive.\n\nMicrosoft\u2019s model, Phi 3 mini, is around half the size of Llama 3 8B at 3.8\nbillion parameters. It is small enoughthat it can run on a high end smartphone\nat a reasonable speed. Incredibly, Phi actually beats Llama 3 8B, which is\ntwice as big, at a few popular benchmarks including MMLU which approximately\nmeasures \u201chow well does this model behave as a chatbot\u201d and HumanEval which\nmeasures \u201chow well can this model write code?\u201d.\n\nI\u2019ve also read a lot of anecdotal evidence about people chatting to these\nmodels and finding them quite engaging and useful chat partners (as opposed to\nprevious generation small models). This seems to back up the benchmark\nperformance and provide some validation of the models\u2019 utility.\n\nBoth Microsoft and Meta have stated that the key difference between these\nmodels and previous iterations of their smaller LLMs is the training regime.\nInterestingly, both companies applied very different training strategies .\nMeta trained Llama3 on over 15 trillion tokens (words) which is unusually\nlarge for a small model. Microsoft trained Phi on much smaller training sets\ncurated for high quality.\n\n### Can Phi, Llama and other Small Models Do Biology?\n\nHaving a model small enough to run on your phone and generate funny poems or\ntrivia questions is neat. However, for AI and NLP practitioners, a more\ninteresting question is \u201cdo these models generalise well to new, unseen\nproblems?\u201d\n\nI set out to gather some data about how well Phi and Llama 3 8B generalise to\na less-well-known task. As it happened, I have recently been working with my\nfriend Dan Duma on a test harness for BioAsq Task B. This is a less widely-\nknown, niche NLP task in the bio-medical space. The model is fed a series of\nsnippets from scientific papers and asked a question which it must answer\ncorrectly. There are four different formats of question which I\u2019ll explain\nbelow.\n\nThe 11th BioASQ Task B leaderboard is somewhat dominated by GPT-4 entrants\nwith perfect scores at some of the sub-tasks. If you were somewhat cynical,\nyou might consider this task \u201csolved\u201d. However, we think it\u2019s an interesting\narena for testing how well smaller models are catching up to big commercial\nofferings.\n\nBioASQ B is primarily a reading comprehension task with a slightly niche\nsubject-matter. The models under evaluation are unlikely to have been\nexplicitly trained to answer questions about this material. Smaller models are\noften quite effective at these sorts of RAG-style problems since they do not\nneed to have internalised lots of facts and general information. In fact, in\ntheir technical report, the authors of Phi-3 mini call out the fact that their\nmodel can\u2019t retain factual information but could be augmented with search to\nproduce reasonable results. This seemed like a perfect opportunity to test it\nout.\n\n### How The Task Works\n\nThere are 4 types of question in task B. Factoid, Yes/No, List and Summary.\nHowever, since summary is quite tricky to measure, it is not part of the\nBioASQ leaderboard. We also chose to omit summary from our tests.\n\nEach question is provided along with a set of snippets. These are full\nsentences or paragraphs that have been pre-extracted from scientific papers.\nIncidentally, that activity is BioASQ Task A and it requires a lot more moving\nparts since there\u2019s retrieval involved too. In Task B we are concerned with\nexisting sets of snippets and questions only.\n\nIn each case the model is required to respond with a short and precice exact\nanswer to the question. The model may optionally also provide an ideal answer\nwhich provides some rationale for that answer. The ideal answer may provide\nuseful context for the user but is not formally evaluated as part of BioASQ.\n\nYes/No questions require an exact answer of just \u201cyes\u201d or \u201cno\u201d. For List\nquestions, we are looking for a list of named entities (for example symptoms\nor types of microbe). For factoid we are typically looking for a single named\nentity. Models are allowed to respond to factoids with multiple answer.\nTherefore, factoids answers are scored by how closely to the top of the list\nthe \u201ccorrect\u201d answer is ranked.\n\nThe Figure from the Hseuh et al 2023 Paper below illustrates this quite well:\n\nFigure 1 from the Hseuh et al 2023 Paper illustrates the different task types\nsuccinctly\n\n### Our Setup\n\nWe wrote a python script that passes the question, context and guidance about\nthe type of question to the model. We used a patched version of Ollama that\nallowed us to put restrictions on the shape of the model output. This allowed\nus to ensure responses were valid JSON in the same shape and structure as the\nBioASQ examples. These forced grammars saved us loads of time trying to coax\nJSON out of models in the structure we want. This is something that smaller\nmodels aren\u2019t great at. Sometimes models would still fail to give valid\nresponses. For example, sometimes they get stuck in infinite loops spitting\nout brackets or newlines. We gave models up to 3 chances to produce a JSON\nresponse before a question is marked unanswerable and skipped.\n\n#### Prompts\n\nWe used exactly the same prompts for all of the models which may have left\nroom for further performance improvements. The exact prompts and grammar\nconstraints that we used can be found here. Snippets are concatenated together\nwith newlines in between them and provided as \u201ccontext\u201d in the prompt\ntemplate.\n\nWe used the official BioASQ scoring tool to evaluate the responses and produce\nthe results below. We evaluated our pipeline on the Task 11B Golden Enriched\ntest set. You have to create a free account at bioasq to log in and download\nthe data.\n\n#### Models\n\nWe compared quantized versions of Phi and Llama with some other similarly\nsized models which perform well at benchmarks.\n\n  * Llama 3 8B\n  * Phi 3 Mini 3.8B\n  * Mistral 7B\n  * Starling LM 7B\n  * Zephyr 7B\n\nNote that although Phi is approx. half the size of the other models, the\nauthors report competitive results against much larger models for a number of\nwidely used benchmarks so it seems reasonable to compare it with these 7B and\n8B models as oppose to only benchmarking against other 4B and smaller models.\n\n### Results\n\n#### Yes/No Questions\n\nThe simplest type of BioASQ question is Yes/No. These results are measured\nwith macro F1 to allow us to get a single metric across the performance at\nboth \u201cyes\u201d and \u201cno\u201d questions.\n\nThe results show that all 5 models perform reasonably well at this task but\nPhi 3 lags behind the others a little bit, but only by about 10% next to it\u2019s\nclosest competitor. The best solutions to this task are coming in at 1.0 F1.\nLlama3 and Starling both achieve pretty close to perfect results here.\n\n#### Factoid Questions\n\nFor factoid answers we measure responses in MRR since the model can return\nmultiple possible answers. We are interested in how close the right answers\nare to the top of the list.\n\nThis graph is a lot starker than the yes/no graph. Llama 3 outperforms it\u2019s\nnext closest neighbour by a significant margin (roughly +0.40 MRR) . The best\nsolution to this task, again a GPT-4-based entrant, weighs in at 0.6316 MRR so\nit\u2019s pretty impressive that Llama 3 8B is providing results in the same\nballpark as a model many times larger. For this one, Phi is in third place\nafter Starling-LM 7B. Again, given that Phi is half the size of this model,\nit\u2019s quite impressive performance.\n\n#### List Questions\n\nWe measure list questions in F1. A false positive is when something irrelevant\nis included in the answer and a false negative is when something relevant is\nmissed from an answer. F1 gives us a single statistic that balances the two.\n\nThis one was a little surprising to me as Phi does a lot worse than any of its\ncounterparts. We noticed that Phi produced a much higher rate of unanswerable\nquestions than any of the other models which may be due to the somewhat\ncomplex JSON structure required by list type questions. It may be worth re-\ntesting with different formatting arrangements to see if the failure to format\nthe model masks reasonable performance at the task.\n\nLlama 3 8B wins again. The current best solution, again a GPT-4-based system,\nachieves an F1 of 0.72 so even Llama 3 8B leaves a relatively wide gap here.\nIt would be worth testing the larger variants of Llama 3 to see how well they\nperform at this task and whether they are competitive with GPT-4.\n\n## Discussion and Conclusion\n\n### Llama3\n\nWe\u2019ve seen that Llama 3 8B and, to a lesser extent, Phi 3 Mini, are able to\ngeneralise reasonably well to a reading comprehension task in a field that\nwasn\u2019t a primary concern for either se of model authors. This isn\u2019t conclusive\nevidence for or against the general performance of these models on unseen\ntasks. However, it is certainly an interesting data point that shows that,\nparticularly Llama 3 really is competitive with much larger models at this\ntask. I wonder if that\u2019s because it was trained on such a large corpus which\nmay have included some biomedical content as part of its training corpus.\n\n### Phi\n\nI\u2019m reluctant to too-harshly critique Phi\u2019s reasoning and reading\ncomprehension ability since there\u2019s a good chance that it was disadvantaged by\nour test setup and the forced JSON structure, particularly for the list task.\nHowever, the weaker performance at the yes/no questions may be a hint that it\nisn\u2019t quite as good at generalised reading comprehension as the competing\nlarger models.\n\nWe know that Phi3, like it\u2019s predecessor was trained on data that \u201cconsists of\nheavily filtered web data (according to the \u201ceducational level\u201d) from various\nopen internet sources, as well as synthetic LLM-generated data.\u201d However, we\ndon\u2019t know specifically what was included or excluded. If Llama 3 went for\n\u201ccast the net wide\u201d approach to data collection, it\u2019s likely that the latter\nmodel may have been exposed to more biomedical content \u201cby chance\u201d and thus be\nbetter at reasoning about concepts that perhaps Phi had never seen before.\n\nI do want to again call out that Phi is approximately half the size of the\nnext biggest model in our benchmark so it\u2019s performance is quite impressive in\nthat light.\n\n### Further Experiments\n\n#### Model Size\n\nI won\u2019t conjecture about whether 3.8B parameters is \u201ctoo small\u201d to generalise\ngiven the issues mentioned above but I\u2019d love to see some more tests of this\nin future. Do the larger variants of Phi (trained on the same data but simpliy\nwith more parameters) suffer from the same issues?\n\n#### Model Fine Tuning\n\nThe models that I\u2019ve been testing are small enough that they can be fine-tuned\non specific problems on a consumer-grade gaming GPU for very little cost. It\nseems entirely plausible to me that by fine-tuning these models on biomedical\ntext ands historical BioASQ training sets their performance could be improved\neven more significantly. The challenge would be in finding the right mix of\ndata.\n\n#### Better Prompts\n\nWe did not spend a lot of time attempting to build effective prompts during\nthis experiment. It may be that performance was left on the table because of\nthis oversight. Smaller models are often quite fussy about prompts. It might\nbe interesting to use a prompt optimisation framework like DSPy to be more\nsystematic about better prompts.\n\n#### Other Tasks\n\nI tried these models on BioAsq but this is lightyears away from conclusive\nevidence for whether or not these new-generation small models can generalise\nwell. It\u2019s simply a test of whether they can do biology. It will be very\ninteresting to try other novel tasks and see how well they perform. Watch this\nspace!\n\nPrevious: Self-hosting Llama 3 on a home server\n\n### Leave a Reply Cancel reply\n\n## Categories\n\n  * AI and Machine Learning\n  * Data Science\n  * Engineering Leadership\n  * Personal\n  * Philosophy and Thinking\n  * Software Development\n  * Tools for Thought\n  * Uncategorised\n\n## Quick Links\n\n  * About Me\n  * Digital Garden\n\njamesravey\n\n@jamesravey@brainsteam.co.uk\n\nCopyright \u00a9 James Ravenscroft 2023. All Rights Reserved.\n\nNotifications\n\n", "frontpage": false}
