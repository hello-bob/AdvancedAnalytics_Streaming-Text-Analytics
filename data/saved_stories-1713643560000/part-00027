{"aid": "40096261", "title": "We Built Slack AI to Be Secure and Private", "url": "https://slack.engineering/how-we-built-slack-ai-to-be-secure-and-private/", "domain": "slack.engineering", "votes": 1, "user": "hh91", "posted_at": "2024-04-20 10:32:46", "comments": 0, "source_title": "How We Built Slack AI To Be Secure and Private - Slack Engineering", "source_text": "How We Built Slack AI To Be Secure and Private - Slack Engineering\n\nSkip to content\n\n# How We Built Slack AI To Be Secure and Private\n\nHow We Built Slack AI To Be Secure and Private\n\nKelly Moran Senior Staff Tech Lead Manager\n\nCurtis Allen Senior Staff Software Engineer\n\nAlex Benjamin Senior Staff Software Engineer\n\n7 minutes \u2022 Written 2 days ago\n\nAt Slack, we\u2019ve long been conservative technologists. In other words, when we\ninvest in leveraging a new category of infrastructure, we do it rigorously.\nWe\u2019ve done this since we debuted machine learning-powered features in 2016,\nand we\u2019ve developed a robust process and skilled team in the space.\n\nDespite that, over the past year we\u2019ve been blown away by the increase in\ncapability of commercially available large language models (LLMs) \u2014 and more\nimportantly, the difference they could make for our users\u2019 biggest pain\npoints. Too much to read? Too hard to find stuff? Not anymore \u2014 90% of users\nwho adopted AI reported a higher level of productivity than those who didn\u2019t.\n\nBut as with any new technology, our ability to launch a product with AI is\npredicated on finding an implementation that meets Slack\u2019s rigorous standards\nfor customer data stewardship. So we set out to build not just awesome AI\nfeatures, but awesome and trusted AI.\n\nThe generative model industry is quite young; it is still largely research-\nfocused, and not enterprise-customer focused. There were few existing\nenterprise-grade security and privacy patterns for us to leverage when\nbuilding out the new Slack AI architecture.\n\nInstead, to inform how we built out Slack AI, we started from first\nprinciples. We began with our requirements: upholding our existing security\nand compliance offerings, as well as our privacy principles like \u201cCustomer\nData is sacrosanct.\u201d Then, through the specific lens of generative AI, our\nteam created a new set of Slack AI principles to guide us.\n\n  * Customer data never leaves Slack.\n  * We do not train large language models (LLMs) on customer data.\n  * Slack AI only operates on the data that the user can already see.\n  * Slack AI upholds all of Slack\u2019s enterprise-grade security and compliance requirements.\n\nThese principles made designing our architecture clearer, although sometimes\nmore challenging. We\u2019ll walk through how each of these informed what Slack AI\nlooks like today.\n\n## Customer data never leaves Slack\n\nThe first, and perhaps most important, decision we faced was how to ensure\nthat we could use a top-tier foundational model while never allowing customer\ndata to leave Slack-controlled VPCs. In the generative model industry, most\ncustomers of foundational models were calling the hosted services directly,\nand alternative options were scarce.\n\nWe knew this approach wouldn\u2019t work for us. Slack, and our customers, have\nhigh expectations around data ownership. In particular, Slack is FedRAMP High\nauthorized, which confers specific compliance requirements, including not\nsending customer data outside of our trust boundary. We wanted to ensure our\ndata didn\u2019t leave our AWS Virtual Private Cloud (VPC) so that we could\nguarantee that third parties would not have the ability to retain it or train\non it.\n\nSo we began to look for creative solutions where we could host a foundational\nmodel on our own infrastructure. However, most foundational models are closed-\nsource: Their models are their secret sauce, and they don\u2019t like to hand them\nto customers to deploy on their own hardware.\n\nFortunately, AWS has an offering where it can be the trusted broker between\nfoundational model provider and customer: AWS SageMaker. By using SageMaker,\nwe are able to host and deploy closed-source large language models (LLMs) in\nan escrow VPC, allowing us to control the lifecycle of our customers\u2019 data and\nensure the model provider has no access to Slack\u2019s customers\u2019 data. For more\non how Slack is using SageMaker, check out this post on the AWS blog.\n\nAnd there we had it: We had access to a top tier foundational model, hosted in\nour own AWS VPC, giving us assurances on our customer data.\n\n## We do not train large language models (LLMs) on customer data\n\nThe next decision was also key: We chose to use off-the-shelf models instead\nof training or fine-tuning models. We\u2019ve had privacy principles in place since\nwe began employing more traditional machine learning (ML) models in Slack,\nlike the ones that rank search results. Among these principles are that data\nwill not leak across workspaces, and that we offer customers a choice around\nthese practices; we felt that, with the current, young state of this industry\nand technology, we couldn\u2019t make strong enough guarantees on these practices\nif we trained a generative AI model using Slack\u2019s customers\u2019 data.\n\nSo we made the choice to use off-the-shelf models in a stateless way by\nemploying Retrieval Augmented Generation (RAG). With RAG, you include all of\nthe context needed to perform a task within each request, so the model does\nnot retain any of that data. For example, when summarizing a channel, we\u2019ll\nsend the LLM a prompt containing the messages to be summarized, along with\ninstructions for how to do so. The statelessness of RAG is a huge privacy\nbenefit, but it\u2019s a product benefit as well. All of Slack AI\u2019s results are\ngrounded in your company\u2019s knowledge base \u2014 not the public Internet \u2013 which\nmakes the results more relevant and accurate. You get the benefit of\nincorporating your proprietary and individual data set without the risk of a\nmodel retaining that data.\n\nUsing RAG can narrow down the set of models you can use; they need to have\n\u201ccontext windows\u201d large enough for you to pass in all the data you want to use\nin your task. Additionally, the more context you send an LLM, the slower your\nrequest will be, as the model needs to process more data. As you can imagine,\nthe task of summarizing all messages in a channel can involve quite a bit of\ndata.\n\nThis posed a challenge for us: Find a top-tier model with a large context\nwindow with fairly low latency. We evaluated a number of models and found one\nthat suited our first use cases, summarization and search, well. There was\nroom for improvement, though, and we began a long journey of both prompt\ntuning and chaining more traditional ML models with the generative models to\nimprove the results.\n\nRAG is getting easier and faster with each iteration of models: Context\nwindows are growing, as is the models\u2019 ability to synthesize data across a\nlarge context window. We\u2019re confident that this approach can get us both the\nquality we\u2019re aiming for while helping ensure our customers\u2019 data is\nprotected.\n\n## Slack AI only operates on the data that the user can already see\n\nIt is one of our core tenets that Slack AI can only see the same data that the\nrequesting user can see. Slack AI\u2019s search feature, for example, will never\nsurface any results to the user that standard search would not. Summaries will\nnever summarize content that the user could not otherwise see while reading\nchannels.\n\nWe ensure this by using the requesting user\u2019s Access Control List (ACLs) when\nfetching the data to summarize or search and by leveraging our existing\nlibraries that fetch the data to display in channel or on the search results\npage.\n\nThis wasn\u2019t hard to do, technically speaking, but it needed to be an explicit\nchoice; the best way to guarantee this was to build on top of, and reuse,\nSlack\u2019s core feature sets while adding some AI magic at the end.\n\nIt\u2019s worth noting, too, that only the user who invokes Slack AI can see the\nAI-generated output. This builds confidence that Slack is your trusted AI\npartner: Only the data that you can see goes in, and then only you can see the\noutput.\n\n## Slack AI upholds all of Slack\u2019s enterprise-grade security and compliance\nrequirements\n\nThere\u2019s no Slack AI without Slack, so we ensured that we integrated all of our\nenterprise grade compliance and security offerings. We follow the principle of\nleast data: We store only the data needed to complete the task, and only for\nthe duration necessary.\n\nSometimes the least data is: None. Where possible, Slack AI\u2019s outputs are\nephemeral: Conversation summaries and search answers all generate point-in-\ntime responses that are not stored on disk.\n\nWhere that\u2019s not possible, we reused as much of Slack\u2019s existing compliance\ninfrastructure as possible, and built new support where we had to. Many of our\ncompliance offerings come built in with our existing infrastructure, such as\nEncryption Key Management and International Data Residency. For others, we\nbuilt in special support to make sure that derived content, like summaries,\nare aware of the messages that went into them; for example, if a message is\ntombstoned because of Data Loss Protection (DLP), any summaries derived from\nthat message are invalidated. This makes DLP and other administrative controls\npowerful with Slack AI: Where these controls were already active on Slack\u2019s\nmessage content, they are also active Slack AI outputs.\n\nWhew \u2014 that was a long journey! And I didn\u2019t even get to take you through how\nwe build prompts, evaluate models, or handle spiky demand; we\u2019ll save that for\nnext time. But I\u2019m glad we started here, with security and privacy: We want\nour customers to know how seriously we take protecting their data, and how\nwe\u2019re safeguarding it each step of the way.\n\nInterested in helping us build Slack's AI capabilities? We're hiring! Apply\nnow\n\n  * aws\n  * engineering\n  * infrastructure\n  * machine-learning\n  * security\n  * software-architecture\n\n#### Post Types\n\n  * Post (161)\n\n#### Categories\n\n  * Uncategorized (139)\n\n#### Tags\n\n  * infrastructure (23)\n  * javascript (20)\n  * android (13)\n  * performance (13)\n  * software-development (13)\n\n#### Year\n\n  * 2024 (2)\n  * 2023 (16)\n  * 2022 (21)\n  * 2021 (24)\n  * 2020 (26)\n\n## Most Recent\n\nScared Robot\n\n### The Scary Thing About Automating Deploys\n\n@Sean McIlroy\n\nOur Journey Migrating to AWS IMDSv2\n\n### Our Journey Migrating to AWS IMDSv2\n\n@Archie Gunasekara\n\n### Building Custom Animations in the Workflow Builder\n\n@Christina Mudarth\n\n### @SlackEng how can I stay up-to-date on what's happening over there?\n\n### Follow us on Twitter\n\n## Recommended Reading\n\nScared Robot\n\n### The Scary Thing About Automating Deploys\n\n@Sean McIlroy\n\n### Managing Slack Connect\n\n@Yuriy Loginov\n\n### Traffic 101: Packets Mostly Flow\n\n@Slack Engineering\n\nSlack uses cookies to allow us to better understand how the site is used. By\ncontinuing to use this site, you consent to this policy. Click to learn more.\n\n# Search results\n\nFiltersShow filters\n\nSort by:\n\n\u2022\u2022\n\n## No results found\n\n## Filter options\n\nSearch powered by Jetpack\n\n", "frontpage": false}
