{"aid": "40196879", "title": "Memary is a cutting-edge long-term memory system based on a knowledge graph", "url": "https://github.com/kingjulio8238/memary", "domain": "github.com/kingjulio8238", "votes": 50, "user": "james_chu", "posted_at": "2024-04-29 11:12:51", "comments": 14, "source_title": "GitHub - kingjulio8238/memary: Longterm Memory for Autonomous Agents.", "source_text": "GitHub - kingjulio8238/memary: Longterm Memory for Autonomous Agents.\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nkingjulio8238 / memary Public\n\n  * Notifications\n  * Fork 14\n  * Star 221\n\nLongterm Memory for Autonomous Agents.\n\n### License\n\nMIT license\n\n221 stars 14 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# kingjulio8238/memary\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n7 Branches\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nkingjulio8238Update README.mdApr 28, 202413efee5 \u00b7 Apr 28, 2024Apr 28, 2024\n\n## History\n\n228 Commits  \n  \n### dev\n\n|\n\n### dev\n\n| add routing agent fr| Apr 26, 2024  \n  \n### diagrams\n\n|\n\n### diagrams\n\n| docs: update memary_logo| Apr 26, 2024  \n  \n### src\n\n|\n\n### src\n\n| fix: formatter| Apr 26, 2024  \n  \n### streamlit_app\n\n|\n\n### streamlit_app\n\n| fix: delete json| Apr 26, 2024  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| feat: finish custom entity extraction| Mar 19, 2024  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| init: LICENSE| Apr 25, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update README.md| Apr 28, 2024  \n  \n### requirements.txt\n\n|\n\n### requirements.txt\n\n| fix:: reorganization| Apr 25, 2024  \n  \n## Repository files navigation\n\n# memary: Open-Source Longterm Memory for Autonomous Agents\n\nmemary demo\n\n## Why use memary?\n\nAgents use LLMs that are currently constrained to finite context windows.\nmemary overcomes this limitation by allowing your agents to store a large\ncorpus of information in knowledge graphs, infer user knowledge through our\nmemory modules, and only retrieve relevant information for meaningful\nresponses.\n\n## Features\n\n  * Routing Agent: Leverage a ReAct agent to route a query for execution amongst many tools.\n  * Knowledge Graph Creation & Retrieval: Leverage Neo4j to create knowledge graphs storing agent responses for later retrieval.\n  * Memory Stream: Track all entities stored in the knowledge graph using entity extraction. This stream reflects the user's breadth of knowledge.\n  * Entity Knowledge Store: Group and order all the entities in the memory stream and pass the top N entities into the context window. This knowledge store reflects the user's depth of knowledge.\n\n## How it works\n\nThe current structure of memary is detailed in the diagram below.\n\nThe above process includes the routing agent, knoweldge graph and memory\nmodule are all integrated into the ChatAgent class located in the src/agent\ndirectory.\n\nRaw source code for these components can also be found in their respective\ndirectories including benchmarks, notebooks, and updates.\n\n## Installation\n\n  1. Create your virtual environment and activate it.\n\n  2. Install Python dependencies:\n    \n        pip install -r requirements.txt\n\n## Demo\n\nTo run the Streamlit app:\n\n  1. Ensure that a .env exists with necessary API keys and Neo4j credentials.\n\n    \n    \n    OPENAI_API_KEY=\"YOUR_API_KEY\" NEO4J_PW=\"YOUR_NEO4J_PW\" NEO4J_URL=\"YOUR_NEO4J_URL\" PERPLEXITY_API_KEY=\"YOUR_API_KEY\" GOOGLEMAPS_API_KEY=\"YOUR_API_KEY\"\n\n  2. Run:\n    \n        streamlit run streamlit_app/app.py\n\n## Detailed Component Breakdown\n\n### Routing Agent\n\n  * Uses the ReAct agent to plan and execute a query given the tools provided. This type of agent can reason over which of the tools to use next to further the response, feed inputs into the selected tool, and repeat the process with the output until it determines that the answer is satisfactory.\n  * Current tool suite: While we didn't emphasize equipping the agent with many tools, we hope to see memary help agents in the community equipped with a vast array of tools covering multi-modalities.\n\n    * Location - determines the user's current location and nearby surroundings using geocoder and googlemaps.\n    * CV - answers a query based on a provided image using gpt-4-vision-preview.\n    * Search - queries the knowledge graph for a response based on existing nodes and executes an external search if no related entities exist.\n  * How does it work?\n\n    * Takes in each query \u2192 selects a tool \u2192 executes and finds an answer to current step \u2192 repeats this process until it reaches a satisfactory answer.\n  * Purpose in larger system\n\n    * Each response from the agent is saved in the knowledge graph. You can view responses from various tools as distinct elements that contribute to the user's knowledge.\n  * Future contributions\n\n    * Make your own agent and add as many tools as possible! Each tool expands the agent's ability to answer a wide variety of queries.\n    * Create an LLM Judge that scores the routing agent and provides feedback.\n    * Integrate multiprocessing so that the agent can process multiple sub-queries simultaneously. We have open-sourced the query decomposition and reranking code to help with this!\n\n### Knowledge Graph\n\n  * What are knowledge graphs (KG)?\n\n    * KGs are databases that store information in the form of entities, which can be anything from objects to more abstract concepts and their relationships with one another.\n  * KGs vs other knowledge stores\n\n    * KGs provide more depth of essential context that can be easily retrieved.\n    * The knowledge store's graph structure allows information to be centered around certain entities and their relationships with other entities, thus ensuring that the context of the information is relevant.\n    * KGs are more adept at handling complex queries, as the varying relationships between different entities in the query can provide insight into how to join multiple subgraphs together.\n  * Knowledge graphs \u2194 LLMs\n\n    * memary uses a Neo4j graph database to store knoweldge.\n    * Llamaindex was used to add nodes into the graph store based on documents.\n    * Perplexity (mistral-7b-instruct model) was used for external queries.\n  * What can one do with the KG?\n\n    * Inject the final agent responses into existing KGs.\n    * memary uses a recursive retrieval approach to search the KG, which involves determining what the key entities are in the query, building a subgraph of those entities with a maximum depth of 2 away, and finally using that subgraph to build up the context.\n    * When faced with multiple key entities in a query, memary uses multi-hop reasoning to join multiple subgraphs into a larger subgraph to search through.\n    * These techniques reduce latency compared to searching the entire knowledge graph at once.\n  * Purpose in larger system\n\n    * Continuously update the memory module with each node insertion.\n  * Future contributions\n\n    * Expand the graph\u2019s capabilities to support multiple modalities, i.e., images.\n    * Graph optimizations to reduce latency of search times.\n\n### Memory Module\n\n  * What is the memory module?\n\nThe memory module comprises the Memory Stream and Entity Knowledge Store. The\nmemory module was influenced by the design of K-LaMP proposed by Microsoft\nResearch.\n\n  1. The Memory Stream captures all entities inserted into the KG and their associated timestamps. This stream reflects the breadth of the users' knowledge, i.e., concepts users have had exposure to but no depth of exposure is inferred.\n\n     * Timeline Analysis: Map out a timeline of interactions, highlighting moments of high engagement or shifts in topic focus. This helps in understanding the evolution of the user's interests over time.\n     * Extract Themes: Look for recurring themes or topics within the interactions. This thematic analysis can help anticipate user interests or questions even before they are explicitly stated.\n  2. The Entity Knowledge Store tracks the frequency and recency of references to each entity stored in the memory stream. This knowledge store reflects users' depth of knowledge, i.e., concepts they are more familiar with than others.\n\n     * Rank Entities by Relevance: Use both frequency and recency to rank entities. An entity frequently mentioned (high count) and referenced recently is likely of high importance, and the user is well aware of this concept.\n     * Categorize Entities: Group entities into categories based on their nature or the context in which they're mentioned (e.g., technical terms, personal interests). This categorization aids in quickly accessing relevant information tailored to the user's inquiries.\n     * Highlight Changes Over Time: Identify any significant changes in the entities' ranking or categorization over time. A shift in the most frequently mentioned entities could indicate a change in the user's interests or knowledge.\n     * Additional information on the memory modules can be found here\n\n  * Purpose in larger system\n\n    * Compress/summarize the top N ranked entities in the entity knowledge store and pass to the LLM\u2019s finite context window alongside the agent's response and chat history for inference.\n    * Personalize Responses: Use the key categorized entities and themes associated with the user to tailor agent responses more closely to the user's current interests and knowledge level/expertise.\n    * Anticipate Needs: Leverage trends and shifts identified in the summaries to anticipate users' future questions or needs.\n  * Future contributions\n\n    * We currently extract the top N entities from the entitiy knowledge store and pass these entities into the context window for inference. memary can future benefit from more advanced memory compression techniques such as passing only entities that are in the agent's response to the context window. We look forward to related community contributions.\n\n## Future Integrations\n\nCurrently memary is structured so that the ReAct agent can only process one\nquery at a time. We hope to see multiprocessing integrated so that the agent\ncan process many subqueries simultaneously. We expect this to improve the\nrelevancy and accuracy of responses. The source code for both decomposing the\nquery and reranking the many agent responses has been provided, and once\nmultiprocessing has been added to the system, these components can easily be\nintegrated into the main ChatAgent class. The diagram below shows how the\nnewly integrated system would work.\n\n### Query Decomposition\n\n  * What is query decomposition?\n\n    * A preprocessing technique that breaks down complex queries into simpler queries to expedite the LLM\u2019s ability to answer the prompt. It is important to note that this process leaves simple queries unchanged.\n  * Why decompose?\n\n    * User queries are complex and multifaceted, and base-model LLMs are often unable to fully understand all aspects of the query in order to create a succinct and accurate response.\n    * Allows an LLM of similar capabilities to answer easier questions and synthesize those answers to provide an improved response.\n  * How it works\n\n    * Initially, a LlamaIndex fine-tuned query engine approach was taken. However, the LangChain query engine was found to be faster and easier to use. LangChain\u2019s PydanticToolsParser framework was used. The query_engine_with_examples has been given 87 pre-decomposed queries (complex query + set of subqueries) to determine a pattern. Users can invoke the engine with individual queries or collate them into a list and invoke them by batch.\n\n    * Individual Invocation: sub_qs = query_analyzer_with_examples.invoke( {\"question\": \"What is 2 + 2? Why is it not 3?\"} )\n\n    * Batch Invocation: questions = [ \"Where can I buy a Macbook Pro with an M3 chip? What is the difference to the M2 chip? How much more expensive is the M3?\", \"How can I buy tickets to the upcoming NBA game? What is the price of lower bowl seats versus nosebleeds? What is the view like at either seat?\", \"Between a Macbook and a Windows machine, which is better for systems engineering? Which chips are most ideal? What is the price difference between the two?\",]\n\nresponses = [] for question in questions:\nresponses.append(query_analyzer_with_examples.invoke({\"question\": question}))\n\n  * Purpose in larger system\n\n    * In a parallel system, the agent will be able to parse multiple queries at once. The query decomposer (QD) will pass all subqueries (or original query if no subqueries exist) to the agent at once.\n    * Simultaneously, QD will pass the original query to the reranking module to rerank the agent responses based on their relevance to the pre-decomposed query.\n  * Future contributions\n\n    * Once agent multiprocessing is integrated, QD will be valuable to leverage. All user queries will be passed to QD, and the (sub)queries wil be passed to the routing agent for parallel processing.\n    * Self-Learning: Whenever queries are decomposed, those examples will be appended to the engine\u2019s example store as a feedback loop for improved future performance.\n\n### Reranking\n\n  * What is reranking?\n\n    * Reranking is the process of scoring nodes based on their relevancy.\n  * Why rerank agent responses?\n\n    * Ensure that the various responses to subqueries, when merged, are relevant to the original query prior to decomposition.\n  * Our Approach\n\n    * We benchmarked three models to determine which one would best work for reranking: BM25 Reranking Fusion, Cohere Rerank, and ColBERT Rerank. After testing BM25, it was clear that the model was not able to classify the different responses and provide a simple merged answer. Instead of answering the question, it combined all the information on the page, introducing irrelevant information.\n    * Next, when testing out Cohere, the model performed better than BM25 but was still not classifying the paragraphs well. The reranking was not always accurate, as it performed well for some questions but was not able to rank others. Furthermore, the ranking was still pretty inaccurate, performing between 0.25 - 0.5 out of 1.\n    * Finally, we tested ColBERT rerank, and it was found that this model performed best compared to the other two. ColBERT was able to synthesize results from the given data and ranked them very accurately, with reranking scores between 0.6 - 0.7 out of 1. With this, ColBERT had the most potential, being able to determine which responses were most related and important to answering the query.\n  * Purpose in larger system\n\n    * Passes the reranking result to the knowledge graph for storage and to the model as one source of context for inference.\n  * Future contributions\n\n    * Once agent multiprocessing is integrated, reranking can be integrated into the ChatAgent class.\n    * Future Benchmarking: Include the Cohere Rerank 3 model and others in the reranking analysis. The data used for benchmarking can be found here. Add to it!\n\n## Contributing\n\nWe welcome contributions from the community and hope to see memary advance as\nagents do!\n\nInitial Contributors: Julian Saks, Kevin Li, Seyeong Han, Arnav Chopra,\nAishwarya Balaji, Anshu Siripurapu (Hook 'em!)\n\n## About\n\nLongterm Memory for Autonomous Agents.\n\n### Topics\n\nmemory knowledge-graph agents retrieval-augmented-generation\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\nActivity\n\n### Stars\n\n221 stars\n\n### Watchers\n\n3 watching\n\n### Forks\n\n14 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Contributors 4\n\n  * seyeong-han seyeong-han\n  * kevinl424 Kevin Li\n  * kingjulio8238\n  * arnavchopra1864\n\n## Languages\n\n  * Jupyter Notebook 66.6%\n  * Python 26.2%\n  * HTML 4.9%\n  * TypeScript 1.7%\n  * Other 0.6%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": true}
