{"aid": "40141498", "title": "Using object tracking to combat flickering detections in videos", "url": "https://www.celantur.com/blog/object-tracking-in-videos/", "domain": "celantur.com", "votes": 1, "user": "apetkov_at", "posted_at": "2024-04-24 07:04:12", "comments": 0, "source_title": "Using object tracking to combat flickering detections in videos", "source_text": "Using object tracking to combat flickering detections in videos\n\nBlur images with Celantur Cloud API\n\n# Using object tracking to combat flickering detections in videos\n\nHow to decrease the amount of flickering detections in videos with object\ntracking.\n\n18 April 2024, by Georgii KrikunAsk a question\n\n## Introduction\n\nContemporary machine learning models display impressive precision levels,\noften rivaling even human perception. Yet, achieving such precision often\nnecessitates trade-offs. The higher the precision, the longer the model's\ninference time and the greater its demand on video memory. These factors can\npose significant challenges, particularly when deploying solutions in the\ncloud. Some can require up to 24 GB of video memory, pushing the limits of\neven high-end GPUs like the RTX-4090, priced at 1800 Euro. Deploying software\non such costly hardware inevitably leads to soaring operational expenses.\nCompounding the issue, larger models tend to exhibit diminishing performance,\nnecessitating additional running instances. Consequently, deploying large\nmodels becomes financially prohibitive.\n\nThis trend prompts developers to turn to swift and small models. At Celantur,\nwe are trying to use models that don't take more than 5 GB of video memory.\nHowever, our clients entrust us with ensuring the meticulous anonymization of\npersonal information, demanding exceptionally high recall rates. In other\nwords, they expect minimal amount of false negatives (undetected objects).\nNaturally, one can't use small models and hope for exceptionally high recall\nrates and this leads to a huge problem.\n\n## The problem\n\nConsider a scenario where we have a model that successfully detects an object\n999 times out of 1000. Let's apply this model to anonymize dashcam footage of\na trailing car with a frame rate of 60 frames per second (fps). On average,\nevery ~16 seconds of video, we may miss capturing the license plate of the\nleading vehicle. Regardless of the efficacy of our anonymization efforts, the\nsingle missed frame renders the work done in the preceding 999 frames\nineffective. With a precision of 99.99%, a similar situation arises only after\napproximately ~3 minutes.\n\nIn essence, relying solely on machine learning models for anonymization proves\ninsufficient. Even with high precision, the intermittent occurrence of missed\ndetections poses a significant challenge. Without additional algorithms to\naddress these critical yet brief lapses in continuous object detection,\nachieving robust anonymization remains elusive, unless clients are willing to\ninvest substantial sums for each video processed.\n\n## Tracking algorithms\n\nTo bridge these gaps, we'll employ BOTSORT, a multiple object tracking\nalgorithm. Typically, such algorithms assign unique, persistent IDs to\nphysically detected objects across consecutive frames of a video. Although the\ninner workings of these algorithms can be intricate, their workflow can be\ndistilled into several key steps:\n\n  1. Leveraging information from previous algorithmic steps, the tracker anticipates the locations of previously detected objects in the upcoming frame.\n  2. The algorithm takes the bounding boxes detected by the machine learning model in the new frame as input.\n  3. Utilizing a re-identification routine, the algorithm matches the detected bounding boxes with the anticipated predictions from previous steps.\n\n    1. Bounding boxes that were predicted but not successfully re-identified are marked as \"lost.\" The tracking algorithm retains these lost detections for several subsequent frames, aiming to maintain tracking even if the object is temporarily obstructed.\n    2. Bounding boxes that were predicted and successfully re-identified in the new frame have their IDs set consistently with those from previous frames.\n    3. Bounding boxes that were not predicted but appear in the new frame initialize new tracks, from which detection predictions will emerge in subsequent steps.\n\nWhile these steps lead to the consistent assignment of unique IDs to\ndetections, they alone do not fully address the flickering issue. However,\nstep 3.1. proves crucial in this regard by tracking lost detections for\nseveral additional frames. Extracting this data presents a significant\nopportunity: it enables us to predict the location of bounding boxes even when\nthe machine learning model fails. Consequently, this capability dramatically\nreduces the flickering of bounding boxes, enhancing the overall robustness of\nthe tracking system.\n\n## Implementation using the BOTSORT algorithm\n\nTo avoid implementing the algorithm manually, we found already established one\non the GitHub. Integration of this algorithm into our workflow has been\nrelatively seamless, although we encountered a compatibility issue with the\ndetection class used in our Celantur software. As a workaround, we opted to\ninherit from this class, allowing us to tailor certain behaviors to align with\nour requirements:\n\n    \n    \n    import numpy as np from BoT-SORT.tracker.bot_sort import BoTSORT from celantur.detection import Detection as CelanturDetection class BOTSORTWrapper(BoTSORT): class mock_results: ''' This class is used to pass the detections into the BoTSORT.update() method. From the list of detections, one needs to fill out the necessary fields of this class: * Bounding boxes list * Confidence score list of corresponding bounding box * Class/Type list of corresponding bounding boxes ''' def __init__(self, boxes, scores, cls): self.xyxy = [] for b in boxes: self.xyxy.append(list(b)) self.xyxy = np.array(self.xyxy, dtype=np.float32) self.conf = np.array(scores, dtype=np.float32) self.cls = np.array(cls, dtype=np.int32) def __init__(self, args, frame_rate=30): super().__init__(args, frame_rate) def update(self, detections: List[CelanturDetection], img: np.array): ''' This function constructs the mock results object from the list of object detections. ''' boxes = [d.box for d in detections] scores = [d.score for d in detections] classes = [d.type for d in detections] results = BOTSORTWrapper.mock_results(boxes, scores, classes) return super().update(results, img)\n\nThis BOTSORT implementation will create tracks from the detections and will\nkeep them for the next 30 frames. The lost tracks and currently tracked tracks\ncan be easily accessed:\n\n    \n    \n    tracker = BOTSORTWrapper(args) lost_tracks = tracker.lost_stracks tracked_tracks = tracker.tracked_stracks\n\nOne can use any logic he sees fit to blur/anonymise the tracks that are in the\nlost_tracks list. In our case, we just converted them to the CelanturDetection\nobjects and passed them to the next step of the anonymisation pipeline. The\nvideo used in the example is a free-to-download stock video from vecteezy.com.\nAnd the results are already promissing.\n\n! Please note that the processing used to process the example is not\nrepresentative of Celantur software precision. We had to artificially lower\ndetection precision to have a good example of flickering/disappearing\ndetections.\n\nIndeed, upon closer inspection, it's evident that in the case of tracking, the\nbounding box on the black car on the left remains persistent across frames,\nwhereas in the absence of tracking, the bounding box disappears after the\nfirst frame. However, a comprehensive review of the entire video reveals the\nmyriad issues introduced by this approach. The most significant challenge\narises when the object tracker fails to re-identify an object, resulting in\nthe creation of a new track. This behavior can lead to fragmented and\ninaccurate tracking, undermining the effectiveness of the overall system.\n\nAnalyzing these three frames, it's apparent that there are numerous persistent\nbounding boxes that are absent in the no-tracking scenario. These bounding\nboxes were initially detected but subsequently lost, yet they linger for\napproximately 30 frames (ca 1 second) afterward. In some instances, such as\nwith the car on the left, these bounding boxes stem from correctly identified\ndetections. However, in other cases, like the car on the right, the bounding\nboxes represent single false positives that persist for 30 frames. In essence,\nwhile the issue of flickering has been mitigated, a significant problem of\nfalse positives has been introduced. This phenomenon can lead to inaccuracies\nand inconsistencies in object tracking, posing a notable challenge for the\noverall system.\n\n### Fine tuning the results to get the best of both worlds\n\nTo enhance the accuracy of the results, we need to refine the algorithm\nresponsible for determining whether to create a CelanturDetection from a lost\ntrack. While maintaining the ability to track temporarily obscured objects for\n30 frames, we aim to convert them into detections only if the following\nconditions are met:\n\n  1. The track was lost less than 5 frames ago\n  2. The track was detected at least 2 times\n\nThe first condition arises directly from our current problem. If an object\nremains undetected for several consecutive frames, it signifies a need for\nimprovement in our object detection model. While it may seem ideal to\nsubstitute costly machine learning models with a more affordable tracking\nalgorithm, this substitution isn't feasible.\n\nThe second condition is grounded in our business logic. Since we offer an\nanonymization service, we must balance the occurrence of false positives and\nfalse negatives. False negatives pose a greater risk as they can result in the\ninadvertent exposure of personal data. On the other hand, false positives\nprimarily impact the aesthetics of the anonymized video and aren't as\ncritical. Consequently, we prioritize minimizing false negatives. Given our\nexpectation of encountering false positives, it's impractical to carry them\nforward for an extended period. To address this, we can implement a specific\ndata structure that accompanies each track.\n\n    \n    \n    class TrackStatus(): tracked: bool = True # By default tracks are added when detected n_det: int = 0 # Number of detections of this track in frames n_lost = 0 # How many frames has passed since last detection ... tracker_cache_update: Dict[int, TrackStatus] = {} # The dictionary to store track status for each track id\n\nAnd then, during update of BOTSORT, we change these parameters\ncorrespondingly:\n\n    \n    \n    tracker.update(detections, image.data) lost_tracks = tracker.lost_stracks tracked_tracks = tracker.tracked_stracks for track in tracked_tracks: track_id = track.track_id if track_id not in tracker_cache_update: track_status = TrackStatus() tracker_cache_update[track_id] = track_status tracker_cache_update[track_id].tracked = True tracker_cache_update[track_id].n_det += 1 tracker_cache_update[track_id].n_lost = 0 for track in lost_tracks: track_id = track.track_id tracker_cache_update[track_id].tracked = False tracker_cache_update[track_id].n_lost += 1\n\nFinally, based on these parameters, we make a decision whether we want to\ninitialize a detection from the lost track using following logic:\n\n    \n    \n    track_status = tracker_cache_update[track_id] is_detected_now = track_status.tracked was_detected_often_enough = track_status.n_det >= TOTAL_DETECTION_THRESHOLD # Equals to 2 in our case was_detected_recently = track_status.n_lost <= LAST_DETECTION_THRESHOLD # Equals to 5 in our case should_initialize_detection = is_detected_now or (was_detected_often_enough and was_detected_recently) if should_initialize_detection: # Initialize the detection here\n\nThe results of the fine-tuned tracking algorithm are quite promising. The\nflickering is almost gone, the false positives are still present, but they are\ncontained in the area where the object was initially detected:\n\nFor the end user, who isn't presented with detection boxes, the false positive\noccurrence on the left side of the image isn't particularly noticeable. When\nblurred, it simply appears as a slightly larger area being obscured than\nnecessary. Further improvements are feasible from this point onward, but they\nnecessitate extensive fine-tuning, testing, and enhancement of the machine\nlearning model. The crucial consideration is to always balance between false\npositives and false negatives and adapt the algorithm accordingly.\n\n### Unsolved issues\n\nThe primary issue we encountered in our implementation pertained to scenarios\nwhere the object exhibited acceleration in its movement relative to the\ncamera, measured in pixels per second. If you recall high school physics, you\nmight recognize the equations governing such motion, such as x(t) = x_0 +\nv_0*t + 0.5*a*t^2. Our object tracking algorithm operates under the assumption\nof constant velocity, predicting the object's next location as x(t+1) = x(t) +\nv*t, where v is the velocity of the object. However, when the object\naccelerates, this prediction becomes inaccurate.\n\nThis discrepancy arises due to the linear Kalman filter utilized within the\nBOTSORT algorithm. To address this issue, we could employ a non-linear Kalman\nfilter capable of predicting object movement with acceleration. However,\nopting for the non-linear Kalman filter might come with a higher computational\ncost, rendering it unsuitable for use within the BOTSORT algorithm. This\napprach needs yet to be tested.\n\nThe problem is not too common, but it can appear in the following situations:\n\n  1. Physical acceleration of the object in physical space: This occurs when an object experiences acceleration in the real world. For instance, a car accelerating rapidly after a green light at an intersection or making a sharp turn, subjecting the object to physical acceleration.\n  2. Acceleration of the object in image space: In this case, the object undergoes acceleration within the image itself. For example, when an object moves closer to the camera, it appears larger in the frame, causing the bounding box to shift more pixels with each frame. This situation often arises in dashcam videos capturing traffic moving in the opposite direction of the vehicle recording the video.\n\nIn each of these cases, if the acceleration occurs rapidly enough, the tracker\nmay lose track of the object, resulting in multiple false positives. For\ninstance, consider the image below, where three bounding boxes appear behind\nthe yellow car as it makes a right turn (first case scenario, where the object\nexperiences physical acceleration). And, two bounding boxes are visible near\nthe head of the motorcyclist (second case scenario, where the object remains\nstationary but moves relative to the dashcam car with acceleration):\n\n## Conclusion\n\nUsing object tracking based on BOTSORT, we are able to overcome false\nnegatives of the same real-world object that \"slip through\" our detection\nmodel in some frames of a video. It enables us to provide superior\nanonymisation in videos while mantaining high performance.\n\nAsk us Anything. We'll get back to you shortly\n\ntrackingMOTBOTSORTprecisionmachine learningflickeringfalse\nnegativealgorithmenglishStart Demo Contact Us\n\n### Latest Blog Posts\n\nView all\n\n### Using object tracking to combat flickering detections in videos\n\nHow to decrease the amount of flickering detections in videos with object\ntracking.\n\n18 April 2024\n\n### How to copy XMP metadata between JPEG images (again)\n\nCopying XMP metadata between images isn't straightforward. Read how it's done\ncorrectly.\n\n05 March 2024\n\n### 20x Faster Than NumPy: Mean & Std for uint8 Arrays\n\nHow to calculate mean and standard deviation 20 times faster than NumPy for\nuint8 arrays.\n\n02 January 2024\n\n###### Products\n\n  * Cloud Service\n  * Web API\n  * Container\n  * Showcase\n  * Reselling\n\n###### Solutions\n\n  * Mobile Mapping\n  * Indoor Mapping\n  * Laser Scanning\n  * Automotive & ADAS\n\n###### Resources\n\n  * Blog\n  * Documentation\n  * Success Stories\n\n###### About\n\n  * Team\n  * Jobs\n  * Customers\n  * Partners\n  * Trust & Compliance\n  * Technology\n  * Press\n\n###### Legal\n\n  * Terms of Service\n  * Imprint\n  * Privacy Policy\n\n###### Follow us\n\nBuilt in \ud83c\udde6\ud83c\uddf9 with \u2764, \ud83c\udf7b and \u2615 | \u00a9 2019-2024 Celantur GmbH. All Rights Reserved.\n\n", "frontpage": false}
