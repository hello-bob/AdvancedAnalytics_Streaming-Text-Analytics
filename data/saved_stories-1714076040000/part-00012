{"aid": "40157079", "title": "The Assist", "url": "https://qntm.org/assist", "domain": "qntm.org", "votes": 1, "user": "genericlemon24", "posted_at": "2024-04-25 13:10:07", "comments": 0, "source_title": "The Assist @ Things Of Interest", "source_text": "The Assist @ Things Of Interest\n\nThings Of Interest Blog\n\n## The Assist\n\n2024-04-05 by qntm\n\nAt work we recently started experimenting with generative AI for assistance\nwith programming. We have a new Visual Studio Code plugin which we can ask\nquestions in English, and it spits back code. It was a really interesting\npiece of, well, mandatory training. I've formed some opinions.\n\nThe main thing I dislike about AI coding assistance is that I have to very\ncarefully review the AI's code to make sure that it does the right thing and\nnone of the wrong things. And I, personally, find that reviewing code is more\ndifficult than writing equivalent code myself.\n\nI don't know if anybody else has this experience, but I understand code better\nwhen I interact with it directly by hand. Either by bringing code into\nexistence, or by refactoring code which was already there to do something new,\nor to do the same thing more effectively. I need to take things apart and put\nthem back together. I need to play with the code.\n\nPassively reading someone else's code in the dead form of a git diff just\ndoesn't engage my brain in the same way. It takes more focus, it's much easier\nto overlook vital details, there's no flow state, and it's much less\ninteresting. Of course, I do it, all the time, and I have no problem with it.\nHalf of the job is code review, and unblocking my fellow developers is always\na high priority. But I know which half of the job I enjoy more. (There are\nseveral other \"halves\" of any development job, but let's not confuse things.)\n\nIt also takes more time. I can write a simple switch/case statement or an\nexplanatory docstring for a middle-sized method more quickly than I can get an\nAI to do this for me and then manually verify its correctness to an equivalent\nlevel of confidence. This applies even if the AI writing time is zero.\n\n#### *\n\nFor example. I have no idea how to launch a HTTP request from C++. The\nassistant I tried out was able to provide apparently working code for this,\nshortcutting my need to spend hours familiarising myself with new syntax and\nAPIs. It compiled and ran and did the thing.\n\nHowever! Without a priori knowledge of C++ or the HTTP library in question,\nhow am I supposed to know that the AI hasn't made some tragic blunder with its\nseemingly working implementation? Or misused the APIs in question? I don't\neven know what classes of problems to look for. So I'm back to checking the\nAI's work against the documentation. And the value the AI has provided to me \u2014\nor whatever experienced C++ developer I inflict this PR on \u2014 is not zero, but\nit's not the giant leap head start it looks like at face value.\n\nA similar example would be if I said, \"Hey computer, this project uses webpack\nas its bundler. I want you to convert it to use esbuild instead.\" Even if the\nAI appears to do the thing, I'm still going to have to wade through some\nesbuild documentation to make sure it did it right, right?\n\nAdditionally, I find that describing my requirements to the AI in technical\ndetail is often (not always) basically equivalent in complexity to just\nwriting the code. And takes a similar amount of time to writing the code.\n\nCode is actually really efficient, relative to human language. It's extremely\ndense, expressive and specific. Compare the amount of time taken to write out:\n\n> For each element in the field array, we need to emit a debug log entry\n> explaining what element we're working on. Then, examine the type property on\n> the element. If it's \"bool\", then we want to add the element to the array of\n> Boolean fields, otherwise, add it to the array of non-Boolean fields. In\n> either case, emit a debug log entry saying what we did.\n\nversus:\n\n    \n    \n    for (const field of fieldArray) { logger.debug('processing field', field) if (field.type === 'bool') { booleanFields.push(field) logger.debug('field is Boolean') } else { nonBooleanFields.push(field) logger.debug('field is not Boolean') } }\n\nThe amount of typing required for each of these is comparable. The amount of\ntime taken to properly phrase the English prompt is comparable with the amount\nof time taken to just write the code directly. The phrasing will need altering\na few times to get workable output. And the amount of refactoring of the\ngenerated code to create something equivalent to the handwritten code is\nalways non-zero. So, have we gained anything?\n\nOr imagine we have an existing piece of code generated, but the AI has missed\nsomething. Compare the effort of messaging it to say:\n\n> If the first argument isn't a number, the function should throw an exception\n> saying \"not a number\".\n\nand having it regenerate its work, versus just plumbing in:\n\n    \n    \n    if (typeof a !== 'number') { throw Error('not a number') }\n\nThe English is fifty percent longer. It's also far more ambiguous. What about\nNaN, BigInts, numeric strings, boxed Number objects? The code is shorter, and\nabundantly clear on what should happen in all of those cases.\n\nOn top of all of that, I think I would have less of a problem with manually\nreviewing AI-generated code if I felt that this was actually helping someone\nget better at coding. I know that feeding positive and negative votes back to\nthe AI will influence its internal logic and ultimately improve the quality of\nits output. Good for it. But, to borrow a phrase and twist it a bit, I value\nindividuals and interactions over processes and tools. That is, I'm more\npersonally invested in my fellow developers' professional development than I\nam in training any machine. As much of a chore as it can be, truthfully, I do\nenjoy reviewing my teammates' code because it's an opportunity to share good\npractices, and make good decisions, and, as months pass, watch them develop.\n\n#### *\n\nTo back up a little: Humans are extremely bad at software development and we\nneed all the help we can get.\n\nI'm hugely in favour of human processes which make this better. Peer review,\nbest practices, rules of thumb, comments and documentation, forms and\nchecklists. I'm also hugely in favour of automated tools which augment the\nhuman process. Linters, strong type systems, fuzzers, mutation testing. But\nthis shouldn't be an either/or thing. I want all the human checks and all the\ntechnological checks.\n\nAnd as it's currently positioned, AI coding assistance seems like an addition,\nbut it actually removes the human from a part of the process where the human\nwas actually extremely valuable. When a human writes code, we have the\noriginal programmer to vouch for their code, plus another person to review and\ndouble-check their work. With AI assistance, we don't get that first\n\"guarantee\". Nobody with the vital domain knowledge which comes from\ninhabiting physical reality, or who was present at the meetings where we\ndiscussed these new features, participated in the creation of the code. We\nbasically just have two peer reviews.\n\nEssentially, AI assistance replaces coding with code review.\n\nWhich is bad, for all the reasons I mentioned, but also, and mainly, because I\nlove coding!\n\n(In the same way, self-driving cars replace the experience of driving with the\nexperience of being a driving instructor. A lot of the same objections apply\nhere. Most of us who have some experience with driving are capable of acting\nas a driving instructor, but it is more difficult and stressful than driving,\nrequiring greater vigilance. It's a different mental process from driving and\nit's much less enjoyable than driving.)\n\nSo I feel that the better place for the AI assistant is in a supporting role\nlike: \"Hey, AI. I have written this code. Do you see anything wrong with it?\nCould it be improved? Have I missed an obvious simplification? Do my unit\ntests seem to be exhaustive?\"\n\nThis also makes it much simpler for us to evaluate the AI's contributions, in\na relatively objective fashion. Does it have anything useful to say? Is it\ngenerally worth listening to or is it wasting our time?\n\nI think this approach generalises pretty well to other fields where AI is\nbeing applied. Not to do work and then have a human review it, but to review\nhuman work. \"Do you agree? What have I missed?\"\n\nAnd if the AI is all false results, well... I will disable a tool which\ndoesn't add value. I've done it before.\n\nBack to Blog\n\nBack to Things Of Interest\n\n### Discussion (21)\n\n#### 2024-04-05 02:03:57 by qntm:\n\nBased on a Twitter thread from a few days ago.\n\n#### 2024-04-05 02:18:26 by osmarks:\n\nI think your arguments about the density of code are missing the point.\nEmpirically, LLMs find it easier to predict than English (by perplexity). Your\nexamples correctly indicate that exhaustively describing the behaviour of code\nin English is longer than the code, but in most cases it's pretty obvious what\nit should be from context - I'd expect that saying \"check first argument is\nnumber\" or \"filter and log boolean fields\" would get you something close\nenough. I do kind of agree with you in general, mostly because I think most of\nthe code assistant tools are trying to be like a coworker you might delegate\nto, but are less competent, which is annoying and unpleasant. A good one, in\nmy view, would try and act more like an extension of my thoughts, filling in\nblanks and correcting silly mistakes and making tweaks to background files to\nalign them with what I'm currently doing.\n\n#### 2024-04-05 03:13:06 by Emily:\n\nSeveral of my coworkers use this thing and like it. I gave up half an hour\ninto trying it out. I now find myself wondering if the underlying thing here\nis that those people like reviewing code more than you and I do. There is also\nAI code review. Yes, that's a thing. Yes, it's as bad as you expect, because\nit's severely lacking in context one needs to effectively review code.\n(Including the obvious \"you have no idea what this function does and therefore\ncannot say whether I'm using it wrong\", but also shockingly often, *knowing\nwhat language the code is even written in*. One major category of bad\nsuggestions were \"you should check for null/wrong-typed values here!\" in a\nlanguage with a static type system that models nullability.) Even my very AI-\nfriendly company decided not to continue after our trial run with it.\n\n#### 2024-04-05 03:29:15 by ebenezer:\n\nDocs person here. The current project at work is to add documentation for\nfeature XYZ, using a page of content provided by a developer. My approach this\ntime is to read through and attempt to grasp the content (and understand the\nfeature) myself, then write the documentation myself. For some reason your\ncomments about writing vs. reviewing code reminded me of this.\n\n#### 2024-04-05 06:12:34 by Tetragramm:\n\nI much prefer the auto complete version. It only does something when I'm\nwriting code, and it follows my tries) reason of thought. So I am already\nthinking about what comes next, and it requires approximately zero effort to\nthink \"that's right\" and accept the suggestion. Or deny it and just keep\ntyping. It's especially useful for things with almost repeated but slightly\ndifferent phrases, like switch statements, and input validation. Using your\nexample, by the time you get half way through typeof, it's typically\nsuggesting the rest, and I'm done.\n\n#### 2024-04-05 07:20:32 by Aybri:\n\nI don't use AI much in my workflow, as I don't particularly trust most\nprojects that use it. I see AI as a useful tool when it comes to mere support\nand assistance, not replacement, and replacement is the direction most\nprojects have been going in. However, my father (who is also a programmer, and\nsomeone I frequently butt heads with about this kind of thing) uses and\nactually appreciates the auto-complete version of the AI. He showed me a bit\nof it and spoke about how it greatly increased his work flow. It didn't seem\ntoo terrible, but I quickly noticed it tends to miss context and make\nmistakes. In this particular instance, it accidentally made a nested loop,\nwhich I had to call out to my father when he begun to get confused as to why\nhis code wasn't working. I suppose it's a useful tool if you like reading code\nbut not writing it. I don't see much use in it myself, but I like to\nexperiment and toy around, metaphorically \"get my hands dirty.\" Again, I don't\nlike being replaced. But that's just my two cents, really.\n\n#### 2024-04-05 11:35:33 by zaratustra:\n\n\"it's pretty obvious what it should be from context\" is where i spend most of\nmy working hours.\n\n#### 2024-04-05 12:45:20 by mavant:\n\nAgree on all points, but especially deeply agree about needing to play with\nthe code - I just can't get myself to really focus and understand a diff\nwithout doing a bit of pushing it around to see why (and if) the new state is\nreally a local optimum.\n\n#### 2024-04-05 12:59:43 by James Fryer:\n\n> For each element in the field array, we need to emit a debug log entry\n> explaining what element we're working on. Then... This isn't how I use AI\n> for coding though. It's pretty much describing the implementation, in\n> English, then checking the generated code. Writing the code is surely faster\n> in this case. I currently only use AI for small data-processing scripts that\n> I'd use once then throw away. In this case my prompt would include input and\n> output samples, describe any other requirements (\"output to stdout\", \"use\n> csv module\") and then let the method take care of itself. This declarative\n> style of programming using AI I think is useful, although I haven't tried it\n> with any complex requirements.\n\n#### 2024-04-05 15:41:50 by Oliver:\n\nI find that \u201cget the AI to review rather than just generate\u201d applies in a\nlarge range of domains. Maybe (with the current rate of AI progress, probably)\nsomeday it will be good enough it will work like a human programmer and we\nwon\u2019t worry so much about checking it, but until then, review is almost always\nbetter than generation.\n\n#### 2024-04-06 15:05:29 by BoppreH:\n\nGood point on human review of AI code being a waste of training. The thought\nnever occurred to me. What do you think of AI generating test cases? Tests are\nusually boilerplate-heavy, conceptually shallow, and have a lower\nsecurity/quality bar. Seems like the perfect job for AI. \"During tests, the\nbranch a==\"-\" is never taken when b==[]. Doing so crashes the function with\nSomeError (see below). Add failing test case to suite?\" \"This function has no\ntests. Select from suggested tests to add to suite: (13 tests, 2 failing, 98%\ncoverage):\"\n\n#### 2024-04-06 22:49:50 by Nathan:\n\nI like AI programming assistants in two cases: Case 1: I have no idea how to\ndo this thing. In this case I assume the code the AI assistant produces is\ngoing to be wrong, but that's OK because my initial stab at it would probably\nalso be wrong. I'm fine iterating toward a solution with the AI's code as a\nstarting point. If I have to do research either way, the AI code narrows the\nscope of my research. Case 2: I know exactly what I want to do but I don't\nreally feel like typing it all out. There's a decent chance that the function\nsignature plus a succinct comment is all the AI assistant needs to write\nsomething that's close enough to the code in my head that I don't need to\nspend any more time proving to myself that it's correct than I would if I had\ntyped it with my own hands. This sort of saving is minimal, but I find it\nactually enhances the flow state because instead of getting bogged down in\nwriting some trivial string parsing code or JSON deserialization or a unit\ntest that confirms the function with input x produces output y, I'm thinking\nat a higher level and letting the AI fill in the details. To each their own,\nof course\n\n#### 2024-04-07 13:45:11 by JP:\n\nI\u2019m with you; I spend a lot longer debugging LLM derived code than the time I\ngain back from its availability. I find it feels \u201cmore useful\u201d when I\u2019m\nfeeling lazy, but in those moods I\u2019m usually more comfortable spending twice\nas long tutting at & correcting its failures than I would have done getting my\nhead into gear and getting a good working model in my head. Context is key I\nthink (mine or the LLM\u2019s lack); if I don\u2019t care about quality (eg. throw-away\nscript in a language or framework I\u2019m less familiar with) I\u2019ll happily let\nLLMs take a stab then work on correcting, minimizing how much context either\nof us needs to get to \u201cgood enough\u201d. I\u2019d be interested to see an LLM with\nhooks that\u2019d let it pull in metrics about the result (test failures, code\nquality metrics) and provide it with more context. Today it\u2019s a lot like\ntrying to wrangle a less experienced engineer who doesn\u2019t care to learn.\n\n#### 2024-04-07 23:35:53 by TJSomething:\n\nMy big thing is that LLMs are pretty decent at extremely repetitive code where\nI already know the next several lines of code exactly. Maybe I'm copying\nvalues out of an ad hoc DTO. Maybe I'm building an object with a lot of fields\nand I have all the variables in scope. Maybe it's a simple algorithm that's so\ncommon that it would probably be in the standard library if the language had a\ndecent standard library and/or rich enough semantics to make a general purpose\nfunction for that use. For all of these kind of cases, LLMs can usually make\npretty good inferences. And this honestly helps a lot with my RSI. Any code\nthat's so obvious that I can type continuously for a minute or two ends up\nputting me in a small but noticable amount of pain.\n\n#### 2024-04-08 00:39:19 by qntm:\n\nI hid a comment by user named \"jakedowns\" which was responding to multiple\npoints I never made in my essay and in general seemed to be hallucinatory/AI-\ngenerated. If you are jakedowns and you want your comment restored, get in\ncontact and we can talk about this. > What do you think of AI generating test\ncases? Test cases are also code, so all the observations regarding code review\nvs. coding proper apply.\n\n#### 2024-04-09 12:41:31 by RobFisher:\n\nWhat disturbs me about this critique is: I like the idea of having AI help me\n(and people in general) to get more done; but what if it does all the fun\nparts? You know who is good at getting things done while trusting the details\nthey don't understand to others? Managers. So far I have avoided becoming a\nmanager. Where AI has helped me: tedious things (write a command line\ninterface for this function). Tasks of a certain size that are easy to\ndescribe in English (I want a Python program that does a regex search and\nreplace on whatever is in the clipboard and works in both Windows and Linux).\nAnd helping me ask better questions about unfamiliar domains (what if I test\nmy data samples by choosing some at random lots of times and seeing how\nvariable the results are? Apparently in statistics this is called\nbootstrapping, and the AI helped me translate statistics speak into something\nI could understand). I remain hopeful.\n\n#### 2024-04-09 22:15:36 by sdfgeoff:\n\nAI turns coders into code reviewers in the same way CNC machines turned\nexperienced machinists into material loaders and quality control supervisors.\nCNC machines used to be unreliable things that maybe saved time. Now they are\nhuge force multipliers. 2 years ago AI code generation was definitely a net\nnegative productivity gain. Now it is approximately a zero productivity gain.\nIn a few years will we have a software equivalent of the \"Ballad of John\nHenry\" where man races the machine to his own peril? Who knows.\n\n#### 2024-04-11 04:56:55 by dmz:\n\nI think you didn't take the analogy with self-driving cars far enough. Level 2\n\"self-driving\" capabilities like in Teslas are nearly useless because they\nstill require constant supervision. But Level 4+ autonomous vehicles like\nWaymos are amazing. When coding assistants reach that level of reliability,\nthey will be great as well.\n\n#### 2024-04-14 11:33:28 by asdf123:\n\nYou're absolutely right\n\n#### 2024-04-14 23:06:43 by mesyaria:\n\nI think with image generation the human being the reviewer is clearly the way\nto go. The model can do the bulk of the work quickly and well but stumbles on\ndetails, which the human operator can take care of. Even that last bit of work\ncan be done in large part by the model with just a bit of inpainting to nudge\nit in the right direction.\n\n#### 2024-04-20 20:57:55 by David:\n\nsdfgeoff: > In a few years will we have a software equivalent of the \"Ballad\nof John Henry\" where man races the machine to his own peril? Who knows.\nThere's a yearly programming competition called Advent of Code, and in day 3\nof 2022, ostwilkens used code completion combined with automatic input\ndownloading and submission to get a humanly impossible 10-second time for part\n1. However, the LLM's first attempt for part 2 had a bug, and the LLM came in\nseven seconds short of a (human) programmer named 5space. I wrote a version of\nthe \"Ballad of John Henry\" to commemorate the occasion, that you might enjoy:\nhttps://hallofdreams.org/posts/advent-of-code-2022/#day-3-john-5space-henry\nWhat's funny is that now, a year and a half later, GPT-4 could probably write\nthe poem to boot.\n\nContact\n\nAbout\n\n\u00a9 qntm\n\n", "frontpage": false}
