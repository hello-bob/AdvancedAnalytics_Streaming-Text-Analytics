{"aid": "40169717", "title": "Coding the Cloud: A Dive into Data Streaming with Gunnar Morling from Decodable", "url": "https://www.simplyblock.io/post/coding-the-cloud-a-dive-into-data-streaming-with-gunnar-morling", "domain": "simplyblock.io", "votes": 2, "user": "panrobo", "posted_at": "2024-04-26 14:14:41", "comments": 0, "source_title": "Coding the Cloud: A Dive into Data Streaming with Gunnar Morling from Decodable", "source_text": "Coding the Cloud: A Dive into Data Streaming with Gunnar Morling from\nDecodable\n\ntop of page\n\nChat with us\n\n#### simplyblock and Kubernetes\n\nSimplyblock provides high-IOPS and low-latency Kubernetes persistent volumes\nfor your demanding database and other stateful workloads.\n\nBook a demo\n\n  * Chris Engelbert\n  *     * 8 hours ago\n    * 16 min read\n\n# Coding the Cloud: A Dive into Data Streaming with Gunnar Morling from\nDecodable\n\nThis interview is part of the Simplyblock's Cloud Commute Podcast, available\non Youtube, Spotify, iTunes/Apple Podcasts, Pandora, Samsung Podcasts, and our\nshow site.\n\nIn this installment of podcast, we're joined by Gunnar Morling (X/Twitter),\nfrom Decodable, a cloud-native stream processing platform that makes it easier\nto build real-time applications and services, highlights the challenges and\nopportunities in stream processing, as well as the evolving trends in database\nand cloud technologies.\n\nChris Engelbert: Hello everyone. Welcome back to the next episode of\nsimplyblock's Cloud Commute podcast. Today I have a really good guest, and a\nreally good friend with me. We know each other for quite a while. I don't\nknow, many, many, many years. Another fellow German. And I guess a lot of, at\nleast when you're in the Java world, you must have heard of him. You must have\nheard him. Gunnar, welcome. Happy to have you.\n\nGunnar Morling: Chris, hello, everybody. Thank you so much, family. Super\nexcited. Yes, I don't know, to be honest, for how long we have known each\nother. Yes, definitely quite a few years, you know, always running into each\nother in the Java community.\n\nChris Engelbert: Right. I think the German Java community is very\nencapsulated. There's a good chance, you know, a good chunk of them.\n\nGunnar Morling: I mean, you would actively have to try and avoid each other, I\nguess, if you really don't want to meet somebody.\n\nChris Engelbert: That is very, very true. So, well, we already heard who you\nare, but maybe you can give a little bit of a deeper introduction of yourself.\n\nGunnar Morling: Sure. So, I'm Gunnar. I work as a software engineer right now\nat a company called Decodable. We are a small startup in the data streaming\nspace, essentially moving and processing your data. And I think we will talk\nmore about what that means. So, that's my current role. And I have, you know,\na bit of a mixed role between engineering and then also doing outreach work,\nlike doing blog posts, podcasts, maybe sometimes, going to conferences,\ntalking about things. So, that's what I'm currently doing. Before that, I've\nbeen for exactly up to the day, exactly for 10 years at Red Hat, where I\nworked on several projects. So, I started working on different projects from\nthe Hibernate umbrella. Yes, it's still a thing. I still like it. So, I was\ndoing that for roughly five years working on Bean Validation. I was the spec\nlead for Bean Validation 2.0, for instance, which I think is also how we met\nor I believe we interacted somehow with in the context of Bean Validation. I\nremember something there. And then, well, I worked on a project which is\ncalled Debezium. It's a tool and a platform for change data capture. And\nagain, we will dive into that. But I guess that's what people might know me\nfor. I'm also a Java champion as you are, Chris. And I did this challenge. I\nneed to mention it. I did this kind of viral challenge in the Java space. Some\npeople might also have come across my name in that context.\n\nChris Engelbert: All right. Let's get back to the challenge in a moment. Maybe\nsay a couple of words about Decodable.\n\nGunnar Morling: Yes. So, essentially, we built a SaaS, a software as a service\nfor stream processing. This means, essentially, it connects to all kinds of\ndata systems, let's say databases like Postgres or MySQL, streaming platforms\nlike Kafka, Apache Pulsar. It takes data from those kinds of systems. And in\nthe simplest case, it just takes this data and puts it into something like\nSnowflake, like a search index, maybe another database, maybe S3, maybe\nsomething like Apache Pino or Clickhouse. So, it's about data movement in the\nsimplest case, taking data from one place to another. And very importantly,\nall this happens in real time. So, it's not batch driven, like, you know,\nrunning once per hour, once per day or whatever. But this happens in near real\ntime. So, not in the hard, you know, computer science sense of the word, with\na fixed SLA, but with a very low latency, like seconds, typically. But then,\nyou know, going beyond data movement, there's also what we would call data\nprocessing. So, it's about filtering your data, transforming it, routing it,\njoining multiple of those real time data streams, doing things like groupings,\nreal time analytics of this data, so you could gain insight into your data.\nSo, this is what we do. It's based on Apache Flink as a stream processing\nengine. It's based on Debezium as a CDC tool. So, this gives you a source\nconnectivity with all kinds of databases. And yeah, people use it for, as I\nmentioned, for taking data from one place to another, but then also for, I\ndon't know, doing fraud detection, gaining insight into their purchase orders\nor customers, you know, all those kinds of things, really.\n\nChris Engelbert: All right, cool. Let's talk about your challenge real quick,\nbecause you already mentioned stream processing. Before we go on with, like,\nthe other stuff, like, let's talk about the challenge. What was that about?\n\nGunnar Morling: What was that about? Yes, this was, to be honest, it was kind\nof a random thing, which I started over the holidays between, you know,\nChristmas and New Year's Eve. So, this had been on my mind for quite some\ntime, doing something like processing one billion rows, because that's what it\nwas, a one billion row challenge. And this had been on my mind for a while.\nAnd I know somehow, then I had this idea, okay, let me just put it out into\nthe community, and let's make a challenge out of it and essentially ask\npeople, so how fast can you be with Java to process one billion rows of a CSV\nfile, essentially? And the task was, you know, to take temperature\nmeasurements, which were given in that file, and aggregate them per weather\nstation. So, the measurements or the rows in this file were essentially always\nlike, you know, a weather station name and then a temperature value. And you\nhad to aggregate them per station, which means you had to get the minimum, the\nmaximum and the mean value per station. So, this was the task. And then it\nkind of took off. So, like, you know, many people from the community entered\nthis challenge and also like really big names like Aleksey Shipil\u00ebv, Cliff\nClick, Thomas Wuerthinger, the leads of GraalVm at Oracle and many, many\nothers, they started to work on this and they kept working on it for the\nentire month of January. And like really bringing down those execution times,\nessentially, in the end, it was like less than two seconds for processing this\nfile, which I had with 13 gigabytes of size on an eight core CPU\nconfiguration.\n\nChris Engelbert: I think the important thing is he said less than a second,\nwhich is already impressive because a lot of people think Java is slow and\neverything. Right. We know those terms and those claims.\n\nGunnar Morling: By the way, I should clarify. So, you know, I mean, this is\nhighly parallelizable, right? So, the less than a second number, I think like\n350 milliseconds or so this was an old 32 cores I had in this machine with\nhyperthreading, with turbo boost. So, this was the best I could get.\n\nChris Engelbert: But it also included reading those, like 13 gigs, right? And\nI think that is impressive.\n\nGunnar Morling: Yes. But again, then reading from memory. So, essentially, I\nwanted to make sure that disk IO is not part of the equation because it would\nbe super hard to measure for me anyway. So, that's why I said, okay, I will\nhave everything in a RAM disk. And, you know, so everything comes or came out\nof memory for that context.\n\nChris Engelbert: Ok. Got it. But still, it got pretty viral. I've seen it from\nthe start and I was kind of blown away by who joined that discussion. It was\nreally cool to look after and to just follow up. I didn't have time to jump\ninto that myself, but by the numbers and the results I've seen, I would have\nnot won anyway. That was me not wasting time.\n\nGunnar Morling: Absolutely. I mean, people pulled off like really crazy tricks\nto get there. And by the way, if you're at JavaLand in a few weeks, I will do\na talk about some of those things in Java land.\n\nChris Engelbert: I think by the time this comes out, it was a few weeks ago.\nBut we'll see.\n\nGunnar Morling: Ok. I made the mistake for every recording. I made the\ntemporal reference.\n\nChris Engelbert: That's totally fine. I think a lot of the JavaLand talks are\nnow recorded these days and they will show up on YouTube. So when this comes\nout and the talks are already available, I'll just put it in the show notes.\n\nGunnar Morling: Perfect.\n\nChris Engelbert: All right. So that was the challenge. Let's get back to\nDecodable. You mentioned Apache Flink being like the underlying technology\nbuild on. So how does that work?\n\nGunnar Morling: So Apache Flink, essentially, that's an open source project\nwhich concerns itself with real-time data processing. So it's essentially an\nengine for processing either bounded or unbounded streams of events. So\nthere's also a way where you could use it in a batch mode. But this is not\nwhat we are too interested in so far. It's always about unbounded data streams\ncoming from a Kafka topic, so it takes those event streams, it defines\nsemantics on those event streams. Like what's an event time? What does it mean\nif an event arrives late or out of order? So you have the building blocks for\nall those kinds of things. Then you have a stack, a layer of APIs, which allow\nyou to implement stream processing applications. So there's more imperative\nAPIs, which in particular is called the data streaming API. So there you\nreally program in Java, typically, or Scala, I guess, your flow in an\nimperative way. Yeah Scala, I don't know who does it, but that may be some\npeople. And then there's more and more abstract APIs. So there's a table API,\nwhich essentially gives you like a relational programming paradigm. And\nfinally, there's Flink SQL, which also is what Decodable employs heavily in\nthe product. So there you reason about your data streams in terms of SQL. So\nlet's say, you know, you want to take the data from an external system, you\nwould express this as a create table statement, and then this table would be\nbacked by a Kafka topic. And you can do a select then from such a table. And\nthen of course you can do, you know, projections by massaging your select\nclause. You can do filterings by adding where clauses, you can join multiple\nstreams by well using the join operator and you can do windowed aggregations.\nSo I would feel that's the most accessible way for doing stream processing,\nbecause there's of course, a large number of people who can implement a SQL,\nright?\n\nChris Engelbert: Right. And I just wanted to say, and it's all like a SQL\ndialect, it's pretty close as far as I've seen to the original like standard\nSQL.\n\nGunnar Morling: Yes, exactly. And then there's a few extensions, you know,\nbecause you need to have this notion of event time or what does it mean? How\ndo you express how much lateness you would be willing to accept for an\naggregation? So there's a few extensions like that. But overall, it's SQL. For\nmy demos, oftentimes, I can start working on Postgres, developing, develop\nsome queries on Postgres, and then I just take them, paste them into like the\nFlink SQL client, and they might just run as is, or they may need a little bit\nof adjustment, but it's pretty much standard SQL.\n\nChris Engelbert: All right, cool. The other thing you mentioned was the\nDebezium. And I know you, I think you originally started Debezium. Is that\ntrue?\n\nGunnar Morling: It's not true. No, I did not start it. It was somebody else at\nRed Hat, Randall Hauck, he's now at Confluent. But I took over the project\nquite early on. So Randall started it. And I know I came in after a few\nmonths, I believe. And yeah, I think this is when it really took off, right?\nSo, you know, I went to many conferences, I spoke about it. And of course,\nothers as well. The team grew at Red Hat. So yeah, I was the lead for quite a\nfew years.\n\nChris Engelbert: So for the people that don't know, maybe just give a few\nwords about what Debezium is, what it does, and why it is so cool.\n\nGunnar Morling: Right. Yes. Oh, man, where should I start? In a nutshell, it's\na tool for what's called change data capture. So this means it taps into the\ntransaction log of your database. And then whenever there's an insert or an\nupdate or delete, it will capture this event, and it will propagate it to\nconsumers. So essentially, you could think about it like the observer pattern\nfor your database. So whenever there's a data change, like a new customer\nrecord gets created, or purchase order gets updated, those kinds of things,\nyou can, you know, react and extract this change event from the database, push\nit to consumers, either via Kafka or via pullbacks in an API way, or via, you\nknow, Google Cloud PubSub, Kinesis, all those kinds of things. And then well,\nyou can take those events and it enables a ton of use cases. So you know, in\nthe simplest case, it's just about replication. So taking data from your\noperational database to your cloud data warehouse, or to your search index, or\nmaybe to cache. But then also people use change data capture for doing things\nlike microservices, data exchange, because I mean, microservices, they, you\nwant to have them self dependent, but still, they need to exchange data,\nright? So they don't exist in isolation, and change data capture can help with\nthat in particular, with what's called the outbox pattern, just on the side\nnote, people use it for splitting up monolithic systems into microservices,\nyou can use this change event stream as an audit log. I mean, if you kind of\nthink about it, it's, you know, if you just keep those events, all the updates\nto purchase order, we put them into a database, it's kind of like a search\nindex, right? Maybe you want to enrich it with a bit of metadata. You can do\nstreaming queries. So I know you maybe you want to spot specific patterns in\nyour data as it changes, and then trigger some sort of alert. That's the use\ncase, and many, many more, but really, it's a super versatile tool, I would\nsay.\n\nChris Engelbert: Yeah, and I also have a couple of talks on that area. And I\nthink my favorite example, that's something that everyone understands is that\nyou have some order coming in, and now you want to send out invoices. Invoices\ndon't need to be sent like, in the same operation, but you want to make sure\nthat you only send out the invoice if the invoice was, or if the order was\nactually generated in the database. So that is where the outbox pattern comes\nin, or just looking at the order table in general, and filtering out all the\nnew orders.\n\nGunnar Morling: Yes.\n\nChris Engelbert: So yeah, absolutely a great tool. Love it. It supports many,\nmany databases. Any idea how many so far?\n\nGunnar Morling: It keeps growing. I know, certainly 10 or so or more. The\ninteresting thing there is, well, you know, there is not a standardized way\nyou could implement something like Debezium. So each of the databases have\ntheir own APIs, formats, their own ways for extracting those change events,\nwhich means there needs to be a dedicated Debezium connector for each\ndatabase, which we want to support. And then the core team, you know, added\nsupport for MySQL, Postgres, SQL Server, Oracle, Cassandra, MongoDB, and so\non. But then what happened is that also other companies and other\norganizations picked up the Debezium framework. So for instance, now something\nlike Google Cloud Spanner, it's also supported via Debezium, because the team\nat Google decided, that they want to expose change events based on the\nDebezium event format and infrastructure or ScyllaDB. So they maintain their\nown CDC connector, but it's based on Debezium. And the nice thing about that\nis that it gives you as a user, one unified change event format, right? So you\ndon't have to care, which is the particular source database, does it come from\nCloud Spanner, or does it come from Postgres? You can process those events in\na unified way, which I think is just great to see that it establishes itself\nas a sort of a de facto standard, I would say.\n\nChris Engelbert: Yeah, I think that is important. That is a very, very good\npoint. Debezium basically defined a JSON and I think Avro standard.\n\nGunnar Morling: Right. So I mean, you know, it defines the, let's say, the\nsemantic structure, like, you know, what are the fields, what are the types,\nhow are they organized, and then how you serialize it as Avro, JSON, or\nprotocol buffers. That's essentially like a pluggable concern.\n\nChris Engelbert: Right. So we said earlier, Decodable is a cloud platform. So\nyou basically have, in a little bit of a mean term, you have Apache Flink on\nsteroids, ready to use, plus a couple of stuff on top of that. So maybe talk a\nlittle bit about that.\n\nGunnar Morling: Right. So yes, that's the underlying tech, I would say. And\nthen of course, if you want to put those things into production, there's so\nmany things you need to consider. Right. So how do you just go about\ndeveloping and versioning those SQL statements? If you iterate on a statement,\nyou want to have maybe like a preview and get a feeling or maybe just\nvalidation of this. So we have all this editing experience, preview. Then\nmaybe you don't want that all of your users in your organization can access\nall those streaming pipelines, which you have. Right. So you want to have\nsomething like role-based access control. You want to have managed connectors.\nYou want to have automatic provisioning and sizing of your infrastructure. So\nyou don't want to think too much, \"hey, do I need to keep like five machines\nfor this dataflow sitting around?\" And what happens if I don't need them? Do I\nneed to remove them and then scale them back up again? So all this auto\nscaling, auto provisioning, this is something which we do. Then we will\nprimarily allow you to use SQL to define your queries, but then also we\nactually let you run your own custom Flink jobs. If that's something which you\nwant to do, you can do this. We are very close. And again, by the time this\nwill be released, it should be live already. We will have Python, PyFlink\nsupport, and yeah, many, many more things. Right. So really it's a managed\nexperience for those dataflows.\n\nChris Engelbert: Right. That makes a lot of sense. So let me see. From a\nuser's perspective, I'm mostly working with SQL. I'm writing my jobs. I'm\ndeploying those. Those jobs are everything from simple ETL to extract,\ntranslate, load. What's the L again?\n\nGunnar Morling: Load.\n\nChris Engelbert: There you go. Nobody needs to load data. They just magically\nappear. But you can also do data enrichment. You said that earlier. You can do\njoins. Right. So is there anything I have to be aware of that is very\ncomplicated compared to just using a standard database?\n\nGunnar Morling: Yeah. I mean, I think this entire notion of event time, this\ndefinitely is something which can be challenging. So let's say you want to do\nsome sort of windowed analysis, like, you know, how many purchase orders do I\nhave per category and hour, you know, this kind of thing. And now, depending\non what's the source of your data, those events might arrive out of order.\nRight. So it might be that your hour has closed. But then, like, five minutes\nlater, because some event was stuck in some queue, you still get an event for\nthat past hour. Right. And of course, now the question is, there's this\ntradeoff between, okay, how accurate do you want your data to be? Essentially,\nhow long do you want to wait for those late events versus, well, what is your\nlatency? Right. Do you want to get out this updated count at the top of the\nhour? Or can you afford to wait for those five minutes? So there's a bit of a\ntradeoff. I think, you know, this entire complex of event time, I think that's\ncertainly something where people often have at least some time to learn and\ngrasp the concepts.\n\nChris Engelbert: Yeah, that's a very good one. In a previous episode, we had\nthe discussion about connected cars. And connected cars may or may not have an\ninternet connection all the time. So you like super, super late events\nsometimes. All right. Because we're almost running out of time.\n\nGunnar Morling: Wow. Ok.\n\nChris Engelbert: Yeah. 20 minutes is like nothing. What is the biggest trend\nyou see right now in terms of database, in terms of cloud, in terms of\nwhatever you like?\n\nGunnar Morling: Right. I mean, that's a tough one. Well, I guess there can\nonly be one answer, right? It has to be AI. I feel it's like, I know it's\nboring. But well, the trend is not boring. But saying it is kind of boring.\nBut I mean, that's what I would see. The way I could see this impact things\nlike we do, I mean, it could help you just with like scaling, of course, like,\nyou know, we could make intelligent predictions about what's your workload\nlike, maybe we can take a look at the data and we can sense, okay, you know,\nit might make sense to scale out some more compute load already, because we\nwill know with a certain likelihood that it may be needed very shortly. I\ncould see that then, of course, I mean, it could just help you with authoring\nthose flows, right? I mean, with all those LLMs, it might be doable to give\nyou some sort of guided experience there. So that's a big trend for sure. Then\nI guess another one, I would see more technical, I feel like that's a\nunification happening, right, of systems and categories of systems. So right\nnow we have, you know, databases here, stream processing engines there. And I\nfeel those things might come more closely together. And you would have real-\ntime streaming capabilities also in something like Postgres itself. And I know\nmaybe would expose Postgres as a Kafka broker, in a sense. So I could also see\nsome more, you know, some closer integration of those different kinds of\ntools.\n\nChris Engelbert: That is interesting, because I also think that there is a\ngeneral like movement to, I mean, in the past we had the idea of moving to\ndifferent databases, because all of them were very specific. And now all of\nthe big databases, Oracle, Postgres, well, even MySQL, they all start to\nintegrate all of those like multi-model features. And Postgres, being at the\nforefront, having this like super extensibility. So yeah, that would be\ninteresting.\n\nGunnar Morling: Right. I mean, it's always going in cycles, I feel right. And\neven having this trend to decomposition, like it gives you all those good\nbuilding blocks, which you then can put together and I know create a more\ncohesive integrated experience, right. And then I guess in five years, we want\nto tear it apart again, and like, let people integrate everything themselves.\n\nChris Engelbert: In 5 to 10 years, we have the next iteration of\nmicroservices. We called it SOAP, we called it whatever. Now we call it\nmicroservices. Who knows what we will call it in the future. All right. Thank\nyou very much. That was a good chat. Like always, I love talking.\n\nGunnar Morling: Yeah, thank you so much for having me. This was great. Enjoy\nthe conversation. And let's talk soon.\n\nChris Engelbert: Absolutely. And for everyone else, come back next week. A new\nepisode, a new guest. And thank you very much. See you.\n\n  * cloud commute\n  * \u2022\n  * podcast\n\n## Recent Posts\n\nSee All\n\nHow API Gateways help to improve your security with Nicolas Fr\u00e4nkel from\nAPI7.ai\n\nBuilding a Time Series Database in the Cloud with Steven Sklar from QuestDB\n\nProduction-grade PostgreSQL on Kubernetes with \u00c1lvaro Hern\u00e1ndez Tortosa from\nOnGres\n\nSimplyblock is the first company to build a container storage system for\nKubernetes, specifically designed for latency-sensitive, IO-intensive, cloud-\nnative stateful workloads, such as databases, analytics, crypto / blockchain,\ndocument storages, and others.\n\nA fully distributed storage solution, built upon our VASHTM technology,\noffering seamless scalability without downtimes, and is highly optimized for\nAWS NVMe\u00ae (io1/2) and gp3 volumes.\n\n##### Subscribe to keep updated\n\n###### Products\n\nWhy Simplyblock\n\nFrequently Asked Questions\n\nGlossary\n\n###### Use Cases\n\nKubernetes Container Storage\n\nDatabase Storage\n\nCrypto / Blockchain Storage\n\nData Warehouse Storage\n\nCDN / Shared Webserver Storage\n\nFile / Asset Storage\n\nSupported Technologies\n\n###### Compare\n\nSimplyblock vs Amazon EBS\n\nSimplyblock vs Ceph\n\n###### Resources\n\nDocumentation\n\nBlog\n\nWhitepapers\n\nWebinars\n\nCase Studies\n\nPricing\n\n###### Support\n\nSupport\n\n###### Company\n\nAbout\n\nCareers\n\nNewsroom\n\nPress Releases\n\nBrand\n\nContact Us\n\nCloud Commute Podcast\n\n\u00a9 2024 Simplyblock Gmbh | All Rights Reserved.\n\nImprint\n\nPrivacy Policy\n\nThe NVM Express & Design\u00ae and the NVM EXPRESS\u00ae NVMe\u00ae, and NVMe-oFTM word marks\nare registered or unregistered service marks of NVM Express, Inc. Amazon Web\nServices, AWS, the Powered by AWS logo, Amazon EBS, Amazon EKS are trademarks\nof Amazon.com, Inc. or its affiliates. Intel and the Intel logo are trademarks\nof Intel Corporation or its subsidiaries.\n\nbottom of page\n\n", "frontpage": false}
