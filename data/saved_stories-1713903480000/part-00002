{"aid": "40131908", "title": "C isn't a hangover and Rust isn't a cure", "url": "https://scribe.rip/@john_25313/c-isnt-a-hangover-rust-isn-t-a-hangover-cure-580c9b35b5ce", "domain": "scribe.rip", "votes": 2, "user": "CodingThunder", "posted_at": "2024-04-23 13:46:05", "comments": 0, "source_title": "C isn\u2019t a Hangover; Rust isn\u2019t a Hangover Cure", "source_text": "C isn\u2019t a Hangover; Rust isn\u2019t a Hangover Cure\n\n# C isn\u2019t a Hangover; Rust isn\u2019t a Hangover Cure\n\nJohn Viega on 2024-03-14\n\nA few weeks ago, I got a bit miffed reading yet another article that was too\ndismissive about memory safety, basically being mostly dismissive about the\nneed for change. The following weekend, I started seeing flippant responses\nfrom security luminaries, saying essentially that you\u2019re irresponsible and\ndangerous unless you drop C and C++ faster than I dropped my 8 am classes my\nfirst year in college.\n\nI\u2019m going to start with the tl;dr, but then, I\u2019m going to drill down into the\nissue in detail. I\u2019m going to start by trying to present all sides at a level\nwhere most people in the software industry would be able to understand.\n\nThat means, I am definitely going to (intentionally) oversimplify some of the\nmore technical bits. This article is already way too long, and the thesis\nisn\u2019t about the tech per se, it\u2019s that the economics are incredibly\ncomplicated on all sides, and we need to find a way to accept other people\nmaking decisions we don\u2019t like, and still help the world to a better place.\n\nThere is a lot to consider, so I hope you\u2019ll stick with me, and think through\nthe nuance.\n\n## The tl;dr version\n\n  1. The security problem is indeed more significant than many people seem to believe, and many people would absolutely be much better served abandoning C/C++ right away for new projects, and not just from a security perspective. But!\n  2. It\u2019s far more costly and risky to get rid of all the C our applications are already using than many people might think; creating replacements to some key pieces of software will take a decade or more to be effective as a replacement, with no clear overall benefit.\n  3. There is a lot of hidden complexity just when we consider security that complicates the equation to the point where saying \u201cRust is safer than C\u201d might be true, but actually isn\u2019t a total slam dunk at all.\n  4. The economics of something as simple as choice of programming language are actually very intricate. Not only is security not the only non-functional consideration, no matter what you do, there will always be memory unsafe code somewhere (as long as the underlying architectures themselves are unsafe), and there would be many negative consequences to trying to get rid of C quickly.\n  5. Systems languages are overused; C vs Rust is a false choice, because compiled languages like Go are often a much better all-around answer economically. Go in particular has decent enough performance that is sufficient for the vast majority of use cases, will be safe, and has good access to low-level systems APIs.\n\n### Some Security People are Already Bristling\n\nI know from experience, simply suggesting that security might not be an\nabsolute higher priority than anything else will feel like a personal attack\nto some people in the field, as if I am arguing against the security\nindustry\u2019s very existence.\n\nOn the contrary, I think the security industry would be thriving even more\nthan it is if we were more well-rounded, and better understood tradeoffs\noutside of our domain. Some people might have to be dragged into caring about\nsecurity, but non-security people are the majority, and many of them have a\nbalanced view, where they care about security, but want to avoid throwing\nexcessive time and money at it.\n\nOnce, long ago, I was watching a security person arguing with the business,\nand as the security person dug in, I kind of got it, and asked:\n\n> If you think security is the primary concern, then why are you using a\n> computer at all?\n\nPeople are willing to accept risks every day. We know we can \u201ccatch\u201d a virus\nany time we go out in public. We know we can get into an accident and die any\ntime we step in the car.\n\nBut it\u2019s well known that humans in general are horrible at modern risk\nassessment. We have a tendency to either overestimate or underestimate our\nrisk levels dramatically.\n\nGenerally, the security industry probably assumes the average person greatly\nunderestimates risks. And to some extent, that is true. When I was first doing\ncode audits for security in \u201998, it was definitely true that engineers were\nunderestimating risks of memory issues. Back then, if you handed me C or C++\ncode that wasn\u2019t written by Dan J. Bernstein (often refered to simple as djb),\nit was safe to assume there were going to be exploitable memory errors,\nperiod. I even saw them in the code of people I looked up to in the security\nfield.\n\nBut the world is a different place now. Back then, most of the industry\nmassively underestimated risks and were happy in a world with network\nconnections that could be easily tampered with and code that could be easily\nexploited, because they either didn\u2019t think about it at all, or else didn\u2019t\nthink it was going to end up being a big factor.\n\nThere was no patch Tuesday, nor was there widespread compartmentalization or\nother effective compensating controls.\n\nThe rest of the tech world was eventually willing to acknowledge they were\nwrong, due to the hard work of the security industry as a whole. As Greg\nHoglund often said back in the day, the combination of connectivity,\nextensibility and complexity created a perfect storm. Nobody could deny it,\nand as a result, the security industry has had a huge influence, from hardware\narchitectures to network protocols, to programming language design.\n\nBut it wasn\u2019t as easy as it should have been, because we (as an industry) have\noften been unable to truly understand the perspective of people outside the\nindustry.\n\nIf we can keep that in mind, the industry can improve its credibility, and\nmake progress even faster. And we need that, because, as we\u2019ll see, we not\nonly have a lot left to accomplish, but there are many important changes that\nsimply cannot happen quickly.\n\n## How Big a Problem Is Memory Safety?\n\nI\u2019m 100% going to start off by acknowledging the problem. Taking the problem\nseriously early on has been good for my career \u2014 almost 25 years ago, I co-\nauthored the first book on security for developers, before even basic web\nsecurity issues like cross-site scripting (XSS) were a thing. Not too long\nafter, I also co-authored the Secure Programming Cookbook for C and C++.\n\nThose books did well and helped put me on the map, and I worked on those\nbecause I do take memory safety quite seriously. Back then, even more of the\nsoftware out there was written in C/C++ than today, and it was almost\nuniversally more prone not just to having memory safety issues, but far more\nexploitable.\n\nI\u2019ve produced research tools and libraries to help mitigate the problem, and\nseen many other people do the same over the years. But! Generally, developers\ndon\u2019t care about security anywhere as much as security people do. Despite\nplenty of tools, C programmers have tended not to use them.\n\nMemory safety is often considered the most egregious class of vulnerability,\nbecause when such problems are exploitable, they generally enable full\nexecution. Often, problems can be exploited remotely, and sometimes without\nany authentication at all.\n\nBut, the reputation that memory safety problems currently have of being\nplentiful and trivial for sophisticated attackers to find and exploit is\nwrong.\n\nIt was certainly true at the turn of the millennium, but it isn\u2019t any more.\nThe impact of memory unsafe code from a security perspective is definitely\nstill incredibly high, but not so high that, when you consider strong economic\nreasons why you might NOT move away from memory safe languages, it might be\nreasonable to conclude that you\u2019re making a good decision, even risk adjusted.\n\n### The Risks Have Changed\n\nI\u2019m going to focus on why memory safety in C and C++ code isn\u2019t likely to be\nquite as risky as many people believe. Nothing in this section should be\ninterpreted as a reason to choose these languages \u2014 we\u2019ll explore the\ncircumstances where the tradeoffs might make sense later in this article.\nHere, I will firmly acknowledge other languages are inherently safer; I\u2019ll\njust be questioning \u201chow much safer, especially if we have the right controls\nand are vigilant?\u201d\n\nThere are a lot of ways in which the world has changed that directly impacts\nthe risk level (in both directions), including:\n\n  1. Both the hardware architectures and operating systems we all use have done a good job helping thwart exploitability without sacrificing much on performance, starting with StackGuard in 1998, all the way up to Intel\u2019s recent work on control-flow enforcement and ARM\u2019s memory tagging.\n  2. C++ has focused on user interface around its standard libraries, trying to ensure that the average C++ user is not likely to ever use an API where memory safety is a concern. On the other hand, while C has evolved as a language, it has been much more conservative in this regard.\n  3. The full disclosure movement happened, and vulnerability research became a career field, leading to the most common C components getting a fair deal of scrutiny, and helping to dramatically raise awareness of these kinds of issues with C programmers.\n  4. Academia stopped teaching C++, moving to Java and then to Python.\n  5. There\u2019s been a big spike in newer languages that are proper systems languages, but with attention paid to memory safety, most notably Rust, but also Zig, Nim and several others.\n  6. The move to the cloud and other niceties of the modern tech stack are great abstractions, but they do tend to increase attack surface. On the other hand, they promote compartmentalization that can limit impact, and oddly enough, since we don\u2019t put as much reversible binary code into people\u2019s hands anymore, proprietary code tends to benefit from what is essentially security by obscurity, as much as it pains me to admit that (though the fault tolerant design does often help give attackers the ability to automate as many \u201cat bats\u201d as they wish).\n\nSome consequences of the above the above to consider:\n\n  1. Many memory issues in C and C++ code are reported as if they are exploitable, even though there are definitely cases where issues may be impossible to exploit in practice. Way back when I was doing vulnerability research, if I saw a clear memory error, there was not only a good chance it was exploitable, it was going to be really easy for me to build a working exploit. I agree it\u2019s better for us as an industry to just assume any memory issue found is exploitable, because often one can find other bugs to chain to make it so. However, it has become incredibly hard to build working exploits, and has gone from a skill that was easy to develop to one that is pretty rare. So true zero-day exploits tend to be strategically used, most often by governments.\n  2. The economics of the vulnerability research world tends to skew risk perceptions. That\u2019s because such errors are the most valuable in the economy, as I\u2019ll discuss below. However, that means comparing CVEs across languages is suspect. But it also means that, valuable exploits are either sitting places where they\u2019re not likely to get burned, or they go through a \u201cresponsible disclosure\u201d cycle, which means that it makes sense if people conclude that a good patching program can mitigate the risk.\n  3. Sometimes, people who do write in C have a very good reason for writing in C. These reasons will be important to understand. Far fewer newbies think they have to learn C to be effective programmers, but there are still places like embedded systems where C is often far and away a more practical choice.\n\nAs an example of why comparing CVE numbers by language is generally misleading\n\u2014 the Linux kernel recently officially gained the capacity to issue CVEs for\nits own code base. But in their view, any bug under the sun could have\nsecurity implications they don\u2019t understand, so every single bug found in the\nLinux kernel now gets its own CVE, even though mostly they\u2019re not going to be\nexploitable memory problems.\n\n### Understanding reduced exploitability\n\nHere, understanding the evolution of memory errors can help understand\nexploitability. I\u2019ll try to keep away from deeply technical explanations,\nbecause I think the people that most need to understand this are the ones that\nwill (quite rightly) never work at a low enough level where they should even\nneed to understand any of those details. This will, again, oversimplify\nthings.\n\nAnd in some sense, this maybe is gratuitous detail. On one hand, the fact that\nit\u2019s gotten harder to find good exploits by a LOT doesn\u2019t matter, because if\nyou switch away from C, this whole class of issues become non-issues.\n\nOn the other hand, the fact that vulnerability researchers work tremendously\nharder than 20 years ago, to find far fewer bugs, is a signal that practical\nrisks are lower than they used to be (especially in the face of good\ncompensating controls). I would have assumed 20 years ago that any program\nwritten in C had easily exploitable holes somewhere, and probably would have\nbeen right.\n\nToday, I\u2019m prone to think there are good odds of issues in many C programs,\nbut, if you do your design right and pay for the right people to review your\ncode, the economic costs of finding the next bug are high enough that it\u2019s no\nlonger a slam dunk to me that any C code will result in you being owned.\n\nBack when I was getting started in the field, it was often very simple. If you\ncould find a local variable that was an array, there was a pretty good chance\nyou could trick the program into writing outside the array. And, memory layout\nwas so predictable, it was relatively easy to figure out how to exploit such\nconditions.\n\nSpecifically, local variables are generally kept on the program stack. When\nyou enter a function, data gets pushed onto the stack, and when you exit, data\ngets popped from it (kind of). This is different from long-term memory\nallocations that survive function calls (heap storage).\n\nFor instance, it wasn\u2019t uncommon to see code like:\n\n    \n    \n    void open_tmp_file(char *filename) { char full_path[PATH_MAX] = {0,}; strcpy(full_path, base_path); strcat(full_path, filename); // Do something with the file. }\n\nTo the uninitiated, the above code might seem innocent. It creates an array\nthat is zero-initialized of the maximum size the OS supports for a path,\nwhatever that is. Then, it copies into that array, some base directory name,\nthen finally the file name gets appended to the end of that.\n\nBut even on today\u2019s system, if the attacker can control either the input to\nthis function, or to base_path, it is easy to crash this program.\n\nPart of the reason is that C doesn\u2019t keep track of how long things are. In C,\nstrcpy copies byte-by-byte, until it encounters a byte that is valued 0 (the\nso-called NULLbyte). Similarly, strcat works by scanning forward to the first\nnull byte in full_path, and copies from filename until it finds a NULL. In no\nworld do either of these functions check what they\u2019re doing against the length\nof fullpath. So if you can pass in more than PATH_MAX \u2014\nlen(base_path)characters, you will write past the end of the buffer.\n\nIn the \u201cbad old days\u201d, this was a slam-dunk, and it would be trivial to\nexploit.\n\nTraditionally, the program stack intermixes its own runtime data with the\nuser\u2019s data, which is what made the traditional stack overflow such low\nhanging fruit.\n\nEvery time a new function gets called, the stack would get the memory address\nfor the place in code the program should return. So all you had to do once you\nfound one of these conditions, was make sure to craft a exploit payload (the\nmalicious data you send at whatever point) in such a way that it overwrites\nthat return address, replacing it with a pointer back into your own payload...\nwhich also generally includes executable instructions that will do whatever.\nThe executable part of the payload is usually called the shell code, though\ngetting an interactive login (i.e., a shell) doesn\u2019t need to be the goal.\nEither way, when the exploit succeeds, the attacker generally is able to run\nany code they desire from thereon out.\n\nFrom a technical perspective, the most complicated thing back then was the\nshell code itself, as it generally requires at least assembly-level knowledge.\n\nHowever, you didn\u2019t have to write your own shell code, there have always been\nplenty of off-the-shelf payloads.\n\n### Why not just always bounds-check?\n\nGreat question. One might think we could just have programming languages\nalways generate code to check bounds for any accesses, and be done with it.\n\nI\u2019m personally sympathetic. I would point at the massive success of Python to\nshow that, in most cases, performance is even less important than security is.\nSo it\u2019s no surprise that all languages (attempt) to guarantee no out-of-bounds\nwrites, usually with dynamic code.\n\nBut, that checking code, if done everywhere, WILL absolutely have a notable\nimpact on performance, and there are definitely domains where that matters.\n\nFor instance, if you\u2019re a CDN and are trying to cost-effectively handle\nmassive connection volumes, it\u2019s easy to believe that the additional hardware\ncosts could make the business not worth being in, unless the running software\nmanages to skip bounds checks that are low risk.\n\nAnd, individual applications written in Python generally are \u201cfast enough\u201d\n(even if many would disagree, as evidenced by multiple rewrites and a new\nexperimental JIT compiler in the official Python), but would Python be fast\nenough if every bit of code possible that it runs upon was fully bounds\nchecked?\n\nMost of the software we use makes heavy use of low-level systems code written\nin C or C++, even if it\u2019s indirect. Not only will the operating system be\nwritten in such a language, but also many of the libraries a typical\nprogramming language leverages in its runtime will be low-level.\n\nSure, you could \u201crewrite it in Rust\u201d. But, even if we should do that (see\nbelow), it clearly would be a long, arduous journey to get there.\n\nNote that Rust is able to approach C\u2019s speed in part because the compiler can\nessentially \u2018prove\u2019 when it can skip most bounds checks at compile time.\n\nBut it\u2019s not actually all that hard to design APIs on top of C that similarly\ncan avoid memory errors if strictly used, while minimizing the generation of\nruntime code. For instance, in our example above, it\u2019s not hard to provide a\ndifferent C API for strings that always tracks the length, and fully checks.\nWe could provide similar APIs around arrays and other data types. We could\neven provide such austerity to pointers.\n\nThat is effectively what C++ has done quite successfully, and why Bjarne\nStroustrup seems to have taken great offense to the government telling people\nnot to use his language due to memory safety issues. I don\u2019t write modern C++,\nbut it does seem pretty easy from the API docs to never come anywhere near a\nmemory issue.\n\nBut, to the question asked above, the more scale we reach for, the more the\nlittle things make a big difference. If your whole OS and every library Python\nused were fully checked, it\u2019s conceivable that Python would simply be too\nslow.\n\nBut in reality, it probably would just make a lot of software even less cost\neffective. And really, economics is usually the most important consideration\nwhen it comes to software, and those economics have to revolve heavily around\nthe experience of the people using software (and, for example, generally only\ncare about performance if things are slower than expected).\n\nSo if you\u2019re writing low-level systems code, and you want it to get used, it\u2019s\ngot to be widely applicable, and it needs to be cost-effective at the scale of\na CDN or any other big tech company.\n\nThere\u2019s little reason why pretty much anyone should actually want their\ncompiler to generate bound checks when they are unnecessary. Ideally, we\u2019d\nwant to be able to prove when it\u2019s safe to skip, and make sure to produce\nmachine code (whether manually or via compiler) whenever it is safe to skip\nchecks.\n\nUnfortunately, it\u2019s pretty much impossible to get absolute assurance in most\ncases. We need to decide what level of risk of exploitation we even find\nacceptable.\n\nAnd, as I will discuss, anyone who claims the risk should ever be absolute 0\nis way too unrealistic... even ignoring economic arguments.\n\n### Out-of-Bounds memory errors: a history of mitigations\n\nThe question of how much risk should we be willing to accept, leads us to the\nquestion, \u201chow much risk are we currently accepting?\u201d\n\nBecause if the answer is, \u201cnot much\u201d, then we need to think about whether it\u2019s\nworth the cost to add bounds checking.\n\nThe practical level of risk is hard to quantify precisely. The best way to do\nit really is to talk to people who have spent their careers doing\nvulnerability reach where they actually prove exploitation.\n\nAnyone qualified who was there would certainly agree that C/C++ programs\naround the turn of the millennium were an absolute cornucopia of\nexploitability. I was there; it was easy if you had a basic low-level skill\nset.\n\nBut, there have been a quarter century of mitigations, and we live in an\nentirely different world. It\u2019s so tough to prove exploitability, that we often\njust accept the possibility of exploit without any proof, any time we see a\nmemory error we show can give us enough of a foothold where exploitation might\nbe possible. And most of my friends still doing this kind of work would\nabsolutely acknowledge that it is no longer \u201clike shooting fish in a barrel\u201d,\nit\u2019s much, much harder.\n\nIt may not be easy to quantify how much the bar has been raised, but it\u2019s\ngetting pretty darn high.\n\nTo start understanding the risk today, we should go back to the beginning. In\nthis case, let\u2019s go back to the simple stack overflow discussed above.\n\nIn fact, while the code I showed is definitely a memory error, and I\ndefinitely could have exploited it a quarter century ago, determining whether\nI could actually build a practical exploit is so daunting a thought that I\nwouldn\u2019t bother trying.\n\nThe code is still wrong because it doesn\u2019t do bounds checking. And it can\nstill be used to crash the program (which in some sense is a security issue).\nIf you\u2019d like to see, here\u2019s me live-coding the example, and showing it still\ncrashes (no up-charge for my slow typing and silly mistakes).\n\nBut, just because it\u2019s a memory error doesn\u2019t mean it\u2019s easy to exploit, or\neven possible to exploit.\n\nAs crappy as the above code is, StackGuard addressed this problem fairly well,\nstarting all the way back in 1998. The basic idea is that when a program\nstarts up, select a random number of at least 64 bits. Push it onto the stack\nevery time you call the function.\n\nThen, every time you return from a function, check to make sure the random\ncanary is intact. If it isn\u2019t, crash instead of returning.\n\nWhereas one used to be able to figure out deterministically what to write to\nkeep the program working up till the point it started executing your shell\ncode, a naive exploit no longer works, unless the program somehow leaks its\ncanary.\n\nThere certainly were situations where you could work around the above issue\n(especially when you can chain another bug), but StackGuard eliminated some\nissues completely, and certainly raised the work level of the vulnerability\nresearcher.\n\nThe software exploitation community (both \u201cbad guys\u201d and the vulnerability\nresearch community) kept having to work harder and harder on bypass\ntechniques, but have tended to at least find cases where they could work\naround some mitigations, if circumstances are right. For instance, the above\nmitigation doesn\u2019t work if the memory is dynamically allocated, because that\nmemory is kept separately, in the heap.\n\nAnd, certainly, the program doesn\u2019t keep function return addresses in the\nheap. However, many real programs, especially one using dynamic dispatch in\nC++, will keep pointers to functions in the heap, and use them to dynamically\nselect functions to call.\n\nOne of the more effective and well-known defenses has been Address Space\nLayout Randomization (ALSR), which is implemented at the operating system\nlevel. Basically, every time your program starts up, the OS will randomize\nwhere data lives as much as it can. With ASLR, if there\u2019s enough randomization\ninjected, the attack would succeed with such low probability, it could require\nas many attempts to succeed as atoms in the universe.\n\nIn practice, it\u2019s not quite that good \u2014 sometimes the distances between\nparticular bits that are valuable to the attacker might not be randomizable,\nbecause there can be reasons why they need to be knowable (or, within small\nranges). And, if the OS is too aggressive at randomizing, it could break\nprograms. This is particularly true within operating system kernels \u2014 it\u2019s\nhard to get a meaningful amount of randomization inside the kernel.\n\nStill, this was an amazingly effective technique that suddenly made\nexploitation incredibly hard.\n\nIf you\u2019ve never dug under the hood here, but are technical enough, you might\nask two important questions:\n\n  1. Shouldn\u2019t the system be keeping the user\u2019s program data far from its internal state?\n  2. If the payload has to live in the heap or on the stack (other memory is generally non-writable), can\u2019t we prevent those areas from running code??\n\nFor question 1, not only is a single stack per thread easier to implement, it\nis generally faster, because hardware generally will have direct support for a\nprogram stack. And ultimately, while processes have virtual address spaces\nthat provide protection from other processes, within one process, any code in\nthe process can address any memory cell in the process.\n\nThough, it is still valuable to shuffle things around. For instance, in heap\noverruns, function pointers are juicy targets. Storing all function pointers\nin tables allocated statically or in a separate heap of memory definitely is\nbetter than the typical approach of sprinkling function pointers wherever they\nhappen to go.\n\nAs for the second question, it\u2019s 100% possible to prevent code execution out\nof the stack or the heap at the system level, and that is a mitigation totally\nworth having. However, some environments, including some entire programming\nlanguages, use one or the other to implement their own dynamic capabilities\n(like lambda functions that are closures). Still, for most programs, this\nmitigation is basically free, and raises the bar even more.\n\nToday, when there\u2019s a memory issue, attackers generally can\u2019t expect to run\ncode directly. But, imagine that you\u2019re attacking a program written in Python,\nand you can somehow leverage a memory error in the underlying C implementation\nof Python to write to arbitrary places in memory.\n\nUnder the hood, Python implements a virtual machine. There are \u201cinstructions\u201d\nthat can live in the heap or stack that are examined by Python\u2019s built-in\ncode, and that code does different things depending on the instructions.\n\nAs it turns out, when we talk about memory being \u201cnon-executable\u201d really only\nrefers to what executes directly on the underlying system processor, not what\nhappens in an application-level virtual machine.\n\nSo even if the program you\u2019re attacking has its executable code segments non-\nwritable, and all the data you can write is non-executable, you can still\nchange data that controls what the executable code does.\n\nAs an attacker, if there is no virtual machine, you can kind of create one\nyourself, using a technique called Return Oriented Programming (aka, ROP).\nBasically, leveraging memory errors, you try to groom the program\u2019s data so\nthat it will jump around the program\u2019s memory, doing bits you want it to do\n(usually with the goal of having it spawn a login shell, where you then can\nlegitimately run anything you want again).\n\nROP is intrinsically difficult, as it generally requires the attacker to groom\ndata both on the stack and on the heap, which itself is hard. Add in address\nlayout randomization, and you\u2019ll find that most memory errors involving out-\nof-bounds writing are incredibly difficult to actually exploit, and doing so\nwill generally require chaining together multiple bugs, and most often also\napplying ROP.\n\nRecently, Intel introduced Control Flow Integrity (CFI) as an option that\nexplicitly is meant to thwart ROP.\n\nRemember when we said it often didn\u2019t make sense to move return addresses off\nthe stack? Intel thought it would be too difficult to get the world to stop\ndoing that in practice, but instead, if duplicates the return addresses onto a\nshadow stack. Then, when functions return, it ensures that the return\nlocations are consistent.\n\nThat obviously would be effective against stack overflows, but what if\nattacker skips directly writing to the stack at all? For instance, with ROP\none often will just manipulate data in way that causes a jump to misfire\nsomewhere, that will run code they like. When that code hits a \u2018return\u2019\nstatement, it might return to the place that CFI expects.\n\nExcept that CFI can also validate call sites. Often, ROP will involve jumping\ninto the middle of functions, which CFI can stop. And, it can ensure functions\nonly get called from places they should have been called from.\n\nCFI wouldn\u2019t stop our attack against Python\u2019s virtual machine. But for\nprograms without virtual machines embedded, it is very likely to make ROP-\nstyle attacks even more difficult (and thus less practical), for programs\nmaking use of CFI.\n\nIn summary, while we may not be able to quantify well what percent of memory\nvulnerabilities modern mitigations thwarts, it\u2019s likely that plenty such\nerrors are simply not exploitable when all easily available system mitigations\nare applied (many are on for you by default in most apps, but CFI is new\nenough, and not widely used, and there are some places where these mitigations\nare NOT applied; see below).\n\nBut the space is complex, and I\u2019ve long marveled at the ingenuity of everyone\naround the table. While it\u2019s been a long time since most bugs were cookie-\ncutter, or could be exploited without chaining multiple bugs together, the\nexploit development community has innovated in similarly impressive ways, so\nit *is* best to always assume that memory problems are likely exploitable.\nYes, the number of people with the proper skill set is incredibly low compared\nto 20 years ago, but we shouldn\u2019t assume it will ever go to zero.\n\nSo one might rightly conclude, \u201cwhy take the risk? Surely it\u2019s just best to\nget rid of this whole class of issues.\u201d And on one hand, I\u2019d agree, because\nthe bar is high, but the best people can often find and chain together\nmultiple bugs. what they need. Yes, these things can be a problem in Rust and\nother languages, but they will always be a big problem in C, as long as it\nexists.\n\nBut then again, most of the qualified people doing it are either supplying\nexploits to governments, or practicing responsible disclosure, which generally\nshould mean that if you stay on top of patching, the risk can be mitigated\npretty well.\n\nAnd if you\u2019re being targeted by a government, enough that they\u2019d be willing to\nrisk exposing a 0-day to own you, they probably have other methods that will\nwork more reliably. Frankly, xz isn\u2019t the only long game we\u2019ve seen;\ngovernments plant spies in places that can get them access to what they need,\nand that includes within security companies.\n\nThat\u2019s mostly why plenty of of the most paranoid security people hate security\nproducts that are big monoliths \u2014 no matter how intentioned, you\u2019re going to\nend up with a much greater attack surface, and quite likely with far more\nrisk, especially if you\u2019re worried about nation states.\n\nI do expect over time, hardware platforms will continue to raise the bar. If\nwe\u2019re lucky, in another decade, we may even be near the point where such\nmitigations are just about as effective as full bounds checking, but far\ncheaper (at least, in the environments where they\u2019re available).\n\nBut until then, it\u2019s not completely unreasonable for people to believe that,\nwhile these problems are a real concern, that they are making enough of an\ninvestment in compensating controls to not incur all of the costs they\nassociate with other options.\n\n### Other Memory Errors\n\nOut-of-bounds accesses are not the only thing that qualifies as a \u201cmemory\nerror\u201d in C and C++.\n\nThese languages are known for users manually taking on the responsibility of\nallocating and deallocating memory. These languages do come with libraries for\nhelping with memory management, but, unlike many other languages, you are\nstill left to decide when and how to de-allocate heap memory.\n\nThere are plenty of other issues beyond pure array bounds errors, including:\n\n  1. If you de-allocate memory that\u2019s still in use by other objects, that can leak sensitive information (such as pointers to ROP targets), partially because C doesn\u2019t zero out memory or ensure that things are written before they are used. Such Use-After-Free bugs can also easily result in arbitrary data writes too.\n  2. If you can get the program to release memory that has already been released, you can corrupt the internal memory management data structures, which can also have dire consequences. Simply being able to call free() twice on the same memory before it gets allocated can be game over.\n  3. If the program does math to figure out how much memory to allocate, and the attacker can force the math to yield a result large enough, the number might \u201cwrap around\u201d, resulting in a smaller allocation thanW needed, and forcing a buffer overflow (integer overflow issues).\n\nSuch problems have been quite common in code, because it is often very\ndifficult to reason about when to release memory, and C programmers doing it\nmanually, often get it wrong. Automatic memory management (e.g., garbage\ncollection) does generally help address most of these issues... unless you can\nexploit issues with that memory management.\n\nMemory managers in garbage collected languages do tend to be incredibly\ncomplicated, and there have been several major exploits for multiple garbage\ncollectors, including most browser JavaScript engines.\n\n### Modern Mitigation\n\nAs mentioned above, C++ has done a lot of work to help programmers sidestep\nthe above issues:\n\n  1. The standard library is written in a way that, when functions return, most resources (particularly stack-allocated memory) will be automatically released, without the user having to do anything.\n  2. Similarly, C++\u2019s libraries completely avoid raw pointers, and use wrappers that provide automatic memory management via \u2018reference counting\u2019.\n  3. The APIs for C++\u2019s standard data structures prevent array bounds errors.\n  4. C++ has a deep type system, and it\u2019s often possible to be quite confident in the safety of the program, relative to those types. Meaning, if you stick to their guard-rails, you can be sure that type errors (where data is of an unexpected type) are always caught.\n  5. There have also long been Garbage Collectors available for C++. The Boehm Collector came out in 1988, and is still maintained.\n  6. There are a wealth of static analysis tools for C++ that can identify when code doesn\u2019t follow the C++ core guidelines.\n\nThe great thing about the above, is that many C++ programmers are benefitting\nfrom most of the above items.\n\nC programmers, on the other hand, tend NOT to enjoy any of the same benefits.\nWhile, much like C++, the C standard is frequently updated, unlike C++, C is\nvastly more conservative on changes, and does not add niceties the way that\nC++ does.\n\nC programmers can get some of the same benefits:\n\n  1. There are Garbage Collectors for C. In fact, the Boehm Garbage Collector for C++, works just as well with C.\n  2. There are quite good static analysis tools for C, mostly centered around the CLang compiler ecosystem.\n\nBut, for the most part, C sees its place in the world more as a portable\nassembly language. We\u2019ll come back to that shortly.\n\n### Why is the View So Skewed?\n\nTo the outsider, the discussion so far might fly in the face of the \u201cevidence\u201d\nyou\u2019ve heard. For instance, I\u2019ve heard many people claim, \u201cmost CVEs are due\nto memory safety issues\u201d. I recently read yet another article quoting this,\nwhich also gave an interesting statistic: at that time (in March, 2024), there\nhave been 61 CVEs in C/C++ code, but only 6 CVEs in Rust code.\n\nAll of this is true, of course. But none of the data is a good proxy for risk:\n\n  1. We\u2019ve already established that counting memory errors is not the same as counting exploitable memory errors. The later is far more important, and there are good reasons to at least question this.\n  2. C/C++ is like a cockroach infestation, in that, you may not see it day to day, but if you know where to look, you\u2019ll find it everywhere. While C/C++ haven\u2019t been the most popular languages by any metric in a long time, old building blocks are still widely used and maintained. And both are still common for any new systems-level code. So, for most software we use, no matter what language it was written in, there will probably be critical hidden dependencies under the hood that were written in C/C++. So saying Rust has had 10% of the CVE total of C/C++ actually feels like it\u2019s either appallingly high for Rust, or amazingly low for C/C++, given there is actually very little production Rust code in comparison to C and C++.\n  3. Even if all C/C++ errors for which there are CVEs happened to be easily exploitable, the data is absolutely skewed towards highlighting memory vulnerabilities, due to the economics around exploitation.\n\nAs I noted above, governments (and some corporations) do pay for exploits that\nthey can use \u201ctactically\u201d in operations. In plenty of cases, both have been\nknown to pay six figure sums per bug, and sometimes pay even into the 7\nfigures.\n\nVulnerability researchers who play this game generally make more than a year\u2019s\nworth of salary for the average tech worker with just one bug. And there are\nabsolutely people who manage to sell more than one such bug every year.\n\nGenerally, were I a government buying exploits, some of the most important\nconsiderations for me, when figuring out what I should buy:\n\n  1. Reliability. Due to lots of things (randomness included), lots of exploits work only once in a blue moon. Some exploits, however, are absolute slam dunks, and work every time.\n  2. Ubiquity. Particularly, I\u2019d care about ubiquity of software among my target. Meaning, if you want to target a particular foreign actor, what software are you like to find?\n  3. Stealth. Generally, I\u2019d want missions to not be discovered, and to minimize the risk of losing my tools. I\u2019ve seen exploits that can remotely pop common web server software, then leverage another bug to raise privileges (escaping from a container), and then disable SE Linux, all without ever writing to disk, and with only a single log message \u2014 one that simply says SE Linux has stopped running. That\u2019s doing pretty well on the \u2018stealth\u2019 front.\n  4. Execution. Generally, I want to be able to bring tools to help with my activities. So full execution is a must-have. Though, data-driven attacks a pretty big thing even in low-level exploitation these days. But I think a lot of the \u201cbig actors\u201d also like data driven attacks in higher-level cloud exploitation because so much can be achieved with creds or data on users. Still, if I have full execution, I can do those things, and far more.\n  5. Longevity. Ideally, the bug isn\u2019t likely to be found and fixed as a matter of course.\n\nThere are classes of bugs, like command injection attacks, that can affect\nmost programming languages, that can score pretty well on many of the above\nfronts. But a really good memory error generally will score even better.\n\nDue both to the innate value of memory errors, as well as the technical\nchallenges involved in actually finding and exploiting such bugs, memory\nerrors are the most prestigious type of bug in the vulnerability research\ncommunity.\n\nAs a result, they get far more attention and press than more mundane issues.\nThat means that there could easily be plenty of reasonably low hanging fruit\nin code written in other languages, but it also does mean that you\u2019re much\nmore likely to see this kind of vulnerability show up in code you\u2019re using,\nputting you at more risk.\n\nOn the other hand, as time has gone on, exploitation has become harder and\nless reliable overall as a way to infiltrate, so it\u2019s no surprise to see\nnation state actors diversifying to other approaches, which we will discuss\nlater.\n\n## Why don\u2019t people switch?\n\nSo far, we\u2019ve seen that C is indeed far more susceptible to memory errors than\nany higher level language.\n\nAnd while mitigations are reasonably effective, they are not good enough to be\nthe only reason why people don\u2019t switch (well, in the case of C. In the case\nof C++, they\u2019ve got a better case).\n\nSo what are the key factors that keep people from switching?\n\nFor the sake of discussion, we\u2019ll act like Economists, and assume that people\nare rational (acting irrationality would tend to favor C anyway).\n\nFirst, it shouldn\u2019t surprise anyone that, even if everyone agreed that C\nshould die, it would take an awfully long time, considering I cannot name a\nsingle person who believes it\u2019s a travesty COBOL isn\u2019t a common language\nanymore. Still, there are plenty of COBOL applications out there that aren\u2019t\ngoing away any time soon.\n\nThose COBOL applications are still there, because it\u2019s been impossible to rip\nthem out. For every such application I\u2019ve heard of (most of them long-running\nservices in the financial space), the company who owns the application has, at\nmultiple points, tried to migrate away from COBOL, but ultimately failed.\n\nSuch are always large applications that are incredibly important to the\nbusiness\u2019s core economics, and quite complicated. But they have been working\nrobustly for a very long time.\n\nMost people have, over the years, tried building something brand new from\nscratch, but then have realized it would cost vastly more money to get both\nfeature parity, and to achieve the same level of robustness. Fifty years of\ninsights and bug fixes are encoded, but usually without adequate\ndocumentation.\n\nAnd then, how do you get confidence to swap things out? For instance, let\u2019s\nimagine you\u2019ve got a system clearing north of $1 Trillion in transactions a\nday, as it has been doing for many decades. What would it take before you\nwould be willing to cut more than a tiny sliver of that over to a new system,\nknowing that, if you mess it up, the entire business will likely die?\n\nThe monolithic lift-and-shift is too impractical. But even on those old COBOL\nsystems, people have tried swapping out small pieces over time, too. That can\neventually work, but it can also lead to pretty catastrophic failure, and\nseveral companies have deemed it not worth the cost and risk. So, even though\nthey pay to keep around good talent willing to maintain a code base in COBOL,\nthe economics are keeping the language around.\n\nC (along w/ C++) is far more ubiquitous than COBOL ever was, and it has been\nthe bedrock of most important software technology for pretty close to 50\nyears.\n\nYou might think that\u2019s hyperbole, but it absolutely is not. Currently, Python\nhas been the world\u2019s most popular programming language for several years. The\ndominant implementation has always been written in C, and much of the core\nlibraries are backed by C code.\n\nJavaScript is the other popular high level language. Yes, it generally runs in\nbrowsers, which themselves are typically incredibly large C++ projects.\n\nEven programming languages that intentionally were NOT written in a C-like\nlanguage are generally going to be dependent on C code. Typically, fairly low-\nlevel services like DNS will be offloaded to C code, often in the \u201cstandard\u201d\nlibraries.\n\nBut at the end of the day, every major operating system is primarily written\nin C, along with some assembly. Most of the network services we use every day\nare primarily implemented in C or C++. In embedded systems, it\u2019s STILL almost\nunheard of to use a language other than C or C++. Even in Rust-land you will\nfind some C.\n\nThere are reasons for that. Here\u2019s a non-comprehensive list:\n\n  1. A lot of C code has become incredibly widely used and trusted over many years, and the world would have to take on a lot of pain to migrate.\n  2. Plenty of low-level tasks require significantly more gymnastics than necessary in more safe languages, particularly because they are more defensive.\n  3. Many embedded environments are constrained enough that bringing more compiled systems languages to the table with lots of dependencies is not feasible. C is incredibly compact and simple in comparison to Rust.\n  4. When it comes to developers in the market qualified to do low-level programming, you will find vastly more C and C++ programmers than any other alternative.\n  5. Rust is not particularly easy to read or easy to write, and there is not really any other game in town worth mentioning... yet (Zig is getting there). It particularly makes it a huge lift to ask people who have spent 20\u201330 years honing their craft in C or C++ to basically throw away that institutional knowledge. Nobody wants to experience the feeling of swimming through molasses. More on this below.\n\nA few weeks ago, I was talking about this topic, and I brought up the\ntremendous level of effort companies running core services would be incurring\nby migrating from well-designed C code to Rust. The person I was talking to,\nindicated that would be easy, because there are IETF RFCs around all the\npieces, like SMTP and IMAP.\n\nI wish that were good enough. RFCs leave plenty of \u201cSHOULDs\u201d and \u201cCOULDs\u201d, and\noften take time to catch up to evolutions in technology, instead of always\ndefining things before they get built. About 30 years ago, I wrote the first\nversion of Mailman, which by no means needed to implement a full-fledged SMTP\nserver, and I still have the mental scars from new non-conformant use cases\nleading to subtle bugs around interoperability, coming out of the woodwork at\na steady pace.\n\nPeople have been writing mail clients and mail servers around these core\nprotocols for more than 50 years, and the world is full of things that really\nshould be supported, but are undefined or non-compliant behavior. And some of\nthose issues will come up incredibly infrequently, and be hard to reproduce.\n\nIf you\u2019re Google or Microsoft (who run a very large percentage of email these\ndays), how willing are you going to be to jettison code that\u2019s been robust for\ndecades, and is trusted to cover all of those corner cases from the last few\ndecades, especially KNOWING that there\u2019s not a good knowledge base around it?\n\nNow consider that the software here that we run today has built up some trust.\nSure, in the bad-old-days, Sendmail was popular and was peppered with holes,\nbecause it wasn\u2019t defensively written.\n\nOn the Unix side, Postfix has been in active development for 25 years, and is\nwritten in C. And while its security track record isn\u2019t perfect, it has been\nvery good, and was designed from the beginning heavily leveraging the least\nprivilege principal.\n\nAnd even though Postfix was more security conscious from the beginning, and,\nfrankly, easier to configure and use, it took at least a decade (probably\nlonger) before it had mostly displaced Sendmail, mainly because of\ninteroperability. It took time not just to re-discover important anomalies\nthat might disrupt the business, but to prove to people worried about business\ndisruption that it was mature enough to not be a risk on that dimension.\n\nFrom that same era, there was a second upstart SMTP server, qmail, from Dan\nBernstein, which was written even more defensively (but still in C). Its only\nknown issue was an integer overflow back in 2005, when that was a new class of\nvulnerabilities (and Dan has argued that it wasn\u2019t even practical to exploit\nin any real world scenario).\n\nBut, qmail was not only much harder to use than Postfix and sendmail, but it\nwas not able to get to the necessary level of interoperability to get people\nto adopt it in mass. As a result, qmail development stopped 25 years ago\n(although there are still active descendants; they still are not widely used,\nand I couldn\u2019t imagine them being used anywhere where a bug could be\nmeaningful economically).\n\nSo in order to \u201crewrite it in Rust\u201d, and get that to the point of successfully\ndethroning Postfix (never mind Exchange), one would have to:\n\n  1. Build something from scratch that meets all the standards.\n  2. Get enough real-world usage to find and implement most of the \u201cgotchas\u201d that mature software already will handle.\n  3. Build in enough usability that people will consider it in bulk.\n  4. Keep up with all the new work that does get done in the mail world (there have been plenty of additions over the years, especially in the security arena, and nobody should expect that to stop).\n  5. Slowly convince the world to move over.\n\nIn a best-case scenario, that process won\u2019t happen quickly; it\u2019s something\nwhere we\u2019ll see results another decade down the road. And while, on one hand\nI\u2019d love to see it, on the other hand, It\u2019s hard to get excited about a\ndecade-long project to displace something that isn\u2019t generally considered a\nbig risk anyway.\n\nOf course, the above argument applies to a whole lot of legacy infrastructure\nwritten in C. For instance, it\u2019s great to see the Linux community start to\nexperiment with Rust in the code base. But it\u2019s just an experiment at this\nstage, and if it becomes more than that, it certainly will not be a quick\nmigration.\n\n## C\u2019s long-term niche\n\nIn my view, Rust being in the kernel is an impressive accomplishment. Many\npeople have lobbied over the years for C++ to be allowed into the kernel, and\nit has never happened. The argument against it has been simple \u2014 C++ has too\nmuch abstraction. The kernel basically lives at the bottom of the software\nstack, and should not incur any unnecessary costs, so the kernel team needs to\nbe able to reason about performance problems, which means they need to be able\nto see how C code maps to the generated assembly.\n\nI can say from experience, Rust is indeed closer to C on that front. However,\nRust is certainly not better than C in that regard, and there are certain\ntasks where Rust very much gets in the way.\n\nMany of those tasks are low-level enough that they\u2019re true systems-level\ntasks, like memory management, device drivers, exposing new hardware\ncapabilities, and so on.\n\nAnd yes, you can do these things in Rust, but it is laborious in comparison,\nand generally will result in leveraging Rust\u2019s \u2018unsafe\u2019 capabilities, in which\ncase, you\u2019re incurring the same risks, and why not write it in C?\n\nAdditionally, just like the Linux kernel doesn\u2019t include the standard C APIs\n(because they do not make sense in that context; they provide their own\ninternal APIs where needed), Rust cannot use its own APIs; they have to use\nthe kernel\u2019s.\n\nThe hardware architectures we use provide very little inbuilt safety at the\ninstruction level. Certainly, even C\u2019s horrible, basic type system is vastly\nmore help than one gets writing directly to the architecture.\n\nIn any realistic software system, if you drill down to the lowest levels,\nthere will always be code that will end up needing to be written to target\nthese unsafe platforms.\n\nAdditionally, as we noted above, the vast majority of embedded systems use C\nexclusively. That\u2019s not only because in some environments with weaker CPUs\nevery cycle might count. It\u2019s also because other resources are limited:\n\n  1. Memory might be at a huge premium, including stack space, disk space, cache, registers, the works. The size of the compiled executable can be an issue, as can be any unnecessary space taken up by runtime cruft or fat abstractions when they\u2019re present.\n  2. The tooling for such environments generally isn\u2019t good for any other languages. It might be a huge challenge just to get a compiler that produces code at all for the platform, never mind one that provides tools aimed specifically at the platform.\n  3. Due to a number of factors, including space constraints and the difficulty that can be involved with test cycles, such environments often have very few external dependencies. Even standard libraries for these environment may completely lack basics like threading. Newer systems languages these days have been trying to keep their standard libraries fairly small, but they are still too large (and the build times are often way too high, particularly with Rust) to be \u201cgood enough\u201d in the embedded arena.\n\nC has thrived in this world, and it\u2019s one of the many reasons the C standard\naggressively minimizes what API must be present, way, way beyond what any of\nthe other systems languages do.\n\nUnfortunately, the embedded world tends to not support many (or any) of the\ncommon mitigations that make C safe. They also have other disadvantages from a\nsecurity perspective, such as a difficult time finding the entropy needed for\nbasic cryptography. Nor do they have the investment that makes upgrading\nbroken software easier.\n\nHOWEVER, these constraints are more business trade-offs made where cost\nthrough the supply chain is a huge concern. Much of the software dealing with\nsuch constraints is on low-end hardware for a reason, and the products in\nquestion often would not be viable if they carried all of the baggage\nassociated with higher-level platforms.\n\nIt\u2019s not always just a \u201ccost\u201d consideration, either. Beefier hardware requires\nmore power, and if you\u2019re looking at wearables or other devices that might\nneed to stay on battery power for long periods, those things might matter to\npeople far more than the security risks do.\n\nThose are business trade-offs, and C is really the only non-assembly language\nthat is willing to properly service such environments (well, to a much smaller\nextent C++, but that is the only other language that has any real traction in\nthe embedded space at all).\n\nPeople do like to call C a \u201cportable assembler\u201d. That\u2019s not really true at\nall; in my view, C is far more high-level than assembly. When I\u2019m working on\ncompiler tasks and other system-level tasks, were I to be asked to not use C,\nbut to use assembly, that would seem absolutely insane. I only drop to\nassembly if it cannot be done right in C (or cannot easily be done right), and\nthat often is just a few instructions. I also lose plenty of time when I have\nto drop to assembly, because it is so infrequent (and each modern architecture\nis so complex), I have to spend a lot more time in the documentation.\n\nC lives in a unique space that is higher-level than assembly, but lower-level\nthan any other systems programming language (certainly C++, and Rust too).\nIt\u2019s basically a half-step between them, abstracting away many platform\nportability issues, but still basic enough that, if you DO know an\narchitecture and compiler, you can reliably predict what code will be\ngenerated just by looking at the C source.\n\nThere are plenty of tasks that are best done at this level. Moving to Rust for\nsuch tasks would be nowhere near as much extra work as dropping down to\nassembly, but it still would be a lot of extra gymnastics, just to be writing\n\u201cunsafe\u201d blocks anyway. While this would help bound areas where security bugs\nare likely to be, the more need for such \u201cunsafe\u201d blocks, the more likely I\u2019d\nbe to expect negative net benefit.\n\nSome people have said to me, \u201cdoes that mean you don\u2019t consider C a real\nprogramming language?\u201d Far from it. I think that assembly constitutes a real\nprogramming language. However, I think we are best served to think about\nclassifying languages using meaningful axises, as best as we can define them.\nGenerally, people who reach for Rust or C care a lot about performance\n(whether they should or not is another matter). Less often (but still common),\nresource consumption is an important issue. The axis in my mind has\nperformance on one end, and user experience on the other, because that is the\nprimary trade-off as you move along the axis.\n\nI group things relative to that access like so:\n\n  * Assembly languages, which are so intrinsically low-level and devoid of useful abstraction that I believe people should only deal with it directly in a few special circumstances (including building compilers that automatically target it, unlocking functionality, and very explicit optimization efforts that have a lot of data suggesting the work be done, where compilers are just not in a position to come anywhere near optimal).\n  * Pre-assembly languages, that try to keep everything as close to the hardware as possible, while giving the programmer as much control as possible over performance. Here, we might sacrifice a little bit on average on performance to make portability and maintainability much better. But, we should still be writing code at a level where we are providing something that should be fundamental, leveraged at scale by software the author isn\u2019t even considering. OS Kernels definitely fall in that category, and perhaps some of the base low-level library facilities should too, but most tasks probably don\u2019t need to be handled at this level. Realistically, C is the only language in this category. Rust may get there someday, but I don\u2019t feel it is anywhere close today. And okay, the name is horrible, but I\u2019ve got nothing better.\n  * Systems languages. To me, these are languages that focus on providing as much performance as reasonable, while still providing safety in MOST cases (allowing guarded access to unsafe mechanisms). Generally these languages will bend over backwards to avoid traditional garbage collection (or, selectively opt out of it). That means you may have to do some manual work around memory management. This has become a \u201chot\u201d category for language development, and includes C++, Rust, Zig, D and Nim, among many other lesser known ones. Now, the manual memory management generally is far less work and far less error prone than is typical from C. Although, I\u2019d say Rust makes memory management HARDER than best practices in C, but with the benefit of much better safety guarantees. Go\u2019s performance is now good enough that it might even meet some people\u2019s bar for a systems language, though I\u2019ll keep it out of this category due to the lack of first class support for manual memory management.\n  * Compiled languages. In these languages, performance is still a concern, but so is the overall developer experience. By this point, nobody should have to worry much about memory management. And, there should be a fairly rich ecosystem. There\u2019s generally a strong emphasis on type safety too, with decent type systems. Go, Java and C# are long-standing stalwarts in this category, and I\u2019d put Typescript here, since it is often fully compiled, and has a big emphasis on safety. I probably stick Javascript more in the latter bucket (even though it is often compiled; the naming here is definitely not perfect; it\u2019s just that the correlation mostly applies).\n  * Scripting languages. In these languages, performance is pretty low on the priority list. Rapid development is highly valued, leading to very rich languages. Usually people don\u2019t want to wait for results to compile, and might even want to see changes without having to re-run the program. Here, dynamic features have traditionally been more highly valued than type safety, although that has started to change (as evidenced by some of the ghastly bolt-on systems that are nevertheless becoming fairly popular).\n\nOn this axis, security doesn\u2019t actually vary all that much. Yes, proper\ncompiled languages tend to have given it a lot of attention, and systems\nlanguages are better than C. Clearly scripting languages sacrifice some\nsafety, but these days they\u2019re usually just as willing to consider security as\nthe compiled languages.\n\nHere, C and assembly definitely are at a firm disadvantage relative to memory\nsafety, but, as we\u2019ve been discussing, it\u2019s not as big of a gap as one might\nthink because:\n\n  1. In desktop / server worlds where security is more likely to be a high priority, there are often very effective mitigations that mitigate the risks. And if it\u2019s clear that where developers take the highest advantage of those mitigations and put thought into their design and implementation, we might end up pretty reasonably confident (e.g., with Postfix).\n  2. Other systems level-languages still expose unsafe features, and end up with issues there.\n  3. Most languages have dependencies (often, including the compiler) that will be written in memory-unsafe languages.\n  4. Plus, as we will discuss at the very end, these is at least one very important way that other languages tend to be LESS secure than C.\n\nAgain, this is not to try to argue that C isn\u2019t distinctly and uniquely risky.\nWe consistently see the big C targets get popped: browsers, kernels, network\nservices. While it\u2019s more rare and harder than it used to be, there\u2019s no\nshortage exploits flowing. Adding new code quickly to such apps helps add\nattack surface. And, having to rely on a large patchwork of mitigations all\nbeing in place at once isn\u2019t awesome, especially when they\u2019re not in the most\nworrisome places, like embedded systems that are almost never updated.\n\nStill, the people working on OSes, browsers and so on tend to be smart people\nwho want to do the right thing. They\u2019re clearly interested in better security,\nand we need to understand that there are a lot of considerations we usually\nare not considering.\n\nThat is, it\u2019s hard to imagine your OS and browser of choice being completely\nwritten in Rust without unsafe blocks in the next few years, but it\u2019s\ndefinitely reasonable to expect that plenty of effort will go towards slowly\nshifting that way. We can already see it not only in Linux\u2019s acceptance of\nRust, but also in Microsoft\u2019s, who is taking it very seriously, and using it\nfor significant pieces. But the people I\u2019ve talked to there also understand\nthe drawbacks, and are being pragmatic.\n\n### Premature Optimization as a Language\n\nI\u2019ve been arguing that, like it or not (and to be clear, I don\u2019t like it), C\nhas a strong argument for its place in the ecosystem, and it is not going away\nany time soon, because there is nothing appropriate ready to displace it.\n\nI think it\u2019s more concerning that many programmers systemically overestimate\nhow important performance is. My observations:\n\n  1. Python has been so popular for so long despite being 50\u201380x slower than C for much of that run (when languages in the Systems and Compiled categories often have been in the 2x-5x range, and rarely above 10x).\n  2. Most language decisions are made long before there is any relevant performance data to guide a decision.\n  3. In fact, gathering performance data at all is quite rare, even though decisions made in the name of performance are frequent.\n  4. All this despite the fact that it\u2019s oft-repeated that conventional wisdom around how code interactions with the compiler and/or architecture is typically old and wrong. You hear a lot that \u201cit\u2019s generally better to trust the compiler\u201d, and a lot of us who give that advice then turn around and ignore it.\n\nI think any systems programmer being honest would themselves would say\npremature optimization is a huge pitfall, one that they have fallen into it\nmany times.\n\nI myself often fall prey to it. Sure, I do try to measure, especially when\nperformance is an explicit goal. For instance, when I was working on lock-\nfree, wait-free hash tables (and other data structures), I did a LOT of\ncomparative testing, for instance producing at least 15 different\nimplementations that varied often in very subtle ways, so that I could do\ncontrolled testing in a lot of different workloads.\n\nOn the other hand, I have habits built over the decades that I can\u2019t fully\nshake. I still sometimes am compelled to copy function parameters that are\npassed by reference into locals when that value is used frequently in a\nfunction to ensure that it\u2019s in a register, even though:\n\n  1. Were I to measure, it\u2019d almost certainly be irrelevant in all circumstances, in terms of contributing to noticeable performance problems from a user perspective.\n  2. Often, I\u2019m using a language where I can take advantage of link-time optimization (LTO), which involves doing additional optimization once you know every single module going into your executable. LTO didn\u2019t exist for me 30 years ago (modules were all separately compiled under the assumption they might be linked to anything in the future), and no compiler was going to risk copying something behind a pointer across modules in case there were threads. I haven\u2019t dug deeply enough to know if the compilers I use bother, but they certainly could do this, and if it makes an impact, they probably DO. Of course, not only does it not likely matter in practice, but it probably didn\u2019t even matter in anything I ever did back then either. No, it was just \u201cconventional wisdom\u201d I absorbed.\n\nPeople are notoriously bad at estimating performance (and risk), and for\npeople who are going to overestimate, they are going to end up choosing\nsystems languages (or C, especially if they are cavalier about the risk side\nof things) when a plain old compiled language might be fine. In fact, in many,\nmany cases, even Python would be fine. Dropbox, for instance, did very well\nfor themselves with most critical things running in Python.\n\nPersonally, I think we, as an industry should be far more concerned about\npeople making bad choices around performance than we should about security,\nbecause:\n\n  1. If you push people to not overestimate the need for performance, security does tend to get meaningfully better as a matter of course.\n  2. The indirect economic hit from overestimating on performance can easily be far more impactful to businesses than the cost of underestimating on security. For instance, both C and Rust programmers tend to spend significant amounts of time on memory management, and find themselves spending more time trying to understand low-level issues when they do arise, compared to the compiled languages, and even our highest-level languages. Do you really need to forego the garbage collector? Because that alone often adversely affects development time significantly, never mind the benefits of having much richer abstractions available in a standard library that can lower costs.\n\nMeaning, I\u2019d be just as inclined to push people away from Rust and toward Go\nas I would to push them away from C and toward Go. If I take off my \u201csecurity\nguy\u201d hat, and try to form an opinion that\u2019s as objective as possible, I think\nthat is far more meaningful than the benefit of going from C to Rust, even if\nI ignore all of the hidden costs to using Rust we\u2019ve been discussing.\n\nThat\u2019s not to say there aren\u2019t plenty of cases where systems languages, or\neven pre-assemblers aren\u2019t the right choice. I just think we should routinely\npush back on this question, and try to get people to try to objectively\nexamine all aspects of why they\u2019re really making the decision:\n\n  1. Is it really because we can demonstrate a performance need? If so, prove it.\n  2. Otherwise, it could be valid to consider the switching cost, particularly when we start with access to resources seasoned in a particular language, etc.\n  3. And if not, what would most likely help with other goals... getting the functionality built faster? Or maybe quality is important (which would push me to Go or other compiled languages, since scripting languages often are harder to drive up quality due to the dynamic nature, and even in a systems language with a very strong type system like Rust, lower-level code seems to implicitly end up with more hidden \u201cgotchas\u201d.\n\n### We Still Need People Learning C\n\nIn most scenarios where people select a systems language, we should be urging\nthem to pick something higher-level, instead of prematurely optimizing without\ndata.\n\nMore times than not, that will be the right economic decision for individual\nproducts and companies. However, the industry as a whole does need to continue\nto cultivate C programmers somehow.\n\nAs long as COBOL\u2019s legs have been, C\u2019s will be MUCH longer. Today, there are\nincredibly few people with any practical COBOL experience, but things won\u2019t be\ngreat if, 50 years from now, there are as few people with practical C\nexperience, because:\n\n  1. Believe it or not, despite its age, COBOL is much higher level and more accessible than C, so it will be much harder to get good people up and running to be able to maintain critical systems 50 years from now, considering how insane C is compared to other programming languages.\n  2. There will, without any doubt at all, be far more critical C still in widespread use (very likely including OS kernels).\n  3. It facilitates keeping system-level innovation going.\n\nWhat I mean by that last point, is that, 50 years from now, we will still need\npeople who can become experts at the underlying architectures, AND help enable\nsoftware developers with any hardware improvements.\n\nOver the last 30+ years, we\u2019ve benefited tremendously as an industry by a\nlarge influx of people into programming, and some of them have gone down that\npath. But:\n\n  1. We\u2019ve also provided people with much higher-level abstractions at the language level than 30 years ago. Javascript, Python, Go and Java do such a great job of abstracting away the hardware details, that only people who get incredibly interested in learning systems stuff do it.\n  2. If AI-assisted development becomes incredibly effective, the gap for most developers from where they are to mastering systems programming could grow significantly (which isn\u2019t all bad).\n  3. If we do somehow stop most C development, and most people from learning it without any suitable replacement (of which Rust is not, I assure you), we will be massively widening the gap between systems languages and the actual hardware.\n\nC, as horrid as it is, definitely is a much better stepping stone towards\nunderstanding low-level architectures from the programming side, relative to\nany other current language.\n\nIf that stepping stone isn\u2019t there, then the people willing to move down the\nstack all the way to facilitate hardware improvements in software, will drop\ndramatically due to the level of effort required to learn the basics and\naccomplish simple tasks would end up being high enough that more people who\nare interested will either assume they\u2019re incapable or will not want to go\nthrough the pain, and give up. People are very goal-oriented creatures, and we\ntend to not pursue the goals that we perceive as too unachievable, for our own\nmental health.\n\nI\u2019d feel much more comfortable if there were a pre-assembly language that\ncorrected some of C\u2019s most egregious mistakes (the biggest in my view being\nC\u2019s treatment of arrays, but my list of issues is very long), and, during\ndevelopment, always perform as much analysis as possible (not just the\nsanitizers available via the clang project, but runtime safety tools like\nValgrind).\n\nRust currently is the closest thing to that replacement, but over-emphasizing\nthe functional paradigm currently detracts from illuminating the underlying\nimperative Von Neumann architecture in my view.\n\n## Why One Might Not Choose Rust\n\nSome of these we\u2019ve covered, but overall, I\u2019ve heard people cite:\n\n  1. \u201cOur applications don\u2019t need to be written in a systems-level language; we are fine with garbage collection.\u201d\n  2. \u201cWe\u2019re comfortable with the language(s) and ecosystem(s) we know, and do not want to lose so much time to learning a new ecosystem.\u201d\n  3. \u201cWe feel Rust has a steep learning curve, and is hard to write.\u201d\n  4. \u201cWe don\u2019t have people skilled enough with it (everyone learning it together seems like a waste of resources).\u201d\n  5. \u201cOther people\u2019s code written in Rust tends to be hard to understand.\u201d\n  6. \u201cBuild times tend to be very high, often with too many external dependencies.\u201d\n\nI realize that Rust has become very popular with the Technorati in a very\nshort period of time, and for very good reasons. Personally, I have a great\nappreciation of some of Rust\u2019s accomplishments; I too masochistically enjoyed\nmy early fights with the borrow checker, because I could appreciate the raw\ntechnical accomplishment, and it made me feel smart to be able to be effective\nat Rust.\n\nStill, I\u2019ve experienced many of the above listed items first hand. About three\nyears ago, I was reading an academic paper that was poorly written, and\ndecided to see if anyone had implemented it yet. I found two different\nimplementations of the same paper, but only two, and they both happened to be\nin Rust. I\u2019d done some stuff in Rust, so I thought it\u2019d be fine. But, both\nimplementations were both incredibly terse and challenging to comprehend\nthemselves. And if I hadn\u2019t known they implemented the same algorithm, I never\nwould have guessed, because they used substantially different idioms, and\nlooked nothing alike.\n\nIt reminded me of reading other people\u2019s Perl in the mid \u201890\u2019s: Rust sometimes\nfeels like a \u201cwrite-only\u201d language.\n\nAnd, I\u2019ve written enough Rust and talked to plenty of Rust developers to say\nconfidently that a significant number of people will find it difficult to\nadopt, and will take a long time to feel as productive in it relative to their\ncurrent language of choice.\n\nI say that, even having read Google\u2019s blog post where they attempt to debunk\nRust being \u2018hard to learn\u2019. I read that, but beyond them not sharing any real\ndata, there are issues with this work:\n\n  1. The sentiment of people who found themselves \u201cas productive in Rust\u201d is very skewed by Googlers who are used to building to their internal C++ tool stack, which has enough complexity and controls built up over the years that simply being freed from those constraints will make them feel productive.\n  2. Surveying user sentiment instead of metrics around code itself comes with implicit bias that is hard to correct for (look at how much work is applied to correct for such things in political polling, to still have huge misses). Google employees want don\u2019t want to look weak to the people who are surveying them about Rust.\n  3. Even if the data were even remotely useful, there is no direct comparison against other languages. The blog post claims their results are the same as any other language, but they explicitly use the word \u201canecdotally\u201d.\n  4. Google\u2019s workforce also tends to be among the most skilled around. Saying, \u201cwe think it\u2019s easy at Google\u201d doesn\u2019t generalize in the slightest.\n\nIn fact, from what I\u2019ve seen, Rust primarily has only taken off among the\nTechnorati. I think anyone who has done significant work in both Python and\nRust should have an intuition that Python is likely far easier for the masses.\n\nPersonally, this fact, that Rust is explicitly for the Technorati, people who\ncome to the table with an intuitive grasp of a mathematical function and\nrecursion, is what keeps me from embracing it.\n\nI want programming to be more egalitarian. I think there are many smart,\ncapable people in the world, outside our industry, and outside sciences, who\ncould make amazing contributions to the world if they could more easily\ntranslate ideas to computer execution. Much like I don\u2019t like seeing too high\na bar for people to learn systems programming, I care a lot about lowering the\nbar to entry for programming in general (In my view, Python has done the most\nfor the world in that regard).\n\nBut fundamentally, Rust is a great language, and people who are comfortable\nwith it should use it where it makes sense. However, I think people should try\nto be more honest about the economics:\n\n  1. We should be pushing to think more thoroughly about the economics surrounding their choices, particularly pushing to prefer compiled languages over systems languages whenever possible, because the overall economics are likely to be better much of the time.\n  2. When pre-assembly needs are there, we should not be forcing people to avoid C/C++. Challenging the choice is fine, but the world\u2019s needs are never going to be met by a single programming language, even in the systems space.\n  3. We should be promoting the development, and the success of additional systems languages like Zig, and even something that actually could replace C in its niche, so that someday we might be able to put the nails in C\u2019s coffin in a way we haven\u2019t even been able to do with COBOL (despite a forceful push to do so leading up to the millennium).\n\nCurrently, Zig and its ecosystem are well behind Rust\u2019s in terms of readiness\nfor many system programming needs, but it takes a much more egalitarian\napproach to the problem. Between it and Rust, Zig will be a much more\naccessible language to the bulk of people who have written in compiled\nlanguages and even scripting languages.\n\nPart of it is that Rust\u2019s roots are firmly in the functional programming\nworld, where first principles are around mathematically pure functions. There\nare some people in the world who intuitively understand those things, but it\ntends to be people with a significant math background.\n\nOn the other hand, Zig is still a regular imperative language. First\nprinciples essentially are \u201cgiving somebody detailed instructions\u201d. Even kids\nunderstand such things (even if they generally don\u2019t want to follow them). And\nindeed, every successful pre-programming project I\u2019ve ever seen (such as\nScratch) is imperative.\n\nThe fact that functional programming has been widely acknowledged as non-\naccessible, and difficult to adopt for the last 65 years indicates to me that\nevery class of programming language should have a strong procedural language.\n\nI remember that MIT used to brag about starting with Scheme as a first\nlanguage, when everyone else was teaching C++, touting the functional\nparadigm. But they too, eventually moved away from functional languages as an\nintroductory languages.\n\nBut, I mostly feel positive about the functional paradigm, and functional\nlanguages too, because they do have tremendous advantages of their own,\nparticularly, better encouraging more reliable code that is easier to analyze\n(and, my copy of The Implementation of Functional Languages signed by Simon\nPeyton Jones 30 years ago remains among my prized possessions).\n\nI\u2019d say that there\u2019s so much value in the functional paradigm, that I believe\nit\u2019s worth having a good, popular functional language at every abstraction\nlevel down to perhaps pre-assembly (I\u2019m not sure there would be much utility\nin a functional assembly language any time soon. But I\u2019d be excited to be\nproved wrong!)\n\nOn the other hand, I think the object-oriented programming paradigm has much\nmore limited utility and is better off either going away entirely, or at most,\nbeing a de-emphasized feature.\n\n## Where Rust is probably a Bigger Security Risk than C (for now)\n\nI get disappointed when well-known security thought leaders make statements\nlike, \u201cIt\u2019s irresponsible to build critical applications in non-memory safe\nlanguages\u201d, not simply because they ignore the economic complexities and\ntrivialize a complex decision, but also because, even if we only think about\nsecurity, as bad as memory safety issues in C are, it isn\u2019t clear cut that the\nrisks are worse than in other languages.\n\nSpecifically, C programs generally have a small number of external\ndependencies, where often those dependencies are among the most used pieces of\nsoftware out there (such as the C standard library).\n\nMost other languages are much better equipped to support programmers\nleveraging the work of other programmers. In some sense, that\u2019s a good thing\nfrom a business perspective. But from a security perspective, more\ndependencies not only tends to increase our attack surface, but it leaves us\nmore open to supply chain attacks.\n\nOne thing I read in the last year or so that stood out for me was a comment on\nHacker News about Rust, but in a thread about Swift:\n\n> My current problem with rust is the dependency hell. Hundreds of sub\n> dependencies for every top level one. Yes some of them are super common like\n> serde or rand, or oddly some crate that seems to be just to create\n> directories on the filesystem?! A blessed subset of crates is what I was\n> counting on to save the day, but when something like tonic brings in 100 or\n> so fine grained one-off sub dependencies I don\u2019t think that can work. Right\n> now I am just plugging my ears saying \u201cmy code is memory safe and I am\n> fearlessly concurrent!\u201d But I am thinking \u201cwhat horrible thing is lurking in\n> the depths of my dependency tree and which state actor put it for later?\u201d If\n> that seems paranoid look at the recent issues with pypi malicious packages.\n> I know I can roll my own, but that cost money, and if tokio or tonic didn\u2019t\n> exist, and crates wasn\u2019t so darn easy to use, maybe google would have made a\n> monolithic grpc crate instead?\n\nThe xz affair is the most recent and most compelling example of such a supply\nchain attack, but that\u2019s just one that the industry was lucky enough to find.\n\nRust makes it easy to pull in outside dependencies, and much like in the\nJavaScript ecosystem, it seems to have encouraged lots of tiny dependencies.\nThat makes it a lot harder to monitor and manage the problem.\n\nBut Rust\u2019s situation is even worse than in most languages, in that core Rust\nlibraries (major libraries officially maintained by the Rust project) make\nheavy use of third party dependencies. The project needs to take ownership and\nprovide oversight for their libraries.\n\nTo me, this has long been one of the biggest risks in software. I can write C\ncode that is reasonably defensive, but I have a hard time trusting any single\ndependency I use, never mind scaling that out.\n\nProperly securing your dependency supply chain is a much harder problem than\nwriting safe C code. Personally, I only pull in dependencies beyond standard\nlibraries if the work I\u2019d have to do in order to credibly replace the\nfunctionality is so great that, if I didn\u2019t bring in a dependency, I would\nchoose not to do the work.\n\nC is a lot better than Rust in this regard, but it\u2019s not particularly great.\nPartially, that\u2019s because the C standard libraries (which I am always willing\nto use; the core language implementation and runtime is a given) are not at\nall extensive. People who write a lot of C end up building things themselves\nonce and keeping them around and adapting them for decades, including basic\ndata structures like hash tables.\n\nI have personally always been far more concerned about minimizing dependencies\nthan buffer overflows. There are straightforward approaches to minimizing\nmemory safety problems (discussed a bit below), and they\u2019re not too hard to\napply in most applications.\n\nBut digging into each and every dependency? Even the best efforts in supply-\nchain security so far aren\u2019t in a great position to help with attacks like the\nrecent xz affair (note to my spell checker, this is not the xyz affair from\nthe history books, but I am indeed trying to evoke it). Nation states are\nwilling to play the long game, and once a developer builds up trust in some\ndownstream dependency of yours, it\u2019s not too hard to introduce backdoors in a\nway that\u2019s likely to be considered an accidental bug, if other people find it\nat all.\n\nWith xz, the backdoor didn\u2019t directly make it into the source tree, so it was\nmore overtly a backdoor. But, we\u2019ve known for an awfully long time that\nstealthy back doors are indistinguishable from bugs.\n\nI can remember from the early days of the software security industry (probably\nabout 25 years ago), discussing backdoors with Steve Bellovin, who related a\nstory from his days at Bell Labs, where he found a pretty subtle memory error,\nthat had lingered in the code of a former employee, that his instinct told him\nwas an intentional backdoor. It felt intentionally placed, and if I recall\ncorrectly, there had been some bad blood. But being indistinguishable from a\nbug, how could he ever prove it?\n\nCertainly, that was back in the day where many memory errors generally could\nresult in reliable exploits. But it\u2019s still not too hard, especially\nconsidering, even though we have more of a peer review culture than we used\nto, plenty of code that is \u201creviewed\u201d isn\u2019t reviewed thoroughly enough by\npeople who are looking for the right things.\n\nPlus, code review is a lot harder to do well than writing code, which is part\nof the reason I don\u2019t expect to use LLM-based code generation in the near\nfuture \u2014 it turns programmers into both \u201cproduct managers\u201d writing\nrequirements, AND code reviewers. Currently, it feels easier to me to \u201cjust\u201d\nbe an engineer.\n\nAnyway, the more dependencies you have, the larger your circle of implicit\ntrust is, the larger your attack surface is, and the more supply chain risk\nyou\u2019re taking.\n\nThat makes Rust in particular a pretty large risk on the supply chain security\nside, whereas C scores pretty well. And as far as modern risks go, this seems\nan incredibly practical and significant risk for any code that might\neventually get rolled up into something that is used in organizations that\nnation states might want to attack.\n\nC\u2019s advantage in terms of lack of dependencies (which can come with a lower\nattack surface in general) is large, but still doesn\u2019t make it the right\neconomic choice in the first place. It might still be wiser to choose Rust\nwhen all economic factors are considered, but the security argument is just\nnot one I find compelling enough.\n\nGenerally, I think Rust (and pretty much any programming language) would be\nserved well to take ownership of their standard libraries. Pull in all the\ndependencies, and be willing to take ownership.\n\nAlso, I would typically argue for languages to incorporate more functionality\ninto their standard libraries, even though the recent trend is to move to less\nfunctionality. Yes, from a security perspective, this technically increases\nyour attack surface. But not really:\n\n  1. If people feel like they may need to pull in a particular bit of common functionality from the outside, they\u2019re very likely going to do it, whether the standard library exports it or not.\n  2. With link-time-optimization in particular, it\u2019s not too hard to excise bits of a language\u2019s standard libraries that are not actually getting used, which brings the attack surface down to about the same thing (though, many non-systems languages don\u2019t worry about link-time optimization).\n  3. The language maintainers, by taking ownership, can not only better focus on properly vetting the security risks in the solutions people are likely to use, they also are more likely to look for the architectural efficiencies that are going to help with the attack surface.\n\nYou may or may not end up with fewer people touching the code, but the\nlanguages should be willing to be accountable for the functionality people are\nlikely to need, especially when, like Rust, security is billed as one of the\nprimary motivators for using it at all.\n\nLanguages like Go and Python that have extensive standard libraries that the\nlanguage maintainers take responsibility for are actually the best case\nscenario in my opinion. Yes, more people touch the code, but the DIY economics\nare often the wrong choice, and having organizations willing to both be\naccountable, and provide an environment where people can focus on minimizing\ndependencies if they feel its important, is a good thing.\n\nYes, Python has become so popular, that plenty of people use outside\ndependencies, and there are several popular package managers. However, it\u2019s\nstill in a vastly better place from a supply chain perspective than\nJavaScript, which has become famous among developers for hidden dependencies\non trivially small packages.\n\n## Recommendations\n\nFrankly, while I think that today, the supply chain issue might possibly give\nC an edge over Rust thinking strictly about security, that\u2019s an advantage that\nRust can easily make disappear. And when it does, C will be left with concerns\nabout memory management. My intent here isn\u2019t to argue for using C over Rust,\nit\u2019s to show that decisions around language choice are far more complex than\nthe sound bytes people fling around.\n\nI\u2019ve covered a lot of ground (and have already excised quite a lot, including\na lot of example code), but let me try to focus on what\u2019s actionable.\n\n### For teams\n\n  1. Explore the overall economics of choices. When selecting technologies, challenge each other to think about the broader economic implications, and try to get numbers to support your hypotheses. This has many facets, but in the context of this discussion, it should often push you toward languages like Go and away from either Rust or C, because too often, people will go with their gut on this, which will generally over-estimate the importance of performance.\n  2. If you don\u2019t want to analyze up front, don\u2019t start with systems languages. Instead, start with Go, Swift, Java, C#, or any really other good compiled language. Remember here that if your gut is shouting about performance, Python has acceptable performance for all sorts of things (remember Dropbox?) and compiled languages are all still going to be vastly more efficient than that.\n  3. Do consider security. Just because you\u2019re going with Rust doesn\u2019t mean you\u2019re going to get it for free. Bad things can still happen to you. Please dig past the \u201cthought leader\u201d sound bytes on this.\n  4. Avoid unnecessary dependencies. I will leave \u2018unnecessary\u2019 vaguely defined here; you need to be educated and judge all the economic factors. But note that, there are often other benefits to fewer dependencies, from shorter build times to less surface to test, to less risk from API changes or bugs from downstream dependencies.\n  5. Be aware of the dependencies you do have. More than just trying to get your arms around the transitive graph that dependency scanning tools provide, it\u2019s worth digging down to understand what those things aren\u2019t finding. There\u2019s probably some C libraries linked into your runtime somewhere, and it helps when the next xz affair surfaces to be able to more easily, more honestly assess your risks.\n  6. Try to ensure outside security review. For enterprise software, this is almost a given these days, because large buyers will demand evidence of it. But everyone should be considering how they can facilitate such review regularly, and treat it as an opportunity, not just a box to tick.\n  7. If you choose C, provide documentation around your decision. I say this because the security concerns are absolutely real concerns, and people deserve to know you have done a good job. C++ to my view is not in the same category if you stick the with official recommendations, yet plenty of people aren\u2019t really in compliance, so I would err on the side of caution, and do the same with C++.\n  8. If you choose C, proactively address the memory safety concerns effectively. For instance, if at all tractable, use the Boehm collector. Or else, use best of breed techniques when that isn\u2019t possible.\n\nNote, the primary reason why many C developers fall back on unsafe primitives\nisn\u2019t a lack of education about the security risks, its that they generally\nhave a few large dependencies (such as OpenSSL or some other crypto library),\nand it isn\u2019t clear how to, for instance, apply the Boehm Garbage Collector to\nthose third party libraries in an easy, portable way (although in many cases,\nif you compile the source yourself, it\u2019s actually easy to just redefine\nmalloc()and friends to call the Boehm API, and in others, there are some low-\nlevel techniques that can address the issue, but they\u2019re too low-level to\nrecommend).\n\nEven for those building their own lightweight memory management (arena\nallocation is becoming popular, which one could consider a very lightweight\nform of garbage collection), but many of the most common dependencies like\nOpenSSL provide direct ways to hand the library a memory allocator, if you\nlook for them.\n\nI can cover this in detail if there\u2019s enough interest in it (even though I\u2019d\nprefer you to adopt some other language instead).\n\n### For The Security Industry\n\nHere, I certainly mean thought leaders, but anyone who cares enough to be\nmaking decisions based on security, and talk to other people about it at all.\n\n  1. Most importantly, try to keep in mind that just because you think it\u2019s a bad security decision, doesn\u2019t make it a bad decision, overall. Try to listen to non-security people, and learn about how their priorities. You probably rail against \u201cFUD\u201d (fear, uncertainty and doubt), but oversimplifying non-trivial issues to push for security at all costs is essentially you spreading FUD. The use of C is just one example here. But remember, even the security implications are far more complex and subtle than we might be thinking about in the moment.\n  2. Make sure the industry considers the broader economics. Sure, outside the industry, security is far less important than it is, inside the industry. But I think it goes much deeper \u2014 if the security industry tries to push too much work to the rest of the industry, we will at the very least destroy more credibly, but could conceivably do a lot of economic damage, for instance, by driving up the costs and liability risks to small businesses and individuals so high that they just don\u2019t bother.\n  3. Contribute to solving our legacy software problems. I\u2019d love to get rid of C as well, but the reality is that C will be even more pernicious than COBOL has been, for good reasons. Saying, \u201crewrite it all in a safe language, and migrate everything ASAP\u201d is not really a solution. It\u2019s aspirational at best, and completely impractical from a risk management perspective. Like with replacing Sendmail with Postifx, we can make progress, but it will need to be incremental, and we\u2019ll have to be more practical.\n\nIn short, yes, let\u2019s 100% kill C and C++, but let\u2019s be practical about it, and\nbe good collaborators with the rest of industry. Our goal should 100% be to\nput ourselves out of jobs (or, since we\u2019re not at risk of that, try to work\ntoward a world where it\u2019s a shockingly small industry... even if in the far\ndistant future).\n\n### For the rest of the industry\n\nIn general, the rest of the software industry should work with the security\nindustry to visualize what it takes to address risks. Specifically:\n\n  1. Help figure out how can we can keep a fresh pipeline of people who will be able to understand the boundaries software and hardware. That means, we need to eventually get skilled people effectively doing pre-assembly tasks in a way that\u2019s far less risky than doing them in C today (not, trying to keep those tasks from ever happening). If the answer isn\u2019t \u201canother programming language\u201d, then great, but if it needs to be another language, both sides need to be serious about it to ensure success, because the world is littered with languages that have no traction.\n  2. Please push others to show their work on broader economic decisions, and don\u2019t assume the world is as simple as whatever \u201cconventional wisdom\u201d gets passed down. That\u2019s a swiftly moving target.\n  3. Language designers, in addition to the ways you already consider security, please also give much more thought and attention to the impact of third party dependencies, realizing that balancing contribution with security is hard. We need better pragmatic solutions.\n  4. And in all cases, try to assume you\u2019re wrong about the things we all systemically mis-estimate, like performance and security. Assume you can be wrong in either direction, too! Now, try to collect some hard data to give you a better sense of where the reality actually lies.\n\n## Feedback\n\nI\u2019m happy to discuss the topic or take any feedback. As I\u2019ve said, I\u2019m happy\nto keep learning and re-evaluating my views. However, this story will be\npublished while I\u2019m on a long vacation, so please do reach out to me, but I\nwon\u2019t be quick to respond.\n\n", "frontpage": false}
