{"aid": "40130604", "title": "Creating a Transformer from Scratch", "url": "https://benjaminwarner.dev/2023/07/01/attention-mechanism", "domain": "benjaminwarner.dev", "votes": 1, "user": "birriel", "posted_at": "2024-04-23 11:24:28", "comments": 0, "source_title": "Creating a Transformer From Scratch - Part One: The Attention Mechanism", "source_text": "Creating a Transformer From Scratch - Part One: The Attention Mechanism | Mixed Precision\n\n# Creating a Transformer From Scratch\n\n## Part One: The Attention Mechanism\n\nBenjamin Warner\n\nJuly 1, 2023\n\nUpdated: Jul 31, 2023\n\n20 min. read\n\nTransformers are everywhere. They are the backbone of modern language models\nlike ChatGPT. Transformers assist generative models such as Stable Diffusion\nand Dall-E create images from prompts. In most domains, Transformers are\ngiving other model architectures a run for their money.\n\nBut what exactly is a Transformer and how does it work under the hood?\n\nCode for this post can be found here: commented-transformers.This is the first\npost in a multi-part series on creating a Transformer from scratch in PyTorch.\nBy the end of the series, you will be familiar with the architecture of a\nstandard Transformer and common variants you will find across recent models\nsuch as GPT, PaLM, LLaMA, MPT, and Falcon. You will also be able to understand\nhow Transformers are being used in domains other than language.\n\nAll posts in the Creating a Transformer From Scratch series:\n\n  1. The Attention Mechanism\n  2. The Rest of the Transformer\n\nYou cannot create a Transformer without Attention. In this post, I will show\nyou how to write an Attention layer from scratch in PyTorch. By the end of\nthis post, you will be familiar with all three main flavors of Attention:\nBidirectional, Causal, and Cross Attention, and you should be able to write\nyour own implementation of the Attention mechanism in code.\n\n# # Quick Recap of Attention\n\nAttention allows modern neural networks to focus on the most relevant pieces\nof the input whether text, images, or multi-modal inputs. If you are\nunfamiliar with Attention in a neural network context, you should pause and\nread Attention Is All You Need by Vaswani et al(missing reference) or one of\nthe many good Transformer summaries. Personally, I recommend Jay Alammar\u2019s The\nIllustrated Transformer.\n\nA quick recap of Attention in Transformers. Query, key, and value calculation\nin matrix form. From The Illustrated Transformer.Attention works by creating\nquery Q, key K, and value V matrices from inputs X via linear layers with\nlearnable weights WQ, WK, and WV\n\nQ=XWQK=XWKV=XWV\n\nwhere WQ\u2208Rdmodel\u00d7dQ. Or less formally, Q=XWQ is a set of linear equations\nQ=XAQ+BQ where AQ and BQ are learnable parameters for calculating Q from X.\n\nAttention is then calculated by:\n\nAttention(Q,K,V)=softmax(dkQKT)V(1)\n\nwhere dk is a scaling factor, usually based on the individual head\ndimensionFor most Transformer implementations, Q, K, and V all have the same\nshape. or number of heads.\n\nThis process is illustrated by Jay Alammar in Figure 1 on the rightFigure 1 is\nabove if reading on mobile. and Figure 2 below.\n\nSelf-attention calculation in matrix form with attention denoted as Z. From\nThe Illustrated Transformer.\n\nThe resulting Attention(Q,K,V) is usually passed through a linear layer WO\nprojection\n\nOutput=Attention(Q,K,V)WO\n\nas the final step of the Attention layer.\n\nFor all the math, Attention is simply a learned weighted average. Attention\nlearns to generate weights between tokens via queries XWQ and keys XWK. Those\nper-token weights are created by softmax(QKT/dk). The values XWV learn to\ncreate a token representation which can incorporate the weighted average of\nall the other tokens in the final dot product in the Attention layer\nsoftmax(...)V. When someone says a token attends to a second token, this means\nit\u2019s increasing the size of the second token\u2019s weight in the weighted average\nrelative to all the other tokens.\n\n# # Three Flavors of Attention\n\nThe three standard types of Attention layers introduced in Attention is All\nYou Need, are Bidirectional Attention, Causal Attention, and Cross\nAttentionBidirectional Attention can also be referred as \u201cfully-visible\u201d and\nCausal Attention as \u201cAutoregressive\u201d.. Both Bidirectional and Causal Attention\nare forms of Self-Attention, as they only apply Attention to one input\nsequence, while Cross Attention applies Attention on multiple inputsI will\nexplain each type of Attention in detail with code later in the post, so don\u2019t\nworry if this overview is a bit confusing..\n\nBidirectional Attention is used in encoder blocks in encoder-only models\n(BERTJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for Language\nUnderstanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), 4171\u20134186.\nDOI:10.18653/v1/N19-1423) or encoder-decoder models (BARTMike Lewis, Yinhan\nLiu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves\nStoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence\nPre-training for Natural Language Generation, Translation, and Comprehension.\nIn Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, 7871\u20137880. DOI:10.18653/v1/2020.acl-main.703). It allows the\nAttention mechanism to incorporate both prior and successive tokens,\nregardless of order. Bidirectional Attention is used when we want to capture\ncontext from the entire input, such as classification.\n\nCausal Attention is used in decoder blocks in decoder-only models (GPTAlec\nRadford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving\nlanguage understanding by generative pre-training. (2018). Retrieved from\nhttps://openai.com/research/language-unsupervised) or encoder-decoder models\n(BART). Unlike Bidirectional Attention, Causal Attention can only incorporate\ninformation from prior tokens into the current token. It cannot see future\ntokens. Causal Attention is used when we need to preserve the temporal\nstructure or a sequence, such as generating a next token based on prior\ntokens.\n\nCross Attention is used in cross blocks in encoder-decoder models (BART).\nUnlike Bidirectional and Causal Self-Attention, Cross Attention allows the\nAttention mechanism to incorporate a different sequence of tokens to the\ncurrent sequence of tokens. Cross Attention is used when we need to align two\nsequences, such as translating from one language or domain to another, or when\nwe want to incorporate multiple input types into one model, such as text and\nimages in diffusion models.\n\n# # Single Head Self-Attention\n\nA diagram of Single Head Self-Attention, or Scaled Dot Product Attention,\nwithout the last linear projection. From Attention is All You Need.We\u2019ll start\nour implementation of a Attention layer with the simplest Attention mechanism:\nSingle Head Bidirectional Self-Attention.\n\nRecall that we are implementing the following formal definition of Attention:\n\nQ=XWQK=XWKV=XWVAttention(Q,K,V)=softmax(dkQKT)VOutput=Attention(Q,K,V)WO\n\nWith Single Head Attention formally defined, let\u2019s implement it in code.\n\n## # Single Head Initialization\n\nIn a Transformer, each token is passed through the model as a vector. The\nhidden_size defines how wide the token vector is when it reaches the Attention\nmechanism.\n\nOur Attention layers will allow disabling bias terms for linear layers since\nrecent papers and models, such as CrammingJonas Geiping and Tom Goldstein.\n2022. Cramming: Training a Language Model on a Single GPU in One Day.\narXiv:2212.14034., PythiaStella Biderman, Hailey Schoelkopf, Quentin Anthony,\nHerbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu\nPurohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika,\nand Oskar van der Wal. 2023. Pythia: A Suite for Analyzing Large Language\nModels Across Training and Scaling. arXiv:2304.01373., and PaLMAakanksha\nChowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,\nParker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao,\nParker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan\nDu, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard,\nGuy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson,\nLiam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret\nZoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark\nOmernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat,\nAitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele\nCatasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav\nPetrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways.\narXiv:2204.02311., have shown that disabling the bias term results in little-\nto-no downstream performance dropIn a well-trained NLP Transformer, such as\nPythia, the bias term ends up being near or at zero, which is why we can\ndisable them without causing performance issues. while decreasing\ncomputational and memory requirements. However, to match the original\nTransformer implementation, we will set it on by default.\n\n    \n    \n    class SingleHeadAttention(nn.Module): def __init__(self, hidden_size: int, bias: bool = True, ):\n\nIn this implementation, we will start merge WQ, WK, and WV into single linear\nlayer, Wqkv, and unbind the outputs into Q, K, and V. This is accomplished by\nincreasing the output shape by a factor of threeAlternatively, we might use\ntwo layers, one for Q and one for both K and V for implementing KV caching..\nThis is mathematically equivalent to three individual linear layers, each with\nthe same input and output shape.\n\nIn Multi-Head Attention, each individual head size is smaller than the input\nsizeThis reduction to forces WQ, WK, and WV to learn a condensed\nrepresentation of the input tokens., so for Single Head we will arbitrarily\nset the head size to be four times smaller than the input dimension.\n\n    \n    \n    # linear layer to project queries, keys, values Wqkv = nn.Linear(hidden_size, (hidden_size//4)*3, bias=bias)\n\nLike the Wqkv layer, the Attention layer\u2019s linear projection layer also has an\noptional bias term. In the Single Head setting, it also projects the attended\ntokens back to the original shape.\n\n    \n    \n    # linear layer to project final output proj = nn.Linear(hidden_size//4, hidden_size, bias=bias)\n\nAnd that\u2019s it for the Attention initialization. The Attention mechanism in a\nTransformer only has twoOr four if WQ, WK, and WV are all separate linear\nlayers. layers of learnable parameters. Everything else in Attention is an\noperation on the output of the Wqkv linear layer.\n\n    \n    \n    def __init__(self, hidden_size: int, bias: bool = True, ): super().__init__() self.Wqkv = nn.Linear(hidden_size, (hidden_size//4)*3, bias=bias) self.Wo = nn.Linear(hidden_size//4, hidden_size, bias=bias)\n\nWith our linear projection layers created, we can now move to the forward\nmethod of the Attention layer.\n\n## # Single Head Forward\n\nAfter some input shape housekeeping, the first computational step is to\ngenerate our keys, queries, and values. First, we pass the input x through the\nWqkv linear layer. Then we reshape the Wqkv output to batch size, sequence\nlength, one dimension for Q, K, & V, and the head size Which in this example\nis the hidden size divided by four.. Finally, we split the single tensor into\nthe query, key, and value tensors using unbind, where each are of shape B, S,\nC//4.\n\n    \n    \n    # batch size (B), sequence length (S), input dimension (C) B, S, C = x.shape # split into queries, keys, & values of shape # batch size (B), sequence length (S), head size (HS) q, k, v = self.Wqkv(x).reshape(B, S, 3, C//4).unbind(dim=2)\n\nWith the queries, keys, and values generatedInstead of reshape(...).unbind(2)\nwe could also use einops: rearrange(x, \"b s (qkv c) -> qkv b s c\", qkv=3).\nAlthough it may be slower then using PyTorch ops., we can move to the\nmathematical operations of the Attention mechanism.\n\nRemember that Attention is defined by this equation:\n\nAttention(Q,K,V)=softmax(dkQKT)V\n\nSo first, we need to transpose K and take the dot product of Q and KT.\n\n    \n    \n    # calculate dot product of queries and keys of shape # (B, S, S) = (B, S, HS) @ (B, HS, S) attn = q @ k.transpose(-2, -1)\n\nNext, we need to scale the outputs of the QKT by dk\n\n    \n    \n    # scale by square root of head dimension attn = attn / math.sqrt(k.size(-1))\n\nNow that we have scaled QKT, it\u2019s time to calculate the token Attention weight\nusing softmax.\n\n    \n    \n    # apply softmax to get attention weights attn = attn.softmax(dim=-1)\n\nSingle Head Self-Attention weights on a sequence of tokens. Darker shading\nmeans a higher Self-Attention weight. From The Illustrated Transformer. This\nSoftmax output of QKT/dk is how the Attention mechanism weights the strength\nof the relationship between each pair of tokens. Where higher Softmax values\nmeans Attention is placing more importance on these pairs of tokens and lower\nvalues are deemed less important.\n\nThis is illustrated in figure 3, where the Attention mechanism has learned to\nassociate the word \u201cit\u201d to \u201canimal\u201d in the sentence, \u201cthe animal didn\u2019t cross\nthe street because it was too tired.\u201d\n\nNote: Softmax is a function which converts a vector of inputs into a vector of\nprobabilities which are constrained between (0,1), sum to one, and reflect the\nrelative scale of each individual input. Or, more formally,\n\nsoftmax(xi)=\u2211j=1Kexjexi,for i=1,...,K\n\nNext we matrix multiply the Attention weights with our value matrix V which\napplies the Attention weights to our propagating token embeddingsRemember that\nV is of shape B, S, HS, where S, HS are the projected token embeddings for our\nsequence of length S..\n\n    \n    \n    # dot product attention weights to values # (B, S, HS) = (B, S, S) @ (B, S, HS) x = attn @ v\n\nFinally, we project the Attention results through the final linear layer to\nget the Attention layer output, which is back to the input shape.\n\n    \n    \n    # apply final linear layer to get output (B, S, C) return self.Wo(x)\n\nAnd there you have it. A simple rendition of Single Head Bidirectional\nAttention in code.\n\n    \n    \n    class SingleHeadAttention(nn.Module): def __init__(self, hidden_size: int, bias: bool = True, ): super().__init__() self.Wqkv = nn.Linear(hidden_size, (hidden_size//4)*3, bias=bias) self.Wo = nn.Linear(hidden_size//4, hidden_size, bias=bias) def forward(self, x:Tensor): B, S, C = x.shape q, k, v = self.Wqkv(x).reshape(B, S, 3, C//4).unbind(dim=2) attn = q @ k.transpose(-2, -1) attn = attn / math.sqrt(k.size(-1)) attn = attn.softmax(dim=-1) x = attn @ v return self.Wo(x)\n\n# # Multi-Head Self-Attention\n\nTwo-head Self-Attention weights on a sequence of tokens. Darker shading means\na higher Self-Attention weight. From The Illustrated Transformer.Now that we\nhave our Single Head Self-Attention code understood and working, we can update\nit to Bidirectional Multi-Head Self-Attention. But first, there is an obvious\nquestion which needs to be answered: Why do we want Multi-Head Attention in\nthe first place?\n\nThe answer is two parted. First, by projecting the input to multiple randomly\ninitialized heads the Transformer will have multiple representation subspaces\nfor the same input, giving each Transformer layer the ability to\nsimultaneously learn different nuances for the same input tokens.\n\nSecond, multiple heads allow the Attention mechanism to jointly attended to\nmultiple tokens at the same timeAlthough there is a paper which suggests that\nenough layers of Single Head Attention can perform the same function.. Even if\na single weighted average is well behavedA worst case scenario for a Single\nHeaded Attention is the Softmax output only attends to itself or one other\ntoken, with all the other tokens contributing a miniscule amount., it still\nlimits the ability to focus on multiple tokens. This ability to attend to\nmultiple tokens at once is especially important as the context windowThe\ncontext window is the maximum number of tokens in the input sequence that the\nmodel was trained or fine-tuned on. of recent LLMs expands to 4,000, 8,000,\n32,000, 60,000, and even 100,000 tokens.\n\nA diagram of Multi-Head Self-Attention where each Scaled Dot Product Attention\nis per head Single Head Attention. From Attention is All You Need.Formally,\nMulti-Head Attention creates one query Qh, key Kh, and value Vh per head h,\ncalculates the scaled dot-product Attention per head Attention(Qh,Kh,Vh),\nconcatenates all the Attention outputs back into one tensor MultiHead(Q,K,V),\nbefore passing the Multi-Head Attention output through the final linear layer\nWO:\n\nQh=XWhQKh=XWhKVh=XWhVAttention(Qh,Kh,Vh)=softmax(dhQhKhT)VhMultiHead(Q,K,V)=concat(Attention(Qh,Kh,Vh),\nfor all h)Output=MultiHead(Q,K,V)WO\n\nWith Multi-Head Attention formally defined, let\u2019s implement it in code.\n\n## # Multi-Head Initialization\n\nOur Multi-Head Attention __init__ will look quite similar to the Single Head\nimplementation, except with a new argument num_heads to set the number of\nheads.\n\n    \n    \n    class MultiHeadAttention(nn.Module): def __init__(self, hidden_size: int, num_heads: int, bias: bool = True, ):\n\nInstead of projecting the input to a fixed size like in Single Head Attention,\nwe will project the input to hidden_size / num_heads, so we want to assert\nthat the shapes match.\n\n    \n    \n    # input dimension must be divisible by num_heads assert hidden_size % num_heads == 0 # number of attention heads self.nh = num_heads\n\nWe now have all the learnable parameters for our Multi-Head Attention\nmechanismSince each head is a subset of the hidden_size, we can remove the\nprojection reduction added to Single Head Attention..\n\n    \n    \n    def __init__(self, hidden_size: int, num_heads: int, bias: bool = True, ): # input dimension must be divisible by num_heads assert hidden_size % num_heads == 0 # number of attention heads self.nh = num_heads super().__init__() # linear layer to project queries, keys, values self.Wqkv = nn.Linear(hidden_size, hidden_size*3, bias=bias) # linear layer to project final output self.Wo = nn.Linear(hidden_size, hidden_size, bias=bias)\n\n## # Multi-Head Forward\n\nOur Multi-Head forward method is largely the same, with a few changes to\naccount for the multiple heads.\n\nOur input sequence is projected through the linear Wqkv layer as before. Then\nwe need to reshape and transpose the output to batch size, number of heads,\nQh, Kh, & Vh, sequence length, and the head size, which in most Transformers\nis the embedding shape divided by the number of heads. Then we unbind our\nreshaped and transposed output to the separate queries, keys, & values, each\nof shape B, NH, S, HSLike the Single Head Attention, we could also use einops:\nrearrange(x, \"b s (qkv nh hs) -> qkv b nh s hs\", qkv=3, nh=self.nh). Although\nit may be slower than using PyTorch ops..\n\n    \n    \n    # batch size (B), sequence length (S), input dimension (C) B, S, C = x.shape # split into queries, keys, & values of shape # batch size (B), num_heads (NH), sequence length (S), head size (HS) x = self.Wqkv(x).reshape(B, S, 3, self.nh, C//self.nh) q, k, v = x.transpose(3, 1).unbind(dim=2)\n\nThe Attention mechanism is exactly the same as the Single Head code, but the\ndifference in tensor shape means we are calculating the Softmax individually\nper each headThe input shape at the Softmax layer is B, NH, S, S and we are\napplying it on the last dimension..\n\n    \n    \n    # calculate dot product of queries and keys # (B, NH, S, S) = (B, NH, S, HS) @ (B, NH, HS, S) attn = q @ k.transpose(-2, -1) # scale by square root of head dimension attn = attn / math.sqrt(k.size(-1)) # apply softmax to get attention weights attn = attn.softmax(dim=-1)\n\nOur remaining steps are to matrix multiply the Attention outputs with Vh, then\nconcatenate the per-head Attention into one output of our input shape.\n\nWe perform this by transposing the heads and sequences then reshaping to B, S,\nCWe could also use einops: rearrange(x, \"b nh s hs -> b s (nh hs)\").. This is\nmechanically the same as a concatenation, without the requirement of creating\na new tensorAlong with the expensive increase in memory and read+write.\n\n    \n    \n    # dot product attention weights with values # (B, NH, S, HS) = (B, NH, S, S) @ (B, NH, HS, S) x = attn @ v # transpose heads & sequence then reshape back to (B, S, C) x = x.transpose(1, 2).reshape(B, S, C) # apply final linear layer to get output return self.Wo(x)\n\nWith all the pieces defined, we now have a working, albeit incomplete,\nimplementation of Bidirectional Self-Attention.\n\n    \n    \n    class MultiHeadAttention(nn.Module): def __init__(self, hidden_size: int, num_heads: int, bias: bool = True, ): super().__init__() assert hidden_size % num_heads == 0 self.nh = num_heads self.Wqkv = nn.Linear(hidden_size, hidden_size * 3, bias=bias) self.Wo = nn.Linear(hidden_size, hidden_size, bias=bias) def forward(self, x: Tensor): B, S, C = x.shape x = self.Wqkv(x).reshape(B, S, 3, self.nh, C//self.nh) q, k, v = x.transpose(3, 1).unbind(dim=2) attn = q @ k.transpose(-2, -1) attn = attn / math.sqrt(k.size(-1)) attn = attn.softmax(dim=-1) x = attn @ v return self.Wo(x.transpose(1, 2).reshape(B, S, C))\n\nTechnically, we could use this in an encoder model like BERT, but there are a\nfew more items we need to define before all the original Attention is All You\nNeed Bidirectional Self-Attention is fully recreated.\n\n# # Adding Dropout\n\nThe original Attention Is All You Need Attention implementation has dropout\nfor both the Attention weights and the final projection layer. They are simple\nto add, with a default dropout probability of 10 percent.\n\n    \n    \n    def __init__(self, hidden_size: int, num_heads: int, attn_drop: float = 0.1, out_drop: float = 0.1, bias: bool = True, ): super().__init__() assert hidden_size % num_heads == 0 self.nh = num_heads self.Wqkv = nn.Linear(hidden_size, hidden_size * 3, bias=bias) self.Wo = nn.Linear(hidden_size, hidden_size, bias=bias) # attention dropout layer to prevent overfitting self.attn_drop = nn.Dropout(attn_drop) # final output dropout layer to prevent overfitting self.out_drop = nn.Dropout(out_drop)\n\nThe Attention dropout is placed directly after the Attention Softmax is\ncalculated. Note that this will completely remove whole tokens from the\nAttention weightsHugging Face implementations note that this seems odd, but\nit\u2019s what Attention Is All You Need does., as the shape is B, NH, S, S.\n\n    \n    \n    # apply softmax to get attention weights (B, NH, S, S) attn = attn.softmax(dim=-1) # apply dropout to attention weight attn = self.attn_drop(attn)\n\nThe final projection layer dropout is applied after the projection layer is\ncalculated, and it removes a fraction of the token embedding activations, not\ntokens themselves.\n\n    \n    \n    # apply final linear layer to get output (B, S, C) return self.out_drop(self.Wo(x))\n\n# # Bidirectional Attention\n\nThe last thing our Bidirectional Multi-Head Self-Attention layer needs to be\ncomplete is the input Attention mask.\n\nAs Bidirectional Attention is supposed to attend to all tokens in the input\nsequence, the Attention mask primarily exists to support batching different\nlength sequencesAlthough with Nested Tensors on the horizon, the necessity of\nmasking might diminish.. Typically, an encoder or encoder-decoder Transformer\nwill have a pad token, but we don\u2019t want this pad token to interact with any\nof the sequence tokens. That is where the Attention mask comes in.\n\n    \n    \n    # boolean mask of shape (batch_size, sequence_length) # where True is masked and False is unmasked def forward(self, x: Tensor, mask: BoolTensor):\n\nOur Attention mask is applied right before the Softmax calculates the\nAttention weights. For simplicity and conciseness, we are requiring a mask to\nbe passed to the Attention layer.\n\n    \n    \n    # scale by square root of output dimension attn = attn / math.sqrt(k.size(-1)) # reshape and mask attention scores attn = attn.masked_fill(mask.view(B, 1, 1, S), float('-inf')) # apply softmax to get attention weights attn = attn.softmax(dim=-1)\n\nBy setting the padding tokens to a value of negative infinityPyTorch will take\ncare of converting this to the correct value if we are training in mixed\nprecision with autocast. this means softmax will weight these tokens as zero\nwhen calculating the Attention results.\n\nThe entire BidirectionalAttention layer can be seen below, with a fully\ncommented version in commented-transformers.\n\n    \n    \n    class BidirectionalAttention(nn.Module): def __init__(self, hidden_size: int, num_heads: int, attn_drop: float = 0.1, out_drop: float = 0.1, bias: bool = True, ): super().__init__() assert hidden_size % num_heads == 0 self.nh = num_heads self.Wqkv = nn.Linear(hidden_size, hidden_size * 3, bias=bias) self.Wo = nn.Linear(hidden_size, hidden_size, bias=bias) self.attn_drop = nn.Dropout(attn_drop) self.out_drop = nn.Dropout(out_drop) def forward(self, x: Tensor, mask: BoolTensor): B, S, C = x.shape x = self.Wqkv(x).reshape(B, S, 3, self.nh, C//self.nh) q, k, v = x.transpose(3, 1).unbind(dim=2) attn = q @ k.transpose(-2, -1) attn = attn / math.sqrt(k.size(-1)) attn = attn.masked_fill(mask.view(B, 1, 1, S), float('-inf')) attn = attn.softmax(dim=-1) attn = self.attn_drop(attn) x = attn @ v x = x.transpose(1, 2).reshape(B, S, C) return self.out_drop(self.Wo(x))\n\n# # Causal Self-Attention\n\nAn illustration of causal masking on a single input sequence. Gray shading\nindicates a masked token. From The Illustrated GPT2.For Causal Attention, we\nneed to ensure that current tokens can only attend to past tokens, and not\nfuture tokens in the sequence. We can accomplish this through masking.\n\nWe will use an upper triangular matrix for the Causal Attention mask to ensure\nthe current token can only attend to past tokens no matter where the current\ntoken is in the sequence. Figure 7 illustrates how the upper triangular matrix\nis applied on a per-token level, where the diagonal, (1,1), (2,2), etc, is the\ncurrent token in the sequence. Green shaded tokens, both the current token and\ntokens to the left of the current token, are unmasked and can be attended too,\nwhile grey shaded tokens to the right of the current token are masked and\ncannot used in the Attention mechanism.\n\nWe\u2019ll create a permanent causal_mask of shape [context_size, context_size] in\nour CausalAttention initialization method, where context_size is the maximum\ncontext length of our Transformer. To match our padding Attention maskWhere\nTrue is masked and False is unmasked. we will create a matrix of boolean ones.\nThen we use triu to convert our boolean matrix of True values into an upper\ntriangular matrix, with the upper triangle masked (True) and lower triangle\nunmasked (False). Because we want the diagonal of the matrix to be unmasked,\nwe shift the triu diagonal one to the upper-right using diagonal=1.\n\nThen we reshape the input to be broadcastable across the dimensions of QKT,\nwhich is B, NH, S, S, and assign it to a PyTorch bufferThis insures the values\nare not considered parameters, and thus will not be modified by an optimizer..\n\n    \n    \n    # causal mask to ensure that attention is not applied to future tokens # where context_size is the maximum sequence length of the transformer self.register_buffer('causal_mask', torch.triu(torch.ones([context_size, context_size], dtype=torch.bool), diagonal=1) .view(1, 1, context_size, context_size))\n\nThen in our CausalAttention forward method, we use masked_fill again to apply\nthe causal mask to our intermediate Attention results before applying softmax\nto calculate the Attention weights.\n\n    \n    \n    # scale by square root of output dimension attn = attn / math.sqrt(k.size(-1)) # apply causal mask attn = attn.masked_fill(self.causal_mask[:, :, :S, :S], float('-inf')) # apply softmax to get attention weights attn = attn.softmax(dim=-1)\n\nWe can also combine the causal Attention masking with our input Attention\nmask. Since both masks are booleanIn boolean addition, True + False = True,\nand adding True or False to themselves results in no change., we can add them\ntogether thanks to PyTorch broadcasting.\n\n    \n    \n    # apply input and causal mask combined_mask = self.causal_mask[:, :, :S, :S] + mask.view(B, 1, 1, S) attn = attn.masked_fill(combined_mask, float('-inf'))\n\nWith those two changes, we have modified Bidirectional Attention into Causal\nAttention. The entire CausalAttention layer can be seen below, with a fully\ncommented version in commented-transformers.\n\n    \n    \n    class CausalAttention(nn.Module): def __init__(self, hidden_size: int, num_heads: int, context_size: int, attn_drop: float = 0.1, out_drop: float = 0.1, bias: bool = True, ): super().__init__() assert hidden_size % num_heads == 0 self.nh = num_heads self.Wqkv = nn.Linear(hidden_size, hidden_size * 3, bias=bias) self.Wo = nn.Linear(hidden_size, hidden_size, bias=bias) self.attn_drop = nn.Dropout(attn_drop) self.out_drop = nn.Dropout(out_drop) self.register_buffer('causal_mask', torch.triu(torch.ones([context_size, context_size], dtype=torch.bool), diagonal=1) .view(1, 1, context_size, context_size)) def forward(self, x: Tensor, mask: BoolTensor): B, S, C = x.shape x = self.Wqkv(x).reshape(B, S, 3, self.nh, C//self.nh) q, k, v = x.transpose(3, 1).unbind(dim=2) attn = q @ k.transpose(-2, -1) attn = attn / math.sqrt(k.size(-1)) combined_mask = self.causal_mask[:, :, :S, :S] + mask.view(B, 1, 1, S) attn = attn.masked_fill(combined_mask, float('-inf')) attn = attn.softmax(dim=-1) attn = self.attn_drop(attn) x = attn @ v x = x.transpose(1, 2).reshape(B, S, C) return self.out_drop(self.Wo(x))\n\n# # Cross Attention\n\nAll of the versions of Attention we\u2019ve covered so far have been\nimplementations of Self-Attention, which is applying the Attention mechanism\non the same input sequence. Cross Attention differs from Self-Attention in\nthat the Attention mechanism applies across more than one input sequences.\n\nThis means our formal definition of Attention needs some modifications. For\nthis post, I will follow the original Cross Attention implementation, where\nthe query is created from the first sequence X and both the keys and values\nare from the second sequence Y. However, these are not set in stone, with\nother Transformer models adopting different allocations of the first and\nsecond sequence across the queries, keys, and values.\n\nThe formal definition for Cross Attention is the sameFor conciseness, I\nremoved the separate concatenation step and bundled it into the CrossAttention\noperation., but with QX, KY, and VY representing the Multi-Head queries, keys,\nand values for inputs X and Y, respectively.\n\nQX=XWhQKY=YWhKVY=YWhVCrossAttention(QX,KY,VY)=softmax(dhQXKYT)VYOutput=CrossAttention(QX,KY,VY)WO\n\nOur Cross Attention __init__ method differs from prior initializations due to\nrequiring separate linear layers for queries and the keys and values.\n\n    \n    \n    # linear layer to project queries from decoder input self.Wq = nn.Linear(hidden_size, hidden_size, bias=bias) # linear layer to project keys and values from encoder output self.Wkv = nn.Linear(hidden_size, hidden_size * 2, bias=bias)\n\nOur new forward method has two sequence inputs along with the mask. The\ndecoder input X, and the encoder input Y. We\u2019ll assume for sake of simplicity\nand code reuse that both inputs are the same shape.\n\n    \n    \n    def forward(self, x: Tensor, y: Tensor, mask: BoolTensor): # batch size, sequence length, input dimension B, S, C = x.shape # split into queries of shape (B, NH, S, HS) from decoder input q = self.Wq(x).reshape(B, S, self.nh, C//self.nh).transpose(1, 2) # split into keys and values of shape (B, NH, S, HS) from encoder output y = self.Wkv(y).reshape(B, S, 2, self.nh, C//self.nh) k, v = y.transpose(3, 1).unbind(dim=2)\n\nSince we are assuming this Cross Attention is in the decoder side of an\nencoder-decoder transformer, everything after creating the queries, keys, and\nvalues is the same as our Causal Attention implementation.\n\nIf we were creating Cross Attention for an encoder style Attention layer, we\nwould remove the causal mask and optionally keep the input mask if appropriate\nfor our task.\n\nThe entire CausalCrossAttention layer can be seen below, with a fully\ncommented version in the commented-transformers.\n\n    \n    \n    class CausalCrossAttention(nn.Module): def __init__(self, hidden_size: int, num_heads: int, context_size: int, attn_drop: float = 0.1, out_drop: float = 0.1, bias: bool = True, ): super().__init__() assert hidden_size % num_heads == 0 self.nh = num_heads self.Wq = nn.Linear(hidden_size, hidden_size, bias=bias) self.Wkv = nn.Linear(hidden_size, hidden_size * 2, bias=bias) self.Wo = nn.Linear(hidden_size, hidden_size, bias=bias) self.attn_drop = nn.Dropout(attn_drop) self.out_drop = nn.Dropout(out_drop) self.register_buffer('causal_mask', torch.triu(torch.ones([context_size, context_size], dtype=torch.bool), diagonal=1) .view(1, 1, context_size, context_size)) def forward(self, x: Tensor, y: Tensor, mask: BoolTensor): B, S, C = x.shape q = self.Wq(x).reshape(B, S, self.nh, C//self.nh).transpose(1, 2) y = self.Wkv(y).reshape(B, S, 2, self.nh, C//self.nh) k, v = y.transpose(3, 1).unbind(dim=2) attn = q @ k.transpose(-2, -1) attn = attn / math.sqrt(k.size(-1)) combined_mask = self.causal_mask + mask.view(B, 1, 1, S) attn = attn.masked_fill(combined_mask, float('-inf')) attn = attn.softmax(dim=-1) attn = self.attn_drop(attn) x = attn @ v x = x.transpose(1, 2).reshape(B, S, C) return self.out_drop(self.Wo(x))\n\n# # Conclusion\n\nIn this post, I have shown you how to implement all three main flavors of\nAttention in PyTorch: Bidirectional, Causal, and Cross Attention. You should\nnow be able to write your own version of Attention and understand any model-\nspecific Attention implementations.\n\nThere still are a few more items we need to create before we have a fully\nworking Transformer: the feed-forward network, positional encoding, and text\nembedding layers, to name a few. In the next post in this series, I will show\nyou how to create all of these in PyTorch and build the rest of the\nTransformer.\n\n# # References\n\n  1. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 4171\u20134186. DOI:10.18653/v1/N19-1423\n  2. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 7871\u20137880. DOI:10.18653/v1/2020.acl-main.703\n  3. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. (2018). Retrieved from https://openai.com/research/language-unsupervised\n  4. Jonas Geiping and Tom Goldstein. 2022. Cramming: Training a Language Model on a Single GPU in One Day. arXiv:2212.14034.\n  5. Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling. arXiv:2304.01373.\n  6. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling Language Modeling with Pathways. arXiv:2204.02311.\n\nPrevious\n\nHow to Quickly Finetune Your Transformer\n\nPerformance Tips for Faster Training\n\nWhile recent releases of language models have emphasized the large in Large\nLanguage Models, most everyday NLP work uses smaller language models,\nfinetuned on custom...\n\nNext\n\nCreating a Transformer From Scratch\n\nPart Two: The Rest of the Transformer\n\nIn this post, I will show you how to build the rest of the Transformer. By the\nend of this...\n\n\u00a9 2024 Benjamin Warner\n\n", "frontpage": false}
