{"aid": "40130031", "title": "Fine-tune Llama 3 with ORPO", "url": "https://huggingface.co/blog/mlabonne/orpo-llama-3", "domain": "huggingface.co", "votes": 1, "user": "victormustar", "posted_at": "2024-04-23 09:15:39", "comments": 0, "source_title": "Fine-tune Llama 3 with ORPO", "source_text": "Fine-tune Llama 3 with ORPO\n\nHugging Face\n\nBack to Articles\n\n# Fine-tune Llama 3 with ORPO\n\nCommunity Article Published April 22, 2024\n\nUpvote\n\n104\n\nmlabonne Maxime Labonne\n\nORPO is a new exciting fine-tuning technique that combines the traditional\nsupervised fine-tuning and preference alignment stages into a single process.\nThis reduces the computational resources and time required for training.\nMoreover, empirical results demonstrate that ORPO outperforms other alignment\nmethods on various model sizes and benchmarks.\n\nIn this article, we will fine-tune the new Llama 3 8B model using ORPO with\nthe TRL library. The code is available on Google Colab and in the LLM Course\non GitHub.\n\n## \u2696\ufe0f ORPO\n\nInstruction tuning and preference alignment are essential techniques for\nadapting Large Language Models (LLMs) to specific tasks. Traditionally, this\ninvolves a multi-stage process: 1/ Supervised Fine-Tuning (SFT) on\ninstructions to adapt the model to the target domain, followed by 2/\npreference alignment methods like Reinforcement Learning with Human Feedback\n(RLHF) or Direct Preference Optimization (DPO) to increase the likelihood of\ngenerating preferred responses over rejected ones.\n\nHowever, researchers have identified a limitation in this approach. While SFT\neffectively adapts the model to the desired domain, it inadvertently increases\nthe probability of generating undesirable answers alongside preferred ones.\nThis is why the preference alignment stage is necessary to widen the gap\nbetween the likelihoods of preferred and rejected outputs.\n\nNote how the probability of rejected responses increases during supervised\nfine-tuning (from the ORPO paper).\n\nIntroduced by Hong and Lee (2024), ORPO offers an elegant solution to this\nproblem by combining instruction tuning and preference alignment into a\nsingle, monolithic training process. ORPO modifies the standard language\nmodeling objective, combining the negative log-likelihood loss with an odds\nratio (OR) term. This OR loss weakly penalizes rejected responses while\nstrongly rewarding preferred ones, allowing the model to simultaneously learn\nthe target task and align with human preferences.\n\nLORPO=E(x,yw,yl)[LSFT+\u03bb\u22c5LOR] ORPO has been implemented in the major fine-\ntuning libraries, like TRL, Axolotl, and LLaMA-Factory. In the next section,\nwe will see how to use with TRL.\n\n## \ud83d\udcbb Fine-tuning Llama 3 with ORPO\n\nLlama 3 is the latest family of LLMs developed by Meta. The models were\ntrained on an extensive dataset of 15 trillion tokens (compared to 2T tokens\nfor Llama 2). Two model sizes have been released: a 70 billion parameter model\nand a smaller 8 billion parameter model. The 70B model has already\ndemonstrated impressive performance, scoring 82 on the MMLU benchmark and 81.7\non the HumanEval benchmark.\n\nLlama 3 models also increased the context length up to 8,192 tokens (4,096\ntokens for Llama 2), and potentially scale up to 32k with RoPE. Additionally,\nthe models use a new tokenizer with a 128K-token vocabulary, reducing the\nnumber of tokens required to encode text by 15%. This vocabulary also explains\nthe bump from 7B to 8B parameters.\n\nSamples from ORPO-DPO-mix-40k.\n\nORPO requires a preference dataset, including a prompt, a chosen answer, and a\nrejected answer. In this example, we will use mlabonne/orpo-dpo-mix-40k, a\ncombination of the following high-quality DPO datasets:\n\n  * argilla/distilabel-capybara-dpo-7k-binarized: highly scored chosen answers >=5 (2,882 samples)\n  * argilla/distilabel-intel-orca-dpo-pairs: highly scored chosen answers >=9, not in GSM8K (2,299 samples)\n  * argilla/ultrafeedback-binarized-preferences-cleaned: highly scored chosen answers >=5 (22,799 samples)\n  * argilla/distilabel-math-preference-dpo: highly scored chosen answers >=9 (2,181 samples)\n  * unalignment/toxic-dpo-v0.2 (541 samples)\n  * M4-ai/prm_dpo_pairs_cleaned (7,958 samples)\n  * jondurbin/truthy-dpo-v0.1 (1,016 samples)\n\nThanks to argilla, unalignment, M4-ai, and jondurbin for providing the source\ndatasets.\n\nAs per usual, let's start by installing the required libraries:\n\n    \n    \n    pip install -U transformers datasets accelerate peft trl bitsandbytes wandb\n\nOnce it's installed, we can import the necessary libraries and log in to W&B\n(optional):\n\n    \n    \n    import gc import os import torch import wandb from datasets import load_dataset from google.colab import userdata from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training from transformers import ( AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline, ) from trl import ORPOConfig, ORPOTrainer, setup_chat_format wb_token = userdata.get('wandb') wandb.login(key=wb_token)\n\nIf you have a recent GPU, you should also be able to use the Flash Attention\nlibrary to replace the default eager attention implementation with a more\nefficient one.\n\n    \n    \n    if torch.cuda.get_device_capability()[0] >= 8: !pip install -qqq flash-attn attn_implementation = \"flash_attention_2\" torch_dtype = torch.bfloat16 else: attn_implementation = \"eager\" torch_dtype = torch.float16\n\nIn the following, we will load the Llama 3 8B model in 4-bit precision thanks\nto bitsandbytes. We then set the LoRA configuration using PEFT for QLoRA. I'm\nalso using the convenient setup_chat_format() function to modify the model and\ntokenizer for ChatML support. It automatically applies this chat template,\nadds special tokens, and resizes the model's embedding layer to match the new\nvocabulary size.\n\nNote that you need to submit a request to access meta-llama/Meta-Llama-3-8B\nand be logged in to your Hugging Face account. Alternatively, you can load\nungated copies of the model, like NousResearch/Meta--Llama-3-8B.\n\n    \n    \n    # Model base_model = \"meta-llama/Meta-Llama-3-8B\" new_model = \"OrpoLlama-3-8B\" # QLoRA config bnb_config = BitsAndBytesConfig( load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch_dtype, bnb_4bit_use_double_quant=True, ) # LoRA config peft_config = LoraConfig( r=16, lora_alpha=32, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj'] ) # Load tokenizer tokenizer = AutoTokenizer.from_pretrained(base_model) # Load model model = AutoModelForCausalLM.from_pretrained( base_model, quantization_config=bnb_config, device_map=\"auto\", attn_implementation=attn_implementation ) model, tokenizer = setup_chat_format(model, tokenizer) model = prepare_model_for_kbit_training(model)\n\nNow that the model is ready for training, we can take care of the dataset. We\nload mlabonne/orpo-dpo-mix-40k and use the apply_chat_template() function to\nconvert the \"chosen\" and \"rejected\" columns into the ChatML format. Note that\nI'm only using 1,000 samples and not the entire dataset, as it would take too\nlong to run.\n\n    \n    \n    dataset_name = \"mlabonne/orpo-dpo-mix-40k\" dataset = load_dataset(dataset_name, split=\"all\") dataset = dataset.shuffle(seed=42).select(range(1000)) def format_chat_template(row): row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False) row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False) return row dataset = dataset.map( format_chat_template, num_proc= os.cpu_count(), ) dataset = dataset.train_test_split(test_size=0.01)\n\nFirst, we need to set a few hyperparameters:\n\n  * learning_rate: ORPO uses very low learning rates compared to traditional SFT or even DPO. This value of 8e-6 comes from the original paper, and roughly corresponds to an SFT learning rate of 1e-5 and a DPO learning rate of 5e-6. I would recommend increasing it around 1e-6 for a real fine-tune.\n  * beta: It is the $\\lambda$ parameter in the paper, with a default value of 0.1. An appendix from the original paper shows how it's been selected with an ablation study.\n  * Other parameters, like max_length and batch size are set to use as much VRAM as available (~20 GB in this configuration). Ideally, we would train the model for 3-5 epochs, but we'll stick to 1 here.\n\nFinally, we can train the model using the ORPOTrainer, which acts as a\nwrapper.\n\n    \n    \n    orpo_args = ORPOConfig( learning_rate=8e-6, beta=0.1, lr_scheduler_type=\"linear\", max_length=1024, max_prompt_length=512, per_device_train_batch_size=2, per_device_eval_batch_size=2, gradient_accumulation_steps=4, optim=\"paged_adamw_8bit\", num_train_epochs=1, evaluation_strategy=\"steps\", eval_steps=0.2, logging_steps=1, warmup_steps=10, report_to=\"wandb\", output_dir=\"./results/\", ) trainer = ORPOTrainer( model=model, args=orpo_args, train_dataset=dataset[\"train\"], eval_dataset=dataset[\"test\"], peft_config=peft_config, tokenizer=tokenizer, ) trainer.train() trainer.save_model(new_model)\n\nTraining the model on these 1,000 samples took about 2 hours on an L4 GPU.\nLet's check the W&B plots:\n\nWhile the loss goes down, the difference between the chosen and rejects\nanswers is not clear: the average margin and accuracy are only slightly above\nzero and 0.5, respectively.\n\nIn the original paper, the authors trained models on the Anthropic/hh-rlhf\ndataset (161k samples) for 10 epochs, which is a lot longer than our quick\nrun. They also experimented with Llama 3 and kindly shared their logs with me\n(thanks Jiwoo Hong).\n\nTo end this tutorial, let's merge the QLoRA adapter with the base model and\npush it to the Hugging Face Hub.\n\n    \n    \n    # Flush memory del trainer, model gc.collect() torch.cuda.empty_cache() # Reload tokenizer and model tokenizer = AutoTokenizer.from_pretrained(base_model) model = AutoModelForCausalLM.from_pretrained( base_model, low_cpu_mem_usage=True, return_dict=True, torch_dtype=torch.float16, device_map=\"auto\", ) model, tokenizer = setup_chat_format(model, tokenizer) # Merge adapter with base model model = PeftModel.from_pretrained(model, new_model) model = model.merge_and_unload() model.push_to_hub(new_model, use_temp_dir=False) tokenizer.push_to_hub(new_model, use_temp_dir=False)\n\nCongrats, we finished this quick fine-tune of Llama 3:\nmlabonne/OrpoLlama-3-8B. You can play with it using this Hugging Face Space\n(here's a notebook to make your own). Although the model is undertrained, as\nhighlighted by the W&B curves, I ran some evaluations on Nous' benchmark suite\nusing LLM AutoEval.\n\nModel| Average| AGIEval| GPT4All| TruthfulQA| Bigbench  \n---|---|---|---|---|---  \nteknium/OpenHermes-2.5-Mistral-7B \ud83d\udcc4| 52.42| 42.75| 72.99| 52.99| 40.94  \nmeta-llama/Meta-Llama-3-8B-Instruct \ud83d\udcc4| 51.34| 41.22| 69.86| 51.65| 42.64  \nmistralai/Mistral-7B-Instruct-v0.1 \ud83d\udcc4| 49.15| 33.36| 67.87| 55.89| 39.48  \nmlabonne/OrpoLlama-3-8B \ud83d\udcc4| 46.76| 31.56| 70.19| 48.11| 37.17  \nmeta-llama/Meta-Llama-3-8B \ud83d\udcc4| 45.42| 31.1| 69.95| 43.91| 36.7  \n  \nOur ORPO fine-tune is actually pretty decent and improves the base model's\nperformance on every benchmark. This is encouraging and likely means that a\nfine-tune on the entire 40k samples would yield great results.\n\nThis is an exciting time for the open-source community, with more and more\nhigh-quality open-weight models being released. The gap between closed-source\nand open-weight models is slowly closing, and fine-tuning is an essential tool\nto get the best performance for your use cases.\n\n## Conclusion\n\nIn this article, we introduced the ORPO algorithm and explained how it unifies\nthe SFT and preference alignment stages into a single process. Then, we used\nTRL to fine-tune a Llama 3 8B model on a custom preference dataset. The final\nmodel shows encouraging results and highlights ORPO's potential as a new fine-\ntuning paradigm.\n\nI hope it was useful, and I recommend running the Colab notebook to fine-tune\nyour own Llama 3 models. In future articles, we will see how to create high-\nquality datasets \u2014 a point that is often overlooked. If you liked this\narticle, please follow me on Hugging Face and Twitter @maximelabonne.\n\n## References\n\n  * J. Hong, N. Lee, and J. Thorne, ORPO: Monolithic Preference Optimization without Reference Model. 2024.\n  * L. von Werra et al., TRL: Transformer Reinforcement Learning. GitHub, 2020. [Online]. Available: https://github.com/huggingface/trl\n  * Bartolome, A., Martin, G., & Vila, D. (2023). Notus. In GitHub Repository. GitHub. https://github.com/argilla-io/notus\n  * AI at Meta, Introducing Meta Llama 3, 2024.\n\nUpvote\n\n104\n\n", "frontpage": false}
