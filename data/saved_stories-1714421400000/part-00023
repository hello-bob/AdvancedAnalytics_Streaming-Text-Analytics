{"aid": "40197544", "title": "Can we RAG he whole web?", "url": "https://philippeoger.com/pages/can-we-rag-the-whole-web/", "domain": "philippeoger.com", "votes": 2, "user": "jeanloolz", "posted_at": "2024-04-29 12:43:20", "comments": 0, "source_title": null, "source_text": "# Can we RAG the whole web?\n\nRAG, or Retrieval-Augmented Generation, is a method where a language model\nsuch as ChatGPT first searches for useful information in a large database and\nthen uses this information to improve its responses.\n\nThis article implies some prior knowledge on vector embeddings. If you\u2019re not\nquite sure what those are, I strongly recommend this article from Simon\nWillison on the matter, it will help get a better grasp on this blog post.\n\nAs I was reading Anyscale.com benchmark analysis on RAG, a question came to\nme: what would be the most feasible way to vectorize the whole web, and let\nLLM query specific domains or groups of domains when needed? So, here is my,\nmost likely, dead in a water google-killer idea, that I believe is worth\nsharing with the community. It has also been an interesting writing that led\nme somewhere I did not foresee when I started this article.\n\n## In a nutshell\n\nOur objective is to be able to get such prompt from an AI assistant:\n\nUSER:\n\nHi Mistral, Who won the match between FC Barcelona and Madrid last night?\n\nASSISTANT:\n\nIt appears FC Barcelona won 2 - 0 against Madrid:\n\n  * Goal 1 from Messi at minute 49\n  * Goal 2 from Messi at minute 87\n\nI used information found from the following sources. Give them a visit if you\nwant a deeper analysis of the game:\n\n  * http://footballfanatics.com/1324256.htm\n  * http://lequipe.fr/123456.thml\n\nAn LLM on its own is not capable of answering this sort of question as a model\nis only re-trained on new data every once in a while, and has not been trained\nyet on \u201cyesterday\u2019s\u201d data. While continuous training for a model is an active\narea of research, the cheapest route for getting this sort of prompt today is\nvia RAG.\n\n## Problematic: the internet is vast\n\nGoogle index is made of 50 billion pages, while Bing has 4 billion in its\nindex. This is a lot of data and the idea of creating a crawler of this\nmagnitude is a bit daunting. A simpler approach could be using XML sitemaps\nthat a domain may share on the Robots.txt to get every url from a website that\nthey specifically want to index on search engines. This has the advantages of\ngetting every url without having to crawl the domain. But even with a perfect\nXML parser that gives a huge index of urls, we would still need to make a\nrequest for each url found, extract the main content, chunk that content into\nsmaller pieces, tokenize it, and get the embeddings for each chunk. Doing this\nfor a few urls is easy but doing it for billions of urls starts to get tricky\nand expensive (although not completely out of reach).\n\nAnother and bigger problem with said implementation is that at no point did I\nask permission to use this content. We sent some crawlers to domains,\noverloaded their servers, took the content and stored it in our own database\nwith no intention of letting others see it or use it. With this\nimplementation, we\u2019re just taking what is not ours for our own good.\n\n## Open data at domain level\n\nThe XML sitemap that I mentioned earlier is a protocol that was invented by\nGoogle back in 2005 and is widely adopted among the web as a way to indicate\nurls to crawl. A similar protocol also exists for sharing full content: RSS\nfeeds. Traditionally, an RSS feed would contain the last 10 or 20 articles of\na blog, and were not usually used to contain all the content of your domain\n(although it could). Those have existed for many years but have lost steam\nover the years. It was, however, an excellent way to keep up to date with your\nfavourite blog.\n\nI have been wondering for a while now if a new protocol should exist to share\ncontent and matching embeddings from a domain in the form of a dataset. A\ndomain could decide to expose its content in its entirety or partially and,\npotentially, include associated tokens or embeddings, so it could be\nintegrated in any RAG implementation, as fast as possible, with no extra\neffort for each implementation.\n\nAfter ruminating on this idea for a bit, I\u2019ve decided to experiment using\nSQLite. SQLite keeps an entire relational database contained within a single\nfile, allowing for straightforward deployment and minimal setup while still\ndelivering powerful features natively or via extensions. It absolutely shines\nin the context of a read-only database, which is what we need, and because it\nis just one file, it\u2019s a perfect way to share datasets.\n\n## Content sharing proposals\n\nI would propose that this dataset is by default to be found on the path\n/contentmap.db and for the sake of this article, I already created this\ndataset for my blog, so you can download it and see for yourself at\nphilippeoger.com/contentmap.db.\n\nThe easiest and most straightforward way to create a content dataset is a\ntable with 2 columns only: the url and the matching content as raw text. This\nis the DDL currently applied for the contentmap.db of my blog :\n\n    \n    \n    create table content ( url text, content text );\n\nIf you run a blog that has so far 100 posts, that would mean 100 rows in the\ndataset, one for each blog article currently on your domain. This is the\ncontent in its purest form, and if such a dataset existed on every domain, it\nwould most likely reduce server loads as well, as crawlers who account for\nhalf the traffic online would not need to go through every url anymore to get\nthe content. A simple download of contentmap.db would give you all the content\nof the domain already.\n\n## Implementing RAG\n\nHaving a table of content is a perfect way to start building a RAG\nimplementation. Typically, RAG is implemented with a vector similarity search\nalgorithm, and thankfully, SQLite has an extension written by Alex Garcia,\ncalled sqlite-vss, that does exactly that for us. I had the pleasure to\nintegrate sqlite-vss into langchain as an option for a vector store so you can\nsee how to use it in those examples. The code is rather simple, but yet, if\nsomeone had to go and build this database to follow the protocol, it would be\nbest to have code that does it for us.\n\n### Pip install contentmap\n\nI built for fun a python library that can help anyone create their own\ncontentmap.db in just a few lines of code, that may or may not include vector\nsearch (your choice). You can simply install it with \u201cpip install contentmap\u201d,\nand you can check the code on github repo.\n\nAssuming we already have the contents, all we need to do is create data in the\nfollowing format, and pass it to the ContentMapCreator class to build the DB\nfor us.\n\n    \n    \n    from contentmap import ContentMapCreator contents = [ { \"url\": \"https:/example.com/foo\", \"content\": \"this is a foo article\" }, { \"url\": \"https:/example.com/bar\", \"content\": \"this is a bar article\" }, ] cm = ContentMapCreator(contents=contents) cm.build()\n\nThis will build an sqlite database called contentmap.db, with a table called\ncontent as described above.\n\nIf you want to add the vector similarity search, it\u2019s the same code with an\nextra parameter.\n\n    \n    \n    cm = ContentMapCreator(contents=contents, include_vss=True) cm.build()\n\nThe include_vss=True simply use sqlite-vss modules from langchain to add all\nthe tables and virtual tables needed for vector search capabilities. If you\nwant to search using vss on the newly created dataset, simply do it as follow:\n\n    \n    \n    from contentmap.vss import ContentMapVSS vss = ContentMapVSS(db_file=\"contentmap.db\") data = vss.similarity_search(query=\"who is Mistral ai company?\", k=4)\n\n### Using your XML sitemap as a source\n\nPrevious code implies that you could easily pull the content from your own\ndatabase. However someone may use a blog platform, similar to Wordpress, or a\ndrupal website, or basically anything, and not have access to their database\neasily. Or the content stored in the database simply contains html tags.\nVarious scenarios make it somehow hard to get your own content easily in the\npurest form. The contentmap library was created to accelerate its adoption, by\nsimply creating the contentmap.db from your existing XML sitemap.\n\n    \n    \n    from contentmap.sitemap import SitemapToContentDatabase sitemap = \"https://philippeoger.com/sitemap.xml\" db = SitemapToContentDatabase(sitemap_url=sitemap, include_vss=True) db.build()\n\nUnder the hood, what this does is simply parse the XML sitemap to extract\nevery url, and then crawl each url to extract the content using the fantastic\nTrafilatura library. This must be clear, it will crawl your website, so make\nsure if you were to use the contentmap library that your domain can handle it.\nYou can control how much concurrent requests you send to your domain this way:\n\n    \n    \n    db = SitemapToContentDatabase( sitemap_url=sitemap, include_vss=True, concurrency=5 )\n\n## Caveats & comments\n\n  * The article started as a RAG oriented article, however I found the content sharing aspect to be an interesting matter. I may write more in another post. I would have liked to dig on the content licensing aspect for example that could be an extension of this idea.\n  * SQLite-vss is a loadable extension for sqlite and those are notorious for not working well on Mac and Windows. It is best to use in a linux environment. I added a Dockerfile and docker-compose.yaml on the repo of the library to help run the code in case you want to include VSS in your contentmap.db.\n  * The embedding algorithm currently use is all-MiniLM-L6-v2, an open model easily accessible from Huggingface. The library could be extended to others if needed.\n\nIn conclusion, the idea of using Retrieval-Augmented Generation to vectorize\nthe entire web is quite ambitious and challenging if one would want to do it\nfor themselves. While it may be theoretically possible for a single entity to\nRAG the whole web, a more practical and ethical approach could be a\ndecentralized one. By establishing a standardized protocol for domain owners\nto provide their content and matching embeddings, we can enable LLM to query\nspecific domains when needed without the need for extensive crawling. This\napproach would not only reduce server loads but also could address ethical\nconcerns around content ownership and usage.\n\n", "frontpage": false}
