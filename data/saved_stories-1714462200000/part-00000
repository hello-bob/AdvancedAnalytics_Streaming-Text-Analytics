{"aid": "40205042", "title": "FTP is dead. Long live FTP (2017)", "url": "https://www.rsaconference.com/library/blog/ftp-is-dead-long-live-ftp", "domain": "rsaconference.com", "votes": 2, "user": "wannacboatmovie", "posted_at": "2024-04-29 22:48:06", "comments": 0, "source_title": "Just a moment...", "source_text": "Just a moment...\n\n# www.rsaconference.com\n\n## Verifying you are human. This may take a few seconds.\n\nwww.rsaconference.com needs to review the security of your connection before\nproceeding.\n\nVerification successful\n\nWaiting for www.rsaconference.com to respond...\n\nRay ID: 87c370433b379b52\n\nPerformance & security by Cloudflare\n\n", "frontpage": false}
{"aid": "40205094", "title": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs", "url": "https://arxiv.org/abs/2401.06209", "domain": "arxiv.org", "votes": 1, "user": "Jimmc414", "posted_at": "2024-04-29 22:53:55", "comments": 0, "source_title": "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs", "source_text": "[2401.06209] Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal\nLLMs\n\nSkip to main content\n\nWe gratefully acknowledge support from the Simons Foundation, member\ninstitutions, and all contributors. Donate\n\n> cs > arXiv:2401.06209\n\n# Computer Science > Computer Vision and Pattern Recognition\n\narXiv:2401.06209 (cs)\n\n[Submitted on 11 Jan 2024 (v1), last revised 25 Apr 2024 (this version, v2)]\n\n# Title:Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs\n\nAuthors:Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, Saining\nXie\n\nView a PDF of the paper titled Eyes Wide Shut? Exploring the Visual\nShortcomings of Multimodal LLMs, by Shengbang Tong and 5 other authors\n\nView PDF HTML (experimental)\n\n> Abstract:Is vision good enough for language? Recent advancements in\n> multimodal models primarily stem from the powerful reasoning abilities of\n> large language models (LLMs). However, the visual component typically\n> depends only on the instance-level contrastive language-image pre-training\n> (CLIP). Our research reveals that the visual capabilities in recent\n> multimodal LLMs (MLLMs) still exhibit systematic shortcomings. To understand\n> the roots of these errors, we explore the gap between the visual embedding\n> space of CLIP and vision-only self-supervised learning. We identify ''CLIP-\n> blind pairs'' - images that CLIP perceives as similar despite their clear\n> visual differences. With these pairs, we construct the Multimodal Visual\n> Patterns (MMVP) benchmark. MMVP exposes areas where state-of-the-art\n> systems, including GPT-4V, struggle with straightforward questions across\n> nine basic visual patterns, often providing incorrect answers and\n> hallucinated explanations. We further evaluate various CLIP-based vision-\n> and-language models and found a notable correlation between visual patterns\n> that challenge CLIP models and those problematic for multimodal LLMs. As an\n> initial effort to address these issues, we propose a Mixture of Features\n> (MoF) approach, demonstrating that integrating vision self-supervised\n> learning features with MLLMs can significantly enhance their visual\n> grounding capabilities. Together, our research suggests visual\n> representation learning remains an open challenge, and accurate visual\n> grounding is crucial for future successful multimodal systems.\n\nComments:| Project page: this https URL  \n---|---  \nSubjects:| Computer Vision and Pattern Recognition (cs.CV)  \nCite as:| arXiv:2401.06209 [cs.CV]  \n(or arXiv:2401.06209v2 [cs.CV] for this version)  \nhttps://doi.org/10.48550/arXiv.2401.06209arXiv-issued DOI via DataCite  \n  \n## Submission history\n\nFrom: Shengbang Tong [view email] [v1] Thu, 11 Jan 2024 18:58:36 UTC (2,305\nKB) [v2] Thu, 25 Apr 2024 07:12:39 UTC (2,567 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Eyes Wide Shut? Exploring the Visual\nShortcomings of Multimodal LLMs, by Shengbang Tong and 5 other authors\n\n  * View PDF\n  * HTML (experimental)\n  * TeX Source\n  * Other Formats\n\nview license\n\nCurrent browse context:\n\ncs.CV\n\n< prev | next >\n\nnew | recent | 2401\n\nChange to browse by:\n\ncs\n\n### References & Citations\n\n  * NASA ADS\n  * Google Scholar\n  * Semantic Scholar\n\na export BibTeX citation Loading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer (What is the Explorer?)\n\nLitmaps (What is Litmaps?)\n\nscite Smart Citations (What are Smart Citations?)\n\n# Code, Data and Media Associated with this Article\n\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\n\nDagsHub (What is DagsHub?)\n\nGotit.pub (What is GotitPub?)\n\nPapers with Code (What is Papers with Code?)\n\nScienceCast (What is ScienceCast?)\n\n# Demos\n\nReplicate (What is Replicate?)\n\nHugging Face Spaces (What is Spaces?)\n\nTXYZ.AI (What is TXYZ.AI?)\n\n# Recommenders and Search Tools\n\nInfluence Flower (What are Influence Flowers?)\n\nConnected Papers (What is Connected Papers?)\n\nCORE Recommender (What is CORE?)\n\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new\narXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and\naccepted our values of openness, community, excellence, and user data privacy.\narXiv is committed to these values and only works with partners that adhere to\nthem.\n\nHave an idea for a project that will add value for arXiv's community? Learn\nmore about arXivLabs.\n\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n\n  * About\n  * Help\n\n  * Contact\n  * Subscribe\n\n  * Copyright\n  * Privacy Policy\n\n  * Web Accessibility Assistance\n  * arXiv Operational Status Get status notifications via email or slack\n\n", "frontpage": false}
{"aid": "40205126", "title": "Tunnel Try-On: Excavating Spatial-Temporal Tunnels for Virtual Try-On in Videos", "url": "https://arxiv.org/abs/2404.17571", "domain": "arxiv.org", "votes": 2, "user": "Jimmc414", "posted_at": "2024-04-29 22:56:43", "comments": 0, "source_title": "Tunnel Try-on: Excavating Spatial-temporal Tunnels for High-quality Virtual Try-on in Videos", "source_text": "[2404.17571] Tunnel Try-on: Excavating Spatial-temporal Tunnels for High-\nquality Virtual Try-on in Videos\n\nSkip to main content\n\nWe gratefully acknowledge support from the Simons Foundation, member\ninstitutions, and all contributors. Donate\n\n> cs > arXiv:2404.17571\n\n# Computer Science > Computer Vision and Pattern Recognition\n\narXiv:2404.17571 (cs)\n\n[Submitted on 26 Apr 2024]\n\n# Title:Tunnel Try-on: Excavating Spatial-temporal Tunnels for High-quality\nVirtual Try-on in Videos\n\nAuthors:Zhengze Xu, Mengting Chen, Zhao Wang, Linyu Xing, Zhonghua Zhai, Nong\nSang, Jinsong Lan, Shuai Xiao, Changxin Gao\n\nView a PDF of the paper titled Tunnel Try-on: Excavating Spatial-temporal\nTunnels for High-quality Virtual Try-on in Videos, by Zhengze Xu and 8 other\nauthors\n\nView PDF HTML (experimental)\n\n> Abstract:Video try-on is a challenging task and has not been well tackled in\n> previous works. The main obstacle lies in preserving the details of the\n> clothing and modeling the coherent motions simultaneously. Faced with those\n> difficulties, we address video try-on by proposing a diffusion-based\n> framework named \"Tunnel Try-on.\" The core idea is excavating a \"focus\n> tunnel\" in the input video that gives close-up shots around the clothing\n> regions. We zoom in on the region in the tunnel to better preserve the fine\n> details of the clothing. To generate coherent motions, we first leverage the\n> Kalman filter to construct smooth crops in the focus tunnel and inject the\n> position embedding of the tunnel into attention layers to improve the\n> continuity of the generated videos. In addition, we develop an environment\n> encoder to extract the context information outside the tunnels as\n> supplementary cues. Equipped with these techniques, Tunnel Try-on keeps the\n> fine details of the clothing and synthesizes stable and smooth videos.\n> Demonstrating significant advancements, Tunnel Try-on could be regarded as\n> the first attempt toward the commercial-level application of virtual try-on\n> in videos.\n\nComments:| Project Page: this https URL  \n---|---  \nSubjects:| Computer Vision and Pattern Recognition (cs.CV)  \nCite as:| arXiv:2404.17571 [cs.CV]  \n(or arXiv:2404.17571v1 [cs.CV] for this version)  \n  \n## Submission history\n\nFrom: Zhengze Xu [view email] [v1] Fri, 26 Apr 2024 17:55:26 UTC (6,242 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Tunnel Try-on: Excavating Spatial-temporal\nTunnels for High-quality Virtual Try-on in Videos, by Zhengze Xu and 8 other\nauthors\n\n  * View PDF\n  * HTML (experimental)\n  * TeX Source\n  * Other Formats\n\nview license\n\nCurrent browse context:\n\ncs.CV\n\n< prev | next >\n\nnew | recent | 2404\n\nChange to browse by:\n\ncs\n\n### References & Citations\n\n  * NASA ADS\n  * Google Scholar\n  * Semantic Scholar\n\na export BibTeX citation Loading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer (What is the Explorer?)\n\nLitmaps (What is Litmaps?)\n\nscite Smart Citations (What are Smart Citations?)\n\n# Code, Data and Media Associated with this Article\n\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\n\nDagsHub (What is DagsHub?)\n\nGotit.pub (What is GotitPub?)\n\nPapers with Code (What is Papers with Code?)\n\nScienceCast (What is ScienceCast?)\n\n# Demos\n\nReplicate (What is Replicate?)\n\nHugging Face Spaces (What is Spaces?)\n\nTXYZ.AI (What is TXYZ.AI?)\n\n# Recommenders and Search Tools\n\nInfluence Flower (What are Influence Flowers?)\n\nConnected Papers (What is Connected Papers?)\n\nCORE Recommender (What is CORE?)\n\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new\narXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and\naccepted our values of openness, community, excellence, and user data privacy.\narXiv is committed to these values and only works with partners that adhere to\nthem.\n\nHave an idea for a project that will add value for arXiv's community? Learn\nmore about arXivLabs.\n\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n\n  * About\n  * Help\n\n  * Contact\n  * Subscribe\n\n  * Copyright\n  * Privacy Policy\n\n  * Web Accessibility Assistance\n  * arXiv Operational Status Get status notifications via email or slack\n\n", "frontpage": false}
{"aid": "40205129", "title": "Elon Musk's 'Twitter sitter' appeal over Tesla posts rejected by Supreme Court", "url": "https://www.cnn.com/2024/04/29/politics/supreme-court-tosses-elon-musks-twitter-sitter-appeal-over-tesla-posts/index.html", "domain": "cnn.com", "votes": 6, "user": "peutetre", "posted_at": "2024-04-29 22:56:52", "comments": 0, "source_title": "edition.cnn.com", "source_text": "edition.cnn.com\n\n# This edition.cnn.com page can\u2019t be found\n\nNo webpage was found for the web address:\nhttps://edition.cnn.com/2024/04/29/politics/supreme-court-tosses-elon-musks-\ntwitter-sitter-appeal-over-tesla-posts/index.html\n\nHTTP ERROR 404\n\nnull\n\nNo webpage was found for the web address:\nhttps://edition.cnn.com/2024/04/29/politics/supreme-court-tosses-elon-musks-\ntwitter-sitter-appeal-over-tesla-posts/index.html\n\n", "frontpage": true}
