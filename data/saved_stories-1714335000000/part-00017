{"aid": "40188470", "title": "Optimizing for Taste", "url": "https://cra.mr/optimizing-for-taste/", "domain": "cra.mr", "votes": 1, "user": "tie-in", "posted_at": "2024-04-28 13:32:46", "comments": 0, "source_title": "Optimizing for Taste", "source_text": "Optimizing for Taste\n\n#\n\ncra\n\nmr\n\n#\n\nOptimizing for Taste\n\nSep 26, 2023 12 min read\n\nsentry\n\nNote: This was an internal post I made at Sentry, but given this challenge is\nquite common at early stage startups, I\u2019ve decided to publish it externally.\nThis is an opinion and a decision on how we operate at Sentry. I won\u2019t claim\nthat we\u2019re experts on this, but all companies are somewhat unique in their\nculture, their goals, and the methods they choose to adopt. This is just one\nof our choices.\n\nIf you\u2019ve interacted in the product strategy sessions, you\u2019ll find I\u2019ve\nhistorically been opposed to A/B testing - to running behavioral experiments.\nToday we\u2019re going to talk about why that is, and why we are moving away from\nA/B tests in product. Before we get to that, I want to talk about why making\nthis decision matters, and how we\u2019ve been down this road before.\n\nEarly in Sentry\u2019s post-funding days there was a constant battle: should we\nsell on-premise Sentry or only offer SaaS? I, and much of the founding team,\nwere opposed to doing on-prem. There were a variety of reasons, but what\nmatters is our indecisiveness. At some point - far too late into these\nconversations - I said enough was enough. I made the decision that we would\nnot commercialize on-premise Sentry, and that the decision was final. On-\npremise has never come up again, and we are better for it. That distraction,\nthat paralysis to move forward, it risked stagnation. It created an inability\nto focus on what we needed to be doing.\n\nThis leads me to the conversation about testing. This same indecisiveness is\nwhat I see in many conversations around testing, and wrongly thinking testing\nis a treatment for the problem.\n\nIf you\u2019re not familiar with A/B testing, it\u2019s primarily used in performance\nmarketing as a way to optimize for human behavior. For example, if someones\nsearching for a \u201cdog collar\u201d, what language or visual is more likely to draw\nthe individual\u2019s attention? That is often hard to know and it varies greatly.\nImportantly, those variations at large scales can also have a meaningful\nimpact. You sell both a happy, fun, bright pink dog collar and moody, covered\nin spikes, black dog collar, and one of those is more likely to get the\naverage individual\u2019s attention. You may also find that there is variance in\nwhich one matters most based on the context. The reason this all matters is\nultimately because you sell one million dog collars, and a lift of 1% could be\n10s of thousands in additional (or saved) revenue. Multiply those decisions,\nand you have the reason testing is so valued in performance marketing.\n\nNow you might ask, why would I be opposed to such a thing within our product?\nThere are many, many reasons I am opposed, but the one we should care about is\nhow it fosters a culture of decision paralysis. It fosters a culture of\ndecision making without having an opinion, without having to put a stake in\nthe ground. It fosters a culture where making a quick buck trumps a great\nproduct experience. That goes wildly against our core values, how we built\nSentry, and what we want Sentry to be. Pixels Matter, one of our core values,\nis centered around caring about the small details, and that by its very nature\nis subjective. What details? Which ones matter? Those decisions all center on\ntaste, and around someone making a subjective decision.\n\nWhile that single story is what is driving this push, I do want to touch on\nseveral other arcs where I have seen testing fail, create friction, or\notherwise cause a general negative sentiment amongst folks.\n\nSmall targets drive small outcomes. An example might be targeting a small test\nsegment of 100 customers to push Replay. If one customer adopting the product\ndrove ACV of $300, that means we\u2019re targeting a whopping $30,000 in added\nannual revenue. Contrast that to your salary and multiply that by 400. That\u2019s\na small target, a small outcome, and it\u2019s not even guaranteed! On the counter\nside, you\u2019ll note we\u2019ve set our target for Replay adoption to 10% of our\naudience this year. That\u2019s a big target, and it might not be easy to hit, but\nhitting it (or even getting close) will have a meaningful impact to our ARR.\n\nBringing this back to testing, often tests are focused on small targets, on\nthe tiniest of incremental change. This is by design! It\u2019s to ensure you can\nidentify what is actually driving success. Unfortunately now we\u2019re back to\nsmall outcomes. \u201cWhich text for the button will perform the best?\u201d - well if\nonly a few hundred people click that button in a month, the answer is it\ndoesn\u2019t fucking matter. Additionally a few hundred clicks is not going to be\nenough data to achieve a statistically significant result. While I realize\nthis anecdote is extreme, it\u2019s representative of the general problem.\n\nOpportunity Cost. This is an economics theory that you\u2019re likely familiar\nwith. If we spend time doing one thing, it means we are not doing another.\nThis is a key principle we apply in all decision making, but is compounded\nwhen it comes to testing. If you\u2019re testing something you\u2019ve intentionally\ndecided to add cost, complexity, and time to the initiative. To make matters\nworse, testing requires you to have a control in place, and practically\nspeaking that means you avoid making multiple changes within the same sphere\nof influence (for the duration of the test) to maximize correctness. That\nmeans you\u2019ve doubled down on the time spent doing one thing vs another.\n\nAlternatively we simply make decisions based on the information at hand and\nmeasure the results. This means we unblock future development as quickly as we\ncan, but it doesn\u2019t mean we won\u2019t act on the results. If the results perform\nnegatively, we may need to try something else. When they do, you of course\nwant to understand why, but fortunately there are still many ways to do post-\nanalysis. Cohort analysis is particularly useful in this fashion. This is not\nto say cohorts are a superior technique, as they are generally going to be\nless accurate, but its usually enough for you to draw conclusions from, and if\nneeded, you can bisect from there. Again, and most importantly, you\u2019re not\nblocking additional development on the result of the test, which can often\ntake a considerable amount of time, nor are you spending cycles waiting for\ninformation before moving on to the next project.\n\nA good example of this in practice is Sentry Bundles. We launched the bundles\nto only new customers (this limits exposure and risk), and we\u2019re able to\nmeasure the cohort of bundles vs prior cohorts of non-bundles. Are there other\nchanges that might impact the results here? Sure, but that impact - from\nexperience and raw data points - is minimal, especially because our period of\nobservation is only 4 weeks long. You could do this with A/B testing as well,\nbut what would you gain? At best an accuracy improvement, but more likely the\ndata would be insignificant (we only get a few thousand paid customers per\nmonth in total). We could also A/B test different price points, but we\u2019d\nrather have an opinion on what the right package and price point is, and prove\nif that\u2019s going to work or not. If it doesn\u2019t, we\u2019ll take those learnings and\niterate on a v2. Our focus is on narrative and customer experience, not\nmin/max on the bottom line.\n\nExperiments as a substitute for data. This issue really hits home as it has\ncaused measurable financial consequences at Sentry. I\u2019ve often seen this\nhappen by wrongly concluding that data is only measured by the outcome of the\nproject, rather than learned or otherwise informed by past experiences. This\nis where we begin to really articulate what taste ultimately is. This is one\nof the biggest reasons I am against A/B testing, the reason I push for people\nto have an opinion, and this one we\u2019re fortunate to have a recent example we\ncan learn from at Sentry.\n\nSome months back we changed the New Project flow to create multiple projects\ninstead of one. The thesis here was that customers have multiple projects, so\nwe should prompt them to set them up all right away to improve expansion. If\nthis triggers your spidey sense, it should! One of the pieces of feedback we\nhad when this idea was pitched is that users generally are not working on\nmultiple applications at the same time, so prompting them to set up multiple\napplications would increase friction. This feature was implemented, it was run\nthrough a controlled A/B test, and that test suggested the multi-project\ncreation was better. Was the feedback wrong then?\n\nMonths later we noticed an activation issue, and lo and behold we found there\nwere a lot of accounts where they had many empty projects. Reasoning would\nsuggest that yeah, obviously this would create issues, because instead of an\naccount having one project, which is fairly easy to navigate and comprehend,\naccounts now had multiple projects, many of which aren\u2019t set up. You combine\nthat with the fact that most customers are on our Team plan, which does not\nallow cross-project search, and you can easily understand why the user\nexperience is bad. Data is not a substitute for critical thinking. The test\nsaid it was successful, but the outcomes, which is what you base future\ndecisions on, showed otherwise. The lesson here is not that A/B testing was at\nfault, but that running experiments is never a substitute for a vetted\nhypothesis.\n\nA/B tests are useful to the extent that human behavior is aligned with a\ndefinition of correctness. The last issue I\u2019ll touch on goes back to the\npurpose of A/B testing, and our misuse of it at Sentry. There are two common\nplaces I\u2019ve seen us go wrong with this at Sentry. The first I mentioned\nseveral times above, where we\u2019re commonly lacking enough data points to reach\nan accurate conclusion (very often true in Enterprise software, and will be\ntrue in most testing methodologies for us). The second is far more nuanced,\nbut is key to why this is valuable in performance marketing and less so in\nengineering. A/B testing is testing human behavior, it is not testing\ncorrectness, yet we\u2019ve tried to lean on this concept historically to validate\nif some code we\u2019ve written is more correct.\n\nA contrived example of wrongly using A/B tests would be to improve our issue\nfingerprinting behavior. Our goal with issues is to ensure the same error is\nalways grouped together, but we sometimes get it wrong. Changing the way we\nfingerprint however is fairly scary, and hard to measure if its better or\nworse than the previous iteration. You might think A/B tests can help here,\nbut a more correct approach is how many in the ML space solve with\nverification data. A really simplistic way to think about this is that you\u2019d\nbuild a test data set - think about a huge spreadsheet of issues - and you\u2019d\nhave that spreadsheet tell you exactly which issues should and shouldn\u2019t be\ngrouped together. When you change the heuristics, the algorithm to fingerprint\nissues, you\u2019d validate your results by measuring the accuracy (the number of\nthese sample issues grouped together) vs your verification data set (the\ntarget grouping of data).\n\nIn real life it\u2019s much more complex than that, but that is actual verification\nof a problem. If you A/B tested this, you\u2019d introduce variability through\nsubjective human behavior. It\u2019s possible you\u2019d randomly get lucky here, and\nit\u2019s more likely you\u2019d get lucky with a huge amount of data points, but it\ndoesn\u2019t mean the methodology is correct. It\u2019s also importantly not measurable\nin a way that lends itself to repeatability - for example if you wanted to\ncontinue to iterate on algorithm improvements (and factually know that it\u2019s\nmore correct). You may grok that this method of verification is more\ncomplicated, more expensive, and that\u2019s just a natural side effect (and\nrequirement) of the scale of Sentry. tl;dr OpenAI is not A/B testing which LLM\nmodels work better.\n\n\u201dNo A/B testing is certainly better than bad A/B testing\u201d\n\nOur objective at Sentry isn\u2019t, and hasn\u2019t ever been to be great at running\nthis kind of experimentation, which may explain why many of these attempts\nhave been more failure-prone. That is why we are choosing to continue to\nprioritize taste at Sentry. That taste is curated through hiring, and comes\nfrom the team\u2019s domain expertise, their diversity of background, and their\nlearnings both building and using our product. It comes from talking with\ncustomers on Twitter, from engaging them in support tickets and on Github, it\ncomes from direct transparent conversation.\n\nThis is the kind of data we use to inform our decisions here at Sentry.\nSometimes those decisions, those subjective-but-informed decisions, will lead\nto a failure. Those failures help us make better decisions in the future. It\nmeans we are optimizing for iteration speed - another one of our core values.\nThat willingness to iterate, to fail and move forward quickly, that is what\ndrives great outcomes, and that is the culture we want at Sentry.\n\nI\u2019d like to leave you with one lesson I\u2019ve kept with me over the years. The\nstrength of making a decision is making it. You can always make a new one\nlater. Choose the obvious path forward, and if you don\u2019t see one, find someone\nwho does.\n\n## More Reading\n\n### You're Not a CEO\n\nMar 26, 2024\n\n### Enterprise is Dead\n\nMar 7, 2024\n\n### Open Source and a Healthy Dose of Capitalism\n\nFeb 1, 2024\n\n### You Suck at Marketing\n\nJan 25, 2024\n\n### The BUSL Factor\n\nNov 20, 2023\n\n\u00a9 2024 David Cramer \u2014 Archive\n\n", "frontpage": false}
