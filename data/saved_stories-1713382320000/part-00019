{"aid": "40064287", "title": "Llama.cpp: Improve CPU prompt eval speed", "url": "https://github.com/ggerganov/llama.cpp/pull/6414", "domain": "github.com/ggerganov", "votes": 1, "user": "tosh", "posted_at": "2024-04-17 13:28:29", "comments": 0, "source_title": "Improve cpu prompt eval speed by jart \u00b7 Pull Request #6414 \u00b7 ggerganov/llama.cpp", "source_text": "Improve cpu prompt eval speed by jart \u00b7 Pull Request #6414 \u00b7\nggerganov/llama.cpp \u00b7 GitHub\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nggerganov / llama.cpp Public\n\n  * Notifications\n  * Fork 7.8k\n  * Star 55.6k\n\nJump to bottom\n\n# Improve cpu prompt eval speed #6414\n\nMerged\n\nggerganov merged 1 commit into ggerganov:master from jart:sgemm Apr 16, 2024\n\nMerged\n\n# Improve cpu prompt eval speed #6414\n\nggerganov merged 1 commit into ggerganov:master from jart:sgemm Apr 16, 2024\n\n+1,312 \u221212\n\n## Conversation\n\nContributor\n\n###\n\njart commented Apr 1, 2024\n\nThis change upstreams llamafile's cpu matrix multiplication kernels which\nimprove image and prompt evaluation speed. For starters, Q4_0 and Q8_0 weights\nshould go ~40% faster on CPU. The biggest benefits are with data types like\nf16 / f32, which process prompts 2x faster thus making them faster than\nquantized data types for prompt evals.\n\nThis change also introduces bona fide AVX512 support since tinyBLAS is able to\nexploit the larger register file. For example, on my CPU llama.cpp llava-cli\nprocesses an image prompt at 305 tokens/second, using the Q4_K and Q4_0 types,\nwhich has always been faster than if we used f16 LLaVA weights, which at HEAD\ngo 188 tokens/second. With this change, f16 LLaVA performance leap frogs to\n464 tokens/second.\n\nOn Intel Core i9-14900K this change improves F16 prompt perf by 5x. For\nexample, using llama.cpp at HEAD with Mistral 7b f16 to process a 215 token\nprompt will go 13 tok/sec. This change has fixes making it go 52 tok/sec. It's\nmostly thanks to my vectorized outer product kernels but also because I added\nsupport for correctly counting the number of cores on Alderlake, so the\ndefault thread count discounts Intel's new efficiency cores. Only Linux right\nnow can count cores.\n\nThis work was sponsored by Mozilla who's given permission to change the\nlicense of this code from Apache 2.0 to MIT. To read more about what's\nimproved, and how it works, see: https://justine.lol/matmul/\n\nCollaborator\n\n###\n\nphymbert commented Apr 1, 2024\n\nPlease fix the CI builds  \n---  \n  \njart force-pushed the sgemm branch from 3b514d4 to 26d3614 Compare April 1,\n2024 07:35\n\nContributor\n\n###\n\ngithub-actions bot commented Apr 1, 2024 \u2022\n\n\ud83d\udcc8 llama.cpp server for bench-server-baseline on Standard_NC4as_T4_v3: 517\niterations \ud83d\ude80\n\n  * Concurrent users: 8, duration: 10m\n  * HTTP request : avg=9017.64ms p(90)=25806.42ms fails=0, finish reason: stop=517 truncated=0\n  * Prompt processing (pp): avg=238.16tk/s p(90)=711.8tk/s total=203.92tk/s\n  * Token generation (tg): avg=95.48tk/s p(90)=248.93tk/s total=128.87tk/s\n  * ggml-org/models/phi-2/ggml-model-q4_0.gguf parallel=8 ctx-size=16384 ngl=33 batch-size=2048 ubatch-size=256 pp=1024 pp+tg=2048 branch=sgemm commit=8dbe58213391399b2e3b60b5b116b5dd6b864f96\n\n  \n---  \n  \njart force-pushed the sgemm branch from 26d3614 to e867339 Compare April 1,\n2024 07:52\n\nphymbert mentioned this pull request Apr 1, 2024\n\nserver: bench: continuous performance testing #6233\n\nOpen\n\n16 tasks\n\njart force-pushed the sgemm branch from e867339 to 20b2d6c Compare April 1,\n2024 08:14\n\nCollaborator\n\n###\n\nJohannesGaessler commented Apr 1, 2024\n\nSome very quick tests on my Ryzen 5950X (power limited to 95 W):| Model|\nThreads| Test| t/s master| t/s| Speedup  \n---|---|---|---|---|---  \nllama 7B Q4_0| 16| pp 512| 24.60| 32.34| 1.31  \nllama 7B Q4_0| 16| tg 128| 9.75| 9.86| 1.01  \nllama 7B F16| 16| pp 512| 27.70| 42.74| 1.54  \nllama 7B F16| 16| tg 128| 3.20| 3.19| 1.00  \n  \nA very respectable speedup!\n\nSince you did not mention it in the OP, this PR does not touch the handling of\nNUMA nodes, correct?  \n  \nContributor\n\n###\n\nkalomaze commented Apr 1, 2024\n\nIs this not yet set up to support the CPU code used in partial GPU offloading?\nWill those require custom kernels?  \n---  \n  \nCollaborator\n\n###\n\nJohannesGaessler commented Apr 1, 2024\n\n> Is this not yet set up to support the CPU code used in partial GPU\n> offloading? Will those require custom kernels?\n\nThis PR will not speed up CPU+GPU hybrid inference in any meaningful capacity.\nFor large batches you are compute bound and all of the evaluations are done on\nthe GPU. For small batches you are I/O bound and better matrix multiplication\nalgorithms make virtually no difference.  \n---  \n  \nContributor\n\n###\n\nkalomaze commented Apr 1, 2024 \u2022\n\n> For large batches you are compute bound and all of the evaluations are done\n> on the GPU.\n\nDoes this mean it moves layers onto the GPU for large batches instead of\nprocessing all GPU layers for the current batch and then doing the remaining\nlayers on CPU? I'm sort of lost, this works against my current understanding\n(moving from CPU to GPU during inference should be slower)?  \n---  \n  \njart force-pushed the sgemm branch from 20b2d6c to 08f10ec Compare April 1,\n2024 10:21\n\nCollaborator\n\n###\n\nJohannesGaessler commented Apr 1, 2024\n\nCPU layers have their data in RAM. GPU layers have their data in VRAM. GPU\nlayers are always evaluated on the GPU.The most recent update is this PR:\n#6083 . For large batch sizes (prompt processing) all data of a CPU layer is\nmoved to the GPU and the calculations are done there in order to make use of\nthe higher GPU compute. For small batch sizes (token generation) CPU layers\nare evaluated on the CPU. This PR improves the compute efficiency of CPU\nmatrix multiplication. So it only helps in those scenarios where it would also\nbe worthwhile to temporarily move data to VRAM. The improvements in this PR\nare therefore mutually exclusive with CPU+GPU hybrid inference.  \n---  \n  \njart force-pushed the sgemm branch from 08f10ec to 7fb769f Compare April 1,\n2024 13:55\n\n###\n\nzougloub commented Apr 1, 2024\n\n@jart this is pretty awesome ; I would add that since a good portion of the\ncontributed code is very generic and could benefit to many other downstream\nprojects, it would be even more awesome if that code could be in its own repo\n; then a subset could be linked or vendored in here.  \n---  \n  \nContributor Author\n\n###\n\njart commented Apr 1, 2024\n\n@phymbert Tests are green. Please take a look.  \n---  \n  \nCollaborator\n\n###\n\nphymbert commented Apr 1, 2024\n\n> @phymbert Tests are green. Please take a look.\n\nThank you very much for the contribution. For the core library and ggml\nchanges @slaren and @ggerganov will revert to you.  \n---  \n  \nContributor Author\n\n###\n\njart commented Apr 1, 2024\n\n@zougloub Thank you for the encouragement. You can copy sgemm.cpp into your\ncodebase as its own library if you provide an implementation for\nGGML_FP16_TO_FP32(). It would be challenging to create a bona fide library for\nthis, because GEMM has more depth the more stakeholders you have. This code is\nwritten to focus only on what's good for llama.cpp and nothing else. The\nparallel implementation in the llamafile codebase does things a little\ndifferently, based on what's best there.  \n---  \n  \nOwner\n\n###\n\nggerganov commented Apr 1, 2024\n\n@jart Apologies for the slow response - will review the PRs in the following\ndays. Thanks  \n---  \n  \nContributor Author\n\n###\n\njart commented Apr 1, 2024\n\nThanks @ggerganov I'm in no rush.  \n---  \n  \nContributor\n\n###\n\nnetrunnereve commented Apr 2, 2024\n\nThis PR did absolutely nothing for me on Q4_0 and Q8_0, then I realised that\nit only supported AVX2 and AVX512 for those quants. It does support regular\nAVX though for F16 and F32.On my 4c/8t Xeon v2 I get a nice 2x speedup in F16.\nJust like vanilla llama.cpp you get the best CPU performance if you use all\nhyperthreads during prompt processing and switch to one thread per core for\ninference.| model| size| params| backend| threads| test| t/s  \n---|---|---|---|---|---|---  \nllama 1B F16| 2.05 GiB| 1.10 B| CPU| 8| pp 512| 33.36 \u00b1 0.13  \nllama 1B F16| 2.05 GiB| 1.10 B| CPU| 4| pp 512| 32.19 \u00b1 0.02  \nllama 1B F16 (PR)| 2.05 GiB| 1.10 B| CPU| 8| pp 512| 60.47 \u00b1 0.06  \nllama 1B F16 (PR)| 2.05 GiB| 1.10 B| CPU| 4| pp 512| 52.88 \u00b1 0.12  \n  \nCollaborator\n\n###\n\nJohannesGaessler commented Apr 2, 2024\n\n@netrunnereve in case you're not aware, you can run ./llama-bench -o sql | sqlite3 llama-bench.sqlite both on master and a PR and then scripts/compare-llama-bench.py to generate a table with a performance comparison.  \n---  \n  \n###\n\nJeremyGe07 commented Apr 2, 2024\n\nDoes this PR benefit ARM CPU?  \n---  \n  \njart force-pushed the sgemm branch from 7fb769f to bb6ebca Compare April 2,\n2024 11:10\n\ntstanisl reviewed Apr 2, 2024\n\nView reviewed changes\n\ntstanisl reviewed Apr 2, 2024\n\nView reviewed changes\n\njart force-pushed the sgemm branch from bb6ebca to 83a2f14 Compare April 2,\n2024 11:59\n\n###\n\nsorasoras commented Apr 2, 2024 \u2022\n\n> Does this PR benefit ARM CPU?\n\nI think so. it has to be ARMV8.2+ I guess. https://justine.lol/matmul/  \n---  \n  \njart mentioned this pull request Apr 2, 2024\n\nIntroduce bfloat16 support #6412\n\nOpen\n\n###\n\nJipok commented Apr 2, 2024 \u2022\n\nShould I have acceleration for Q8 if I only have AVX and AVX2? I tested and\nfound no differences. Do I need to build with some kind of blas?  \n---  \n  \nCollaborator\n\n###\n\nphymbert commented Apr 2, 2024\n\nhttps://justine.lol/matmul/ is a must read ^^) Thank you @jart, you got a new\nPatron  \n---  \n  \nContributor\n\n###\n\nnetrunnereve commented Apr 4, 2024\n\nWith a quick and hacky AVX implementation (basically using two 128-bit SSE\ninstructions to replace a single AVX2 instruction and stuffing the result back\ninto the 256-bit register) I still see 20% faster prompt processing with Q4_0\non my Xeon V2. I have a fork here if anyone wants to play with it.| model|\nsize| params| backend| threads| test| t/s  \n---|---|---|---|---|---|---  \nllama 7B Q4_0 (master)| 3.56 GiB| 6.74 B| CPU| 8| pp 512| 5.94 \u00b1 0.14  \nllama 7B Q4_0 (PR with my AVX modifications)| 3.56 GiB| 6.74 B| CPU| 8| pp\n512| 7.09 \u00b1 0.01  \n  \nI've actually been working on something similar with the IQ quants vec_dot\nfunctions as those are AVX2 only and the scalar implementation is super slow\non my computer.  \n  \nContributor Author\n\n###\n\njart commented Apr 4, 2024\n\nNice work @netrunnereve. Would you be interested in contributing your change\nto llamafile too?  \n---  \n  \n###\n\nGANJAC commented Apr 4, 2024\n\nHi @ggerganov please \ud83d\ude4f commit this \ud83d\ude4f  \n---  \n  \nCollaborator\n\n###\n\nngxson commented Apr 4, 2024\n\n@GANJAC Please do not rush contributors/owner to merge. We human can make\nmistake, therefore we need time to review the code carefully.  \n---  \n  \n###\n\nGANJAC commented Apr 4, 2024\n\n> @GANJAC Please do not rush contributors/owner to merge. We human can make\n> mistake, therefore we need time to review the code carefully.\n\nHi @ngxson, you are absolutely right, I apologize.  \n---  \n  \nContributor\n\n###\n\nnetrunnereve commented Apr 5, 2024 \u2022\n\n> Nice work @netrunnereve. Would you be interested in contributing your change\n> to llamafile too?\n\nAfter this PR is merged I'll probably polish up the AVX implementation and\nsubmit it back up to llama.cpp. Since your llamafile is based off llama.cpp\nany updates that we make here should eventually get pulled into your project.\nOr you can just pick it up now and use it as you see fit in llamafile, I don't\nmind.  \n---  \n  \n###\n\nmoshemalawach commented Apr 7, 2024\n\nUsing it on many CPU setups and it speeds up everything on context processing!  \n---  \n  \n###\n\nZelinMa557 commented Apr 8, 2024\n\nCan these kernels make the token generation faster?  \n---  \n  \n###\n\nlin72h commented Apr 8, 2024\n\n> Can these kernels make the token generation faster?\n\nI think it probably not, because token generation is memory bandwidth bound  \n---  \n  \nggerganov mentioned this pull request Apr 8, 2024\n\nadd loongarch lsx and lasx optimize code #6454\n\nOpen\n\nggerganov reviewed Apr 10, 2024\n\nView reviewed changes\n\nOwner\n\n###\n\nggerganov left a comment\n\nThere was a problem hiding this comment.\n\n### Choose a reason for hiding this comment\n\nThe reason will be displayed to describe this comment to others. Learn more.\n\nCPU speedups are always welcome, but I\u2019m worried about the maintenance efforts\nfor the core ggml library increasing, so I\u2019m still hesitating how to proceed\nwith this PR.\n\nSimilar discussion was already had in #5780 and there are likely to be other\nmatrix-multiplication improvements proposed:\n\n  * ggml: aarch64: implement mmla kernels for q8_0_q8_0, q4_0_q8_0 and q4_1_q8_1 quantized gemm #4966\n  * add loongarch lsx and lasx optimize code #6454\n  * optimize for ppc64le using VSX intrinsics ggml#784\n\nThis change on one hand is well decoupled which is good, but at the same time\nintroduces a new block-wise matrix-multiplication pattern that is different\nfrom the existing dot-based implementations. It\u2019s obviously significantly more\nperformant since it utilizes the CPU cache much more efficiently, which has\nnot been the case so far. It also seems that the implementation can be\nextended to more instruction sets and quantum types in the future, so the\namount of code has the potential to grow significantly.\n\nThe code is also in C++, while we generally prefer to keep the core\nimplementation in C and allow C++ only in the backend implementations when\ndesired. I\u2019ve been pretty stubborn with this C requirement and it\u2019s probably\nsomething to finally reconsider, but it\u2019s not the time to decide in this PR.\n\nI don\u2019t want to delay this for much longer as I\u2019ve already given this quite\nsome thought and haven\u2019t come to a good conclusion. I think the comments in\n#5780 apply to a good extend here (PTAL), so my suggestion is that we aim for\nthis to become part of the future BLAS/matmul backend. The benefit of doing\nthat is that the code becomes sort of an \"extension\" to ggml and can be\ndeveloped more independently, without drawing a lot of attention from the core\nmaintainers.\n\nIn the meantime, we can merge this change and depending on how the development\nprocess goes (i.e. there is enough support from the community, bugs and issues\nare being resolved, functionality is reasonably extended, remains well\ndecoupled from the rest of the code) we can potentially consider to make this\npart of the core ggml library. But until then it will remain sort of a\n\"second-class citizen\".\n\n@jart If that makes sense, we would need to put the ggml.c change behind a\ndefine (e.g. GGML_USE_TINYBLAS or GGML_USE_LLAMAFILE or something like this),\nso that the sgemm code becomes optional (we generally avoid such special\ncases, but we can make an exception this time). In llama.cpp builds we can\nhave this enabled by default as it seems it is always better than the\nalternatives. This way, llamafile and other downstream projects can directly\nbenefit from the changes, and we'll have more time to figure out what is the\nright way to integrate this into ggml.\n\nIf you are OK with that, we can proceed to merge\n\nliqunfu reviewed Apr 11, 2024\n\nView reviewed changes\n\njart force-pushed the sgemm branch from 8dbe582 to 492b76d Compare April 11,\n2024 07:27\n\nContributor Author\n\n###\n\njart commented Apr 11, 2024\n\nSounds good @ggerganov. Review comments addressed in 492b76d PTAL  \n---  \n  \nContributor Author\n\n###\n\njart commented Apr 11, 2024\n\nAlso just want to draw attention to the loosening of the src1_cont\nrestriction. Could you confirm that's correct?  \n---  \n  \nContributor Author\n\n###\n\njart commented Apr 11, 2024\n\nAnother thing worth mentioning, possibly for future iterations is that:\n\n    \n    \n    template <int RM, int RN> void gemm(int m0, int m, int n0, int n) { int ytiles = (m - m0) / RM; int xtiles = (n - n0) / RN; int tiles = xtiles * ytiles; int duty = (tiles + nth - 1) / nth; int start = duty * ith; int end = start + duty; if (end > tiles) end = tiles; for (int job = start; job < end; ++job) { int ii = m0 + job / xtiles * RM; int jj = n0 + job % xtiles * RN; D Cv[RN][RM] = {0}; for (int l = 0; l < k; l += KN) for (int j = 0; j < RN; ++j) for (int i = 0; i < RM; ++i) Cv[j][i] = madd(load(A + lda * (ii + i) + l), // load(B + ldb * (jj + j) + l), // Cv[j][i]); TC Cd[RN][RM]; for (int j = 0; j < RN; ++j) for (int i = 0; i < RM; ++i) Cd[j][i] = hsum(Cv[j][i]); for (int j = 0; j < RN; ++j) for (int i = 0; i < RM; ++i) C[ldc * (jj + j) + (ii + i)] = Cd[j][i]; } }\n\nIs able to generate the handwritten kernels in the tinyBLAS class. This makes\nit possible to generate an mnpack() method that optimally handles all edge\ncases for weirdly shaped n and m values. See\nhttps://gist.github.com/jart/640231a627dfbd02fb03e23e8b01e592#file-matmul-\ncpp-L295-L609 for an example. The issue is that Clang takes 45 seconds to\ncompile it. Would you want me to simplify the code so it's more abstract but\npotentially slower to compile?  \n---  \n  \nggerganov reviewed Apr 11, 2024\n\nView reviewed changes\n\nOwner\n\n###\n\nggerganov left a comment \u2022\n\nThere was a problem hiding this comment.\n\n### Choose a reason for hiding this comment\n\nThe reason will be displayed to describe this comment to others. Learn more.\n\n> The issue is that Clang takes 45 seconds to compile it.\n\nNot a good idea - the build time should not increase noticeably after these\nchanges.\n\nI did some more tests on M2 Ultra. Generally, text-generation (batch size = 1)\nand prompt processing speed (batch size > 256) are the most important metrics\nto look at, but keeping an eye on the performance for low-sized batches is\nalso important (e.g. parallel decoding, speculative decoding, etc.)\n\nThe following command will give you the speed for various batch sizes:\n\n    \n    \n    ./llama-bench -m models/mistral-instruct-7b-v0.2/ggml-model-f16.gguf -ngl 0 -p 1,2,3,4,5,6,7,8,12,16,32,64,512 -n 0 -r 50 -t 16\n\nThese are the numbers with the llamafile SGEMM disabled:\n\n    \n    \n    LLAMA_NO_LLAMAFILE=1 LLAMA_NO_ACCELERATE=1 make -j llama-bench && ./llama-bench -m models/mistral-instruct-7b-v0.2/ggml-model-f16.gguf -ngl 0 -p 1,2,3,4,5,6,7,8,12,16,32,64,512 -n 0 -r 50 -t 16\n\nmodel| size| params| backend| ngl| test| t/s  \n---|---|---|---|---|---|---  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 1| 15.67 \u00b1 0.25  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 2| 26.14 \u00b1 0.78  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 3| 32.99 \u00b1 0.29  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 4| 37.72 \u00b1 0.48  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 5| 39.51 \u00b1 0.61  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 6| 43.78 \u00b1 0.50  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 7| 45.72 \u00b1 1.26  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 8| 47.13 \u00b1 1.35  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 12| 51.81 \u00b1 0.53  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 16| 53.54 \u00b1 1.59  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 32| 55.89 \u00b1 0.46  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 64| 57.53 \u00b1 0.31  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 512| 58.16 \u00b1 0.22  \n  \nbuild: 492b76d (2645)\n\nThis is the same bench with llamafile SGEMM enabled:\n\n    \n    \n    LLAMA_NO_ACCELERATE=1 make -j llama-bench && ./llama-bench -m models/mistral-instruct-7b-v0.2/ggml-model-f16.gguf -ngl 0 -p 1,2,3,4,5,6,7,8,12,16,32,64,512 -n 0 -r 50 -t 16\n\nmodel| size| params| backend| ngl| test| t/s  \n---|---|---|---|---|---|---  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 1| 15.48 \u00b1 0.73  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 2| 25.94 \u00b1 0.59  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 3| 32.57 \u00b1 1.29  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 4| 37.63 \u00b1 0.57  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 5| 40.86 \u00b1 1.22  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 6| 43.59 \u00b1 0.75  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 7| 45.92 \u00b1 0.40  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 8| 33.38 \u00b1 0.56  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 12| 53.02 \u00b1 0.58  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 16| 69.40 \u00b1 1.32  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 32| 78.17 \u00b1 0.57  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 64| 101.11 \u00b1 0.26  \nllama 7B F16| 13.49 GiB| 7.24 B| Metal| 0| pp 512| 101.94 \u00b1 0.70  \n  \nbuild: 492b76d (2645)\n\nFor BS < 8 there is no difference since the SGEMM routines are not used, but\nat BS = 8 the SGEMM performs worse to mainline. Maybe there's room for\nimprovement there.\n\nIt's also a good idea before merging to run some perplexity tests with F16 and\nQ4_0 7B LLaMA models to verify that the numbers are within expectation:\n\n    \n    \n    # use ./scripts/get-wikitext-2.sh to get wiki test data # run ppl (can take a while) ./perplexity -f wikitext-2-raw/wiki.test.raw -m models/mistral-instruct-7b-v0.2/ggml-model-f16.gguf\n\nggerganov reviewed Apr 11, 2024\n\nView reviewed changes\n\n###\n\nDjip007 commented Apr 11, 2024 \u2022\n\n@jart your work is wonderfull. and I think there is room for more\noptimisation. But some may need more control on this operator. @ggerganov is\nworried with the size / maintenance of ggml core.But what if \"TINYBLAS\" is\nadded as a backend (think like simd_backend...) I've spent the last few days\ntrying to figure out the design of llama.cpp and backend. Look that if\n\"TINYBLAS\" is a backend you can have even more control over what you can\nimplement (choice of block size, storage architecture, etc.)[Append]: I read\nthis PR: #5780 (comment) It seems that there are already discussions about how\nto handle rearranged tensor and the use of new backends ..  \n---  \n  \njart force-pushed the sgemm branch 2 times, most recently from 79705b2 to\n2b83bf5 Compare April 16, 2024 02:45\n\nImprove cpu prompt eval speed\n\n183c4bb\n\n    \n    \n    This change upstreams llamafile's cpu matrix multiplication kernels which improve image and prompt evaluation speed. For starters, Q4_0 and Q8_0 weights should go ~40% faster on CPU. The biggest benefits are with data types like f16 / f32, which process prompts 2x faster thus making them faster than quantized data types for prompt evals. This change also introduces bona fide AVX512 support since tinyBLAS is able to exploit the larger register file. For example, on my CPU llama.cpp llava-cli processes an image prompt at 305 tokens/second, using the Q4_K and Q4_0 types, which has always been faster than if we used f16 LLaVA weights, which at HEAD go 188 tokens/second. With this change, f16 LLaVA performance leap frogs to 464 tokens/second. On Intel Core i9-14900K this change improves F16 prompt perf by 5x. For example, using llama.cpp at HEAD with Mistral 7b f16 to process a 215 token prompt will go 13 tok/sec. This change has fixes making it go 52 tok/sec. It's mostly thanks to my vectorized outer product kernels but also because I added support for correctly counting the number of cores on Alderlake, so the default thread count discounts Intel's new efficiency cores. Only Linux right now can count cores. This work was sponsored by Mozilla who's given permission to change the license of this code from Apache 2.0 to MIT. To read more about what's improved, and how it works, see: https://justine.lol/matmul/\n\njart force-pushed the sgemm branch from 2b83bf5 to 183c4bb Compare April 16,\n2024 02:46\n\nContributor Author\n\n###\n\njart commented Apr 16, 2024\n\n@ggerganov Since my change doesn't help much on M2, I changed it to be off by\ndefault on that platform.\n\n    \n    \n    #ifndef GGML_USE_LLAMAFILE #ifdef __ARM_FEATURE_MATMUL_INT8 #define GGML_USE_LLAMAFILE 0 #else #define GGML_USE_LLAMAFILE 1 #endif #endif\n\nPTAL  \n---  \n  \nContributor\n\n###\n\ngithub-actions bot commented Apr 16, 2024\n\n\ud83d\udcc8 llama.cpp server for bench-server-baseline on Standard_NC4as_T4_v3 for\nphi-2-q4_0: 462 iterations \ud83d\ude80  \n---  \n  \nggerganov approved these changes Apr 16, 2024\n\nView reviewed changes\n\nOwner\n\n###\n\nggerganov left a comment\n\nThere was a problem hiding this comment.\n\n### Choose a reason for hiding this comment\n\nThe reason will be displayed to describe this comment to others. Learn more.\n\n> Since my change doesn't help much on M2, I changed it to be off by default\n> on that platform.\n\nApart from the dip at BS=8, on my machine it does help - at BS=512 the GEMM in\nthis PR is almost 2x faster. This is with LLAMA_NO_ACCELERATE=1 though which\ndisables the Apple's CBLAS implementation from the Accelerate framework - for\nlarge BS this remains more efficient. Anyway, we can refine in the future\n\nRegarding GGML_USE_LLAMAFILE - as it is, when I upstream the changes to the\nggml repo, the build will fail because there is no sgemm.cpp there. My idea\nwas in the llama.cpp Makefile and CMake to define GGML_USE_LLAMAFILE=1 by\ndefault (unless LLAMA_NO_LLAMAFILE is set). I can of course add\nGGML_USE_LLAMAFILE=0 in the ggml repo, but it's better to have this as the\ndefault for now\n\nContributor Author\n\n###\n\njart commented Apr 16, 2024\n\nI vaguely recall when I was working in an experimental branch, the 8x3 kernel\nhttps://twitter.com/JustineTunney/status/1776440470152867930 would make GGML\ngo faster than Accelerate. I've been reluctant to cause too much churn here in\nthe interest of getting this PR in. Is there anything specific you need me to\nchange on my end before this can be merged?  \n---  \n  \nggerganov merged commit 8cc91dc into ggerganov:master Apr 16, 2024\n\n62 checks passed\n\nOwner\n\n###\n\nggerganov commented Apr 16, 2024\n\n> the 8x3 kernel twitter.com/JustineTunney/status/1776440470152867930 would\n> make GGML go faster than Accelerate\n\nI don't think the RPi5 uses the Accelerate framework. AFAIK it's available on\nApple devices and the SGEMM that comes with it runs on some sort of\nspecialized AMX coprocessor available in Apple Silicon, which brings extra\nperformance to the table.  \n---  \n  \nggerganov mentioned this pull request Apr 16, 2024\n\nggml : fix llamafile sgemm wdata offsets #6710\n\nMerged\n\nSign up for free to join this conversation on GitHub. Already have an account?\nSign in to comment\n\nLabels\n\nNone yet\n\n18 participants\n\nAdd this suggestion to a batch that can be applied as a single commit. This\nsuggestion is invalid because no changes were made to the code. Suggestions\ncannot be applied while the pull request is closed. Suggestions cannot be\napplied while viewing a subset of changes. Only one suggestion per line can be\napplied in a batch. Add this suggestion to a batch that can be applied as a\nsingle commit. Applying suggestions on deleted lines is not supported. You\nmust change the existing code in this line in order to create a valid\nsuggestion. Outdated suggestions cannot be applied. This suggestion has been\napplied or marked resolved. Suggestions cannot be applied from pending\nreviews. Suggestions cannot be applied on multi-line comments. Suggestions\ncannot be applied while the pull request is queued to merge. Suggestion cannot\nbe applied right now. Please check back later.\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
