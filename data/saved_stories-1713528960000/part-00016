{"aid": "40083097", "title": "Continued Attacks on HTTP/2", "url": "https://lwn.net/Articles/968600/", "domain": "lwn.net", "votes": 1, "user": "signa11", "posted_at": "2024-04-19 03:09:20", "comments": 0, "source_title": "Continued attacks on HTTP/2 [LWN.net]", "source_text": "Continued attacks on HTTP/2 [LWN.net]\n\nLWN .net News from the source\n\n  * Content\n\n    * Weekly Edition\n    * Archives\n    * Search\n    * Kernel\n    * Security\n    * Events calendar\n    * Unread comments\n    * LWN FAQ\n    * Write for us\n  * Edition\n\n    * Return to the Front page\n\n| |\n\nSubscribe / Log in / New account\n\n# Continued attacks on HTTP/2\n\nPlease consider subscribing to LWNSubscriptions are the lifeblood of LWN.net.\nIf you appreciate this content and would like to see more of it, your\nsubscription will help to ensure that LWN continues to thrive. Please visit\nthis page to join up and keep LWN on the net.  \n---  \n  \nBy Daroc Alden April 10, 2024\n\nOn April 3 security researcher Bartek Nowotarski published the details of a\nnew denial-of-service (DoS) attack, called a \"continuation flood\", against\nmany HTTP/2-capable web servers. While the attack is not terribly complex, it\naffects many independent implementations of the HTTP/2 protocol, even though\nmultiple similar vulnerabilities over the years have given implementers plenty\nof warning.\n\nThe attack itself involves sending an unending stream of HTTP headers to the\ntarget server. This is nothing new \u2014 the Slowloris attack against web servers\nusing HTTP/1.1 from 2009 worked in the same way. In Slowloris, the attacker\nmakes many simultaneous requests to a web server. Each request has an unending\nstream of headers, so that the request never completes and continues tying up\nthe server's resources. The trick is to make these requests extremely slowly,\nso that the attacker has to send relatively little traffic to keep all the\nrequests alive.\n\nIn the wake of the Slowloris attack, most web servers were updated to place\nlimits on the number of simultaneous connections from a single IP address, the\noverall size of headers, and on how long the software would wait for request\nheaders to complete before dropping the connection. In some web servers,\nhowever, these limits were not carried forward to HTTP/2.\n\nIn 2019, there were eight CVEs reported for vulnerabilities exploitable in\nsimilar ways. These vulnerabilities share two characteristics \u2014 they involve\nthe attacker doing unusual things that are not explicitly forbidden by the\nHTTP/2 specification, and they affect a wide variety of different servers. Web\nservers frequently have to tolerate clients that misbehave in a variety of\nways, but the fact that these vulnerabilities went so long before being\nreported is perhaps an indication that there are few truly broken clients in\nuse.\n\nTwo of these vulnerabilities in particular, CVE-2019-9516 and CVE-2019-9518,\ncan involve sending streams of empty headers, which take a disproportional\namount of CPU and memory for the receiving server to process compared to the\neffort required to generate them. Nowotarski's attack seems like an obvious\nvariation \u2014 sending a stream of headers with actual content in them. The\nattack is perhaps less obvious than it seems, given that it took five years\nfor anyone to notice the possibility.\n\n#### Continuation flooding\n\nHTTP/2 is a binary protocol that divides communications between the client and\nthe server into frames. Headers are carried in two kinds of frame: an initial\nHEADERS frame, followed by some number of CONTINUATION frames. The\ncontinuation flood attack involves sending a never-ending string of\ncontinuation frames, with random header values packed inside them. Because\nthey are random, these headers are certainly not meaningful to the receiving\nserver. Despite this, some servers still allocate space for them, slowly\nfilling the server's available memory. Even servers which do place a limit on\nthe size of headers they will accept usually choose a large limit, making it\nrelatively straightforward to consume their memory using multiple connections.\n\nAnother wrinkle is that HTTP/2 requests are not considered complete until the\nlast continuation frame is received. Several servers that are vulnerable to\nthis attack don't log requests \u2014 failed or otherwise \u2014 until they are\ncomplete, meaning that the server can die before any indication of what\nhappened makes it into the logs. The fact that continuation flooding requires\nonly one request also means that traditional abuse-prevention tools, which\nrely on noticing a large number of connections or traffic from one source, are\nunlikely to automatically detect the attack.\n\nNowotarski listed eleven servers that are confirmed to be vulnerable to the\nattack, including the Apache HTTP Server, Node.js, and the server from the Go\nstandard library. The same announcement stated that other popular servers,\nincluding NGINX and HAProxy, were not affected.\n\nIt is tempting to say that all these attacks \u2014 the eight from 2019, and now\ncontinuation flooding \u2014 are possible because HTTP/2 is a complex, binary\nprotocol. It is true that HTTP/2 is substantially more complicated than\nHTTP/1.1, but every version of HTTP had had its share of vulnerabilities. The\nunfortunate truth is that implementing any protocol with as many divergent use\ncases as HTTP is difficult \u2014 especially when context is lost between designers\nand implementers.\n\nThe designers of HTTP/2 were well aware of the potential danger of DoS\nattacks. In July 2014, Roberto Peon sent a message to the ietf-http-wg mailing\nlist talking about the potential for headers to be used in an attack:\n\n> There are three modes of DoS attack using headers: 1) Stalling a connection\n> by never finishing the sending of a full set of headers. 2) Resource\n> exhaustion of CPU. 3) Resource exhaustion of memory.\n>\n> [...]\n>\n> I think #3 is the interesting attack vector.\n\nThe HTTP/2 standard does not set a limit on the size of headers, but it does\npermit servers to set their own limits: \"\"A server that receives a larger\nheader block than it is willing to handle can send an HTTP 431 (Request Header\nFields Too Large) status code.\"\" Yet despite this awareness on the part of the\nprotocol designers, many implementers had not chosen to include such a limit.\n\nIn this case, fixing the vulnerability is relatively straightforward. For\nexample nghttp2, the HTTP/2 library used by the Apache HTTP Server and\nNode.js, imposed a maximum of eight continuation frames on any one request.\nHowever, this vulnerability still raises questions about the security and\nrobustness of the web-server software we rely on.\n\nHTTP/2 is a critical piece of the internet. It accounts for somewhere between\n35.5% and 64% of web sites, depending on how the measurement is conducted.\nThere are several tools to help implementers produce correct clients and\nservers. There is a publicly available conformance testing tool \u2014 h2spec \u2014 to\nsupplement each individual project's unit and integration tests. nghttp2 ships\nits own load-testing tool, and Google's OSS-fuzz provides fuzz testing for\nseveral servers. These tools hardly seem sufficient, however, in light of the\nongoing discovery of vulnerabilities based on slight deviations from the\nprotocol.\n\nThe continuation flood attack is not particularly dangerous or difficult to\nfix, but the fact that it affects so many independent implementations nearly\nnine years after the introduction of HTTP/2 is a stark wakeup call. Hopefully\nwe will see not only fixes for continuation flooding, but also increased\nattention on web server reliability, and the tests to ensure the next issue of\nthis kind does not catch us by surprise.\n\nIndex entries for this article  \n---  \nSecurity| Vulnerabilities/Denial of service  \n  \n(Log in to post comments)\n\n### Continued attacks on HTTP/2\n\nPosted Apr 10, 2024 14:21 UTC (Wed) by josh (subscriber, #17465) [Link]\n\nHTTP/2 has less and less of a use case justifying it. I wouldn't be surprised\nif some new frameworks start only supporting HTTP/1.1 (for compatibility) and\nHTTP/3 (for performance).\n\n### Continued attacks on HTTP/2\n\nPosted Apr 10, 2024 20:04 UTC (Wed) by barryascott (subscriber, #80640) [Link]\n\nHttp/3 would be attackable in the same way as http/2 I assume as the\ndifference between them is that UDP is used instead of TCP.\n\nBoth will need memory limits and suitable timeouts to defend against attacks\nor client bugs.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 10, 2024 21:09 UTC (Wed) by Heretic_Blacksheep (subscriber,\n#169992) [Link]\n\nProbably. Josh's argument would fit nearly any HTTP/1.x replacement. \"No need\nfor HTTP/4, HTTP/1.x for compatibility & HTTP/5 for 'performance'.\" ... Ooops\nforgot to set limits on resource limits for all HTTP/5 sessions... HTTP/6 for\n\"performance.... Pretty soon it's XKCD territory again.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 10, 2024 21:38 UTC (Wed) by josh (subscriber, #17465) [Link]\n\nI'm not suggesting otherwise. I'm observing that it's easier to secure and\noptimize two versions of HTTP than three versions of HTTP, in terms of attack\nsurface area.\n\n(And no, the differences between HTTP/2 and HTTP/3 are more substantial than\njust TCP vs UDP.)\n\n### Continued attacks on HTTP/2\n\nPosted Apr 10, 2024 22:19 UTC (Wed) by flussence (subscriber, #85566) [Link]\n\nIf you install a mainstream webserver today, HTTP/3 practically doesn't even\nexist. It's unlikely to exist in that realm for a year or two, and when it\nfinally becomes an option it'll be an invasive change for system\nadministrators. I'm sure we'll start seeing the usual CVE per month when that\nhappens. It's going to be fun to discover the consequences of throwing out the\nlast 15 years of TCP congestion control work for userspace rate-control code\nof varying quality too.\n\nMaybe the reason we haven't heard much of security problems with H3 yet is\nthat it's more or less the exclusive domain of global domination CDNs with\nenough money to afford top-to-bottom control of their stack; people who only\nconcern themselves with situations that show up as an outlier on a slick,\njavascript-animated metrics dashboard graph, perhaps hosted on a vanity gTLD.\nAt its core H3 currently suffers from the same phenomenon that saddled btrfs\nwith a decade-long reputation for losing your data: Facebook has a million-\nserver failover setup and does not care about \u201cEarth dweller\u201d matters like\nRAID5/6 being broken.\n\nNot that this is to imply HTTP/1.1 is secure, mind you. The push for H2\nadoption was in part a collective panic from finding out how many ways a\nsimple line-based text protocol can be screwed up.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 11, 2024 9:31 UTC (Thu) by paulj (subscriber, #341) [Link]\n\nHTTP/3 runs over QUIC, and QUIC requires a congestion controller to be\nimplemented. That's usually either BBRv1 and/or CUBIC in implementations I'm\nfamiliar with. I.e., they're doing the same congestion control as TCP, and\nthey should work fairly with existing TCP flows.\n\n### HTTP/3 packaging problems: is OpenSSL at fault?\n\nPosted Apr 13, 2024 13:57 UTC (Sat) by DemiMarie (subscriber, #164188) [Link]\n\nIs this because of OpenSSL not supporting third-party QUIC implementations?\n\n### HTTP/3 packaging problems: is OpenSSL at fault?\n\nPosted Apr 13, 2024 17:24 UTC (Sat) by mbunkus (subscriber, #87248) [Link]\n\nSomewhat related: I saw a toot[1] by Daniel Stenberg (the main curl\ndeveloper/maintainer) the other day about the still bad performance of the\nQUIC implementation of even the latest & greatest OpenSSL release 3.3. It\nlinked to the a mailing list post[2] which included some more details about\nboth the API being insufficient (having to fall back to inefficient pulling) &\nthe performance severely lacking.\n\nMake of that what you will.\n\n[1] https://mastodon.social/@bagder/112243310605678729 [2]\nhttps://curl.se/mail/distros-2024-04/0001.html\n\n### HTTP/3 packaging problems: is OpenSSL at fault?\n\nPosted Apr 14, 2024 5:42 UTC (Sun) by wtarreau (subscriber, #51152) [Link]\n\nThe problem is that the openssl devs are stubborn and decided that it suddenly\nbecame their job to develop a transport layer that would work everywhere.\nThere are plenty of different QUIC stacks and each of them is different\nprecisely because the constraints are not the same in each implementation.\nWhat was needed from them was just to accept the 28 patches from quictls to\nexpose the minimally needed internals, nothing more. But apparently this was\nnot giving a good image of their work to the foundation so they preferred to\npretend they would develop the ultimate QUIC stack to keep some funding.\nThey'd rather work on fixing their monstrous locking issues that plague their\ncode on modern systems instead. That's where everyone is expecting them to\nspend their time, in order to address the mess they created and that only them\ncan fix.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 11, 2024 9:29 UTC (Thu) by paulj (subscriber, #341) [Link]\n\nI don't know if there's any significant difference between HTTP/3 and 2 at the\nHTTP application layer. However, HTTP/3 runs over QUIC streams. QUIC streams\nare individually flow-controlled. The receiver will advertise a window - it's\nbuffer size for that stream - and the sender can not send more until the\nreceiver has processed some or all of the buffer.\n\ntl;dr: The described DoS does not apply to at least the QUIC streams portion\nof HTTP/3.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 11, 2024 18:09 UTC (Thu) by danielkza (subscriber, #66161) [Link]\n\nI also thought that was the case, but QPACK used in HTTP/3 for header encoding\n(replacing HPACK from HTTP/2) works differently to avoid requiring blocks of\nheader data to be processed in order, and has strict limits on delays and\nmemory usage that need to observed by the sender, otherwise risking complete\nrejection of the request.\n\nThough I found QPACK quite complex (having been able to implement HPACK myself\nwithout much difficulty in the past), it does seem to be safer against\nSlowloris-style attacks by design. Implementing the RFC as-is already gets you\nquite close to avoiding these attacks altogether.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 10, 2024 22:59 UTC (Wed) by Curan (subscriber, #66186) [Link]\n\n> It is tempting to say that all these attacks \u2014 the eight from 2019, and now\n> continuation flooding \u2014 are possible because HTTP/2 is a complex, binary\n> protocol.\n\nIt is and it is most likely true, isn't it? What was the real benefit of\nHTTP/2 over HTTP/1.1? Was it the compressed headers? Than it might just be\neasier to get rid of all the pointless ones and save space. Are there real\nissues we could not have saved by mandating pipelining for HTTP/2 or a\nhypothetical HTTP/1.3 over 1.1? Scheduling of responses on the server is also\nan issue thanks to the wonderful idea of multiplexing. And many more issues.\n\nAnd hey, there is always https://queue.acm.org/detail.cfm?id=2716278 to point\nto.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 11, 2024 17:13 UTC (Thu) by barryascott (subscriber, #80640) [Link]\n\nhttp/2 only needs to do one TCP connection and then multiplexies lots of HTTP\ntransactions over that one connection. This speeds up loading many resources\nin parallel.\n\nI cannot remember if there is header compression, I don't recall seeing that,\nbut I've not been keeping up.\n\nhttp/3 uses UDP so that the big problem of http/2, head-of-line blocking,\ncould be solved.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 11, 2024 19:07 UTC (Thu) by danielkza (subscriber, #66161) [Link]\n\nHTTP2 uses a custom encoding/compression for headers named HPACK, HTTP3 uses a\nsucessor based on same principles named QPACK.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 11, 2024 12:26 UTC (Thu) by wtarreau (subscriber, #51152) [Link]\n\nWhat's really annoying with the security threater these days is that the same\nvulnerabilities are \"rediscovered\" in loops every 10 years because neither\nimplementers nor security researchers read specs. The risks of DoS around\nCONTINUATION were discussed to death 10 years ago during the design phase\nbetween those of us who just didn't want it, those who wanted infinite\nheaders, and led to tons of proposals and \"I will never ever implement this\ndangerous crap\" for every proposal. In the end the current status on\nCONTINUATION was the least rejected, was clearly documented as dangerous in\nsection 10.5 of the spec by then, and guess what ? 10 years later you find\nthat the stacks that are finally completely unaffected are the ones from those\nwho were saying \"warning, DoS in sight\". Just found the archives, they're here\n(most of the longest threads such as \"striving for compromise\", \"jumo !=\nfragmentation\", \"large frames\", \"continuation\" etc):\nhttps://lists.w3.org/Archives/Public/ietf-http-wg/2014Jul...\n\nWhile I can hardly imagine why someone would be {cr,l}azy enough to want to\nallocate RAM for an incomplete in-order frame instead of naturally memcpy() it\nat the end of the previous one (except maybe when dealing with problematic\nlanguages that make it hard to simply store data in anything but an object), I\nfind it ironical that in the end it's the Go, Rust, JS and C++ stacks that are\nvulnerable and that the 3 pure-C ones are not :-) It's at least a hint to fuel\nmy suspicion that not being constrained by a language to do things only\nfollowing certain (possibly wrong) ways avoids certain classes of control-\nbased bugs.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 11, 2024 15:07 UTC (Thu) by tialaramex (subscriber, #21167) [Link]\n\nI would suggest that what you've seen is that in a language (C) where even\neasy things are difficult, people refrained from trying to do hard things\nentirely, and so you won't find any bugs in the hard things only because they\nweren't implemented at all.\n\nIf we're sure hard things are just always a bad idea then I agree that's a win\nfor C. But, maybe hard things are sometimes a good idea, and then in C you're\ngoing to really struggle.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 11, 2024 17:53 UTC (Thu) by wtarreau (subscriber, #51152) [Link]\n\n> you won't find any bugs in the hard things only because they weren't\n> implemented at all\n\nI'm definitely certain that it's not the case since this stuff was designed\n(based on the working group feedback) to just be of moderate difficulty, and\nwas implemented as necessary by everyone. And when I did it on my side, I did\nit with DoS in mind. I.e. the vast majority of the time the code will be used\nwill not be because Chrome wants to grease the connection but because an\nattack is sending 10 million frames a second to the front machine and that one\nmust resist.\n\n> If we're sure hard things are just always a bad idea then I agree that's a\n> win for C. But, maybe hard things are sometimes a good idea, and then in C\n> you're going to really struggle.\n\nFor some complex features that's generally true. I.e. I wouldn't write a\nbrowser. But when dealing with network protocols and wire parsers in general,\nC is really well suited because operations are generally simple, and you want\nto be able to use every opportunity to save every single possible nanosecond\nsince it's just a matter of life or death for your program when facing the\nbrutality of the net.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 13, 2024 7:42 UTC (Sat) by epa (subscriber, #39769) [Link]\n\nWhat you say is probably true for expert programmers. But for anyone less than\nperfect C is a dangerous choice for wire parsers or for decoding any binary\nformat. The past thirty years have seen a steady stream of vulnerabilities\nfrom file format parsers written in C. A safer language (with checked array\naccess, error on overflow, and less need for pointer manipulation) would be a\nbetter choice for most fallible humans when faced with a skilled attacker who\nonly needs to be lucky once.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 15, 2024 12:22 UTC (Mon) by wtarreau (subscriber, #51152) [Link]\n\nThe problem is more about the practice and the way the language was taught\nrather than the language itself (others have the same limitations, starting\nfrom assembly). The problem is that when you're first taught programming, it's\nexplained to you that if you go out of bounds, it will segfault and crash. And\nparticularly since systems have been starting to ship with ulimit -c 0 by\ndefault, this has been perceived as the cheap error handling: why write a\nlength check and go through error handling if the result is the same ?\n\nWhat we need is computer teachers explaining first how to exploit a parser bug\nvia a config file, a log line, a large HTTP header, etc so that early\ndevelopers keep this in mind and never ever start with a bad habit. Laziness\nshould not be an option. In parallel, compilers should make it easier to write\ncorrect code. For now they're doing the opposite, they're making it ultra-\ndifficult to write correct code, forcing users to case, put attributes and\nwhat not to shut stupid warnings, so the simplest path to valid code is the\nleast safe.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 15, 2024 12:47 UTC (Mon) by Cyberax (\u272d supporter \u272d, #52523) [Link]\n\n> And particularly since systems have been starting to ship with ulimit -c 0\n> by default, this has been perceived as the cheap error handling: why write a\n> length check and go through error handling if the result is the same ?\n\nWhen I was studying at a university about 25 years ago, we actually had a\nmandatory course where we wrote an exploit, with simple shell code, for a\ndeliberately vulnerable server written in C. So people certainly know about\nthe danger of OOB access.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 3:23 UTC (Tue) by wtarreau (subscriber, #51152) [Link]\n\nThat's great that you had this opportunity. The first time a person taught me\nabout the ability to overflow a buffer and execute code 30 years ago, I almost\nlaughed, and said \"you'd be lucky if that would surprisingly work\", and he\ntold me \"it works more often than you think\". That's when I started\nexperimenting with it and figured how hard it was to achieve on sparc (due to\nswitched register banks) that I wrote a generic exploitation tool for this and\nfinally managed to get root on some systems :-) I just felt sad that it was so\nmuch ignored by teachers themselves.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 15, 2024 13:30 UTC (Mon) by farnz (subscriber, #17727) [Link]\n\nThe person I worked with who was worst for writing code with security bugs was\ntaught in exactly the way you describe; his attitude after graduating was that\nthis was \"just theory\", and therefore he didn't have to care about secure\nhandling of predictable errors since \"it crashes, so we'll know if it's got a\nbug because we'll get bug reports\". He was great at exploiting bugs, but\nuseless at preventing them.\n\nIME, the thing that helps you learn is to work in languages where you simply\ncannot write certain classes of bug without a compiler error; writing code\nthat compiles in Agda is a lot harder to learn to do than writing code that C\ncompilers will accept, but if you're used to thinking in terms of \"how do I\nwrite this in a way that the compiler can see is bug-free?\", you're better at\nwriting code that is genuinely bug free, even when you then learn how to write\nC (albeit that you're also more likely to write a program extractor that takes\nyour Agda, removes the proof-related bits, and outputs C).\n\n### Continued attacks on HTTP/2\n\nPosted Apr 15, 2024 19:52 UTC (Mon) by epa (subscriber, #39769) [Link]\n\nEven if it did reliably segfault and crash, that\u2019s still not a good choice for\na format parser run in-process as part of a larger application. Heck, even a\ncommand line tool would be considered buggy if it crashed on bad input instead\nof giving a helpful error.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 11:05 UTC (Tue) by paulj (subscriber, #341) [Link]\n\nThis was kind of the experience with the \"wise\" C network code I maintained.\nThe parsers - as per other comments - were structured to use a simple,\nchecking abstraction layer to read/write atoms. If the higher-level parser\nmade a logical mistake and issued an out of bounds read or write, the checking\nlayer would abort().\n\nThis solved the problem of buffer overflows. However, we would still generally\nhave a DoS security bug from the process ending itself. Obviously, an abort()\nand possible DoS is still /way/ better than an overflow and RCE security bug,\nbut also still not ideal.\n\nThe next challenge was to make the parsers logically robust. Memory safe\nlanguages do not solve this.\n\nExplicit state machines, in languages (programming or DSLs) that can verify\nerror events are always handled (e.g., by proceeding to some common error\nstate) can help. And even C these days can trivially ensure that all events\nare handled in an a state machine. Requires programmer discipline though.\n\nIt's worth noting in this story (IIUC) that the implementations in \"memory\nsafe\" languages were vulnerable to the bug, and implementation in the \"unsafe\"\nlanguage was not. ;)\n\n### Continued attacks on HTTP/2\n\nPosted Apr 15, 2024 14:07 UTC (Mon) by paulj (subscriber, #341) [Link]\n\nIt is possible write parsers safely, in C. All that is needed is to separate\nthe parser from the data manipulation. E.g., by providing a small, simple\nbuffer abstraction that provides methods to safely read and write the various\natoms which the parser should deal in. The methods do the bounds checking.\nOften such abstractions provide a cursor, from which to next read or write (or\nseparate read and write cursors), but not necessary.\n\nSuch an abstraction is pretty trivial to write. It's pretty simple, and low in\nbranches - trivial to make provably correct (by inspection, by 100% branch\ncoverage, etc.), and it's still going to be fast in C. The parser can be\nguaranteed to be immune to buffer overflows, and guaranteed that it will\nterminate cleanly if it makes a syntactical error. It's also good practice in\nwriting parsers to structure code this way, regardless of how safe the\nlanguage is.\n\nDoesn't handle logical errors in state, but then... neither do \"safe\"\nlanguages.\n\nI've worked on network code using such, with significant internet exposed\nparsers, and never had any buffer overflow bugs or undefined behaviour, in...\ndecades. (Thank you Kunihiro Ishiguro for your wisdom on this - back in the\n90s, an era full of C network code that lacked your wisdom, consequences of\nwhich we still pay for today).\n\nThat said... There is the problem that other programmers will sometimes come\nalong, the kind who are sure they can write parsers that directly twiddle\npointers and who think such abstractions are overhead, and they will....\nignore those abstractions, and write direct-pointer-twiddling parsing code.\nAnd such programmers - of course - are never as infallible and great as they\nthought, and such code does eventually end-up having overflow and attacker-\ncontrolled, undefined memory write bugs. And even a more disciplined\nmaintainer/reviewer type tries to say no to these programmers, they will take\numbrage - causing other problems, and they may find ways to get around it and\nget their code in anyway.\n\nSo, if you have the discipline to correctly layer your parsing code, into a\nlow-level, simple, checked, lexer (of whatever atoms - binary or otherwise) a\nhigher-level logical parser, you can do this safely and trivially in C.\nHowever, I also understand the argument that - through ignorance and/or ego -\nsome programmers simply can not be trusted to exercise that discipline, and a\nprogramming language that enforces it on them is required.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 15, 2024 14:46 UTC (Mon) by Wol (subscriber, #4433) [Link]\n\n> So, if you have the discipline to correctly layer your parsing code, into a\n> low-level, simple, checked, lexer (of whatever atoms - binary or otherwise)\n> a higher-level logical parser, you can do this safely and trivially in C.\n\nAgain, this is what I bang on about always having a state table - in your head\nif nowhere else! If you have two input state variables, and three output\nstates, then you have a massive logic hole that someone can exploit! I take\nPizza's point about that 1024x1024 state table, but at the end of the day\ndealing with that is (mathematically) trivial. Especially if you can say\n\"these are the state bits I am dealing with, everything else is EXPLICITLY a\nno-op\".\n\nTalking of parsers, my supervisor wrote a lexer/parser he was very proud of\nyears ago. A couple of months later I was having a load of trouble with it (it\nwas meant to process simple calculations in financial reporting sheets, so\ngetting the right answer was rather important...). So I got out my brother's\nbook on compilers, replaced EIGHT pages of printout that was the lexer with\none line - a statement call that was built into the language - and rewrote the\nparser to output an RPN state table. In the process, I discovered that my\nsupervisor had messed up his lexing rules and all the bugs were down to\npartially-lexed output being fed into the parser. Whoops. My resulting engine\nwas much more robust, and much more powerful (in that it had fully-functional\nprecedence rules).\n\nALWAYS handle ALL possible states, even if it's a no-op, but it has to be a\nconscious no-op! Anything else is an error, a bug, a potential vulnerability.\n\nCheers, Wol\n\n### Continued attacks on HTTP/2\n\nPosted Apr 15, 2024 15:25 UTC (Mon) by paulj (subscriber, #341) [Link]\n\nDealing with state machines in a structured way is also good in parsing, yeah.\n\nThere are tools out there for this, though, most seem geared for FSMs for\ntext. So you probably have to roll your own. But that isn't difficult either\nin most languages.\n\nNetwork protocol state machines are highly unlikely to have a distinct space\nof 1024 states by 1024 events though. It's usually much more tractable. And if\na network protocol state machine did have that, I'd start looking at backing\naway from that protocol. (E.g., by supporting only a simplified subset in some\ncompat mode, and designing my own simpler version for my needs). ;)\n\n### Continued attacks on HTTP/2\n\nPosted Apr 15, 2024 15:06 UTC (Mon) by mb (subscriber, #50428) [Link]\n\nYep, simply don't write bugs and you'll end up having completely bug free\ncode. It's that simple.\n\nExcept that is apparently isn't. The reality proves that.\n\nWe have been trying that for decades. But still about half of the security\nrelevant bugs are simple memory corruption bugs.\n\nIt's great that you know how to write a safe parser. You are an experienced\ndeveloper and you did your parser implementations mistakes probably decades\nago. You leant from it.\n\nMost people are not on that level, though. But still, they will write parsers.\nFor example because their employer demands it. Or because they think they are\nable to correctly do it.\n\nAnd that is exactly where memory safe languages come into play. They detect\nbugs that beginners make and they give an assurance to professionals that\ntheir code is indeed correct. Both are a win.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 15, 2024 15:18 UTC (Mon) by paulj (subscriber, #341) [Link]\n\nI'm old enough to have the scars, yeah.\n\nI guess my point is that: 1. I agree that it's better to use safe languages by\ndefault. 2. However, I would /disagree/ with anyone who said we should /never/\nuse C anymore for network parsers. If done correctly, with the right layering\nof checked abstractions, it can be done safely. It's not automatically bad.\nAnd performance does still matter.\n\nI'll echo Willy's point about CPU and energy use mattering in DC contexts.\nEvery % of extra CPU in lower-level network code costs significant amounts of\nmoney per year in both OpEx and CapEx when it is underpinning the majority of\nconnections in all the applications you're running on X thousands of servers.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 15, 2024 16:09 UTC (Mon) by mb (subscriber, #50428) [Link]\n\n>And performance does still matter.\n\nI don't think that a Rust implementation would be any slower than your hand\nwritten verify-every-raw-tokenization C implementation. In fact, the Rust\nimplementation could potentially even be faster, if it is able to eliminate\ncertain down-stream checks by proving that they can't happen. (e.g. token enum\ncan't have an invalid value). But at the very least it will be as fast as the\nC code, in this case. The verification checks of each token access in the raw\nstream are required in both cases.\n\nYes, there's probably no reason to rush and rewrite all existing parsers in\nmemory safe languages. But there's also hardly any reason to still write new\nparsers in unsafe languages.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 15, 2024 15:37 UTC (Mon) by paulj (subscriber, #341) [Link]\n\nOh, and my other point there was there is a very simple way to make parsing in\nC safe - just a very simple checking abstraction. It's reallly not much code,\nnor complex.\n\nYou don't have to write it. You can find battle-tested ones. They are very\neasy to use.\n\nFor some reason, for decades, C programmers just - by and large - have ignored\nthis option. It goes beyond ignorance or hubris of individual programmers. It\nspeaks to a failure in Software Engineering as a profession. That we have so\nmany engineers who simply don't understand how to do safe parsing in a low-\nlevel language.\n\nCause it isn't hard. It really is not.\n\nIt's actually a trivial problem. That /specific/ problem should have been\nfixed /decades/ ago simply by educating young engineers correctly. It should\n/not/ have needed a) the invention of new languages with either complex\nruntimes and/or complex type systems; b) the wide-spread adoption of such\nlanguages; c) the rewriting of old code into such languages. As a), b) and c)\nobviously imply a very significant amount of time to solve the problem.\n\nI am not arguing against a, b and c - because they can solve many other issues\nbesides just the specific problem of preventing buffer overflows in network\nparsers due to syntactical or logical errors. However, that specific problem\nis trivial and should have been addressed decades ago.\n\nThere is a deeper problem here in the profession of software engineering. We\nsuck at being \"professional engineers\".\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 8:47 UTC (Tue) by paulj (subscriber, #341) [Link]\n\nAnd just to illustrate how. Consider how many times in professional\npublications (inc. LWN) you have read articles of the following form:\n\n1\\. Here's a shiny new language, which can fix the regularly occurring problem\nof buffer overflows in network exposed software, leading to serious or\ncatastrophic security issues, which has been a long standing issue. Here are\nthe shiny features which can prevent such issues, and make programmers more\nproductive. The language is almost stable, and more and more people are using\nit, and we're seeing more serious software being written in it. The more it's\nused the better! Shiny!\n\n2\\. Buffer overflows in network exposed software, leading to serious or\ncatastrophic security issues, are a long standing issue in our profession. The\nlanguage of 1 or 2 (or ...) has a lot of promise in systematically solving\nthis problem. However, existing language are certain to remain in use until\nlanguage X is stable and widely used. Further it will take a long time before\nmuch software in existing language is rewritten to language 1. This article\ncovers simple techniques that can be used to make network exposed parsers at\nleast secure to buffer overflows, which every programmer writing network code\nin existing languages should know. It includes (pointers to) proven, simple,\nlibrary code that can be used.\n\nThink how many times over the last few *decades* you have seen articles of the\nfirst form, and how often of the latter form. I am sure you have seen the\nfirst form many times. I will wager it is rare you have the seen former,\nindeed, possibly never. Yet, a _serious_ profession of engineering would\n_ensure_ (via the conscientiousness of the engineers practicing it) that\narticles of the latter form were regularly printed, to drum it in other\nengineers.\n\nWorse, there is a culture of even putting down ad-hoc comments making the\npoints of form 2, (and I'm _not_ trying to make a dig at commenters here!)\nwith \"Sure, yeah, just write bug-free software!\" usually with something like\n\"Seriously, shiny language is the only way.\". And yes, there is a lot of\n*truth* to it that $SHINY_LANGUAGE is the way forward for the future. However\nthat is on a _long_ term basis. In the here and now, techniques that apply to\ncurrent languages, and current software, and do /not/ need to wait for shiny\nlanguage to mature, stabilise and be widely deployed, understood and with a\nbody of software to build on (library code, learning from, etc.), are\n_required_ to solve problems _until then_. Frowning on attempts to distribute\nthat knowledge has _not_ helped this engineering profession avoid many\nsecurity bugs over the last decade+ while we have waited for (e.g.) Rust.\n\nA serious engineering profession would be able to _both_ look forward to the\nnext-generation shiny stuff, _and_ be able to disseminate critical information\nabout best-practice techniques for _existing_ widely-used tooling. It's not an\neither-or! :)\n\nFellow engineers, let's do better. :)\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 14:08 UTC (Tue) by wtarreau (subscriber, #51152) [Link]\n\n100% agreed.\n\nBut there's also another factor which does not help, and I'm sure many of us\nhave known such people: the lower level the language you practise, the most\nexpert you look to some people. And there are a lot of people using C and ASM\nwho regularly brag saying \"ah, you didn't know this ? I genuinely thought\neveryone did\". That hurts a lot because it puts a barrier in the transmission\nof knowledge on how to do better. In addition, with the C spec not being\nfreely available (OK the latest draft is, but that's not a clean approach),\nyou figure that a lot of corner cases have long been ignored by a lot of\npractisers (by far the vast majority, starting from teachers at school). For\nexample I learned that signed ints were not supposed to wrap only after 25\nyears of routinely using them that way, when the GCC folks suddenly decided to\nabuse that UB. Having done ASM long before C, what a shock to me to discover\nthat the compiler was no longer compatible with the hardware and that the\nlanguage supported this decision! Other communities are probably more\nwelcoming to newbies and do not try to impose their tricks saying \"look,\nthat's how experienced people do it\". As such I think that some langauges\ndevelop a culture but that there's still little culture around C or ASM,\noutside of a few important projects using these languages like our preferred\noperating system.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 15, 2024 17:12 UTC (Mon) by farnz (subscriber, #17727) [Link]\n\n> Yep, simply don't write bugs and you'll end up having completely bug free\n> code. It's that simple.\n>\n> Except that is apparently isn't. The reality proves that.\n\nNo; what reality proves is that most of us can't dependably do the \"don't\nwrite bugs\" part of that, and that the temptation of 1% better on a metric is\nenough that you will almost always find someone who succumbs to temptation to\nmove the metric, even at the risk of introducing serious bugs.\n\nFor example, if you design your code such that the parser attempts to consume\nas many bytes from the buffer as it can, returning a request that I/O grows\nthe buffer if it can't produce another item, then you get a great pattern for\nbuilding reliable network parsers in any language, and the only thing that\nmakes it easier in newer languages is that they're got more expressive type\nsystems.\n\nBut it's always tempting to do some \"pre-parsing\" in the I/O layer: for\nexample, checking a length field to see if the buffer is big enough to parse\nanother item. And then, once you're doing that, and eliding the call to the\n\"full\" parser when it will clearly fail, it becomes tempting to inline\n\"simple\" parsing in the I/O function, which will perform slightly better on\nsome benchmarks (not those compiled with both PGO and LTO, but not everyone\ntakes care to first get the build setup right then benchmark).\n\nAnd, of course, it also gets tempting to do just a little I/O in the parser -\nwhy not see if there's one more byte buffered if that's all you need to\ncomplete your parse. If one byte is OK, why not 2 or 3? Or maybe a few more?\n\nThat's where tooling comes in really handy - if you use tokio_util::codec to\nwrite your parser, it's now really hard to make either of those mistakes\nwithout forking tokio_util, and it's thus something that's less likely to\nhappen than if you wrote both parts as separate modules in the same project.\nBut there's nothing about this that \"needs\" Rust; you can do it all in\nassembly language, C, FORTRAN 77, or any other language you care to name -\nit's \"just\" a tradeoff between personal discipline of everyone on the project\nand tool enforcement.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 15, 2024 17:41 UTC (Mon) by mb (subscriber, #50428) [Link]\n\n>personal discipline\n\nYeah. We have been told that for decades. \"Just do it right.\" \"Adjust your\npersonal discipline.\" \"Educate yourself before writing code.\"\n\nBut it doesn't work. Otherwise we would not still have massive amounts of\nmemory corruption bugs in C programs.\n\nRust takes the required \"personal discipline\" part and throws it right back\nonto the developer, if she does not behave. Bad programs won't compile or will\npanic.\n\nForcing the developer to explicitly mark code with \"I know what I'm doing, I\neducated myself\" (a.k.a. unsafe) is really the only thing that successfully\nactually reduced memory corruption bugs in a systems programming language, so\nfar.\n\nSo, if you *really* know what you are doing, feel free to use the unchecked\nvariants. You just need to mark it with \"unsafe\". It's fine.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 9:08 UTC (Tue) by farnz (subscriber, #17727) [Link]\n\nIt does work - if it didn't work, then Rust would fail because people would\njust use unsafe without thinking. Rust lowers the needed level of personal\ndiscipline by making it clearer when you're \"cheating\", because you have to\nwrite unsafe to indicate that you're going into the high-discipline subset,\nbut otherwise it's the same as C.\n\nSo, by your assertions, I should expect to see a large amount of Rust code\nwith undisciplined uses of unsafe, since all developers cheat on the required\nprogramming discipline that way. In practice, I don't see this when I look at\ncrates.io, which leads me to think that your assertion that no programmer is\ncapable of remaining disciplined in the face of temptation is false.\n\nWhat we do know is that population-wide, we have insufficient programmers\ncapable of upholding the required level of discipline to use C or C++ safely\neven under pressure to deliver results. It's entirely consistent to say that\nwtarreau and paulj are capable of sustaining this level of discipline in a C\ncodebase, while also saying that the majority of developers will be tempted to\nmove a metric by 1% at the risk of unsafety in a C codebase.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 9:52 UTC (Tue) by mb (subscriber, #50428) [Link]\n\nNope, you mis-interpreted what I was trying to say. Keep in mind that in C\neverything is unsafe.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 10:40 UTC (Tue) by farnz (subscriber, #17727) [Link]\n\nNope, you misinterpreted what I was trying to say.\n\nKeep in mind that things being unsafe is only a problem if you fail to\nmaintain sufficient discipline - and that individuals often can maintain that\ndiscipline, even when a wider group can't.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 10:10 UTC (Tue) by Wol (subscriber, #4433) [Link]\n\n> What we do know is that population-wide, we have insufficient programmers\n> capable of upholding the required level of discipline to use C or C++ safely\n> even under pressure to deliver results. It's entirely consistent to say that\n> wtarreau and paulj are capable of sustaining this level of discipline in a C\n> codebase, while also saying that the majority of developers will be tempted\n> to move a metric by 1% at the risk of unsafety in a C codebase.\n\nThe point of \"unsafe\" is the programmer has to EXPLICITLY opt in to it. The\nproblem with C, and assembler, and languages like that, is that the programmer\ncan use unsafe without even realising it.\n\nThat's also my point about state tables. EVERY combination should be\nEXPLICITLY acknowledged. The problem is that here the programmer has to\nactively opt in to safe behaviour. Very few do. I certainly try, but doubt I\nsucceed. But if somebody reported a bug against my code and said \"this is a\nstate you haven't addressed\" I'd certainly acknowledge it as a bug - whether I\nhave time to fix it or not. It might get filed as a Round Tuit, but it would\nalmost certainly be commented, in the code, as \"this needs dealing with\".\n\nCheers, Wol\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 11:27 UTC (Tue) by farnz (subscriber, #17727) [Link]\n\nIt is entirely possible to have sufficient discipline (as an individual) to\nnot use unsafe code without realising it, and to have a comment in place that\nacknowledges every use of a partially defined operation (which is the issue\nwith unsafe code - there are operations in unsafe code that are partially\ndefined) and justifies how you're sticking to just the defined subset of the\noperation.\n\nHowever, this takes discipline to resist the temptation to do something\npartially defined that works in testing; and the big lesson of the last 200\nyears is that there's only two cases where people can resist the temptation:\n\n  1. Liability for latent faults lies with an individual, who is thus incentivized to ensure that there are no latent faults. This is how civil engineering today handles it - and it took us decades to get the process for this liability handling correct such that either a structure has sign-off from a qualified individual who takes the blame if there are latent faults, or the appropriate people get penalised for starting construction without sign-off.\n  2. It's simple and easy to verify that no latent faults exist; this is a capability introduced from mathematics into engineering, where proving something is hard, but verifying an existing proof is simple. This is also used in civil engineering - actually proving that a structure built with certain materials will stand up is hard, but it's trivial to confirm that the proof that it will stand up is valid under the assumptions we care about.\n\nAnd that's where modern languages help; we know from mathematics that it's\npossible to construct a system with a small set of axioms, and to then have\nproofs of correctness that are easy to verify (even if some are hard to\nconstruct). Modern languages apply this knowledge to cases where we know that,\nas a profession, we make problematic mistakes frequently, and thus make it\neasier to catch cases where the human programmer has made a mistake.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 11:11 UTC (Tue) by paulj (subscriber, #341) [Link]\n\n> the majority of developers will be tempted to move a metric by 1% at the\n> risk of unsafety in a C codebase.\n\nAnd then those developers will write features ignoring the checking\nabstraction that makes the parsers safe (at least from overflows), and submit\npatches with the typical, hairy, dangerous, C parser-directly-twiddling-memory\ncode. And they'll get annoyed when the maintainer objects and asks them to\nrewrite using the safe abstraction that has protected the project well for\nyears.\n\nSigh.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 11:22 UTC (Tue) by farnz (subscriber, #17727) [Link]\n\nExactly, and the direction of travel from hand-writing machine code, through\nassembly, then macro assemblers, and into BLISS, C, Rust and other languages\nis to move from \"this works just fine, it's just hard to verify\" to \"this is\neasy to verify, since the machine helps a lot, and the human can easily do the\nrest\".\n\nBut it's not that Rust makes it possible to avoid mistakes that you cannot\navoid in C; it's that Rust makes it easier to avoid certain mistakes than C,\njust as C makes it easier to avoid certain mistakes than assembly does, and\nassembly is easier to get right than hand-crafted machine code.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 15, 2024 15:07 UTC (Mon) by paulj (subscriber, #341) [Link]\n\nOh, I mostly have network protocols in mind where there is a priori knowledge\nof the length of an atom, before parsing that atom. E.g., integers of fixed\nsizes, or an array or structure with an up-front length field (TLVs). Parsing\nother formats, without an up-front bound (text e.g.), you want an additional\nbuffer abstraction to handle the look ahead, and not complicate the lexer too\nmuch with those details. DoS bugs like this CONTINUATION bug can then be\nbounded at this lowest layer - by imposing an absolute bound on any possible\nlook-ahead, regardless of higher layers. (Higher layers may well have their\nown, tighter, atom or parsing-context specific bounds - but if they mess up,\nthat lower layer can still catch).\n\nAll standard parsing stuff, but... the kinds of programmers who get into\nnetwork coding often differ from the kinds of programmers who learn about how\nto do well-structured parsing. ;)\n\nI am possibly projecting my own tendencies here, as the former type of\nprogrammer who long was - and stilll mostly is - ignorant of the latter ;).\nHowever my experience of others in networking is that there might be a wider\ntruth to it.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 3:34 UTC (Tue) by wtarreau (subscriber, #51152) [Link]\n\nI generally agree with the points you make. For having written some parsers\nusing an FSM in a switch/case loop and with direct gotos between all cases\n(hidden in macros), it ended up as one of the most auditable, reliable and\nunchanged code over 2 decades, that could trivially be improved to support\nevolving protocols and whose performance remained unbeaten by a large margin\ncompared to many other approaches I've seen used. It takes some time but not\nthat much, it mostly requires a less common approach of the problem and a\nparticular mindset of course.\n\nOne reason that it's rarely used is probably that before it's complete, it\ndoes nothing, and it's only testable once complete, contrary to stuffing\nstrncmp() everywhere to get a progressive (but asymptotic) approach to the\nproblem.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 8:52 UTC (Tue) by paulj (subscriber, #341) [Link]\n\nNice.\n\nWith direct jumps between the different handlers? I'd be curious how you\nstructured that to be readable? Functions conforming to some \"interface\"\nfunction pointer are a common way to give handlers some structure. But I don't\nknow of any reasonable way to avoid the indirect jump.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 9:11 UTC (Tue) by wtarreau (subscriber, #51152) [Link]\n\nThe handlers were simple and all inlined in the switch/case so that was easy.\nNowadays the code looks like this:\nhttps://github.com/haproxy/haproxy/blob/master/src/h1.c#L503\n\nIt does a little bit more than it used to but it remains quite maintainable\n(it has never been an issue to adapt protocol processing there). The few\nmacros like EAT_AND_JUMP_OR_RETURN() perform the boundary checks and decide to\ncontinue or stop here so that most of the checks are hidden from the visible\ncode.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 14:56 UTC (Tue) by foom (subscriber, #14868) [Link]\n\nA few lines down looks like a nice example of a performance hack which\nintroduces UB in the code.\n\nhttps://github.com/haproxy/haproxy/blob/50d8c187423d6b7e9...\n\nThat's a great illustratation of the impossibility of remembering all the\nrules you must follow in C to avoid undefined behavior, and/or the widespread\nculture in the C/C++ development community of believing it's okay to \"cheat\"\nthose rules. \"Surely it is harmless to cheat, if you know what your CPU\narchitecture guarantees?\", even though the compiler makes no such guarantee.\n\nIn this case, building with -fsanitize=undefined (or more specifically\n-fsanitize=alignment) could catch this intentional-mistake.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 16:44 UTC (Tue) by adobriyan (subscriber, #30858) [Link]\n\nmanual shift is probably unnecessary too\n\n> if (likely((unsigned char)(*ptr - 33) <= 93)) { /* 33 to 126 included */\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 16:48 UTC (Tue) by wtarreau (subscriber, #51152) [Link]\n\nNot at all, it's not cheating nor a mistake. It's perfectly defined thanks to\nthe ifdef above it, which guarantees that we *do* support unaligned accesses\non this arch. Rest assured that such code runs fine on sparc, mips, riscv64,\narmbe/le/64, x86 of course, and used to on PARISC, Alpha and VAX though I\nhaven't tested it there for at least 10 years so I wouldn't promise anything\non that front :-)\n\nThere are other places where we need to read possibly unaligned data (in\nprotocol essentially) and it's done reliably instead.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 17:41 UTC (Tue) by farnz (subscriber, #17727) [Link]\n\nIf I understand correctly, it breaks the C rules for type punning. Note that\nI'm not completely clear on the rules myself (I used to do C++, I now do Rust,\nboth of which have different rules to C), but my understanding is that in\nstandard C, you can cast any type to char * and dereference, but you cannot\ncast char * to anything and dereference it. You can do this via a union, and\n(at least GCC and Clang) compilers have -fno-strict-aliasing to change the\nrules such that this is OK (and I've not looked at your build system to know\nif you unconditionally set -fno-strict-aliasing).\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 20:13 UTC (Tue) by foom (subscriber, #14868) [Link]\n\nIt is cheating.\n\nYou have an ifdef testing for architectures that have unaligned memory access\ninstructions at the machine level. But it is undefined behavior to read\nmisaligned pointers at the C-language level, even so. No compiler I'm aware of\nhas made any guarantee that'll work, even on your list of architectures.\n\nYes, empirically this code generates a working program on common compilers for\nthose architectures today, despite the intentional bug. But that does NOT make\nthe code correct. It may well stop working at any compiler upgrade, because\nyou are breaking the rules.\n\nBut I don't mean to pick on just this code: this sort of thinking is\nubiquitous in the C culture. And it's a serious cultural problem.\n\n(And, btw, there's not a good reason to break the rules here: a memcpy from\nthe buffer into a local int could generate the exact same machine\ninstructions, without the UB.)\n\n### Continued attacks on HTTP/2\n\nPosted Apr 17, 2024 9:21 UTC (Wed) by farnz (subscriber, #17727) [Link]\n\nThere's a significant bit of history behind this sort of thinking, though.\nIt's only relatively recently (late 1990s) that C compilers became more\nsophisticated than the combination of a macro assembler with peephole\noptimization and register allocation.\n\nIf you learnt C \"back in the day\", then you almost certainly built a mental\nmodel of how C works based on this compilation model: each piece of C code\nturns into a predictable sequence of instructions (so c = a + b always turns\ninto the sequence \"load a into virtual register for a, load b into virtual\nregister for b, set virtual register for c to the sum of virtual register for\na plus virtual register for b, store virtual register for c to c's memory\nlocation\"), then the compiler goes through and removes surplus instructions\n(e.g. if a and b are already loaded into registers, no need to reload), then\nit does register allocation to turn all the virtual registers into real\nregisters with spilling to the stack as needed.\n\nThat's not the modern compilation model at all, and as a result intuition\nabout C that dates from that era is often wrong.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 17, 2024 12:37 UTC (Wed) by wtarreau (subscriber, #51152) [Link]\n\n\"This thinking\" is just what everyone educated about processors does when they\ncare about performance. CPU vendors go through great difficulties to make sure\nthat these extremely common patterns work efficiently because they're in\ncritical paths, to the point that old CPUs which didn't support them are now\ndead (armv5, mips, sparc).\n\nOne would be completely crazy or ignorant to ruin the performance of their\nprogram doing one byte at a time on a machine which purposely dedicates\nsilicon to address such common patterns. Look at compressors, hashing\nfunctions etc. Everyone uses this. A quick grep in the kernel shows me 8828\noccurrences. Originally \"undefined behavior\" meant \"not portable, will depend\non the underlying hardware\", just like the problem with signed addition\noverflow, shifts by more than the size of the word, etc. It's only recently\nthat within the clang vs gcc battle it was found funny to purposely break\nprograms relying on trustable and reliable implementations after carefully\ndetecting them.\n\nAnd even for the unaligned access I'm not even sure it's UB, I seem to\nremember it was mentioned as implementation specific, because that's just a\nconstraint that's 100% hardware-specific, then should be relevant to the psABI\nonly. And if you had programmed under MS-DOS, you'd know that type-based\nalignment was just not a thing by then, it was perfectly normal *not* to align\ntypes. All holes were filled. Yet C did already exist. It was only the 386\nthat brought this alignment flag whose purpose was not much understood by then\nand that nobody used. In any case the compiler has no reason to insert\nspecific code to make your aligned accesses work and detect the unaligned ones\nand make them fail. Such casts exist because they're both needed and useful.\n\nActually, I'm a bit annoyed by that new culture consisting of denying\neverything that exists, trying to invent hypothetical problems that would\nrequire tremendous efforts to implement, just for the sake of denigrating the\nfacilities offered by hardware that others naturally make use of after reading\nthe specs. But it makes people talk and comment, that's great already. It\nwould be better if they would talk about stuff they know and that are useful\nto others, of course.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 17, 2024 13:03 UTC (Wed) by farnz (subscriber, #17727) [Link]\n\n> \"This thinking\" is just what everyone educated about processors does when\n> they care about performance. CPU vendors go through great difficulties to\n> make sure that these extremely common patterns work efficiently because\n> they're in critical paths, to the point that old CPUs which didn't support\n> them are now dead (armv5, mips, sparc).\n\nNo, it's not. And one of the reasons that C is such a mess is that people who\ntreat C like a macro assembler assume that everyone else thinks about code the\nway they do, and make sweeping false statements about the ways other people\nthink.\n\nEverything else in your comment flows from \"if the compiler acts like a macro\nassembler, then this is all true, and nothing else can possibly be true as a\nresult\"; however, a well-written language specification and compiler is\nperfectly capable of detecting that you're doing a memcpy of 4 unaligned bytes\ninto a 32 bit unsigned integer, and converting that into a load instruction.\n\nThe compiler should also then notice that your operations have exactly the\nsame effect regardless of endianness, and will therefore not bother with\nbyteswapping, since the effect of byteswapping is a no-op, and will optimize\nstill further on that basis.\n\nYou'll notice that this is a very different style of performance reasoning,\nsince the compiler is now part of your reasoning. But it is what most people\neducated about processors have done when I've been hand-optimizing code with\nthem; they've been sticking within the defined semantics of the language,\nknown what optimizations the compiler can do, and have cross-checked assembly\noutput against what they want to be confident that they're getting what they\nwant.\n\nAnd you'll note that I didn't talk about MS-DOS; I talked about all old\ncompilers; compilers of that era simply didn't do any complex analysis of the\ncodebase to optimize it, and would pessimise the code that a modern expert on\nprocessor performance would write.\n\nI'm very annoyed that there's a bunch of old-timers who have a culture of\ndenying everything that's improved since the 1970s, trying to invent\nhypothetical reasons why compilers can't do the stuff that they do day-in,\nday-out, just for the sake of denigrating the facilities offered by hardware\nthat others make use of after reading the specs. It would be better if they\ntalked about stuff they know and that is useful to others, rather than\ndecrying change because it's made things different to the ancient era.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 17, 2024 14:52 UTC (Wed) by mb (subscriber, #50428) [Link]\n\nWell said.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 17, 2024 14:59 UTC (Wed) by wtarreau (subscriber, #51152) [Link]\n\nI think we'll never agree anyway. You seem to consider that the compiler\nalways knows well, and I'm among those who spend 20% of their time fighting\nthe massive de-optimization caused by those compilers who think they know what\nyou're trying to do. And I'll always disagree with the memcpy() hack, because\nit's a hack. The compiler is free to call an external function for this (and\nthere's a reason memcpy() exists as a function) and you have no control over\nwhat is done. I can assure you that on quite a bunch of compilers I've seen\nreal calls, that just defeated all the purpose of the access. All what you\nexplain is fine for a GUI where nanoseconds do not exist, they're just totally\nunrealistic for low-level programming. It has nothing to do with being \"old-\ntimers\" or whatever. Just trying to pretend people introduce bugs by doing\nwrong things while in practice they're just doing what the language offers as\na standard to support what the architecture supports is non-sense. It would be\nfine if the code was not protected etc but when the case is specifically\nhandled for the supported platforms, the language obiously supports this since\nit's a well-known and much used case.\n\nIn this specific case, what would be wrong would be to use memcpy(), because\nyou'd just give up the ifdef and always use that, and doing that on a strict-\nalignment CPU would cost a lost. In this case you precisely want to work one\nbyte at a time and certainly not memcpy()!\n\nAs you called me names \"old-timers\" I would equally say that this new way of\nthinking comes from junkies, but I'll not, you'll probably have plenty of time\nleft in your life to learn about all the nice stuff computers can do when\nthey're not forced to burn cycles, coal or gas just to make the code look\nnice.\n\nFortunately there's room for everyone, those who prefer code that is highly\nreadable and flexible and supports fast evolutions and those who prefer to\nwork in areas that do not stand as fast changes because time is important.\nWhat's sure is that those are rarely the same persons. Different preferences\netc come into play. But both are needed to make the computers and systems\nwe're using today.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 17, 2024 19:30 UTC (Wed) by foom (subscriber, #14868) [Link]\n\n> In this specific case, what would be wrong would be to use memcpy(), because\n> you'd just give up the ifdef and always use that, and doing that on a\n> strict-alignment CPU would cost a lost.\n\nWhy would you delete the ifdef just because you fixed the UB?\n\nThere's nothing wrong with keeping an ifdef to choose between two different\ncorrect implementations, one of which has better performance on certain\narchitectures...\n\n### Continued attacks on HTTP/2\n\nPosted Apr 18, 2024 11:39 UTC (Thu) by kleptog (subscriber, #1183) [Link]\n\n> Fortunately there's room for everyone, those who prefer code that is highly\n> readable and flexible\n\nWell, that code is readable in the sense that you see what the code does.\nHowever, I spent a good five minutes looking at it to see if I could prove to\nmyself it actually does what the comment claims it does. I'm still not really\nsure, but I guess it must if everyone else claims it's fine. Using memcpy()\nwon't fix that. There's this niggling worry that the borrowing during the\nsubtractions opens a hole somewhere. There's probably an appropriate way to\nlook it that does make sense.\n\nBut it is an interesting question: what are the chances the compiler will see\nthat code, figure that if it unrolls the loop a few times it can use SSE\ninstructions to make it even faster, and they *do* require alignment. It\nprobably won't happen for the same reason I can't prove to myself the code\nworks now.\n\nWould it make a difference if you used unsigned int instead? At least then\nsubtraction underflow is well defined. I don't know.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 17, 2024 16:22 UTC (Wed) by Wol (subscriber, #4433) [Link]\n\n> No, it's not. And one of the reasons that C is such a mess is that people\n> who treat C like a macro assembler assume that everyone else thinks about\n> code the way they do, and make sweeping false statements about the ways\n> other people think.\n\nThing is, that's the way Kernighan and Ritchie thought. So everyone who thinks\nthat way, thinks the way the designers of C thought.\n\nMaybe times should move on. But if you're going to rewrite the entire\nphilosophy of the language, DON'T. It will inevitably lead to the current\nreligious mess we've now got! It's worse than The Judean People's Front vs.\nthe People's Front of Judea!\n\nCheers, Wol\n\n### Continued attacks on HTTP/2\n\nPosted Apr 17, 2024 17:07 UTC (Wed) by farnz (subscriber, #17727) [Link]\n\nThat ship sailed in the 1980s (before Standard C existed), when C was ported\nto Cray systems. It's been a problem for a long time; it would be less of a\nproblem if C90 had been \"the language as a macro assembler\", and C99 had\nchanged the model, but the issue remains that Standard C has never worked this\nway, since by the time we standardised C, this wasn't the model used by\nseveral significant compiler vendors.\n\nAnd the underlying tension is that people who want C compilers to work as\nmacro assemblers with knobs on aren't willing to switch to C compilers that\nwork that way; if people used TinyCC instead of GCC and Clang, for example,\nthey'd get what they want. They'd just have to admit that C the way they think\nof it is not C the way MSVC, GCC or Clang implements it, but instead C the way\nTinyCC and similar projects implement it.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 17, 2024 16:16 UTC (Wed) by foom (subscriber, #14868) [Link]\n\n> under MS-DOS, you'd know that type-based alignment was just not a thing by\n> then, it was perfectly normal *not* to align types. All holes were filled.\n> Yet C did already exist.\n\nAn implementation may set the alignment of every type to 1, if it desired. In\nsuch a case, you'd never need to worry about misaligned pointers.\n\n> hypothetical problems\n\nOkay, a non-hypothetical case of this cheating causing problems. \"Modern\"\n32bit Arm hardware accepts unaligned pointers for 1, 2, and 4-byte load/store\noperations. People saw this hardware specification, and broke the rules in\ntheir C code. That code worked great in practice despite breaking the rules.\nYay.\n\nBut, on that same hardware, 8-byte load/store operations require 4-byte-\naligned pointers. And at some point, the compiler leaned how to coalesce two\nsequential known-4-byte-aligned 4-byte loads into a single 8-byte load. This\nis good for performance. And this broke real code which broke the rules and\nwas accessing misaligned \"int*\".\n\nCompiler writers of course say: Your fault. We followed the spec, those\naccesses were already prohibited, what do you want? Don't break the rules if\nyou want your program to work.\n\nUsers say: But I followed the unwritten spec of \"I'm allowed to cheat the\nrules and do whatever the heck I want, as long as it seems to work right now.\"\nAnd besides who can even understand all the rules, there's too many and\nthey're too complex to understand.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 11, 2024 22:30 UTC (Thu) by ballombe (subscriber, #9523) [Link]\n\n> If we're sure hard things are just always a bad idea then I agree that's a\n> win for C. But, maybe hard things are sometimes a good idea, and then in C\n> you're going to really struggle.\n\nAgreed. But the tendency is to do hard thing when it is not needed. Security\nrequires you to pick the simplest possible solution instead.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 12, 2024 3:25 UTC (Fri) by wtarreau (subscriber, #51152) [Link]\n\n> Security requires you to pick the simplest possible solution instead.\n\nIt's rarely that simple, otherwise all products would be secure by design. The\nreality is that it depends a lot on how the product is used and the risks it\nfaces. The biggest security risk for edge HTTP servers is DoS and dealing with\nDoS requires to do complex things to remain efficient in all situations. What\nmakes some vulnerable to DoS precisely is to apply the principle of the\nsimplest solution. Simple is easy to audit but often also easy to DoS.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 11, 2024 16:07 UTC (Thu) by nomaxx117 (subscriber, #169603) [Link]\n\nAt least in the case of Rust's hyper crate (which I am one of the maintainers\nof), our impact was more to do with the fact that parsing frames like this is\ncpu usage that users wouldn't be able to catch normally via looking at logs -\nwe actually never had the memory issues because we already limited the amount\nof headers we will buffer and store.\n\nRust is honestly a pretty good language for this stuff specifically because\nit's a lot more flexible than C is and we can express much simpler and easier\nto follow logic around asynchronous patterns than in C. It's a lot easier to\nreview and audit code using futures rather than a complex state machine in an\nepoll callback table.\n\nAlso, a lot of the folks using Rust to build these sorts of things are pretty\nexperienced in those useful patterns that come from C ;)\n\n### Continued attacks on HTTP/2\n\nPosted Apr 11, 2024 17:59 UTC (Thu) by wtarreau (subscriber, #51152) [Link]\n\n> At least in the case of Rust's hyper crate (which I am one of the\n> maintainers of), our impact was more to do with the fact that parsing frames\n> like this is cpu usage that users wouldn't be able to catch normally via\n> looking at logs - we actually never had the memory issues because we already\n> limited the amount of headers we will buffer and store.\n\nOK that's good to know.\n\n> Rust is honestly a pretty good language for this stuff specifically because\n> it's a lot more flexible than C is and we can express much simpler and\n> easier to follow logic around asynchronous patterns than in C.\n\nIt's a matter of taste, I'm still finding it impossible to understand it :-/\nToo many unpronounceable characters per line for me.\n\n> It's a lot easier to review and audit code using futures rather than a\n> complex state machine in an epoll callback table.\n\nWell, I *want* my epoll loop so that I can optimize every single bit of it to\nsave whatever expensive syscall can be saved, and can handle differently the\nFDs that are shared between threads and those that are local to each thread,\netc. One thing I wouldn't deny is that it's time consuming, but the returns on\ninvestments are worth the time spent.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 12, 2024 10:34 UTC (Fri) by tialaramex (subscriber, #21167) [Link]\n\n> One thing I wouldn't deny is that it's time consuming, but the returns on\n> investments are worth the time spent.\n\nThat's the part I don't believe, as somebody who spent many years programming\nC. Or rather, I never experienced for myself and never saw anybody else\nexperience having sufficient available time capital to make every net\nprofitable investment. At work we could definitely do every single card on the\nboard, they're all a good idea, they all improve the software and the users\nwant those improvements. But that would cost far more time than we have, so it\nwon't happen, some cards won't get done this month, or this year, or ever.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 13, 2024 16:18 UTC (Sat) by wtarreau (subscriber, #51152) [Link]\n\nI can assure you that for some of our users and customers, the savings are\nhuge, especially when it allows them to halve the number of servers compared\nto the closest competitor and they're even willing to pay a lot for that\nbecause that can literally save them millions per year when the saved servers\nare counted in thousands. In the end, everyone wins: the devs are happy do to\ninteresting work, the company can pay them to optimize their work, the\ncustomers are happy to save money and energy, and other users are happy to use\neven less resources.\n\nParadoxically the cloud platforms which encourage a total waste of resources\nare also the ones that favor optimizations to cut costs!\n\n### Continued attacks on HTTP/2\n\nPosted Apr 15, 2024 21:06 UTC (Mon) by tialaramex (subscriber, #21167) [Link]\n\n> the savings are huge, especially when it allows them to halve the number of\n> servers compared to the closest competitor\n\nBut that's a very *specific* improvement. So now it's not just \"it's better\"\nwhich I don't deny, but that specifically the choice to do this is buying you\na 50% cost saving, which is huge. By Amdahl's law this means *at least half*\nof the resource was apparently wasted by whatever it is the closest competitor\nis doing that you're not. In your hypothetical that's... using language sugar\nrather than hand writing a C epoll state machine. Does that feel plausible?\nThat the machine sugar cost the same as the actual work ?\n\nMy guess is the reality is that competitor just isn't very good, which is\ngreat for your ego, but might promise a future battering if you can't\n\"literally save them millions per year\" over a competitor who shows up some\nday with a cheaper product without hand writing C epoll state machines. Or\nindeed if somebody decides that for this much money they'll hand write the\nmachine code to scrape a few extra cycles you can't reach from C and perform\n*better* than you.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 3:19 UTC (Tue) by wtarreau (subscriber, #51152) [Link]\n\n> if somebody decides that for this much money they'll hand write the machine\n> code to scrape a few extra cycles you can't reach from C and perform\n> *better* than you.\n\nAbsolutely, it's a matter of invested effort for diminishing returns. But if\nwe can improve our own design ourselves, others can also do better. It's just\nthat having a fully-controllable base offers way more optimization\nopportunities than being forced to rely on something already done and coming\npre-made as a lib. In the distant past, when threads were not a thing and we\nhad to squeeze every CPU cycle, I even remember reimplementing some of the\ncritical syscalls in asm to bypass glibc's overhead, then doing them using the\nVDSO, then experimenting with KML (kernel-mode linux) which was an interesting\ntrick reducing the cost of syscalls. All of these things used to provide\nmeasurable savings, and that's the reward of having full control over your\ncode.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 8:50 UTC (Tue) by tialaramex (subscriber, #21167) [Link]\n\nNo, no, \"invested effort for diminishing returns\" was exactly the point I\ntried to make above and you contradicted it, insisting that instead you can\nafford to invest in absolutely *everything* because it shows such massive\nreturns - the thing I'd never seen, and, from the sounds of it, still haven't.\n\nThe \"fully-controllable base\" doesn't really mean anything in software, others\ntoo could make such a choice to use things or not use them as they wish. Swap\nto OpenBSD or Windows, write everything in Pascal or in PowerPC machine code,\nimplement it on an FPGA or custom design silicon, whatever. The skill set\nmatters of course, you struggle with all the non-alphabetic symbols in Rust,\nwhereas say I never really \"got\" indefinite integrals, probably neither of us\nis an EDA wizard. But for a corporate entity they can hire in skills to fill\nthose gaps.\n\nIf you've decided actually the diminishing returns do matter then we're\nprecisely back to actually hand writing the epoll loop probably isn't the best\nway to expend those limited resources for almost anybody and you've got a\nsingular anecdote where it worked out OK for you, which is good news for you\nof course.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 9:21 UTC (Tue) by wtarreau (subscriber, #51152) [Link]\n\n> If you've decided actually the diminishing returns do matter then we're\n> precisely back to actually hand writing the epoll loop probably isn't the\n> best way to expend those limited resources for almost anybody and you've got\n> a singular anecdote where it worked out OK for you, which is good news for\n> you of course.\n\nOften that's how you can prioritize your long-term todo-list. But sometimes\nhaving to rely on 3rd-party blocks (e.g. libev+ssl) gets you closer to your\ngoal faster, then blocks you in a corner because the day you say \"enough is\nenough, I'm going to rewrite that part now\", you have an immense amount of\nwork to do to convert all the existing stuff to the new preferred approach.\nInstead when your started small (possibly even with select() or poll()) and\nprogressively grew your system based on production feedback, it's easier to do\nmore frequent baby steps in the right direction, and to adjust that direction\nbased on feedback.\n\nTypically one lib I'm not seeing myself replace with a home-grown one is\nopenssl, and god, it's the one causing me the most grief! But for the rest,\nI'm glad I haven't remained stuck into generic implementations.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 11:19 UTC (Tue) by paulj (subscriber, #341) [Link]\n\nHave you ever considered that in some cases there might be code that is open-\nsourced by a giant tech company, that potentially has _deliberately_ bad\nperformance, because if a competitor of theirs uses that code and ends up with\nbad performance that doesn't hurt said giant tech company?\n\nEven if not outright deliberate, the giant tech company at least has no\nmotivation to make the open sourced code perform well.\n\nAs an example, the Google QUIC code-base - from Chromium - has _terrible_\nperformance server side. It's kind of the reference implementation for QUIC.\nAnd I'm pretty sure it's /not/ what GOOG are using internally on their\nservers. Based on presentations they've given on how they've improved server\nperformance, they've clearly worked on it and not released the improvements.\n\nIt's in C++.\n\nOne of the quickest implementations I know of (though, I havn't tested Willy's\nversion ;) ) is LSQUIC. C.\n\nI think a lot of it is more philosophy of the programmer than the actual\nlanguage. However, a performance philosophy is more frequently found in C\nprogrammers. There is a cultural element, which correlates to languages.\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 13:55 UTC (Tue) by wtarreau (subscriber, #51152) [Link]\n\n> One of the quickest implementations I know of (though, I havn't tested\n> Willy's version ;) ) is LSQUIC. C.\n\nHaven't benchmarked it but we've pulled 260 Gbps out of a server a year ago\nfrom ours, it didn't seem bad by then, but we should improve that soon ;-)\n\n> I think a lot of it is more philosophy of the programmer than the actual\n> language. However, a performance philosophy is more frequently found in C\n> programmers. There is a cultural element, which correlates to languages.\n\nI totally agree with this. The number of times I've heard \"you're spending\nyour time for a millisecond?\" to which I replied \"but if this millisecond is\nwasted 1000 times a second, we're spending our whole lives in it\". And it's\ntrue that the culture varies with languages, in fact, languages tend to\nattract people sharing the same culture and/or main concerns (performance,\nsafety, ease of writing code, abundance of libraries, wide community etc).\n\n### Continued attacks on HTTP/2\n\nPosted Apr 16, 2024 14:08 UTC (Tue) by Wol (subscriber, #4433) [Link]\n\n> > I think a lot of it is more philosophy of the programmer than the actual\n> language. However, a performance philosophy is more frequently found in C\n> programmers. There is a cultural element, which correlates to languages.\n\nI dunno about C. I have the same philosophy and I started with FORTRAN. I\nthink it's partly age, and partly field of programming.\n\nMy first computer that I bought was a Jupiter Ace. 3 KILObytes of ram. My\ncontemporaries had ZX80s, which I think was only 1 KB. The first computer I\nworked on was a Pr1me 25/30, which had 256KB for 20 users. That's the age\nthing - we didn't have resource to squander.\n\nAnd one job I always mention on my CV is, I was asked to automate a job and\ntold I had 6 weeks to do it - they needed the results for a customer. 4 weeks\nin, I'd completed the basic program, then estimated the *run* time to complete\nthe job (given sole use of the mini available to me) as \"about 5 weeks\". Two\ndays of hard work optimising the program to speed it up, and I handed it over\nto the team who were actually running the job for the customer. We made the\ndeadline (although, immediately on handing the program over, I went sick and\nsaid \"ring me if you need me\". Had a lovely week off :-) And that's the field\nof programming - if you're short of resource for the job in hand (still true\nfor many microcontrollers?) you don't have resource to squander.\n\nCheers, Wol\n\n### Continued attacks on HTTP/2\n\nPosted Apr 15, 2024 20:39 UTC (Mon) by Wol (subscriber, #4433) [Link]\n\n> But that would cost far more time than we have, so it won't happen, some\n> cards won't get done this month, or this year, or ever.\n\nYou're asking the wrong question (or rather sounds like your bosses are). I\noverheard some of our bot engineers making the same mistake. \"It costs several\nhundred pounds per bot. Over all our bots that's millions of quid. It's too\nexpensive, it won't happen\". WHAT'S THE PAYBACK TIME?\n\nWho cares if it costs a couple hundred quid a time. If it saves a couple of\nhundred quid over a year or so, you find the money! (Or you SHOULD.)\n\nI'm not very good at putting a cost figure on the work I'm trying to do. I\njustify it in time - if I can make the work of downstream easier for our hard-\npressed planners, and enable them to do more work (we spend FAR too much time\nwatching the spinning hourglass ...) then I can justify it that way. Time is\nmoney :-)\n\nBosses should be asking \"is this going to pay for itself\", and if the answer\nis yes, you find a way of doing it. (Yes I know ... :-)\n\nCheers, Wol\n\nCopyright \u00a9 2024, Eklektix, Inc. This article may be redistributed under the\nterms of the Creative Commons CC BY-SA 4.0 license Comments and public\npostings are copyrighted by their creators. Linux is a registered trademark of\nLinus Torvalds\n\n", "frontpage": false}
