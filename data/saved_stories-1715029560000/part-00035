{"aid": "40275469", "title": "Avoiding the Frankenstein Prompt", "url": "https://d-v-dlee.github.io/god-damn/2024/05/05/prompting.html", "domain": "d-v-dlee.github.io", "votes": 1, "user": "georgehill", "posted_at": "2024-05-06 14:59:43", "comments": 0, "source_title": "Avoiding the Frankenstein prompt", "source_text": "Avoiding the Frankenstein prompt | Blog of David Lee\n\nBlog of David Lee\n\n# Avoiding the Frankenstein prompt\n\nMay 5, 2024\n\nIt\u2019s not hard to write a pretty good prompt. There are plenty of great guides\nand documentation. However, crafting one that\u2019s robust enough for production\ncan be challenging.\n\nOne common trap I\u2019ve fallen into is what I call the Frankenstein Prompt. It\nessentially goes like this:\n\nYou create a POC which receives positive feedback. The stakeholders are\nenthusiastic and ready to advance. The POC is powered by \u201cgood enough\u201d prompts\nthat work well in the demo but start failing during more rigorous UAT and\nevals.\n\nAs edge cases and exceptions emerge, you start haphazardly adding to the\nprompts, trying to patch the holes. You find yourself in a game of whack-a-\nmole, where fixing one issue causes previously working parts to break. The\nmore you try to address individual problems, the more fragile the prompts\nbecome.\n\nBefore you know it, your prompts have turned into a Frankenstein\u2019s monster - a\nlarge, brittle chimera that\u2019s difficult to manage and maintain. Each iteration\nadds more complexity and confusion, making it harder to understand and modify\nthe prompts.\n\nFinally, after countless frustrating debugging sessions, you take a step back\nand reassess the situation. With a zoomed-out perspective, you finally\nunderstand what the customer needs and how the model should behave across\nevery scenario. You realize that the best course of action is to kill the\nFrankenstein prompt and start from scratch.\n\nArmed with your newfound clarity, you sit down and rewrite the prompts from\nthe ground up. This time, the logic is clean and clear. The resulting prompts\nare easier to understand and maintain. They\u2019re robust, passing your evals. Now\nthey\u2019re ready for production.\n\nThe purpose of this post is to capture my current best practices for prompt\ngeneration and evaluation, specifically for Anthropic\u2019s Claude 3. In it, we\u2019ll\ncover:\n\n  1. Questionaires for guiding prompt creation\n  2. System prompts\n  3. Task-specific prompts\n  4. Visual document understanding prompts\n  5. Evaluation\n\n## 1\\. Questionnaires\n\nTo avoid falling into the Frankenstein Prompt trap, it\u2019s crucial to invest\ntime upfront in understanding the problem, requirements, and expected\nbehaviors. This is where questionnaires come in. They help you gather and\norganize the necessary information to create robust, production-ready prompts.\n\nI created two questionnaires (see Appendix) to aid in this process: a system-\nlevel questionnaire and a task-level questionnaire. The system-level\nquestionnaire focuses on the big picture, capturing the overall problem, end-\nto-end process, and tasks the model will assist with. The task-level\nquestionnaire dives into the specifics of each task, covering aspects like\ntask description, input data, outputs, definitions, heuristics, instructions,\nedge cases, examples, and evaluation criteria.\n\nBy completing these questionnaires, you create detailed artifacts akin to\nrequirements documents or PRFAQs. These can then serve as robust foundations\nfor generating precise and effective prompts, whether done manually or using\ntools like metaprompt.\n\nThis approach doesn\u2019t eliminate the need for testing and iteration, but it\ndoes make the process more efficient. By having a clearer understanding of the\nrequirements from the start, you can reduce the number of iterations required\nand save yourself from a lot of headaches!\n\n## 2\\. System Prompts\n\nMy system prompt typically has six parts:\n\n  1. Persona - who are you and what are you good at\n\n     * world\u2019s finest AI assistant at X, with deep expertise in X, Y, Z\n  2. Role - what are you helping humans do\n\n     * You help X do\n  3. Big Picture - what is the big thing we\u2019re trying to solve\n\n     * At a high-level, the end-to-end process goes as following...\n  4. Overview of specific tasks - what will you specifically be responsible for\n\n     * You are responsible for these specific tasks...\n  5. General behavior - guidance on what should you do all the time\n\n     * You always take your time, think step-by-step, and pay extreme attention to detail...\n  6. Format guidance - what should the expected output of the prompts be\n\n     * XML vs JSON\n\nThe big picture is helpful because often times you\u2019re running LLM calls in\nparallel or they\u2019re working on a task in a greater chain of tasks without that\nknowledge. The system prompt can help \u201cpull\u201d all these calls in the same\ndirection.\n\n## 3\\. Task-Specific Prompts\n\nI typically follow this format, with a specific tag in thinking tied to each\nstep:\n\n    \n    \n    Please carefully read the following: <context_description> </context_description> ... </context_description> </context_description> <instructions> 1. Do X 2. Do Y 3. Do Z 4. Carefully think step-by-step and take your time to return an answer in this format: <thinking> <some_tag>output of step 1</some_tag> <some_other_tag>output of step 2</some_other_tag> <another_tag>output of step 3</another_tag> </thinking> <answer> <tag_to_parse>output description</tag_to_parse> <tag_to_parse>output description </tag_to_parse> </answer> </instructions>\n\nI\u2019ve found that explicitly creating XML tags within thinking for each step\nimproves model adherence to instructions.\n\nPreviously, I used a different instructions flow where I\u2019d list several steps\nthen tell the model to do those within <thinking>.:\n\n    \n    \n    <instructions> 1. Do X 2. Do Y 3. Do Z 4. Carefully think step-by-step and perform steps 1, 2, 3, writing your notes and thinking process within <thinking></thinking> tags. 5. Once you have completed the above, return your final answer in the format below. <answer> <tag_to_parse>output description</tag_to_parse> <tag_to_parse>output description </tag_to_parse> </answer> </instructions>\n\nThe problem here is that the model could be inconsistent in carrying out all\nsteps in thinking, sometimes executing all steps and other times only\nexecuting a few.\n\n### Decision Trees\n\nFor more complicated instructions with if-else statements, special\ndefinitions, and exceptions, I\u2019ve found that creating a decision tree using a\nmermaid diagram and adding it to the instructions helps. The questionnaire is\nuseful for this. You can pass the same artifact to an LLM to create a mermaid\ndiagram and iterate on it to ensure accuracy. The mermaid diagram also becomes\na useful visualization to share with your customers.\n\n    \n    \n    graph TD A[Size] -->|Small| B[Color] A -->|Medium| C[Weight] A -->|Large| D[Texture] B -->|Red| E[Apple] B -->|Green| F[Lime] B -->|Yellow| G[Lemon] C -->|Light| H[Feather] C -->|Heavy| I[Stone] D -->|Smooth| J[Plastic] D -->|Rough| K[Wood] E[Apple] F[Lime] G[Lemon] H[Feather] I[Stone] J[Plastic] K[Wood]\n\nThe main idea is that creating a decision tree forces you to define expected\nbehavior and identify edge cases in the prompt, helping the model follow the\ncorrect process.\n\nOverall, marrying a specific tag in thinking to each step and defining\ndecision trees in your prompt via mermaid diagrams improves instruction\nfollowing and leads to more consistent outputs.\n\n## 4\\. Vision Prompts\n\nYou can use Claude for document understanding tasks. As someone who used to do\na lot of Textract and LayoutLM fine-tuning, I find this extremely useful,\nespecially for \u201ccomplicated\u201d tables. For example, consider this electric\nsummary sheet from Puget Sound Energy, which shows rate schedules. Based on\nattributes such as lamp type (High Pressure Sodium Vapor or LED), wattage,\neffective date, and bill components, we might want to parse a specific value.\n\nWhile simple for a human, traditional methods like Textract would struggle\nwith this table due to factors like multi-level headers and shared columns\nbetween sub-tables. Fortunately, Claude\u2019s vision capabilities perform better\nin these scenarios.\n\nFor simple document understanding tasks, you can write straightforward queries\nlike \u201cWhat is ...?\u201d. However, for more complex tables like the one above, I\nhave found the following approach helpful:\n\n  1. Have a powerful model like Sonnet or Opus generate a rich description and visual representation of the table and add that to the prompt.\n  2. Write step-by-step instructions similar to the previous sections. One of the steps should ask the model to visualize and recreate the specific section of the document they should extract from.\n  3. Have a powerful model like Sonnet or Opus create few-shot examples based on the instructions.\n\nOnce your prompt is written, you can execute it using Claude Haiku, saving you\ntime and money. Here\u2019s what a prompt for parsing the above image might look\nlike:\n\n    \n    \n    <document_description> The image shows an Electric Summary Sheet from Puget Sound Energy effective 5/1/2024. It contains detailed rate schedules for customer-owned street lighting energy service (SCH 54) that is not available to new customers. Table layout: - The main table has rows labeled with lamp wattages (e.g., 50, 70, 100) and columns representing bill components for different effective dates. - The bill components include Energy Charge, Power Cost Adjustment, Low Income Program, and more. Each component has subcolumns for cents per kWh and dollar amounts. - The cells contain the charge amounts in dollars or cents per kWh for each combination of lamp wattage, bill component, and effective date. - Below the main table is a second table labeled \"SCHEDULE 54 - LED\" which shows rates for LED street lighting. It has a similar structure but the rows are for different LED wattage ranges. </document_description> <visual_representation> CUSTOMER OWNED STREET LIGHTING ENERGY SERVICE Lamp Wattage SCH 54 SCH 95 SCH 120 SCH 129 129D SCH 140 SCH 141A SCH 141COL SCH 141N SCH 141R SCH 141Z SCH 142 SCH 99A SCH 137 50 X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX 70 X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX ... SCHEDULE 54 - LED LED Wattage SCH 54 SCH 95 SCH 120 SCH 129 129D SCH 140 SCH 141A SCH 141COL SCH 141N SCH 141R SCH 141Z SCH 142 SCH 99A SCH 137 0-30 X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX 30.01-60 X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX X.XX ... </visual_representation> The tables provide charge amounts based on the following attributes: - Lamp Type: The type of lamp (e.g., High Pressure Sodium Vapor or LED). - Wattage: The wattage of the lamp. For High Pressure Sodium Vapor, it's a specific value (e.g., 50, 100, 250). For LED, it's a wattage range (e.g., 0-30, 90.01-120). - Bill Component: The specific component of the bill (e.g., Energy Charge, Low Income Program Charge). - Effective Date: The date the rates are effective from (e.g., SCH 54 1/1/2023, SCH 95 5/1/2023, SCH 129 1/1/2024). </document_description> <instructions> </instructions> <example name='example1'> <attributes>example attr</attributes> <output>example thinking + answer </output> </example> <example name='example2'> <attributes>example attr</attributes> <output>example thinking + answer </output> </example> Your goal is to X based on the following attributes: <attributes> </attributes>\n\n## 5\\. Evaluation\n\nEvaluating LLM outputs can be difficult and is currently more art than\nscience, relying partially on subjective customer feedback and the eye test.\nPromptFoo is a useful tool for generating test cases and comparing prompts\nhead-to-head in a more programmatic way.\n\nPromptFoo uses a YAML configuration:\n\n    \n    \n    prompts: - file://prompts/some_prompt.txt providers: - 'exec: python scripts/claude_v3.py' defaultTest: assert: - type: python value: file://scripts/is_valid_xml.py - type: python value: file://scripts/check_classification.py tests: - vars: prompt_var: file://context/some_context.md prompt_var2: file://context/other_context.md for_ui: \"easy for you to read\" expected_answer: \"some answer\"\n\n  * prompts points to a .txt file containing the prompts to test, separated by ---.\n  * providers points to a Python file defining the model and its parameters.\n  * defaultTest allows you to apply tests to all examples using custom Python scripts.\n\nCustom test scripts typically follow this format:\n\n    \n    \n    if __name__ == \"__main__\": output = sys.argv[1] # this is LLM output inputs = sys.argv[2] # formatted prompt + input vars from yaml # do something response = { \"pass\": some_condition, \"score\": some_score, \"reason\": additional_information } print(json.dumps(response))\n\nThrough these scripts, you can run tests like:\n\n  * ensuring proper XML or JSON formatting\n  * measuring classification accuracy\n  * searching for hallucinations\n\nFor example, you could parse citations from the LLM output (argv[1]) then\ncheck if they\u2019re grounded in the context originally shared with the model\n(argv[2]).\n\nWhile I focus purely on programmatic evals, PromptFoo also supports LLM-as-\nevaluator evals. I don\u2019t think PromptFoo supports image evals yet, so for\ndocument understanding tasks I\u2019ve relied on custom tests. If you\u2019ve figured\nthat out, let me know.\n\n## 6\\. Conclusion\n\nPrompting opens up a lot of possibilities! By applying the techniques covered\nin this post, such as using questionnaires, structuring prompts thoughtfully,\nand incorporating decision trees and custom evaluation methods, you can create\nmore reliable and consistent production-ready prompts for Generative AI\nsolutions.\n\nPerhaps these all become obsolete with Claude 3.x but until then, happy\nprompting!\n\n## Appendix\n\n### Questionaire\n\nSystem Prompt Questionaire\n\n  1. What is the overall problem you\u2019re trying to solve?\n  2. What is the end-to-end process look like?\n  3. What are the tasks within the process that you want the model to help with?\n\nTask-Level Questionaire\n\n  1. Task Description\n\n    1. What is the main task or goal you want the model to accomplish?\n    2. A clear and concise description of the task\n  2. Input Data\n\n    1. How many inputs?\n    2. Brief description of each input\n    3. How should they be used?\n  3. Outputs\n\n    1. What is the desired format, XML vs JSON?\n    2. What are the output fields?\n  4. Definitions\n\n    1. Define any key terms or definitions\n    2. Define any acronyms that may be passed in as context\n  5. Heuristic\n\n    1. Working backwards, what are the steps to derive the correct answer(s)?\n    2. What does a human do today (if addressing existing workflow)?\n    3. Convert this heuristic into high-level step-by-step instructions\n  6. Instructions\n\n    1. For each step, what should the model do?\n  7. Ambiguity and Edge Cases\n\n    1. Are there if-else flows?\n    2. What are potential edge cases?\n    3. Are there ambiguous steps that the model can interpret differently run-to-run?\n  8. Few-shot Examples\n\n    1. Do you have examples of input-output pairs (even if you don\u2019t have the intermediate thinking steps)?\n  9. DOs and DONTs\n\n    1. Is there behavior you want the model to definitely do?\n    2. Is there behavior you want the model to avoid?\n  10. Evaluation\n\n    1. Can the output(s) be graded in a supervised or binary fashion?\n    2. If not, what criteria can be used to evaluate the quality?\n    3. Is there a direction you want to steer it in?\n\n      1. for example - if you extracting citations, a FN (missing something important) might be really bad so you might say its better to overcite\n\n### Prompting Gotchas\n\nOrder Matters\n\nConsider these two different instructions.\n\n  1. Classify x into a, b, or c. Think step-by-step and explain your reason.\n  2. Your task will be to classify x into a, b, or c. Before choosing a classification, think step-by-step to...\n\nIn the first prompt, the model might classify (sometimes incorrectly) and then\njustify its answer. In the second prompt, the model will think first, then\nclassify.\n\nThe order in which you present instructions to the model can significantly\nimpact the output quality and accuracy. Always consider the optimal sequence\nof steps to guide the model towards the desired outcome.\n\n### Related Reading / Resources\n\n  * Everything I\u2019ll forget about prompting LLMs - inspo\n  * Bedrock Claude 3 Deep Dive - slide deck with useful tips\n  * Anthropic Cookbook - code examples\n\n## Blog of David Lee\n\n  * Blog of David Lee\n\nloading...\n\n", "frontpage": false}
