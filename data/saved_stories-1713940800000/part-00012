{"aid": "40136972", "title": "Tiny but mighty: The Phi-3 small language models with big potential", "url": "https://news.microsoft.com/source/features/ai/the-phi-3-small-language-models-with-big-potential/", "domain": "microsoft.com", "votes": 4, "user": "mihau", "posted_at": "2024-04-23 20:37:41", "comments": 0, "source_title": "Tiny but mighty: The Phi-3 small language models with big potential", "source_text": "Tiny but mighty: The Phi-3 small language models with big potential - Source\n\nSkip to main content\n\nMicrosoft\n\nSource\n\nSource\n\n0 Cart 0 items in shopping cart\n\nSource\n\n  * Category: AI\n\nApril 23, 2024\n\n# Tiny but mighty: The Phi-3 small language models with big potential\n\n##### By\n\n  * Sally Beatty\n\nSometimes the best way to solve a complex problem is to take a page from a\nchildren\u2019s book. That\u2019s the lesson Microsoft researchers learned by figuring\nout how to pack more punch into a much smaller package.\n\nLast year, after spending his workday thinking through potential solutions to\nmachine learning riddles, Microsoft\u2019s Ronen Eldan was reading bedtime stories\nto his daughter when he thought to himself, \u201chow did she learn this word? How\ndoes she know how to connect these words?\u201d\n\nThat led the Microsoft Research machine learning expert to wonder how much an\nAI model could learn using only words a 4-year-old could understand \u2013 and\nultimately to an innovative training approach that\u2019s produced a new class of\nmore capable small language models that promises to make AI more accessible to\nmore people.\n\nLarge language models (LLMs) have created exciting new opportunities to be\nmore productive and creative using AI. But their size means they can require\nsignificant computing resources to operate.\n\nWhile those models will still be the gold standard for solving many types of\ncomplex tasks, Microsoft has been developing a series of small language models\n(SLMs) that offer many of the same capabilities found in LLMs but are smaller\nin size and are trained on smaller amounts of data.\n\nThe company announced today the Phi-3 family of open models, the most capable\nand cost-effective small language models available. Phi-3 models outperform\nmodels of the same size and next size up across a variety of benchmarks that\nevaluate language, coding and math capabilities, thanks to training\ninnovations developed by Microsoft researchers.\n\nMicrosoft is now making the first in that family of more powerful small\nlanguage models publicly available: Phi-3-mini, measuring 3.8 billion\nparameters, which performs better than models twice its size, the company\nsaid.\n\nStarting today, it will be available in the Microsoft Azure AI Model Catalog\nand on Hugging Face, a platform for machine learning models, as well as\nOllama, a lightweight framework for running models on a local machine. It will\nalso be available as an NVIDIA NIM microservice with a standard API interface\nthat can be deployed anywhere.\n\nMicrosoft also announced additional models to the Phi-3 family are coming soon\nto offer more choice across quality and cost. Phi-3-small (7 billion\nparameters) and Phi-3-medium (14 billion parameters) will be available in the\nAzure AI Model Catalog and other model gardens shortly.\n\nGraphic illustrating how the quality of new Phi-3 models, as measured by\nperformance on the Massive Multitask Language Understanding (MMLU) benchmark,\ncompares to other models of similar size. (Image courtesy of Microsoft)\n\nSmall language models are designed to perform well for simpler tasks, are more\naccessible and easier to use for organizations with limited resources and they\ncan be more easily fine-tuned to meet specific needs.\n\n\u201cWhat we\u2019re going to start to see is not a shift from large to small, but a\nshift from a singular category of models to a portfolio of models where\ncustomers get the ability to make a decision on what is the best model for\ntheir scenario,\u201d said Sonali Yadav, principal product manager for Generative\nAI at Microsoft.\n\n\u201cSome customers may only need small models, some will need big models and many\nare going to want to combine both in a variety of ways,\u201d said Luis Vargas,\nvice president of AI at Microsoft.\n\nChoosing the right language model depends on an organization\u2019s specific needs,\nthe complexity of the task and available resources. Small language models are\nwell suited for organizations looking to build applications that can run\nlocally on a device (as opposed to the cloud) and where a task doesn\u2019t require\nextensive reasoning or a quick response is needed.\n\n> Some customers may only need small models, some will need big models and\n> many are going to want to combine both in a variety of ways.\n\nLarge language models are more suited for applications that need orchestration\nof complex tasks involving advanced reasoning, data analysis and understanding\nof context.\n\nSmall language models also offer potential solutions for regulated industries\nand sectors that encounter situations where they need high quality results but\nwant to keep data on their own premises, said Yadav.\n\nVargas and Yadav are particularly excited about the opportunities to place\nmore capable SLMs on smartphones and other mobile devices that operate \u201cat the\nedge,\u201d not connected to the cloud. (Think of car computers, PCs without Wi-Fi,\ntraffic systems, smart sensors on a factory floor, remote cameras or devices\nthat monitor environmental compliance.) By keeping data within the device,\nusers can \u201cminimize latency and maximize privacy,\u201d said Vargas.\n\nLatency refers to the delay that can occur when LLMs communicate with the\ncloud to retrieve information used to generate answers to users prompts. In\nsome instances, high-quality answers are worth waiting for while in other\nscenarios speed is more important to user satisfaction.\n\nBecause SLMs can work offline, more people will be able to put AI to work in\nways that haven\u2019t previously been possible, Vargas said.\n\nFor instance, SLMs could also be put to use in rural areas that lack cell\nservice. Consider a farmer inspecting crops who finds signs of disease on a\nleaf or branch. Using a SLM with visual capability, the farmer could take a\npicture of the crop at issue and get immediate recommendations on how to treat\npests or disease.\n\n\u201cIf you are in a part of the world that doesn\u2019t have a good network,\u201d said\nVargas, \u201cyou are still going to be able to have AI experiences on your\ndevice.\u201d\n\n## The role of high-quality data\n\nJust as the name implies, compared to LLMs, SLMs are tiny, at least by AI\nstandards. Phi-3-mini has \u201conly\u201d 3.8 billion parameters \u2013 a unit of measure\nthat refers to the algorithmic knobs on a model that help determine its\noutput. By contrast, the biggest large language models are many orders of\nmagnitude larger.\n\nThe huge advances in generative AI ushered in by large language models were\nlargely thought to be enabled by their sheer size. But the Microsoft team was\nable to develop small language models that can deliver outsized results in a\ntiny package. This breakthrough was enabled by a highly selective approach to\ntraining data \u2013 which is where children\u2019s books come into play.\n\nTo date, the standard way to train large language models has been to use\nmassive amounts of data from the internet. This was thought to be the only way\nto meet this type of model\u2019s huge appetite for content, which it needs to\n\u201clearn\u201d to understand the nuances of language and generate intelligent answers\nto user prompts. But Microsoft researchers had a different idea.\n\n\u201cInstead of training on just raw web data, why don\u2019t you look for data which\nis of extremely high quality?\u201d asked Sebastien Bubeck, Microsoft vice\npresident of generative AI research who has led the company\u2019s efforts to\ndevelop more capable small language models. But where to focus?\n\nInspired by Eldan\u2019s nightly reading ritual with his daughter, Microsoft\nresearchers decided to create a discrete dataset starting with 3,000 words \u2013\nincluding a roughly equal number of nouns, verbs and adjectives. Then they\nasked a large language model to create a children\u2019s story using one noun, one\nverb and one adjective from the list \u2013 a prompt they repeated millions of\ntimes over several days, generating millions of tiny children\u2019s stories.\n\n> SLMs are uniquely positioned for ... computations where you don\u2019t need to go\n> to the cloud to get things done.\n\nThey dubbed the resulting dataset \u201cTinyStories\u201d and used it to train very\nsmall language models of around 10 million parameters. To their surprise, when\nprompted to create its own stories, the small language model trained on\nTinyStories generated fluent narratives with perfect grammar.\n\nNext, they took their experiment up a grade, so to speak. This time a bigger\ngroup of researchers used carefully selected publicly-available data that was\nfiltered based on educational value and content quality to train Phi-1. After\ncollecting publicly available information into an initial dataset, they used a\nprompting and seeding formula inspired by the one used for TinyStories, but\ntook it one step further and made it more sophisticated, so that it would\ncapture a wider scope of data. To ensure high quality, they repeatedly\nfiltered the resulting content before feeding it back into a LLM for further\nsynthesizing. In this way, over several weeks, they built up a corpus of data\nlarge enough to train a more capable SLM.\n\n\u201cA lot of care goes into producing these synthetic data,\u201d Bubeck said,\nreferring to data generated by AI, \u201clooking over it, making sure it makes\nsense, filtering it out. We don\u2019t take everything that we produce.\u201d They\ndubbed this dataset \u201cCodeTextbook.\u201d\n\nThe researchers further enhanced the dataset by approaching data selection\nlike a teacher breaking down difficult concepts for a student. \u201cBecause it\u2019s\nreading from textbook-like material, from quality documents that explain\nthings very, very well,\u201d said Bubeck, \u201cyou make the task of the language model\nto read and understand this material much easier.\u201d\n\nDistinguishing between high- and low-quality information isn\u2019t difficult for a\nhuman, but sorting through more than a terabyte of data that Microsoft\nresearchers determined they would need to train their SLM would be impossible\nwithout help from a LLM.\n\n\u201cThe power of the current generation of large language models is really an\nenabler that we didn\u2019t have before in terms of synthetic data generation,\u201d\nsaid Ece Kamar, a Microsoft vice president who leads the Microsoft Research AI\nFrontiers Lab, where the new training approach was developed.\n\nStarting with carefully selected data helps reduce the likelihood of models\nreturning unwanted or inappropriate responses, but it\u2019s not sufficient to\nguard against all potential safety challenges. As with all generative AI model\nreleases, Microsoft\u2019s product and responsible AI teams used a multi-layered\napproach to manage and mitigate risks in developing Phi-3 models.\n\nFor instance, after initial training they provided additional examples and\nfeedback on how the models should ideally respond, which builds in an\nadditional safety layer and helps the model generate high-quality results.\nEach model also undergoes assessment, testing and manual red-teaming, in which\nexperts identify and address potential vulnerabilities.\n\nFinally, developers using the Phi-3 model family can also take advantage of a\nsuite of tools available in Azure AI to help them build safer and more\ntrustworthy applications.\n\n## Choosing the right-size language model for the right task\n\nBut even small language models trained on high quality data have limitations.\nThey are not designed for in-depth knowledge retrieval, where large language\nmodels excel due to their greater capacity and training using much larger data\nsets.\n\nLLMs are better than SLMs at complex reasoning over large amounts of\ninformation due to their size and processing power. That\u2019s a function that\ncould be relevant for drug discovery, for example, by helping to pore through\nvast stores of scientific papers, analyze complex patterns and understand\ninteractions between genes, proteins or chemicals.\n\n\u201cAnything that involves things like planning where you have a task, and the\ntask is complicated enough that you need to figure out how to partition that\ntask into a set of sub tasks, and sometimes sub-sub tasks, and then execute\nthrough all of those to come with a final answer ... are really going to be in\nthe domain of large models for a while,\u201d said Vargas.\n\nBased on ongoing conversations with customers, Vargas and Yadav expect to see\nsome companies \u201coffloading\u201d some tasks to small models if the task is not too\ncomplex.\n\nSonali Yadav, principal product manager for Generative AI at Microsoft. (Photo\nby Dan DeLong for Microsoft)\n\nFor instance, a business could use Phi-3 to summarize the main points of a\nlong document or extract relevant insights and industry trends from market\nresearch reports. Another organization might use Phi-3 to generate copy,\nhelping create content for marketing or sales teams such as product\ndescriptions or social media posts. Or, a company might use Phi-3 to power a\nsupport chatbot to answer customers\u2019 basic questions about their plan, or\nservice upgrades.\n\nInternally, Microsoft is already using suites of models, where large language\nmodels play the role of router, to direct certain queries that require less\ncomputing power to small language models, while tackling other more complex\nrequests itself.\n\n\u201cThe claim here is not that SLMs are going to substitute or replace large\nlanguage models,\u201d said Kamar. Instead, SLMs \u201care uniquely positioned for\ncomputation on the edge, computation on the device, computations where you\ndon\u2019t need to go to the cloud to get things done. That\u2019s why it is important\nfor us to understand the strengths and weaknesses of this model portfolio.\u201d\n\nAnd size carries important advantages. There\u2019s still a gap between small\nlanguage models and the level of intelligence that you can get from the big\nmodels on the cloud, said Bubeck. \u201cAnd maybe there will always be a gap\nbecause you know \u2013 the big models are going to keep making progress.\u201d\n\nRelated links:\n\n  * Read more: Introducing Phi-3, redefining what\u2019s possible with SLMs\n  * Learn more: Azure AI\n  * Read more: Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone\n\nTop image: Sebastien Bubeck, Microsoft vice president of Generative AI\nresearch who has led the company\u2019s efforts to develop more capable small\nlanguage models. (Photo by Dan DeLong for Microsoft)\n\n  * AI\n\nEnglish (United States) Your Privacy Choices Consumer Health Privacy\n\n", "frontpage": false}
