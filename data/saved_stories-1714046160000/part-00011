{"aid": "40152107", "title": "Snowflake Arctic", "url": "https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/", "domain": "snowflake.com", "votes": 1, "user": "helloericsf", "posted_at": "2024-04-25 01:18:09", "comments": 0, "source_title": "Snowflake Arctic - LLM for Enterprise AI", "source_text": "Snowflake Arctic - LLM for Enterprise AI\n\nSkip to content\n\n### Subscribe to our blog!\n\n### Thank you for your submission.\n\nAuthor\n\nSnowflake AI Research\n\nShare\n\nSubscribe\n\nApr 24, 2024\n\n# Snowflake Arctic: The Best LLM for Enterprise AI \u2014 Efficiently Intelligent,\nTruly Open\n\n  * Product and Technology\n\n    * AI & ML\n\nBuilding top-tier enterprise-grade intelligence using LLMs has traditionally\nbeen prohibitively expensive and resource-hungry, and often costs tens to\nhundreds of millions of dollars. As researchers, we have grappled with the\nconstraints of efficiently training and inferencing LLMs for years. Members of\nthe Snowflake AI Research team pioneered systems such as ZeRO and DeepSpeed,\nPagedAttention / vLLM, and LLM360 which significantly reduced the cost of LLM\ntraining and inference, and open sourced them to make LLMs more accessible and\ncost-effective for the community.\n\nToday, the Snowflake AI Research Team is thrilled to introduce Snowflake\nArctic, a top-tier enterprise-focused LLM that pushes the frontiers of cost-\neffective training and openness. Arctic is efficiently intelligent and truly\nopen.\n\n  * Efficiently Intelligent: Arctic excels at enterprise tasks such as SQL generation, coding and instruction following benchmarks even when compared to open source models trained with significantly higher compute budgets. In fact, it sets a new baseline for cost-effective training to enable Snowflake customers to create high-quality custom models for their enterprise needs at a low cost.\n\n  * Truly Open: Apache 2.0 license provides ungated access to weights and code. In addition, we are also open sourcing all of our data recipes and research insights.\n\nSnowflake Arctic is available from Hugging Face today or via your model garden\nor catalog of choice, including Snowflake Cortex, Amazon Web Services (AWS),\nMicrosoft Azure, NVIDIA API catalog, Lamini, Perplexity, Replicate and\nTogether over the coming days.\n\nFig 1. Enterprise intelligence \u2013 average of Coding (HumanEval+ and MBPP+), SQL\nGeneration (Spider), and Instruction following (IFEval) \u2013 vs. Training cost\n\n### Top-tier enterprise intelligence at incredibly low training cost\n\nAt Snowflake, we see a consistent pattern in AI needs and use cases from our\nenterprise customers. Enterprises want to use LLMs to build conversational SQL\ndata copilots, code copilots and RAG chatbots. From a metrics perspective,\nthis translates to LLMs that excel at SQL, code, complex instruction following\nand the ability to produce grounded answers. We capture these abilities into a\nsingle metric we call enterprise intelligence by taking an average of Coding\n(HumanEval+ and MBPP+), SQL Generation (Spider) and Instruction following\n(IFEval).\n\nArctic offers top-tier enterprise intelligence among open source LLMs, and it\ndoes so using a training compute budget of roughly under $2 million (less than\n3K GPU weeks). This means Arctic is more capable than other open source models\ntrained with a similar compute budget. More importantly, it excels at\nenterprise intelligence, even when compared to those trained with a\nsignificantly higher compute budget. The high training efficiency of Arctic\nalso means that Snowflake customers and the AI community at large can train\ncustom models in a much more affordable way.\n\nAs seen in Figure 1, Arctic is on par or better than both LLAMA 3 8B and LLAMA\n2 70B on enterprise metrics, while using less than 1\u20442 of the training compute\nbudget. Similarly, despite using 17x less compute budget, Arctic is on par\nwith Llama3 70B in enterprise metrics like Coding (HumanEval+ & MBPP+), SQL\n(Spider) and Instruction Following (IFEval). It does so while remaining\ncompetitive on overall performance. For example, despite using 7x less compute\nthan DBRX it remains competitive on Language Understanding and Reasoning (a\ncollection of 11 metrics) while being better in Math (GSM8K). For a detailed\nbreakdown of results by individual benchmark, see the Metrics section.\n\nTable 1 Model architecture and training compute for Arctic, Llama-2 70B, DBRX\nand Mixtral 8x22B. Training compute is proportional to the product of active\nparameters and training tokens.\n\n### Training efficiency\n\nTo achieve this level of training efficiency, Arctic uses a unique Dense-MoE\nHybrid transformer architecture. It combines a 10B dense transformer model\nwith a residual 128\u00d73.66B MoE MLP resulting in 480B total and 17B active\nparameters chosen using a top-2 gating. It was designed and trained using the\nfollowing three key insights and innovations:\n\n1) Many-but-condensed experts with more expert choices: In late 2021, the\nDeepSpeed team demonstrated that MoE can be applied to auto-regressive LLMs to\nsignificantly improve model quality without increasing compute cost.\n\nIn designing Arctic, we noticed, based on the above, that the improvement of\nthe model quality depended primarily on the number of experts and the total\nnumber of parameters in the MoE model, and the number of ways in which these\nexperts can be combined together.\n\nBased on this insight, Arctic is designed to have 480B parameters spread\nacross 128 fine-grained experts and uses top-2 gating to choose 17B active\nparameters. In contrast, recent MoE models are built with significantly fewer\nexperts as shown in Table 2. Intuitively, Arctic leverages a large number of\ntotal parameters and many experts to enlarge the model capacity for top-tier\nintelligence, while it judiciously chooses among many-but-condensed experts\nand engages a moderate number of active parameters for resource-efficient\ntraining and inference.\n\nFigure 2. Standard MoE Architecture vs. Arctic\n\n2) Architecture and System Co-design: Training vanilla MoE architecture with a\nlarge number of experts is very inefficient even on the most powerful AI\ntraining hardware due to high all-to-all communication overhead among experts.\nHowever, it is possible to hide this overhead if the communication can be\noverlapped with computation.\n\nOur second insight is that combining a dense transformer with a residual MoE\ncomponent (Fig 2) in the Arctic architecture enables our training system to\nachieve good training efficiency via communication computation overlap, hiding\na big portion of the communication overhead.\n\n3) Enterprise-Focused Data Curriculum: Excelling at enterprise metrics like\nCode Generation and SQL requires a vastly different data curriculum than\ntraining models for generic metrics. Over hundreds of small-scale ablations,\nwe learned that generic skills like common sense reasoning can be learned in\nthe beginning, while more complex metrics like coding, math and SQL can be\nlearned effectively towards the latter part of the training. One can draw\nanalogies to human life and education, where we acquire capabilities from\nsimpler to harder. As such, Arctic was trained with a three-stage curriculum\neach with a different data composition focusing on generic skills in the first\nphase (1T Tokens), and enterprise-focused skills in the latter two phases\n(1.5T and 1T tokens). A high-level summary of our dynamic curriculum is shown\nhere.\n\nTable 2. Dynamic data composition for three-phase training of Arctic with\nemphasis on enterprise intelligence.\n\n### Inference efficiency\n\nFigure 3. Enterprise intelligence \u2013 average of Coding (HumanEval+ and MBPP+),\nSQL Generation (Spider), and Instruction following (IFEval) vs. Active\nParameters during Inference\n\nTraining efficiency represents only one side of the efficient intelligence of\nArctic. Inference efficiency is equally critical to allow for the practical\ndeployment of the model at a low cost. Arctic represents a leap in MoE model\nscale, using more experts and total parameters than any other open sourced\nauto-regressive MoE model. As such, several system insights and innovations\nare necessary to run inference on Arctic efficiently:\n\na) At interactive inference of a small batch size, e.g., batch size of 1, an\nMoE model\u2019s inference latency is bottlenecked by the time it takes to read all\nthe active parameters, where the inference is memory bandwidth bounded. At\nthis batch size, Arctic (17B active parameters) can have up to 4x less memory\nreads than Code-Llama 70B, and up to 2.5x less than Mixtral 8x22B (44B active\nparameters), leading to faster inference performance.\n\nWe have collaborated with NVIDIA and worked with NVIDIA (TensorRT-LLM) and the\nvLLM teams to provide a preliminary implementation of Arctic for interactive\ninference. With FP8 quantization, we can fit Arctic within a single GPU node.\nWhile far from fully optimized, at a batch size of 1, Arctic has a throughput\nof over 70+ tokens/second for effective interactive serving.\n\nb) As the batch size increases significantly e.g., thousands of tokens per\nforward pass, Arctic switches from being memory bandwidth bound to compute\nbound, where the inference is bottlenecked by the active parameters per token.\nAt this point, Arctic incurs 4x less compute than CodeLlama 70B and Llama 3\n70B.\n\nTo enable compute bound inference and high relative throughput that\ncorresponds to the small number of active parameters in Arctic (as shown in\nFig 3), a large batch size is needed. Achieving this requires having enough KV\ncache memory to support the large batch size while also having enough memory\nto store nearly 500B parameters for the model. While challenging, this can be\nachieved with two-node inference using a combination of system optimizations\nsuch as FP8 weights, split-fuse and continuous batching, tensor parallelism\nwithin a node and pipeline parallelism across nodes.\n\nWe have worked closely with NVIDIA to optimize inference for NVIDIA NIM\nmicroservices powered by TensorRT-LLM. In parallel, we are working with the\nvLLM community, and our in-house development team is also enabling efficient\ninference of Arctic for enterprise use cases in the coming weeks.\n\n### Truly open\n\nArctic was built upon the collective experiences of our diverse team, as well\nas major insights and learnings from the community. Open collaboration is key\nto innovation, and Arctic would not have been possible without open source\ncode and open research insights from the community. We are thankful to the\ncommunity and eager to give back our own learnings to enrich the collective\nknowledge and empower others to succeed.\n\nOur commitment to a truly open ecosystem goes beyond open weights and code but\nalso having open research insights and open source recipes.\n\n#### Open research insights\n\nThe construction of Arctic has unfolded along two distinct trajectories: the\nopen path, which we navigated swiftly thanks to the wealth of community\ninsights, and the hard path, which is characterized by the segments of\nresearch that lacked prior community insights, necessitating intensive\ndebugging and numerous ablations.\n\nWith this release, we\u2019re not just unveiling the model; we\u2019re also sharing our\nresearch insights through a comprehensive \u2018cookbook\u2019 that opens up our\nfindings from the hard path. The cookbook is designed to expedite the learning\nprocess for anyone looking to build world-class MoE models. It offers a blend\nof high-level insights and granular technical details in crafting an LLM akin\nto Arctic so you can build your desired intelligence efficiently and\neconomically \u2014 guided by the open path instead of the hard one.\n\nThe cookbook spans a breadth of topics, including pre-training, fine-tuning,\ninference and evaluation, and also delves into modeling, data, systems and\ninfrastructure. You can preview the table of contents, which outlines over 20\nsubjects. We will be releasing corresponding Medium.com blog posts daily over\nthe next month. For instance, we\u2019ll disclose our strategies for sourcing and\nrefining web data in \u201cWhat data to use?\u201d We\u2019ll discuss our data composition\nand curriculum in \u201cHow to compose data.\u201d Our exploration of MoE architecture\nvariations will be detailed in \u201cAdvanced MoE architecture,\u201d discussing the co-\ndesign of model architecture and system performance. And for those curious\nabout LLM evaluation, our \u201cHow to evaluate and compare model quality \u2014 less\nstraightforward than you think\u201d will shed light on the unexpected complexities\nwe encountered.\n\nThrough this initiative, we aspire to contribute to an open community where\ncollective learning and advancement are the norms to push the boundaries of\nthis field further.\n\n#### Open source serving code\n\n  * We are releasing model checkpoints for both the base and instruct-tuned versions of Arctic under an Apache 2.0 license. This means you can use them freely in your own research, prototypes and products.\n\n  * Our LoRA-based fine-tuning pipeline, complete with a recipe, allows for efficient model tuning on a single node.\n\n  * In collaboration with NVIDIA TensorRT-LLM and vLLM, we are developing initial inference implementations for Arctic, optimized for interactive use with a batch size of one. We are excited to work with the community to tackle the complexities of high-batch size inference of really large MoE models.\n\n  * Arctic is trained using a 4K attention context window. We are developing an attention-sinks-based sliding window implementation to support unlimited sequence generation capability in the coming weeks. We look forward to working with the community to extend to a 32K attention window in the near future.\n\n##### Metrics\n\nOur focus from a metrics perspective is primarily on what we call enterprise\nintelligence metrics, a collection of skills that are critical for enterprise\ncustomers that includes, Coding (HumanEval+ and MBPP+), SQL Generation\n(Spider) and Instruction following (IFEval).\n\nAt the same time, it is equally important to evaluate LLMs on the metrics the\nresearch community evaluates them on. This includes world knowledge, common\nsense reasoning and math capabilities. We refer to these metrics as academic\nbenchmarks.\n\nHere is a comparison of Arctic with multiple open source models across\nenterprise and academic metrics:\n\nFor enterprise metrics, Arctic demonstrates top-tier performance compared to\nall other open source models regardless of the compute class. For other\nmetrics, it achieves top-tier performance at its compute class and even\nremains competitive with models trained with higher compute budgets. Snowflake\nArctic is the best open source model for off-the-shelf enterprise use cases.\nAnd if you are looking to train your own model from scratch at the lowest\ntotal cost of ownership (TCO), the training infrastructure and systems\noptimization descriptions in our cookbook should be of great interest.\n\nFor academic benchmarks, there has been a focus on world knowledge metrics\nsuch as MMLU to represent model performance. With high-quality web and STEM\ndata, MMLU monotonically moves up as a function of training FLOPS. Since one\nobjective for Arctic was to optimize for training efficiency while keeping the\ntraining budget small, a natural consequence is lower MMLU performance\ncompared to recent top-tier models. In line with this insight, we expect our\nongoing training run at a higher training compute budget than Arctic to exceed\nArctic\u2019s MMLU performance. We note that performance on MMLU world knowledge\ndoesn\u2019t necessarily correlate with our focus on enterprise intelligence.\n\nTable 3. Full Metrics Table. Comparing Snowflake Arctic with DBRX, LLAMA-3 8B,\nLLAMA-3 70B, Mixtral 8x7B, Mixtral 8x22B (instruction-tuned or chat variants\nif available).^1 ^2\n\n### Getting started with Arctic\n\nSnowflake AI Research also recently announced and open sourced the Arctic\nEmbed family of models that achieves SoTA in MTEB retrieval. We are eager to\nwork with the community as we develop the next generation in the Arctic family\nof models. Join us at our Data Cloud Summit on June 3-6 to learn more.\n\nHere\u2019s how we can collaborate on Arctic starting today:\n\n  * Go to Hugging Face to directly download Arctic and use our Github repo for inference and fine-tuning recipes.\n  * For a serverless experience in Snowflake Cortex, Snowflake customers with a payment method on file will be able to access Snowflake Arctic for free until June 3. Daily limits apply.\n  * Access Arctic via your model garden or catalog of choice including Amazon Web Services (AWS), Lamini, Microsoft Azure, NVIDIA API catalog, Perplexity, Replicate and Together AI over the coming days.\n  * Chat with Arctic! Try a live demo now on Streamlit Community Cloud or on Hugging Face Streamlit Spaces, with an API powered by our friends at Replicate.\n  * Get mentorship and credits to help you build your own Arctic-powered applications during our Arctic-themed Community Hackathon.\n\nAnd finally, don\u2019t forget to read the first edition of our cookbook recipes to\nlearn more about how to build your own custom MoE models in the most cost-\neffective way possible.\n\n### Acknowledgments\n\nWe would like to thank AWS for their collaboration and partnership in building\nArctic\u2019s training cluster and infrastructure, and Nvidia for their partnership\nin enabling Arctic support on NVIDIA TensorRT-LLM. We also thank the open\nsource community for producing the models, datasets and dataset recipe\ninsights we could build on top of to make this release possible. We would also\nlike to thank our partners in AWS, Microsoft Azure, NVIDIA API catalog,\nLamini, Perplexity, Replicate and Together AI for their collaboration in\nmaking Arctic available\n\n1\\. The 11 metrics for Language Understanding and Reasoning include ARC-Easy,\nARC-Challenge, BoolQ, CommonsenseQA, COPA, HellaSwag, LAMBADA, OpenBookQA,\nPIQA, RACE and WinoGrande.\n\n2\\. Evaluation scores for HumanEval+/MBPP+ v0.1.0 were obtained assuming (1)\nbigcode-evaluation-harness using model-specific chat templates and aligned\npost-processing, (2) greedy decoding. We evaluated all models with our\npipeline to ensure consistency. We validated that our evaluations results are\nconsistent with EvalPlus leaderboard. In fact, our pipeline produces numbers\nthat are a few points higher than the numbers in EvalPlus for all models\ngiving us confidence that we are evaluating each model in the best way\npossible.\n\nShare\n\n## Related content\n\n  * Product and Technology\n\n    * AI & ML\n\nApr 16, 2024\n\n### Snowflake Launches the World\u2019s Best Practical Text-Embedding Model for\nRetrieval Use Cases\n\nToday Snowflake is launching and open-sourcing with an Apache 2.0 license the\nSnowflake Arctic embed family of models. Based on the Massive Text Embedding\nBenchmark (MTEB) Retrieval Leaderboard, the largest...\n\nMore\n\nRead More\n\n  * Product and Technology\n\n    * AI & ML\n\nMar 05, 2024\n\n### Easy and Secure LLM Inference and Retrieval Augmented Generation (RAG)\nUsing Snowflake Cortex\n\nBecause human-machine interaction using natural language is now possible with\nlarge language models (LLMs), more...\n\nMore Details\n\nRead More\n\n### Essential Guide to Gen AI\n\nDownload now\n\n### Build Your Code in Snowflake Using Snowpark and Your Favorite...\n\nTo develop and deploy code with Snowpark, developers have always had the\nchance to work from their favorite IDE or notebook....\n\nSee how\n\nRead More\n\n### What is a Data Marketplace?\n\nA data marketplace, also known as a data exchange, is an online transactional\nlocation or store that facilitates the buying,...\n\nFull Details\n\nRead More\n\n### Inside the Data Cloud | Snowflake Blog\n\nFrom technical articles about AI and application strategies to real-world\nsuccess stories, the Snowflake Blog explores...\n\nFull Details\n\nRead More\n\n### The Data Cloud Explained\n\nLearn more about the Data Cloud, a global network that connects organizations\nto the data and apps most critical to their business.\n\nHere's How\n\nRead More\n\n  * Platform\n\n    * Cloud Data Platform\n    * Pricing\n    * Marketplace\n    * Security & Trust\n  * Solutions\n\n    * Snowflake for Financial Services\n    * Snowflake for Advertising, Media, & Entertainment\n    * Snowflake for Retail & CPG\n    * Healthcare & Life Sciences Data Cloud\n    * Snowflake for Marketing Analytics\n  * Resources\n\n    * Resource Library\n    * Webinars\n    * Documentation\n    * Community\n    * Procurement\n    * Legal\n  * Explore\n\n    * News\n    * Blog\n    * Trending\n    * Guides\n    * Developers\n  * About\n\n    * About Snowflake\n    * Investor Relations\n    * Leadership & Board\n    * Snowflake Ventures\n    * Careers\n    * Contact\n\nSign up for Snowflake Communications\n\nThanks for signing up!\n\n  * Privacy Notice\n  * Site Terms\n  * Cookie Settings\n  * Do Not Share My Personal Information\n\n\u00a9 2024 Snowflake Inc. All Rights Reserved | If you\u2019d rather not receive future emails from Snowflake, unsubscribe here or customize your communication preferences\n\n", "frontpage": false}
