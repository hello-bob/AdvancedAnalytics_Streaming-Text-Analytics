{"aid": "40117316", "title": "Waste Inferences!", "url": "https://www.sublayer.com/blog/posts/waste-inferences", "domain": "sublayer.com", "votes": 1, "user": "Stwerner", "posted_at": "2024-04-22 18:33:50", "comments": 0, "source_title": "Waste Inferences!", "source_text": "Waste Inferences! | Sublayer Blog\n\n  * Home\n  * Blueprints\n  * Docs\n  * Blog\n\n# Waste Inferences!\n\nScott Werner\n\nScott Werner\n\nBack in the 1970\u2019s, Caltech professor Carver Mead suggested that, given the\nimplications of Moore\u2019s Law (which he coined!), we should embrace the growing\nabundance of transistors and \u201cwaste\u201d them. Computing power was becoming\ncheaper at an exponential rate, and what that meant was that we should work to\ncreate more powerful, flexible, and innovative designs, even if it meant that\nthey seemed inefficient at first glance.\n\nJust a couple weeks ago Ethan Mollick released a newsletter (What Just\nHappened, What is Happening Next) that started off with the line \u201cThe current\nbest estimates of the rate of improvement in Large Language Models show\ncapabilities doubling every 5 to 14 months\u201d. Not only are LLMs getting more\npowerful at an incredible rate, but the costs of using them are decreasing at\na similar pace.\n\nWhile many things are different this generation, the implication of the\nexponential growth of computing power and reduction in cost remains the same.\nWe should embrace it and use it to create more powerful, flexible, and\ninnovative designs even if it means they seem inefficient at first glance. In\nother words, we should be \u201cwasting inferences\u201d \u2013 using the abundant computing\npower of LLMs and plummeting costs to experiment with novel applications and\ndesigns, even if they don\u2019t seem optimally efficient initially.\n\n## Thin Wrappers\n\nThe main way to dismiss anything anyone was building on LLMs over the last\nyear was \u201cisn\u2019t this just a thin wrapper on top of GPT4?\u201d. My answer was\nalways some variation of \u201cWell isn\u2019t Salesforce just a thin wrapper on top of\na relational database?\u201d. It wasn\u2019t until I read Venkatesh Rao\u2019s A Camera, Not\nan Engine that I realized I wasn\u2019t taking that line of thinking far enough. In\nthe newsletter, he makes the case that we should be thinking of these\ngenerative AI models more as a discovery rather than an invention. Jeff Bezos\nmade a similar observation right around the same time.\n\nIf we think about these models as a discovery rather than an invention,\nbuilding thin wrappers over them is exactly what should be done! So many\nuseful things in our daily lives are thin wrappers on top of discoveries.\nLightbulbs, toasters, and air conditioners are all just thin wrappers on top\nof electricity. Building wrappers that provide new interfaces and uses for\ndiscoveries are how they are able to make meaningful change in people\u2019s lives.\nThis principle applies not only to physical inventions but also to\ngroundbreaking digital discoveries like LLMs.\n\n## Beyond Conversational\n\nIf your exposure to LLMs since the release of ChatGPT has been limited to\nvarious chatbots, you might find the comparison between LLMs and the discovery\nof electricity to be an exaggeration. However, the potential applications of\nLLMs extend far beyond conversational interfaces. Once you start building\nsoftware with them, and you realize that what you have access to is a near-\nuniversal function of string to string, you start to grasp how transformative\nthese things truly are.\n\nObie Fernandez wrote about this recently in his post The Future of Ruby and\nRails in the Age of AI. In the post he describes a component he\u2019s building for\nhis AI-powered consultants platform Olympia and ends the section with \u201cThe\ninternal API for this component is plain text. I\u2019m literally taking what would\nhave been dozens or hundreds of lines of code and letting AI handle the job in\na black-box fashion. Already. Today.\u201d\n\nThings that previously would have required teams of people, taken multiple\nsprints, or even quarters worth of work, are now an API call away. Obie\nFernandez\u2019s example demonstrates how LLMs can significantly reduce development\ntime and effort. As more developers recognize the potential of \u201cwasting\ninferences\u201d on innovative applications, we\u2019ll likely see a surge in powerful,\nAI-driven solutions across many different domains.\n\n## Where To Start\n\nOk so if you\u2019re still with me, you may be thinking: \u201cWhere do I start? How can\nI waste inferences and make more thin wrappers?\u201d Well I\u2019m glad you asked! My\nrecommendation is to start small. At Sublayer we\u2019re building a Ruby AI\nframework that works with all available models to do just that.\n\nAt its simplest, the way you work with language models can be summed up as:\nGather information -> Send information to an LLM -> Do something with the\nLLM\u2019s output\n\nOur framework is designed to help you build programs with that flow as simply\nas possible, and we have tutorials up for how to build simple things like a\nTDD bot with an LLM writing code for you, a simple voice chat application in\nRails, and coming soon, a tutorial for how we built CLAG, a command line\nprogram that generates command line commands for you with it. We\u2019re constantly\nmaking new tutorials, so keep an eye out and let us know if there\u2019s anything\nyou build with it, we\u2019d love to help spread the word!\n\nThese tutorials are just examples though. They\u2019re meant to show how quickly\nyou\u2019re able to create new applications and \u201cwaste inferences\u201d on powerful,\nflexible, and innovative things that may not seem the most efficient at first\nglance.\n\nMake sure to also check out our docs site that has interactive code generation\nbuilt right into it to make getting started even faster.\n\n## Learn More\n\nReady to learn more?\n\nWe spend most of our time in the Sublayer discord, so if you have questions,\nrequests for more tutorials or features, and want to learn more about how to\n\u201cwaste inferences\u201d, come join us! You\u2019ll have the opportunity to collaborate\nwith like-minded developers, get support for your projects, and stay up-to-\ndate with the latest advancements in AI-powered development. We\u2019d love to meet\nyou and push the limits of what these models are capable of together!\n\nThere\u2019s also a larger, more general community of Rubyists forming around AI\nthat we\u2019re a part of. Join the Ruby AI Builders discord to connect with\ndevelopers who are exploring various applications of AI in the Ruby ecosystem.\nIt\u2019s a great place to exchange ideas, share your projects, and learn from the\nexperiences of others\n\nKeep an eye out for our post next week where we go into more details about why\nwe think Ruby is a sleeping giant in the world of AI application development\nand is perfect for \u201cWasting Inferences\u201d\n\n## More Posts\n\n### Introducing Blueprints: A New Approach to AI-Assisted Coding\n\nMarch 7, 2024\n\nToday, we\u2019re excited to officially announce the release of Blueprints! A new,\nopen-source, (and soon, model-agnostic!) approach to AI-assisted coding that\nhelps you leverage patterns in your existing codebase for code generation.\nPersonalized to you and your team\u2019s unique style. Introduction There is a lot\nof excitement these days around AI programming assistants like Github ...\n\nScott Werner\n\n### Hallucinations Are a Feature, Not a Bug\n\nFebruary 20, 2024\n\nOne thing that becomes hard to ignore with generative AI once you get past the\ninitial wave of amazement is their tendency to hallucinate. Inaccuracies in\nanswers and artifacts in images reveal the AI\u2019s true lack of understanding.\nRemember how much trouble they had generating hands early last year? Money and\nengineering effort are pouring ...\n\nScott Werner\n\n", "frontpage": false}
