{"aid": "40048960", "title": "Should You Use Upper Bound Version Constraints (In Python)?", "url": "https://iscinumpy.dev/post/bound-version-constraints/", "domain": "iscinumpy.dev", "votes": 1, "user": "BerislavLopac", "posted_at": "2024-04-16 06:33:56", "comments": 0, "source_title": "Should You Use Upper Bound Version Constraints?", "source_text": "Should You Use Upper Bound Version Constraints? -\n\n# Should You Use Upper Bound Version Constraints?\n\nPosted on December 9, 2021 (Last modified on July 31, 2023) | Henry Schreiner\n\nBound version constraints (upper caps) are starting to show up in the Python\necosystem. This is causing real world problems with libraries following this\nrecommendation, and is likely to continue to get worse; this practice does not\nscale to large numbers of libraries or large numbers of users. In this\ndiscussion I would like to explain why always providing an upper limit causes\nfar more harm than good even for true SemVer libraries, why libraries that pin\nupper limits require more frequent updates rather than less, and why it is not\nscalable. After reading this, hopefully you will always consider every cap you\nadd, you will know the (few) places where pinning an upper limit is\nreasonable, and will possibly even avoid using libraries that pin upper limits\nneedlessly until the author updates them to remove these pins.\n\nIf this 10,000 word behemoth is a bit long for you, then skip around using the\ntable of contents, or see the TL;DR section at the end, or read version\nnumbers by Bern\u00e1t G\u00e1bor, which is shorter but is a fantastic read with good\nexamples and cute dog pictures. Or Hynek\u2019s Semantic Versioning Will Not Save\nYou Be sure to check at least the JavaScript project analysis before you\nleave!\n\nAlso be warned, I pick on Poetry quite a bit. The rising popularity of Poetry\nis likely due to the simplicity of having one tool vs. many for packaging, but\nit happens to also have a special dependency solver, a new upper bound syntax,\nand a strong recommendation to always limit upper versions - in direct\nopposition to members of the Python core developer team and PyPA developers.\nNot all libraries with excessive version capping are Poetry projects (like\nTensorFlow), but many, many of them are. To be clear, Poetry doesn\u2019t force\nversion pinning on you, but it does push you really, really hard to always\nversion cap, and it\u2019s targeting new Python users that don\u2019t know any better\nyet than to accept bad recommendations. And these affect the whole ecosystem,\nincluding users who do not use poetry, but want to depend on libraries that\ndo! I do really like other aspects of Poetry, and would like to eventually\nhelp it build binary packages with Scikit-build (CMake) via a plugin, and I\nuse it on some of my projects happily. If I don\u2019t pick on Poetry enough for\nyou, don\u2019t worry, I have a follow-up post that picks on it in much more\ndetail. Also, check out pdm, which gives many of the benefits of Poetry while\nfollowing PEP standards.\n\nThis turned out be quite long (and even longer after reviews by PyPA and\nPython core developers), so I\u2019ve included a table of contents. Feel free to\njump to the thing that you care about. This was also split into three posts,\nthe first is application vs. library, and the final one is Poetry versions.\n\n# Intro\n\nWhat is version capping? It\u2019s when you have a dependency in\nproject.dependencies / tool.poetry.dependencies / install_requires, and\ninstead of this:\n\n    \n    \n    click>=7\n\nYou write this:\n\n    \n    \n    click>=7,<8 # Equivalent, the tilde lets the final number be larger click~=7.0\n\nOr, only in Poetry, this:\n\n    \n    \n    [tool.poetry.dependencies] click = \"^7\"\n\nThis allows any newer version up to but not including the next \u201cmajor\u201d\nversion. The syntax here is governed by PEP 440, except for Poetry\u2019s addition,\nwhich comes from other languages like JavaScript\u2019s npm.\n\n## SemVer\n\nLet\u2019s briefly define SemVer - both true SemVer and realistic SemVer. One\ndifference between this post and previous attempts by others is that I will\neven be addressing dependencies that use true SemVer (which there are,\nadmittedly, none, true minor and patch releases are an impossible concept to\nachieve).\n\nSemVer states there are three version digits in the form Major.Minor.Patch.\nThe \u201crule\u201d is that only fixes are allowed if the patch number is increased,\nonly additions are allowed if the minor version is bumped, and if you do\nanything could break downstream users, then the major version must be bumped.\nI recommend the excellent article here by the tox/virtualenv maintainer and\nfellow PyPA member, Bern\u00e1t G\u00e1bor.\n\nWhenever any downstream code breaks and it was not a major release, then\nyou\u2019ll have a smattering of people that immediately start complaining that the\nlibrary \u201cdidn\u2019t follow SemVer\u201d, and that it was not a problem with SemVer, but\nyour problem for not following it. A long discussion can be found here, but\nI\u2019ll give a tiny taste of it. Which version do you bump when you update? It\nturns out one person\u2019s bugfix is another\u2019s breaking change. Basically, a\n\u201cperfect\u201d SemVer library would pretty much always bump the major version,\nsince you almost always could possibly break a user with any change (and if\nyou have enough users, by Hyrum\u2019s law, you will do this). This makes \u201ctrue\u201d\nSemVer pointless. Minor releases are impossible, and patch releases are nearly\nimpossible. If you fix a bug, someone could be depending on the buggy\nbehaviour (distutils, looking at you). Of course, even a SemVer purist will\nadmit users should not break if you add or fix something, but that does mean\nthere is no such thing as \u201cpure\u201d SemVer.\n\nDoes dropping Python 2 require a major release? Many (most) packages did this,\nbut the general answer is ironically no, it is not an addition or a breaking\nchange; the version solver will ensure the correct version is used (unless the\nRequires-Python metadata slot is empty or not updated, never forget this,\nnever set lower than what you test!).\n\nNow, don\u2019t get me wrong, I love \u201crealistic\u201d or \u201calmost\u201d SemVer - I personally\nuse it on all my libraries (I don\u2019t maintain a single CalVer library like pip\nor only-live-at-head library, like googletest). Practical SemVer mostly\nfollows the rule above, but acknowledges the fact that it\u2019s not perfect. It\nalso often adds a new rule to the mix: if you deprecate a feature (almost\nalways in a minor release), you can remove that feature in a future minor\nrelease. You have to check the library to see what the deprecation period is -\nNumPy and Python use three minor releases. This is also used in CalVer\n(versioning based on dates) - you can set a deprecation period in time. Really\nlarge libraries hate making major releases - Python 2->3 was a disaster.\nSemVer purists argue that this makes minor releases into major releases, but\nit\u2019s not that simple - the deprecation period ensures the \u201cnext\u201d version\nworks, which is really useful, and usually gives you time to adjust before the\nremoval happens. It\u2019s a great balance for projects that are well kept up using\nlibraries that move forward at a reasonable pace. If you make sure you can see\ndeprecations, you will almost always work with the next several versions.\n\nJust to avoid potential future confusion, some libraries are so hesitant about\nreleasing new major versions (like Python, NumPy, and CMake) that they make\nbreaking changes (like removing deprecated features) on minor releases, which\npromotes the minor versions to major versions according to SemVer. This is a\nsocial expectation in the ecosystem, and one reason some libraries use CalVer.\n\nThe best description of realistic SemVer I\u2019ve seen is that it\u2019s an\n\u201cabbreviated changelog\u201d. I love this, because I love changelogs - I think it\nis the most important part of documentation you have, a well written changelog\nlets you see what was missing before so you know not to look for it in older\nversions, it lets you know what changed so you can update your code (both to\nsupport a version as well as again when you drop older versions), and is a\ngreat indicator of the health and stability of a project. With SemVer, you can\nlook at the version, and that gives you a quick idea of how large and what\nsort of changes have occurred before checking the changelog.\n\n## Solver\n\nWe need to briefly mention the solver, as there happen to be several, and one\nreason this is more relevant today than a few years ago is due to changes in\nthe solver.\n\nPip\u2019s solver changed in version 20.3 to become significantly smarter. The old\nsolver would ignore incompatible transitive requirements much more often than\nthe new solver does. This means that an upper cap in a library might have been\nignored before, but is much more likely to break things or change the solve\nnow.\n\nIt tries to find a working set of dependencies that all agree with each other.\nBy looking back in time, it\u2019s happy to solve very old versions of packages if\nnewer ones are supposed to be incompatible. This can be helpful, but is slow,\nand also means you can easily get a very ancient set of packages when you\nthought you were getting the latest versions.\n\nPoetry has a unique and very strict (and slower) solver that goes even farther\nhunting for solutions. It forces you to cap Python if a dependency does^1. One\nkey difference is that Poetry has the original environment specification to\nwork with every time, while pip does not know what the original environment\nconstraints were. This enables Poetry to roll back a dependency on a\nsubsequent solve, while pip does not know what the original requirements were\nand so does not know if an older package is valid when it encounters a new\ncap.\n\nConda\u2019s solver is like Poetry, and due to the number of builds in conda\nchannels, this should scare you - initial solves for an environment with a\nlarge number of dependencies (including a single package like ROOT) can take\nminutes, and updates to existing environments can take more than a day. I\u2019ve\nnever had Poetry take more than 70 seconds, but I\u2019ve also not used it on\nanything large; it also always has the original specification, while conda\nonly has it if you use a file based update (which is faster). Conda has gotten\nbetter by taking more shortcuts^2 and guessing things (I haven\u2019t had a 25+\nhour solve in a while), and Mamba\u2019s C implementation and better algorithms\nreally help, but doing a \u201csmart\u201d solve is hard.\n\nWe\u2019ll see it again, but just to point it out here: solver errors are pure\nevil, and can\u2019t be fixed downstream. If a library requires pyparsing>=3 and\nanother library requests pyparsing<3, that\u2019s the end, you are out of business.\n\u201cSmart\u201d solvers may look for older versions of those libraries to see if one\nexists that does not have that cap - if the one with the high lower bound had\na release with a lower upper bound, that\u2019s what it will choose; regardless of\nwhat bugs have been fixed, etc. since that release. We\u2019ll discuss the problems\nthis causes later. However, an under-constrained build is completely trivial\nto fix for any user. It\u2019s just a minor inconvenience to a large number of\nusers.\n\n# The problem: Relying on SemVer for capping versions\n\nNow comes the problem: If you have a dependency, should you add an upper cap?\nLet\u2019s look at the different aspects of this. Be sure you understand the\ndifference between libraries and applications, as defined in a previous post.\n\nWe\u2019ll cover the valid use cases for capping after this section. But, just to\nbe clear, if you know you do not support a new release of a library, then\nabsolutely, go ahead and cap it as soon as you know this to be true. If\nsomething does not work, you should cap (or maybe restrict a single version if\nthe upstream library has a temporary bug rather than a design direction that\u2019s\ncausing the failure). You should also do as much as you can to quickly remove\nthe cap, as all the downsides of capping in the next sections still apply.\n\nThe following will assume you are capping before knowing that something does\nnot work, but just out of general principle, like Poetry recommends and\ndefaults to with poetry add and the default template. In most cases, the\nanswer will be \u201cdon\u2019t\u201d. For simplicity, I will also assume you are being\ntempted to cap to major releases (^1.0.0 in Poetry or ~=1.0 in all other\ntooling that follows Python standards via PEP 440). If you cap to minor\nversions (~=1.0.0), this is much worse, and the arguments below apply even\nmore strongly.\n\n## Version limits break code too\n\nLibrary: \u2705 (applies)\n\nApplication: \u2733\ufe0f (partially applicable)\n\nNo one likes having an update break users. For example, IPython depends on a\nlibrary called Jedi. Jedi 0.18 removed something that IPython used, so until\nIPython 7.20 was released, a \u201cnormal\u201d solve (like pip install ipython)\nresulted in a broken install. It\u2019s tempting to look back at that and say\n\u201cwell, if IPython capped it\u2019s dependencies, that would have been avoided\u201d. In\nfact, every time this happens, you will find well-meaning but misguieded\nsuggestions claiming this is proof you should have chosen reasonable upper\nbounds for all requirements.\n\nHowever, capping dependencies also breaks things, and you can\u2019t fix it\ndownstream. If I write a library or application that depends on a library that\nhas a broken dependency, I can limit it, and then my users are happy. In the\ncase above, the interim solution was to just manually pin Jedi, such as pip\ninstall ipython jedi<0.18 for a user or to cap it in dependencies for a\nlibrary or application. Any user can easily do that - irritating, and a leaky\nabstraction, but fixable. But you can\u2019t fix an over-constraint - and this is\nnot just a pip issue; you\u2019d have to throw out the entire dependency solver\nsystem to get things to install. Most other Jedi releases have been fine,\ncapping on other versions would have been problematic for users who don\u2019t care\nor know about IPython using Jedi.\n\nThere is actually a solution for this outside of PyPI/pip packages; you can\nsupport metadata patches, allowing a release to be modified afterwards to add\nnew compatibility constraints when they are discovered. Conda supports this\nnatively; you can also approximate this with post releases and yanking in\nPyPI, but it\u2019s a pain.\n\nIf you put upper limits, this also then can\u2019t easily be fixed by your\ndependencies - it usually forces the fix on the library that does the pinning.\nThis means every single major release of every dependency you cap immediately\nrequires you to make a new release or everyone using your library can no\nlonger use the latest version of those libraries. If \u201cmake a new release\nquickly\u201d from above bothered you; well, now you have to make it on every\nversion bump of every pinned dependency.^3\n\nIt also means you must support a wide version range; ironically, this is the\nvery opposite of the syntax Poetry adds for capping. For example, let\u2019s say\nyou support click^7. Click 8 comes out, and someone writes a library requiring\nclick^8. Now your library can\u2019t be installed at the same time as that other\nlibrary, your requirements do not overlap. If you update to requiring click\n^8, your update can\u2019t be installed with another library still on click^7. So\nyou have to support click>=7,<9 for a while until most libraries have\nsimilarly updated (and this makes the ^ syntax rather useless, IMO). This\nparticular example is especially bad, because a) it\u2019s common, b) click is used\nfor the \u201capplication\u201d part of the code, which is likely not even used by your\nusage if you are using it as a library, and c) the main breaking change in\nClick 8 was the removal of Python <3.6 support, which is already included in\nthe solve.\n\nA library that requires a manual version intervention is not broken, it\u2019s just\nirritating. A library that can\u2019t be installed due to a version conflict is\nbroken. If that version conflict is fake, then you\u2019ve created an unsolvable\nproblem where one didn\u2019t exist.\n\n## Fixes are not always backported\n\nLibrary: \u2705 (applies)\n\nApplication: \u2705 (applies)\n\nI\u2019ll defer to Bern\u00e1t\u2019s post to explain that many Python projects simply do not\nhave the time or resources to continue to provide fixes and security patches\nfor old major versions. I\u2019ve had old patch update requests refused from\nprojects like pip and pandas. Actually, I\u2019ve even refused them for CLI11, my\nCI was broken on the old 1.x version so I couldn\u2019t run tests.\n\nLet\u2019s look at a concrete example. Let\u2019s say some library version 6.1.0 worked.\nYou pin to <7\\. Then 6.2.0 comes out, and breaks your code. The problem is\ndiscovered and fixed, but the development has gone on too far to easily\nbackport, or it\u2019s too involved, so 7.0.1 works again. Your cap is now broken,\nand your code does not work forever (next section). I have seen fixes like\nthis multiple times, and have been responsible for them, as well. Often the CI\nsystem breaks for old, unmaintained releases, and it\u2019s not feasible to go back\nand fix the old version.\n\nNewer version of code are intended to be better than older versions of code;\nthey fix bugs, the add new hardware/software compatibility, and they fix\nsecurity releases. You should never be limiting your user\u2019s ability to update\nthings you happen to depend on (dependencies should not be a leaky\nabstraction; a user shouldn\u2019t have to know or care about what you depend on).\nUpper limits dramatically limit the ability to update without the end user\u2019s\nknowledge.\n\nYou want users to use the latest versions of your code. You want to be able to\nrelease fixes and have users pick up those fixes. And, in general, you^4 don\u2019t\nlike having to release new patch versions for old major (or even minor)\nversions of your library, at least very far back. So why force your\ndependencies to support old versions with patch releases because you\u2019ve capped\nthem?\n\n## Code does not work forever\n\nLibrary: \u2705 (applies)\n\nApplication: \u2705 (applies)\n\nOne claim I\u2019ve seen Poetry developers make is that capping your dependencies\nmeans your code will work in the future. Hopefully I don\u2019t have to tell you\nthis is wrong; it\u2019s completely wrong for a library for the reason outlined\nabove, and partially wrong for an application. There are lots of reasons code\nbreaks without changing it, let\u2019s look at a few.\n\nOne of the most recent ones was the macOS Apple Silicon transition. In order\nto get working code, you have to have the latest versions of packages. Python\n3.9 (and later backported to 3.8) is required on Apple Silicon, so if you\ndepend on a library capped to not support those versions, you are out of luck,\nthat code does not work. This is also true with all the major libraries, which\ndid not backport Apple Silicon support very far. You need a recent NumPy, pip,\npackaging, Poetry, etc. And just in case you think you have plenty of time,\nnote that Apple no longer even sells an Intel based notebook, exactly one year\nafter the transition started.\n\nSimilar support rollouts have happened (or are happening) for Linux and\nWindows architectures (like ARM and PowerPC), operating system updates (macOS\n11\u2019s new numbering system broke pip, Poetry, and lots of other things, might\nalso happen to a lesser extent on Windows 11), new manylinux versions, PyPy\nsupport, Musllinux wheels, and even just adding wheel support in general,\nactually. Code simply will never work forever, and allowing the possibility of\nusing newer libraries increases the chance it can be used. If you limit to\nNumPy to 1.17, you will never support Apple Silicon. However, if you don\u2019t\nlimit it, and the final version you actually support happens to be 1.21, then\nyour code will work with Apple Silicon, and future users may have to manually\nlimit versions to <1.22 eventually, but it will work.\n\nArtificially limiting versions will always reduce the chances of it working in\nthe future. It just avoids users in the future from having to add extra\nlimits, but this is a problem that has a simple user workaround, and is not\nthat likely to happen for many dependencies. If someone is using a multiple-\nyear old version of your code, either you disappeared (and you therefore can\u2019t\nfix broken upper limits), or they are being forced to use an old version,\nprobably because someone someone else (artificially) pinned your library.\n\n## SemVer never promises to break your code\n\nLibrary: \u2705 (applies)\n\nApplication: \u2705 (applies)\n\nA really easy but incorrect generalization of the SemVer rules is \u201ca major\nversion will break my code\u201d. It\u2019s the basis for Poetry\u2019s recommendation to\nalways cap versions, but it\u2019s a logical fallacy. Even if the library follows\ntrue SemVer perfectly, a major version bump does not promise to break\ndownstream code. It promises that some downstream code may break. If you use\npytest to test your code, for example, the next major version will be very\nunlikely to break. If you write a pytest extension, however, then the chances\nof something breaking are much higher (but not 100%, maybe not even 50%).\nQuite ironically, the better a package follows SemVer, the smaller the change\nwill trigger a major version, and therefore the less likely a major version\nwill break a particular downstream code.\n\nAs a general rule, if you have a reasonably stable dependency, and you only\nuse the documented API, especially if your usage is pretty light/general, then\na major update is extremely unlikely to break your code. It\u2019s quite rare for\nlight usage of a library to break on a major update. It can happen, of course,\nbut is unlikely. If you are using something very heavily, if you are working\non a framework extension, or if you use internals that are not publicly\ndocumented, then your chances of breaking on a major release are much higher.\nAs mentioned before, Python has a culture of producing FutureWarnings,\nDeprecationWarnings, or PendingDeprecationWarnings (make sure they are on in\nyour testing, and turn into errors), good libraries will use them.\n\nIt may sound ridiculous, but I should probably point out that CalVer libraries\ndo not follow SemVer (usually). poetry add packaging will still do ^21 for the\nversion it adds. You shouldn\u2019t be capping versions, but you really shouldn\u2019t\nbe capping CalVer. Poetry itself depends on packaging = ^20.4 at the time of\nwriting (though it actually vendors it). In fact, if I could find where Poetry\n(not poetry-core, which vendors it) uses packaging, I know how to trigger a\nbug due to the cap on packaging. But I can\u2019t find where it\u2019s imported other\nthan in tests.\n\n## Patch versions could break too\n\nIf you allow new patch versions, you can still be broken, especially by\nperceived security flaws. A recent example was Python 3.7.14, 3.8.14, 3.9.14,\nand 3.10.7, which all fixed a CVE related to denial of service with converting\nlarge integers to strings. This broke some users, like Sage; there\u2019s no way\nthey would have expected to add the complex cap this would have required. You\nsimply can\u2019t predict the future. (And, in this case, you probably also can\u2019t\ncontrol the patch version of Python users use, either!\n\nAnd if you cap patch versions too, you won\u2019t get security fixes. When a\nvulnerability is patched and released and made public, all someone has to do\nis find a library that caps it out, then target it\u2019s users with that\nvulnerability. This is the reason GitHub provides moving tags for GitHub\nActions; to help avoid someone grepping for old vulnerable pinned actions\nafter fixes are released.\n\n## It doesn\u2019t scale\n\nLibrary: \u2705 (applies)\n\nApplication: \u274c (not applicable)\n\nIf you have a single library that doesn\u2019t play well, then you probably will\nget a working solve easily - this is one reason that this practice doesn\u2019t\nseem so bad at first. If more packages start following this tight capping,\nhowever, you end up with a situation where things simply cannot solve - a\nmoderately sized application can have a hundred or more dependencies when\nexpanded. The entire point of packaging is to allow you to get lots of\npackages that each do some job for you - we should be trying to make it easy\nto be able to add dependencies, not harder.\n\nThe implication of this is you should be very careful when you see tight\nrequirements in packages and you have any upper bound caps anywhere in the\ndependency chain. If something caps dependencies, there\u2019s a very good chance\nadding two such packages will break your solve, so you should pick just one -\nor just avoid them altogether, so you can add one in the future. This is a\ngood rule, actually: Never add a library to your dependencies that has\nexcessive upper bound capping. When I have failed to follow this rule for a\nlarger package, I have usually come to regret it.\n\nIf you are doing the capping and are providing a library, you now have a\ncommitment to quickly release an update, ideally right before any capped\ndependency comes out with a new version. Though if you cap, how to you install\ndevelopment versions or even know when a major version is released? This makes\nit harder for downstream packages to update, because they have to wait for all\nthe caps to be moved for all upstream.\n\n## It conflicts with tight lower bounds\n\nLibrary: \u2705 (applies)\n\nApplication: \u274c (not applicable)\n\nA tight lower bound is only bad if packages cap upper bounds. If you can avoid\nupper-cap packages, you can accept tight lower bound packages, which are much\nbetter; better features, better security, better compatibility with new\nhardware and OS\u2019s. A good packaging system should allow you to require modern\npackages; why develop for really old versions of things if the packaging\nsystem can upgrade them? But a upper bound cap breaks this. Hopefully anyone\nwho is writing software and pushing versions will agree that tight lower\nlimits are much better than tight upper limits, so if one has to go, it\u2019s the\nupper limits.\n\nIt is also rather rare that packages solve for lower bounds in CI (I would\nlove to see such a solver become an option, by the way!), so setting a tight\nlower bound is one way to avoid rare errors when old packages are cached that\nyou don\u2019t actually support. CI almost never has a cache of old packages, but\nusers do.\n\nPlease test with a constraints.txt file that forces your lower bounds, by the\nway, at least if you have a reasonable number of users.\n\n## Capping dependencies hides incompatibles\n\nLibrary: \u2705 (applies)\n\nApplication: \u2705 (applies)\n\nAnother serious side effect of capping dependencies is that you are not\nnotified properly of incoming incompatibilities, and you have to be extra\nproactive in monitoring your dependencies for updates. If you don\u2019t cap your\ndependencies, you are immediately notified when a dependency releases a new\nversion, probably by your CI, the first time you build with that new version.\nIf you are running your CI with the --dev flag on your pip install (uncommon,\nbut probably a good idea), then you might even catch and fix the issue before\na release is even made. If you don\u2019t do this, however, then you don\u2019t know\nabout the incompatibility until (much) later.\n\nIf you are not following all of your dependencies, you might not notice that\nyou are out of date until it\u2019s both a serious problem for users and it\u2019s\nreally hard for you to tell what change broke your usage because several\nversions have been released. While I\u2019m not a huge fan of Google\u2019s live-at-head\nphilosophy (primarily because it has heavy requirements not applicable for\nmost open-source projects), I appreciate and love catching a dependency\nincompatibility as soon as you possibly can; the smaller the change set, the\neasier it is to identify and fix the issue.\n\n## Capping all dependencies hides real incompatibilities\n\nLibrary: \u2705 (applies)\n\nApplication: \u2705 (applies)\n\nIf I see X>=1.1, that tells me that you are using features from 1.1 and do not\nsupport 1.0. If I see X<1.2, this should tell me that there\u2019s a problem with\n1.2 and the current software (specifically something you know the dependency\nwill not fix/revert). Not that you just capped all your dependencies and have\nno idea if that will or won\u2019t work at all. A cap should be like a TODO; it\u2019s a\nknown issue that needs to be worked on soon. As in yesterday.\n\n## Libraries that ask you to cap\n\nLibrary: \u2705 (applies)\n\nApplication: \u274c (not applicable)\n\nThere are some libraries who ask users to cap. If a huge change is coming, and\nmost user code is going to be broken, it\u2019s not great, but necessary. If that\ndoes happen, explain it in the readme, and follow it as a user if the\nexplanation applies to you (or look for another library that doesn\u2019t\nintentionally plan to break you). Unfortunately, some libraries are asking\nusers to cap, \u201cjust in case\u201d an API breaking change needs to be made. This is\nwrong for several reasons, especially for writing libraries.\n\nFirst, SemVer is a battle between frequent and rare major bumps; \u201cTrue\u201d SemVer\nforces basically any change to be a breaking change; it\u2019s based on how many\nusers more than how large the change is. This direction makes capping to major\nversions meaningless, because you have too many of them. The other direction\n(which is what any library that thinks they have no major versions coming) is\nnot true SemVer, it\u2019s \u201cpractical\u201d SemVer, and as I\u2019ve shown, you should not\ncap to that. You don\u2019t know a major version will break you, and you don\u2019t know\na minor / patch version will not introduce subtle (or major) bugs. If that\nmatters, you still have to learn how to use an application lock file. There\u2019s\nno free pass here. For the upstream library, they are now basically removing\ntheir ability to make smaller breaking changes, like removing a deprecated\nitem, because that should be a major change, but a major change will cause a\nmassive number of user to become unsupported until they bump their pin (and\nremember, libraries can depend on libraries which depend on libraries which\ndepend... You get the point.)\n\nThe old version must be maintained. If users are asked to pin to major\nversion, or if you just decide to pin to major version, that version must have\nat least critical security and version dependency updates (like new OSs,\narchitectures, or Python versions) applied to it until all the (recursive)\npins are updated. If it\u2019s not a \u201cLTS\u201d release, you should not pin to it,\nunless you have to. There are a few of these LTS in Python, but it\u2019s not the\nsocial norm - Python is OSS and most of us do not have the resources or\nintention to maintain multiple branches of packages. We expect you to use the\nlatest releases. Would you depend on a package you know is going to be\nabandoned? I hope not. If you cap, then that\u2019s what you are doing - the thing\nyou are depending on is going to be abandoned when the next release is made.\nThis forces you to make a new release, and has a trickle-down effect; if you\ncap obviously you need to expect users may cap you!\n\nUnfortunately, what some library authors are using this for is a free pass to\nbreak people\u2019s code without deprecation periods. Not everyone will read the\nREADME, etc, and even if they do, they might dislike capping (for the next\nreason, for example, or any of the other reasons listed here) and not cap your\nlibrary anyway. So a breaking release will break some number of users\nproportional to the total number of users anyway, regardless of what you put\nin your README. Especially if you don\u2019t give a reason, and just have it there\n\u201cjust in case\u201d; but regardless, many (most) users will not read anything\nanyway.\n\n## Backsolving is usually wrong\n\nLibrary: \u2705 (applies)\n\nApplication: \u2705 (applies)\n\nLet\u2019s assume you depend on a library, we\u2019ll call it tree. That library depends\non dirt>=7.0. You also depend on a library bush. That also depends on dirt>=2.\nWhen tree 2.1.3 is released, they noticed they were broken by bush 7.0, so\nthey cap dirt<7.0. Question: what do you think the solver will do? Most people\nwho are on the pro-capping side will answer \u201cproduce an error, because the two\nlibraries have incompatible dependencies and there\u2019s no solution\u201d. In fact,\nI\u2019ve spent quite a bit of time talking about solver errors.\n\nBut this is not one of those, not for a \u201csmart\u201d solver like we are seeing\ntoday. Instead, the solver will backsolve to tree 2.1.2! This is surely not\nwhat the tree developers wanted; that\u2019s probably why they released tree 2.1.3\nin the first place! (This is not just hypothetical - IPython ran into this\nwith Jedi 0.18).\n\nNote that unlike the errors you are likely to see from getting the latest,\nincompatible versions (usually attribute errors and such), these caps can\nintroduce old versions with subtle bugs, security holes, or exactly the same\nattribute errors and such, and this is completely hidden from the user! They\ndon\u2019t know that they are getting old versions, or why. If they are using a\nlibrary that does the capping, they don\u2019t see it, and they thought they were\ngetting the latest versions of everything.\n\nIf you have even a single release with less strict caps than a newer release,\nthis can happen. If Python had editable metadata, and every author could be\ntrusted to edit every past release with the proper caps once one is\nknown/discovered, then this system would be okay. In fact, conda-forge does\nexactly this (albeit, conda-forge admins can do the metadata overrides for\npast releases, which is an important feature that PyPI would never do).\nHowever, changing this for PyPI would be a massive undertaking; you\u2019d need to\nhave a extra patch file (wheels are hashed for security), everything, from\nPip, wheel, twine, warehouse, all installers, locking packages managers,\neverything would need to handle these extra files. Then there are security\nimplications - what do you do if some makes a good release then adds a\nmalicious dependency though metadata editing?\n\nThe recommend solution is to keep capping to an absolute minimum. These issues\nonly occur when you mix a low upper cap with a high lower cap, so fewer upper\ncaps reduce the chances of this happening.\n\nThis is assuming you are participating in an Open Source Software ecosystem.\nIf all you control all your libraries (such as often happens in industry, or\neven within a collection of highly connected libraries that are not to be used\nindependently of each other, then pins between versions are fine - you control\nall the libraries pinning each other, so you can avoid mismatched\ndependencies.\n\nNow let\u2019s make this even more fun as a segue into our next issue. Let\u2019s say,\nbefore numba 0.55 was released, you ask for numba and you are on Python 3.10.\nIf you use Numba, you probably have seen this - you get not 0.54, but 0.50,\nsince that was the last uncapped version of Numba - and the error you see is\nnot the nice setup.py error telling you that Python is too new, but instead a\ncompilation failure for a Numba dependency, llvmlite. No package installer\nthat I\u2019m aware of handles caps on Python versions correctly - and there may\nnot be a \u201ccorrect\u201d way, but this is the same problem as above, just for Python\nversion. But it gets worse.\n\nNow let\u2019s say you are on Python 3.9, and you are using a locking package\nmanager (Poetry, PDM, Pipenv, ...). You leave the default value for Python\ncaps, either no cap or something like ^3.6. What Numba version are you going\nto get? You guessed it, 0.50. Okay, you probably didn\u2019t guess it. Why? Because\nthe lock file it is generating is supposed to be valid for all versions of\nPython you requested - so it picked 0.50, since that\u2019s valid on Python 3.99.\nNever, never put a cap into the Python-Requires metadata slot; and this leads\nus into our next point...\n\n## Pinning the Python version is special\n\nLibrary: \u2705 (applies)\n\nApplication: \u2705 (applies)\n\nAnther practice pushed by Poetry is adding an upper cap to the Python version.\nThis is misusing a feature designed to help with dropping old Python versions\nto instead stop new Python versions from being used. \u201cScrolling back\u201d through\nolder releases to find the newest version that does not restrict the version\nof Python being used is exactly the wrong behavior for an upper cap, and that\nis what the purpose of this field is. All current solvers (Pip, Poetry, PDM)\ndo not work correctly if this field is capped, and implement the scroll back\nbehavior.\n\nThere is a discussion about ways to fix this at\nhttps://discuss.python.org/t/requires-python-upper-limits!\n\nTo be clear, this is very different from a library: specifically, you can\u2019t\ndowngrade your Python version^5 if this is capped to something below your\ncurrent version. You can only fail. So this does not \u201cfix\u201d something by\ngetting an older, working version, it only causes hard failures if it works\nthe way you might hope it does. This means instead of seeing the real failure\nand possibly helping to fix it, users just see a \u201cPython doesn\u2019t match\u201d error.\nAnd, most of the time, it\u2019s not even a real error; if you support Python 3.x\nwithout warnings, you should support Python 3.x+1 (and 3.x+2, too).\n\nCapping to <4 (something like ^3.6 in Poetry) is also directly in conflict\nwith the Python developer\u2019s own statements; they promise the 3->4 transition\nwill be more like the 1->2 transition than the 2->3 transition. It\u2019s not\nlikely to happen soon, and if it does, it likely will be primarily affecting\nStable ABI / Limited API builds and/or GIL usage; it likely will not affect\nnormal Python packages more than normal updates will. When Python 4 does come\nout, it will be really hard to even run your CI on 4 until all your\ndependencies uncap. And you won\u2019t actually see the real failures, you\u2019ll just\nsee incompatibility errors, so you won\u2019t even know what to report to those\nlibraries. And this practice makes it hard to test development versions of\nPython.\n\nAnd, if you use Poetry, as soon as someone caps the Python version, every\nPoetry project that uses it must also cap, even if you believe it is a\ndetestable practice and confusing to users. It is also wrong unless you fully\npin the dependency that forced the cap - if the dependency drops it in a patch\nrelease or something else you support, you no longer would need the cap. Even\nworse, if someone adds a cap or tightens a cap, unless they yank every single\nolder release, a locking solver like Poetry or PDM will backsolve to the last\nversions without the cap so that the lock file it creates will be \u201cvalid\u201d on\nall the Python versions you are requesting! This is because these solvers are\nusing the cap for the lock file - the lock file - lock files cannot lock the\nPython version (another fundamental difference), so they are computing the\nrange of Python versions the lock file is valid for. This is different than\nthe Python-Requires metadata slot, but Poetry and PDM both do not have\nseparate settings. If metadata was mutable (it is not) and you actually\ntrusted library authors to go back and check every old release for the correct\nPython cap (not going to happen), upper capping here is worse than useless.\n\nIf you are developing a package like Numba, where internal Python details\n(bytecode) are relied on so there really is a 0% chance of it working,\nmanually adding an error in your setup.py is fine, but still do not limit\nhere! This metadata field was not designed to support upper caps, and an upper\ncap should always translate an error; it does not change your solve. Never\nprovide an upper cap to your Python version. I generally will not use a\nlibrary that has an upper cap to the Python version; when I have missed this,\nI\u2019ve been bitten by it, hard (cibuildwheel, pybind11, and several other\npackage\u2019s CI went down). To be clear, in that case, Python 3.10 was perfectly\nfine, and you could install a venv with 3.9 and then upgrade to 3.10 and it\nwould still work. It just broke installing with 3.10, and pre-commit.ci and\nbrew were updating to 3.10, breaking CI. This took hours of my time to roll\nback across half a dozen repos, it caused people trusting my style\nrecommendations to also be affected, all for an untested version cap - Python\n3.10 didn\u2019t break the application at all.\n\nAdding a range with a limit for your lockfile would be completely fine. But\nsystems like Poetry and PDM currently don\u2019t differentiate between the\nRequires-Python metadata slot and the lockfile range! Adding the limit to\nRequires-Python forces others to do it and breaks users locking ability.\n\n## Applications are slightly different\n\nNow if you have a true application (that is, if you are not intending your\npackage to be used as a library), upper version constraints are much less\nproblematic. You notice not all the reasons above apply for applications. This\ndue to two reasons.\n\nFirst, if you are writing a library, your \u201cusers\u201d are specifying your package\nin their dependencies; if an update breaks them, they can always add the\nnecessary exclusion or cap for you to help end users - it\u2019s a leaky\nabstraction, they shouldn\u2019t have to care about what your dependencies are, but\nwhen capping interferes with what they can use, that\u2019s also a leaky and\nunfixable abstraction. For an application, the \u201cusers\u201d are more likely to be\ninstalling your package directly, where the users are generally other\ndevelopers adding to requirements for libraries.\n\nSecond, for an app that is installed from PyPI, you are less likely to have to\nworry about what else is installed (the other issues are still true). Many\n(most?) users will not be using pipx or a fresh virtual environment each time,\nso in practice, you\u2019ll still run into problems with tight constraints, but\nthere is a workaround (use pipx, for example). You still are still affected by\nmost of the arguments above, though, so personally I\u2019d still not recommend\nadding untested caps.\n\nYou should never depend only on SemVer for a deployed application, like a\nwebsite. I won\u2019t repeat the SemVer article verbatim here, but in general, you\nare roughly as likely to get a breakage (usually unintentional) from a minor\nor patch release of a library than from a major version. Depending on the\nstability and quality of the library, often more likely. So applications only\nhave one choice: They should supply a lock file that has every dependency\nexplicitly listed. All systems have the ability to do this - you can use pip-\ntools for pip, Poetry, pdm, and pipenv make lock files automatically, PEP 665\neven proposes a standard lock file format, etc. This gives users a way to\ninstall using exactly the known working dependencies. In production (say for a\nwebsite), you must do this. Otherwise, you will randomly break. This is why\npatch releases exist, it\u2019s because a major, minor, or even other patch release\nbroke something!\n\nIf you are not using Poetry or pip-tools, you can still make a simple lock\nfile with:\n\n    \n    \n    pip freeze > requirements-lock.txt\n\nThen you can install it with:\n\n    \n    \n    pip install --no-deps -r requirements-lock.txt\n\nWhile this does not include hashes like Poetry, pipenv, or pip-tools will, it\ncovers many low-risk use cases, like setting up a simple web application. By\nthe way, since I\u2019ve been harsh on Poetry, I should point out it really shines\nhere for this use.\n\nWhat about your general requirements that control the locking process? With a\nlockfile, you\u2019ll know when you try to update it that something breaks, and\nthen you can add a (temporary) pin for that dependency. Adding arbitrary pins\nwill reduce your ability to update your lock file with the latest\ndependencies, and obscure what actually is not supported with what is\narbitrarily pinned.\n\n# Python is not JavaScript\n\nPoetry gets a lot of inspiration from npm for JavaScript, including the ^\noperator syntax (meaning newer minor/patch releases are okay but not major\nones). If you are coming from a language like JavaScript, you might be tempted\nto use upper pins since you are used to seeing them there. But there are two\nbig differences between Python packaging and JavaScript.\n\n## Technical difference\n\n    \n    \n    packages (flat) | packages (nested) - package_1 | - package_1 - package_2 | - shared_dep==1.0 - shared_dep | - package_2 | - shared_dep==2.0\n\nExample of a flat system (like Python) and a nested system (like JavaScript).\nThe requirements of package_1 and package_2 must be compatible in the flat\nsystem.\n\nThe technical difference is that npm (and JavaScript) has the idea of local\ndependencies. If you have several packages that request the same dependency,\nthey each get a copy of that dependency. They are free to have conflicting\nversion requirements; each gets a copy so the copies could be different\nversions. That invalidates several of the arguments above. It also is much\nharder to add a pin as a user, because you have to add a nested pin (you can\nuse Yarn or you can manually edit your lock file). Poetry does not implement\nthis model, by the way - it still is a traditional Python system with fully\nshared dependencies.\n\nThis does not solve all problems, by the way. It just keeps them from randomly\nconflicting by keeping them localized - JavaScript libraries act much more\nlike applications under my definitions. It also has the idea of peer\ndependencies (for plugins), which have all the conflict issues listed so far.\nIn fact, here\u2019s a quote from nodejs.org about peer dependencies:\n\n> One piece of advice: peer dependency requirements, unlike those for regular\n> dependencies, should be lenient.\n\nIn Python, all dependencies are \u201cpeer dependencies\u201d!\n\n## Social difference\n\nThe social difference (which stems from the technical difference) is that\nPython libraries (and Python itself) do not like to do hard, backward\nincompatible changes without warnings. This is more accepted in languages with\nlocal packages, but a stable Python package that breaks backward compatibility\nwithout any sort of warning is likely to be avoided if it happens too often.\nGenerally there is a set deprecation period. This was enforced by the Python 3\ntransition; Python itself and major libraries have promised to never do that\nhard of a break again.\n\nThe other factor possibly responsible for the social difference is that Python\nlibraries often have a small number of maintainers, often with split\npriorities. This means they cannot devote resources to keeping up multiple\nmajor versions of software - usually only the latest version is supported.\nThis means you are expected to use the latest major and minor version to be\nsupported with security and compatibility fixes; but you can\u2019t do this if any\nof your dependencies force a cap. (Given the number of vulnerabilities\nreported above, I don\u2019t think JavaScript library maintainers are releasing\nmany new patch releases for old major versions either.)\n\nI have seen a +3 rule for capping proposed by NumPy. If your code works\nwithout warnings on the current NumPy version 1.x, you should cap to 1.(x+3).\nThis is better; you can test development versions, you can avoid version bound\nclashes for a while, but still unnecessary for most code - you are very\nunlikely to be using something in NumPy that is about to be removed. It\u2019s\nbetter to simply acknowledge that if you use a library years after it\nreleases, you might need to manually control some of the versions. If you\nvanish of the face of the internet, then an uncapped library will be more\ncompatible than a wrongly capped one, and if you are still around, you\u2019ll keep\nit up to date. If you heavily use NumPy, though, this could be an acceptable\ncap. It\u2019s also much less of a problem if you have a large library with\nmultiple maintainers and regular releases.\n\nAlso, my focus here has been on smaller libraries; if you have a dozen\ndevelopers working on the project, I\u2019m assuming you already have a reasonable\n(anti) capping policy (though, to be fair, TensorFlow made this mistake for a\nwhile). A large library that has regular updates would probably be quite\nreasonable on this +3 rule.\n\n## Watch for warnings\n\nSo Python and its ecosystem does not have an assumption of strict SemVer, and\nhas a tradition of providing deprecation warnings. If you have good CI, you\nshould be able to catch warnings even before your users see them. Try the\nfollowing pytest configuration:\n\n    \n    \n    [tool.pytest.ini_options] filterwarnings = [\"error\"]\n\nThis will turn warnings into errors and allow your CI to break before users\nbreak. You can ignore specific warnings as well; see the pytest docs or\nscikit-hep\u2019s developer pytest pages.\n\n## Analysis of a JavaScript project\n\nI have been maintaining several open source gitbook projects and gitbook went\nclosed-source years ago. I decided to do an analysis on the lock file for\nModern CMake.^6\n\nFor the project, there are either 2 or 7 user level packages (gitbook and\nsvgexport, as well as five gitbook plugins). This installs 576 packages, which\nwould be flattened to 315 unique packages if you ignored version pinning, or\n426 packages if you include the version with the package - yes, that\u2019s over\n100 times that a package gets installed multiple times with different\nversions. Though not all of those are conflicts (npm doesn\u2019t seem to try very\nhard to get consistent versions), but I still counted at least 30 unsolvable\nversion conflicts if this was in a flat system. This is exactly what we will\nrun into if we try to replicate version capping in a flat dependency system\nlike Python and more people start following version capping - it does not\nscale in a flat system.\n\nAlso, building this today reports 153 vulnerabilities (11 low, 47 moderate, 90\nhigh, 5 critical). And many packages are stuck on versions up to 10 major\nversions old - and this is even counting just svgexport, since gitbook is a\ndead (in terms of open source) project.\n\n# Upper limits are valid sometimes\n\n## When is it okay to set an upper limit?\n\nValid reasons to add an upper limit are:\n\n  1. If a dependency is known to be broken, block out the broken version. Try very hard to fix this problem quickly, then remove the block if it\u2019s fixable on your end. If the fix happens upstream, excluding just the broken version is fine (or they can \u201cyank\u201d the bad release to help everyone).\n  2. If you know upstream is about to make a major change that is very likely to break your usage, you can cap. But try to fix this as quickly as possible so you can remove the cap by the time they release. Possibly add development branch/release testing until this is resolved. TensorFlow 1-2, for example, was a really major change that moved things around. But fixing it was really as simple as importing from tensorflow.v1.\n  3. If upstream asks users to cap, then I still don\u2019t like it, but it is okay if you want to follow the upstream recommendation. You should ask yourself: do you want to use a library that may intentionally break you and require changes on your part without help via deprecation periods? A one-time major rewrite might be an acceptable reason. Also, if you are upstream, it is very un-Pythonic to break users without deprecation warnings first. Don\u2019t do it if possible. A good upstream (like NumPy) may ask for a future cap (NumPy asks for +3 versions for large dependent packages).\n  4. If you are writing an extension for an ecosystem/framework (pytest extension, Sphinx extension, Jupyter extension, etc), then capping on the major version of that library is acceptable. Note this happens once - you have a single library that can be capped. You must release as soon as you possibly can after a new major release, and you should be closely following upstream - probably using development releases for testing, etc. But doing this for one library is probably manageable.\n  5. You are releasing two or more libraries in sync with each other. You control the release cadence for both libraries. This is likely the \u201cbest\u201d reason to cap. Some of the above issues don\u2019t apply in this case - since you control the release cadence and can keep them in sync.\n  6. You depend on private internal details of a library. You should also rethink your choices - this can be broken in a minor or patch release, and often is (pyparsing 3.0.5, for example).\n\nIf you cap in these situations, I wouldn\u2019t complain, but I wouldn\u2019t really\nrecommend it either:\n\n  7. If you have a heavy dependency on a library, maybe cap. A really large API surface is more likely to be hit by the possible breakage.\n  8. If a library is very new, say on version 1 or a ZeroVer library, and has very few users, maybe cap if it seems rather unstable. See if the library authors recommend capping (reason 3 above) - they might plan to make a large change if it\u2019s early in development. This is not blanket permission to cap ZeroVer libraries!\n  9. If a library looks really unstable, such as having a history of making big changes, then cap. Or use a different library. Even better, contact the authors, and make sure that your usage is safe for the near future.\n\nAll these are special cases, and are uncommon; no more than 1-2 of your\ndependencies should fall into the categories above. In every other case, do\nnot cap your dependences, expecially if you are writing a library! You could\nprobably summarize it like this: if there\u2019s a high chance (say 75%+) that a\ndependency will break for you when it updates, you can add a cap. But if\nthere\u2019s no reason to believe it will break, do not add the cap; you will cause\nmore severe (unfixable) pain than the breakage would.\n\nIf you have an app instead of a library, you can be cautiously slightly\nstricter, but not much. Apps do not have to live in shared environments,\nthough they might.\n\nNotice many of the above instances are due to very close/special interaction\nwith a small number of libraries (either a plugin for a framework,\nsynchronized releases, or very heavy usage). Most libraries you use do not\nfall into this category. Remember, library authors don\u2019t want to break users\nwho follow their public API and documentation. If they do, it\u2019s for a special\nand good reason (or it is a bad library to depend on). They will probably have\na deprecation period, produce warnings, etc.\n\nIf you do version cap anything, you are promising to closely follow that\ndependency, update the cap as soon as possible, follow beta or RC releases or\nthe development branch, etc. When a new version of a library comes out, end\nusers should be able to start trying it out. If they can\u2019t, your library\u2019s\ndependencies are a leaky abstraction (users shouldn\u2019t have to care about what\ndependencies libraries use).\n\n#### What about build time requirements?\n\nThis are always used in a clean virtual environment, so are very tempting to\ncap or even pin. But, at the same time, capping these can also break if you\ntry to use new systems, new Pythons, etc. So as always, ask yourself: How\nlikely is this to break? You can use --no-build-isolation to turn this off\n(you also need --no-dependency-check if things are capped or pinned when using\nbuild). Some cases are special - numpy should never be directly present (if\ncompiling against the C-API at least), always use oldest-supported-numpy\ninstead (uncapped/unpinned).\n\nYou can limit build time dependencies with PIP_CONSTRAINT, like this:\n\n    \n    \n    echo \"cython<3\" > constraint.txt export PIP_CONSTRAINT=$PWD/normalize_keys # Now pip install, build, etc.\n\n## Rapid updates can hide the problem\n\nIf a library author is very quick at updating their library when new releases\ncome out (like rich), upper capping doesn\u2019t cause immediate issues (though it\ndoes still interfere with testing development versions). However, as soon as\nthose rapid updates stop, the library starts to decay much faster than a\nlibrary without upper caps. The dependencies cannot \u201cfix\u201d this by releasing a\nnew version with a backport of whatever they removed/changed because they\ndidn\u2019t realise someone was using it, either, because they are capped. This\nleads into the next reason to use caps.\n\n## Planned obsolescence\n\nThere\u2019s one more reason to add upper version caps that I did not include\nabove. If you plan to eventually change the licence of your library and/or\nmake it closed source, then adding upper version caps will ensure that the\nold, open source versions of your library will become obsolete quickly. Using\nupper version caps forces your users to depend on you for frequent version\nupdates. If you stop and move to a different model, your code quickly becomes\nuninstallable with the latest security updates, on newer Python versions, on\nnewer hardware or OSs, or with newer libraries. This often is transitive;\nthere\u2019s a limit on package X and package X adds support for Musllinux, but you\ncan\u2019t access it because of the upper limit.\n\nThis might happen eventually if you don\u2019t limit, but it will happen much\nfaster and with more assurance with hard limits. Remember, caps can\u2019t be fixed\nby users.\n\n## Examples of acceptable caps\n\nNumba pins LLVMLight exactly, and puts a hard cap on Python (and recently,\ntemporarily NumPy too). They control both Numba and LLVMLight, so the pinning\nthere is okay (reason 5 above).^7 They use Python bytecode to decompile Python\nfunctions; this is an internal detail to Python, so every minor release is\nallowed to (does) change bytecode, so Numba must support each version manually\n(reason 6 above). They know the most recent version of NumPy is incompatible\n(reason 1 above). In both cases, they also put a check in setup.py, but\nremember, that only affects building Numba, so that works for Python, but may\nnot work for normal dependencies like NumPy since normally users install\nwheels, not SDists, so setup.py does not run. Numba should (and will) release\nthis pin as quickly as possible, because there are quite a few reasons to use\nNumPy 1.21, including it being the first NumPy to support Python 3.10.\n\nI personally limit hist to the minor release of boost-histogram. I control\nboth packages, and release them in sync; a hist release always follows a new\nminor release of boost-histogram. They are tightly coupled, but part of a\nfamily (reason 1 above). At this point, boost-histogram is likely stable\nenough even in internal details to ease up a bit, but this way I can also\nadvertise the new boost-histogram features as new hist features. ;)\n\nMany packages follow Flit\u2019s recommendation and use requires = [\"flit_core\n>=3.2,<4\"] in the pyproject.toml build specification. This is reason 3 above;\nFlit asks you to do this. It\u2019s also in the pyproject.toml, which by definition\nwill never be \u201cshared\u201d with anything, it\u2019s a new, disposable virtual\nenvironment that is created when building a wheel, and then thrown away,\nmaking it much more like an application requirement. However, if you only use\nthe PEP 621 configuration for Flit, I see no reason to cap it; this is a\npublished standard and isn\u2019t going to change, so Flit 4 will not \u201cbreak\u201d usage\nunless a bug is introduced. And Flit actually now reflects this in the version\nlimit recommendation!\n\n## Examples of bad caps\n\n### TensorFlow\n\nNow let\u2019s look at a bad upper limit and the mess it caused. TensorFlow used to\nput an upper cap on everything (note some of the comments there are wrong, for\nexample, order does not matter to the solve). This was a complete mess.\nSeveral of the dependencies here are small little libraries that are not going\nto break anyone on updates, like wrapt and six. Probably the worst of all\nthough is typing_extensions. This is a backport module for the standard\nlibrary typing module, and it\u2019s pinned to 3.7.x. First, new versions of\ntyping_extensions are not going to remove anything at least for five years,\nand maybe not ever - this is a compatibility backport (the stdlib typing might\nbe cleaned up after 5 years). Second, since this is a backport, setting a high\nlower bound on this is very, very common - if you want to use Python 3.10\nfeatures, you have to set a higher lower bound. Black, for example, sets\n3.10.0 as the minimum. This is completely valid, IMO - if you have a backport\npackage, and you want the backports from Python 3.10, you should be able to\nget them. Okay, so let\u2019s say you run this:\n\n    \n    \n    python3 -m venv .venv ./.venv/bin/pip install black tensorflow\n\n(or pretend that\u2019s in a requirements.txt file for a project, etc - however\nyou\u2019d like to think of that). First, the resolver will download black 21.8b0.\nThen it will start downloading TensorFlow wheels, working it\u2019s way back\nseveral versions - if you are on a limited bandwidth connection, be warned\neach one is several hundred MB, this is multiple GB to do. Eventually it will\ngive up, and start trying older black versions. It will finally find a set\nthat\u2019s compatible, since older black versions don\u2019t have the high pin, and\nwill install that. Now try this:\n\n    \n    \n    python3 -m venv .venv ./.venv/bin/pip install black ./.venv/bin/pip tensorflow\n\nThis will force typing-extensions to be rolled back, and then will be broken\nwith:\n\n    \n    \n    ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. black 21.8b0 requires typing-extensions>=3.10.0.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n\nIn other words, simply having black pre-installed will keep you from\ninstalling TensorFlow, even though they are completely unrelated. The reason?\nTensorFlow thinks it has to have typing_extensions 3.7 instead of 3.10, which\nis wrong. This is literally a standard library backport package (for typing)!\nWith such strong pinning, TensorFlow was effectively an application, it could\nnot play nicely with pretty much any other library.\n\nDue to the problems this caused, Tensorflow has removed the upper caps on most\ndependencies, and you can now install it again with other libraries.\n\n### Packaging and PyParsing\n\nPackaging is a foundational library for most of Python packaging. Everybody\neither depends on it (tox, cibuildwheel, pdm, etc) or vendors it (pip, Poetry,\npipenv). Packaging has very few dependencies, but it does require pyparsing.\nIn version 3, pyparsing changed the name of some tokens - but provided\nbackward compatible names. Packaging worked just fine with version 3, but it\naffected the text of one error message that was being compared in the tests,\nso packaging capped pyparsing to <3, and then released packaging 21.2 with no\nother change (compared to 21.1) except this cap. This immediately started\nbreaking things (like Google App Engine deployment, and other complaints\nstating \u201cton of dependency conflicts\u201d). To be clear, it didn\u2019t solve anything\nexcept one test inside packaging itself. Then pyparsing 3.0.5 was released\nwith a change to in internal method name (starting with an underscore). This\nwas used by packaging (bad), so the real limit was !=3.0.5 (pyparsing was nice\nand restored this for 3.0.6, though they could have said it was packaging\u2019s\nfault - which it was). The correct fix is to not use a private implementation\ndetail, which packaging fixed, but old versions still exist.\n\n# TL;DR\n\nCapping dependencies has long term negative effects, especially for libraries,\nand should never be taken lightly. A library is not installed in isolation; it\nhas to live with other libraries in a shared environment. Only add a cap if a\ndependency is known to be incompatible or there is a high (>75%) chance of it\nbeing incompatible in its next release. Do not cap by default - capping\ndependencies makes your software incompatible with other libraries that also\nhave strict lower limits on dependencies, and limits future fixes. Anyone can\nfix a missing cap, but users cannot fix an over restrictive cap causing solver\nerrors. It also encourages hiding issues until they become harder to fix, it\ndoes not scale to larger systems, it limits your ability to access security\nand bugfix updates, and some tools (Poetry) force these bad decisions on your\ndownstream users if you make them. Never cap Python, it is fundamentally\nbroken at the moment. Also, even packing capping has negative consequences\nthat can produce unexpected solves.\n\nEven perfect SemVer does not promise your usage will be broken, and no library\ncan actually perfectly follow SemVer anyway; minor versions and even patch\nversions are often more likely to break you than major versions for a well\ndesigned, stable library. You must learn to use a locking package system if\nyou need application reliability - SemVer capping is not a substitute. Python\nhas a culture of using deprecation warnings and slow transitions, unlike an\necosystem with a nested dependency system like npm. We saw a realistic NPM\nproject has 30+ version conflicts if it was to be flattened like Python -\nversion capping does not scale when dependencies are shared. Provide an\noptional working set of fully pinned constraints if that\u2019s important to you\nfor applications - this is the only way to ensure a long term working set of\ndependencies (including for npm).\n\nIf you absolutely must set upper limits, you should release a new version as\nsoon as possible with a higher cap when a dependency updates (ideally before\nthe dependency releases the update). If you are committing to this, why not\njust quickly release a patch release with caps only after an actual conflict\nhappens? It will be less common, and will help you quickly sort out and fix\nincompatibilities, rather than hiding your true compatibilities and delaying\nupdates. You want users to use the latest versions of your libraries if\nthere\u2019s a problem, so why can\u2019t you offer the same consideration to the\nlibraries you depend on and use?\n\nIf you need a TL;DR for the TL;DR, I\u2019ll just quote Python Steering Council\nMember and packaging expert Brett Cannon:\n\n> Libraries/packages should be setting a floor, and if necessary excluding\n> known buggy versions, but otherwise don\u2019t cap the maximum version as you\n> can\u2019t predict future compatibility\n\nAlso, this is not generalizable to systems that are able to provide unique\nversions to each package - like Node.js. These systems can avoid resolver\nconflicts by providing different versions locally for each package; and this\ncreates different social expectations about acceptable changes and about LTS\nsupport for major versions. This is very, very different from a system that\nalways solves for a shared single version. Those systems are also where the ^\nsyntax is much more useful. Some tools (like Poetry) seem to be trying to\napply part of those systems (caret syntax, for example) without applying the\nlocal version feature, which is key to how they work. Having local (per\ndependency) copies of all dependencies solves many of the issues above and\npractically turns libraries into applications, though some of the arguments\nabove still apply, like hiding incompatibilities until the changeset is very\nlarge.\n\n# Acknowledgements\n\nThanks to Python steering council member Brett Cannon, Python core developer\nPaul Ganssle, fellow PyPA members Bern\u00e1t G\u00e1bor, Pradyun Gedam and @layday,\nfellow RSE Troy Comi, and fellow IRIS-HEP member Alex Held for their comments\non early drafts. Also I\u2019d like to acknowledge the excellent article Why you\nshouldn\u2019t invoke setup.py directly from Paul Ganssle for convincing me that a\nProustian monstrosity of a post can be useful. All typos and mistakes are my\nown.\n\n  1. Poetry is prioritizing the truthfulness of the lock file here. If you make a lockfile (and Poetry always does) and a dependency pins python<3.10, then that lockfile will not load on Python 3.10. This is understandable, but there\u2019s no way to set the Requires-Python metadata slot other than with this setting! If you are developing a library, you should not be forced to do this because of a lock file which is not even in the distribution. I\u2019d rather a warning + a correct Python range only in the lockfile, or a way to set them separately, like with PEP 621 metadata support combined with the old specification. \u21a9\ufe0e\n\n  2. Fun fact: one shortcut includes checking to see if the latest version of everything is valid. This immediately is broken if there\u2019s an upper cap that affects the solve. \u21a9\ufe0e\n\n  3. One common pushback here is that a smart dependency solver will get old versions, so updating is not pressing. But new libraries shouldn\u2019t have to support really old versions of things just because they can\u2019t live with libraries with old caps. Libraries shouldn\u2019t have to keep pushing updates to old major/minor releases to support new hardware and Python versions, etc. So yes, you are \u201cpromising\u201d to update rapidly if capped dependencies update. Otherwise, your library cannot be depended on. \u21a9\ufe0e\n\n  4. I\u2019m obviously making assumptions about you, my reader, here. But I rather expect I am right. If not, I\u2019d like you to join all my projects and start releasing old backports for all major versions for me. ;) \u21a9\ufe0e\n\n  5. In pip or Poetry. Conda can do this, because Python is more like a library there. But we aren\u2019t discussing conda, and at least conda-forge has it\u2019s own system, and it\u2019s not tied to your normal packaging config at all, the package names may not even be the same, etc. \u21a9\ufe0e\n\n  6. This was done to provide this argument, not just to play with Python 3.10 pattern matching because I always work on libraries and don\u2019t get to play with all the new toys... \u21a9\ufe0e\n\n  7. Though, as a maintainer for the conda-forge Numba package, I have to say it is does make the update slower given there\u2019s no wiggle room at all, and for some reason Numba seems to release before the matching llvmlite is available on PyPI. \u21a9\ufe0e\n\nprogramming python\n\n  * \u2190 Previous Post\n  * Next Post \u2192\n\nHenry Schreiner \u2022 \u00a9 2024 \u2022 ISciNumPy.dev\n\nHugo v0.119.0 powered \u2022 Theme Beautiful Hugo adapted from Beautiful Jekyll\n\n", "frontpage": false}
