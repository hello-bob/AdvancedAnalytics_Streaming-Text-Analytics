{"aid": "40049579", "title": "Notes on how to use LLMs in your product", "url": "https://lethain.com/mental-model-for-how-to-use-llms-in-products/", "domain": "lethain.com", "votes": 1, "user": "kiyanwang", "posted_at": "2024-04-16 08:23:36", "comments": 0, "source_title": "Notes on how to use LLMs in your product.", "source_text": "Notes on how to use LLMs in your product. | Irrational Exuberance\n\n# Notes on how to use LLMs in your product.\n\nPublished on April 8, 2024. llm (3), product (6)\n\nPretty much every company I know is looking for a way to benefit from Large\nLanguage Models. Even if their executives don\u2019t see much applicability, their\ninvestors likely do, so they\u2019re staring at the blank page nervously trying to\ncome up with an idea. It\u2019s straightforward to make an argument for LLMs\nimproving internal efficiency somehow, but it\u2019s much harder to describe a\nbelievable way that LLMs will make your product more useful to your customers.\n\nI\u2019ve been working fairly directly on meaningful applicability of LLMs to\nexisting products for the last year, and wanted to type up some semi-\ndisorganized notes. These notes are in no particular order, with an intended\naudience of industry folks building products.\n\n## Rebuild your mental model\n\nMany folks in the industry are still building their mental model for LLMs,\nwhich leads to many reasoning errors about what LLMs can do and how we should\nuse them. Two unhelpful mental models I see many folks have regarding LLMs\nare:\n\n  1. LLMs are magic: anything that a human can do, an LLM can probably do roughly as well and vastly faster\n  2. LLMs are the same as reinforcement learning: current issues with hallucinations and accuracy are caused by small datasets. Accuracy problems will be solved with larger training sets, and we can rely on confidence scores to reduce the impact of inaccuracies\n\nThese are both wrong in different but important ways. To avoid falling into\nthose mental model\u2019s fallacies, I\u2019d instead suggest these pillars for a useful\nmental model around LLMs:\n\n  1. LLMs can predict reasonable responses to any prompt \u2013 an LLM will confidently provide a response to any textual prompt you write, and will increasingly provide a response to text plus other forms of media like image or video\n  2. You cannot know whether a given response is accurate \u2013 LLMs generate unexpected results, called hallucinations, and you cannot concretely know when they are wrong. There are no confidence scores generated that help you reason about a specific answer from an LLM\n  3. You can estimate accuracy for a model and a given set of prompts using evals \u2013 You can use evals \u2013 running an LLM against a known set of prompts, recording the responses, and evaluating those responses \u2013 to evaluate the likelihood that an LLM will perform well in a given scenario\n  4. You can generally increase accuracy by using a larger model, but it\u2019ll cost more and have higher latency \u2013 for example, GPT 4 is a larger model than GPT 3.5, and generally provides higher quality responses. However, it\u2019s meaningfully more expensive (~20x more expensive), and meaningfully slower (2-5x slower). However, the quality, cost and latency are improving at every price point. You should expect the year-over-year performance at a given cost, latency or quality point to meaningfully improve over the next five years (e.g. you should expect to get GPT 4 quality at the price and latency of GPT 3.5 in 12-24 months)\n  5. Models generally get more accurate as the corpus it\u2019s built from grows in size \u2013 the accuracy of reinforcement learning tends to grow predictability as the dataset grows. That remains generally true for LLMs, but is less predictable. Small models generally underperform large models. Large models generally outperform small models with higher quality data. Supplementing large general models with specific data is called \u201cfine-tuning\u201d and it\u2019s currently ambiguous when fine-tuning a smaller model will outperform using a larger model. All you can really do is run evals based on the available models and fine-tuning datasets for your specific usecase\n  6. Even the fastest LLMs are not that fast \u2013 even a fast LLM might take 10+ seconds to provide a reasonably sized response. If you need to perform multiple iterations to refine the initial response, or to use a larger model, it might take a minute or two to complete. These will get faster, but they aren\u2019t fast today\n  7. Even the most expensive LLMs are not that expensive for B2B usage. Even the cheapest LLM is not that cheap for Consumer usage \u2013 because pricing is driven by usage volume, this is a technology that\u2019s very easy to justify for B2B businesses with smaller, paying usage. Conversely, it\u2019s very challenging to figure out how you\u2019re going to pay for significant LLM usage in a Consumer business without the risk of significantly shrinking your margin\n\nThese aren\u2019t perfect, but hopefully they provide a good foundation for\nreasoning about what will or won\u2019t work when it comes to applying LLMs to your\nproduct. With this foundation in place, now it\u2019s time to dig into some more\nspecific subtopics.\n\n## Revamp workflows\n\nThe workflows in most modern software are not designed to maximize benefit\nfrom LLMs. This is hardly surprising\u2013they were built before LLMs became\ncommon\u2013but it does require some rethinking about workflow design.\n\nTo illustrate this point, let\u2019s think of software for a mortgage provider:\n\n  1. User creates an account\n  2. Product asks user to fill in a bunch of data to understand the sort of mortgage user wants and user\u2019s eligibility for such a mortgage\n  3. Product asks user to provide paperwork to support the data user just provided, perhaps some recent paychecks, bank account balances, and so on\n  4. Internal team validates the user\u2019s data against the user\u2019s paperwork\n\nIn that workflow, LLMs can still provide significant value to the business, as\nyou could increase efficiency of validating the paperwork matching with the\nuser supplied information, but the user themselves won\u2019t see much benefit\nother than perhaps faster validation of their application.\n\nHowever, you can adjust the workflows to make them more valuable:\n\n  1. User creates an account\n  2. Product asks user to provide paperwork\n  3. Product uses LLM to extract values from paperwork\n  4. User validates the extracted data is correct, providing some adjustments\n  5. Internal team reviews the user\u2019s adjustments, along with any high risk issues raised by a rule engine of some sort\n\nThe technical complexity of these two products is functionally equivalent, but\nthe user experience is radically different. The internal team experience is\nimproved as well. My belief is that many existing products will find they can\nonly significantly benefit their user experience from LLMs by rethinking their\nworkflows.\n\n## Retrieval Augmented Generation (RAG)\n\nModels have a maximum \u201ctoken window\u201d of text that they\u2019ll consider in a given\nprompt. The maximum size of token windows are expanding rapidly, but larger\ntoken windows are slower to evaluate and cost more to evaluate, so even the\nexpanding token windows don\u2019t solve the entire problem.\n\nOne solution to navigate large datasets within a fixed token window is\nRetrieval Augmented Generation (RAG). To come up with a concrete example, you\nmight want to create a dating app that matches individuals based on their\nfree-form answer to the question, \u201cWhat is your relationship with books, tv\nshows, movies and music, and how has it changed over time?\u201d No token window is\nlarge enough to include every user\u2019s response from the dating app\u2019s database\ninto the LLM prompt, but you could find twenty plausible matching users by\nfiltering on location, and then include those twenty users\u2019 free-form answers,\nand match amongst them.\n\nThis makes a lot of sense, and the two phase combination of an unsophisticated\nalgorithm to get plausible components of a response along with an LLM to\nfilter through and package the plausible responses into an actual response\nworks pretty well.\n\nWhere I see folks get into trouble is trying to treat RAG as a solution to a\nsearch problem, as opposed to recognizing that RAG requires useful search as\npart of its implementation. An effective approach to RAG depends on a high-\nquality retrieval and filtering mechanism to work well at a non-trivial scale.\nFor example, with a high-level view of RAG, some folks might think they can\nreplace their search technology (e.g. Elasticsearch) with RAG, but that\u2019s only\ntrue if your dataset is very small and you can tolerate much higher response\nlatencies.\n\nThe challenge, from my perspective, is that most corner-cutting solutions look\nlike they\u2019re working on small datasets while letting you pretend that things\nlike search relevance don\u2019t matter, while in reality relevance significantly\nimpacts quality of responses when you move beyond prototyping (whether they\u2019re\nliterally search relevance or are better tuned SQL queries to retrieve more\nappropriate rows). This creates a false expectation of how the prototype will\ntranslate into a production capability, with all the predictable consequences:\nunderestimating timelines, poor production behavior/performance, etc.\n\n## Rate of innovation\n\nModel performance, essentially the quality of response for a given budget in\neither dollars or milliseconds, is going to continue to improve, but it\u2019s not\ngoing to continue improving at this rate absent significant technology\nbreakthroughs in the creation or processing of LLMs. I\u2019d expect those\nbreakthroughs to happen, but to happen less frequently after the first several\nyears, and slow from there. It\u2019s hard to determine where we are in that cycle\nbecause there\u2019s still an extraordinary amount of capital flowing into this\nspace.\n\nIn addition to technical breakthroughs, the other aspect driving innovation is\nbuilding increasingly large models. It\u2019s unclear if today\u2019s limiting factor\nfor model size is availability of Nvidia GPUs, larger datasets to train models\nupon that are plausibly legal, capital to train new models, or financial\nmodels suggesting that the discounted future cashflow from training larger\nmodels doesn\u2019t meet a reasonable payback period. My assumption is that all of\nthese have or will be the limiting constraint on LLM innovation over time, and\nvarious competitors will be best suited to make progress depending on which\nconstraint is most relevant. (Lots of fascinating albeit fringe scenarios to\ncontemplate here, e.g. imagine a scenario where the US government disbands\ncopyright laws to allow training on larger datasets because it fears losing\nthe LLM training race to countries that don\u2019t respect US copyright laws.)\n\nIt\u2019s safe to assume model performance will continue to improve. It\u2019s likely\ntrue that performance will significantly improve over the next several years.\nI find it relatively unlikely to assume that we\u2019ll see a Moore\u2019s Law scenario\nwhere LLMs continue to radically improve for several decades, but lots of\nthings could easily prove me wrong. For example, at some point nuclear fusion\nis going to become mainstream and radically change how we think about energy\nutilization in ways that will truly rewrite the world\u2019s structure, and LLM\ntraining costs could be one part of that.\n\n## Human-in-the-Loop (HITL)\n\nBecause you cannot rely on LLMs to provide correct responses, and you cannot\ngenerate a confidence score for any given response, you have to either accept\npotential inaccuracies (which makes sense in many cases, humans are wrong\nsometimes too) or keep a Human-in-the-Loop (HITL) to validate the response.\n\nAs discussed in the workflow section, many companies already have humans\nperforming validation work who can now move into supervision of LLM responses\nrather than generating the responses themselves. In other scenarios, it\u2019s\npossible to adjust your product\u2019s workflows to rely on external users to serve\nas the HITL instead. I suspect most products will depend on both techniques\nalong with heuristics to determine when internal review is necessary.\n\n## Hallucinations and legal liability\n\nAs mentioned before, LLMs often generate confidently wrong responses. HITL is\nthe design principle to prevent acting on confidently wrong responses. This is\nbecause it shifts responsibility (specifically, legal liability) away from the\nLLM itself and to the specific human. For example, if you use Github Copilot\nto generate some code that causes a security breach, you are responsible for\nthat security breach, not Github Copilot. Every large-scale adoption of LLMs\ntoday is being done in a mode where it shifts responsibility for the responses\nto a participating human.\n\nMany early-stage entrepreneurs are dreaming of a world with a very different\nloop where LLMs are relied upon without a HITL, but I think that will only be\ntrue for scenarios where it\u2019s possible to shift legal liability (e.g. Github\nCopilot example) or there\u2019s no legal liability to begin with (e.g. generating\na funny poem based on their profile picture).\n\n## \u201cZero to one\u201d versus \u201cOne to N\u201d\n\nThere\u2019s a strong desire for a world where LLMs replace software engineers, or\nwhere software engineers move into a supervisory role rather than writing\nsoftware. For example, an entrepreneur wants to build a copy of Reddit, and\nuses an LLM to implement that implementation. There\u2019s enough evidence that you\ncan assume it\u2019s possible today to go from zero to one on a new product idea in\na few weeks with an LLM and some debugging skills.\n\nHowever, most entrepreneurs lack a deep intuition on operating and evolving\nsoftware with a meaningful number of users. Some examples:\n\n  * Keeping users engaged after changing the UI requires active, deliberate work\n  * Ensuring user data is secure and meets various privacy compliance obligations\n  * Providing controls to meet SOC2 and providing auditable evidence of maintaining those controls\n  * Migrating a database schema with customer data in it to support a new set of columns\n  * Ratcheting down query patterns to a specific set of allowed patterns that perform effectively at higher scale\n\nAll of these are straightforward, basic components of scaling a product (e.g.\ngoing from \u201cone to N\u201d) that an LLM is simply not going to perform effectively\nat, and where I am skeptical that we\u2019ll ever see a particularly reliable LLM-\nbased replacement for skilled, human intelligence. It will be interesting to\nwatch, though, as we see how far folks try to push the boundaries of what LLM-\nbased automation can do to delay the onset of projects needing to hire\nexpertise.\n\n## Copyright law\n\nCopyright implications are very unclear today, and will remain unclear for the\nforeseeable future. All work done today using LLMs has to account for\ndivergent legal outcomes. My best guess is that we will see an era of legal\nbalkanization regarding whether LLM generated content is copyright-able, and\nlonger-term that LLMs will be viewed the same as any other basic technical\ncomponent, e.g. running a spell checker doesn\u2019t revoke your copyright on the\nspell checked document. You can make all sorts of good arguments why this\nperspective isn\u2019t fair to copyright holders whose data was trained on, but\nlong-term I just don\u2019t think any other interpretation is workable.\n\n## Data Processing Agreements\n\nOne small but fascinating reality of working with LLMs today is that many\ncustomers are sensitive to the LLM providers (OpenAI, Anthropic, etc) because\nthese providers are relatively new companies building relatively new things\nwith little legal precedent to derisk them. This means adding them to your\nData Processing Agreement (DPA) can create some friction. The most obvious way\naround that friction is relying on LLM functionality served via your existing\ncloud vendor (AWS, Azure, GCP, etc).\n\n## Provider availability\n\nI used to think this was very important, but my sense is that LLM hosting is\nalready essentially equivalent to other cloud services (e.g. you can get\nAnthropic via AWS or OpenAI via Azure), and that very few companies will\nbenefit from spending too much time worrying about LLM availability. I do\nthink that getting direct access to LLMs via cloud providers\u2013companies that\nare well-versed at scalability\u2013is likely the winning pick here as well.\n\nThere\u2019s lots of folks out there who have spent more time thinking deeply about\nLLMs than I have\u2013e.g. go read some Simon Willison\u2013but hopefully the notes here\nare useful. Curious to discuss if folks disagree with any of these\nperspectives.\n\nHi folks. I'm Will aka @lethain. If you're looking to reach out to me, here\nare ways I help. If you'd like to get a email from me, subscribe to my weekly\nnewsletter.\n\nI wrote An Elegant Puzzle, Staff Engineer, and The Engineering Executive's\nPrimer.\n\nPopular\n\n  * Useful tradeoffs are multi-dimensional.\n  * Layers of context.\n  * Writers who operate.\n  * Deciding to leave your (executive) job.\n  * Writing an engineering strategy.\n\nRecent\n\n  * Notes on how to use LLMs in your product.\n  * Ex-technology companies.\n  * Leadership requires taking some risk.\n  * Friction isn't velocity.\n  * More (self-)publishing thoughts.\n\nRelated\n\n  * Playing with Streamlit and LLMs.\n  * Poking around OpenAI.\n  * Notes on Escaping the Build Trap\n  * Valuing already-solved problems.\n  * Some delightful developer experiences in 2019.\n\n\u00a9 Will Larson 2024 Tags Newsletter RSS About\n\n", "frontpage": false}
