{"aid": "40071411", "title": "LLM Inference Acceleration Based on Hybrid Model Branch Prediction", "url": "https://www.mdpi.com/2079-9292/13/7/1376", "domain": "mdpi.com", "votes": 3, "user": "PaulHoule", "posted_at": "2024-04-17 23:52:21", "comments": 0, "source_title": "Large Language Model Inference Acceleration Based on Hybrid Model Branch Prediction", "source_text": "Electronics | Free Full-Text | Large Language Model Inference Acceleration Based on Hybrid Model Branch Prediction\n\nLoading [MathJax]/jax/output/HTML-CSS/jax.js\n\n  * Consent\n  * Details\n  * [#IABV2SETTINGS#]\n  * About\n\n## This website uses cookies\n\nWe use cookies to personalise content and ads, to provide social media\nfeatures and to analyse our traffic. We also share information about your use\nof our site with our social media, advertising and analytics partners who may\ncombine it with other information that you\u2019ve provided to them or that they\u2019ve\ncollected from your use of their services.\n\nShow details\n\n  * Necessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies.\n\n    * Cookiebot\n\n1\n\nLearn more about this provider\n\n1.gifUsed to count the number of sessions to the website, necessary for\noptimizing CMP product delivery.\n\nExpiry: SessionType: Pixel\n\n    * Crazyegg\n\n2\n\nLearn more about this provider\n\n_ce.cchStores the user's cookie consent state for the current domain\n\nExpiry: SessionType: HTTP\n\nce_successful_csp_checkDetects whether user behaviour tracking should be\nactive on the website.\n\nExpiry: PersistentType: HTML\n\n    * Google\n\n1\n\nLearn more about this provider\n\ntest_cookieUsed to check if the user's browser supports cookies.\n\nExpiry: 1 dayType: HTTP\n\n    * LinkedIn\n\n2\n\nLearn more about this provider\n\nli_gcStores the user's cookie consent state for the current domain\n\nExpiry: 180 daysType: HTTP\n\nbscookieThis cookie is used to identify the visitor through an application.\nThis allows the visitor to login to a website through their LinkedIn\napplication for example.\n\nExpiry: 1 yearType: HTTP\n\n    * commenting.mdpi.com\n\n2\n\nSESS#Preserves users states across page requests.\n\nExpiry: SessionType: HTTP\n\nXSRF-TOKENEnsures visitor browsing-security by preventing cross-site request\nforgery. This cookie is essential for the security of the website and visitor.\n\nExpiry: SessionType: HTTP\n\n    * commenting.mdpi.com consent.cookiebot.com\n\n2\n\nCookieConsent [x2]Stores the user's cookie consent state for the current\ndomain\n\nExpiry: 1 yearType: HTTP\n\n    * matomo.mdpi.com\n\n1\n\n_pk_testcookie_domainThis cookie determines whether the browser accepts\ncookies.\n\nExpiry: 1 dayType: HTTP\n\n    * mdpi.com\n\n3\n\n__cfruidThis cookie is a part of the services provided by Cloudflare -\nIncluding load-balancing, deliverance of website content and serving DNS\nconnection for website operators.\n\nExpiry: SessionType: HTTP\n\ncf_clearanceThis cookie is used to distinguish between humans and bots.\n\nExpiry: 1 yearType: HTTP\n\nMDPIPHPSESSIDPending\n\nExpiry: SessionType: HTTP\n\n    * mdpi.com mdpi.org mdpi-res.com sciprofiles.com\n\n4\n\n__cf_bm [x4]This cookie is used to distinguish between humans and bots. This\nis beneficial for the website, in order to make valid reports on the use of\ntheir website.\n\nExpiry: 1 dayType: HTTP\n\n    * www.jisc.ac.uk\n\n2\n\nAWSALBRegisters which server-cluster is serving the visitor. This is used in\ncontext with load balancing, in order to optimize user experience.\n\nExpiry: 7 daysType: HTTP\n\nAWSALBCORSRegisters which server-cluster is serving the visitor. This is used\nin context with load balancing, in order to optimize user experience.\n\nExpiry: 7 daysType: HTTP\n\n    * www.mdpi.com\n\n7\n\ncf_chl_1This cookie is a part of the services provided by Cloudflare -\nIncluding load-balancing, deliverance of website content and serving DNS\nconnection for website operators.\n\nExpiry: 1 dayType: HTTP\n\niconify0Used by the website's content management system (CMS) to determine how\nthe website's menu-tabs should be displayed.\n\nExpiry: PersistentType: HTML\n\niconify1This cookie is set to ensure proper product displays on the website.\n\nExpiry: PersistentType: HTML\n\niconify2Used by the website's content management system (CMS) to determine how\nthe website's menu-tabs should be displayed.\n\nExpiry: PersistentType: HTML\n\niconify3Determines the device used to access the website. This allows the\nwebsite to be formatted accordingly.\n\nExpiry: PersistentType: HTML\n\niconify-countUsed by the website's content management system (CMS) to\ndetermine how the website's menu-tabs should be displayed.\n\nExpiry: PersistentType: HTML\n\niconify-versionUsed by the website's content management system (CMS) to\ndetermine how the website's menu-tabs should be displayed.\n\nExpiry: PersistentType: HTML\n\n  * Preference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in.\n\n    * LinkedIn\n\n1\n\nLearn more about this provider\n\nlidcRegisters which server-cluster is serving the visitor. This is used in\ncontext with load balancing, in order to optimize user experience.\n\nExpiry: 1 dayType: HTTP\n\n    * www.mdpi.com\n\n2\n\nmdpi_layout_typeThis cookie is used to store user setting of using fixed\ndesktop layout instead of the default responsive layout\n\nExpiry: 1 yearType: HTTP\n\nsettingsThis cookie is used to determine the preferred language of the visitor\nand sets the language accordingly on the website, if possible.\n\nExpiry: PersistentType: HTML\n\n  * Statistic cookies help website owners to understand how visitors interact with websites by collecting and reporting information anonymously.\n\n    * Crazyegg\n\n8\n\nLearn more about this provider\n\n_ce.clock_dataCollects data on the user\u2019s navigation and behavior on the\nwebsite. This is used to compile statistical reports and heatmaps for the\nwebsite owner.\n\nExpiry: 1 dayType: HTTP\n\n_ce.clock_eventCollects data on the user\u2019s navigation and behavior on the\nwebsite. This is used to compile statistical reports and heatmaps for the\nwebsite owner.\n\nExpiry: 1 dayType: HTTP\n\n_ce.gtldHolds which URL should be presented to the visitor when visiting the\nsite.\n\nExpiry: SessionType: HTTP\n\n_ce.sCollects data on the user\u2019s navigation and behavior on the website. This\nis used to compile statistical reports and heatmaps for the website owner.\n\nExpiry: 1 yearType: HTTP\n\ncebsTracks the individual sessions on the website, allowing the website to\ncompile statistical data from multiple visits. This data can also be used to\ncreate leads for marketing purposes.\n\nExpiry: SessionType: HTTP\n\ncebsp_Collects data on the user\u2019s navigation and behavior on the website. This\nis used to compile statistical reports and heatmaps for the website owner.\n\nExpiry: SessionType: HTTP\n\nce_fvdCollects data on the user\u2019s navigation and behavior on the website. This\nis used to compile statistical reports and heatmaps for the website owner.\n\nExpiry: PersistentType: HTML\n\ncetabidSets a unique ID for the session. This allows the website to obtain\ndata on visitor behaviour for statistical purposes.\n\nExpiry: SessionType: HTML\n\n    * Google\n\n5\n\nLearn more about this provider\n\ncollectUsed to send data to Google Analytics about the visitor's device and\nbehavior. Tracks the visitor across devices and marketing channels.\n\nExpiry: SessionType: Pixel\n\n_gaRegisters a unique ID that is used to generate statistical data on how the\nvisitor uses the website.\n\nExpiry: 2 yearsType: HTTP\n\n_ga_#Used by Google Analytics to collect data on the number of times a user\nhas visited the website as well as dates for the first and most recent visit.\n\nExpiry: 2 yearsType: HTTP\n\n_gatUsed by Google Analytics to throttle request rate\n\nExpiry: 1 dayType: HTTP\n\n_gidRegisters a unique ID that is used to generate statistical data on how the\nvisitor uses the website.\n\nExpiry: 1 dayType: HTTP\n\n    * Hotjar\n\n5\n\nLearn more about this provider\n\nhjActiveViewportIdsThis cookie contains an ID string on the current session.\nThis contains non-personal information on what subpages the visitor enters \u2013\nthis information is used to optimize the visitor's experience.\n\nExpiry: PersistentType: HTML\n\nhjViewportIdSaves the user's screen size in order to adjust the size of images\non the website.\n\nExpiry: SessionType: HTML\n\n_hjSession_#Collects statistics on the visitor's visits to the website, such\nas the number of visits, average time spent on the website and what pages have\nbeen read.\n\nExpiry: 1 dayType: HTTP\n\n_hjSessionUser_#Collects statistics on the visitor's visits to the website,\nsuch as the number of visits, average time spent on the website and what pages\nhave been read.\n\nExpiry: 1 yearType: HTTP\n\n_hjTLDTestRegisters statistical data on users' behaviour on the website. Used\nfor internal analytics by the website operator.\n\nExpiry: SessionType: HTTP\n\n    * LinkedIn\n\n1\n\nLearn more about this provider\n\nAnalyticsSyncHistoryUsed in connection with data-synchronization with third-\nparty analysis service.\n\nExpiry: 30 daysType: HTTP\n\n    * Twitter Inc.\n\n1\n\nLearn more about this provider\n\npersonalization_idThis cookie is set by Twitter - The cookie allows the\nvisitor to share content from the website onto their Twitter profile.\n\nExpiry: 400 daysType: HTTP\n\n    * matomo.mdpi.com\n\n2\n\n_pk_id#Collects statistics on the user's visits to the website, such as the\nnumber of visits, average time spent on the website and what pages have been\nread.\n\nExpiry: 1 yearType: HTTP\n\n_pk_ses#Used by Piwik Analytics Platform to track page requests from the\nvisitor during the session.\n\nExpiry: 1 dayType: HTTP\n\n    * www.mdpi.com\n\n1\n\nsentryReplaySessionRegisters data on visitors' website-behaviour. This is used\nfor internal analysis and website optimization.\n\nExpiry: SessionType: HTML\n\n  * Marketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers.\n\n    * Meta Platforms, Inc.\n\n3\n\nLearn more about this provider\n\nlastExternalReferrerDetects how the user reached the website by registering\ntheir last URL-address.\n\nExpiry: PersistentType: HTML\n\nlastExternalReferrerTimeDetects how the user reached the website by\nregistering their last URL-address.\n\nExpiry: PersistentType: HTML\n\n_fbpUsed by Facebook to deliver a series of advertisement products such as\nreal time bidding from third party advertisers.\n\nExpiry: 3 monthsType: HTTP\n\n    * Google\n\n2\n\nLearn more about this provider\n\npagead/1p-user-list/#Tracks if the user has shown interest in specific\nproducts or events across multiple websites and detects how the user navigates\nbetween sites. This is used for measurement of advertisement efforts and\nfacilitates payment of referral-fees between websites.\n\nExpiry: SessionType: Pixel\n\ntdRegisters statistical data on users' behaviour on the website. Used for\ninternal analytics by the website operator.\n\nExpiry: SessionType: Pixel\n\n    * LinkedIn\n\n4\n\nLearn more about this provider\n\nbcookieUsed by the social networking service, LinkedIn, for tracking the use\nof embedded services.\n\nExpiry: 1 yearType: HTTP\n\nli_sugrCollects data on user behaviour and interaction in order to optimize\nthe website and make advertisement on the website more relevant.\n\nExpiry: 3 monthsType: HTTP\n\nUserMatchHistoryEnsures visitor browsing-security by preventing cross-site\nrequest forgery. This cookie is essential for the security of the website and\nvisitor.\n\nExpiry: 30 daysType: HTTP\n\nli_adsIdCollects data on user behaviour and interaction in order to optimize\nthe website and make advertisement on the website more relevant.\n\nExpiry: PersistentType: HTML\n\n    * Twitter Inc.\n\n3\n\nLearn more about this provider\n\ni/adsct [x2]The cookie is used by Twitter.com in order to determine the number\nof visitors accessing the website through Twitter advertisement content.\n\nExpiry: SessionType: Pixel\n\nmuc_adsCollects data on user behaviour and interaction in order to optimize\nthe website and make advertisement on the website more relevant.\n\nExpiry: 400 daysType: HTTP\n\n    * YouTube\n\n22\n\nLearn more about this provider\n\n#-#Pending\n\nExpiry: SessionType: HTML\n\niU5q-!O9@$Registers a unique ID to keep statistics of what videos from YouTube\nthe user has seen.\n\nExpiry: SessionType: HTML\n\nLAST_RESULT_ENTRY_KEYUsed to track user\u2019s interaction with embedded content.\n\nExpiry: SessionType: HTTP\n\nLogsDatabaseV2:V#||LogsRequestsStorePending\n\nExpiry: PersistentType: IDB\n\nnextIdUsed to track user\u2019s interaction with embedded content.\n\nExpiry: SessionType: HTTP\n\nremote_sidNecessary for the implementation and functionality of YouTube video-\ncontent on the website.\n\nExpiry: SessionType: HTTP\n\nrequestsUsed to track user\u2019s interaction with embedded content.\n\nExpiry: SessionType: HTTP\n\nServiceWorkerLogsDatabase#SWHealthLogNecessary for the implementation and\nfunctionality of YouTube video-content on the website.\n\nExpiry: PersistentType: IDB\n\nTESTCOOKIESENABLEDUsed to track user\u2019s interaction with embedded content.\n\nExpiry: 1 dayType: HTTP\n\nVISITOR_INFO1_LIVETries to estimate the users' bandwidth on pages with\nintegrated YouTube videos.\n\nExpiry: 180 daysType: HTTP\n\nVISITOR_PRIVACY_METADATAStores the user's cookie consent state for the current\ndomain\n\nExpiry: 180 daysType: HTTP\n\nYSCRegisters a unique ID to keep statistics of what videos from YouTube the\nuser has seen.\n\nExpiry: SessionType: HTTP\n\nyt.innertube::nextIdRegisters a unique ID to keep statistics of what videos\nfrom YouTube the user has seen.\n\nExpiry: PersistentType: HTML\n\nytidb::LAST_RESULT_ENTRY_KEYStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: PersistentType: HTML\n\nYtIdbMeta#databasesUsed to track user\u2019s interaction with embedded content.\n\nExpiry: PersistentType: IDB\n\nyt-remote-cast-availableStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-cast-installedStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-connected-devicesStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: PersistentType: HTML\n\nyt-remote-device-idStores the user's video player preferences using embedded\nYouTube video\n\nExpiry: PersistentType: HTML\n\nyt-remote-fast-check-periodStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-session-appStores the user's video player preferences using embedded\nYouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-session-nameStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\n    * cdn.pbgrd.com\n\n2\n\npagead/gen_204Collects data on visitor behaviour from multiple websites, in\norder to present more relevant advertisement - This also allows the website to\nlimit the number of times that they are shown the same advertisement.\n\nExpiry: SessionType: Pixel\n\ncsiCollects data on visitors' preferences and behaviour on the website - This\ninformation is used make content and advertisement more relevant to the\nspecific visitor.\n\nExpiry: SessionType: Pixel\n\n    * pub.mdpi-res.com\n\n1\n\nOAIDRegisters a unique ID that identifies a returning user's device. The ID is\nused for targeted ads.\n\nExpiry: 1 yearType: HTTP\n\n  * Unclassified cookies are cookies that we are in the process of classifying, together with the providers of individual cookies.\n\n    * Crazyegg\n\n1\n\nLearn more about this provider\n\n_ce.irvPending\n\nExpiry: SessionType: HTTP\n\n    * matomo.mdpi.com\n\n1\n\n_pk_hsr.0.01efPending\n\nExpiry: 1 dayType: HTTP\n\n    * www.mdpi.com\n\n3\n\nhypothesis.testKeyPending\n\nExpiry: PersistentType: HTML\n\nmdpi_layout_type_v2Pending\n\nExpiry: 1 yearType: HTTP\n\nsettings_cachedPending\n\nExpiry: SessionType: HTTP\n\nCross-domain consent[#BULK_CONSENT_DOMAINS_COUNT#] [#BULK_CONSENT_TITLE#]\n\nList of domains your consent applies to: [#BULK_CONSENT_DOMAINS#]\n\nCookie declaration last updated on 3/25/24 by Cookiebot\n\n## [#IABV2_TITLE#]\n\n[#IABV2_BODY_INTRO#]\n\n[#IABV2_BODY_LEGITIMATE_INTEREST_INTRO#]\n\n[#IABV2_BODY_PREFERENCE_INTRO#]\n\n[#IABV2_BODY_PURPOSES_INTRO#]\n\n[#IABV2_BODY_PURPOSES#]\n\n[#IABV2_BODY_FEATURES_INTRO#]\n\n[#IABV2_BODY_FEATURES#]\n\n[#IABV2_BODY_PARTNERS_INTRO#]\n\n[#IABV2_BODY_PARTNERS#]\n\nCookies are small text files that can be used by websites to make a user's\nexperience more efficient.\n\nThe law states that we can store cookies on your device if they are strictly\nnecessary for the operation of this site. For all other types of cookies we\nneed your permission.\n\nThis site uses different types of cookies. Some cookies are placed by third\nparty services that appear on our pages.\n\nYou can at any time change or withdraw your consent from the Cookie\nDeclaration on our website.\n\nLearn more about who we are, how you can contact us and how we process\npersonal data in our Privacy Policy.\n\nPlease state your consent ID and date when you contact us regarding your\nconsent.\n\nPowered by Cookiebot by Usercentrics\n\nNext Article in Journal\n\nModeling and Analyzing the Strategy Game \u201cFactorio\u201d Using Modular Petri Nets\nand the General-Purpose Petri Net Simulator\n\nNext Article in Special Issue\n\nKey Information Extraction for Crime Investigation by Hybrid Classification\nModel\n\nPrevious Article in Journal\n\nHow to Keep Balance between Interaction and Automation? Toward User Overall\nPositive Experience of IoT-Based Smart Home Design\n\nPrevious Article in Special Issue\n\nNatural Language Processing Influence on Digital Socialization and Linguistic\nInteractions in the Integration of the Metaverse in Regular Social Life\n\n## Journals\n\nActive Journals Find a Journal Proceedings Series\n\n## Topics\n\n## Information\n\nFor Authors For Reviewers For Editors For Librarians For Publishers For\nSocieties For Conference Organizers\n\nOpen Access Policy Institutional Open Access Program Special Issues Guidelines\nEditorial Process Research and Publication Ethics Article Processing Charges\nAwards Testimonials\n\n## Author Services\n\n## Initiatives\n\nSciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS\nProceedings Series\n\n## About\n\nOverview Contact Careers News Press Blog\n\nSign In / Sign Up\n\n## Notice\n\nclear\n\n## Notice\n\nYou are accessing a machine-readable page. In order to be human-readable,\nplease install an RSS reader.\n\nContinue Cancel\n\nclear\n\nAll articles published by MDPI are made immediately available worldwide under\nan open access license. No special permission is required to reuse all or part\nof the article published by MDPI, including figures and tables. For articles\npublished under an open access Creative Common CC BY license, any part of the\narticle may be reused without permission provided that the original article is\nclearly cited. For more information, please refer to\nhttps://www.mdpi.com/openaccess.\n\nFeature papers represent the most advanced research with significant potential\nfor high impact in the field. A Feature Paper should be a substantial original\nArticle that involves several techniques or approaches, provides an outlook\nfor future research directions and describes possible research applications.\n\nFeature papers are submitted upon individual invitation or recommendation by\nthe scientific editors and must receive positive feedback from the reviewers.\n\nEditor\u2019s Choice articles are based on recommendations by the scientific\neditors of MDPI journals from around the world. Editors select a small number\nof articles recently published in the journal that they believe will be\nparticularly interesting to readers, or important in the respective research\narea. The aim is to provide a snapshot of some of the most exciting work\npublished in the various research areas of the journal.\n\nOriginal Submission Date Received: .\n\n  * Journals\n\n    *       * Active Journals\n      * Find a Journal\n      * Proceedings Series\n\n  * Topics\n  * Information\n\n    *       * For Authors\n      * For Reviewers\n      * For Editors\n      * For Librarians\n      * For Publishers\n      * For Societies\n      * For Conference Organizers\n\n      * Open Access Policy\n      * Institutional Open Access Program\n      * Special Issues Guidelines\n      * Editorial Process\n      * Research and Publication Ethics\n      * Article Processing Charges\n      * Awards\n      * Testimonials\n\n  * Author Services\n  * Initiatives\n\n    *       * Sciforum\n      * MDPI Books\n      * Preprints.org\n      * Scilit\n      * SciProfiles\n      * Encyclopedia\n      * JAMS\n      * Proceedings Series\n\n  * About\n\n    *       * Overview\n      * Contact\n      * Careers\n      * News\n      * Press\n      * Blog\n\nSign In / Sign Up Submit\n\nJournals\n\nElectronics\n\nVolume 13\n\nIssue 7\n\n10.3390/electronics13071376\n\nSubmit to this Journal Review for this Journal Propose a Special Issue\n\n\u25ba \u25bc Article Menu\n\n## Article Menu\n\n  * Academic Editor\n\nArkaitz Zubiaga\n\n  * Subscribe SciFeed\n  * Recommended Articles\n  * Related Info Link\n\n    * Google Scholar\n\n  * More by Authors Links\n\n    * on DOAJ\n\n      * Duan, G.\n      * Chen, J.\n      * Zhou, Y.\n      * Zheng, X.\n      * Zhu, Y.\n\n    * on Google Scholar\n\n      * Duan, G.\n      * Chen, J.\n      * Zhou, Y.\n      * Zheng, X.\n      * Zhu, Y.\n\n    * on PubMed\n\n      * Duan, G.\n      * Chen, J.\n      * Zhou, Y.\n      * Zheng, X.\n      * Zhu, Y.\n\n/ajax/scifeed/subscribe\n\nArticle Views 429\n\n  * Table of Contents\n\n    * Abstract\n    * Introduction\n    * Related Work\n    * Prior Knowledge\n    * Methods\n    * Analysis\n    * Experiments\n    * Conclusions\n    * Author Contributions\n    * Funding\n    * Data Availability Statement\n    * Conflicts of Interest\n    * References\n\nAltmetric share Share announcement Help format_quote Cite question_answer\nDiscuss in SciProfiles thumb_up\n\n...\n\nEndorse textsms\n\n...\n\nComment\n\n## Need Help?\n\n### Support\n\nFind support for a specific problem in the support section of our website.\n\nGet Support\n\n### Feedback\n\nPlease let us know what you think of our products and services.\n\nGive Feedback\n\n### Information\n\nVisit our dedicated information section to learn more about MDPI.\n\nGet Information\n\nclear\n\n## JSmol Viewer\n\nclear\n\nfirst_page\n\nDownload PDF\n\nsettings\n\nOrder Article Reprints\n\nFont Type:\n\nArial Georgia Verdana\n\nFont Size:\n\nAa Aa Aa\n\nLine Spacing:\n\nColumn Width:\n\nBackground:\n\nOpen AccessArticle\n\n# Large Language Model Inference Acceleration Based on Hybrid Model Branch\nPrediction\n\nby\n\nGaoxiang Duan\n\nGaoxiang Duan\n\nSciProfiles Scilit Preprints.org Google Scholar\n\n^ 1,2^,\n\nJiajie Chen\n\nJiajie Chen\n\nSciProfiles Scilit Preprints.org Google Scholar\n\n^ 1,2^,\n\nYueying Zhou\n\nYueying Zhou\n\nSciProfiles Scilit Preprints.org Google Scholar\n\n^ 1,2^,\n\nXiaoying Zheng\n\nXiaoying Zheng\n\nSciProfiles Scilit Preprints.org Google Scholar\n\n^ 1,2,*^ and\n\nYongxin Zhu\n\nYongxin Zhu\n\nSciProfiles Scilit Preprints.org Google Scholar\n\n^ 1,2,*^\n\n^1\n\nShanghai Advanced Research Institute, Chinese Academy of Sciences, Shanghai\n201210, China\n\n^2\n\nUniversity of Chinese Academy of Sciences, Beijing 100049, China\n\n^*\n\nAuthors to whom correspondence should be addressed.\n\nElectronics 2024, 13(7), 1376; https://doi.org/10.3390/electronics13071376\n\nSubmission received: 4 March 2024 / Revised: 1 April 2024 / Accepted: 2 April\n2024 / Published: 5 April 2024\n\n(This article belongs to the Special Issue Advanced Natural Language\nProcessing Technology and Applications)\n\nDownload keyboard_arrow_down\n\nDownload PDF Download PDF with Cover Download XML Download Epub\n\nBrowse Figures\n\nReview Reports Versions Notes\n\nArticle Views\n\nCitations -\n\n## Abstract\n\nAs the size of deep learning models continues to expand, the elongation of\ninference time has gradually evolved into a significant challenge to\nefficiency and practicality for autoregressive models. This work introduces a\nhybrid model acceleration strategy based on branch prediction, which\naccelerates autoregressive model inference without requiring retraining and\nensures output consistency with the original model. Specifically, the\nalgorithm employs two models with different parameter sizes aimed at the same\ntask. The smaller model generates a series of potential tokens that are then\nparallelly validated by the larger model to determine their acceptability. By\norchestrating the workflow of the large and small models through a branch-\nprediction strategy, the algorithm conceals the validation time of the larger\nmodel when predictions are successful, thereby accelerating inference. We\npropose a binomial distribution-based prediction function that blends\ntheoretical principles with empirical evidence, specifically designed for the\nnuanced requirements of accelerating inference within a hybrid model\nframework. The entire algorithm was designed and implemented on the llama\nmodel for text generation and translation tasks. The experimental results\nindicate significant improvements. The proposed algorithm achieves a 1.2\u00d7 to\n3.4\u00d7 increase in inference speed compared to the original model, consistently\noutperforming the speculative sampling inference acceleration algorithm.\n\nKeywords:\n\nlarge language model; auto regressive model; branch prediction; decoder;\ninference\n\n## 1\\. Introduction\n\nAutoregressive large language models (LLMs) based on the transformer [1]\narchitecture have revolutionized the field of natural language processing\n(NLP). The scale advantage of these models has brought significant performance\nimprovements to language generation tasks, contributing to groundbreaking\nimprovements across a wide array of applications, from automated content\ncreation to real-time translation services [2,3,4]. The pivotal role of LLMs\nin advancing the frontiers of artificial intelligence and their utility in\nprocessing and understanding complex language patterns cannot be overstated.\n\nHowever, the evolution of these models introduces a critical challenge: their\nincreased complexity leads to longer inference times. As the size and\nsophistication of these models grow, so does the computational cost required\nto generate outputs, leading to inefficiencies that limit their practical\napplicability, especially in scenarios demanding real-time responses. This\nburgeoning problem has catalyzed a flurry of research aimed at devising\nstrategies to accelerate inference without sacrificing the models\u2019 output\nquality, a crucial endeavor for sustaining the momentum of innovation in the\nfield of NLP.\n\nEfforts to mitigate the inference bottleneck of LLMs in NLP have concentrated\non two principal strategies. Firstly, optimizations of transformer model\narchitectures aimed at enhancing decoder efficiency have been pursued to\ndiminish the overall runtime [4,5,6,7]. Such optimizations, however, often\nnecessitate extensive retraining, incurring significant computational costs\nand negating the benefits of existing pretrained models. Secondly, hybrid\nmodel strategies have been explored, where smaller models assist in the\ninitial generation task, potentially accelerating the process. Despite their\npromise, these strategies are limited by the inherent output quality\ndiscrepancies between different model sizes, which can adversely affect\noverall performance. Google\u2019s 2023 proposal of a hybrid model strategy called\nspeculative sampling [8], which employs small models for draft generation and\nlarge models for draft validation, marked a significant departure by\ndecoupling the sequential execution of large models in the generation process,\nthereby achieving notable inference time reductions. However, the emergence of\nworkflow blockages during the validation phase has been identified as a\ncritical performance bottleneck. In response, building upon the principles of\nspeculative sampling, this study introduces an advanced fast inference\nalgorithm that incorporates branch-prediction techniques. Specifically\ndesigned to curtail the inference time of large autoregressive models without\nsacrificing the output quality, this refined approach addresses the\nlimitations inherent in prior methods. By leveraging and extending the\nspeculative sampling framework, our proposed solution significantly enhances\nthe inference efficiency within the NLP domain, offering a sophisticated\nadvancement over existing strategies. To address the blocking issue in the\nvalidation process of hybrid model inference, this paper introduces branch-\nprediction technology and proposes a fast inference algorithm for hybrid\nmodels based on branch prediction. By immediately predicting the possible\nstarting positions with the help of a predefined prediction function after the\nsmall model execution is completed, the parallel execution of the small model\nand large model validation is achieved. This method significantly reduces the\ninference time.\n\nThe contributions of this paper include the following:\n\n  * Firstly, we propose a hybrid model acceleration inference method based on branch prediction. By using branch prediction, the validation time during hybrid model inference would be reduced, which significantly reduces the inference time.\n\n  * Secondly, we construct a branch-prediction function based on the binomial distribution assumption to fit the empirical distribution, further accelerating the inference speed.\n\n  * Lastly, through experiments, it is demonstrated that the proposed algorithm achieves better acceleration effects in tasks of generating combinations of models of different scales.\n\nThe remainder of this paper is organized as follows: We discuss the related\nworks in Section 2. The whole method is described in Section 4, and the\ndetailed analysis is introduced in Section 5. We present the experimental\nresults in Section 6. The conclusion is drawn in Section 7.\n\n## 2\\. Related Work\n\nThere has been extensive research on efficient inference for large models\n[7,9,10]. Previous work on efficient inference for LLMs has mainly focused on\ntwo aspects: accelerating the inference speed of the single-round decoder and\nthe entire inference process. One category of techniques attempts to reduce\nthe computational complexity by changing the model structure to accelerate\nsingle-model inference, such as distillation [11], sparsification [12],\nquantization [13], and architectural modifications [14]. These methods have\naccelerated the model-inference process but require the model to be retrained,\nwhich has the drawback of not being able to use existing pretrained models.\nMoreover, due to modifications in the model structure, the final output cannot\nmaintain consistency with the original target model, resulting in considerable\nperformance loss. Knowledge distillation transfers knowledge from large models\nto smaller ones to reduce computational load, but this process often requires\nadditional training for the smaller models. Model sparsification techniques\nreduce the number of non-zero parameters in the model to lower storage and\ncomputational demands, which may lead to a decrease in the model\nexpressiveness. Quantization methods compress the model by reducing the bit\nwidth of the model parameters, which can significantly reduce computational\nresources while maintaining performance but may introduce quantization errors.\nArchitectural modifications optimize inference speed by redesigning the model\nstructure, such as introducing more efficient attention mechanisms, but may\nrequire complete model restructuring.\n\nAnother category, adaptive computation methods, attempts to accelerate the\nentire inference process. These methods are based on an important observation:\nduring the inference-generation process, some simple generation tasks can be\napproximated by smaller models. Prior work has made related attempts\n[4,7,13,15]. Recently, Han et al. [16] proposed an adaptive computation method\nthat adapts the computational effort to the difficulty of the problem.\nSukhbaatar et al. [17] accelerated inference by having a small model handle\nsome simple tasks in the output. However, these methods also cannot maintain\nconsistent output with the original target model. In 2023, the Google team,\nled by Leviathan et al. [8], proposed a speculative sampling-based hybrid\nmodel adaptive inference method, using a small model as the approximate model.\nBy generating drafts through the approximate small model and validating with\nthe target large model, the time dependency in the autoregressive model-\ninference process is decoupled, allowing the inference process of the large\nmodel to be parallelized. The speculative sampling-based hybrid model\nacceleration inference algorithm effectively speeds up the inference time and\nhas been applied in actual large language model inference scenarios. The\ncomparison between different acceleration methods is listed in Table 1.\n\nTable 1. Comparison of different acceleration algorithms.\n\nHowever, in the validation process of the large model, the generation by the\nsmall model must wait for the validation by the target large model to finish\nbefore restarting, making workflow blocking the main bottleneck in the\ninference speed of hybrid model algorithms.\n\n## 3\\. Prior Knowledge\n\nEnhancing the speed of inference for modern LLMs has become an unavoidable\nchallenge. Traditional inference methods are confronted with substantial\ncomputational resource consumption, which hinders the realization of real-time\ninference demands. Recently, speculative sampling [8] has been proposed as an\neffective method to accelerate inference and has demonstrated superior\nperformance in various studies. However, despite the achievements in\naccelerating inference, the potential for efficiency improvement in\nspeculative sampling has not been fully explored. In light of this, our study\naims to explore the application of branch prediction, a technology widely used\nin computer architecture, in speculative sampling to further enhance the\nefficiency of hybrid model inference acceleration algorithms. By integrating\nthe core concepts from both domains, we aspire to offer a new perspective and\nmethodology for accelerating inference in LLMs. This section will introduce\nthe fundamental principles and application backgrounds of speculative sampling\nand branch prediction, laying the groundwork for understanding the\ncontributions of our work.\n\n#### 3.1. Speculative Sampling\n\nBefore delving into our research, it is crucial to understand the core idea\nand implementation of speculative sampling. Speculative sampling is a method\ndesigned to accelerate the inference process of large language models. It is\npredicated on the assumption that not all steps in the inference process of a\nlarge autoregressive language model equally impact the quality of the final\noutput. Therefore, by predicting which steps have minimal impact on the output\nquality and using smaller, faster models for inference at these steps, the\ninference process can be significantly accelerated without substantially\nsacrificing the output quality. The implementation of speculative sampling\ninvolves two key components: a smaller, faster model with being the\ndistribution of the small model based on the prefix and the original, larger\nmodel with being the distribution of the large model based on the prefix . The\nwhole inference process is listed as follows:\n\n  * The smaller model is used to quickly generate a series of inference output drafts characters (tokens) .\n\n  * These drafts are then verified and, if necessary, corrected by the original large model. In detail, the number of accepted drafts is defined as\n\nwhere .\n\n  * After validation, adjust the result:\n\nwhere is defined to ensure the final output is just like the sample from the\nlarge model :\n\nThe essence of this approach lies in the ability of the smaller model to\npredict and skip over parts that have little impact on the final output,\nconcentrating computational resources on inference steps critical to the\noutput quality. Figure 1 illustrates an example of how speculative sampling\naccelerates the entire generation process.\n\nFigure 1. The schematic diagram of speculative sampling compared with original\ngeneration. In the original output scenario, due to the autoregressive nature\nof the model, the generation of each subsequent word must commence only after\nthe preceding word has been finalized, with the large model requiring six\nunits of time for a single execution. However, under the speculative sampling\napproach, this sequential execution is limited to the generation phase by a\nfaster, smaller model, which requires only one unit of time for execution. The\nlarger model\u2019s validation process can then be conducted in parallel, based on\nthe content already produced by the smaller model, thereby accelerating\ninference speed and increasing throughput by efficiently utilizing the time\ndifferential between the small and large models.\n\n#### 3.2. Branch Prediction\n\nBranch prediction is a pivotal technique in computer architecture, aimed at\nenhancing the efficiency of executing instruction sequences by modern\nprocessors. The core idea revolves around predicting the behavior of\nconditional branches such as if\u2013else statements in a program to reduce delays\ncaused by waiting for branch decisions. In processor design, branch prediction\nallows the processor to preload and execute instructions predicted as the\nsubsequent steps before the actual branch outcome is determined. When\npredictions are accurate, this significantly improves the execution\nefficiency.\n\nThe key to implementing branch prediction lies in the design of prediction\nalgorithms, which must predict the execution of conditional branches in a\nprogram quickly and accurately. The most basic branch-prediction strategies\ninclude static prediction and dynamic prediction:\n\n  * Static prediction typically relies on simple rules, such as always predicting that a branch will go in a specific direction (for example, always predicting that a loop will continue).\n\n  * Dynamic prediction, on the other hand, depends on information collected at runtime, predicting future branch decisions based on historical branch behavior. This category includes a range of complex algorithms, such as Two-Level Adaptive Training and History Table-Based Prediction.\n\n## 4\\. Methods\n\nThis paper proposes a hybrid model acceleration inference algorithm based on\nbranch prediction, with the schematic diagram shown in Figure 2. Section 3.1\nintroduces the workflow of the hybrid model generation algorithm based on\nbranch prediction. Section 3.2 discusses the design of the prediction function\nin the acceleration algorithm.\n\nFigure 2. The schematic diagram of hybrid model inference based on branch\nprediction. For a single execution, the large model requires six units of\ntime, whereas the small model requires one unit of time. Utilizing a\npredictive function allows for the immediate determination of a new starting\npoint upon the completion of small model generation, enabling parallel\nvalidation by the large model. In scenarios where the prediction is\nsuccessful, the validation time of the large model is effectively concealed,\nrendering it negligible from a runtime perspective. Conversely, in cases of\nprediction failure, the runtime remains consistent with that of speculative\nsampling. Considering both successful and unsuccessful predictions, our\napproach invariably enhances inference speed.\n\n#### 4.1. Hybrid Model Inference Acceleration Algorithm Based on Branch\nPrediction\n\nThe inference acceleration algorithm based on branch prediction proposed in\nthis paper divides the overall generation task into multiple rounds, each\ncomprising the following four steps. The algorithm flowchart is shown in\nFigure 1.\n\nLet be the target large model to be accelerated, with being the distribution\nof the target model based on the prefix . The time required for a single\ninference of the target large model is denoted as T. We define as a smaller-\nscale approximate model tasked with the same inference job, and the\nprobability distribution of this model, conditioned on the prefix , is\nreferred to as . The time needed for a single inference of the approximate\nmodel is t, which is T divided by .\n\n  * Small Model Draft Generation: Initially, the small model generates drafts of tokens , which could be the potential generation output after the validation of the large model. The objective of this phase is to swiftly produce preliminary inference outcomes using the small model. These initial results then enable the large model to overcome sequence limitations by utilizing these drafts as a foundation.\n\n  * Branch Prediction: The prediction function is employed to predict the number of acceptable drafts . The first tokens are accepted, and is set as the starting point for the next round of small model inference, followed by the commencement of the next round generation. By introducing branch prediction, we optimized the scheduling of the entire inference process. In the subsequent steps, we will observe that the total inference time is reduced when predictions are accurate.\n\n  * Large Model Validation: Concurrently with the generation of the next round of drafts by the small model, the large model parallelly evaluates all draft tokens from of this round and their respective probabilities to determine the actual number of accepted . If , it signifies that the output result of the target model at this position is consistent with that of the approximate model, accepting the token generated by the approximate model at the ith position of this round; otherwise, it is considered a failure in draft generation by the approximate model at this position. The longest consecutive number of accepted tokens starting from the initial position is counted as the real number of accepted for this round:\n\nThe validation step will determine the actual acceptable draft length, similar\nto speculative sampling. The retained tokens will ensure consistency with the\nresults directly sampled from the large model.\n\n  * Branch-Prediction Result Check: After the validation by the large model is completed, the prediction for this round is checked:\n\nIf , the prediction is deemed successful, and the small model proceeds to the\nnext round of draft generation. Otherwise, in the event of prediction failure,\nthe process reverts to the verification point and restarts the generation\nprocess. The determination of whether a prediction is successful is based on\nthe comparison between the predicted values and the actual values. Whether the\nprediction is successful or not also decides if the inference process can be\naccelerated.\n\nThe steps mentioned above constitute the complete algorithm implementation.\nThe method proposed in this article is built upon the foundation of\nspeculative sampling schemes, further enhancing efficiency. It completely\nconceals the generation time of the large model when predictions are\nsuccessful and maintains the same generation speed as speculative sampling\nwhen predictions fail. This means that the introduction of a branch-prediction\nstrategy will result in a speed increase, even with a poor prediction accuracy\nrate.\n\n#### 4.2. Design of Prediction Function for Acceleration\n\nBefore the inference starts, based on the determined , , and the completion\nnumber n generated each round, obtain the probability density function for the\nnumber of tokens that can be accepted each round. The parameters a and b are\ndetermined by the choice of models , , and n. The probability density\nfunction, as the prediction function in branch prediction, is determined\nbefore the inference starts, and the predicted value is determined by sampling\nfrom the probability density function with .\n\n## 5\\. Analysis\n\n#### 5.1. Impact of Branch-Prediction Strategy on Throughput\n\nThis paper measures the inference speed of different strategies using the\nnumber of tokens generated per unit time, which is the throughput . Since the\nproposed acceleration method aims to align with the target model output, a\nhigher throughput indicates less total inference time used. Here, we analyze\nand compare the throughput of our acceleration algorithm with the standard\ninference process and the fast inference algorithm based on speculative\nsampling [8]. The throughput of the target large model is\n\nThe throughput of the fast inference algorithm based on speculative sampling\nis\n\nwhere is the average number of tokens that can be accepted in each round of n\ngenerated tokens. A larger indicates a more significant acceleration effect.\nThe throughput of the inference acceleration algorithm proposed in this paper\nis\n\nwhere p represents the probability of correct branch prediction. The impact of\nthe branch-prediction strategy on time depends on the outcome of the branch\nprediction. When the prediction result is correct, which is the check function\n, the round of generation saves the time needed for large model validation.\nWhen the prediction result is incorrect, which is the check function , a\nrecall operation is performed to return to the last generation end point and\nchoose the correct position to restart. At this time, the time spent is the\nsame as the algorithm without branch prediction [8]. Based on the above\nanalysis, the fast inference algorithm based on branch prediction proposed in\nthis paper compared to algorithm [8]:\n\nOverall, compared to the fast inference algorithm based on speculative\nsampling, choosing the acceleration algorithm of this paper will reduce the\ninference time. The degree of acceleration depends on the effectiveness of the\nprediction function, i.e., the size of the probability p of correct branch\nprediction.\n\n#### 5.2. Prediction Function Optimization\n\n#### 5.2.1. Prediction Function Based on Independent and Identically\nDistributed (IID) Theory Analysis\n\nThe choice of the prediction function directly affects our acceleration\neffect. Here, this paper follows the assumption of speculative sampling [8];\nthat is, each time the output of the approximate model and the target model is\nconsistent is an independent and identically distributed event. At this time,\nthe probability of the first k words being correctly output is\n\nwhere is the probability that the output of the approximate model is\nconsistent with the target model given the same prefix. Based on the IID\nassumption and according to the probability density function, the distribution\nfunction regarding the correct times has the following characteristics:\n\n  * The probability density distribution function is a monotonically decreasing convex function.\n\n  * There is a significant increase in probability at n compared to the adjacent probability before it. The probability at n should be the sum of all probabilities greater than or equal to n; that is:\n\nFigure 3 shows the possible probability distribution functions under different\nwhen the single-round generation count is based on the I.I.D. assumption.\n\nFigure 3. Probability distribution of the number of correct predictions based\non the I.I.D. assumption.\n\n#### 5.2.2. Prediction Function Based on Empirical Distribution Experimental\nAnalysis\n\nThis paper uses the llama model 15 M as the approximate model and 42 M as the\ntarget model, counting the results of 500 text-generation-task experiments.\nThe statistics of the consecutive correct prediction frequency for single-\nround counts of and are shown in Figure 4.\n\nFigure 4. Frequency distribution of the number of correct predictions.\n\nFrom the figure, it can be observed that the frequency distribution of correct\npredictions has the following characteristics:\n\n  * The continuous approximation function of the frequency distribution is a monotonically decreasing convex function.\n\n  * There is a significant abnormal increase in the frequency distribution at the end n compared to the adjacent frequency data before it.\n\n#### 5.2.3. Determination of the Prediction Function\n\nThe theoretical probability distribution function and the frequency\ndistribution obtained from the empirical distribution have similar\ndistribution function characteristics: both are monotonically decreasing\nconvex functions, and there is a significant increase at n compared to the\nadjacent data before it. The similarity of the frequency distribution to the\nfrequency distribution to some extent demonstrates the reliability of the\ntheoretical assumption in 4.2. Further, the I.I.D. assumption is a simple\napproximation of reality; therefore, this paper adds two correction terms to\nmore closely fit the final empirical probability density function. The final\nprediction function is obtained as\n\nwhere a and c are parameters of the correction terms.\n\nAccording to the prediction function, the impact of the number of drafts\ngenerated per round is shown in Figure 5, Figure 6 and Figure 7.\n\nFigure 5. The impact of the number of drafts generated per round by our work\non throughput. BP represents the method with branch-prediction strategy. The\nvertical bar represents the minimum number of tokens that can be generated in\na single round. Higher is better.\n\nFigure 6. The impact of the number of drafts generated per round by the\nspeculative sampling strategy on throughput. Non-BP represents without branch\nprediction. The vertical bar represents the minimum number of tokens that can\nbe generated in a single round. Higher is better.\n\nFigure 7. The comparison of the impact of the number of drafts generated per\nround on throughput. The vertical bar signifies the minimum number of tokens\nthat can be generated in a single epoch. A higher value is preferable. Given\nthe same probability settings, the throughput of the method employing branch\nprediction consistently surpasses that of the method without branch\nprediction.\n\n## 6\\. Experiments\n\n#### 6.1. Experiments Setup\n\nAll the models were thoroughly pretrained on diverse text corpora prior to the\nexperiments. Both our approach and the benchmark speculative sampling\nalgorithm ensure that the outputs perfectly aligned with the target model,\nresulting in consistent outputs across the three models compared. Our\nexperiment focuses solely on the throughput of each model.\n\nThe models used in this experiment are based on the llama [19] architecture,\nincluding four different scales of models with parameter sizes of 260 K, 15 M,\n42 M, and 110 M. Table 2 outlines the parameter configurations of the llama\nmodels utilized in our experiments. In the experimental setup, the model\ntemperature is set to 0.8, with top-p sampling at 0.9. The use of top-p\nsampling ensures that tokens with minuscule probabilities are not sampled,\nthus contributing to the efficiency and relevance of the generated text.\n\nTable 2. Parameters of models at different scales.\n\n#### 6.2. The Impact of Branch-Prediction Algorithm across Diverse Model\nScales\n\nThis section validates the proposed branch-prediction acceleration algorithm\u2019s\neffectiveness across a spectrum of model scales, ranging from small to large\nconfigurations. For the text-generation task, this study employs the\nTinyStories dataset [20]. Meanwhile, the translation tasks utilize the WMT\n2018 English\u2013French dataset [21].\n\nThis experiment utilizes throughput as a quantitative measure to evaluate the\nmodels\u2019 generation speed:\n\nThis experiment compared the computation times of different algorithms under\nthe same model scale configuration. The comparative results of the computation\ntime for the text-generation task are presented in Figure 8, and those for the\ntranslation task are also shown in Figure 9. Furthermore, the detailed\nthroughput data for the text-generation task are listed in Table 3 and for the\ntranslation task in Table 4.\n\nFigure 8. Comparative analysis of computing times across various algorithms\nfor text-generation tasks under different model configurations: lower times\nindicate better performance.\n\nFigure 9. Comparative analysis of computing times across various algorithms\nfor translation tasks under different model configurations: lower times\nindicate better performance.\n\nTable 3. Comparison of inference throughput under different hybrid model\nconfigurations in text-generation tasks.\n\nTable 4. Comparison of inference throughput under different hybrid model\nconfigurations in translation tasks.\n\nThe experimental results indicate that the branch-prediction algorithm\nproposed in this paper, while maintaining the output consistency with the\ntarget model, achieves a higher throughput than both the target large model\nand the speculative sampling algorithm proposed by the Google team [8] across\ndifferent tasks and model configurations.\n\nIn text-generation tasks, the branch-prediction algorithm demonstrates a\nsignificant throughput improvement compared to both the target large model and\nthe speculative sampling algorithm proposed by the Google team. Specifically,\nwithin the setup using a 260 K small model coupled with a 15 M target model,\nthe branch-prediction algorithm reached a throughput of 65.33 tokens/s. This\nis 1.3 times the throughput of the speculative sampling algorithm at 50.12\ntokens/s and 1.4 times higher than the target model\u2019s 45.59 tokens/s.\nRemarkably, in the setup of 15 M plus 110 M, the branch-prediction algorithm\nachieved a throughput of 15.73 tokens/s, which is 2.6 times the throughput of\nthe target large model at 6.13 tokens/s and substantially exceeds the\nspeculative sampling algorithm\u2019s 12.21 tokens/s. Table 5 demonstrates our\napproach in a text-generation task, achieving the shortest computation time\nwhile maintaining a consistent content output.\n\nTable 5. Performance comparison in text-generation task between our work,\nspeculative sampling, and the origin method.\n\nIn translation tasks, the branch-prediction algorithm continues to demonstrate\nits superior efficacy. For instance, within the framework utilizing a 260 K\nsmall model in conjunction with a 15 M target model, the branch-prediction\nalgorithm attained a throughput of 59.53 tokens/s. This performance is\napproximately 31% higher than the speculative sampling algorithm\u2019s 45.33\ntokens/s and nearly 1.9 times the throughput of the target model at 31.29\ntokens/s. Even more impressive, in a configuration combining a 42 M small\nmodel with a 110 M target model, the branch-prediction algorithm realized a\nthroughput of 13.42 tokens/s. This rate is 1.34 times the throughput achieved\nby the speculative sampling algorithm at 10.01 tokens/s and 3.4 times greater\nthan the target model\u2019s 4.38 tokens/s. These outcomes underscore the branch-\nprediction algorithm\u2019s remarkable acceleration capability, especially in\nconfigurations involving larger models.\n\nThese results clearly demonstrate that the branch-prediction algorithm, while\nmaintaining output consistency with the target model, offers substantial\nperformance improvements in accelerating autoregressive model inference\ncompared to the existing speculative sampling algorithm. The achievement of\nsuch acceleration effects, especially in translation and text-generation tasks\nwith high-throughput demands, validates the effectiveness and superiority of\nthe branch-prediction algorithm in practical applications.\n\nAdditionally, the scale of the approximate model does not linearly correlate\nwith the acceleration effect. In the text-generation task targeting the 110 M\nmodel, the 15 M approximate model achieved the highest acceleration effect of\n2.6 times. In the translation task, targeting the same 110 M model, the 42 M\napproximate model reached the highest acceleration effect of 3.4 times. In\npractical applications, the appropriate approximate model should be selected\nbased on the specific task scenario and target model.\n\n#### 6.3. Ablation Experiment on the Impact of Single-Round Token Generation\nQuantity on Acceleration Effect\n\nThis section analyzes the impact of the number of tokens generated by the\napproximate model in a single round on the acceleration effect, with the 110 M\nmodel as the target model and the 15 M model as the approximate model in the\ntext-generation experiment. The minimum value of n must satisfy the condition\nthat the single-round draft time is greater than the target model\u2019s single\nverification time:\n\nIn this experiment, the test range is chosen as . The experimental results are\nshown in Figure 10. From Figure 10, it can be seen that the number of tokens\ngenerated in a single round, n, is negatively correlated with the acceleration\neffect. At n = 8, we observe the optimal acceleration effect, achieving an\ninference speed of 15.73 tokens/s, representing a 2.6-fold increase over the\ntarget model\u2019s acceleration ratio. Conversely, at n = 20, the acceleration\neffect diminishes to its lowest, with an inference speed of 7.11 tokens/s,\nmarking only a 1.2-fold improvement in the acceleration ratio relative to the\ntarget model. The impact of the number of tokens generated by the approximate\nmodel in a single round on the acceleration effect is consistent with the\nprediction in Section 5.\n\nFigure 10. Comparison of different algorithms\u2019 throughput as the number of\ntokens generated by the approximate model in a single round changes. Higher is\nbetter.\n\n#### 6.4. Extreme Trade-Off Strategy: Exploring Exhaustive Methods\n\nFurther, this section presents a more extreme trade-off strategy to explore\nthe limit of time-friendly algorithms. After each execution of the small\nmodel, instead of branch prediction, all possible cases are executed in\nparallel, while the large model performs validation in parallel. After the\nnext round of the small model execution is completed, the validation work of\nthe large model has also been finished. Choose the correct path among the all-\nparallel small models verified as correct to continue execution and repeat the\nabove operation. The comparison diagram is shown in Figure 11.\n\nFigure 11. The comparison between the exhaustive method and the branch-\nprediction-based algorithm for hybrid model inference highlights a fundamental\ndifference in approach. Instead of selecting a single potential starting point\nwith the assistance of the branch-prediction function, the exhaustive method\nenumerates all possible starting points and performs generation in parallel.\nThis approach significantly diminishes the time required for large model\nvalidation.\n\nThe exhaustive method can completely hide the validation time of the large\nmodel from a temporal perspective, but since it requires the parallel\nexecution of n small models, it also significantly increases the use of\ncomputing resources. This section chose the 15 M and 110 M models for the\ntext-generation-task experiment. The experimental results are shown in Figure\n12. From the figure, it can be seen that the strategy of using the exhaustive\nmethod reaches the highest inference speed of 20.01 tokens/s. However, it also\nincreases up to eight times the computing resources of the approximate model.\nAs n increases, the additional amount of the approximate model also\ncontinuously increases.\n\nFigure 12. Comparative analysis of computational resource consumption between\nexhaustive and branch-prediction methods. This figure illustrates the\ncomparison of computational resource consumption between the exhaustive method\nand branch-prediction method in text-generation tasks. The exhaustive method,\nachieving the highest inference speed of 20.01 tokens/s, correspondingly\nincurs an up to 8-fold increase in computational resource usage. The graph\ndistinctly shows how the exhaustive method\u2019s demand for computational\nresources exponentially grows with an increase in n (the number of tokens\ngenerated in a single round), highlighting the importance of balancing\ncomputational resources when choosing implementation strategies.\n\n## 7\\. Conclusions\n\nThis study addresses the problem of performance bottlenecks caused by\nincreasing model sizes in large language model inference and proposes a hybrid\nmodel inference acceleration algorithm based on branch prediction. By\nintroducing a branch-prediction scheduling strategy and designing personalized\nprediction functions for different model combinations based on empirical\ndistributions and theoretical assumptions, the inference process is optimized\nfor acceleration. The branch-prediction-based inference acceleration algorithm\nproposed in this paper significantly improves inference speed while aligning\nwith the output of the target large model.\n\nIn terms of validating previous theories, this paper thoroughly analyzes\nexisting research attempts to accelerate large model inference and chooses a\nhybrid model acceleration strategy to avoid the need for retraining models. It\nfurther introduces branch prediction to accelerate hybrid inference and reduce\nworkflow congestion. The research results show that compared to the target\nmodel and the latest advanced acceleration efforts, the algorithm proposed in\nthis paper effectively enhances the inference speed. This method solves the\nperformance issue of the increased inference time in large language models,\noffering a viable solution to the challenge of maintaining real-time inference\ncapabilities as model sizes continue to expand.\n\n## Author Contributions\n\nConceptualization, G.D. and J.C.; methodology, G.D. and Y.Z. (Yueying Zhou);\nsoftware, G.D.; validation, G.D.; formal analysis, G.D.; investigation, G.D.;\nresources, G.D.; data curation, G.D.; writing\u2014original draft preparation,\nG.D.; writing\u2014review and editing, X.Z. and Y.Z. (Yongxin Zhu); visualization,\nG.D.; supervision, X.Z.; project administration, X.Z. and Y.Z. (Yongxin Zhu);\nfunding acquisition, X.Z. and Y.Z. (Yongxin Zhu). All authors have read and\nagreed to the published version of the manuscript.\n\n## Funding\n\nThis research was supported by the National Natural Science Foundation of\nChina under grant number 12373113, as well as the National SKA Program of\nChina (grant no. 2020SKA0120202).\n\n## Data Availability Statement\n\nThe Part of this research data and model could be found in\nhttps://github.com/karpathy/llama2.c/blob/master/run.c accessed on 3 March\n2024.\n\n## Conflicts of Interest\n\nThe authors declare no conflicts of interest. The funders had no role in the\ndesign of the study; in the collection, analyses, or interpretation of data;\nin the writing of the manuscript; or in the decision to publish the results.\n\n## References\n\n  1. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.N.; Kaiser, \u0141.; Polosukhin, I. Attention is all you need. Adv. Neural Inf. Process. Syst. 2017, 30. [Google Scholar]\n  2. Kenton, J.D.M.W.C.; Toutanova, L.K. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the naacL-HLT, Minneapolis, MN, USA, 2\u20137 June 2019; Volume 1, p. 2. [Google Scholar]\n  3. Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux, M.A.; Lacroix, T.; Rozi\u00e8re, B.; Goyal, N.; Hambro, E.; Azhar, F.; et al. Llama: Open and efficient foundation language models. arXiv 2023, arXiv:2302.13971. [Google Scholar]\n  4. Liu, Y.; Pan, D.; Zhang, H.; Zhong, K. Degradation-Trend-Aware Deep Neural Network with Attention Mechanism for Bearing Remaining Useful Life Prediction. IEEE Trans. Artif. Intell. 2023, 1\u201315. [Google Scholar] [CrossRef]\n  5. Kitaev, N.; Kaiser, \u0141.; Levskaya, A. Reformer: The efficient transformer. arXiv 2020, arXiv:2001.04451. [Google Scholar]\n  6. Ghimire, D.; Kil, D.; Kim, S.h. A survey on efficient convolutional neural networks and hardware acceleration. Electronics 2022, 11, 945. [Google Scholar] [CrossRef]\n  7. Zhou, Z.; Zhu, Y.; He, C.; Wang, Y.; Yan, S.; Tian, Y.; Yuan, L. Spikformer: When spiking neural network meets transformer. arXiv 2022, arXiv:2209.15425. [Google Scholar]\n  8. Leviathan, Y.; Kalman, M.; Matias, Y. Fast inference from transformers via speculative decoding. In Proceedings of the International Conference on Machine Learning, Vienna, Austria, 21\u201317 July 2023; pp. 19274\u201319286. [Google Scholar]\n  9. Dehghani, M.; Arnab, A.; Beyer, L.; Vaswani, A.; Tay, Y. The efficiency misnomer. arXiv 2021, arXiv:2110.12894. [Google Scholar]\n  10. Lin, T.; Wang, Y.; Liu, X.; Qiu, X. A survey of transformers. AI Open 2022, 3, 111\u2013132. [Google Scholar] [CrossRef]\n  11. Hinton, G.; Vinyals, O.; Dean, J. Distilling the knowledge in a neural network. arXiv 2015, arXiv:1503.02531. [Google Scholar]\n  12. Zhai, X.; Kolesnikov, A.; Houlsby, N.; Beyer, L. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, New Orleans, LA, USA, 18\u201324 June 2022; pp. 12104\u201312113. [Google Scholar]\n  13. Qiu, J.; Ma, H.; Levy, O.; Yih, S.W.t.; Wang, S.; Tang, J. Blockwise self-attention for long document understanding. arXiv 2019, arXiv:1911.02972. [Google Scholar]\n  14. Beltagy, I.; Peters, M.E.; Cohan, A. Longformer: The long-document transformer. arXiv 2020, arXiv:2004.05150. [Google Scholar]\n  15. Schwartz, R.; Stanovsky, G.; Swayamdipta, S.; Dodge, J.; Smith, N.A. The right tool for the job: Matching model and instance complexities. arXiv 2020, arXiv:2004.07453. [Google Scholar]\n  16. Han, Y.; Huang, G.; Song, S.; Yang, L.; Wang, H.; Wang, Y. Dynamic neural networks: A survey. IEEE Trans. Pattern Anal. Mach. Intell. 2021, 44, 7436\u20137456. [Google Scholar] [CrossRef] [PubMed]\n  17. Schuster, T.; Fisch, A.; Jaakkola, T.; Barzilay, R. Consistent accelerated inference via confident adaptive transformers. arXiv 2021, arXiv:2104.08803. [Google Scholar]\n  18. Shazeer, N. Fast transformer decoding: One write-head is all you need. arXiv 2019, arXiv:1911.02150. [Google Scholar]\n  19. Zhang, R.; Han, J.; Zhou, A.; Hu, X.; Yan, S.; Lu, P.; Li, H.; Gao, P.; Qiao, Y. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. arXiv 2023, arXiv:2303.16199. [Google Scholar]\n  20. Eldan, R.; Li, Y. TinyStories: How Small Can Language Models Be and Still Speak Coherent English? arXiv 2023, arXiv:2305.07759. [Google Scholar]\n  21. Sennrich, R.; Haddow, B.; Birch, A. Improving neural machine translation models with monolingual data. arXiv 2015, arXiv:1511.06709. [Google Scholar]\n\nFigure 1. The schematic diagram of speculative sampling compared with original\ngeneration. In the original output scenario, due to the autoregressive nature\nof the model, the generation of each subsequent word must commence only after\nthe preceding word has been finalized, with the large model requiring six\nunits of time for a single execution. However, under the speculative sampling\napproach, this sequential execution is limited to the generation phase by a\nfaster, smaller model, which requires only one unit of time for execution. The\nlarger model\u2019s validation process can then be conducted in parallel, based on\nthe content already produced by the smaller model, thereby accelerating\ninference speed and increasing throughput by efficiently utilizing the time\ndifferential between the small and large models.\n\nFigure 2. The schematic diagram of hybrid model inference based on branch\nprediction. For a single execution, the large model requires six units of\ntime, whereas the small model requires one unit of time. Utilizing a\npredictive function allows for the immediate determination of a new starting\npoint upon the completion of small model generation, enabling parallel\nvalidation by the large model. In scenarios where the prediction is\nsuccessful, the validation time of the large model is effectively concealed,\nrendering it negligible from a runtime perspective. Conversely, in cases of\nprediction failure, the runtime remains consistent with that of speculative\nsampling. Considering both successful and unsuccessful predictions, our\napproach invariably enhances inference speed.\n\nFigure 3. Probability distribution of the number of correct predictions based\non the I.I.D. assumption.\n\nFigure 4. Frequency distribution of the number of correct predictions.\n\nFigure 5. The impact of the number of drafts generated per round by our work\non throughput. BP represents the method with branch-prediction strategy. The\nvertical bar represents the minimum number of tokens that can be generated in\na single round. Higher is better.\n\nFigure 6. The impact of the number of drafts generated per round by the\nspeculative sampling strategy on throughput. Non-BP represents without branch\nprediction. The vertical bar represents the minimum number of tokens that can\nbe generated in a single round. Higher is better.\n\nFigure 7. The comparison of the impact of the number of drafts generated per\nround on throughput. The vertical bar signifies the minimum number of tokens\nthat can be generated in a single epoch. A higher value is preferable. Given\nthe same probability settings, the throughput of the method employing branch\nprediction consistently surpasses that of the method without branch\nprediction.\n\nFigure 8. Comparative analysis of computing times across various algorithms\nfor text-generation tasks under different model configurations: lower times\nindicate better performance.\n\nFigure 9. Comparative analysis of computing times across various algorithms\nfor translation tasks under different model configurations: lower times\nindicate better performance.\n\nFigure 10. Comparison of different algorithms\u2019 throughput as the number of\ntokens generated by the approximate model in a single round changes. Higher is\nbetter.\n\nFigure 11. The comparison between the exhaustive method and the branch-\nprediction-based algorithm for hybrid model inference highlights a fundamental\ndifference in approach. Instead of selecting a single potential starting point\nwith the assistance of the branch-prediction function, the exhaustive method\nenumerates all possible starting points and performs generation in parallel.\nThis approach significantly diminishes the time required for large model\nvalidation.\n\nFigure 12. Comparative analysis of computational resource consumption between\nexhaustive and branch-prediction methods. This figure illustrates the\ncomparison of computational resource consumption between the exhaustive method\nand branch-prediction method in text-generation tasks. The exhaustive method,\nachieving the highest inference speed of 20.01 tokens/s, correspondingly\nincurs an up to 8-fold increase in computational resource usage. The graph\ndistinctly shows how the exhaustive method\u2019s demand for computational\nresources exponentially grows with an increase in n (the number of tokens\ngenerated in a single round), highlighting the importance of balancing\ncomputational resources when choosing implementation strategies.\n\nTable 1. Comparison of different acceleration algorithms.\n\nAcceleration Algorithms| Acceleration Type| Need Retrain| Align Original\nOutput  \n---|---|---|---  \ndistillation [11]| single-round decoder| Yes| No  \nsparsification [12]| Yes| No  \nquantization [13]| Yes| No  \narchitectural modifications [14]| Yes| No  \nconfident adaptive transformers [17]| entire inference process| Yes| No  \nfast transformer decoding [18]| Yes| No  \nspeculative sampling [8]| No| Yes  \n  \nTable 2. Parameters of models at different scales.\n\nModel| Dimension| Number of Attention Heads| Layers| Parameters  \n---|---|---|---|---  \n260 K| 64| 8| 5| 260 K  \n15 M| 288| 6| 6| 15 M  \n42 M| 512| 8| 8| 42 M  \n110 M| 768| 12| 12| 110 M  \n  \nTable 3. Comparison of inference throughput under different hybrid model\nconfigurations in text-generation tasks.\n\nHybrid Model Configuration| Small Model Throughput (tokens/s)| Target Model\nThroughput (tokens/s)| Speculative Sampling Throughput (tokens/s)| Branch-\nPrediction Throughput (tokens/s)  \n---|---|---|---|---  \n260 K + 15 M| 2400| 45.59| 50.12| 65.33 (1.4\u00d7)  \n260 K + 42 M| 2400| 16.29| 18.02| 21.18 (1.3\u00d7)  \n15 M + 42 M| 45.59| 16.29| 25.23| 35.51 (2.2\u00d7)  \n260 K + 110 M| 2400| 6.13| 6.72| 7.24 (1.2\u00d7)  \n15 M + 110 M| 45.59| 6.13| 12.21| 15.73 (2.6\u00d7)  \n42 M + 110 M| 16.29| 6.13| 10.09| 13.58 (2.2\u00d7)  \n  \nTable 4. Comparison of inference throughput under different hybrid model\nconfigurations in translation tasks.\n\nHybrid Model Configuration| Small Model Throughput (tokens/s)| Target Model\nThroughput (tokens/s)| Speculative Sampling Throughput (tokens/s)| Branch-\nPrediction Throughput (tokens/s)  \n---|---|---|---|---  \n260 K + 15 M| 1852| 31.29| 45.33| 59.53 (1.9\u00d7)  \n260 K + 42 M| 1852| 12.06| 17.79| 19.21 (1.6\u00d7)  \n15 M + 42 M| 31.29| 12.06| 22.48| 31.01 (2.6\u00d7)  \n260 K + 110 M| 1852| 4.38| 5.52| 6.89 (1.6\u00d7)  \n15 M + 110 M| 31.29| 4.38| 11.28| 13.31 (3.2\u00d7)  \n42 M + 110 M| 12.06| 4.38| 10.01| 13.42 (3.4\u00d7)  \n  \nTable 5. Performance comparison in text-generation task between our work,\nspeculative sampling, and the origin method.\n\nMethod| Origin (110 M)| Speculative Sampling (110M + 42 M)| Our Work (110 M +\n42 M)  \n---|---|---|---  \nContent| One day, Lily met a Shoggoth. He was very shy, but was also very\ngenerous. Lily said \u201cHello Shoggy! Can I be your friend?\u201d Shoggy was happy to\nhave a friend and said \u201cYes, let\u2019s explore the universe together!\u201d So they set\noff on a journey to explore the universe. As they travelled, Shoggy was happy\nto explain to Lily about all the wonderful things in the universe. At the end\nof the day, Lily and Shoggy had gathered lots of wonderful things from the\nuniverse, and they both felt very proud. They promised to explore the universe\nas one big pair and to never stop being generous to each other.| One day, Lily\nmet a Shoggoth. He was very shy, but was also very generous. Lily said \u201cHello\nShoggy! Can I be your friend?\u201d Shoggy was happy to have a friend and said\n\u201cYes, let\u2019s explore the universe together!\u201d So they set off on a journey to\nexplore the universe. As they travelled, Shoggy was happy to explain to Lily\nabout all the wonderful things in the universe. At the end of the day, Lily\nand Shoggy had gathered lots of wonderful things from the universe, and they\nboth felt very proud. They promised to explore the universe as one big pair\nand to never stop being generous to each other.| One day, Lily met a Shoggoth.\nHe was very shy, but was also very generous. Lily said \u201cHello Shoggy! Can I be\nyour friend?\u201d Shoggy was happy to have a friend and said \u201cYes, let\u2019s explore\nthe universe together!\u201d So they set off on a journey to explore the universe.\nAs they travelled, Shoggy was happy to explain to Lily about all the wonderful\nthings in the universe. At the end of the day, Lily and Shoggy had gathered\nlots of wonderful things from the universe, and they both felt very proud.\nThey promised to explore the universe as one big pair and to never stop being\ngenerous to each other.  \nTotal Computing Time: s| 18.10| 10.99| 8.10  \n  \nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in\nall publications are solely those of the individual author(s) and\ncontributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s)\ndisclaim responsibility for any injury to people or property resulting from\nany ideas, methods, instructions or products referred to in the content.  \n---  \n\u00a9 2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an\nopen access article distributed under the terms and conditions of the Creative\nCommons Attribution (CC BY) license\n(https://creativecommons.org/licenses/by/4.0/).\n\n## Share and Cite\n\nMDPI and ACS Style\n\nDuan, G.; Chen, J.; Zhou, Y.; Zheng, X.; Zhu, Y. Large Language Model\nInference Acceleration Based on Hybrid Model Branch Prediction. Electronics\n2024, 13, 1376. https://doi.org/10.3390/electronics13071376\n\nAMA Style\n\nDuan G, Chen J, Zhou Y, Zheng X, Zhu Y. Large Language Model Inference\nAcceleration Based on Hybrid Model Branch Prediction. Electronics. 2024;\n13(7):1376. https://doi.org/10.3390/electronics13071376\n\nChicago/Turabian Style\n\nDuan, Gaoxiang, Jiajie Chen, Yueying Zhou, Xiaoying Zheng, and Yongxin Zhu.\n2024. \"Large Language Model Inference Acceleration Based on Hybrid Model\nBranch Prediction\" Electronics 13, no. 7: 1376.\nhttps://doi.org/10.3390/electronics13071376\n\nNote that from the first issue of 2016, this journal uses article numbers\ninstead of page numbers. See further details here.\n\n## Article Metrics\n\nYes\n\n### Citations\n\nNo citations were found for this article, but you may check on Google Scholar\n\nNo\n\n### Article Access Statistics\n\nFor more information on the journal statistics, click here.\n\nMultiple requests from the same IP address are counted as one view.\n\nZoom | Orient | As Lines | As Sticks | As Cartoon | As Surface | Previous Scene | Next Scene\n\n## Cite\n\nExport citation file: BibTeX | EndNote | RIS\n\nMDPI and ACS Style\n\nDuan, G.; Chen, J.; Zhou, Y.; Zheng, X.; Zhu, Y. Large Language Model\nInference Acceleration Based on Hybrid Model Branch Prediction. Electronics\n2024, 13, 1376. https://doi.org/10.3390/electronics13071376\n\nAMA Style\n\nDuan G, Chen J, Zhou Y, Zheng X, Zhu Y. Large Language Model Inference\nAcceleration Based on Hybrid Model Branch Prediction. Electronics. 2024;\n13(7):1376. https://doi.org/10.3390/electronics13071376\n\nChicago/Turabian Style\n\nDuan, Gaoxiang, Jiajie Chen, Yueying Zhou, Xiaoying Zheng, and Yongxin Zhu.\n2024. \"Large Language Model Inference Acceleration Based on Hybrid Model\nBranch Prediction\" Electronics 13, no. 7: 1376.\nhttps://doi.org/10.3390/electronics13071376\n\nNote that from the first issue of 2016, this journal uses article numbers\ninstead of page numbers. See further details here.\n\nclear\n\nElectronics, EISSN 2079-9292, Published by MDPI\n\nRSS Content Alert\n\n### Further Information\n\nArticle Processing Charges Pay an Invoice Open Access Policy Contact MDPI Jobs\nat MDPI\n\n### Guidelines\n\nFor Authors For Reviewers For Editors For Librarians For Publishers For\nSocieties For Conference Organizers\n\n### MDPI Initiatives\n\nSciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS\nProceedings Series\n\n### Follow MDPI\n\nLinkedIn Facebook Twitter\n\n\u00a9 1996-2024 MDPI (Basel, Switzerland) unless otherwise stated\n\nDisclaimer\n\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in\nall publications are solely those of the individual author(s) and\ncontributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s)\ndisclaim responsibility for any injury to people or property resulting from\nany ideas, methods, instructions or products referred to in the content.\n\nTerms and Conditions Privacy Policy\n\nWe use cookies on our website to ensure you get the best experience. Read more\nabout our cookies here.\n\nAccept\n\n## Share Link\n\nCopy\n\nclear\n\n## Share\n\nclear\n\nBack to TopTop\n\n", "frontpage": true}
