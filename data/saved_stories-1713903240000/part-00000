{"aid": "40131248", "title": "Swapping Disks in Kubernetes for Fun and Profit", "url": "https://techblog.citystoragesystems.com/p/swapping-disks-in-kubernetes", "domain": "citystoragesystems.com", "votes": 3, "user": "sjuls", "posted_at": "2024-04-23 12:38:19", "comments": 0, "source_title": "Swapping Disks in Kubernetes for Fun and Profit", "source_text": "Swapping Disks in Kubernetes for Fun and Profit\n\nShare this post\n\n#### Swapping Disks in Kubernetes for Fun and Profit\n\ntechblog.citystoragesystems.com\n\n#### Discover more from City Storage Systems\n\nEngineering the future of food at Otter & CloudKitchens\n\nContinue reading\n\nSign in\n\n# Swapping Disks in Kubernetes for Fun and Profit\n\n### Introducing the PvcAutoscaler at City Storage Systems\n\nApr 23, 2024\n\n1\n\nShare this post\n\n#### Swapping Disks in Kubernetes for Fun and Profit\n\ntechblog.citystoragesystems.com\n\nShare\n\nWritten by Jakob Schultz-Falk, member of the storage team at City Storage\nSystems who led the development of the PvcAutoscaler.\n\nSince the introduction of the StatefulSet more and more stateful workloads\nhave been added to the Kubernetes ecosystem. Unfortunately there are still\nplenty of caveats to running stateful workloads in Kubernetes, some of which\nare caused by fundamental limitations of the StatefulSet controller and the\nPVCs it generates.\n\nWhile stateless workloads enjoy almost boundless elasticity in Kubernetes,\nstateful workloads, once deployed, are bound by the volumes they mount. These\nvolumes are static and practically immutable, making it difficult to scale to\nmatch the ever changing needs of the workloads using them.\n\nThis post will present the solution we\u2019ve developed at City Storage Systems\naimed at reclaiming stateful elasticity in order to improve cost efficiency\nand reduce toil.\n\n#\n\nThe Problem\n\nWhile there is some support in Kubernetes for expanding storage volumes, there\nis no such mechanism for reducing storage capacity, nor for changing the\nunderlying storage type. In fact the original Kubernetes Enhancement Proposal\n(KEP) for supporting volume expansion explicitly lists volume reduction as a\nnon-goal due to the complexities of shrinking volumes.\n\nFurthermore, the volumeClaimTemplates section of the StatefulSet spec is\ncurrently immutable, preventing operators from expanding volumes via a high\nlevel API, though a future enhancement has been proposed.\n\nThis leads to a situation where the total capacity provisioned only grows, and\nany storage related change requires a high-effort and risky migration,\ninflating the cost of running stateful workloads.\n\nTo summarize, the two primary culprits restraining stateful elasticity are:\n\n  * Only volume expansion is supported, and only by a subset of volume provisioners\n\n  * Immutable volume claim templates embedded in the StatefulSet spec\n\n#\n\nOur Objectives\n\nNow that we have a clear understanding of the problem, let\u2019s list the\nobjectives we want to achieve with our new solution.\n\n  * Volume expansion for growing storage needs\n\n  * Volume shrinking to reclaim cost overhead from unused storage\n\n  * Volume modification, e.g. be able to swap out an HDD with an SSD or vice-versa\n\nAll the objectives above should be solved by an on-demand, declarative, and\ntoil-free solution which can be used by any software engineer accustomed to\nKubernetes and StatefulSets.\n\n#\n\nKubernetes Storage Concepts\n\nBefore diving into the inner workings of the PvcAutoscaler, let\u2019s briefly\nintroduce some of the core Kubernetes components we will rely on to achieve\nour objectives. These concepts are described in more detail in the official\nKubernetes storage documentation.\n\n##\n\nPersistent Volumes and Persistent Volume Claims\n\nPersistent Volumes (PV) are resources in Kubernetes which contain information\nregarding the underlying storage they represent. These are typically (but not\nnecessarily) cloud provider managed disks.\n\nPersistent Volume Claims (PVC) are resources which bind to PVs, thereby\nreserving usage of the PV across the cluster. Pods in the same namespace as\nthe PVC can then reference the PVC to use the underlying storage.\n\n##\n\nThe StorageClass\n\nIn order to provision different types of storage, Kubernetes offers the\nconcept of a StorageClass. These can be used to describe different types of\nstorage, e.g. one StorageClass could be configured to provision PVs backed by\nSSDs while another provides access to HDDs.\n\nIn general the process of provisioning a new PV/PVC combination to be used by\na pod, is done by creating a PVC referencing a StorageClass. As the PVC is not\nbound to any PV at creation the volume provisioner of the StorageClass will\nattempt to provision a PV and bind it to the PVC.\n\nThe StorageClass contains a reference to a volume provisioner along with\nparameters needed to create a new PV backed by some underlying storage. E.g. a\nvolume provisioner developed by a cloud provider would take the input\nparameters and provision a network attached disk with the desired\nconfiguration.\n\n##\n\nThe StatefulSet\n\nThis resource type was added to Kubernetes in order to get stable naming\nmapped to persistent storage across pod restarts. So while the Deployment\nobject would create pods with unique names on each creation, the StatefulSet\nwill create pods with serial ordinals mapping one-to-one to a PVC of the same\nordinal.\n\nThe PVCs can either be manually provisioned upfront, or it can be left up to\nthe StatefulSet controller to create the PVCs based on the volume claim\ntemplates defined in its spec. As mentioned above the StatefulSet offers no\nsupport for changes to the PVCs or the underlying storage once provisioned.\n\n#\n\nThe PvcAutoscaler\n\nNow that we\u2019ve introduced the core storage concepts we will be referencing, we\ncan begin using them to build up our PvcAutoscaler solution. We start out by\naddressing the shortcomings of the StatefulSet.\n\n##\n\nTaking Control\n\nSince volume claim templates in StatefulSets are immutable, our first task is\nto detach them so we can modify them as needed. We accomplish this by creating\na new Custom Resource Definition (CRD) called PvcAutoscaler. One PvcAutoscaler\nresource can contain a single volume claim template and a reference to the\nStatefulSet whose PVCs it manages.\n\n    \n    \n    apiVersion: apps/v1 kind: StatefulSet metadata: name: nginx namespace: default spec: serviceName: \"nginx\" selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: registry.Kubernetes.io/nginx:latest ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - apiVersion: v1 kind: PersistentVolumeClaim metadata: name: www spec: accessModes: - ReadWriteOnce resources: requests: storage: 4Gi storageClassName: hdd-sc volumeMode: Filesystem\n\nFigure 1: Splitting out the volume claim templates from the StatefulSet\n\n    \n    \n    apiVersion: apps/v1 kind: StatefulSet metadata: name: nginx namespace: default spec: serviceName: \"nginx\" selector: matchLabels: app: nginx replicas: 1 template: metadata: labels: app: nginx spec: containers: - name: nginx image: registry.Kubernetes.io/nginx:latest ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html --- apiVersion: storage.css.com/v1 kind: PvcAutoscaler metadata: name: nginx namespace: default spec: statefulSetName: nginx volumeClaimTemplate: apiVersion: v1 kind: PersistentVolumeClaim metadata: name: www spec: accessModes: - ReadWriteOnce resources: requests: storage: 4Gi storageClassName: hdd-sc volumeMode: Filesystem\n\nFigure 2: Moving the volume claim templates into PvcAutoscaler resources\n\nNow that the volume claim templates are isolated into PvcAutoscaler resources\nwe can create a Kubernetes operator for the CRD which will generate PVCs for\neach pod in the referenced StatefulSet based off of the embedded volume claim\ntemplate.\n\nFigure 3: PvcAutoscaler becoming responsible for the creation of PVCs for a\nStatefulSet\n\nWe now have PVCs for the pods of a StatefulSet based off of a volume claim\ntemplate embedded in a PvcAutoscaler custom resource. Unfortunately the\nStatefulSet controller is completely unaware of these PVCs and as such cannot\ninclude them in the Pod specs to make them mountable volumes. This brings us\nto our next task.\n\n##\n\nAttaching Volumes\n\nLuckily Kubernetes gives us a tool we can use to ensure PVCs are available as\nvolumes in the pod spec. By adding a mutating webhook targeting the pods of\nStatefulSets referenced by a PvcAutoscaler, we can look up and attach the PVCs\nto a pods\u2019 volumes spec prior to validation and creation.\n\nFigure 4: Mutating pod webhook attaching PvcAutoscaler PVCs to StatefulSet\npods\n\nAfter attaching the PVCs, each pod now has a valid spec with volumeMounts all\nreferencing correctly declared volumes.\n\n##\n\nExpansion Unlocked\n\nAfter all that work we have returned to a functional setup similar to the\nplain StatefulSet. We can define a StatefulSet and a number of PvcAutoscaler\nresources which will manage PVCs and attach them to the pods at creation.\n\nHowever we have unlocked one great benefit from this exercise. We have gained\ncomplete control over the volume claim template embedded in the PvcAutoscaler\nand can decide what limitations we wish to impose, i.e. we can make properties\nmutable.\n\nThe first target for mutability is the .spec.resources.storage.requests\nproperty since our first objective is to allow seamless volume expansion.\nAllowing volume expansion when the underlying volume provisioner natively\nsupports it is trivial - we just propagate the change in requests to all PVCs\nand wait for the provisioner to finish the expansion.\n\nIn the cases where volume expansion is not natively supported, we would need\nto follow a more cumbersome process detailed in the next sections.\n\n##\n\nThe Volume Populator\n\nWhile volume expansion is supported by some volume provisioners, neither\nshrinking volumes nor changing the underlying storage device are in any way\nsupported. To enable these features we introduce a new custom component, the\nvolume-populator.\n\nPVCs have for some time supported the DataSource property, originally intended\nto bootstrap a PVC from a volume snapshot. However by enabling the\nAnyVolumeDataSource gate (which is enabled by default since 1.24) it is\npossible to use the DataSource property to reference any custom resource (CR).\n\nTo leverage this we create a new CRD called the PvcSourcePopulator, which is\nowned by the volume-populator and whose only purpose is to reference the old\nPVC, and can itself be referenced in a new PVC through the DataSource\nproperty.\n\n    \n    \n    apiVersion: storage.css.com/v1 kind: PvcSourcePopulator metadata: name: populator-pvc-0-ba51f namespace: default spec: sourcePvcRef: name: pvc-0-8770e namespace: default --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-0-ba51f namespace: default spec: accessModes: - ReadWriteOnce dataSourceRef: apiGroup: storage.css.com kind: PvcSourcePopulator name: populator-pvc-0-ba51f resources: requests: storage: 2Gi storageClassName: ssd-sc volumeMode: Filesystem\n\nFigure 5: A PvcSourcePopulator CR being referenced by a new PVC with a\ndifferent storage class and smaller requested capacity\n\nThe volume-populator is responsible for transferring the content of the old\nPVC referenced in the PvcSourcePopulator to the new PVC. The process it\nfollows can be summarized in the following bullets:\n\n  1. PVC_new is created with a PvcSourcePopulator data source referencing PVC_old. PVC_new enters a Pending state since the volume provisioner does not recognize the datasource type - this allows us to take over the task of eventually binding a PV to PVC_new\n\n  2. The volume-populator monitors pods in the namespace waiting for the PVC_new to be referenced to ensure the data transfer is executed immediately prior to pod initialization\n\n  3. Once PVC_new is referenced, the volume-populator creates an empty PVC_tmp with identical content to PVC_new, but without a datasource. Without the datasource, the volume provisioner is able to create a PV and bind it to PVC_tmp\n\n  4. The volume-populator spins up a pod which mounts both the PVC_old and PVC_tmp, and transfers content from PVC_old to PVC_tmp\n\n  5. Once transfer completes, the volume-populator unbinds the PV from PVC_tmp and instead binds it to PVC_new\n\n  6. The volume-populator discards PVC_tmp which no longer has a PV bound to it\n\n  7. PVC_new becomes ready as it now has a PV bound and the pod can initialize normally\n\nFigure 6: The volume-populator bootstrapping a new PVC for pod-0\n\n##\n\nJust Change\n\nBy using the volume-populator in the PvcAutoscaler operator we can efficiently\nand safely swap out the PVC to match the desired state of the volume claim\ntemplate. This allows us to support changes to almost all aspects of the\nvolume claim template, including the storage class to change the underlying\nstorage device.\n\nWhen the PvcAutoscaler operator detects drift between the volume claim\ntemplate and the current PVCs of a StatefulSet it will try to determine\nwhether the change can be done via online expansion. If it cannot, it\ninitiates the process of swapping out the PVCs using the volume-populator.\nThis process can be summarized by the following:\n\n  1. Create new PVCs for each pod containing a PvcSourcePopulator DataSource referencing the old PVC\n\n  2. Initiate a rolling restart of the StatefulSet\n\n  3. Upon pod creation the mutating webhook will inject the new PVC\n\n  4. The volume-populator will detect a pod being created with a reference to a PVC with a PvcSourcePopulator DataSource\n\n  5. The volume-populator will transfer the data and once it completes the pod will start up normally\n\n  6. This process is repeated for each pod in the StatefulSet\n\nFigure 7: Swapping out the PVCs by using the volume-populator to transfer all\ndata from the previous PVC\n\nOnce all the pods have been restarted and had their PVC bootstrapped, the\nStatefulSet re-enters a steady state. After a pre-configured retention period\nthe PvcAutoscaler will automatically delete the old PVCs to reclaim any cost\nassociated.\n\nThe capabilities of the PvcAutoscaler allows us to ignore future requirements\nfor a stateful workload's storage characteristics, and focus on the current\nneed. We can start with a small HDD, confident that we can boost it to SSD if\nthe need arises. We can add on extra capacity, knowing that we can scale-in\nagain to save on cost later. All on-demand, declarative, and toil-free.\n\n#\n\nNext Steps\n\nThe current solution is quite robust and battle tested, and has drastically\nimproved the cost efficiency of our stateful workloads, but there is always\nroom for improvement. In the future we\u2019re likely to explore the following\nareas.\n\n  * Actual auto-scaling to PvcAutoscaler allowing automated execution of PVC modifications based off of declared sizing strategies and disk metrics\n\n  * Support for stateful workloads not using StatefulSets but are instead running using a custom pod controller\n\n  * Hot copying of data to reduce the time it takes to scale down disks and switch storage types\n\n#\n\nConclusion\n\nThe key takeaway we\u2019ve taken to heart from developing the PvcAutoscaler is\nthat Kubernetes is immensely extensible. With enough insight into the\necosystem it is possible to extend even core components with new features and\nremove otherwise hard limitations. This ability to extend kubernetes is\nespecially relevant for stateful workloads which tend to have more specific\nrequirements and limitations compared to stateless workloads.\n\nOverall the PvcAutoscaler has provided great value since it was rolled out.\nWhile the original focus was on reducing cost by eliminating wasteful over-\nprovisioning of storage capacity, the PvcAutoscaler has also enabled us to\neasily move workloads onto more performant disks when usage reached IOPS and\nthroughput limits.\n\nWithout the PvcAutoscaler we would have been forced to expend a lot more time\nand resources to right-size the storage of our stateful workloads as consumer\nworkload usage patterns change over time. In many instances the effort\nrequired to scale in storage would not be worth the cost reduction, leading to\nan ever increasing overhead. But the PvcAutoscaler makes even small cuts in\nstorage worthwhile.\n\n### Subscribe to City Storage Systems\n\nBy CSS Engineering \u00b7 Launched a month ago\n\nEngineering the future of food at Otter & CloudKitchens\n\n1 Like\n\n1\n\nShare this post\n\n#### Swapping Disks in Kubernetes for Fun and Profit\n\ntechblog.citystoragesystems.com\n\nShare\n\nManaging 100s of Kubernetes Clusters using Cluster API\n\nAutomating every step from cluster creation to workload-ready. Turtles all the\nway down.\n\nMar 26\n\n8\n\nShare this post\n\n#### Managing 100s of Kubernetes Clusters using Cluster API\n\ntechblog.citystoragesystems.com\n\nMoving Millions of Orders with Robotic Conveyance\n\nReducing conveyance time with predictive dispatch and multi-order pickup\n\nMar 15\n\n21\n\nShare this post\n\n#### Moving Millions of Orders with Robotic Conveyance\n\ntechblog.citystoragesystems.com\n\nWhy Our Food Prep Time Prediction Works Better\n\nOur prediction model improves upon estimates from delivery companies by\nleveraging additional prep state transitions.\n\nMar 19\n\n15\n\nShare this post\n\n#### Why Our Food Prep Time Prediction Works Better\n\ntechblog.citystoragesystems.com\n\nReady for more?\n\n\u00a9 2024 City Storage Systems\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n", "frontpage": false}
