{"aid": "40235034", "title": "Othello-GPT Has a Linear Emergent World Representation (2023)", "url": "https://www.neelnanda.io/mechanistic-interpretability/othello", "domain": "neelnanda.io", "votes": 1, "user": "reqo", "posted_at": "2024-05-02 11:48:27", "comments": 0, "source_title": "Actually, Othello-GPT Has A Linear Emergent World Representation", "source_text": "Actually, Othello-GPT Has A Linear Emergent World Representation \u2014 Neel Nanda\n\n0\n\nSkip to Content\n\nNeel Nanda\n\nNeel Nanda\n\n# Actually, Othello-GPT Has A Linear Emergent World Representation\n\nMar 28\n\nWritten By Neel Nanda\n\n# Othello-GPT\n\nEpistemic Status: This is a write-up of an experiment in speedrunning\nresearch, and the core results represent ~20 hours/2.5 days of work (though\nthe write-up took way longer). I'm confident in the main results to the level\nof \"hot damn, check out this graph\", but likely have errors in some of the\nfiner details.\n\nDisclaimer: This is a write-up of a personal project, and does not represent\nthe opinions or work of my employer\n\nThis post may get heavy on jargon. I recommend looking up unfamiliar terms in\nmy mechanistic interpretability explainer\n\nThanks to Chris Olah, Martin Wattenberg, David Bau and Kenneth Li for valuable\ncomments and advice on this work, and especially to Kenneth for open sourcing\nthe model weights, dataset and codebase, without which this project wouldn't\nhave been possible!\n\n## Overview\n\n  * Context: A recent paper trained a model to play legal moves in Othello by predicting the next move, and found that it had spontaneously learned to compute the full board state - an emergent world representation.\n\n    * This could be recovered by non-linear probes but not linear probes.\n    * We can causally intervene on this representation to predictably change model outputs, so it's telling us something real\n  * I find that actually, there's a linear representation of the board state!\n\n    * But that rather than \"this cell is black\", it represents \"this cell has my colour\", since the model plays both black and white moves.\n    * We can causally intervene with the linear probe, and the model makes legal moves in the new board!\n  * This is evidence for the linear representation hypothesis: that models, in general, compute features and represent them linearly, as directions in space! (If they don't, mechanistic interpretability would be way harder)\n\n    * The original paper seemed at first like significant evidence for a non-linear representation - the finding of a linear representation hiding underneath shows the real predictive power of this hypothesis!\n  * This (slightly) strengthens the paper's evidence that \"predict the next token\" transformer models are capable of learning a model of the world.\n  * Part 2: There's a lot of fascinating questions left to answer about Othello-GPT - I outline some key directions, and how they fit into my bigger picture of mech interp progress\n\n    * Studying modular circuits: A world model implies emergent modularity - many early circuits together compute a single world model, many late circuits each use it. What can we learn about what transformer modularity looks like, and how to reverse-engineer it?\n\n      * Prior transformer circuits work focuses on end-to-end circuits, from the input tokens to output logits. But this seems unlikely to scale!\n      * I present some preliminary evidence reading off a neuron's function from its input weights via the probe\n    * Neuron interpretability and Studying Superposition: Prior work has made little progress on understanding MLP neurons. I think Othello GPT's neurons are tractable to understand, yet complex enough to teach us a lot!\n\n      * I further think this can help us get some empirical data about the Toy Models of Superposition paper's predictions\n      * I investigate max activating dataset examples and find seeming monosemanticity, yet deeper investigation show it seems more complex.\n    * A transformer circuit laboratory: More broadly, the field has a tension between studying clean, tractable yet over-simplistic toy models and studying the real yet messy problem of interpreting LLMs - Othello-GPT is toy enough to be tractable yet complex enough to be full of mysteries, and I detail many more confusions and conjectures that it could shed light on.\n  * Part 3: Reflections on the research process\n\n    * I did the bulk of this project in a weekend (~20 hours total), as a (shockingly successful!) experiment in speed-running mech interp research.\n    * I give a detailed account of my actual research process: how I got started, what confusing intermediate results look like, and decisions made at each point\n    * I give some process-level takeaways on doing research well and fast.\n  * See the accompanying colab notebook and codebase to build on the many dangling threads!\n\n## Table of Contents\n\n  * Othello-GPT\n\n    * Overview\n    * Table of Contents\n    * Introduction\n    * Background\n\n      * Naive Implications for Mechanistic Interpretability\n    * My Findings\n    * Takeaways\n\n      * How do models represent features?\n      * Conceptual Takeaways\n    * Probing\n\n      * Technical Setup\n      * Results\n      * Intervening\n  * Future work I am excited about\n\n    * Why and when to work on toy models\n\n      * This is not about world models\n    * Finding Modular Circuits\n\n      * Preliminary Results On Modular Circuits\n    * Neuron Interpretability and Studying Superposition\n\n      * Empirically Testing Toy Models of Superposition\n      * Preliminary Results On Neuron Interpretability\n\n        * Case Study: Neurons and Probes are Confusing\n    * A Transformer Circuit Laboratory\n    * Where to start?\n\n      * Concrete starter projects\n      * Cleaning Up\n  * The Research Process\n\n    * Takeaways on doing mech interp research\n    * Getting Started\n    * Patching\n\n      * Tangent on Analysing Neurons\n      * Back to patching\n    * Neuron L5N1393\n    * Citation Info\n\n## Introduction\n\nThis piece spends a while on discussion, context and takeaways. If you're\nfamiliar with the paper skip to my findings, skip to takeaways for my updates\nfrom this, and if you want technical results skip to probing\n\nEmergent World Representations is a fascinating recent ICLR Oral paper from\nKenneth Li et al, summarised in Kenneth's excellent post on the Gradient. They\ntrained a model (Othello-GPT) to play legal moves in the board game Othello,\nby giving it random games (generated by choosing a legal next move uniformly\nat random) and training it to predict the next move. The headline result is\nthat Othello-GPT learns an emergent world representation - despite never being\nexplicitly given the state of the board, and just being tasked to predict the\nnext move, it learns to compute the state of the board at each move. (Note\nthat the point of Othello-GPT is to play legal moves, not good moves, though\nthey also study a model trained to play good moves.)\n\nThey present two main pieces of evidence. They can extract the board state\nfrom the model's residual stream via non-linear probes (a two layer ReLU MLP).\nAnd they can use the probes to causally intervene and change the model's\nrepresentation of the board (by using gradient descent to have the probes\noutput the new board state) - the model now makes legal moves in the new board\nstate even if they are not legal in the old board, and even if that board\nstate is impossible to reach by legal play!\n\nI've strengthened their headline result by finding that much of their more\nsophisticated (and thus potentially misleading) techniques can be\nsignificantly simplified. Not only does the model learn an emergent world\nrepresentation, it learns a linear emergent world representation, which can be\ncausally intervened on in a linear way! But rather than representing \"this\nsquare has a black/white piece\", it represents \"this square has my/their\npiece\". The model plays both black and white moves, so this is far more\nnatural from its perspective. With this insight, the whole picture clarifies\nsignificantly, and the model becomes far more interpretable!\n\n## Background\n\nFor those unfamiliar, Othello is a board game analogous to chess or go, with\ntwo players, black and white, see the rules outlined in the figure below. I\nfound playing the AI on eOthello helpful for building intuition. A single move\ncan change the colour of pieces far away (so long as there's a continuous\nvertical, horizontal or diagonal line), which means that calculating board\nstate is actually pretty hard! (to my eyes much harder than in chess)\n\nBut despite the model just needing to predict the next move, it spontaneously\nlearned to compute the full board state at each move - a fascinating result. A\npretty hot question right now is whether LLMs are just bundles of statistical\ncorrelations or have some real understanding and computation! This gives\nsuggestive evidence that simple objectives to predict the next token can\ncreate rich emergent structure (at least in the toy setting of Othello).\nRather than just learning surface level statistics about the distribution of\nmoves, it learned to model the underlying process that generated that data. In\nmy opinion, it's already pretty obvious that transformers can do something\nmore than statistical correlations and pattern matching, see eg induction\nheads, but it's great to have clearer evidence of fully-fledged world models!\n\nFor context on my investigation, it's worth analysing exactly the two pieces\nof evidence they had for the emergent world representation, the probes and the\ncausal interventions, and their strengths and weaknesses.\n\nThe probes give suggestive, but far from conclusive evidence. When training a\nprobe to extract some feature from a model, it's easy to trick yourself. It's\ncrucial to track whether the probe is just reading out the feature, or\nactually computing the feature itself, and reading out much simpler features\nfrom the model. In the extreme case, you could attach a much more powerful\nmodel as your \"probe\", and have it just extract the input moves, and then\ncompute the board state from scratch! They found that linear probes did not\nwork to recover board state (with an error rate of 20.4%): (ie, projecting the\nresidual stream onto some 3 learned directions for each square, corresponding\nto empty, black and white logits). While the simplest non-linear probes (a two\nlayer MLP with a single hidden ReLU layer) worked extremely well (an error\nrate of 1.7%). Further (as described in their table 2, screenshot below),\nthese non-linear probes did not work on a randomly initialised network, and\nworked better on some layers than others, suggesting they were learning\nsomething real from the model.\n\nProbes on their own can mislead, and don't necessarily tell us that the model\nuses this representation - the probe could be extracting some vestigial\nfeatures or a side effect of some more useful computation, and give a\nmisleading picture of how the model computes the solution. But their causal\ninterventions make this much more compelling evidence. They intervene by a\nfairly convoluted process (detailed in the figure below, though you don't need\nto understand the details), which boils down to choosing a new board state,\nand applying gradient descend to the model's residual stream such that our\nprobe thinks the model's residual stream represents the new board state. I\nhave an immediate skepticism of any complex technique like this: when applying\na powerful method like gradient descent it's so easy to wildly diverge from\nwhat the models original functioning is like! But the fact that the model\ncould do the non-trivial computation of converting an edited board state into\na legal move post-edit is a very impressive result! I consider it very strong\nevidence both that the probe has discovered something real, and that the\nrepresentation found by the probe is causally linked to the model's actual\ncomputation!\n\n### Naive Implications for Mechanistic Interpretability\n\nI was very interested in this paper, because it simultaneously had the\nfascinating finding of an emergent world model (and I'm also generally into\nany good interp paper), yet something felt off. The techniques used here\nseemed \"too\" powerful. The results were strong enough that something here\nseemed clearly real, but my intuition is that if you've really understood a\nmodel's internals, you should just be able to understand and manipulate it\nwith far simpler techniques, like linear probes and interventions, and it's\neasy to be misled by more powerful techniques.\n\nIn particular, my best guess about model internals is that the networks form\ndecomposable, linear representations: that the model computes a bunch of\nuseful features, and represents these as directions in activation space. See\nToy Models of Superposition for some excellent exposition on this. This is\ndecomposable because each feature can vary independently (from the perspective\nof the model - on the data distribution they're likely dependent), and linear\nbecause we can extract a feature by projecting onto that feature's direction\n(if the features are orthogonal - if we have something like superposition it's\nmessier). This is a natural way for models to work - they're fundamentally a\nseries of matrix multiplications with some non-linearities stuck in convenient\nplaces, and a decomposable, linear representation allows it to extract any\ncombination of features with a linear map!\n\nUnder this framework, if a feature can be found by a linear probe then the\nmodel has already computed it, and if that feature is used in a circuit\ndownstream, we should be able to causally intervene with a linear\nintervention, just changing the coordinate along that feature's direction. So\nthe fascinating finding that linear probes do not work, but non-linear probes\ndo, suggests that either the model has a fundamentally non-linear\nrepresentation of features (which it is capable of using directly for\ndownstream computation!), or there's a linear representation of simpler and\nmore natural features, from which the probe computes board state. My prior was\non a linear representation of simpler features, but the causal intervention\nfindings felt like moderate evidence for the non-linear representation. And\nthe non-linear representation hypothesis would be a big deal if true! If you\nwant to reverse-engineer a model, you need to have a crisp picture of how its\ncomputation maps onto activations and weights, and this would break a lot of\nmy beliefs about how this correspondance works! Further, linear\nrepresentations are just really convenient to reverse-engineer, and this would\nmake me notably more pessimistic about mechanistic interpretability working.\n\n## My Findings\n\nI'm of the opinion that the best way to become less confused about a\nmysterious model behaviour is to mechanistically analyse it. To zoom in on\nwhatever features and circuits we can find, build our understanding from the\nbottom up, and use this to form grounded beliefs about what's actually going\non. This was the source of my investigation into grokking, and I wanted to\napply it here. I started by trying activation patching and looking for\ninterpretable circuits/neurons, and I noticed a motif whereby some neurons\nwould fire every other move, but with different parity each game. Digging\nfurther, I stumbled upon neuron 1393 in layer 5, which seemed to learn\n(D1==white) AND (E2==black) on odd moves, and (D1==black) AND (E2==white) on\neven moves.\n\nGeneralising from this motif, I found that, in fact, the model does learn a\nlinear representation of board state! But rather than having a direction\nsaying eg \"square F5 has a black counter\" it says \"square F5 has one of my\ncounters\". In hindsight, thinking in terms of my vs their colour makes far\nmore sense from the model's perspective - it's playing both black and white,\nand the valid moves for black become valid moves for white if you flip every\npiece's colour! (I've since this same observation in Haoxing Du's analysis of\nGo playing models)\n\nIf you train a linear probe on just odd/even moves (ie with black/white to\nplay) then it gets near perfect accuracy! And it transfers reasonably well to\nthe other moves, if you flip its output.\n\nI speculate that their non-linear probe just learned to extract the two\nfeatures of \"I am playing white\" and \"this square has my colour\" and to do an\nXOR of those. Fascinatingly, without the insight to flip every other\nrepresentation, this is a pathological example for linear probes - the\nrepresentation flips positive to negative every time, so it's impossible to\nrecover the true linear structure!\n\nAnd we can use our probe to causally intervene on the model. The first thing I\ntried was just negating the coordinate in the direction given by the probe for\na square (on the residual stream after layer 4, with no further intervention),\nand it just worked - see the figure below! Note that I consider this the\nweakest part of my investigation - on further attempts it needs some hyper-\nparameter fiddling and is imperfect, discussed later, and I've only looked at\ncase studies rather than a systematic benchmark.\n\nThis project was an experiment in speed-running mech interp research, and I\ngot all of the main results in this post over a weekend (~2.5 days/20 hours).\nI am very satisfied with the results of this experiment! I discuss some of my\nprocess-level takeaways, and try to outline the underlying research process in\na pedagogical way - how I got started, how I got traction on the problem, and\nwhat the compelling intermediate results looked like.\n\nI also found a lot of tantalising hints of deeper structure inside the model!\nFor example, we can use this probe to interpret input and output weights of\nneurons, eg Neuron 1393 in Layer 5 which seems to represent (C0==blank) AND\n(D1==theirs) AND (E2==mine) (we convert the probe to two directions, blank -\n0.5 * my - 0.5 * their, and my - their)\n\nOr, if we look at the top 1% of dataset examples for some layer 4 neurons and\nlook at the frequency by which a square is non-empty, many seem to activate\nwhen a specific square is empty! (But some neighbours are present)\n\nI haven't looked hard into these, but I think there's a lot of exciting\ndirections to better understand this model, that I outline in future work. An\nangle I'm particularly excited about here is moving beyond just studying \"end-\nto-end\" transformer circuits - existing work (eg indirect object\nidentification or induction heads) tends to focus on a circuit that goes from\nthe input tokens to the output logits, because it's much easier to interpret\nthe inputs and outputs than any point in the middle! But our probe can act as\na \"checkpoint\" in the middle - we understand what the probe's directions mean,\nand we can use this to find early circuits mapping the input moves to compute\nthe world model given by the probe, and late circuits mapping the world model\nto the output logits!\n\nMore generally, the level of traction I've gotten suggests there's a lot of\nlow hanging fruit here! I think this model could serve as an excellent\nlaboratory to test other confusions and claims about models - it's\nsimultaneously clean and algorithmic enough to be tractable, yet large and\ncomplex enough to be exciting and less toy. Can we find evidence of\nsuperposition? Can we find monosemantic neurons? Are all neurons monosemantic,\nor can we find and study polysemanticity and superposition in the wild? How do\ndifferent neuron activations (GELU, SoLU, SwiGLU, etc) affect\ninterpretability? More generally, what kinds of circuits can we find?!\n\n## Takeaways\n\n### How do models represent features?\n\nMy most important takeaway is that this gives moderate evidence for models, in\npractice, learning decomposable, linear representations! (And I am very glad\nthat I don't need to throw away my frameworks for thinking about models.) Part\nof the purpose of writing such a long background section is to illustrate that\nthis was genuinely in doubt! The fact that the original paper needed non-\nlinear probes, yet could causally intervene via the probes, seemed to suggest\na genuinely non-linear representation, and this could have gone either way.\nBut I now know (and it may feel obvious in hindsight) that it was linear.\n\nAs further evidence that this was genuinely in doubt, I've since become aware\nof an independent discussion between Chris Olah and Martin Wattenberg (an\nauthor of the paper), where I gather that Chris pre-registered the prediction\nthat the probe was doing computation on an underlying linear representation,\nwhile Martin thought the model learned a genuinely non-linear representation.\n\nModels are complex and we aren't (yet!) very good at reverse-engineering them,\nwhich makes evidence for how best to think about them sparse and speculative.\nOne of the best things we have to work with is toy models that are complex\nenough that we don't know in advance what gradient descent will learn, yet\nsimple enough that we can in practice reverse-engineer them, and Othello-GPT\nformed an unexpectedly pure natural experiment!\n\n### Conceptual Takeaways\n\nA further smattering of conceptual takeaways I have about mech interp from\nthis work - these are fairly speculative, and are mostly just slight updates\nto beliefs I already held, but hopefully of interest!\n\nAn obvious caveat to all of the below is that this is preliminary work on a\ntoy model, and generalising to language models is speculative - Othello is a\nfar simpler environment than language/the real world, a far smaller state\nspace, Othello-GPT is likely over-parametrised for good performance on this\ntask while language models are always under-parametetrised, and there's a\nground truth solution to the task. I think extrapolation like this is better\nthan nothing, but there are many disanalogies and it's easy to be\noverconfident!\n\n  * Mech interp for science of deep learning: A motivating belief for my grokking work is that mechanistic interpretability should be a valuable tool for the science of deep learning. If our claims about truly reverse-engineering models are true, then the mech interp toolkit should give grounded and true beliefs about models. So when we encounter mysterious behaviour in a model, mechanistic analysis should de-mystify it!\n\n    * I feel validated in this belief by the traction I got on grokking, and I feel further validated here!\n  * Mech interp == alien neuroscience: A pithy way to describe mech interp is as understanding the brain of an alien organism, but this feels surprisingly validated here! The model was alien and unintuitive, in that I needed to think in terms of my colour vs their colour, not black vs white, but once I'd found this new perspective it all became far clearer and more interpretable.\n\n    * Similar to how modular addition made way more sense when I started thinking in Fourier Transforms!\n  * Models can be deeply understood: More fundamentally, this is further evidence that neural networks are genuinely understandable and interpretable, if we can just learn to speak their language. And it makes me mildly more optimistic that narrow investigations into circuits can uncover the underlying principles that will make model internals make sense\n\n    * Further, it's evidence that as you start to really understand a model, mysteries start to dissolve, and it becomes far easier to control and edit - we went from needing to do gradient descent against a non-linear probe to just changing the coordinate along a single direction at a single activation.\n  * Probing is surprisingly legit: As noted, I'm skeptical by default about any attempt to understand model internals, especially without evidence from a mechanistically understood case study!\n\n    * Probing, on the face of it, seems like an exciting approach to understand what models really represent, but is rife with conceptual issues:\n\n      * Is the probe computing the feature, or is the model?\n      * Is the feature causally used/deliberately computed, or just an accident?\n      * Even if the feature does get deliberately computed and used, have we found where the feature is first computed, or did we find downstream features computed from it (and thus correlated with it)\n    * I was pleasantly surprised by how well linear probes worked here! I just did naive logistic regression (using AdamW to minimise cross-entropy loss) and none of these issues came up, even though eg some squares had pretty imbalanced class labels.\n    * In particular, even though it later turned out that the board state was fully computed by layer 4, and I trained my probe on layer 6, it still picked up on the correct features (allowing intervention at layer 4) - despite the board state being used by layers 5 and 6 to compute downstream features!\n  * Dropout => redundancy: Othello-GPT was, alas trained with attention and residual dropout (because it was built on the MinGPT codebase, which was inspired by GPT-2, which used them). Similar to the backup name movers in GPT-2 Small, I found some suggestive evidence of redundancy built into the model - in particular, the final MLP layer seemed to contribute negatively to a particular logit, but would reduce this to compensate when I patched some model internal.\n  * Basic techniques just kinda worked?: The main tools I used in this investigation, activation patching, direct logit attribution and max activating dataset examples, basically just worked. I didn't probe hard enough to be confident they didn't mislead me at all, but they all seemed to give me genuinely useful data and hints about model internals.\n  * Residual models are ensembles of shallow paths: Further evidence that the residual stream is the central object of a transformer, and the meaningful paths of computation tend not to go through every layer, but heavily use the skip connections. This one is more speculative, but I often noticed that eg layer 3 and layer 4 did similar things, and layer 5 and layer 6 neurons did similar things. (Though I'm not confident there weren't subtle interactions, especially re dropout!)\n  * Can LLMs understand things?: A major source of excitement about the original Othello paper was that it showed a predict-the-next-token model spotaneously learning the underlying structure generating its data - the obvious inference is that a large language model, trained to predict the next token in natural language, may spontaneously learn to model the world. To the degree that you took the original paper as evidence for this, I think that my results strengthen the original paper's claims, including as evidence for this!\n\n    * My personal take is that LLMs obviously learn something more than just statistical correlations, and that this should be pretty obvious from interacting with them! (And finding actual inference-time algorithms like induction heads just reinforces this). But I'm not sure how much the paper is a meaningful update for what actually happens in practice.\n    * Literally the only thing Othello-GPT cares about is playing legal moves, and having a representation of the board is valuable for that, so it makes sense that it'd get a lot of investment (having 128 probe directions gets you). But likely a bunch of dumb heuristics would be much cheaper and work OK for much worse performance - we see that the model trained to be good at Othello seems to have a much worse world model.\n\n      * Further, computing the board state is way harder than it seems at first glance! If I coded up an Othello bot, I'd have it compute the board state iteratively, updating after each move. But transformers are built to do parallel, not serial processing - they can't recurse! In just 5 blocks, it needs to simultaneously compute the board state at every position (I'm very curious how it does this!)\n      * And taking up 2 dimensions per square consumes 128 of the residual stream's 512 dimensions (ignoring any intermediate terms), a major investment!\n    * For an LLM, it seems clear that it can learn some kind of world model if it really wants to, and this paper demonstrates that principle convincingly. And it's plausible to me that for any task where a world model would help, a sufficiently large LLM will learn the relevant world model, to get that extra shred of recovered loss. But this is a fundamentally empirical question, and I'd love to see data studying real models!\n\n      * Note further that if an LLM does learn a world model, it's likely just one circuit among many and thus hard to reliably detect - I'm sure it'll be easy to generate gotchas where the LLM violates what that world model says, if only because the LLM wants to predict the next token, and it's easy to cue it to use another circuit. There's been some recent Twitter buzz about Bing Chat playing legal chess moves, and I'm personally pretty agnostic about whether it has a real model of a chess board - it seems hard to say either way (especially when models are using chain of thought for some basic recursion!).\n      * One of my hopes is that once we get good enough at mech interp, we'll be able to make confident statements about what's actually going on in situations like this!\n\n## Probing\n\n### Technical Setup\n\nI use the synthetic model from their paper, and you can check out that and\ntheir codebase for the technical details. In brief, it's an 8 layer GPT-2\nmodel, trained on a synthetic dataset of Othello games to predict the next\nmove. The games are length 60, it receives the first 59 moves as input (ie\n[0:-1]) and it predicts the final 59 moves (ie [1:]). It's trained with\nattention dropout and residual dropout. The model has vocab size 61 - one for\neach square on the board (1 to 60), apart from the four center squares that\nare filled at the start and thus unplayable, plus a special token (0) for\npassing.\n\nI trained my probe on four million synthetic games (though way fewer would\nsuffice), you can see the training code in tl_probing_v1.py in my repo. I\ntrained a separate probe on even, odd and all moves. I only trained my probe\non moves [5:-5] because the model seemed to do weirder things on early or late\nmoves (eg the residual stream on the first move has ~20x the norm of every\nother one!) and I didn't want to deal with that. I trained them to minimise\nthe cross-entropy loss for predicting empty, black and white, and used AdamW\nwith lr=1e-4, weight_decay=1e-2, eps=1e-8, betas=(0.9, 0.99). I trained the\nprobe on the residual stream after layer 6 (ie get_act_name(\"resid_post\", 6)\nin TransformerLens notation). In hindsight, I should have trained on layer 6,\nwhich is the point where the board state is fully computed and starts to\nreally be used. Note that I believe the original paper trained on the full\ngame (including early and late moves), so my task is somewhat easier than\ntheir's.\n\nFor each square, each probe has 3 directions, one for blank, black and for\nwhite. I convert it to two directions: a \"my\" direction by taking my_probe =\nblack_dir - white_dir (for black to play) and a \"blank\" direction by taking\nblank_probe = blank_dir - 0.5 * black_dir - 0.5 * white_dir (the last one\nisn't that principled, but it seemed to work fine) (you can throw away the\nthird dimension, since softmax is translation invariant). I then normalise\nthem to be unit vectors (since the norm doesn't matter - it just affects\nconfidence in the probe's logits, which affects loss but not accuracy). I just\ndid this for the black to play probe, and used these as my meaningful\ndirections (this was somewhat hacky, but worked!)\n\n### Results\n\nThe probe works pretty great for layer 6! And odd (black to play) transfers\nfairly wel zero shot to even (white to play) by just swapping what mine and\nyour's means (with worse accuracy on the corners). (This is the accuracy taken\nover 100 games, so 5000 moves, only scored on the middle band of moves)\n\nFurther, if you flip either probe, it transfers well to the other side's\nmoves, and the odd and even probes are nearly negations of each other. We\nconvert a probe to a direction by taking the difference between the black\ndirection and white direction. (In hindsight, it'd have made been cleaner to\ntrain a single probe on all moves, flipped the labels for black to play vs\nwhite to play)\n\nIt actually transfers zero-shot to other layers - it's pretty great at layer 4\ntoo (but isn't as good at layer 3 or layer 7):\n\n### Intervening\n\nMy intervention results are mostly a series of case studies, and I think are\nless compelling and rigorous than the rest, but are strong enough that I buy\nthem! (I couldn't come up with a principled way of evaluating this at scale,\nand I didn't have much time left). The following aren't cherry picked -\nthey're just the first few things I tried, and all of them kinda worked!\n\nTo intervene, I took the model's residual stream after layer 4 (or layer 3),\ntook the coordinate when projecting onto my_probe, and negated that and\nmultiplied by the hyper-parameter scale (which varied from 0 to 16).\n\nMy first experiment had layer 4 and scale 1 (ie just negating) and worked\npretty well:\n\nSubsequent experiments showed that the scale parameter mattered a fair bit - I\nspeculate that if I instead looked at the absolute coefficient of the\ncoordinate it'd work better.\n\nOn the first case where it didn't really work, I got good results by\nintervening at layer 3 instead - evidence that model processing isn't\nperfectly divided by layer, but somewhat spreads across adjacent layers when\nit can get away with it.\n\nIt seems to somewhat work for multiple edits - if I flip F5 and F6 in the\nabove game to make G6 illegal, it kinda realises this, though is a weaker\neffect and is jankier and more fragile:\n\nNote that my edits do not perfectly recover performance - the newly legal\nlogits tend to not be quite as large as the originally legal logits. To me\nthis doesn't feel like a big deal, here's some takes on why this is fine:\n\n  * I really haven't tried to improve edit performance, and expect there's low hanging fruit to be had. Eg, I train the probe on layer 6 rather than layer 4, and I train on black and white moves separately rather than on both at once. And I am purely scaling the existing coordinate in this direction, rather than looking at its absolute value.\n  * Log probs cluster strongly on an unedited game - correct log probs are near exactly the same (around -2 for these games - uniform probability), incorrect log probs tend to be around -11. So even if I get from -11 to -4, that's a major impact\n  * I expect parallel model computation to be split across layers - in theory the model could have mostly computed board state by layer 3, use that partial result in layer 4 and finish computing it in layer 4, and use the full result later. If so, then we can't expect to get a perfect model edit.\n  * A final reason is that this model was trained with dropout, which makes everything (especially anything to do with model editing) messy. The model has built in redundancy, and likely doesn't have exactly one dimension per feature. (This makes anything to do with patching or editing a bit suspect and unpredictable, unfortunately)\n\n# Future work I am excited about\n\nThe above sections leaves me (and hopefully you!) pretty convinced that I've\nfound something real and dissolved the mystery of whether there's a linear vs\nnon-linear representation. But I think there's a lot of exciting mysteries\nleft to uncover in Othello-GPT, and that doing so may be a promising way to\nget better at reverse-engineering LLMs (the goal I actually care about). In\nthe following sections, I try to:\n\n  * Justify why I think further work on Othello-GPT is interesting\n\n    * (Note that my research goal here is to get better at transformer mech interp, not to specifically understand emergent world models better)\n  * Discuss how this unlocks finding modular circuits, and some preliminary results\n\n    * Rather than purely studying circuits mapping input tokens to output logits (like basically all prior transformer circuits work), using the probe we can study circuits mapping the input tokens to the world model, and the world model to the output logits - the difference between thinking of a program as a massive block of code vs being split into functions and modules.\n    * If we want to reverse-engineer large models, I think we need to get good at this!\n  * Discuss how we can interpret Othello-GPT's neurons - we're very bad at interpreting transformer MLP neurons, and I think that Othello-GPT's are simple enough to be tractable yet complex enough to teach us something!\n  * Discuss how, more broadly, Othello-GPT can act as a laboratory to get data on many other questions in transformer circuits - it's simple enough to have a ground truth, yet complex enough to be interesting\n\nMy hope is that some people reading this are interested enough to actually try\nworking on these problems, and I end this section with advice on where to\nstart.\n\n## Why and when to work on toy models\n\nThis is a long and rambly section about my research philosophy of mech interp,\nand you should feel free to move on to the next section if that's not your jam\n\nAt first glance, playing legal moves in Othello (not even playing good moves!)\nhas nothing to do with language models, and I think this is a strong claim\nworth justifying. Can working on toy tasks like Othello-GPT really help us to\nreverse-engineer LLMs like GPT-4? I'm not sure! But I think it's a plausible\nbet worth making.\n\nTo walk through my reasoning, it's worth first thinking on what's holding us\nback - why haven't we already reverse-engineered the most capable models out\nthere? I'd personally point to a few key factors (though note that this is my\npersonal hot take, is not comprehensive, and I'm sure other researchers have\ntheir own views!):\n\n  * Conceptual frameworks: To reverse-engineer a transformer, you need to know how to think like a transformer. Questions like: What kinds of algorithms is it natural for a transformer to represent, and how? Are features and circuits the right way to think about it? Is it even reasonable to expect that reverse-engineering is possible? How can we tell if a hypothesis or technique is principled vs hopelessly confused? What does it even mean to have truly identified a feature or circuit?\n\n    * I personally thought A Mathematical Framework significantly clarified my conceptual frameworks for transformer circuits!\n    * This blog post is fundamentally motivated by forming better conceptual frameworks - do models form linear representations?\n  * Practical Knowledge/Techniques: Understanding models is hard, and being able to do this in practice is hard. Getting better at this both looks like forming a better toolkit of techniques that help us form true beliefs about models, and also just having a bunch of practical experience with finding circuits and refining the tools - can we find any cases where they break? How can we best interpret the results?\n\n    * A concrete way this is hard is that models contain many circuits, each of which only activates on certain inputs. To identify a circuit we must first identify where it is and what it does, out of the morass! Activation patching (used in ROME, Interpretability in the Wild and refined with causal scrubbing) is an important innovation here.\n  * Understanding MLP Layers: 2/3 of the parameters in transformers are in MLP layers, which process the information collected at each token position. We're pretty bad understanding them, and getting better at this is vital!\n\n    * We think these layers represent features as directions in space, and if each neuron represents a single feature, we're pretty good! But in practice this seems to be false, because of the poorly understood phenomena of superposition and polysemanticity\n    * Toy Models of Superposition helped clarify my conceptual frameworks re superposition, but there's still a lot more to de-confuse! And a lot of work to do to form the techniques to deal with it in practice. I'm still not aware of a single satisfying example of really understanding a circuit involving MLPs in a language model\n  * Scalability: LLMs are big, and getting bigger all the time. Even if we solve all of the above in eg four layer transformers, this could easily involve some very ad-hoc and labour intensive techniques. Will this transfer to models far larger? And how well do the conceptual frameworks we form transformer - do they just break on models that are much more complex?\n\n    * This often overlaps with forming techniques (eg, causal scrubbing is an automated algorithm with the potential to scale, modulo figuring out many efficiency and implementation details). But broadly I don't see much work on this publicly, and would be excited to see more - in particular, checking how well our conceptual frameworks transfer, and whether all the work on small models is a bit of a waste of time!\n\n      * My personal hot take is that I'm more concerned about never getting really good at interpreting a four layer model, than about scaling if we're really good at four layer models - both because I just feel pretty confused about even small models, and because taking understood yet labour-intensive techniques and making them faster and more automatable seems hard but doable (especially with near-AGI systems!). But this is a complex empirical question and I could easily be wrong.\n\nWithin this worldview, what should our research goals be? Fundamentally, I'm\nan empiricist - models are hard and confusing, it's easy to trick yourself,\nand often intuitions can mislead. The core thing of any research project is\ngetting feedback from reality, and using it to form true beliefs about models.\nThis can either look like forming explicit hypotheses and testing them, or\nexploring a model and seeing what you stumble upon, but the fundamental\nquestion is whether you have the potential to be surprised and to get feedback\nfrom reality.\n\nThis means that any project is a trade-off between tractability and relevance\nto the end goal. Studying toy, algorithmic models is a double edged sword.\nThey can be very tractable: they're clean and algorithmic which incentivises\nclean circuits, there's an available ground truth for what the model should be\ndoing, and they're often in a simple and nicely constrained domain. But it's\nextremely easy for them to cease to be relevant to real LLMs and become a\nnerd-snipe. (Eg, I personally spent a while working on grokking, and while\nthis was very fun, I think it's just not very relevant to LLMs)\n\nIt's pretty hard to do research by constantly checking whether you're being\nnerd-sniped, and to me there are two natural solutions:\n\n  * (1) To pick a concrete question you care about in language models, and to set out to specifically answer that, in a toy model that you're confident is a good proxy for that question\n\n    * Eg Toy Models of Superposition built a pretty good toy model of residual stream superposition\n  * (2) To pick a toy model that's a good enough proxy for LLMs in general, and just try hard to get as much traction on reverse-engineering that model as you can.\n\n    * Eg A Mathematical Framework - I think that \"train a model exactly like an LLM, but with only 1 or 2 layers\" is pretty good as proxies go, though not perfect.\n\nTo me, working on Othello-GPT is essentially a bet on (2), that there in\ngneeral some are underlying principles of transformers and how they learn\ncircuits, and that the way they manifest in Othello-GPT can teach us things\nabout real models. This is definitely wrong in some ways (I don't expect the\nspecific circuits we find to be in GPT-3!), and it's plausible this is wrong\nin enough ways to be not worth working on, but I think it seems plausible\nenough to be a worthwhile research direction. My high-level take is just \"I\nthink this is a good enough proxy about LLMs that studying it hard will teach\nus generally useful things\".\n\nThere's a bunch of key disanalogies to be careful of! Othello is fundamentally\nnot the same task as language: Othello is a much simpler task, there's only 60\nmoves, there's a rigid and clearly defined syntax with correct and incorrect\nanswers (not a continuous mess), the relevant info about moves so far can be\nfully captured by the current board state, and generally many sub-tasks in\nlanguage will not apply.\n\nBut it's also surprisingly analogous, at least by the standards of toy models!\nMost obviously, it's a transformer trained to predict the next token! But the\ntask is also much more complex than eg modular addition, and it has to do it\nin weird ways! The way I'd code Othello is by doing it recursively - find the\nboard state at move n and use it to get the state at move n+1. But\ntransformers can't do this, they need to do things with a fixed number of\nserial steps but with a lot of computation in parallel (ie, at every move it\nmust simultaneously compute the board state at that move in parallel) - it's\nnot obvious to me how to do this, and I expect that the way it's encoded will\nteach me a lot about how to represent certain kinds of algorithms in\ntransformers. And it needs to be solving a bunch of sub-tasks that interact in\nweird ways (eg, a piece can be taken multiple times in each of four different\ndirections), computing and remembering a lot of information, and generally\nforming coherent circuits.\n\nIn the next few sections I'll argue for how finding modular circuits can help\nbuild practical knowledge and techniques, what we could learn from\nunderstanding its MLPs, and more broadly how it could act as a laboratory for\nforming better conceptual frameworks (it's clearly not a good way to study\nscalability lol)\n\n### This is not about world models\n\nA high-level clarification: Though the focus of the original paper was on\nunderstanding how LLMs can form emergent world models, this is not why I am\narguing for these research directions. My interpretation of the original paper\nwas that it was strong evidence for the fact that it's possible for \"predict\nthe next token\" models to form world emergent models, despite never having\nexplicit access to the ground truth of the world/board state. I personally was\nalready convinced that this was possible, but think the authors did great work\nthat showed this convincingly and well (and I am even more convinced after my\nfollow-up!), and that there's not much more to say on the \"is this possible\"\nquestion.\n\nThere's many interesting questions about whether these happen in practice in\nLLMs and what this might look like and how to interpret it - my personal guess\nis that they do sometimes, but are pretty expensive (in terms of parameters\nand residual stream bandwidth) and only form when it's high value for reducing\nnext token loss and the model is big enough to afford it. Further, there's\noften much cheaper hacks, eg, BingChat doesn't need to have formed an explicit\nchess board model to be decent at playing legal moves in chess! Probably not\neven for reasonably good legal play: the chess board state is way easier than\nOthello, pieces can't even change colour! And you can get away with an\nimplicit rather than explicit world model that just computes the relevant\nfeatures from the context, eg to see where to a move a piece from, just look\nup the most recent point where that piece was played and look at the position\nit was moved to.\n\nBut Othello is very disanalogous to language here - playing legal moves in\nOthello has a single, perfectly sufficient world model that I can easily code\nup (though not quite in four transformer layers!), and which is incredibly\nuseful for answering the underlying task! Naively, Othello-GPT roughly seems\nto be spending 128 of its 512 residual stream dimensions of this model, which\nis very expensive (though it's probably using superposition). So while it's a\nproof of concept that world models are possible, I don't think the finer\ndetails here tell us much about whether these world models actually happen in\nreal LLMs. This seems best studied by actually looking at language models, and\nI think there's many exciting questions here! (eg doing mech interp on Patel\net al's work) The point of my investigation was more to refine our conceptual\nframeworks for thinking about models/transformers, and the goal of these\nproposed directions is to push forward transformer mech interp in general.\n\n## Finding Modular Circuits\n\nBasically all prior work on circuits (eg, induction heads, indirect object\nidentification, the docstring circuit, and modular addition) have been on what\nI call end-to-end circuits. We take some model behaviour that maps certain\ninputs to certain outputs (eg the input of text with repetition, and the\noutput of logits correctly predicting the repetition), and analyse the circuit\ngoing from the inputs to the outputs.\n\nThis makes sense as a place to start! The inputs and outputs are inherently\ninterpretable, and the most obvious thing to care about. But it stands in\ncontrast to much of the image circuits work, that identified neurons\nrepresenting interpretable features (like curves) and studied how they were\ncomputed and how these were used to computed more sophisticated features (like\ncar wheels -> cars). Let's consider the analogy of mech interp to reverse-\nengineering a compiled program binary to source code. End-to-end circuits are\nlike thinking of the source code as a single massive block of code, and\nidentifying which sections we can ignore.\n\nBut a natural thing to aim for is to find variables, corresponding to\ninterpretable activations within the network that correspond to features, some\nproperty of the input. The linear representation hypothesis says that these\nshould be directions in activation space. It's not guaranteed that LLMs are\nmodular in the sense of forming interpretable intermediate features, but this\nseems implied by exiasting work, eg in the residual stream (often studied with\nprobes), or in the MLP layers (possibly as interpretable neurons). If we can\nfind interpretable variables, then the reverse-engineering task becomes much\neasier - we can now separately analyse the circuits that form the feature(s)\nfrom the inputs or earlier features, and the circuits that use the feature(s)\nto compute the output logits or more complex feature.\n\nI call a circuit which starts or ends at some intermediate activation a\nmodular circuit (in contrast to end-to-end circuits). These will likely differ\nin two key ways from end-to-end circuits:\n\n  * They will likely be shallower, ie involving fewer layers of composition, because they're not end-to-end. Ideally we'd be able to eg analyse a single neuron or head in isolation.\n\n    * And hopefully easier to find!\n  * They will be composable - rather than needing to understand a full end-to-end circuit, we can understand different modular circuits in isolation, and need only understand the input and output features of each circuit, not the circuits that computed them.\n\n    * Hopefully this also makes it easier to predict model behaviour off distribution, by analysing how interpretable units may compose in unexpected ways!\n\nI think this is just obviously a thing we're going to need to get good at to\nhave a shot at real frontier models! Modular circuits mean that we can both\nre-use our work from finding circuits before, and hopefully have many fewer\nlevels of composition. But they introduce a new challenge - how do we find\nexactly what direction corresponds to the feature output by the first circuit,\nie the interface between the two circuits? I see two natural ways of doing\nthis:\n\n  * Exploiting a privileged basis - finding interpretable neurons or attention patterns (if this can be thought of as a feature?) and using these as our interpretable foothold.\n\n    * This is great if it works, but superposition means this likely won't be enough.\n  * Using probes to find an interpretable foothold in the residual stream or other activations - rather than assuming there's a basis direction, we learn the correct direction\n\n    * This seems the only kind of approach that's robust to superposition, and there's a lot of existing academic work to build upon!\n    * But this introduces new challenges - rather than analysing discrete units, it's now crucial to find the right direction and easy to have errors. It seems hard to produce composable circuits if we can't find the right interface.\n\nSo what does any of this have to do with Othello-GPT? I think we'll learn a\nlot by practicing finding modular circuits in Othello-GPT. Othello-GPT has a\nworld model - clear evidence of spontaneous modularity - and our linear probe\ntells us where it is in the residual stream. And this can be intervened upon -\nso we know there are downstream circuits that use it. This makes it a great\ncase study! By about layer 4, of the 512 dimensions of the residual stream, we\nhave 64 directions corresponding to which cell has \"my colour\" and 60\ndirections corresponding to which cells are blank (the 4 center cells are\nnever blank). This means we can get significant traction on what any circuit\nis reading or writing from the residual stream.\n\nThis is an attempt to get at the \"practical knowledge/techniques\" part of my\nbreakdown of mech interp bottlenecks - Othello-GPT is a highly imperfect model\nof LLMs, but I expect finding modular circuits here to be highly tractable and\nto tell us a lot. Othello-GPT cares a lot about the world model - the input\nformat of a sequence of moves is hard and messy to understand, while \"is this\nmove legal\" can be answered purely from the board state. So the model will\nlikely devote significant resources to computing board state, forming fairly\nclean circuits. Yet I still don't fully know how to do it, and I expect it to\nbe hard enough to expose a bunch of the underlying practical and conceptual\nissues and to teach us useful things about doing this in LLMs.\n\nGnarly conceptual issues:\n\n  * How to find the right directions with a probe. Ie the correct interface between world-model-computing circuits and world-model-using circuits, such that we can think of the two independently. I see two main issues:\n\n    * Finding all of the right direction - a probe with cosine sim of 0.7 to the \"true\" direction might work totally fine\n\n      * In particular, can we stop the probe from picking up on features that are constant in this context? Eg \"is cell B6 my colour\" is only relevant if \"is cell B6 blank\" is False, so there's naively no reason for the probe to be orthogonal to it.\n    * Ignoring features that correlate but are not causally linked - the corner cell can only be non-blank if at least one of the three neighbouring cells are, so the \"is corner blank\" direction should overlap with these.\n\n      * But my intuition is that the model is learning a causal world model, not correlational - if you want to do complex computations it's useful to explicitly distinguish between \"is corner blank\" as a thing to compute and use downstream, and all the other features. Rather than picking up on statistical correlations in the data.\n  * If we find interpretable directions in the residual stream that are not orthogonal, how do we distinguish between \"the model genuinely wants them to overlap\" vs \"this is just interference from superposition\"?\n\n    * Eg, the model should want \"is cell A4 blank\" to have positive cosine sim with the unembed for the \"A4 is legal\" logit - non-blank cells are never legal!\n  * The world model doesn't seem to be fully computed by layer X and only used in layer X+1 onwards - you sometimes need to intervene before layer 4, and sometimes the calculation hasn't finished before layer 5. How can we deal with overlapping layers? Is there a clean switchover layer per cell that we can calculate separately?\n  * How can we distinguish between two features having non-zero dot product because of noise/superposition, vs because they are correlated and the model is using one to compute the other.\n\nQuestions I want answered:\n\n  * How can we find the true probe directions, in a robust and principled way? Ideas:\n\n    * Use high weight decay to get rid of irrelevant directions. SGD (maybe with momentum) may be cleaner than AdamW here\n    * Use more complex techniques than logistic regression, like amnesiac probing (I found Eleuther's Tuned Lens paper a useful review)\n    * Find the directions that work best for causal interventions instead.\n    * Maybe use the janky probe directions to try to find the heads and neurons that compute the world model, and use the fact that these are a privileged-ish basis to refine our understanding of the probe directions - if they never contribute to some component of the probe, probably that component shouldn't be there!\n    * Maybe implicitly assume that the probe directions should form an orthogonal set\n    * Maybe train a probe, then train a second probe on the residual stream component orthogonal to the first probe. Keep going until your accuracy sucks, and then take some kind of weighted average of the residual stream.\n  * How is the blank world model computed?\n\n    * This should be really easy - a cell is blank iff it has never been played, so you can just have an attention head that looks at previous moves. Maybe it's done after the layer 0 attention!\n    * This is trivial with an attention head per cell, but probably the model wants to be more efficient. What does this look like?\n\n      * Eg it might have a single attention head look at all previous moves with uniform attention. This will get all of the information, but at magnitude 1/current_move, maybe it has the MLP0 layer sharpen this to have constant magnitude?\n    * Meta question: What's a principled way to find the \"is blank\" direction here? The problem is one of converting a three-way classifier (blank vs my vs their) to a binary classifier that can be summarised with a single direction. I'm currently taking blank - (my + their)/2, but this is a janky approach\n  * How is the \"my vs their\" world model computed?\n\n    * This seems like where the actual meat of the problem is!\n\n      * Consider games where\n  * Which techniques work well here? My money is on activation patching and direct logit attribution being the main place to start, see activation patching demoed in the accompanying notebook.\n\n    * I'd love for someone to try out attribution patching here!\n    * By activation patching, I both mean resample ablations (patching a corrupted activation into a clean run to see which activations are vs aren't necessary) and causal tracing (patching a clean activation into a corrupted run to see which activations contain sufficient information to get the task right)\n\n### Preliminary Results On Modular Circuits\n\nThe point of this section is to outline exciting directions of future work,\nbut as a proof of concept I've done some preliminary poking around. The meta-\nlevel point that makes me excited about this is that linear probes are really\nnice objects for interpretability. Fundamentally, transformers are made of\nlinear algebra! Every component (layer, head and neuron) reads its input from\nthe residual stream with a linear map, and writes it output by adding it to\nthe residual stream, which is a really nice structure.\n\nProbing across layers: One way this is nice is that we can immediately get a\nfoothold into understanding how the world model is computed. The residual\nstream is the sum of the embeddings and the output of every previous head and\nneuron. So when we apply a linear map like our probe, we can also break this\ndown into a direct contribution from each previous head and neuron.\n\nThis is the same key idea as direct logit attribution, but now our projection\nis onto a probe direction rather than the unembed direction for a specific\nnext token. This means we can immediately zoom in to the step of the circuit\nimmediately before the probe, and see which components matter for each cell!\n\nAs an example, let's look at move 20 in this game:\n\nThe probe can perfectly predict the board state by layer 4\n\nWe can now look at how much the output of each attention and each MLP layer\ncontributed to this (concretely we take the output of each attention and each\nMLP layer on move 30, and project them onto the is_blank direction and the\nis_mine direction for each cell, and plot this as a heatmap - check the\naccompanying notebook for details). The MLP layer contributions to whether a\ncell has my or their colour is particularly interesting - we can see that it\nnormally does nothing, but has a strong effect on the central stripe of cells\nthat were just taken by the opponent - plausibly MLPs calculate when a cell is\ntaken, and attention aggregates this? I'd love to see if there are specific\nneurons involve.\n\nReading Off Neuron Weights: Another great thing about a linear probe is that\nit gives us a meaningful set of directions and subspace in the residual stream\n(beyond that given by the embedding and unembedding). This means that we can\ntake any component's input or output weights, and project them onto the probe\ndirections to see how that component reads to or writes from the probe's\nsubspace - from this we can often just read off what's going on!\n\nThe probe intervention works best between layer 4 and layer 5, so we might\nhypothesise that some neurons in layer 5 are reading from the probe's subspace\n- we can check by taking the cosine sim of the neuron's input vector and the\nprobe's directions to see how it responds to each, see the accompanying\nnotebook for details. Here's neuron L5N1393 which seems to mostly represent\nC0==BLANK & D1==THEIRS & E2==MINE (cherry-picked for reasons unrelated to the\nprobe, discussed more in post 3). Reading the figure: Blue = THEIRS, Red=MINE,\nWhite can be either blank or 50-50 mine vs their's, so can't be read easily.\n\nHere's the neurons with the largest standard deviation of activation in layer\n3 (a pretty arbitrary way of choosing some that might be interesting) - when\nwe take the cosine sim of the output weights of these and the my colour probe,\nwe see some that are pretty striking (though note that this is only a 0.3\ncosine sim, so other stuff may be going on!)\n\nNote that this is a deliberately janky analysis - eg, I'm not ensuring that\nthe probe directions are orthogonal so I may double count, and I'm not looking\nfor other residual stream features. You can track how reasonable this approach\nby tracking what fraction of the neuron's input is explained by the probe's\nsubspaces, which is 64% in this case (these could otherwise be entirely\nspurious numbers!).\n\nI go into neuron interpretability in more detail in the next section, but I\nthink this technique is exciting in combination with what I discuss there,\nbecause it provides another somewhat uncorrelated technique - if many janky\ntechniques give the same explanation about a neuron, it's probably legit!\n\n## Neuron Interpretability and Studying Superposition\n\nAs argued earlier, I think that the current biggest open problem in\ntransformer mech interp is understanding the MLP layers of transformers. These\nrepresent over 2/3 of the parameters in models, but we've had much more\ntraction understanding attention-focused circuits. I'm not aware of a single\npublic example of what I'd consider a well-understood circuit involving\ntransformer MLP layers (beyond possibly my work on modular addition in a one\nlayer transformer, but that's cheating). There are tantalising hints about the\ncircuits they're used in in eg SoLU and ROME, but I broadly still feel\nconfused re what is mechanistically going on. I think this is a thing we\nobviously need to make progress on as a field! And I think we'll learn useful\nthings from trying to understand Othello-GPT's MLP layers!\n\nWhat could progress on understanding MLPs in general look like? I think that\nwe both need to get practice just studying MLP layers, and that we need to\nform clearer conceptual frameworks. A lot of our intuitions about transformer\nneurons come from image models, where neurons seem to (mostly?) represent\nfeatures, have ReLU activations, and seem to be doing fairly discrete kinds of\nlogic, eg \"if car wheel present and car body present and car window present\n(in the right places) -> it's a car\".\n\nTransformers are different in a bunch of ways - there's attention layers,\nthere's a residual stream (with significantly smaller dimension than the\nnumber of neurons in each layer!), and smoother and weirder GELU activations.\nMost importantly, polysemanticity seem to be a much bigger deal - single\nneurons often represent multiple features rather than a feature per neuron -\nand we think this is because models are using superposition - they represent\nfeatures as linear combinations of neurons and use this to compress in more\nfeatures than they have dimensions. This was argued for pretty convincingly in\nToy Models of Superposition, but their insights were derived from a toy model,\nwhich can easily be misleading. I'm not aware of any work so far exhibiting\nsuperposition or properly testing the predictions of that paper in a real\nmodel. I expect some ideas will transfer but some will break, and that I'll\nlearn a lot from seeing which is which!\n\nOthello-GPT is far from a real language model, but I expect that understanding\nits MLP layers would teach me a bunch of things about how transformer MLP\nlayers work in general. The model needs to compress a fairly complex and wide-\nranging set of features and computation into just eight layers, and the\ndetails of how it does this will hopefully expose some principles about what\nis and is not natural for a transformer to express in MLP neurons.\n\nWhat would progress here look like? My high-level take is that a solid\nstrategy is just going out, looking for interesting neurons, and trying to\nunderstand them deeply - no grander purpose or high-level questions about the\nmodel needed. I'd start with similar goals as I gave in the previous section -\nlook for the neurons that are used to compute the probe, and directly used by\nthe probe. I also outline some further preliminary results that may serve as\ninspiration.\n\nI've learned a lot from case studies looking deeply at concrete case studies\nof circuits in models: Interpretability in the Wild found backup heads (that\ntook over when earlier heads were ablated) and negative heads (that\nsystematically boosted incorrect solutions), and the docstring circuit found a\npolysemantic attention head, and a head which used the causal attention mask\nto re-derive positional information. I would love to have some similar case\nstudies of meaningful neurons!\n\n### Empirically Testing Toy Models of Superposition\n\nThe sections of my mech interp explainer on superposition and on the toy\nmodels of superposition paper may be useful references\n\nI'm particularly excited about using Othello-GPT to test and validate some of\nthe predictions of Toy Models of Superposition about what we might find in\ntransformers. Empirical data here seems really valuable! Though there are some\nimportant ways that the setup of Othello-GPT differs from their toy model.\nNotably, they study continuous (uniform [0, 1]) features, while Othello-GPT's\nfeatures seem likely to be binary (on or off), as they're discrete and logical\nfunctions of the board state and of the previous moves. Binary features seem\nmore representative of language, especially early token-level features like\nbigrams and multi-token words, and are also easier to put into superposition,\nbecause you don't need to distinguish low values of the correct feature from\nhigh values of the incorrect feature\n\nA broader point is whether we expect Othello-GPT to use superposition at all?\nTheir model has more features to represent than dimensions, and so needs to\nuse superposition to pack things in. It's not obvious to me how many features\nOthello-GPT wants to represent, and how this compares to the number of\ndimensions - my guess is that it still needs to use superposition, but it's\nnot clear. Some considerations:\n\n  * There's actually a lot of very specific features it might want to learn - eg in the board state -> output logit parts there seems to be a neuron representing C0==BLANK & D1==THEIR'S & E2==MINE, ie can I place a counter in C0 such that it flanks exactly one counter on the diagonal line to the down and right - if this kind of thing is useful, it suggests the model is dealing with a large combinatorial explosion of cases for the many, many similar configurations!\n\n    * Further, computing the board state from the moves also involves a lot of messy cases, eg dealing with the many times and directions a piece can be flipped and combining this all into a coherent story.\n\n      * Reminder: Transformers are not recurrent - it can't compute the board state at move n from the state at move n-1, it needs to compute the state at every move simultaneously with just a few layers of attention to move partial computation forwards. This is actually really hard, and it's not obvious to me how you'd implement this in a transformer!\n  * There are two different kinds of superposition, residual stream superposition and neuron superposition (ie having more features than dimensions in the residual stream vs in the MLP hidden layer).\n\n    * The residual stream has 512 dimensions, but there's 8 layers of 2048 neurons each (plus attention heads) - unless many neurons do nothing or are highly redundant, it seems very likely that there's residual stream superposition!\n\n      * Though note that it's plausible it just has way fewer than 2048 features worth computing, and is massively over-parametrised. I'm not sure what to think here!\n      * The board state alone consumes 25% of the dimensions, if each feature gets a dedicated dimension, and I expect there's probably a bunch of other features worth computing and keeping around?\n\nConcrete questions I'd want to test here - note that the use of dropout may\nobfuscate these questions (by incentivising redundancy and backup circuits),\nand this may be best answered in a model without dropout. These also may be\nbest answered in a smaller model with fewer layers and a narrower residual\nstream, and so with a stronger incentive for superposition!:\n\n  * Do important features get dedicated dimensions in the residual stream? (ie, orthogonal to all other features)\n\n    * Guesses for important features - whether black or white is playing, the board state, especially features which say which center cells have my colour vs their's.\n  * Conversely, can we find evidence that there is overlap between features in the residual stream?\n\n    * This is surprisingly thorny, since you need to distinguish this kind of genuine interference vs intentional overlapping, eg from the source of the first feature actually wanting to contribute a bit to feature two as well.\n  * Do the important neurons seem monosemantic?\n\n    * Important could mean many things eg high effect when patching, high average activation or standard deviation of activation, high cost when ablated, or any other range of measurements, high gradient or gradient x activation\n    * My workflow would be to use the probe and unembed to interpret neuron weights, max activating dataset examples to help form a hypothesis, and then use a spectrum plot to properly analyse it (discussed more below).\n  * Do we get seemingly unrelated features sharing a neuron? The paper predicts superposition is more likely when there are two uncorrelated or anti-correlated features, because then the model doesn't need to track the simultaneous interference of both being there at once.\n  * Can we find examples of a feature being computed that needs more than one neuron? Analogous to how eg modular addition uses ReLUs to multiply two numbers together, which takes at least three to do properly. This is a bit of a long shot, since I think any kind of discrete, Boolean operation can probably be done with a single GELU, but I'd love to be proven wrong!\n  * Do features actually seem neuron aligned at all?\n\n    * If we find features in superposition, do they tend to still be sparse (eg linear combinations of 5 ish neurons) or diffuse (no noticable alignment with the neuron basis)\n  * Can we find any evidence of spontaneous sorting of superposed features into geometric configurations? (A la the toy models paper)\n  * Can you construct any adversarial examples using evidence from the observed polysemanticity?\n  * Can you find any circuits used to deal with interference superposition? Or any motifs, like the asymmetric inhibition motif?\n\n### Preliminary Results On Neuron Interpretability\n\nNote that this section has some overlap with results discussed in my research\nprocess\n\nIn addition to the results above using the probe to interpret neuron weights,\nan obvious place to start is max activating dataset examples - run the model\nover a bunch of games and see what moves the neuron activates the most on.\nThis is actually a fair bit harder to interpret than language, since \"what are\nthe connections between these sequences of moves\" isn't obvious. I got the\nmost traction from studying board state - in particular, the average number of\ntimes each cell is non-empty, and the average number of times a cell is mine\nvs their's. Here's a plot of the latter for neuron L5N1393 that seems\nimmediately interpretable - D1 is always their's, E2 is always mine! (across\n50 games, so 3000 moves) I sometimes get similar results with other layer 5\nand layer 6 neurons, though I haven't looked systematically.\n\nLooking at the fraction of the time a cell is blank or not seems to give\npretty interesting results for layer 3 and layer 4 neurons.\n\nI expect you can stretch max activating dataset examples further by taking\ninto account more things about the moves - what time in the game they\nhappened, which cells are flipped this turn (and how many times in total!),\nwhich cell was played, etc.\n\nMy guess from this and probe based analysis earlier was that neuron L5N1393\nmonosemantically represented the diagonal line configuration C0==BLANK &\nD1==THEIR'S & E2==MINE. This makes sense as a useful configuration since it\nsays that C0 is a legal move, because it and E2 flank D1! But this seems\ninconsistent with the direct logit attribution of the neuron (ie the output\nvector of the neuron projected by the unembed onto the output logits), which\nseems to boost C0 a lot but also D1 a bit - which seems wildly inconsistent\nwith it firing on D1 being their colour (and thus not a legal place to play!)\n\nThese techniques can all be misleading - max activating dataset examples can\ncause interpretability illusions, direct logit attribution can fail for\nneurons that mostly indirectly affect logits, and probes can fail to interpret\nneurons that mostly read out unrelated features. One of the more robust tools\nfor checking what a neuron means is a spectrum plot - if we think a neuron\nrepresents some feature, we plot a histogram of the \"full spectrum\" of the\nneuron's activations by just taking the neuron activation on a ton of data,\nand plotting a histogram grouped by whether the feature is present or not\n(used in curve detectors and multimodal neurons). If a neuron is monosemantic,\nthis should fairly cleanly separate into True being high and False being low!\n\nNote that the y axis is percent (ie it's normalised by group size so both True\nand False's histograms add up to 100 in total, though True is far more spread\nout so it doesn't look it. This is hard to read, so here it is on a log scale\n(different to read in a different way!).\n\nThese plots are somewhat hard to interpret, but my impression is that this\nneuron is plausibly monosemantic-ish, but with a more refined feature -\nbasically all of the high activations have the diagonal line hypothesised, but\nthis is necesssary not sufficient - there's a bunch of negative activations\nwith the line as well! Plausibly it's still monosemantic but there's some\nextra detail I'm missing, I'm not sure! My next steps would be to refine the\nhypothesis by inspecting the most positive and most negative True examples,\nand if I can get a cleaner histogram to then try some causal interventions (eg\nmean ablating the neuron and seeing if it has the effect my hypothesis would\npredict). I'd love to see someone finish this analysis, or do a similar deep\ndive on some other neurons!\n\nSpectrum plots are a pain to make in general, because they require automated\nfeature detectors to do properly (though you can do a janky version by\nmanually inspecting randomly sampled examples, eg a few examples from each\ndecile). One reason I'm excited about neuron interpretability in Othello-GPT\nis that it's really easy to write automated tests for neurons and thus get\nspectrum plots, and thus to really investigate monosemanticity! If we want to\nbe able to make real and robust claims to have identified circuits involving\nneurons or to have mechanistically reverse-engineered a neurons, I want to\nbetter understand whether we can claim the neuron is genuinely only used for a\nsingle purpose (with noise) or is also used more weakly to represent other\nfeatures. And a concrete prediction of the toy models framework is that there\nshould be some genuinely monosemantic neurons for the most important features.\n\nThat said, showing genuine monosemanticity is hard and spectrum plots are\nlimited. Spectrum plots will still fall down for superposition with very rare\nfeatures - these can be falsely dismissed as just noise, or just never occur\nin the games studied! And it's hard to know where to precisely draw the line\nfor \"is monosemantic\" - it seems unreasonable to say that the smallest True\nactivation must be larger than the largest False one! To me the difference is\nwhether the differences genuinely contribute to the model having low loss, vs\non average contributing nothing. I think questions around eg how best to\ninterpret these plots are an example of the kind of practical knowledge I want\nto get from practicing neuron interpretability!\n\n#### Case Study: Neurons and Probes are Confusing\n\nAs a case study in how this can be confusing, here's an earlier draft graph\nfor the section on finding modular circuits - looking at the output weights of\ntop layer 4 neurons (by std) in the blank probe basis. It initially seems like\nthese are all neurons dedicated to computing that a single cell is blank. And\nI initially got excited and thought this made a great graph for the post! But\non reflection this is weird and surprising (exercise: think through why before\nyou read on)\n\nI argue that this is weird, because figuring out whether a cell is blank\nshould be pretty easy - a cell can never become non-empty, so a cell is blank\nif and only if it has never been played. This can probably be done in a single\nattention layer, and the hard part of the world model is computing which cells\nare mine vs their's. So what's up with this?\n\nIt turns out that what's actually going on is that the blank probe is highly\ncorrelated with the unembed (the linear map from the final residual to the\nlogits). A cell can be legal only if it is blank, if a cell has a high logit\nat the end of the model, then it's probably blank. But our probe was computed\nafter layer 6, when there's a lot of extraneous information that probably\nobscures the blankness information - probably, the probe also learned that if\nthere's going to be a high logit for a cell then that cell is definitely\nblank, and so the blank directions are partially aligned with the unembed\ndirections. Though on another interpretation, is_blank and the unembed are\nintentionally aligned, because the model knows there's a causal link and so\nuses the is_blank subspace to also contribute to the relevant unembed.\n\nAnd we see that the alignment with the unembed is even higher! (Around cosine\nsim of 0.8 to 0.9)\n\n## A Transformer Circuit Laboratory\n\nMy final category is just the meta level point that I'm confused in many ways\nabout the right conceptual frameworks when thinking about transformer\ncircuits, and think that there's a lot of ways we could make progress here!\nJust as Othello-GPT helped provide notable evidence for the hypothesis that\nmodels form linear representations of features, I hope it can help clarify\nsome of these - by concretely understanding what happens inside of it, we can\nmake more informed guesses about transformers in general. Here's a rough\nbrainstorm of weird hypotheses and confusions about what we might find inside\ntransformers - I expect that sufficient investigation of Othello-GPT will shed\nlight on many of them!\n\nSince Othello-GPT is an imperfect proxy for LLMs, it's worth reflecting on\nwhat evidence here looks like. I'm most excited about Othello-GPT providing\n\"existence proofs\" for mysterious phenomena like memory management: case\nstudies of specific phenomena, making it seem more likely that they arise in\nreal language models. Proofs that something was not used/needed are great, but\nneed to be comprehensive enough to overcome the null hypothesis of \"this\nwas/wasn't there but we didn't look hard enough\", which is a high bar!\n\n  * Does it do memory management in the residual stream? Eg overwriting old features when they're no longer needed. I'd start by looking for neurons with high negative cosine sim between their input and output vectors, ie which basically erase some direction.\n\n    * One hypothesis is that it implicitly does memory management by increasing the residual stream norm over time - LayerNorm scales it to have fixed norm, so this suppresses earlier features. If this is true, we might instead observe signal boosting - key features get systematically boosted over time (eg whether we're playing black or white)\n    * This might come up with cells that flip many times during previous moves - maybe the model changes its guess for the cell's colour back and forth several times as it computes more flips? Do each of these write to the probe direction and overwrite the previous one, or is it something fancier?\n  * Do heads and neurons seem like the right units of analysis of the model? Vs eg entire layers, superposition-y linear combinations of neurons/heads, subsets of heads, etc.\n  * Do components (heads and neurons) tend to form tightly integrated circuits where they strongly compose with just a few other components to form a coherent circuit, or tend to be modular, where each component does something coherent in isolation and composes with many other components.\n\n    * For example, an induction head could be either tightly integrated (the previous token head is highly coupled to the induction head and not used by anything else, and just communicates an encoded message about the previous token directly to the induction head) or could form two separate modules, where the previous token head's output writes to a \"what was in the previous position\" subspace that many heads (including the induction head!) read from\n\n      * My guess is the latter, but I don't think anyone's checked! Most working finding concrete circuits seems to focus on patching style investigations on a narrow distribution, rather than broadly checking behaviour on diverse inputs.\n    * On a given input, can we clearly detect which components are composing? Is this sparse?\n  * When two components (eg two heads or a head and a neuron) compose with each other, do they tend to write to some shared subspace that many other components read and write from, or is there some specific encod\n\n    * Do components form modules vs integrated circuits vs etc.\n  * Can we find examples of head polysemanticity (a head doing different things in different contexts) or head redundancy (multiple heads doing seemingly the same thing).\n\n    * Do we see backup heads? That is, heads that compensate for an earlier head when that head is ablated. This model was trained with attention dropout, so I expect they do!\n\n      * Do these backup heads do anything when not acting as backups?\n      * Can we understand mechanistically how the backup behaviour is implemented?\n      * Are there backup backup heads?\n    * Can we interpret the heads at all? I found this pretty hard, but there must be something legible here!\n    * If we find head redundancy, can we distinguish between head superposition (there's a single \"effective head\" that consists of a linear combination of these )\n    * Can we find heads which seem to have an attention pattern doing a single thing, but whose OV circuit is used to convey a bunch of different information, read by different downstream circuits\n    * Can we find heads which have very similar attention patterns (ie QK circuits) whose OV circuits add together to simulate a single head with an OV circuit of twice the rank?\n  * Is LayerNorm ever used as a meaningful non-linearity (ie, the scale factor differs between tokens in a way that does useful computation), or basically constant? Eg, can you linearly replace it?\n\n    * Are there emergent features in the residual stream? (ie dimensions in the standard basis that are much bigger than the rest). Do these disproportionately affect LayerNorm?\n  * The model has clearly learned some redundancy (because it was trained with dropout, but also likely would learn some without any dropout). How is this represented mechanistically?\n\n    * Is it about having backup circuits that takeover when the first thing is ablated? Multiple directions for the same feature? Etc.\n  * Can you find more evidence for or against the hypothesis that features are represented linearly?\n\n    * If so, do these get represented orthogonally?\n  * Ambitiously, do we have a shot at figuring out everything that the model is doing? Does it seem remotely possible to fully-reverse engineer it?\n\n    * Is there a long tail of fuzzy, half-formed features that aren't clean enough to interpret, but slightly damage loss if ablated? Are there neurons that just do nothing either way?\n    * Some ambitious plans for interpretability for alignment involve aiming for enumerative safety, the idea that we might be able to enumerate all features in a model and inspect this for features related to dangerous capabilities or intentions. Seeing whether this is remotely possible for Othello-GPT may be a decent test run.\n  * Do the residual stream or internal head vectors have a privileged basis? Both with statistical tests like kurtosis, and in terms of whether you can actually interp directions in the standard basis?\n  * Do transformers behave like ensembles of shallow paths? Where each meaningful circuit tends to only involve a few of the 16 sublayers, and makes heavy use of the residual stream (rather than 16 serial steps of computation).\n\n    * Prior circuits work and techniques like the logit lens seems to heavily imply this, but it would be good to get more data!\n    * A related hypothesis - when a circuit involves several components (eg a feature is computed by several neurons in tandem) are these always in the same layer? One of my fears is that superposition gives rise to features that are eg linear combinations of 5 neurons, but that these are spread across adjacent layers!\n\n## Where to start?\n\nIf you've read this far, hopefully I've convinced you there are interesting\ndirections here that could be worth working on! The next natural question is,\nwhere to start? Some thoughts:\n\n  * Read the original paper carefully\n  * If you're new to mech interp, check out my getting started guide.\n\n    * I particularly recommend getting your head around how a transformer works, and being familiar with linear algebra\n  * Use my accompanying notebook as a starting point which demonstrates many of the core techniques\n\n    * I highly recommend using my TransformerLens library for this, I designed it to enable this kind of research\n    * Check out the underlying codebase (made by the original authors, thanks to Kenneth Li for the code and for letting me make additions!)\n  * My concrete open problems sequence has a bunch of tips on doing good mech interp research, especially in the posts on circuits in toy language models, on neuron interpretability, and on superposition.\n  * Read through my notes on my research process to get a sense of what making progress on this kind of work looks like, and in particular the decisions I made and why.\n\n### Concrete starter projects\n\nI'll now try to detail some concrete open problems that I think could be good\nplaces to start. Note that these are just preliminary suggestions - the above\nsections outline my underlying philosophy of which questions I'm excited about\nand a bunch of scattered thoughts about how to make progress on them. If\nthere's a direction you personally feel excited about, you should just jump\nin.\n\nIdeas for gentle starter projects (Note that I have not actually tried these -\nI expect them to be easy, but I expect at least one is actually cursed! If you\nget super stuck, just move on):\n\n  * How does the model decide that the cell for the current move is not blank?\n\n    * What's the natural way for a transformer to implement this? (Hint: Do you need information about previous moves to answer this?)\n    * At which layer has the model figured this out?\n    * Try patching between two possibilities for the current move (with the same previous game) and look at what's going on\n  * Pick a specific cell (eg B3). How does the model compute that it's blank?\n\n    * I'd start by studying the model on a few specific moves. At which layer does the model conclude that it's blank? Does this come from any specific head or neuron?\n    * Conceptually, a cell is not blank if and only if it was played as a previous move - how could a transformer detect this? (Hint: A single attention head per cell would work)\n  * Take a game where a center cell gets flipped many times. Look at what colour the model thinks that cell is, after each layer and move. What patterns can you see? Can you form any guesses about what's going on? (This is a high-level project - the goal is to form hypotheses, not to reach clear answers)\n  * Take the is_my_colour direction for a specific cell (eg D7) and look for neurons whose input weight has high cosine similarity with this. Look at this neuron's cosine sim with every other probe direction, and form a guess about what it's doing (if it's a mess then try another neuron/cell). Example guesses might be\n\n    * Then look at the max activating dataset examples (eg the top 10 over 50 games) and check if your guess worked!\n    * Extension: Plot a spectrum plot and check how monosemantic it actually is\n  * Repeat the above for the is_blank direction.\n  * Take the average of the even minus the average of the odd positional embeddings to get an \"I am playing white\" direction. Does this seem to get its own dedicated dimension, or is it in superposition?\n\n    * A hard part about answering this question is distinguishing there being non-orthogonal features, vs other components doing memory management and eg systematically signal boosting the \"I am playing white\" direction so it's a constant fraction of the residual stream. Memory management should act approximately the same between games, while other features won't.\n\n### Cleaning Up\n\nThis was (deliberately!) a pretty rushed and shallow investigation, and I cut\na bunch of corners. There's some basic cleaning up I would do if I wanted to\nturn this into a real paper or build a larger project, and this might be a\ngood place to start!\n\n  * Training a better probe: I cut a lot of corners in training this probe... Some ideas:\n\n    * Train it on both black and white moves! (to predict my vs their's, so flip the state every other move)\n    * I cut out the first and last 5 moves - does this actually help/matter? Check how well the current probe works on early and late moves.\n    * The state of different cells will be correlated (eg a corner can only be filled if a neighbouring cell is filled), so the probes may be non-orthogonal for boring reasons. Does it help to constrain them to be orthogonal?\n    * What's the right layer to train a probe on?\n    * The probe is 3 vectors (three-way logistic regression), but I want a is_blank_vs_filled and is_mine_vs_theirs_conditional_on_not_being_blank direction - what's the most principled way of doing this?\n  * Rigorously testing interventions: I'm pretty convinced that intervening the probe does something, but\n\n    * Currently I take the current coordinate with respect to the probe direction, negate that, and then scale. Plausibly, this is dumb and the magnitude of the original coordinate doesn't matter, and I should instead replace it with a constant magnitude. The place I'd start is to just plot a histogram of the coordinates in the probe directions\n    * Replicating the paper's analysis of whether their intervention works (their natural and unnatural benchmark)\n  * Re-train the model: The model was trained with attention and residual dropout - this is not representative of modern LLMs, and incentivises messy and redundant representations and backup circuits, I expect that training a new model from scratch with no dropout will make your life much easier. (Note that someone is currently working on this)\n\n    * The current model is 8 layers with a residual stream of width 512. I speculate this is actually much bigger than it needs to be, and things might be cleaner with fewer layers and a wider stream, a narrower stream, or both.\n\n# The Research Process\n\nThis project was a personal experiment in speed-running doing research, and I\ngot the core results in in ~2.5 days/20 hours. This post has some meta level\ntakeaways from this on doing mech interp research fast and well, followed by a\n(somewhat stylised) narrative of what I actually did in this project and why -\nyou can see the file tl_initial_exploration.py in the paper repo for the code\nthat I wrote as I went (using VSCode's interactive Jupyter mode).\n\nI wish more work illustrated the actual research process rather than just a\nfinal product, so I'm trying to do that here. This is approximately just me\nconverting my research notes to prose, see the section on process-level\ntakeaways for a more condensed summary of my high-level takeaways.\n\nThe meta level process behind everything below is to repeatedly be confused,\nplot stuff a bunch, be slightly less confused, and iterate. As a result,\nthere's a lot of pictures!\n\n## Takeaways on doing mech interp research\n\nWarning: I have no idea if following my advice about doing research fast is\nactually a good idea, especially if you're starting out in the field! It's\nmuch easier to be fast and laissez faire when you have experience and an\nintuition for what's crucial and what's not, and it's easy to shoot yourself\nin the foot. And when you skimp on rigour, you want to make sure you go back\nand check! Though in this case, I got strong enough results with the probe\nthat I was fairly confident I hadn't entirely built a tower of lies. And\ngenerally, beware of generalising from one example - in hindsight I think I\ngot pretty lucky on how fruitful this project was!\n\n  * Be decisive: Subjectively, by far the most important change was suppressing my perfectionism and trying to be bold and decisive - make wild guesses and act on them, be willing to be less rigorous, etc.\n\n    * If I noticed myself stuck on doing the best or most principled thing, I'd instead try to just do something.\n\n      * Eg I wanted to begin by patching between two similar sequences of moves - I couldn't think of a principled way to change a move without totally changing the downstream game, so I just did the dumb thing of patching by changing the final move.\n      * Eg when I wanted to try intervening with the probe, I couldn't think of a principled way to intervene on a bunch of games or to systematically test that this worked, or exactly how best to intervene, so I decided to instead say \"YOLO, let's try intervening in the dumbest possible way, by flipping the coefficient at a middle layer, on a single move, and see what happens\"\n    * Pursue the hypothesis that seems \"big if true\"\n\n      * Eg I decided to try training a linear probe on just black moves after a hunch that this might work given some suggestive evidence from interpreting neuron L5N1393\n    * Notice when I get stuck in a rabbit hole/stop learning things and move on\n\n      * Eg after training a probe I found it easy to be drawn into eg inspecting more and more neurons, or looking at head attention patterns, and it worked much better to just say\n    * Be willing to make quick and dirty hacks\n\n      * Eg when I wanted to look at the max activating dataset examples for neurons, I initially thought I'd want to run the model on thousands to millions of games, to get a real sample size. But in practice, just running the model on a batch of 100 games and taking the top 1% of moves by neuron act in there, worked totally fine.\n  * The virtue of narrowness - depth over breadth: A common mistake in people new to mech interp is to be reluctant to do projects that feel \"too small\" - eg interpreting a single neuron or head rigorously. And to think that something is interesting only if it's automatable and scalable. But here, being willing to just dive in to patching on specific examples, targeting specific neurons that stood out, etc worked great, and ultimately pointed me to the general principles underlying the model (namely, that it thought in mine vs their's)\n  * Gain surface area: I felt kinda stuck when figuring out where to start. Early on, by far the most useful goal was to gain surface area on the problem - to just dive into anything that seemed interesting, play around, and build intuitions about the moving parts of the model and how it was behaving, without necessarily having a concrete goal beyond understanding and following my curiosity.\n\n    * A good way of doing this was to play around with concrete examples, and in particular to patch between similar examples and analyse where the differences came from.\n  * Work on algorithmic problems: Empirically, algorithmic problems are just way cleaner and more tractable to interpret - there's a ground truth, it's easier to reason about, and it's easy to craft synthetic inputs. This is a double-edged sword, since they're also less interesting and less true to real models, but it's very convenient for goodharting on \"research insight per unit hour\"\n  * Domain knowledge is super useful!\n\n    * Spending 30-60 minutes at the start playing against the eOthello AI was really valuable for building intuitions (I went in knowing absolutely nothing about Othello), though I got carried away by how fun it was and could have got away with less time.\n\n      * Eg that the start and end of the game are weird, that you occasionally need to pass but can basically ignore it, that a single piece can change colour many times, including from a move pretty far away, and even dumb things like \"you can take diagonally, and this happens a lot\"\n    * Having experience doing mech interp helped a ton - being better able to generate hypotheses, figure out what's interesting, reach for the right techniques, and interpret results\n\n      * In particular, having stared at the mechanical structure of a transformer and what kinds of algorithms are and are not natural to implement remains super useful for building intuitions. (I try to convey a bunch of these in my walkthrough of A Mathematical Framework)\n  * Good tooling is crucial: If you want to do research fast, tight feedback loops are key, and having good, responsive tooling that you understand well is invaluable, even for a throwaway project on a tight deadline. I've created an accompanying colab with most of my tools, and I hope they're useful! (Sorry for the jankiness)\n\n    * TransformerLens is a library I made for mech interp of language models, with the explicit goal of making exploratory research easier, and it worked great here! Eg for easily caching model activations, and for trying out different patching and interventional experiments.\n\n      * In general, it's far easier to use software you've written yourself, but I've heard good things from other people trying to use TransformerLens!\n    * Building good visualisations was pretty valuable - especially visualising model logits as a heatmap on the board, and converting a set of moves into a plot of the state of the board. Though I probably spent ~4 hours on making beautiful plotly visualisations (and debugging plotly animations...), and could have gotten away with much less.\n    * Basic software engineering - noticing the code I kept writing and converting it to functions (eg dumb stuff around changing moves from nice written notation, to the model's vocabulary, to the format used to compute board state; or intervening with the probe; or converting a set of moves to a list of valid moves at each turn, etc)\n  * MLPs > attention: I went into this expecting it to be way easier to interpret attention heads/patterns, but I actually didn't make much headway there, but did great with MLP neurons.\n\n    * I think the difference was that I didn't really know how to think about the sequence of prior moves (and thus which moves were attended to), while I did know how to think about the current board state and thus about valid output logits (and direct logit attribution) and about the max activating dataset examples).\n    * And the fact that there were seemingly a bunch of monosemantic neurons, rather than a polysemantic mess of superposition\n  * Activation patching is great: Models are complex and full of many circuits for different tasks - even on a single input, likely many circuits are relevant to completing the task! This makes it difficult to isolate out anything specific, and thus is hard to be concrete. Activation patching/causal tracing is a great way to get around this - you set up two similar inputs that differ in one crucial detail, and you patch specific activations between the two and analyse what changes (eg whether an output logit changes). Because the two inputs are so similar, this controls for all the stuff you don't care about, and lets you isolate out a specific circuit.\n\n## Getting Started\n\nThere was first a bunch of general figuring stuff out and getting oriented -\nlearning how Othello worked, reading the existing code, loading in the data\nand games, figuring out how to convert a sequence of moves into a board state\nand valid moves, getting everything into a format I could work easily with (eg\nmassive tensors of game moves rather than a list of lists) and making pretty\nplotting functions. I also decided to filter out weird edge cases I didn't\nreally care about, like games of less than 60 moves, or with passes in them.\nIn hindsight, it would have been better to do some of this later when I had a\nclearer picture of what did and did not need optimisation, but *shrug*.\n\nThe most useful bits of infrastructure I set up (both now, and later) were:\n\n  * Convenience functions to convert moves between 1 to 60 (inputs and outputs of the model, since center squares can't be player), 0 to 63 as the actual indexes, and A0 to H7 as the printable labels\n  * Plotting function to plot either a single board state (and valid moves), and an animation showing a whole game with a slider (the latter turned out to be a deep rabbit hole of Plotly animation bugs though...)\n  * Creating a single tensor of all games stacked together (in my case, I took all 4.5M games, since it fit into my RAM - 10,000 would have been more than enough)\n  * Running and caching the model activations on 100 games, so I could use this as an easy reference without needing to run the model every time (eg to look at neurons with big average activations)\n\nI didn't have a clear next step (my main actual idea was taking one of the\nauthor's pre-trained non-linear probes and trying to interpret how that\nworked, but this seemed like a pain), so I tried to start gaining surface area\non what was going on by just trying shit. It's easy to interpret the output\nlogits, and so looking at how each model component directly affects the logits\nis a good hook to get some insight in any model.\n\nThe first actual research I tried was inputting an arbitrary game, and looking\nat the direct logit attribution of each layer's output on a few of the moves.\nEyeballing things, there was a clearish trend where MLP5, MLP6 and Attn7\nmattered a lot, other parts were less important. Interestingly, MLP7 (naively,\nthe obvious place to start, since it can only affect the output logits).\nExample graph below:\n\nBeing more systematic supported this. This is a bit of a weird problem,\nbecause there are many (and a variable number of!) valid next moves, rather\nthan a single correct next token, so I tried to both look at the difference in\naverage direct logit attribution for the correct/incorrect next logit, and the\ndifference in min/max contribution. The former doesn't capture bits that\ndisambiguate between borderline correct and borderline incorrect moves, since\nmost moves will be obviously bad, and the latter is misleading because you're\ntaking the max and min over large-ish sets, which is always sketchy (eg it\ngives misleading results for random noise) - you get a weird spectrum from\nearly to late moves because there are more options in the middle. I also saw\nthat layer 7 acts very differently at the first and last move, presumably\nbecause those are easier special cases, but decided this was out of scope and\nto ignore it for now. I tried breaking the attention layers down into separate\nheads, but didn't have much luck.\n\nI was then kinda stuck. I tried plotting attention patterns and staring at\nthem, looking for interesting heads, and didn't get much traction (in part\nbecause I didn't really get how to interpret moves!). I did see some heads\nwhich only attended to moves of the same parity as the current one, which was\nmy first hint for what was going on (not that I noticed lol).\n\n## Patching\n\nPart of why interpreting models is hard is because they're full of different\ncircuits that combine to answer a question. But each circuit will only\nactivate on certain inputs, and each input will likely require a bunch of\ncircuits, making it a confusing mess.\n\nActivation patching is a great way to cut through this! The key idea is to set\nup a careful counterfactual, where you have two inputs, a clean input and a\ncorrupted input, which differ in one key detail. Ideally, the difference\nbetween any activation on the clean and corrupted run will purely represent\nthat key detail. You can then iterate over each activation and patch them from\nthe clean run to the corrupted run to see which can most recover the clean\noutput (or from the corrupted run to the clean run to see which can most\ndamage the clean output), and hopefully, a few activations matter a lot and\nmost don't. This can let you isolate which activations actually matter for\nthis detail!\n\nI knew that I wanted to try patching something, but sadly it was kind of a\nmess, because an input needs to be a sequence of legal moves. I wanted two\nsequences which had similar board states but whose moves differed in some key\nplaces, so I could track down how board state was computed.\n\nI gave up on this idea because it seemed too hard, and instead decided to be\ndecisive and do the dumb thing of changing just the most recent move! I picked\nan arbitrary game, took the first 30 moves, and changed the final move from H0\nto G0 to get a corrupted input. This changed cell C0 (I index my columns at\nzero not one, sorry) from legal to illegal. This meant I could take the C0\nlogit as my patching metric - it's high on clean, low on corrupted, and so it\ncan tell me how much my patched activation tracks \"the way that the most\nrecent move being G0 rather than H0 is used to determine that C0 is illegal\"\n(or vice versa). This is a very niche thing to study, but it's a start! And\nthe virtue of narrowness says to favour deep understanding of something\nspecific, over aiming for a broad understanding but not knowing where to\nstart.\n\nThe first thing to try is patching each layer's output - I found that MLP5,\nMLP6 and MLP0 mattered a lot, Attn7 and MLP4 mattered a bit. The rest didn't\nmatter at all, so I could probably ignore them!\n\nI now wanted to narrow things down further, and got a bit stuck again - I\nneeded to refine \"this layer matters\" into something more specific. I had the\nprior that it's way easier to understand attention than MLPs, so I tried\nlooking at the difference in attention pattern from clean to corrupted for\neach head (from each source token to the final move), but I couldn't\nimmediately see anything interesting (though in hindsight, I see alternating\nbands of on and off!):\n\nI then just tried looking at the difference in direct logit attribution (to\nC0) between clean and corrupted for every neuron. This looked way more\npromising - most neurons were irrelevant, but a few mattered a ton. This\nsuggested I could mostly ignore everything except the neurons that mattered.\nThis gave me, like, 10 neurons to understand, which was massive progress!\nBizarrely, MLP7 had two neurons, which both mattered a ton, but near exactly\ncancelled out (+2.43 v -2.47).\n\n### Tangent on Analysing Neurons\n\nFinding that there were clean and interpretable neurons was exciting, and I\ngot pretty side tracked looking at neurons in general - no particular goal,\njust trying to gain surface area and figure out what was up. Looking at the\nneuron means across 100 games on the middle moves ([5:-5]) showed that there\nwere some major outliers, and that layer 6 and 7 were the biggest by far. (The\ngraph is sorted, because it's really hard to read graphs with 2000 points on\nthe x axis with no meaningful ordering!)\n\nI then tried looking at the direct logit attribution of the top neurons in\neach layer (top = mean > 0.2, chosen pretty arbitrarily), and they seemed\nsuper interpretable - it was visually extremely sparse, and it looked like\nmany neurons connected to a single output logit. Layer 7 had some weird\nneurons that seemed specialised to the first move. Aside: I highly recommend\nplotting heatmaps like this with 0 as white - makes it much easier to read\npositive and negative things visually (this is the plotly color scheme RdBu,\npx.imshow(tensor, color_continuous_scale='RdBu',\ncolor_continuous_midpoint=0.0) works to get these graphs)\n\n### Back to patching\n\nI then ran out of steam and went back to patching. I now tried to patch in\nindividual heads and look at their effect on the C0 logit (now normalised such\nthat 1 means \"fully recovered\" and 0 means \"no change\"). Head L7H0 was the\nmain significant one, but I couldn't get much out of it.\n\nI then tried patching in individual neurons - doing all 16000 would be too\nslow, so I just took the neurons with highest activation difference and\npatched in those - activation difference had some big outliers. I first tried\nresample ablating (replacing a clean neuron with corrupted and seeing what\nbreaks) and found that none were necessary (this isn't super surprising -\nneurons are small, and dropout incentivises redundancy), though the layer 7\nneurons matter a bit (they directly affect the logits, so this makes sense!)\n\nBut when I tried causal tracing (replacing a corrupted neuron with its clean\ncopy) I got some striking results - several neurons mattered a bunch, and\nL5N1393 was enough to recover 75% on its own?! (Notably, this was a\nsignificantly bigger effect than just its direct logit attribution)\n\n## Neuron L5N1393\n\nThis was a sufficiently wild result that I pivoted to focusing on that neuron\n(the 1393th in layer 5).\n\nMy starting goal was the incredibly narrow question \"figure out why patching\nin just that neuron into the corrupted run is such a big deal\". Again, focus\non understanding a narrow questions deeply and properly, even against a flinch\nof \"this is too narrow and there's no way it'll generalise!\".\n\nTo start with, I cached all activations on the run with a corrupted input but\na clean neuron L5N1393, and started comparing the three. The obvious place to\nstart was direct logit attribution of layers - MLP7 went from not mattering in\neither clean or corrupted to being significant?!\n\nDigging into the MLP7 neurons and their direct logit attribution, I found that\nboth clean and corrupted had a single, dominant, extremely negative neuron.\nBut in the patched run, both were significantly suppressed. My guess was that\nthis was some dropout solving circuit firing, and thus that MLP7 was mostly to\ndeal with dropout - I subjectively decided this didn't seem that interesting\nand moved on. Interestingly, this is similar to how negative name movers in\nthe Indirect Object Identification circuit act as backups - they significantly\nsuppress the model's ability to do the task, but if you ablate the positive\nname movers they'll significantly reduce their negative effect to help\ncompensate. (There it's likely a response to attention dropout)\n\nIt also significantly changed some layer 6 neurons, which seemed maybe more\nlegit:\n\nAt this point I decided to pivot to just trying to interpret neuron L5N1393\nitself, because it seemed interesting. And at this point I was pretty\nconvinced that the model had interpretable (and maybe monosemantic?) neurons.\n\nLooking at the direct logit attribution of the neuron, it strongly boosted C0\nand slightly boosted D1 (one step diagonally down and right)\n\nThe next easiest place to start was max activating dataset examples - I\ninitially felt an impulse to run the model across tens of thousands of games\nto collect the actual top dataset examples, but I realised this would be a\nheadache and probably unnecessary. I had run the model for 50 games (thus 3000\nmoves) and decided to just inspect the neuron on the top 30 (1%) of games\nthere.\n\nI manually inspected a few, and then decided to aggregate the board state\nacross the top 30 moves. I decided to try averaging \"is non-empty\", the actual\nboard state (ie 1 for black, 0 for empty, -1 for white) and the flipped board\nstate (ie 1 for mine, 0 for empty, -1 for their's) - this was kinda janky,\nsince I wanted to distinguish \"even probability of being white or black\" and\n\"always empty\", but it seemed good enough to be useful.\n\nI don't recall exactly how I had the idea for a flipped board state - I think\na combination of doing a heatmap of which games/moves the neuron fired on and\nseeing that it wasn't a consistent parity between games, but it did alternate\nwithin a game. And inspecting the top few examples, and seeing that some had\nblack at D1 and white at E2, and some had white at D1 and black at E2 (and\nalready having identified that part of the board as important). I spent a bit\nof time stuck on figuring out how best to aggregate a flipped board state,\nbefore realising I could do the stupid thing of using a for loop to generate\nan alternating tensor of 1s and -1s and just multiply by it.\n\nBut now I had the flipped board state, it was pretty clear that this was the\nright way to interpret the neuron - it was literally 1 in D1 and -1 in E2\n(here 1 meant \"their's\", because I hadn't realised I'd need a good\nconvention). I looked at the max activating dataset examples for a few other\nneurons (taking the top 10 by norm in each layer) and saw a few others that\nwere clean in the flipped state but not in the normal state, and this was\nenough to generate the idea that the relevant colour was \"next\" vs \"previous\"\nplayer (I only realised after the fact that \"my\" vs \"their\" colour was a\ncleaner interpretation, thanks to Chris Olah for this!)\n\nThis is literally written in my notes as (immediately after I briefly decided\nto go and do a deep dive on neuron L6N1339 instead lol)\n\n> Omg idea! Maybe linear probes suck because it's turn based - internal repns\n> don't actually care about white or black, but training the probe across game\n> move breaks things in a way that needs smth non-linear to patch\n\nAt this point my instincts said to go and validate the hypothesis properly,\nlook at a bunch more neurons, etc. But I decided that in the spirit of being\ndecisive and pursuing \"big if true\" hypotheses (and because at this point I\nwas late for work) I'd just say YOLO and try training a linear probe under\nthis model.\n\nI'm particularly satisfied with this decision, since I felt a lot of\nperfectionism, that I would have normally pursued, and ignoring it in the\ninterests of speed went great:\n\n  * I'd never trained a probe before, and figured there's a bunch of standard gotchas I needed to learn - eg how to deal with imbalanced class sizes (corners are normally empty), setting up good controls etc\n  * Getting a probe working on the flipped board state (across all moves) - this seemed like more of a pain to code so I just decided to do even and odd moves\n  * Figuring out the right layer to probe on - I just picked layer 6 since it was late enough to feel safe, and I didn't want to spend time figuring out the right layer to probe on\n  * I had no idea what the right optimiser or hyper-parameters for training a probe are (I just guessed AdamW with lr=1e-4,wd=1e-2,b1=0.9,b2=0.99 and batch size 100 which seemed to work)\n  * Getting accuracy to work for the probe was a headache (it involved a bunch of fiddling with one hotting the state in the right way)\n  * Getting good summary statistics of how the run was going - I decided to just have overall loss per probe, and then loss per probe on an arbitrary square (I think C2)\n  * Figuring out how to get good performance on probe training - there's a bunch of optimisations around stopping the model once it gets to the right layer, turning off autodiff on the model parameters, etc, I just decided to not bother and do the simple thing that should work.\n\nI somehow managed to write training code that was bug free on the first long\ntraining run, and could see from the training curves that my probes were\nobviously working! From here on, things felt pretty clear, and I found the\nresults in the initial section on analysing the probe!\n\n## Citation Info\n\nPlease cite this work as eg (if you have takes on how to properly cite blog\nposts, hit me up):\n\n@misc{nanda_othello_2023, title={Actually, Othello-GPT Has A Linear Emergent\nWorld Model}, url={<https://neelnanda.io/mechanistic-\ninterpretability/othello>}, journal={neelnanda.io}, author={Nanda, Neel},\nyear={2023}, month={Mar}}\n\nNeel Nanda\n\nPrevious\n\nPrevious\n\n## Tiny Mech Interp Projects: Emergent Positional Embeddings of Words\n\nNext\n\nNext\n\n## Paper Replication Walkthrough: Reverse-Engineering Modular Addition\n\n### Neel Nanda\n\nBlog About\n\nSubscribe to hear about new posts (RSS)! Give feedback here!\n\n", "frontpage": false}
