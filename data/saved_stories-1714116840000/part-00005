{"aid": "40162077", "title": "It's always DNS \u2013 \u2013 \u2013 except when it's not: A deep dive through gRPC, Ku (2022)", "url": "https://www.datadoghq.com/blog/engineering/grpc-dns-and-load-balancing-incident/", "domain": "datadoghq.com", "votes": 2, "user": "fanf2", "posted_at": "2024-04-25 19:42:04", "comments": 0, "source_title": "It's always DNS . . . except when it's not: A deep dive through gRPC, Kubernetes, and AWS networking", "source_text": "It's Always DNS . . . Except When It's Not: A Deep Dive Through GRPC, Kubernetes, and AWS Networking | Datadog\n\nRead the 2024 State of DevSecOps Study!Read the State of DevSecOps Study!\n\n  * Product\n\nInfrastructure\n\n    * Infrastructure Monitoring\n\n    * Network Performance Monitoring\n\n    * Network Device Monitoring\n\n    * Container Monitoring\n\n    * Serverless\n\n    * Cloud Cost Management\n\n    * Cloudcraft\n\nLogs\n\n    * Log Management\n\n    * Sensitive Data Scanner\n\n    * Audit Trail\n\n    * Observability Pipelines\n\nApplications\n\n    * Application Performance Monitoring\n\n    * Universal Service Monitoring\n\n    * Continuous Profiler\n\n    * Database Monitoring\n\n    * Data Streams Monitoring\n\n    * Service Catalog\n\n    * Dynamic Instrumentation\n\nSecurity\n\n    * Software Composition Analysis\n\n    * Application Security Management\n\n    * Cloud Security Management\n\n    * Cloud SIEM\n\nDigital Experience\n\n    * Browser Real User Monitoring\n\n    * Mobile Real User Monitoring\n\n    * Synthetic Monitoring\n\n    * Mobile App Testing\n\n    * Session Replay\n\n    * Error Tracking\n\nSoftware Delivery\n\n    * CI Pipeline Visibility\n\n    * Test Visibility & Intelligent Test Runner\n\n    * Continuous Testing\n\nPlatform Capabilities\n\n    * Bits AI\n\n    * OpenTelemetry\n\n    * Workflow Automation\n\n    * CoScreen\n\n    * Dashboards\n\n    * Watchdog\n\n    * Alerts\n\n    * Incident Management\n\n    * Integrations\n\n    * IDE Plugins\n\n    * API\n\n    * Case Management\n\n  * Customers\n\n  * Pricing\n\n  * Solutions\n\nIndustry\n\n    * Financial Services\n\n    * Manufacturing & Logistics\n\n    * Healthcare/Life Sciences\n\n    * Retail/E-Commerce\n\n    * Government\n\n    * Education\n\n    * Media & Entertainment\n\n    * Technology\n\n    * Gaming\n\nTechnology\n\n    * Amazon Web Services Monitoring\n\n    * Azure Monitoring\n\n    * Google Cloud Platform Monitoring\n\n    * Kubernetes Monitoring\n\n    * Red Hat OpenShift\n\n    * Pivotal Platform\n\n    * OpenAI\n\n    * SAP Monitoring\n\n    * OpenTelemetry\n\nUse-case\n\n    * Cloud Migration\n\n    * Monitoring Consolidation\n\n    * Unified Commerce Monitoring\n\n    * DevOps\n\n    * Shift-Left Testing\n\n    * Digital Experience Monitoring\n\n    * Security Analytics\n\n    * Compliance for CIS Benchmarks\n\n    * Hybrid Cloud Monitoring\n\n    * IoT Monitoring\n\n    * Machine Learning\n\n    * Real-Time BI\n\n    * On-Premises Monitoring\n\n    * Log Analysis & Correlation\n\n  * About\n\n    * Contact\n\n    * Partners\n\n    * Latest News\n\n    * Events & Webinars\n\n    * Leadership\n\n    * Careers\n\n    * Analyst Reports\n\n    * Investor Relations\n\n    * Awards\n\n    * ESG Report\n\n  * Blog\n\n    * The Monitor\n\n    * Engineering\n\n    * Pup Culture\n\n    * Security Labs\n\n  * Docs\n\n  * Login\n\n  * Get Started Free\n\n# It's always DNS . . . except when it's not: A deep dive through gRPC,\nKubernetes, and AWS networking\n\nLaurent Bernaille\n\nDavid Lentz\n\nPublished: April 13, 2022\n\nIt's always DNS . . . except when it's not: A deep dive through gRPC,\nKubernetes, and AWS networking\n\n#### Further Reading\n\nDatadog Platform Datasheet\n\nLearn about the key components, capabilities, and features of the Datadog\nplatform.\n\nDownload to learn more\n\nengineering / kubernetes / grpc / dns\n\nThis story began when a routine update to one of our critical services caused\na rise in errors. It looked like a simple issue\u2014logs pointed to DNS and our\nmetrics indicated that the impact to users was very low. But weeks later, our\nengineers were still puzzling over dropped packets, looking for clues in\nkernel code, and exploring the complexities of Kubernetes networking and gRPC\nclient reconnect algorithms. However, no single team was able to fully\nunderstand the issue from their vantage point.\n\nIn this post, we\u2019ll tell the story of how we investigated and ultimately\nresolved this incident. And we\u2019ll share some of the knowledge we gained along\nthe way, including:\n\n  * Sometimes it\u2019s actually not DNS (even when it really looks like it)\n  * AWS ENA metrics and VPC Flow Logs provide important visibility into connection tracking\n  * Cilium, Kubernetes, and AWS networking interact in elaborate ways\n  * Sometimes reverse path filtering considers packets as Martian when it shouldn\u2019t\n  * DNS propagation time is influenced by many factors\n\n## It looks like DNS\n\nBeginning in September 2021, we saw increased errors when we rolled out\nupdates to our metrics query service, which is responsible for retrieving\nreal-time and historical metric data from our data stores. It makes these\nmetrics available to multiple clients, including our frontend web application\nand monitor evaluation clients, which use that data to determine whether to\nalert.\n\nClients automatically retry failed queries, so user-facing errors were\nminimized. But the retries introduced latency that affected the performance of\ndashboards and monitors.\n\nThe service\u2019s logs showed us that DNS errors were preventing it from\nconnecting to its dependencies\u2014the data stores that hold our metric data. With\nthis information, our next move was to investigate DNS activity inside the\nKubernetes cluster where the service was hosted.\n\n### NodeLocal DNSCache reaches its concurrency limit\n\nWe use NodeLocal DNSCache in our infrastructure to improve performance by\nhaving a local DNS cache on every node. NodeLocal DNSCache is deployed as a\nDaemonSet\u2014named node-local-dns\u2014that runs a slimmed-down version of CoreDNS.\n\nWe saw that the node-local-dns pods were experiencing OOM errors. They were\nconfigured with a 64 MB memory limit and a max_concurrent parameter that\nlimits them to processing 1,000 concurrent requests. We confirmed that\u2014as\nexpected\u2014they were rejecting requests above that concurrency limit during\nrollouts.\n\nWe knew from the OOM errors that 64 MB of memory was not enough to support our\ncaches plus 1,000 concurrent requests, so we increased the pods\u2019 memory\nallocation to 256 MB. Although this stopped the OOM errors, the DNS errors\npersisted.\n\nIt was also unclear why node-local-dns was even reaching its limit of 1,000\nconcurrent requests. We expect it to process each query in about 5 ms or less,\nso even with a 0 percent cache hit rate, it should support at least 200,000\nqueries per second. The service\u2019s request rate was mostly around only 400\nqueries per second. This increased to nearly 2,000 during rollouts, but\nremained far below its expected capacity.\n\nAt this point, we knew that rollouts of the service caused errors which\nincreased user-facing latency, and that DNS failures were causing those\nerrors. We looked at node-local-dns metrics and discovered that the local DNS\ncache was marking upstream DNS resolvers as unhealthy.\n\nEach request that node-local-dns forwards upstream consumes a slot that counts\ntoward the max_concurrent limit. Node-local-dns establishes TCP connections to\nupstream resolvers and reuses these connections until they expire (after 10\nseconds, by default). Health check failures likely indicated that node-local-\ndns could not establish connections to these upstreams, which would also\nexplain why we were reaching the max_concurrent limit: the forward plugin has\na 5-second timeout to get an answer from upstreams so if it can not connect to\nthem, we reach the limit with only 200 queries per second.\n\nWe began to wonder if this could be network related. We confirmed that we were\nwell below the 5-Gbps maximum sustained throughput capacity of the instance\ntypes we were using, but we did see a rise in TCP retransmits correlated with\nthe rollouts.\n\nThe increase in TCP retransmits indicated that packets were being lost, most\nlikely due to network saturation. We suspected that microbursts of\ndata\u2014traffic spikes too brief to register on the throughput graph\u2014were causing\nbrief bottlenecks, so we expanded our investigation to include additional\nnetworking-related metrics from AWS.\n\n## AWS VPC connection tracking leads to dropped packets\n\nBefore we rolled out the next update of the metrics query service, we enabled\nElastic Network Adapter (ENA) metrics on the Datadog Agent to gain deeper\nvisibility into network performance. Some of these ENA metrics, like bandwidth\nand throughput, didn\u2019t show any issues that correlated with the rollouts.\nHowever, we did spot an increase in a metric named\nconntrack_allowance_exceeded that looked significant.\n\nThis metric tracks the number of packets that get dropped when the VPC\u2019s\nconnection tracking (conntrack) mechanism becomes saturated. Conntrack keeps\ntabs on the status of network traffic to and from the host and allows for\nnetwork features like the stateful packet filtering used in EC2 security\ngroups. Each conntrack table entry represents a connection\u2014a flow of related\npackets. Conntrack updates each connection\u2019s status in the table as the flow\ncontinues, and then removes connections once they have timed out or completed.\nThe conntrack table sets an upper limit on the number of entries it can\nsupport. Once the table is full, the host can\u2019t create any additional\nconnections.\n\nConnection tracking on these instances takes place in two separate conntrack\ntables: the VPC conntrack\u2014which is a per instance conntrack maintained at the\nhypervisor level\u2014and the Linux conntrack within each instance.\n\nWe were surprised when the ENA metrics showed us that the VPC conntrack was\nsaturated, because we could see that the number of connections in the Linux\nconntrack\u2014shown below\u2014was relatively low. We\u2019d seen instances manage hundreds\nof thousands of connections without any issues, but during these rollouts,\nconnection counts were peaking below 60,000.\n\nWe opened a case with AWS support, who confirmed that the capacity of the\nLinux conntrack varies by instance type, and that the maximum number of\nentries for the VPC conntrack was much higher than we were observing in the\nLinux conntrack. This made little sense to us at that point: we were\nsaturating the VPC conntrack but the Linux conntrack showed a number of\nentries it should have easily managed. We decided to test larger instance\ntypes\u2014which, according to AWS, could track more connections\u2014and we verified\nthat we could resolve this issue simply by scaling up the service\u2019s\ninfrastructure. But we wanted to understand why the table was full so that we\ncould find a more sustainable and cost-efficient path forward.\n\nWe needed to know more about the service\u2019s network patterns to understand why\nthe conntrack was filling up, so we started looking at Amazon VPC Flow Logs.\n\n### VPC Flow Logs shed light on a full conntrack\n\nVPC Flow Logs are an extremely useful tool for monitoring low-level networking\nbehaviors. In this case, they revealed some key information about the metrics\nquery service\u2019s traffic.\n\nWhen we rolled out another update of the service, we analyzed VPC Flow Logs to\nsee the volume of traffic to and from each pod as it got replaced by a new\none. We saw that after a pod had been deleted, metrics query clients were\nstill trying to connect to its old IP address. We expected only a few\nconnection attempts during the short period of time before clients knew that\nthe old pod was no longer available.\n\nTo learn more, we aggregated our VPC Flow Logs by TCP flag to see the types of\ntraffic coming in. The graph below shows ingress traffic to a single old IP\naddress. The blue line shows a steady rate of long-lived connections that were\nestablished prior to the rollout. The red line shows FIN requests coming from\nclients\u2014as expected\u2014when gRPC gracefully shuts down the connection. The yellow\nline shows that, at the start of the rollout, clients generated a very high\nrate of SYN packets trying to establish communication with the service\u2019s\nhosts. Based on the source IP addresses we saw in the VPC Flow Logs, we knew\nthat these connection requests were coming from a subset of clients,\nspecifically clients responsible for evaluating metric data. We expected to\nsee a proportional increase in RST packets, which indicate that the host will\nnot accept the connection request. But instead\u2014surprisingly\u2014we saw that the\nSYN packets were unanswered. This data showed us that a single old IP address\nreceived ~90,000 connection attempts in about 90 seconds.\n\nAt this point, ENA metrics had shown us that a saturated VPC conntrack was\npreventing network connections and leading to DNS errors. And VPC Flow Logs\nhad revealed that a specific type of client was sending SYN requests to try to\nconnect to the old pod, but the host was not responding.\n\n## Learning how traffic is moving inside the node\n\nTo understand why the VPC conntrack was filling up\u2014and why the Linux conntrack\nwas not\u2014we looked deeper into what was happening within the network. We\nfocused on two key elements that define how packets travel to and from the\nmetrics query service: Cilium and reverse path filtering.\n\n### A look at how Cilium, Kubernetes, and AWS networking manage pods and\nrouting\n\nThe EC2 instances that host the metrics query service pods use two Elastic\nNetwork Interfaces (ENIs). The primary ENI\u2014identified on the host as\nens5\u2014sends and receives traffic to and from processes running in the host\nnetwork namespace, while the secondary ENI\u2014ens6\u2014sends traffic to and from the\npods on the host.\n\nWe use Cilium in ipam:eni mode mode to manage networking within the Kubernetes\ncluster. When a new pod is created, Cilium is responsible for enabling that\npod to send and receive traffic to and from the rest of the cluster. It\nreserves a subset of IP addresses from the VPC\u2019s CIDR range and provides a\nnative routable address for each pod on the secondary ENI.\n\nCilium creates a virtual Ethernet device (veth) that will serve as the\nendpoint for traffic destined to the pod\u2019s IP. Next, it updates the host\u2019s\nroute table with a route to that veth so that traffic coming in from the VPC\ncan reach the pod. It also adds an IP routing rule on the host to send\noutgoing traffic from the pod\u2019s IP through the secondary ENI.\n\nAt the start of a rollout, Kubernetes deletes each old pod and Cilium removes\nits corresponding route table entry and IP routing rule on the host. At this\npoint, even though the old pod no longer exists, VPC networking will still\nsend traffic destined to its IP address to the secondary ENI on the host where\nit had lived. To understand what was happening to packets sent to the old IP\naddress, we simulated traffic and analyzed the host\u2019s routing behavior. We\ncreated two nodes, A and B, and sent traffic between them.\n\nFrom node A, with IP 10.a.b.c, we sent packets to 10.x.y.z, an IP address\nwhich was allocated to node B\u2019s secondary ENI (ens6) but not used by a pod on\nnode B:\n\n    \n    \n    nodeA:~$ nc -vz 10.x.y.z 12345\n\nThere was no response from node B, but it did see the incoming SYN request and\nretries:\n\n    \n    \n    nodeB:~$ sudo tcpdump -pni ens6 \"port 12345\" listening on ens5, link-type EN10MB (Ethernet), capture size 262144 bytes 08:28:52.086251 IP 10.a.b.c.51718 > 10.x.y.z.12345: Flags [S], seq 4126537246, win 26883, options [mss 8961,sackOK,TS val 2002199904 ecr 0,nop,wscale 9], length 0\n\nLooking up the relevant route showed an error:\n\n    \n    \n    $ ip route get 10.x.y.z from 10.a.b.c iif ens6 RTNETLINK answers: Invalid cross-device link\n\nWe knew that this error came from reverse path filtering, and when we examined\nthe kernel logs for further information, we saw that our test traffic was\nbeing identified as Martian packets:\n\n    \n    \n    Oct 28 08:25:54 ip-10-y-y-z kernel: IPv4: martian source 10.x.y.z from 10.a.b.c, on dev ens6\n\nThis simulation showed us that reverse path filtering was dropping packets, so\nnext we set out to discover why.\n\n### Reverse path filtering drops Martian packets\n\nReverse path filtering is a Linux security feature that examines each packet\nand determines its validity based on its source, destination, and return\nroute. This helps prevent spoofing by identifying and ignoring Martian\npackets\u2014requests whose return path would use an interface other than the one\non which they arrived.\n\nDuring the rollout, whenever a client sent a SYN packet to a deleted pod\u2019s IP\naddress, VPC routed it to the secondary ENI on the relevant host. Although it\nwas coming in on the secondary ENI, there was no return route using that\ninterface because Cilium deleted the IP rule when Kubernetes deleted the old\npod. This meant that the host\u2019s reply to that request would go out via the\ndefault route\u2014the primary ENI. But reverse path filtering saw this as a\nMartian packet and dropped it before it got added to the Linux conntrack. The\nhost did not respond with a SYN-ACK, an RST, or an ICMP host unreachable\npacket, so clients were unaware that their connection requests were directed\nat an outdated IP address.\n\nWe saw kernel logs confirming this behavior:\n\n    \n    \n    Oct 28 08:25:54 nodeB kernel: IPv4: martian source 10.x.y.z from 10.a.b.c, on dev ens6\n\nThis confirmed that reverse path filtering was dropping packets, but this\nservice\u2019s hosts are configured to use loose mode\u2014which allows for asymmetrical\nrouting\u2014so we did not expect these packets to get dropped. Loose mode should\nonly drop packets if they\u2019re not routable through any interface; a packet that\narrives on the secondary ENI should be allowed even if its return route is on\nthe primary ENI. (Strict mode, on the other hand, drops packets that can\u2019t be\nrouted back out through the interface on which they arrived.) Reverse path\nfiltering was behaving as if it were in strict mode, when in fact it was in\nloose mode.\n\nWe didn\u2019t understand the reason for this until after we\u2019d closed the incident.\nWe explored the kernel code that implements reverse path filtering, and we\ndiscovered that the RTNETLINK answers: Invalid cross-device link error\u2014and the\nMartian packet log\u2014were triggered by the return -EXDEV statement here:\n\n    \n    \n    e_rpf: return -EXDEV;\n\nLeading to that return statement, we found the last_resort condition, which\nchecks the value of the rpf parameter (which is 2 in our case, representing\nloose mode):\n\n    \n    \n    last_resort: if (rpf) goto e_rpf;\n\nBut how did we get to last_resort? This test looked suspicious:\n\n    \n    \n    if (no_addr) goto last_resort;\n\nno_addr is set here:\n\n    \n    \n    no_addr = idev->ifa_list == NULL;\n\nifa_list contains the list of IPs associated with the device, but in our case\nens6\u2014the secondary ENI\u2014does not have an IP address. We added an IP address\n(that was completely unrelated to the addresses we use in our network) to ens6\nand tested again.\n\n    \n    \n    $ ip addr add 192.168.1.1/32 dev ens6 $ ip route get 10.x.y.z from 10.a.b.c iif ens6 10.x.y.z from 10.a.b.c via 10.m.n.1 dev ens5 cache iif ens6\n\nThis confirmed that reverse path filtering was behaving as expected only if\nthe secondary ENI had an IP address assigned. This part of our investigation\nhelped us understand why reverse path filtering was not behaving as expected,\neven though we could not explain why the no_addr check was written this way.\n\nTo prevent requests for deleted pods from becoming Martian packets, we later\ncontributed a PR to the Cilium project to add an \u201cunreachable\u201d route for the\nold pod\u2019s IP address so that incoming SYNs are not dropped by reverse path\nfiltering and clients get a clear ICMP error.\n\nMeanwhile, we now understood that the Linux conntrack was not filling up\nbecause reverse path filtering was dropping those packets before they were\nadded to the conntrack table. We knew that metrics query clients were sending\na high rate of SYN requests to the old IP address, but we didn\u2019t understand\nwhy. So we turned our investigation to the gRPC settings that define the\nbehavior of the clients.\n\n## Understanding DNS propagation time\n\nTo explain the clients\u2019 delay connecting to the service\u2019s new pods, we wanted\nto understand how\u2014and how quickly\u2014changes in the pods\u2019 IP addresses were\npropagating after the rollout. We found several factors that played a role in\nDNS propagation, and we analyzed each to see how it might be adding latency to\nthe process.\n\nWe use external-dns to provide communication between services deployed across\nmultiple clusters. Our external-dns configuration uses a 15-second TTL, and\nthe version we use doesn\u2019t update a pod\u2019s DNS record until the kubelet has\nfinished deleting the pod. (Newer versions of external-dns deregister the old\naddress as soon as the Kubernetes API receives the delete command.) After\nreceiving the SIGTERM, the service calls GracefulStop and then applies a\n10-second timeout before shutting down and allowing the kubelet to finalize\npod deletion. After the pod is deleted, its DNS address in Route 53 is not\nupdated until the next external-dns sync, which in our case takes place every\n15 seconds. Based on this, we expected an average of 25 seconds (10 seconds\nfor the timeout and 15 seconds for the TTL expiration and Route 53 update)\u2014and\na worst-case scenario of 40 seconds\u2014for the updated DNS record to be available\nto the client.\n\nIf a connection is lost, the gRPC client will resolve the service\u2019s DNS name\nto re-establish a connection. It will continue to re-resolve the DNS name\nuntil it has successfully connected to the backends it needs to reach. The\nclient\u2019s re-resolution frequency\u2014regardless of the frequency of connection\nattempts\u2014is based on its min_time_between_resolutions parameter, which gRPC\nprovides in order to avoid overloading the DNS server. The clients are\nconfigured with the default value for this parameter, so they would wait 30\nseconds between resolutions. Accounting for all the necessary DNS update\nactivity, clients have the updated DNS information 30 or\u2014more likely\u201460\nseconds after the rollout has initiated pod deletion.\n\n## Why were clients sending so many SYN requests?\n\nWe now knew that clients continued to send SYN requests to old pods during the\n~60 seconds it took them to get updated DNS information, but we still didn\u2019t\nknow why they were trying to connect at such a high rate.\n\nWe began to look for an explanation in the gRPC settings of the service\u2019s\nclients, and we found a change in their load balancing policy that appeared to\ncorrelate with this issue. Our clients had historically used gRPC\u2019s pick_first\nload balancing policy. In June 2021, we resolved a separate issue by changing\nthe affected service\u2019s clients to use gRPC\u2019s round_robin load balancing\npolicy. Following that incident, we decided to apply round_robin as the\ndefault policy for clients across multiple services. When we rolled out an\nupdate to the metrics query clients in August 2021, the new default was\napplied and the clients started using the round_robin configuration for the\nfirst time.\n\nWe believed that this change explained the clients\u2019 high rate of SYN requests.\nWith the pick_first load balancing policy, each client connects to only one of\nthe service\u2019s backends. But when they use the round_robin policy, clients\nconnect to all backends. Because each client sends a SYN request to open each\nconnection, we believed that the round_robin policy explained the SYN flood we\nsaw in the VPC Flow Logs.\n\nBut on closer look, we discovered that this application was creating one gRPC\nchannel for each backend, so pick_first and round_robin should have behaved\nthe same, since each channel was only given a single target.\n\nWe no longer suspected that the SYN flood was caused by the number of\nconnections, but we knew that it correlated with the change in gRPC load\nbalancing policies, so we looked for an explanation in their reconnect\nbehavior. Using the pick_first configuration, if clients became disconnected\nfrom the service, they would not automatically try to reconnect. Instead, they\nwould wait for the application to request a reconnection. When we switched\nclients to use the round_robin policy, they would automatically reconnect any\ntime a channel became disconnected.\n\nOne factor in the clients\u2019 reconnect rate is their configured connection\nbackoff value. When we looked at the client code, we found a very aggressive\ngRPC reconnect timeout\u2014a parameter we had set a long time ago\u2014that caused them\nto retry more frequently when they could not connect. The pick_first load\nbalancing policy had generated relatively few reconnects, so this aggressive\nsetting did not have a noticeable effect until we changed to the round_robin\npolicy. The service has ~900 clients\u2014about 600 pods, each with 15 containers.\nEach client was trying to reconnect approximately every 300 milliseconds,\nresulting in a total of up to 30,000 SYN requests per second.\n\nRecall that the SYN requests sent to the old pods were dropped by reverse path\nfiltering, but not before they were added to the VPC conntrack. Normally, the\nVPC connection tracking logic would maintain these records, updating their\nstatus when the host replied with an RST or an ACK (to acknowledge the\nrequest). Connection requests that aren\u2019t acknowledged within 60 seconds\nshould be automatically removed from the VPC conntrack table, but during the\nrollout, client requests were accumulating in the table much faster than they\nwere being removed.\n\nOnce we understood that our custom gRPC reconnect parameter was causing the\nSYN floods, we created a PR to remove those parameters\u2014shown below\u2014and restore\nthe defaults.\n\nThe next rollout did not show the errors we had seen correlated with rollouts\nbeginning in September, and we resolved the incident.\n\n## The ongoing challenge of complexity\n\nThis series of incidents showed us that edge cases within the powerful\nabstractions we rely on\u2014gRPC, Kubernetes, and AWS\u2014occasionally lead to errors\nthat surface a complexity that is hidden most of the time. It forced us to\ngain a deeper understanding of our gRPC configuration, which brought insight\ninto behaviors we had seen in our services for quite some time but had not\nfully understood. After spending months investigating, we ultimately found the\nroots of these incidents in changes we\u2019d made ourselves. These changes\u2014to work\naround a bug in a dependency, resolve previous incidents, and override default\nreconnect parameters\u2014seemed safe at the time, but they had unintended effects\nthat surprised us much later when it was difficult to spot the correlations.\n\nWe hope you\u2019ve found this story intriguing and illuminating. If so, you may\nalso be interested in working with us.\n\nWant to work on projects like this? We're hiring!\n\n### Related jobs at Datadog\n\nENGINEERING\n\nSoftware Engineer - Fabric RPC (Lisbon)\n\nLisbon,\n\nENGINEERING\n\nSoftware Engineer - Fabric RPC\n\nParis, Madrid, Bordeaux, Lyon, Nantes, Nice, Montpellier, Grenoble,\n\nENGINEERING\n\nSenior Software Engineer - Network Edge\n\nNew York, Massachusetts\n\nSee all jobs at Datadog\n\n#### Further Reading\n\nDatadog Platform Datasheet\n\nLearn about the key components, capabilities, and features of the Datadog\nplatform.\n\nDownload to learn more\n\n### Start monitoring your metrics in minutes\n\nFIND OUT HOW\n\nFree Trial\n\nDownload mobile app\n\nProduct\n\nFeatures Infrastructure Monitoring Container Monitoring NPM NDM Serverless\nCloud Cost Management Cloudcraft Log Management Sensitive Data Scanner APM\nError Tracking Continuous Profiler Data Streams Monitoring Database Monitoring\nCI Pipeline Visibility Test Visibility & Intelligent Test Runner Service\nCatalog Dynamic Instrumentation Universal Service Monitoring\n\nBrowser Real User Monitoring Mobile Real User Monitoring Synthetic Monitoring\nMobile App Testing Continuous Testing Session Replay Software Composition\nAnalysis Application Security Management Cloud Security Management Cloud SIEM\nBits AI Workflow Automation CoScreen Dashboards Watchdog Alerts Incident\nManagement Integrations API Case Management\n\nResources\n\nPricing Documentation Support Certification Open Source\n\nEvents and Webinars Security Privacy Center Knowledge Center Learning\nResources\n\nAbout\n\nContact Us Partners Press Leadership Careers\n\nLegal Investor Relations Analyst Reports ESG Report Vendor Help\n\nBlog\n\nThe Monitor Engineering\n\nPup Culture Security Labs\n\n\u65e5\u672c\u8a9e\n\n\u00a9 Datadog 2024 Terms | Privacy |\n\nCookie Preferences\n\n###### Request a personalized demo with a Datadog engineer\n\nGet Started with Datadog\n\nWE VALUE YOUR PRIVACY\n\nWe use cookies and similar technologies (\"Cookies\") to provide and improve our\nservices and for marketing, as outlined in our Privacy Policy and Cookie\nPolicy. By clicking \"Accept All,\" you agree that you consent to our use of\nCookies.\n\nPrivacy Policy\n\nPowered by:\n\n", "frontpage": false}
