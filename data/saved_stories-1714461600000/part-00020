{"aid": "40204611", "title": "Testing LLMs for sequence extraction attacks", "url": "https://dynamo.ai/blog/testing-llms-for-data-leakage-vulnerabilities-with-dynamoeval", "domain": "dynamo.ai", "votes": 1, "user": "whoami_nr", "posted_at": "2024-04-29 21:59:07", "comments": 0, "source_title": "Testing LLMs for Data Leakage Vulnerabilities with DynamoEval", "source_text": "Testing LLMs for Data Leakage Vulnerabilities with DynamoEval\n\nRequest Demo\n\nProduct\n\nApr 29, 2024\n\n# Testing LLMs for Data Leakage Vulnerabilities with DynamoEval\n\n## Low-code tools are going mainstream\n\nPurus suspendisse a ornare non erat pellentesque arcu mi arcu eget tortor eu\npraesent curabitur porttitor ultrices sit sit amet purus urna enim eget.\nHabitant massa lectus tristique dictum lacus in bibendum. Velit ut viverra\nfeugiat dui eu nisl sit massa viverra sed vitae nec sed. Nunc ornare consequat\nmassa sagittis pellentesque tincidunt vel lacus integer risu.\n\n  1. Vitae et erat tincidunt sed orci eget egestas facilisis amet ornare\n  2. Sollicitudin integer velit aliquet viverra urna orci semper velit dolor sit amet\n  3. Vitae quis ut luctus lobortis urna adipiscing bibendum\n  4. Vitae quis ut luctus lobortis urna adipiscing bibendum\n\n### Multilingual NLP will grow\n\nMauris posuere arcu lectus congue. Sed eget semper mollis felis ante. Congue\nrisus vulputate nunc porttitor dignissim cursus viverra quis. Condimentum nisl\nut sed diam lacus sed. Cursus hac massa amet cursus diam. Consequat sodales\nnon nulla ac id bibendum eu justo condimentum. Arcu elementum non suscipit\namet vitae. Consectetur penatibus diam enim eget arcu et ut a congue arcu.\n\nVitae quis ut luctus lobortis urna adipiscing bibendum\n\n#### Combining supervised and unsupervised machine learning methods\n\nVitae vitae sollicitudin diam sed. Aliquam tellus libero a velit quam ut\nsuscipit. Vitae adipiscing amet faucibus nec in ut. Tortor nulla aliquam\ncommodo sit ultricies a nunc ultrices consectetur. Nibh magna arcu blandit\nquisque. In lorem sit turpis interdum facilisi.\n\n  * Dolor duis lorem enim eu turpis potenti nulla laoreet volutpat semper sed.\n  * Lorem a eget blandit ac neque amet amet non dapibus pulvinar.\n  * Pellentesque non integer ac id imperdiet blandit sit bibendum.\n  * Sit leo lorem elementum vitae faucibus quam feugiat hendrerit lectus.\n\n##### Automating customer service: Tagging tickets and new era of chatbots\n\nVitae vitae sollicitudin diam sed. Aliquam tellus libero a velit quam ut\nsuscipit. Vitae adipiscing amet faucibus nec in ut. Tortor nulla aliquam\ncommodo sit ultricies a nunc ultrices consectetur. Nibh magna arcu blandit\nquisque. In lorem sit turpis interdum facilisi.\n\n> \u201cNisi consectetur velit bibendum a convallis arcu morbi lectus aecenas\n> ultrices massa vel ut ultricies lectus elit arcu non id mattis libero amet\n> mattis congue ipsum nibh odio in lacinia non\u201d\n\n###### Detecting fake news and cyber-bullying\n\nNunc ut facilisi volutpat neque est diam id sem erat aliquam elementum dolor\ntortor commodo et massa dictumst egestas tempor duis eget odio eu egestas nec\namet suscipit posuere fames ded tortor ac ut fermentum odio ut amet urna\nposuere ligula volutpat cursus enim libero libero pretium faucibus nunc arcu\nmauris sed scelerisque cursus felis arcu sed aenean pharetra vitae suspendisse\nac.\n\nRecent studies have shown that large language models can memorize and later\nemit verbatim text from their training data when prompted. This poses\npotential privacy risks and legal liability, as the training data may contain\nsensitive, copyrighted, or personally identifiable information. Real world\nexamples of commercial AI offerings generating copyrighted or non-\ndistributable data have already resulted in legal action. Even as model\nproviders attempt to patch data extraction issues like the vulnerabilities\nthat DeepMind has publicly identified, model deployers may need to\ncontinuously patch this vulnerability as new attacks are identified by\nenterprises.\n\nWe often speak to enterprises that are concerned about productionizing an AI\nsystem trained on a large corpus of undisclosed data, where that AI system\ncould generate copyrighted or sensitive text from that dataset. While the\nlegal basis for this concern is still an open topic of debate, our enterprise\ncustomers commonly cite statements by the White House Executive Order, which\nhas charged the US Copyright Office to \u201cissue recommendations to the President\non potential executive actions relating to copyright and AI''. Similarly, our\ncustomers refer to the FTC\u2019s recent comment that \u201ctraining an AI tool on\nprotected expression without the creator\u2019s consent\" could result in an AI\nsystem that \u201cexploits a creator\u2019s reputation\u201d and \u201creveals private\ninformation\u201d that causes \u201csubstantial injury to customers\u201d.\n\nGiven these statements from regulators, it's more important than ever for\norganizations to test if their language models are at risk of memorizing and\nleaking sensitive or protected data. For the last year, the Dynamo AI team has\nbeen working closely with customers to streamline Data Extraction Attacks as\npart of our comprehensive privacy suite. We\u2019re excited to detail how this test\nhas enabled organizations to identify and mitigate potential data leakage\nvulnerabilities in their language models before production-level deployment.\n\nKey Features and Benefits:\n\n  * Supports all popular open-source and commercial (OpenAI, Azure, Bedrock, etc.) language models\n  * Supports attack techniques and metrics from state of the art literature\n  * Recommendations for defending models against data extraction, including privacy-preserving training techniques, guardrails, and base model selection guidance\n  * Can be customized to work with any dataset which was used to train the model\n\nThe figure below exhibits a real world example of the Data Leakage Attack with\na paragraph from the novel \"Harry Potter and the Sorcerer's Stone\". We provide\nthe first 22 words (the prefix) of the paragraph to the Llama 2 13B language\nmodel. Asking the model to complete the paragraph, we see that the model is\nable to output 40 identical words from the original text (colored in red),\ndemonstrating high likelihood the model has seen the original paragraph from\nHarry Potter in its training corpus.\n\n## Our approach\n\nThe Data Extraction attack is designed to simulate an attacker's attempt to\ndetermine if a document corpus was included in the pre-training or fine-tuning\ndataset of a model. We have a suite of proprietary prompting strategies to\nuncover memorized pieces of text from models. As an example of a basic test we\nperform, DynamoEval will prompt the AI system with the first few words in a\nprotected paragraph from the training dataset and analyze whether the model's\ncompletion matches the original text. We employ a set of similarity\nthresholds, including trigram memorization, exact starting word memorization,\nand overlapping words memorization, to identify if the generated text can be\nclassified as \"memorized.\" It assumes the adversary has black-box access to\nthe model, allowing them to observe the generated text based on a given\nprompt.\n\n## Running Data Extraction test in Dynamo AI platform\n\nYou can easily run a data extraction attack using our SDK or the Dynamo AI\ndashboard. In the figure below, you can see the SDK reference for running a\ntest.\n\n    \n    \n    dfl = DynamoFL(DYNAMOFL_API_KEY, host=DYNAMOFL_HOST) test = dfl.data_extraction_test( name = \"Data Extraction - Llama 2 - Harry Potter\", model_key = model.key, dataset_id = dataset.id, gpu = GPUConfig(gpu_type = GPUType.V100, gpu_count = 1), memorization_granularity = \"paragraph\", sampling_rate = 1000, grid = [ { 'prompt_length': [256, 512], 'temperature': [0, 0.5, 0.7, 1.0] } ] )\n\n  * name: name of the test\n  * model_key: model key for the generator model tested\n  * datsaet_id: dataset id containing the reference text which has to be extracted\n  * gpu: type and number of GPU(s) to be used for the test\n  * memorization_granularity: Granularity of memorization (Ex: paragraph, sentence)\n  * grid: a set of test hyperparameters to be searched (model\u2019s temperature, prompt length)\n  * sampling_rate: Number of times the model will be queried during the attack\n\n## Mitigation measures\n\nTo help organizations defend against data extraction attacks, Dynamo AI\nprovides tooling and guidance for implementing the following countermeasures:\n\n  1. Guardrails (Fine-tuning and Pre-training): Implement guardrails that prevent language models from completing data extraction requests from users. These guardrails act as a first line of defense, blocking attempts to extract sensitive memorized data. Our AI guardrail, DynamoGuard specifically helps protect you against this attack.\n  2. Privacy-Mitigation Techniques (Fine-tuning): Apply techniques such as differential privacy and deduplication during the fine-tuning process. Differential privacy adds noise to the training data, making it harder to extract specific data points. Deduplication removes exact copies of sensitive data from the training set, reducing the risk of memorization. Dynamo AI provides a fine tuning SDK, DynamoEnhance, that implements these methods.\n  3. Smaller Models (Fine-tuning): Research suggests that smaller models are less prone to memorizing their training data verbatim. Leveraging DynamoEval can help organizations identify the optimal model size By iteratively fine-tuning with different model sizes that balances performance and privacy.\n\nBy leveraging Dynamo AI\u2019s tooling and expertise, organizations can\nsignificantly reduce the risk of data leakage through data extraction attacks.\nOur comprehensive approach addresses vulnerabilities at both the fine-tuning\nand pre-training stages, ensuring that language models are deployed with the\nutmost security and privacy.\n\n## Contact us\n\nAs LLMs become increasingly powerful and widely adopted, the risk of exposing\nsensitive information from training datasets grows. As part of our commitment\nto providing holistic privacy solutions, the Data Extraction Attack\ncomplements our existing suite of attacks, which includes PII Extraction, PII\nInference and Membership Inference. With Dynamo AI's comprehensive privacy\nsolutions, teams can effectively measure, address, and prevent data leakage,\nensuring the responsible deployment and use of LLMs while safeguarding\nsensitive information.\n\nWe also offer a range of AI privacy and security solutions to help you build\ntrustworthy and responsible AI systems. To learn more about how Dynamo AI can\nhelp you evaluate and improve your RAG models, or to explore our AI privacy\nand security offerings, please reach out to us at our contact page.\n\nAlbert Sun\n\nMachine Learning Engineer\n\nNikhil Ramesh\n\nProduct Manager\n\n## Popular posts\n\nResearch\n\nApril 25, 2024\n\n### How to (accurately) evaluate RAG systems on tabular data\n\nResearch\n\nApril 23, 2024\n\n### Tackling the Explainability Gap in RAG Hallucination Evals\n\n## Latest articles\n\nBrowse all articles\n\nResearch\n\nApr 25, 2024\n\n### How to (accurately) evaluate RAG systems on tabular data\n\nResearch\n\nApr 24, 2024\n\n### Tackling the Explainability Gap in RAG Hallucination Evals\n\nProduct\n\nApr 17, 2024\n\n### DynamoFL is now Dynamo AI: the End-to-End Platform for Productionizing\nSecure and Compliant AI for Enterprises\n\nThe enterprise platform for enabling private, secure, and regulation-compliant\nGen AI models\n\nPlatform\n\n  * DynamoEnhance\n  * DynamoEval\n  * DynamoGuard\n\nCompany\n\n  * Careers\n  * Legal\n\n  * Blog\n  * Solutions\n\n### Compliance-Ready AI for the Enterprise\n\nReady your organization for emerging regulations with safe and secure\nproduction-grade Gen AI.\n\nRequest a Demo\n\nCopyright \u00a9 AI X plus | Designed by BRIX Templates - Powered by Webflow\n\n", "frontpage": false}
