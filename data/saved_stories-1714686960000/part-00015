{"aid": "40237546", "title": "Show HN: SpRAG \u2013 Open-source RAG implementation for challenging real-world tasks", "url": "https://github.com/SuperpoweredAI/spRAG", "domain": "github.com/superpoweredai", "votes": 7, "user": "zmccormick7", "posted_at": "2024-05-02 15:44:51", "comments": 0, "source_title": "GitHub - SuperpoweredAI/spRAG: High-performance RAG framework for unstructured data", "source_text": "GitHub - SuperpoweredAI/spRAG: High-performance RAG framework for unstructured\ndata\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nSuperpoweredAI / spRAG Public\n\n  * Notifications\n  * Fork 0\n  * Star 5\n\nHigh-performance RAG framework for unstructured data\n\n### License\n\nMIT license\n\n5 stars 0 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# SuperpoweredAI/spRAG\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nzmccormick7Update README.mdMay 2, 2024515cf30 \u00b7 May 2, 2024May 2, 2024\n\n## History\n\n39 Commits  \n  \n### eval/financebench\n\n|\n\n### eval/financebench\n\n| added min versions and updated file paths| Apr 30, 2024  \n  \n### sprag\n\n|\n\n### sprag\n\n| convert tests to use 'unittest' library| May 1, 2024  \n  \n### tests\n\n|\n\n### tests\n\n| convert tests to use 'unittest' library| May 1, 2024  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| Initial commit| Apr 17, 2024  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Initial commit| Apr 17, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update README.md| May 2, 2024  \n  \n### VERSION\n\n|\n\n### VERSION\n\n| add version file| May 1, 2024  \n  \n### requirements.txt\n\n|\n\n### requirements.txt\n\n| langchain-text-splitters name fix| May 1, 2024  \n  \n### setup.py\n\n|\n\n### setup.py\n\n| udpate contact url| May 2, 2024  \n  \n## Repository files navigation\n\n# spRAG\n\nspRAG is a high-performance RAG framework for unstructured data. It is\nespecially good at handling complex queries over dense text, like financial\nreports and legal documents.\n\nspRAG achieves substantially higher accuracy than vanilla RAG baselines on\ncomplex open-book question answering tasks. On one especially challenging\nbenchmark, FinanceBench, spRAG gets accurate answers 83% of the time, compared\nto the vanilla RAG baseline which only gets 19% of questions correct.\n\nThere are two key methods used to improve performance over vanilla RAG\nsystems:\n\n  1. AutoContext\n  2. Relevant Segment Extraction (RSE)\n\n#### AutoContext\n\nAutoContext automatically injects document-level context into individual\nchunks prior to embedding them. This gives the embeddings a much more accurate\nand complete representation of the content and meaning of the text. In our\ntesting, this feature leads to a dramatic improvement in retrieval quality. In\naddition to increasing the rate at which the correct information is retrieved,\nAutoContext also substantially reduces the rate at which irrelevant results\nshow up in the search results. This reduces the rate at which the LLM\nmisinterprets a piece of text in downstream chat and generation applications.\n\nThe implementation of AutoContext is fairly straightforward. All we do is\ngenerate a 1-2 sentence summary of the document, add the file name to it, and\nthen prepend that to each chunk prior to embedding it.\n\n#### Relevant Segment Extraction\n\nRelevant Segment Extraction (RSE) is a post-processing step that takes\nclusters of relevant chunks and intelligently combines them into longer\nsections of text that we call segments. These segments provide better context\nto the LLM than any individual chunk can. For simple factual questions, the\nanswer is usually contained in a single chunk; but for more complex questions,\nthe answer usually spans a longer section of text. The goal of RSE is to\nintelligently identify the section(s) of text that provide the most relevant\ninformation, without being constrained to fixed length chunks.\n\nFor example, suppose you have a bunch of SEC filings in a knowledge base and\nyou ask \u201cWhat were Apple\u2019s key financial results in the most recent fiscal\nyear?\u201d RSE will identify the most relevant segment as the entire \u201cConsolidated\nStatement of Operations\u201d section, which will be 5-10 chunks long. Whereas if\nyou ask \u201cWho is Apple\u2019s CEO?\u201d the most relevant segment will be identified as\na single chunk that mentions \u201cTim Cook, CEO.\u201d\n\n# Tutorial\n\nThe easiest way to install spRAG is to use the Python package: pip install\nsprag\n\n#### Quickstart\n\nBy default, spRAG uses OpenAI for embeddings, Claude 3 Haiku for AutoContext,\nand Cohere for reranking, so to run the code below you'll need to make sure\nyou have API keys for those providers set as environmental variables with the\nfollowing names: OPENAI_API_KEY, ANTHROPIC_API_KEY, and CO_API_KEY.\n\nYou can create a new KnowledgeBase directly from a file using the\ncreate_kb_from_file function:\n\n    \n    \n    from sprag.create_kb import create_kb_from_file file_path = \"spRAG/tests/data/levels_of_agi.pdf\" kb_id = \"levels_of_agi\" kb = create_kb_from_file(kb_id, file_path)\n\nKnowledgeBase objects persist to disk automatically, so you don't need to\nexplicitly save it at this point.\n\nNow you can load the KnowledgeBase by its kb_id (only necessary if you run\nthis from a separate script) and query it using the query method:\n\n    \n    \n    from sprag.knowledge_base import KnowledgeBase kb = KnowledgeBase(\"levels_of_agi\") search_queries = [\"What are the levels of AGI?\", \"What is the highest level of AGI?\"] results = kb.query(search_queries) for segment in results: print(segment)\n\n#### Basic customization\n\nNow let's look at an example of how we can customize the configuration of a\nKnowledgeBase. In this case, we'll customize it so that it only uses OpenAI\n(useful if you don't have API keys for Anthropic and Cohere). To do so, we\nneed to pass in a subclass of LLM and a subclass of Reranker. We'll use\ngpt-3.5-turbo for the LLM (this is what gets used for document summarization\nin AutoContext) and since OpenAI doesn't offer a reranker, we'll use the\nNoReranker class for that.\n\n    \n    \n    from sprag.llm import OpenAIChatAPI from sprag.reranker import NoReranker llm = OpenAIChatAPI(model='gpt-3.5-turbo') reranker = NoReranker() kb = KnowledgeBase(kb_id=\"levels_of_agi\", reranker=reranker, auto_context_model=llm)\n\nNow we can add documents to this KnowledgeBase using the add_document method.\nNote that the add_document method takes in raw text, not files, so we'll have\nto extract the text from our file first. There are some utility functions for\ndoing this in the document_parsing.py file.\n\n    \n    \n    from sprag.document_parsing import extract_text_from_pdf file_path = \"spRAG/tests/data/levels_of_agi.pdf\" text = extract_text_from_pdf(file_path) kb.add_document(doc_id=file_path, text=text)\n\n# Architecture\n\n## KnowledgeBase object\n\nA KnowledgeBase object takes in documents (in the form of raw text) and does\nchunking and embedding on them, along with a few other preprocessing\noperations. Then at query time you feed in queries and it returns the most\nrelevant segments of text.\n\nKnowledgeBase objects are persistent by default. The full configuration needed\nto reconstruct the object gets saved as a JSON file upon creation and\nupdating.\n\n## Components\n\nThere are five key components that define the configuration of a\nKnowledgeBase, each of which are customizable:\n\n  1. VectorDB\n  2. ChunkDB\n  3. Embedding\n  4. Reranker\n  5. LLM\n\nThere are defaults for each of these components, as well as alternative\noptions included in the repo. You can also define fully custom components by\nsubclassing the base classes and passing in an instance of that subclass to\nthe KnowledgeBase constructor.\n\n#### VectorDB\n\nThe VectorDB component stores the embedding vectors, as well as a small amount\nof metadata.\n\n#### ChunkDB\n\nThe ChunkDB stores the content of text chunks in a nested dictionary format,\nkeyed on doc_id and chunk_index. This is used by RSE to retrieve the full text\nassociated with specific chunks.\n\n#### Embedding\n\nThe Embedding component defines the embedding model.\n\n#### Reranker\n\nThe Reranker components define the reranker. This is used after the vector\ndatabase search (and before RSE) to provide a more accurate ranking of chunks.\n\n#### LLM\n\nThis defines the LLM to be used for document summarization, which is only used\nin AutoContext.\n\n## Document upload flow\n\nDocuments -> chunking -> embedding -> chunk and vector database upsert\n\n## Query flow\n\nQueries -> vector database search -> reranking -> RSE -> results\n\n# Community and support\n\nYou can join our Discord to ask questions, make suggestions, and discuss\ncontributions.\n\n## About\n\nHigh-performance RAG framework for unstructured data\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\nActivity\n\nCustom properties\n\n### Stars\n\n5 stars\n\n### Watchers\n\n1 watching\n\n### Forks\n\n0 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Contributors 2\n\n  * zmccormick7 Zach McCormick\n  * justinmclark Justin Clark\n\n## Languages\n\n  * Python 100.0%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": true}
