{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afc85e8f-7045-4239-85d3-43791acc9601",
   "metadata": {},
   "source": [
    "### Possibly interesting features based on https://news.ycombinator.com/item?id=36590226\n",
    "- Time of day [Done]\n",
    "- how many posts 1 hr before post made (indicative of whether it's peak hour or not)\n",
    "- Analysis on title: TF-IDF? Remove stopwords? Any other importance measures, and onehot encode the impt words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac021992-c9ee-470c-a5be-0fa5109aa117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import col,sum,desc,when,udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8fc9fb9-2333-4abf-8a0f-51a449beaae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.46.155.228:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[2] appName=PySparkShell>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06811479-467a-427d-8baa-f7912ddd001b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store',\n",
       " 'README.md',\n",
       " '.gitattributes',\n",
       " '.ipynb_checkpoints',\n",
       " '.git',\n",
       " 'data']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change file path\n",
    "os.chdir(\"/Users/hydraze/Library/CloudStorage/GoogleDrive-tohziyu2@gmail.com/My Drive/Studies/KU Leuven/Courses/Classes/Y1S2/Advanced Analytics in Business/Project/3/AdvancedAnalytics_Streaming-Text-Analytics\")\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b6de2820-d794-4a51-b7a1-49eff9277cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all folder names, then read into one textfile\n",
    "file_path = \"/Users/hydraze/Library/CloudStorage/GoogleDrive-tohziyu2@gmail.com/My Drive/Studies/KU Leuven/Courses/Classes/Y1S2/Advanced Analytics in Business/Project/3/AdvancedAnalytics_Streaming-Text-Analytics/\"\n",
    "folders_names = [file_path + 'data/' + folder for folder in os.listdir(file_path + 'data/') if \"saved_stories\" in folder]\n",
    "compiled_stories = sc.textFile(','.join(folders_names[0:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e9ad3cc9-187c-4567-8fb6-3452049b52f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(compiled_stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2b7f4008-0795-4e97-8594-8c43d3d29854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|          user|votes|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+\n",
      "|40273751|       0|         ervicee.com|    false|2024-05-06 12:18:16|ervicee - service...|ervicee - service...|Show HN: I made t...| https://ervicee.com| pedrofonsecaa|    1|\n",
      "|40273924|       0|thezerostate.subs...|    false|2024-05-06 12:39:41|Naruto's Guide to...|Naruto's Guide to...|Naruto's Guide to...|https://thezerost...|         bcopa|    1|\n",
      "|40273671|       0|         reuters.com|    false|2024-05-06 12:08:36|     reuters.com\\n\\n|         reuters.com|India EV Sales to...|https://www.reute...|     alephnerd|    1|\n",
      "|40273894|       0|       neocities.org|    false|2024-05-06 12:35:56|Neocities: Create...|           Neocities|           Neocities|https://neocities...|ritabratamaiti|    2|\n",
      "|40273756|       0|          reddit.com|    false|2024-05-06 12:18:48|Blocked\\n\\n# whoa...|             Blocked|What's the most s...|https://old.reddi...|   thunderbong|    1|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d4342d61-1a1c-4a5a-9cd4-98d142163092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- aid: string (nullable = true)\n",
      " |-- comments: long (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- frontpage: boolean (nullable = true)\n",
      " |-- posted_at: string (nullable = true)\n",
      " |-- source_text: string (nullable = true)\n",
      " |-- source_title: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- votes: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "16625275-2d9a-4fcd-99ac-6e729616733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert frontpage to numeric\n",
    "df = df.withColumn('frontpage', when(df.frontpage==True, 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d3c0449c-ca90-4780-a3ff-db082a9b1ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>aid</th>\n",
       "      <th>comments</th>\n",
       "      <th>domain</th>\n",
       "      <th>frontpage</th>\n",
       "      <th>posted_at</th>\n",
       "      <th>source_text</th>\n",
       "      <th>source_title</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>user</th>\n",
       "      <th>votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>300</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>4.016383627035831E7</td>\n",
       "      <td>0.7882736156351792</td>\n",
       "      <td>None</td>\n",
       "      <td>0.1498371335504886</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>3.570032573289902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>80842.22633604385</td>\n",
       "      <td>3.9029223946944507</td>\n",
       "      <td>None</td>\n",
       "      <td>0.35749442089287836</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>8.737127655708031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>40050271</td>\n",
       "      <td>0</td>\n",
       "      <td>abc.net.au</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-04-16 10:13:15</td>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>'Architecture by conference' is a really bad idea</td>\n",
       "      <td>'Architecture by conference' is a bad idea</td>\n",
       "      <td>http://leehite.org/Chimes.htm</td>\n",
       "      <td>AlbeeDang</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>40274373</td>\n",
       "      <td>45</td>\n",
       "      <td>zork.net</td>\n",
       "      <td>1</td>\n",
       "      <td>2024-05-06 13:25:39</td>\n",
       "      <td>‘To the Future’: Saudi Arabia Spends Big to Be...</td>\n",
       "      <td>‘To the Future’: Saudi Arabia Spends Big to Be...</td>\n",
       "      <td>Zig 0.12.0 Release Notes</td>\n",
       "      <td>https://zork.net/~st/jottings/sais.html</td>\n",
       "      <td>zerojames</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                  aid            comments      domain  \\\n",
       "0   count                  307                 307         307   \n",
       "1    mean  4.016383627035831E7  0.7882736156351792        None   \n",
       "2  stddev    80842.22633604385  3.9029223946944507        None   \n",
       "3     min             40050271                   0  abc.net.au   \n",
       "4     max             40274373                  45    zork.net   \n",
       "\n",
       "             frontpage            posted_at  \\\n",
       "0                  307                  307   \n",
       "1   0.1498371335504886                 None   \n",
       "2  0.35749442089287836                 None   \n",
       "3                    0  2024-04-16 10:13:15   \n",
       "4                    1  2024-05-06 13:25:39   \n",
       "\n",
       "                                         source_text  \\\n",
       "0                                                307   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "3                                               \\n\\n   \n",
       "4  ‘To the Future’: Saudi Arabia Spends Big to Be...   \n",
       "\n",
       "                                        source_title  \\\n",
       "0                                                300   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "3  'Architecture by conference' is a really bad idea   \n",
       "4  ‘To the Future’: Saudi Arabia Spends Big to Be...   \n",
       "\n",
       "                                        title  \\\n",
       "0                                         307   \n",
       "1                                        None   \n",
       "2                                        None   \n",
       "3  'Architecture by conference' is a bad idea   \n",
       "4                    Zig 0.12.0 Release Notes   \n",
       "\n",
       "                                       url       user              votes  \n",
       "0                                      307        307                307  \n",
       "1                                     None       None  3.570032573289902  \n",
       "2                                     None       None  8.737127655708031  \n",
       "3            http://leehite.org/Chimes.htm  AlbeeDang                  1  \n",
       "4  https://zork.net/~st/jottings/sais.html  zerojames                101  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing data and etc. 20% not front page\n",
    "df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "29d06e05-d9ff-411e-a971-7279d1355f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|votes|\n",
      "+-----+\n",
      "|1    |\n",
      "|1    |\n",
      "|1    |\n",
      "|2    |\n",
      "|1    |\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check out individual columns\n",
    "#df.select('aid').show()\n",
    "#df.select('comments').show()\n",
    "#df.select('domain').show()\n",
    "#df.select('frontpage').show()\n",
    "#df.select('posted_at').show()\n",
    "#df.select('source_text').show(n=1, truncate=False)\n",
    "#df.select('source_title').show(n=5, truncate=False)\n",
    "#df.select('title').show(n=5, truncate=False)\n",
    "#df.select('url').show(n=5, truncate=False)\n",
    "#df.select('title').show(n=5, truncate=False)\n",
    "df.select('votes').show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "182628b0-b55b-4bff-af76-f8f2b4515ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 162:==================================================>  (292 + 2) / 307]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|     aid|count(aid)|\n",
      "+--------+----------+\n",
      "|40273751|         1|\n",
      "|40273924|         1|\n",
      "|40273671|         1|\n",
      "|40273894|         1|\n",
      "|40273756|         1|\n",
      "|40273772|         1|\n",
      "|40273760|         1|\n",
      "|40273913|         1|\n",
      "|40273754|         1|\n",
      "|40273683|         1|\n",
      "|40273749|         1|\n",
      "|40273936|         1|\n",
      "|40273630|         1|\n",
      "|40273619|         1|\n",
      "|40273628|         1|\n",
      "|40273629|         1|\n",
      "|40273705|         1|\n",
      "|40273711|         1|\n",
      "|40273780|         1|\n",
      "|40273892|         1|\n",
      "+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Checking for duplicates: Need to find a way to groupby\n",
    "df.groupby('aid').agg({'aid':'count'}).orderBy(desc(\"count(aid)\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "41e3f7b1-3c68-4dac-b3a5-6abbc8b902a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+-------+--------+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|          user|votes|isAskHN|isShowHN|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+-------+--------+\n",
      "|40273751|       0|         ervicee.com|        0|2024-05-06 12:18:16|ervicee - service...|ervicee - service...|Show HN: I made t...| https://ervicee.com| pedrofonsecaa|    1|      N|       Y|\n",
      "|40273924|       0|thezerostate.subs...|        0|2024-05-06 12:39:41|Naruto's Guide to...|Naruto's Guide to...|Naruto's Guide to...|https://thezerost...|         bcopa|    1|      N|       N|\n",
      "|40273671|       0|         reuters.com|        0|2024-05-06 12:08:36|     reuters.com\\n\\n|         reuters.com|India EV Sales to...|https://www.reute...|     alephnerd|    1|      N|       N|\n",
      "|40273894|       0|       neocities.org|        0|2024-05-06 12:35:56|Neocities: Create...|           Neocities|           Neocities|https://neocities...|ritabratamaiti|    2|      N|       N|\n",
      "|40273756|       0|          reddit.com|        0|2024-05-06 12:18:48|Blocked\\n\\n# whoa...|             Blocked|What's the most s...|https://old.reddi...|   thunderbong|    1|      N|       N|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------+-----+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extracting type of post: Show HN\n",
    "def extract_ShowHN(title):\n",
    "    if \"Show HN\" in title: \n",
    "        return \"Y\"\n",
    "    else: \n",
    "        return \"N\"\n",
    "\n",
    "extract_ShowHN_udf = udf(extract_ShowHN)\n",
    "\n",
    "df = df.withColumn('isShowHN', extract_ShowHN_udf(df.title))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "94f2293d-a5e5-4b40-8720-fd614a6dfa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting time of day\n",
    "def extract_time_of_day(posted_at):\n",
    "    return posted_at[11:13]\n",
    "\n",
    "extract_time_of_day_udf = udf(extract_time_of_day)\n",
    "\n",
    "df = df.withColumn('time_of_day', extract_time_of_day_udf(df.posted_at))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ed15eeb2-5312-40ba-8b79-6b0b45b431b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting day of week\n",
    "from datetime import datetime\n",
    "\n",
    "weekDay =  udf(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S').strftime('%w'))\n",
    "\n",
    "df = df.withColumn('day_of_week', weekDay(df.posted_at))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "dae63ae6-c3ed-4a7f-93c3-0c71887c0d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "train,test = df.randomSplit([0.8,0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "eb2e06b0-283c-48be-be01-157080e7deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical, categorical and target columns\n",
    "TARGET_COL = ['frontpage']\n",
    "NUM_COL = ['votes', 'comments']\n",
    "CAT_COL = ['time_of_day', 'isShowHN', 'day_of_week']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b2fbd129-3db8-45c5-8039-69ab09bf1313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "# Creating numerical vector\n",
    "numerical_vector_assembler = VectorAssembler(inputCols=NUM_COL,\n",
    "                                             outputCol='num_col_vector')\n",
    "\n",
    "train = numerical_vector_assembler.transform(train)\n",
    "test = numerical_vector_assembler.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "4cde6cad-e33c-4401-bb4e-05c0ecc16223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Standard scaling\n",
    "scaler = StandardScaler(inputCol='num_col_vector',\n",
    "                        outputCol='scaled_num_col_vector',\n",
    "                        withStd=True, withMean=True)\n",
    "\n",
    "scaler = scaler.fit(train)\n",
    "\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d0960ba2-753f-4231-8b8d-e421c3cca4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- aid: string (nullable = true)\n",
      " |-- comments: long (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- frontpage: integer (nullable = false)\n",
      " |-- posted_at: string (nullable = true)\n",
      " |-- source_text: string (nullable = true)\n",
      " |-- source_title: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- votes: long (nullable = true)\n",
      " |-- isAskHN: string (nullable = true)\n",
      " |-- isShowHN: string (nullable = true)\n",
      " |-- time_of_day: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "aa0fe36c-f16b-4256-a8d0-d5289ded33a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+-------+--------+-----------+-----------+--------------+---------------------+--------------+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|         user|votes|isAskHN|isShowHN|time_of_day|day_of_week|num_col_vector|scaled_num_col_vector|isShowHN_index|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+-------+--------+-----------+-----------+--------------+---------------------+--------------+\n",
      "|40273751|       0|         ervicee.com|        0|2024-05-06 12:18:16|ervicee - service...|ervicee - service...|Show HN: I made t...| https://ervicee.com|pedrofonsecaa|    1|      N|       Y|         12|          1|     [1.0,0.0]| [-0.2851446926108...|           1.0|\n",
      "|40273924|       0|thezerostate.subs...|        0|2024-05-06 12:39:41|Naruto's Guide to...|Naruto's Guide to...|Naruto's Guide to...|https://thezerost...|        bcopa|    1|      N|       N|         12|          1|     [1.0,0.0]| [-0.2851446926108...|           0.0|\n",
      "|40273671|       0|         reuters.com|        0|2024-05-06 12:08:36|     reuters.com\\n\\n|         reuters.com|India EV Sales to...|https://www.reute...|    alephnerd|    1|      N|       N|         12|          1|     [1.0,0.0]| [-0.2851446926108...|           0.0|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+-------+--------+-----------+-----------+--------------+---------------------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol='isShowHN',\n",
    "                        outputCol='isShowHN_index')\n",
    "\n",
    "indexer = indexer.fit(train)\n",
    "train = indexer.transform(train)\n",
    "test = indexer.transform(test)\n",
    "\n",
    "train.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "72c47593-b749-4058-a9c6-606801b9a36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+-------+--------+-----------+-----------+--------------+---------------------+--------------+-------------+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|         user|votes|isAskHN|isShowHN|time_of_day|day_of_week|num_col_vector|scaled_num_col_vector|isShowHN_index| isShowHN_OHE|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+-------+--------+-----------+-----------+--------------+---------------------+--------------+-------------+\n",
      "|40273751|       0|         ervicee.com|        0|2024-05-06 12:18:16|ervicee - service...|ervicee - service...|Show HN: I made t...| https://ervicee.com|pedrofonsecaa|    1|      N|       Y|         12|          1|     [1.0,0.0]| [-0.2851446926108...|           1.0|    (1,[],[])|\n",
      "|40273924|       0|thezerostate.subs...|        0|2024-05-06 12:39:41|Naruto's Guide to...|Naruto's Guide to...|Naruto's Guide to...|https://thezerost...|        bcopa|    1|      N|       N|         12|          1|     [1.0,0.0]| [-0.2851446926108...|           0.0|(1,[0],[1.0])|\n",
      "|40273671|       0|         reuters.com|        0|2024-05-06 12:08:36|     reuters.com\\n\\n|         reuters.com|India EV Sales to...|https://www.reute...|    alephnerd|    1|      N|       N|         12|          1|     [1.0,0.0]| [-0.2851446926108...|           0.0|(1,[0],[1.0])|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+-------+--------+-----------+-----------+--------------+---------------------+--------------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(inputCol='isShowHN_index',\n",
    "                                outputCol='isShowHN_OHE')\n",
    "\n",
    "one_hot_encoder = one_hot_encoder.fit(train)\n",
    "\n",
    "train = one_hot_encoder.transform(train)\n",
    "test = one_hot_encoder.transform(test)\n",
    "\n",
    "train.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e23a4369-fedb-4721-9095-0671569f1dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile s\n",
    "assembler = VectorAssembler(inputCols=['scaled_num_col_vector',\n",
    "                                       'isShowHN_OHE'],\n",
    "                            outputCol='final_feature_vector')\n",
    "\n",
    "train = assembler.transform(train)\n",
    "test = assembler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b5e1ee3f-f488-46e0-bf23-97dd8769fa2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(final_feature_vector=DenseVector([-0.2851, -0.199, 0.0])),\n",
       " Row(final_feature_vector=DenseVector([-0.2851, -0.199, 1.0])),\n",
       " Row(final_feature_vector=DenseVector([-0.2851, -0.199, 1.0])),\n",
       " Row(final_feature_vector=DenseVector([-0.1752, -0.199, 1.0])),\n",
       " Row(final_feature_vector=DenseVector([-0.2851, -0.199, 1.0]))]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.select('final_feature_vector').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "5c63a64c-0c8e-4ece-8055-7c6ad7855945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [9.645817965512176,-0.41529264512651737,4.889823051624562]\n",
      "Intercept: -6.102534133927195\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0, elasticNetParam=0.5, family=\"binomial\",\n",
    "                        featuresCol='final_feature_vector', labelCol='frontpage')\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(train)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "abba4b43-5539-46ff-9035-c1613b03dc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|                 FPR|                TPR|\n",
      "+--------------------+-------------------+\n",
      "|                 0.0|                0.0|\n",
      "|                 0.0|0.05128205128205128|\n",
      "|                 0.0|0.07692307692307693|\n",
      "|                 0.0|0.10256410256410256|\n",
      "|                 0.0| 0.1282051282051282|\n",
      "|                 0.0|0.15384615384615385|\n",
      "|                 0.0| 0.1794871794871795|\n",
      "|                 0.0|0.20512820512820512|\n",
      "|                 0.0|0.23076923076923078|\n",
      "|                 0.0| 0.2564102564102564|\n",
      "|                 0.0|0.28205128205128205|\n",
      "|                 0.0| 0.3076923076923077|\n",
      "|                 0.0| 0.3333333333333333|\n",
      "|0.004672897196261682|  0.358974358974359|\n",
      "|0.004672897196261682|0.38461538461538464|\n",
      "|0.004672897196261682|0.41025641025641024|\n",
      "|0.004672897196261682| 0.4358974358974359|\n",
      "|0.004672897196261682|0.46153846153846156|\n",
      "|0.004672897196261682| 0.5128205128205128|\n",
      "|0.009345794392523364| 0.5384615384615384|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "areaUnderROC: 0.9851425832734244\n"
     ]
    }
   ],
   "source": [
    "# Extract the summary from the returned LogisticRegressionModel instance trained\n",
    "# in the earlier example\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "trainingSummary.roc.show()\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a9c16fa3-3dca-4293-84c0-5467e5ff5ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression_cc56f839b637"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model threshold to maximize F-Measure\n",
    "fMeasure = trainingSummary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
    "    .select('threshold').head()['threshold']\n",
    "lr.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "40dacdd1-40e5-4d48-bc4b-15b2ae6bf9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33220655140284583"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.getThreshold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "114ff5f0-8a87-4041-aee5-7b9414415fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "label does not exist. Available: aid, comments, domain, frontpage, posted_at, source_text, source_title, title, url, user, votes, isAskHN, isShowHN, time_of_day, day_of_week, num_col_vector, scaled_num_col_vector, isShowHN_index, isShowHN_OHE, final_feature_vector, CrossValidator_053f4b8aca4f_rand, rawPrediction, probability, prediction",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[201], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m crossval \u001b[38;5;241m=\u001b[39m CrossValidator(estimator\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m     13\u001b[0m                           estimatorParamMaps\u001b[38;5;241m=\u001b[39mparamGrid,\n\u001b[1;32m     14\u001b[0m                           evaluator\u001b[38;5;241m=\u001b[39mBinaryClassificationEvaluator(),\n\u001b[1;32m     15\u001b[0m                           numFolds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# use 3+ folds in practice\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Run cross-validation, and choose the best set of parameters.\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m cvModel \u001b[38;5;241m=\u001b[39m \u001b[43mcrossval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(cvModel\u001b[38;5;241m.\u001b[39mavgMetrics[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# # Make predictions on test documents. cvModel uses the best model found (lrModel).\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# prediction = cvModel.transform(test)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# for row in selected.collect():\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#     print(row)\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/spark/spark-3.5.1-bin-hadoop3/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/Downloads/spark/spark-3.5.1-bin-hadoop3/python/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    841\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubModel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcollectSubModelsParam\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/aa24/lib/python3.12/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m~/anaconda3/envs/aa24/lib/python3.12/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[0;32m~/Downloads/spark/spark-3.5.1-bin-hadoop3/python/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    841\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m~/Downloads/spark/spark-3.5.1-bin-hadoop3/python/pyspark/util.py:342\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    341\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39msetLocalProperties(properties)\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/spark/spark-3.5.1-bin-hadoop3/python/pyspark/ml/tuning.py:118\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(modelIter)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n\u001b[0;32m--> 118\u001b[0m metric \u001b[38;5;241m=\u001b[39m \u001b[43meva\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepm\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, metric, model \u001b[38;5;28;01mif\u001b[39;00m collectSubModel \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/spark/spark-3.5.1-bin-hadoop3/python/pyspark/ml/evaluation.py:111\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_evaluate(dataset)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m~/Downloads/spark/spark-3.5.1-bin-hadoop3/python/pyspark/ml/evaluation.py:148\u001b[0m, in \u001b[0;36mJavaEvaluator._evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/spark/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Downloads/spark/spark-3.5.1-bin-hadoop3/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: label does not exist. Available: aid, comments, domain, frontpage, posted_at, source_text, source_title, title, url, user, votes, isAskHN, isShowHN, time_of_day, day_of_week, num_col_vector, scaled_num_col_vector, isShowHN_index, isShowHN_OHE, final_feature_vector, CrossValidator_053f4b8aca4f_rand, rawPrediction, probability, prediction"
     ]
    }
   ],
   "source": [
    "# Gridsearch\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "# paramGrid = ParamGridBuilder() \\\n",
    "#     .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "#     .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "#     .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(train)\n",
    "print(cvModel.avgMetrics[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3624f01e-79a6-4a57-829c-813cf76c1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "179f3144-be3e-482a-ba45-5ef97ed99ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model\n",
    "lrModel.save(file_path+\"models/lrm_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747db9d2-6015-46d3-a100-da5c21ebee80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
