{"aid": "40279580", "title": "Putting Words on the Screen", "url": "https://www.eno-writer.com/014-putting-words-on-the-screen/", "domain": "eno-writer.com", "votes": 1, "user": "gcassie", "posted_at": "2024-05-06 21:12:17", "comments": 0, "source_title": "014 - putting words on the screen", "source_text": "014 - putting words on the screen | eno writer\n\n# eno writer\n\n# 014 - putting words on the screen\n\n03 May, 2024\n\nEvery day we look at words on screens. This is the principal way that\ncomputers communicate information to us. This was true with the early\ncomputers that could only display text on a terminal, and it's true today with\nChatGPT which displays the results of the most cutting edge AI models as words\non your screen.\n\nOver the decades, programmers have made it very easy to display words on the\nscreen. Most developers today do not spend much time thinking about this\nproblem. In my last ten years as a programmer I spent zero time thinking about\nhow to get words to display on a screen. I simply put the words I wanted to\ndisplay in HTML and then the web browser like Chrome or Safari made them\nappear on the screen. Now, I am writing a piece of software \"from scratch\" so\nI don't want to have Chrome put the words on the screen. I want to put them\nthere myself.\n\nAt the lowest level, all a computer can do is store zeroes and ones. These are\ncalled bits. You can put several bits together to store larger numbers. For\ninstance, with two digits, you can store four discrete values 00, 01, 10, 11.\nWe could use these values to count to three: 0, 1, 2, 3. When we put 8 bits\ntogether we can store 256 discrete values. This is called a byte and it can\nstore the numbers from 0 to 255. If we want to store numbers greater than 255\nwe can put several bytes together. The math that determines how many values\ncan be stored in a sequence of bits is the length of the sequence raised to\nthe power of two. This is why many computer specs are powers of two (like\n256gb harddrive or 512mb of ram).\n\nComputers cannot store letters. Only numbers. To get them to store letters, we\nneed to agree on a way to translate numbers into letters. For instance, we\ncould say that A is 1 and Z is 26. The computer would also need to know about\nlower case letters: we could number them 27 to 52. We would also want some\nother symbols like ! and $. We could make these numbers 53 and 54. This is\ncalled an encoding. An encoding maps each available number to exactly one\ncharacter.\n\nAn encoding only works if everyone we share data with also knows the encoding.\nIf I call 1 A and someone else calls 1 a then when we share data with one\nanother, all the capitalization will be reversed. To deal with this,\nstandardized encodings were invented that everyone in the world could agree\non.\n\nThe simplest, widely used encoding is the \"American Standard Code for\nInformation Interchange\". ASCII for short. ASCII maps each of the 256 values\nof a byte to exactly one character. For instance A is 65. Funnily enough, the\narabic numbers we are familiar with also have to be mapped to numbers. So 1 is\n49 in ASCII. To store the word hello in ASCII encoding, you store five numbers\nin a row: 104 101 108 108 111. It can be a little confusing working with\nregular numbers like this, because a single byte could be a two digit or a\nthree digit number. To solve this, programmers often represent numbers as\nhexadecimal values. hexadecimal allows us to count to 16 with a single digit\nby using 6 extra letters. So you count 7, 8, 9, A, B, C, D, E, F. We can\nrepresent hello in hexadecimal as 68 65 6C 6C 6F.\n\nSome languages use more characters in their writing than English. For example,\nin French, vowels commonly have accents on them. To address this, other\nencodings emerged. For example, the Latin-1 encoding drops some of the lesser\nused symbols in ASCII in favour of characters like \u00e1 and \u00e9. Things get really\ncrazy when you try to make an encoding for a language like Japanese or Chinese\nwhere there are thousands of characters - way more than you can ever fit in a\nsingle byte. To solve this problem, an encoding called Unicode was invented. A\nunicode character can be up to 4 bytes. This allows it to represent up to\n1,112,064 distinct characters. For instance, the emoji \"\ud83d\udca9\" is represented by\nthree bytes: 01 F4 A9.\n\nNow we know how you store a word in a computer. Getting the computer to\ndisplay the word on a screen is a whole other thing. Computer displays are\nmade up of pixels. To display a character, you need to know which pixels to\nturn on and which ones to turn off to create the visual representation of the\ncharacter. This information is called a glyph. Of course there is more than\none way to visualize a character, so we group visually coherent sets of glyphs\ninto a font. The font Times New Roman, provides glyphs for all the upper and\nlowercase letters (and many other characters) with a particular visual style.\n\nIn early days of computing, glyphs were stored as bitmaps like this:\n\nA bitmap is a list of bytes, with each byte describing what value a specific\npixel should have. In a grayscale bitmap, a value of 0 means a pixel should be\nblack and and a value of 255 means it should be white. For a color bitmap, we\nneed three bytes for each pixel to represent each of the Red, Green and Blue\nvalues.\n\nOnce computer monitor resolutions improved, it was possible to get glyphs to\nlook more like the glyphs of traditional printing presses stamped on paper. To\nstore these high resolution glyphs in bitmaps would be very storage intensive\nand every single font size would need a separate bitmap. To get around this,\nmodern fonts store glyphs as mathematical vector equations describing shapes.\nWhen it's known what font size is wanted, the vector shapes are \"rasterized\"\ninto pixel values at that particular font size and pixel density.\n\n\"Rasterizing\" glyphs is an expensive process. So, instead of rasterizing every\nletter on the screen, each letter is rasterized only once and stored at some\ncoordinates on a large bitmap. The coordinates are stored separately if you\nneed that glyph again, you can simply look up the coordinates, find that\nlocation on the bitmap, and copy the pixels. This is called a glyph atlas.\n\nTo put the letters on the screen, you need to know where to put them. In\nbitmap fonts this was easy because each character was the same width. In\nmodern fonts, the glyphs are all different widths. Also, there are special\nrules that when certain letters appear together, they should be closer. For\nexample, in many fonts short letters can be tucked in a little closer under\nthe overhang of an \"f\".\n\nIn early fonts, each character mapped to exactly one glyph. Modern fonts often\nhave something called \"ligatures\". A ligature is when two characters are\nmerged together into a single glyph. It is very common for \"f\" and \"l\" and \"f\"\nand \"i\" to be a ligatures: \"fl\" \"fi\". Here's the fi ligature in Times New\nRoman:\n\nYou might notice that picture looks a bit blurry. That's because I resized it\nfrom 500 by 500 pixels to 250 by 250 pixels. Remember what I said earlier\nabout having to rasterize glyphs at every font size you want to display?\n\nPutting all these pixels on the screen is quite a bit of work, and with 4k\nmonitors and Retina displays it gets even more intensive. Today's computers\nusually have a processing unit called a GPU which is specially designed for\ngraphics work. We hear a lot about GPUs lately because they are also very\nuseful for AI. The strength of GPUs is that they are massively parallel,\nmeaning they can take a tiny piece of code and run it many times at the exact\nsame time with different values. This is very useful for drawing thousands of\npixels to the screen at a high frame rate. If you want to have a 60fps\napplication, then every frame needs to be calculated and drawn in less than\n16.6 milliseconds.\n\nDrawing letters on a screen at 60fps is no different than drawing any other\ngraphics on the screen, so, in a state of the art modern word processor, you\ndefinitely want to use the GPU for rendering your glyphs. To do this, you need\nto rasterize all the vector glyphs of the font to a glyph atlas and then\nupload the glyph atlas to the GPU. Then, every frame, you pass the GPU a list\nof coordinates that describe where every letter on the screen should go and\nwhere to find the appropriate glyph in the glyph atlas. The GPU then does the\nwork of filling in all the necessary pixels. And voila! You have words on the\nscreen.\n\nAs you can see, it's pretty straightforward to put words on the screen.\n\nIf you enjoyed this post, subscribe below to get notifications for the next\none.\n\nWe also have an RSS feed\n\nPowered by Bear \u0295\u2022\u1d25\u2022\u0294\n\n", "frontpage": false}
