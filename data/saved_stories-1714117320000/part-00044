{"aid": "40163772", "title": "How AI is unfairly targeting and discriminating against Black people", "url": "https://blog.mozilla.org/en/mozilla/ai/artificial-intelligence-dangers-black-people-african-americans/", "domain": "blog.mozilla.org", "votes": 1, "user": "andrewstetsenko", "posted_at": "2024-04-25 22:18:48", "comments": 0, "source_title": "What Are The Real Dangers Of AI For Black People?", "source_text": "What Are The Real Dangers Of AI For Black People?\n\nWhat Are The Real Dangers Of AI For Black People? | The Mozilla Blog\n\nSkip to content\n\nMozilla\n\n  * Internet Culture\n\n    * Deep Dives\n    * Mozilla Explains\n    * Interviews\n    * Videos\n  * Privacy & Security\n  * Products\n\n    * Firefox\n    * Pocket\n    * Mozilla VPN\n  * Mozilla\n\n    * News\n    * Internet Policy\n    * Leadership\n\n      * Mitchell Baker, CEO\n      * Mark Surman, ED Foundation\n      * Eric Rescorla, Firefox CTO\n\nDownload Firefox\n\n#### Search\n\nAI\n\n# How AI is unfairly targeting and discriminating against Black people\n\nMarch 19, 2024\n\nAron Yohannes\n\nThe rise of Artificial Intelligence (AI) is here, and it\u2019s bringing a new era\nof technology that is already creating and impacting the world. It was the\nstory of 2023, and its emphasis isn\u2019t going anywhere anytime soon.\n\nWhile the creative growth of AI occurring so rapidly is a fascinating\ndevelopment for our society, it\u2019s important to remember its harms that cannot\nbe ignored, especially pertaining to racial bias and discrimination against\nAfrican-Americans.\n\nIn recent years, there has been research revealing that AI technologies have\nstruggled to identify images and speech patterns of nonwhite people. Black AI\nresearchers at tech giants creating AI technology have raised concerns about\nits harms against the Black community.\n\nThe concerns surrounding AI\u2019s racial biases and harms against Black people are\nserious and should be a big focus as 2024 gets underway. We invited University\nof Michigan professor, Harvard Faculty Associate and former Mozilla Foundation\nSenior Fellow in Trustworthy AI, Apryl Williams, to dive into this topic\nfurther. Williams studies experiences of gender and race at the intersection\nof digital spaces and algorithmic technocultures, and her most recent book,\n\u201cNot My Type: Automating Sexual Racism in Online Dating,\u201d exposes how race-\nbased discrimination is a fundamental part of the most popular and influential\ndating algorithms.\n\nTo start, as a professor, I\u2019m curious to know: How aware do you think students\nare of the dangers of the technology they\u2019re using? Beyond the simple things\nlike screen time notifications they might get, and more about AI problems,\nmisinformation, etc.?\n\nThey don\u2019t know. I show two key documentaries in my classes every semester. I\nteach a class called \u201cCritical Perspectives on the Internet.\u201d And then I have\nanother class that\u2019s called \u201cCritical AI\u201d and in both of those classes, the\nstudents are always shook. They always tell me, \u201cYou ruin everything for me, I\ncan never look at the world the same,\u201d which is great. That\u2019s my goal. I hope\nthat they don\u2019t look at the world the same when they leave my classes, of\ncourse. But I show them \u201cCoded Bias\u201d by Shalini Kantayya and when they watched\nthat just this past semester they were like, \u201cI can\u2019t believe this is legal,\nlike, how are they using facial recognition everywhere? How are they able to\ndo these things on our phones? How do they do this? How do they do that? I\ncan\u2019t believe this is legal. And why don\u2019t people talk about it?\u201d And I\u2019m\nlike, \u201cWell, people do talk about it. You guys just aren\u2019t necessarily keyed\ninto the places where people are talking about.\u201d And I think that\u2019s one of the\nfeelings of sort of like these movements that we\u2019re trying to build is that\nwe\u2019re not necessarily tapped into the kinds of places young people go to get\ninformation.\n\nWe often assume that AI machines are neutral in terms of race, but research\nhas shown that some of them are not and can have biases against Black people.\nWhen we think about where this problem stems from, is it fair to say it begins\nwith the tech industry\u2019s lack of representation of people who understand and\ncan work to address the potential harms of these technologies?\n\nI would say, yes, that is a huge part of it. But the actual starting point is\nthe norms of the tech industry. So we know that the tech industry was created\nby and large by the military, industrial, complex \u2014 like the internet was a\nmilitary device. And so because of that, a lot of the inequity or like\ninequality, social injustice of the time that the internet work was created\nwere baked into the structure of the internet. And then, of course, industries\nthat spring up from the internet, right? We know that the military was using\nthe internet for surveillance. And look now, we have in 2024, widespread\nsurveillance of Black communities, of marginalized communities, of\nundocumented communities, right? So really, it\u2019s the infrastructure of the\ninternet that was built to support white supremacy, I would say, is the\nstarting point. And because the infrastructure of the internet and of the tech\nindustry was born from white supremacy, then, yes, we have these hiring\npractices, and not just the hiring practices, but hiring practices where,\nlargely, they are just hiring the same kinds of people \u2014 Cisgender, hetero\nwhite men. Increasingly white women, but still we\u2019re not seeing the kinds of\ndiversity that we should be seeing if we\u2019re going to reach demographic parity.\nSo we have the hiring. But then also, we have just the norms of the tech\nindustry itself that are really built to service, I would say, the status quo,\nthey\u2019re not built to disrupt. They\u2019re built to continue the norm. And if\npeople don\u2019t stop and think about that, then, yeah, we\u2019re going to see the\nreplication of all this bias because U.S. society was built on bias, right?\nLike it is a stratified society inherently. And because of that, we\u2019re always\ngoing to see that stratification in the tech industry as well.\n\nIssues of bias in AI tend to impact the people who are rarely in positions to\ndevelop the technology. How do you think we can enable AI communities to\nengage in the development and governance of AI to get it where it\u2019s working\ntoward creating systems that embrace the full spectrum of inclusion?\n\nYes, we should enable it. But also the tech industry, like people in these\ncompanies, need to sort of take the onus on themselves to reach out to\ncommunities in which they are going to deploy their technology, right? So if\nyour target audience, let\u2019s say on TikTok, is Black content creators, you need\nto be reaching out to Black content creators and Black communities before you\nlaunch an algorithm that targets those people. You should be having them at\nyour headquarters. You should be doing listening sessions. You should be\nelevating Black voices. You should be listening to people, right? Listening to\nthe concerns, having support teams in place, before you launch the technology,\nright? So instead of retroactively trying to Band-aid it when you have an oops\nor like a bad PR moment, you should be looking to marginalize communities as\nexperts on what they need and how they see technology being implemented in\ntheir lives.\n\nA lot of the issues with these technologies in relation to Black people is\nthat they are not designed for Black people \u2014 and even the people they are\ndesigned for run into problems. It feels like this is a difficult spot for\neveryone involved?\n\nYeah, that\u2019s an interesting question. I feel like it\u2019s really hard for good\npeople on the inside of tech companies to actually say, \u201cHey, this thing that\nwe\u2019re building might be generating money, but it\u2019s not generating long-term\nlongevity,\u201d right? Or health for our users. And I get that \u2014 not every tech\ncompany is health oriented. They may act like they are, but they\u2019re not, like\nto a lot of them, money is their bottom line. I really think it\u2019s up to sort\nof like movement builders and tech industry shakers to say or to be able to\ncreate buy-in for programs, algorithms, ideas, that foster equity. But we have\nto be able to create buy-in for that. So that might look like, \u201cHey, maybe we\nmight lose some users on this front end when we implement this new idea, but\nwe\u2019re going to gain a whole lot more users.\u201d Folks of color, marginalized\nusers, queer users, trans users, if they feel like they can trust us, and\nthat\u2019s worth the investment, right? So it\u2019s really just valuing the whole\nperson, rather than just sort of valuing the face value of the money only or\nwhat they think it is, but looking to see the potential of what would happen\nif people felt like their technology was actually trustworthy.\n\nAI is rapidly growing. What are things we can add to it as it evolves, and\nwhat are things we should work to eliminate?\n\nI would say we need to expand our definition of safety. I think that safety\nshould fundamentally include your mental health and well-being, and if the\ncompany that you\u2019re using it for to find intimacy or to connect with friends\nis not actually keeping you safe as a person of color, as a trans person, as a\nqueer person, then you can\u2019t really have like full mental wellness if you are\nconstantly on high alert, you\u2019re constantly in this anxious position, you\u2019re\nhaving to worry that your technology is exploiting you, right? So, if we\u2019re\ngoing to have all of this buzz that I\u2019m seeing about trust and safety, that\ncan\u2019t just stop at the current discourse that we\u2019re having on trust and\nsafety. It can\u2019t just be about protecting privacy, protecting data, protecting\nwhite people\u2019s privacy. That has to include reporting mechanisms for users of\ncolor when they encounter abuse. Whether that is racism or homophobia, right?\nLike it needs to be more inclusive. I would say that the way that we think\nabout trust and safety and automated or algorithmic systems needs to be more\ninclusive. We really need to widen the definition of safety. And probably the\ndefinition of trust also.\n\nIn terms of subtracting, they\u2019re just a lot of things that we shouldn\u2019t be\ndoing, that we\u2019re currently doing. Honestly, the thing that we need to\nsubtract the most is this idea that we move fast and break things in tech\nculture. It\u2019s sort of like, we are just moving for the sake of innovation. We\nmight really need to dial back on this idea of moving for the sake of\ninnovation, and actually think about moving towards a safer humanity for\neverybody, and designing with that goal in mind. We can innovate in a safe\nway. We might have to sacrifice speed, a nd I think we need to say, it\u2019s okay\nto sacrifice speed in some cases.\n\nWhen I started to think about the dangers of AI, I immediately remembered the\nsituation with Robert Williams a few years ago, when he was wrongly accused by\npolice that used AI facial recognition. There is more to it than just the\nstrange memes and voice videos people create. What are the serious real world\nharms that you think of when it comes to Black people and AI that people are\noverlooking?\n\nI don\u2019t know that it\u2019s overlooked, but I don\u2019t think that Black people are\naware of the amount of surveillance of everyday technologies. When you go to\nthe airport, even if you\u2019re not using Clear or other facial recognition\ntechnology at the airport for expedited security, they\u2019re still using facial\nrecognition technology. When you\u2019re crossing borders, when you are even flying\ndomestically, they\u2019re still using that tech to look at your face. You look\ninto the camera, they take your picture. They compare it to your ID. Like,\nthat is facial recognition technology. I understand that that is for our\nnational safety, but that also means that they\u2019re collecting a lot of data on\nus. We don\u2019t know what happens with that data. We don\u2019t know if they keep it\nfor 24 hours or if they keep it for 24 years. Are they keeping logs of what\nyour face looks like every time you go? In 50 years, are we going to see a\nsystem that\u2019s like \u201cWe\u2019ve got these TSA files, and we\u2019re able to track your\naging from the time that you were 18 to the time that you\u2019re 50, just based on\nyour TSA data,\u201d right? Like, we really don\u2019t know what\u2019s happening with the\ndata. And that\u2019s just one example.\n\nWe have constant surveillance, especially in our cars. The smarter our cars\nget, the more they\u2019re surveilling us. We are seeing increasing use of those\nsystems and cars being used, and police cases to see if you were paying\nattention. Were you talking on your phone? Were you texting and driving?\nThings like that. There is automation in cars that\u2019s designed to identify\npeople and to stop right to avoid hitting you. And as we know, a lot of the\nsystems misidentify Black people as trash cans, and will instead hit them.\nThere are so many instances where AI is part of our life, and I don\u2019t think\npeople realize the depth of which it really does drive our lives. And I think\nthat\u2019s the thing that scares me the most for people of color is that we don\u2019t\nunderstand just how much AI is part of our everyday life. And I wish people\nwould stop and sort of think about, yes, I get easy access to this thing, but\nwhat am I trading off to get that easy access? What does that mean for me? And\nwhat does that mean for my community? We have places like Project Blue light,\nProject Green Light, where those systems are heavily surveilled in order to\n\u201cprotect communities.\u201d But are those created to protect white communities at\nthe expense of Black and brown communities? Right? That\u2019s what we have to\nthink about when we say that these technologies, especially surveillance\ntechnologies, are being used to protect people, who are they protecting? And\nwho are they protecting people from? And is that idea that they\u2019re protecting\npeople from a certain group of people realistic? Or is that grounded in some\ncultural bias that we have.\n\nLooking bigger picture this year: It\u2019s an election year and AI will certainly\nbe a large talking point for candidates. Regardless of who wins this fall, in\nwhat ways do you think the administration can ensure that policies and\nenforcement are instilled to address AI to make sure that racial and other\ninequities don\u2019t continue and evolve?\n\nThey need to enforce or encourage that tech companies have the onus of\ntransparency on them. There needs to be some kind of legislative prompting,\nthere has to be some kind of responsibility where tech companies actually\nsuffer consequences, legal consequences, economic consequences, when they\nviolate trust with the public, when they extract data without telling people.\nThere also needs to be more two-way conversations. Often tech companies will\njust tell you, \u201cThese are the terms of service, you have to agree with them,\u201d\nand if you don\u2019t, you opt-out, that means you can\u2019t use the tech. There needs\nto be some kind of system where tech companies can say, \u201cOkay, we\u2019re thinking\nabout rolling this out or updating our terms of service in this way, how does\nthe community feel about that?\u201d And a way that really they can be accountable\nto their users. I think we really just need some legislation that makes tech\ncompanies sort of put their feet to the fire in terms of them actually having\nresponsibility to their users.\n\nWhen it comes to fighting against racial biases and struggles, sometimes the\nmost important people that can help create change and bring awareness are\nthose not directly impacted by what\u2019s going on \u2014 for example, a white person\nbeing an ally and protesting for a Black person. What do you think most normal\npeople can do to influence change and bring awareness to AI challenges for\nBlack people?\n\nI would say, for those people who are in the know about what tech companies\nare doing, talk about that with your kids, right? When you\u2019re sitting down and\nyour kids are telling you about something that their friend posted, that\u2019s a\nperfect time to be like, \u201cLet\u2019s talk about that technology that your friend is\nusing or that you\u2019re using.\u201d Did you know that on TikTok, this happens? Did\nyou know that on TikTok, often Black creator voices are hidden, or Black\ncontent creators are shadow-banned? Did you know what happens on Instagram?\nThese kinds of regular conversations, that way, these kinds of tech injustices\nare part of the everyday vernacular for kids as they\u2019re coming up so that they\ncan be more aware, and also so that they can advocate for themselves and for\ntheir communities.\n\n### Get Firefox\n\nGet the browser that protects what\u2019s important\n\nPrevious Post\n\nDr. J. Nathan Matias on leading technology research for better digital rights\n\nNext Post\n\n6 takeaways from The Washington Post Futurist Tech Summit in D.C.\n\n## Related Articles\n\nAI\n\n## Unboxing AI with the next generation\n\nRead More\n\nAI\n\n## Open Source in the Age of LLMs\n\nRead More\n\nInternet Culture\n\n## Marek Tuszynski reflects on curating thought-provoking experiences at the\nintersection of technology and activism\n\nRead More\n\n", "frontpage": false}
