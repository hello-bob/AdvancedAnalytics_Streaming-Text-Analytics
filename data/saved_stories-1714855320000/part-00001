{"aid": "40257783", "title": "Unified model links recency and central tendency biases in working memory", "url": "https://elifesciences.org/articles/86725", "domain": "elifesciences.org", "votes": 1, "user": "PaulHoule", "posted_at": "2024-05-04 14:08:37", "comments": 0, "source_title": "Unifying network model links recency and central tendency biases in working memory", "source_text": "Unifying network model links recency and central tendency biases in working memory | eLife\n\nLoading [MathJax]/jax/output/HTML-CSS/jax.js\n\n  * Consent\n  * Details\n  * [#IABV2SETTINGS#]\n  * About\n\n## This website uses cookies\n\nWe use cookies to deliver our services and improve our website. By using this\nsite, you agree to its use of cookies. You can opt-out or change the types of\ncookies by using \u201cManage cookies\u201d below. For more information, please see our\nprivacy notice.\n\nManage cookies\n\n  * Necessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies.\n\n    * Cookiebot\n\n1\n\nLearn more about this provider\n\nCookieConsentStores the user's cookie consent state for the current domain\n\nExpiry: 1 yearType: HTTP\n\n  * Preference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in.\n\n    * Google\n\n1\n\nLearn more about this provider\n\ntest_cookieUsed to check if the user's browser supports cookies.\n\nExpiry: 1 dayType: HTTP\n\n    * YouTube\n\n9\n\nLearn more about this provider\n\nVISITOR_INFO1_LIVETries to estimate the users' bandwidth on pages with\nintegrated YouTube videos.\n\nExpiry: 180 daysType: HTTP\n\nytidb::LAST_RESULT_ENTRY_KEYStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: PersistentType: HTML\n\nyt-remote-cast-availableStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-cast-installedStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-connected-devicesStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: PersistentType: HTML\n\nyt-remote-device-idStores the user's video player preferences using embedded\nYouTube video\n\nExpiry: PersistentType: HTML\n\nyt-remote-fast-check-periodStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-session-appStores the user's video player preferences using embedded\nYouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-session-nameStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\n  * Statistic cookies help website owners to understand how visitors interact with websites by collecting and reporting information anonymously.\n\n    * Google\n\n6\n\nLearn more about this provider\n\n_gaRegisters a unique ID that is used to generate statistical data on how the\nvisitor uses the website.\n\nExpiry: 2 yearsType: HTTP\n\n_ga_#Used by Google Analytics to collect data on the number of times a user\nhas visited the website as well as dates for the first and most recent visit.\n\nExpiry: 2 yearsType: HTTP\n\n_gatUsed by Google Analytics to throttle request rate\n\nExpiry: 1 dayType: HTTP\n\n_gidRegisters a unique ID that is used to generate statistical data on how the\nvisitor uses the website.\n\nExpiry: 1 dayType: HTTP\n\ncollectUsed to send data to Google Analytics about the visitor's device and\nbehavior. Tracks the visitor across devices and marketing channels.\n\nExpiry: SessionType: Pixel\n\ntdRegisters statistical data on users' behaviour on the website. Used for\ninternal analytics by the website operator.\n\nExpiry: SessionType: Pixel\n\n    * Heap Analytics\n\n4\n\nLearn more about this provider\n\n_hp2_#Collects data on the user\u2019s navigation and behavior on the website. This\nis used to compile statistical reports and heatmaps for the website owner.\n\nExpiry: 1 dayType: HTTP\n\n_hp2_id.#Collects data on the user\u2019s navigation and behavior on the website.\nThis is used to compile statistical reports and heatmaps for the website\nowner.\n\nExpiry: 1 yearType: HTTP\n\n_hp2_props.#Heap Analytics. Event properties cookie (stores properties set by\nthe addEventProperties API)\n\nExpiry: 1 yearType: HTTP\n\n_hp2_ses_props.#Collects data on the user\u2019s navigation and behavior on the\nwebsite. This is used to compile statistical reports and heatmaps for the\nwebsite owner.\n\nExpiry: 1 dayType: HTTP\n\n    * Hotjar\n\n4\n\nLearn more about this provider\n\n_hjSession_#Collects statistics on the visitor's visits to the website, such\nas the number of visits, average time spent on the website and what pages have\nbeen read.\n\nExpiry: 1 dayType: HTTP\n\n_hjSessionUser_#Collects statistics on the visitor's visits to the website,\nsuch as the number of visits, average time spent on the website and what pages\nhave been read.\n\nExpiry: 1 yearType: HTTP\n\nhjActiveViewportIdsThis cookie contains an ID string on the current session.\nThis contains non-personal information on what subpages the visitor enters \u2013\nthis information is used to optimize the visitor's experience.\n\nExpiry: PersistentType: HTML\n\nhjViewportIdSaves the user's screen size in order to adjust the size of images\non the website.\n\nExpiry: SessionType: HTML\n\n    * YouTube\n\n1\n\nLearn more about this provider\n\nYSCRegisters a unique ID to keep statistics of what videos from YouTube the\nuser has seen.\n\nExpiry: SessionType: HTTP\n\n  * Marketing cookies are used to track visitors across websites. We only use marketing cookies to identify when a visitor has come to our site in response to an advert we have placed elsewhere, which enables us to understand the effectiveness of our advertising.\n\n    * Google\n\n7\n\nLearn more about this provider\n\nCOMPASSPending\n\nExpiry: 1 dayType: HTTP\n\nGFE_RTTUsed to implement the content through Google Docs.\n\nExpiry: 1 dayType: HTTP\n\nIDEUsed by Google DoubleClick to register and report the website user's\nactions after viewing or clicking one of the advertiser's ads with the purpose\nof measuring the efficacy of an ad and to present targeted ads to the user.\n\nExpiry: 1 yearType: HTTP\n\n_gcl_auUsed by Google AdSense for experimenting with advertisement efficiency\nacross websites using their services.\n\nExpiry: 3 monthsType: HTTP\n\nads/ga-audiencesUsed by Google AdWords to re-engage visitors that are likely\nto convert to customers based on the visitor's online behaviour across\nwebsites.\n\nExpiry: SessionType: Pixel\n\nNIDRegisters a unique ID that identifies a returning user's device. The ID is\nused for targeted ads.\n\nExpiry: 6 monthsType: HTTP\n\npagead/1p-conversion/#/Pending\n\nExpiry: SessionType: Pixel\n\n    * Heap Analytics\n\n1\n\nLearn more about this provider\n\napi/telemetryCollects data on user behaviour and interaction in order to\noptimize the website and make advertisement on the website more relevant.\n\nExpiry: SessionType: Pixel\n\n    * YouTube\n\n11\n\nLearn more about this provider\n\n#-#Pending\n\nExpiry: SessionType: HTML\n\niU5q-!O9@$Registers a unique ID to keep statistics of what videos from YouTube\nthe user has seen.\n\nExpiry: SessionType: HTML\n\nLAST_RESULT_ENTRY_KEYUsed to track user\u2019s interaction with embedded content.\n\nExpiry: SessionType: HTTP\n\nLogsDatabaseV2:V#||LogsRequestsStorePending\n\nExpiry: PersistentType: IDB\n\nnextIdUsed to track user\u2019s interaction with embedded content.\n\nExpiry: SessionType: HTTP\n\nremote_sidNecessary for the implementation and functionality of YouTube video-\ncontent on the website.\n\nExpiry: SessionType: HTTP\n\nrequestsUsed to track user\u2019s interaction with embedded content.\n\nExpiry: SessionType: HTTP\n\nServiceWorkerLogsDatabase#SWHealthLogNecessary for the implementation and\nfunctionality of YouTube video-content on the website.\n\nExpiry: PersistentType: IDB\n\nTESTCOOKIESENABLEDUsed to track user\u2019s interaction with embedded content.\n\nExpiry: 1 dayType: HTTP\n\nVISITOR_PRIVACY_METADATAStores the user's cookie consent state for the current\ndomain\n\nExpiry: 180 daysType: HTTP\n\nYtIdbMeta#databasesUsed to track user\u2019s interaction with embedded content.\n\nExpiry: PersistentType: IDB\n\n  * Unclassified cookies are cookies that we are in the process of classifying, together with the providers of individual cookies.\n\nWe do not use cookies of this type.\n\nCross-domain consent[#BULK_CONSENT_DOMAINS_COUNT#] [#BULK_CONSENT_TITLE#]\n\nList of domains your consent applies to: [#BULK_CONSENT_DOMAINS#]\n\nCookie declaration last updated on 4/25/24 by Cookiebot\n\n## [#IABV2_TITLE#]\n\n[#IABV2_BODY_INTRO#]\n\n[#IABV2_BODY_LEGITIMATE_INTEREST_INTRO#]\n\n[#IABV2_BODY_PREFERENCE_INTRO#]\n\n[#IABV2_BODY_PURPOSES_INTRO#]\n\n[#IABV2_BODY_PURPOSES#]\n\n[#IABV2_BODY_FEATURES_INTRO#]\n\n[#IABV2_BODY_FEATURES#]\n\n[#IABV2_BODY_PARTNERS_INTRO#]\n\n[#IABV2_BODY_PARTNERS#]\n\nVisit our privacy policy page to learn more on how we use your data and how we\nkeep it confidential.\n\nFor all queries relating to personal data and privacy, please contact us at\ndata@elifesciences.org.\n\nPowered by Cookiebot by Usercentrics\n\nSkip to Content\n\neLife home page\n\nSkip to Content\n\neLife home page\n\n  * Research Article\n\n  1. Neuroscience\n\n# Unifying network model links recency and central tendency biases in working\nmemory\n\n  1. Vezha Boboeva\n  2. Alberto Pezzotta\n  3. Claudia Clopath\n  4. Athena Akrami\n\n  1. Sainsbury Wellcome Centre, University College London, United Kingdom;\n  2. Department of Bioengineering, Imperial College London, United Kingdom;\n  3. Gatsby Computational Neuroscience Unit, University College London, United Kingdom;\n  4. The Francis Crick Institute, United Kingdom;\n\nApr 24, 2024\n\nhttps://doi.org/10.7554/eLife.86725.3\n\n  * Open access\n  * Copyright information\n\n##### Version of Record\n\nThe authors declare this version of their article to be the Version of Record.\n\nAbout eLife's process\n\n  1. Download\n\nA two-part list of links to download the article, or parts of the article, in\nvarious formats.\n\n### Downloads (link to download the article as PDF)\n\n     * Article PDF\n     * Figures PDF\n\n### Open citations (links to open the citations from this article in various\nonline reference manager services)\n\n     * Mendeley\n\n### Cite this article (links to download the citations from this article in\nformats compatible with various reference manager tools)\n\n    1. Vezha Boboeva\n    2. Alberto Pezzotta\n    3. Claudia Clopath\n    4. Athena Akrami\n\n(2024)\n\nUnifying network model links recency and central tendency biases in working\nmemory\n\neLife 12:RP86725.\n\nhttps://doi.org/10.7554/eLife.86725.3\n\n     * Download BibTeX\n     * Download .RIS\n\n  2. Cite\n  3. Share\n  4. Comment Open annotations (there are currently 0 annotations on this page).\n\n  * 1,248 views\n  * 98 downloads\n  * 1 citations\n\nVersion of Record published\n\n    April 24, 2024 (This version)\nReviewed preprint version 2\n\n    January 5, 2024 (Go to version)\nReviewed preprint version 1\n\n    August 2, 2023 (Go to version)\nSent for peer review\n\n    March 9, 2023\nPreprint posted\n\n    January 27, 2023 (Go to version)\n\n  1.     1. Of interest\n\n#### Cholecystokinin facilitates motor skill learning by modulating\nneuroplasticity in the motor cortex\n\nHao Li, Jingyu Feng ... Jufang He\n\nResearch Article May 3, 2024\n\n  2. Further reading\n\n###### Share this article\n\n###### Cite this article\n\n  1. Vezha Boboeva\n  2. Alberto Pezzotta\n  3. Claudia Clopath\n  4. Athena Akrami\n\n(2024)\n\nUnifying network model links recency and central tendency biases in working\nmemory\n\neLife 12:RP86725.\n\nhttps://doi.org/10.7554/eLife.86725.3\n\n  1. Download BibTeX\n  2. Download .RIS\n\n  * Abstract\n  * eLife assessment\n  * eLife digest\n  * Introduction\n  * Results\n  * Discussion\n  * Methods\n  * Appendix 1\n  * Data availability\n  * References\n  * Article and author information\n  * Metrics\n\n## Abstract\n\nThe central tendency bias, or contraction bias, is a phenomenon where the\njudgment of the magnitude of items held in working memory appears to be biased\ntoward the average of past observations. It is assumed to be an optimal\nstrategy by the brain and commonly thought of as an expression of the brain\u2019s\nability to learn the statistical structure of sensory input. On the other\nhand, recency biases such as serial dependence are also commonly observed and\nare thought to reflect the content of working memory. Recent results from an\nauditory delayed comparison task in rats suggest that both biases may be more\nrelated than previously thought: when the posterior parietal cortex (PPC) was\nsilenced, both short-term and contraction biases were reduced. By proposing a\nmodel of the circuit that may be involved in generating the behavior, we show\nthat a volatile working memory content susceptible to shifting to the past\nsensory experience \u2013 producing short-term sensory history biases \u2013 naturally\nleads to contraction bias. The errors, occurring at the level of individual\ntrials, are sampled from the full distribution of the stimuli and are not due\nto a gradual shift of the memory toward the sensory distribution\u2019s mean. Our\nresults are consistent with a broad set of behavioral findings and provide\npredictions of performance across different stimulus distributions and\ntimings, delay intervals, as well as neuronal dynamics in putative working\nmemory areas. Finally, we validate our model by performing a set of human\npsychophysics experiments of an auditory parametric working memory task.\n\n## eLife assessment\n\nThis important study combines disparate results from both psychophysics and\nneural silencing experiments to suggest a new interpretation of how animals\nand humans represent and interpret recent events in our memory. A key aspect\nof the model put forward here is the presence of discrete jumps in neural\nactivity within the posterior parietal region of the cortex. The model is\ndistinct from other models, and the authors provide convincing evidence to\nsupport it both from existing results as well as from novel experiments.\n\nhttps://doi.org/10.7554/eLife.86725.3.sa0\n\n  * About eLife assessments\n\n## eLife digest\n\nDuring cognitive tasks, our brain needs to temporarily hold and manipulate the\ninformation it is processing to decide how best to respond. This ability,\nknown as working memory, is influenced by how the brain represents and\nprocesses the sensory world around us, which can lead to biases, such as\n\u2018central tendency\u2019.\n\nConsider an experiment where you are presented with a metal bar and asked to\nrecall how long it was after a few seconds. Typically, our memories, averaged\nover many trials of repeating this memory recall test, appear to skew towards\nan average length, leading to the tendency to mis-remember the bar as being\nshorter or longer than it actually was. This central tendency occurs in most\nspecies, and is thought to be the result of the brain learning which sensory\ninput is the most likely to occur out of the range of possibilities.\n\nWorking memory is also influenced by short-term history or recency bias, where\na recent past experience influences a current memory. Studies have shown that\n\u2018turning off\u2019 a region of the rat brain called the posterior parietal cortex\nremoves the effects of both recency bias and central tendency on working\nmemory. Here, Boboeva et al. reveal that these two biases, which were thought\nto be controlled by separate mechanisms, may in fact be related.\n\nBuilding on the inactivation study, the team modelled a circuit of neurons\nthat can give rise to the results observed in the rat experiments, as well as\nbehavioural results in humans and primates. The computational model contained\ntwo modules: one of which represented a putative working memory, and another\nwhich represented the posterior parietal cortex which relays sensory\ninformation about past experiences.\n\nBoboeva et al. found that sensory inputs relayed from the posterior parietal\ncortex module led to recency biases in working memory. As a result, central\ntendency naturally emerged without needing to add assumptions to the model\nabout which sensory input is the most likely to occur. The computational model\nwas also able to replicate all known previous experimental findings, and made\nsome predictions that were tested and confirmed by psychophysics tests on\nhuman participants.\n\nThe findings of Boboeva et al. provide a new potential mechanism for how\ncentral tendency emerges in working memory. The model suggests that to achieve\ncentral tendency prior knowledge of how a sensory stimulus is distributed in\nan environment is not required, as it naturally emerges due to a volatile\nworking memory which is susceptible to errors. This is the first mechanistic\nmodel to unify these two sources of bias in working memory. In the future,\nthis could help advance our understanding of certain psychiatric conditions in\nwhich working memory and sensory learning are impaired.\n\n## Introduction\n\nA fundamental question in neuroscience relates to how brains efficiently\nprocess the statistical regularities of the environment to guide behavior.\nExploiting such regularities may be of great value to survival in the natural\nenvironment, but may lead to biases in laboratory tasks. Repeatedly observed\nacross species and sensory modalities is the central tendency (\u2018contraction\u2019)\nbias, where performance in perceptual tasks seemingly reflects a shift of the\nworking memory (WM) representation toward the mean of the sensory history\n(Hollingworth, 1910; Jou et al., 2004; Berliner et al., 1977; Hellstr\u00f6m, 1985;\nRaviv et al., 2012; Fischer and Whitney, 2014). Equally common are sequential\nbiases, either attractive or repulsive, toward the immediate sensory history\n(Akrami et al., 2018; Raviv et al., 2012; Kiyonaga et al., 2017; Cicchini et\nal., 2017; Czoschke et al., 2019; Alais et al., 2018; Manassi et al., 2018;\nManassi et al., 2017; Su\u00e1rez-Pinilla et al., 2018; Fischer and Whitney, 2014;\nPapadimitriou et al., 2015).\n\nIt is commonly thought that these biases occur due to disparate mechanisms \u2013\ncontraction bias is widely thought to be a result of learning the statistical\nstructure of the environment, whereas serial biases are thought to reflect the\ncontents of WM (Lieder et al., 2019; Barbosa and Compte, 2020). Recent\nevidence, however, challenges this picture: our recent study of a parametric\nworking memory (PWM) task discovered that the rat posterior parietal cortex\n(PPC) plays a key role in modulating contraction bias (Akrami et al., 2018).\nWhen the region is optogenetically inactivated, contraction bias is\nattenuated. Intriguingly, however, this is also accompanied by the suppression\nof bias effects induced by the recent history of the stimuli, suggesting that\nthe two phenomena may be interrelated. Interestingly, other behavioral\ncomponents, including WM of immediate sensory stimuli (in the current trial),\nremain intact. In another recent study with humans, a double dissociation was\nreported between three cohorts of subjects: subjects on the autistic spectrum\n(ASD) expressed reduced biases due to recent statistics, whereas dyslexic\nsubjects (DYS) expressed reduced biases toward long-term statistics, relative\nto neurotypical subjects (NT) (Lieder et al., 2019). Finally, further\ncomplicating the picture is the observation of not only attractive serial\ndependency, but also repulsive biases (Fritsche and Spaak, 2020). It is as of\nyet unclear how such biases occur and what mechanisms underlie such history\ndependencies.\n\nThese findings stimulate the question of whether contraction bias and the\ndifferent types of serial biases are actually related, and if so, how.\nAlthough normative models have been proposed to explain these effects\n(Ashourian and Loewenstein, 2011; Fritsche and Spaak, 2020; Lieder et al.,\n2019), the neural mechanisms and circuits underlying them remain poorly\nunderstood. We address this question through a model of the putative circuit\ninvolved in giving rise to the behavior observed in Akrami et al., 2018. Our\nmodel consists of two continuous (bump) attractor sub-networks, representing a\nWM module and the PPC. Given the finding that PPC neurons carry more\ninformation about stimuli presented during previous trials, the PPC module\nintegrates inputs over a longer timescale relative to the WM network and\nincorporates firing rate adaptation.\n\nWe find that both contraction bias and short-term sensory history effects\nemerge in the WM network as a result of inputs from the PPC network.\nImportantly, we see that these effects do not necessarily occur due to\nseparate mechanisms. Rather, in our model, contraction bias emerges as a\nstatistical effect of errors in WM, occurring due to the persisting memory of\nstimuli shown in the preceding trials. The integration of this persisting\nmemory in the WM module competes with that of the stimulus in the current\ntrial, giving rise to short-term history effects. We conclude that contraction\nbiases in such paradigms may not necessarily reflect explicit learning of\nregularities or an \u2018attraction toward the mean\u2019 on individual trials. Rather,\nit may be an effect emerging at the level of average performance, when in each\ntrial errors are made according to the recent sensory experiences whose\ndistribution follow that of the input stimuli. Furthermore, using the same\nmodel, we also show that the biases toward long-term (short-term) statistics\ninferred from the performance of ASD (DYS) subjects (Lieder et al., 2019) may\nactually reflect short-term biases extending more or less into the past with\nrespect to NT subjects, challenging the hypothesis of a double-dissociation\nmechanism. Last, we show that as a result of neuronal integration of inputs\nand adaptation, in addition to attraction effects occurring on a short\ntimescale, repulsion effects are observed on a longer timescale (Fritsche and\nSpaak, 2020).\n\nWe make specific predictions on neuronal dynamics in the PPC and downstream WM\nareas, as well as how contraction bias may be altered, upon manipulations of\nthe sensory stimulus distribution, intertrial and interstimulus delay\nintervals. We show agreements between the model and our previous results in\nhumans and rats. Finally, we test our model predictions by performing new\nhuman auditory PWM tasks. The data is in agreement with our model and not with\nan alternative Bayesian model.\n\n## Results\n\n### The PPC as a slower integrator network\n\nPWM tasks involve the sequential comparison of two graded stimuli that differ\nalong a physical dimension and are separated by a delay interval of a few\nseconds (Figure 1A and B; Romo and Salinas, 2003; Akrami et al., 2018;\nAshourian and Loewenstein, 2011). A key feature emerging from these studies is\ncontraction bias, where the averaged performance is as if the memory of the\nfirst stimulus progressively shifts toward the center of a prior distribution\nbuilt from past sensory history (Figure 1C). Additionally, biases toward the\nmost recent sensory stimuli (immediately preceding trials) have also been\ndocumented (Akrami et al., 2018; Raviv et al., 2012).\n\nFigure 1\n\nDownload asset Open asset\n\n###### The posterior parietal cortex (PPC) as a slower integrator network.\n\n(A) In any given trial, a pair of stimuli (here, sounds) separated by a\nvariable delay interval is presented to a subject. After the second stimulus,\nand after a go cue, the subject must decide ...\n\nIn order to investigate the circuit mechanisms by which such biases may occur,\nwe use two identical one-dimensional continuous attractor networks to model WM\nand PPC modules. Neurons are arranged according to their preferential firing\nlocations in a continuous stimulus space, representing the amplitude of\nauditory stimuli. Excitatory recurrent connections between neurons are\nsymmetric and a monotonically decreasing function of the distance between the\npreferential firing fields of neurons, allowing neurons to mutually excite one\nanother; inhibition, instead, is uniform. Together, both allow a localized\nbump of activity to form and be sustained (Figure 1D and E; Sebastian Seung,\n1998; Wang, 2001; Zhong et al., 2020; Spalla et al., 2021; Wu and Amari, 2005;\nWu et al., 2016; Fung et al., 2008; Fung et al., 2010; Trappenberg, 2005).\nBoth networks have free boundary conditions. Neurons in the WM network receive\ninputs from neurons in the PPC coding for the same stimulus amplitude (Figure\n1D). Building on experimental findings (Murray et al., 2014; Siegle et al.,\n2021; Gao et al., 2020; Wang et al., 2023; Mej\u00edas and Wang, 2022; Ding et al.,\n2022), we designed the PPC network such that it integrates activity over a\nlonger timescale compared to the WM network section \u2018The model\u2019. Moreover,\nneurons in the PPC are equipped with neural adaptation that can be thought of\nas a threshold that dynamically follows the activation of a neuron over a\nlonger timescale.\n\nTo simulate the PWM task, at the beginning of each trial, the network is\nprovided with a stimulus for a short time via an external current as input to\na set of neurons (see Appendix 1\u2014table 1). Following , after a delay interval,\na second stimulus is presented (Figure 1E). The pair is drawn from the\nstimulus set shown in Figure 1B, where they are all equally distant from the\ndiagonal , and are therefore of equal nominal discrimination, or difficulty.\nThe stimuli are co-varied in each trial so that the task cannot be solved by\nrelying on only one of the stimuli (Hern\u00e1ndez et al., 1997). As in the study\nin Akrami et al., 2018 using an interleaved design, across consecutive trials,\nthe interstimulus delay intervals are randomized and sampled uniformly between\n2, 6, and 10 s. The intertrial interval (ITI), instead, is fixed at 5 s.\n\nWe additionally include psychometric pairs (indicated in the box in Figure 1B)\nwhere the distance to the diagonal, hence the discrimination difficulty, is\nvaried. The task is a binary comparison task that aims at classifying whether\nor vice versa. In order to solve the task, we record the activity of the WM\nnetwork at two time points: slightly before and after the onset of (Figure\n1E). We repeat this procedure across many different trials and use the\nrecorded activity to assess performance (see section \u2018Simulation\u2019) for\ndetails. Importantly, at the end of each trial, the activity of both networks\nis not re-initialized, and the state of the network at the end of the trial\nserves as the initial network configuration for the next trial.\n\n### Contraction bias and short-term stimulus history effects as a result of\nPPC network activity\n\nProbing the WM network performance of psychometric stimuli (Figure 1B, purple\nbox, 10% of all trials) shows that the comparison behavior is not error-free\nand that the psychometric curves (different colors) differ from the optimal\nstep function (Figure 2A, green dashed line). The performance of pyschometric\ntrials is also better for shorter interstimulus delay intervals, as has been\nshown in previous work (Sinclair and Burton, 1996; Akrami et al., 2018). In\nour model, errors are caused by the displacement of the activity bump in the\nWM network due to the inputs from the PPC network. These displacements in the\nWM activity bump can result in different outcomes: by displacing it away from\nthe second stimulus, they either do not affect the performance or improve it\n(Figure 2B, right panel, \u2018Bias+\u2019), if noise is present. Conversely, the\nperformance can suffer, if the displacement of the activity bump is toward the\nsecond stimulus (Figure 2B, left panel, \u2018Bias-\u2019). Note, however, that in these\ntwo specific trials the activity bump in PPC is strong and it displaces the\nactivity bump in the WM network, but this is not the only kind of dynamics\npresent in the network (see section \u2018Multiple timescales at the core of short-\nterm sensory history effects\u2019 for a more detailed analysis of the network\ndynamics).\n\nFigure 2 with 2 supplements\n\nDownload asset Open asset\n\n###### Contraction bias and short-term sensory history effects as a result of\nposterior parietal cortex (PPC) network activity.\n\n(A) Performance of network model for psychometric stimuli (shades of green) is\nnot error-free (black dashed line). A shorter interstimulus delay interval\nyields a better performance. (B) Errors ...\n\nFigure 2\u2014figure supplement 2\n\nDownload asset Open asset\n\n###### Model predictions for a block design.\n\n(A) As in Figure 2A. Performance of the network model for the psychometric\nstimuli improves with a short delay interval (light green) and worsens as this\ndelay is increased (dark green). (B) As in Fi...\n\nFigure 2\u2014figure supplement 1\n\nDownload asset Open asset\n\n###### Inactivating the inputs from the posterior parietal cortex (PPC)\nnetwork improves performance, in line with experimental findings.\n\n(A) As in Figure 2C. The performance of the network when the strength of the\ninputs from the PPC to the working memory (WM) network is weakened (modelling\nthe optogenetic inactivation of the PPC) is ...\n\nPerformance of stimulus pairs that are equally distant from the diagonal can\nbe similarly impacted and the network produces a pattern of errors that are\nconsistent with contraction bias: performance is at its minimum for stimulus\npairs in which is either largest or smallest, and at its maximum for stimulus\npairs in which is largest or smallest (Figure 2C, left panel; Ashourian and\nLoewenstein, 2011; Fassihi et al., 2014; Akrami et al., 2018; Fassihi et al.,\n2017; Esmaeili and Diamond, 2019). These results are consistent with the\nperformance of humans and rats on the auditory task, as previously reported\n(Figure 2C, middle and right panels, data from Akrami et al., 2018).\n\nCan the same circuit also give rise to short-term sensory history biases\n(Akrami et al., 2018; Loewenstein et al., 2021)? We analyzed the fraction of\ntrials the network response was \u2018\u2019 in the current trial conditioned on\nstimulus pairs presented in the previous trial and found that the network\nbehavior is indeed modulated by the preceding trial\u2019s stimulus pairs (Figure\n2D, panel 1). We quantified these history effects as well as how many trials\nback they extend to. We computed the bias by plotting, for each particular\npair (of stimuli) presented at the current trial, the fraction of trials the\nnetwork response was \u2018\u2019 as a function of the pair presented in the previous\ntrial minus the mean performance over all previous trial pairs (Figure 2D,\npanel 2; Akrami et al., 2018). Independent of the current trial, the previous\ntrial exerts an \u2018attractive\u2019 effect, expressed by the negative slope of the\nline: when the previous pair of stimuli is small, in the current trial is, on\naverage, misclassified as smaller than it actually is, giving rise to the\nattractive bias in the comparison performance; the converse holds true when\nthe previous pair of stimuli happens to be large. These effects extend to two\ntrials back and are consistent with the performance of humans and rats on the\nauditory task (Figure 2D, panels 3\u20136, data from Akrami et al., 2018).\n\nIt has been shown that inactivating the PPC in rats performing the auditory\ndelayed comparison task markedly reduces the magnitude of contraction bias\nwithout impacting non-sensory biases (Akrami et al., 2018). We assay the\ncausal role of the PPC in generating the sensory history effects as well as\ncontraction bias by weakening the connections from the PPC to the WM network,\nmimicking the inactivation of the PPC. In this case, we see that the\nperformance of the psychometric stimuli is greatly improved (yellow curve,\nFigure 2E, top panel), consistent also with the inactivation of the PPC in\nrodents (yellow curve, Figure 2E, bottom panel, data from Akrami et al.,\n2018). Performance is improved also for all pairs of stimuli in the stimulus\nset (Figure 2\u2014figure supplement 1A). The breakdown of the network response in\nthe current trial conditioned on the specific stimulus pair preceding it\nreveals that the previous trial no longer exerts a notable modulating effect\non the current trial (Figure 2\u2014figure supplement 1B). Quantifying this bias by\nsubtracting the mean performance over all of the previous pairs reveals that\nthe attractive bias is virtually eliminated (yellow curve, Figure 2F, left\npanel), consistent with findings in rats (Figure 2F, right panel, data from\nAkrami et al., 2018).\n\nTogether, our results suggest a possible circuit through which both\ncontraction bias and short-term history effects in a PWM task may arise. The\nmain features of our model are two continuous attractor networks, both\nintegrating the same external inputs, but operating over different timescales.\nCrucially, the slower one, a model of the PPC, includes neuronal adaptation\nand provides input to the faster one, intended as a WM circuit. Note that a\nblock design where the delay interval is kept fixed yields similar results\n(Figure 2\u2014figure supplement 2). In the next section, we show how the slow\nintegration and firing rate adaptation in the PPC network give rise to the\nobserved effects of sensory history.\n\n### Multiple timescales at the core of short-term sensory history effects\n\nThe activity bumps in the PPC and WM networks undergo different dynamics due\nto the different timescales with which they integrate inputs, the presence of\nadaptation in the PPC, and the presence of global inhibition. The WM network\nintegrates inputs over a shorter timescale, and therefore the activity bump\nfollows the external input with high fidelity (Figure 3A [purple bumps] and\nFigure 3B [purple line]). The PPC network, instead, has a longer integration\ntimescale, and therefore fails to sufficiently integrate the input to induce a\ndisplacement of the bump to the location of a new stimulus, at each single\ntrial. This is mainly due to the competition between the inputs from the\nrecurrent connections sustaining the bump and the external stimuli that are\nintegrated elsewhere: if the former is stronger, the bump is not displaced.\nIf, however, these inputs are weaker, they will not displace it, but may still\nexert a weakening effect via the global inhibition in the connectivity. The\nexternal input, as well as the presence of adaptation (Figure 3\u2014figure\nsupplement 1B and C), induces a small continuous drift of the activity bump\nthat is already present from the previous trials (lower-right panel of Figure\n2B, Figure 3A [pink bumps] and Figure 3B [pink line]). The build-up of\nadaptation in the PPC network, combined with the global inhibition from other\nneurons receiving external inputs, can extinguish the bump in that location\n(see also Figure 3\u2014figure supplement 1 for more details). Following this, the\nPPC network can make a transition to an incoming stimulus position (that may\nbe either or ), and a new bump is formed. The resulting dynamics in the PPC\nare a mixture of slow drift over a few trials, followed by occasional jumps\n(Figure 3A).\n\nFigure 3 with 2 supplements\n\nDownload asset Open asset\n\n###### Multiple timescales at the core of short-term sensory history effects.\n\n(A) Schematics of activity bump dynamics in the working memory (WM) vs.\nposterior parietal cortex (PPC) network. Whereas the WM responds quickly to\nexternal inputs, the bump in the PPC drifts slowly ...\n\nFigure 3\u2014figure supplement 2\n\nDownload asset Open asset\n\n###### The role of neuronal adaptation in generating short-term history\nbiases.\n\nIn order to better understand the network mechanisms that give rise to short-\nterm history effects, we removed neural adaptation in the posterior parietal\ncortex (PPC) network and assessed the ...\n\nFigure 3\u2014figure supplement 1\n\nDownload asset Open asset\n\n###### Dynamics of responses in a one-dimensional continuous attractor network\nin the presence of adaptation.\n\n(A) We study a one-dimensional line attractor in which neurons code for a\nstimulus feature that varies along a physical dimension, such as amplitude of\nan auditory stimulus. The connection between ...\n\nAs a result of such dynamics, relative to the WM network, the activity bump in\nthe PPC represents the stimuli corresponding to the current trial in a smaller\nfraction of the trials and represents stimuli presented in the previous trial\nin a larger fraction of the trials (Figure 3C). This yields short-term sensory\nhistory effects in our model (Figure 2D and E) as input from the PPC leads to\nthe displacement of the WM bump to other locations (Figure 3D). Given that\nneurons in the WM network integrate this input, once it has built up\nsufficiently, it can surpass the self-sustaining inputs from the recurrent\nconnections in the WM network. The WM bump, then, can move to a new location,\ngiven by the position of the bump in the PPC (Figure 3D). As the input from\nthe PPC builds up gradually, the probability of bump displacement in WM\nincreases over time. This in return leads to an increased probability of\ncontraction bias (Figure 3E) and short-term history (one-trial back) biases\n(Figure 3F), as the interstimulus delay interval increases.\n\nAdditionally, a non-adapted input from the PPC has a larger likelihood of\ndisplacing the WM bump. This is highest immediately following the formation of\na new bump in the PPC or, in other words, following a \u2018bump jump\u2019 (Figure 3F).\nAs a result, one can reason that those trials immediately following a jump in\nthe PPC are the ones that should yield the maximal bias toward stimuli\npresented in the previous trial. We therefore separated trials according to\nwhether or not a jump has occurred in the PPC in the preceding trial (we\ndefine a jump to have occurred if the bump location across two consecutive\ntrials in the PPC is displaced by an amount larger than the typical width of\nthe bump [section \u2018The model\u2019]). In line with this reasoning, only the set\nthat included trials with jumps in the preceding trial yields a one-trial back\nbias (Figure 3G).\n\nRemoving neuronal adaptation entirely from the PPC network further\ncorroborates this result. In this case, the network dynamics show a very\ndifferent behavior: the activity bump in the PPC undergoes a smooth drift\n(Figure 3\u2014figure supplement 2A), and the bump distribution is much more peaked\naround the mean (Figure 3\u2014figure supplement 2B), relative to when adaptation\nis present (Figure 4A). In this regime, there are no jumps in the PPC (Figure\n3\u2014figure supplement 2A), and the activity bump corresponds to the stimuli\npresented in the previous trial in a fewer fraction of the trials (Figure\n3\u2014figure supplement 2C), relative to when adaptation is present (Figure 3B).\nAs a result, no short-term history effects can be observed (Figure 3\u2014figure\nsupplement 2C and D), even though a strong contraction bias persists (Figure\n3\u2014figure supplement 2E).\n\nFigure 4 with 1 supplement\n\nDownload asset Open asset\n\n###### Errors are drawn from the marginal distribution of stimuli, giving rise\nto contraction bias.\n\n(A) The bump locations in both the working memory (WM) network (in pink) and\nthe posterior parietal cortex (PPC) network (in purple) have identical\ndistributions to that of the input stimulus ...\n\nFigure 4\u2014figure supplement 1\n\nDownload asset Open asset\n\n###### The stimulus distribution impacts the pattern of contraction bias.\n\nThe model makes different predictions for the performance, depending on the\nshape of the stimulus distribution. (A) Panel 1: schema of model prediction.\nRegions shaded in red correspond to the ...\n\nAs in the study in Akrami et al., 2018, we can further study the impact of the\nPPC on the dynamics of the WM network by weakening the weights from the PPC to\nthe WM network, mimicking the inactivation of PPC (Figure 2E and F, Figure\n2\u2014figure supplement 1A and B). Under this manipulation, the trajectory of the\nactivity bump in the WM network immediately before the onset of the second\nstimulus closely follows the external input, consistent with an enhanced WM\nfunction (Figure 2\u2014figure supplement 1C and D).\n\nThe drift-jump dynamics in our model of the PPC give rise to short-term\n(notably one- and two-trial back) sensory history effects in the performance\nof the WM network. In addition, we observe an equally salient contraction bias\n(bias toward the sensory mean) in the WM network\u2019s performance, increasing\nwith the delay period (Figure 3E). However, we find that the activity bump in\nboth the WM and the PPC network corresponds to the mean over all stimuli in\nonly a small fraction of trials, expected by chance (Figure 3B, see section\n\u2018Computing bump location\u2019 for how it is calculated). Rather, the bump is\nlocated more often at the current trial stimulus (), and to a lesser extent,\nat the location of stimuli presented at the previous trial (). As a result,\ncontraction bias in our model cannot be attributed to the representation of\nthe running sensory average in the PPC. In the next section, we show how\ncontraction bias arises as an averaged effect when single-trial errors occur\ndue to short-term sensory history biases.\n\n### Errors are drawn from the marginal distribution of stimuli, giving rise to\ncontraction bias\n\nIn order to illustrate the statistical origin of contraction bias in our\nnetwork model, we consider a mathematical scheme of its performance (Figure\n4B). In this simple formulation, we assume that the first stimulus to be kept\nin WM, , is volatile. As a result, in a fraction of the trials, it is\nsusceptible to replacement with another stimulus (by the input from the PPC,\nwhich has a given distribution ; Figure 4A). However, this replacement does\nnot always lead to an error, as evidenced by Bias- and Bias+ trials (i.e.,\nthose trials in which the performance is affected negatively and positively,\nrespectively; Figure 2B). For each stimulus pair, the probability to make an\nerror, , is integral of over values lying on the wrong side of the diagonal\n(Figure 4C). For instance, for stimulus pairs below the diagonal (Figure 4C,\nblue squares) the trial outcome is erroneous only if is displaced above the\ndiagonal (red part of the distribution). As one can see, the area above the\ndiagonal increases as increases, giving rise to a gradual increase in error\nrates (Figure 4C). This mathematical model can capture the performance of the\nattractor network model, as can be seen through the fit of the network\nperformance, when using the bump distribution in the PPC as and as a free\nparameter (see Equation 9 in section \u2018The probability to make errors is\nproportional to the cumulative distribution of the stimuli, giving rise to\ncontraction bias\u2019, Figure 4D and E).\n\nCan this simple statistical model also capture the behavior of rats and humans\n(Figure 2C)? We carried out the same analysis for rats and humans by replacing\nthe bump location distribution of PPC with that of the marginal distribution\nof the stimuli provided in the task based on the observation that the former\nis well-approximated by the latter (Figure 4A). In this case, we see that the\nmodel roughly captures the empirical data (Figure 4F and G), with the addition\nof another parameter that accounts for the lapse rate. Interestingly, such\n\u2018lapse\u2019 also occurs in the network model (as seen by the small amount of\nerrors for pairs of stimuli where is smallest and largest; Figure 4E). This\noccurs because of the drift present in the PPC network that eventually, for\nlong enough delay intervals, causes the bump to arrive at the boundaries of\nthe attractor, which would result in an error.\n\nThis simple analysis implies that contraction bias in the WM network in our\nmodel is not the result of the representation of the mean stimulus in the PPC,\nbut is an effect that emerges as a result of the PPC network\u2019s sampling\ndynamics, mostly from recently presented stimuli. Indeed, a \u2018contraction to\nthe mean\u2019 hypothesis only provides a general account of which pairs of stimuli\nshould benefit from a better performance and which should suffer, but does not\nexplain the gradual accumulation of errors upon increasing (decreasing) , for\npairs below (above) the diagonal (Fassihi et al., 2014; Fassihi et al., 2017;\nAkrami et al., 2018). Notably, it cannot explain why the performance in trials\nwith pairs of stimuli where is most distant from the mean stand to benefit the\nmost from it. Altogether, our model suggests that contraction bias may be a\nsimple consequence of errors occurring at single trials, driven by inputs from\nthe PPC that follow a distribution similar to that of the external input\n(Figure 4B).\n\n### Contraction bias in continuous recall\n\nCan contraction bias also be observed in the activity of the WM network prior\nto binary decision-making? Many studies have evidenced contraction bias also\nin delayed estimation (or production) paradigms, where subjects must retain\nthe value of a continuous parameter in WM and reproduce it after a delay\n(Papadimitriou et al., 2015; Jazayeri and Shadlen, 2010). Given that we\nobserve contraction bias in the behavior of the network, we reasoned that this\nshould also be evident prior to binary decision-making. Similar to delayed\nestimation tasks, we therefore analyzed the position of the bump , at the end\nof the delay interval, for each value of . Consistent with our reasoning, we\nobserve contraction bias of the value of , as evidenced by the systematic\ndeparture of the curve corresponding to the bump location from that of the\nnominal value of the stimulus (Figure 5A). We also find that this contraction\nbias becomes greater as the delay interval increases (Figure 5A, right). We\nnext analyzed the effect of the previous trial on the current trial by\ncomputing the displacement of the bump during the WM delay as a function of\nthe distance between the current trial\u2019s stimulus and the previous trial\u2019s\nstimulus (Figure 5B). We found that when this distance is larger, the\ndisplacement of the bump during WM is on average also larger (Figure 5B). This\ndisplacement is also attractive. Breaking down these effects by delay, we find\nthat longer delays lead to greater attraction (Figure 5B, right).\n\nFigure 5\n\nDownload asset Open asset\n\n###### Contraction bias in continuous recall.\n\n(A) We observe contraction bias of the bump of activity after the delay period\n: the average over trials (black dots) deviates from the identity line\n(diagonal dashed line) toward the mean of ...\n\nThese results point to attractive effects of the previous trial, leading in\nturn to contraction bias in our model. To better understand the dynamics\nleading to them, we next looked at the distribution of bump displacements\nconditioned on a specific value of the second stimulus of the previous trial\n(Figure 5C). These distributions are characterized by a mode around 0,\ncorresponding to a majority of trials in which the bump is not displaced, and\nanother mode around , corresponding to the displacement in the direction of\nthe preceding trial\u2019s stimulus described in section \u2018Multiple timescales at\nthe core of short-term sensory history effects\u2019 and Figure 2B. However, note\nthat the variance of this second mode can be large, reflecting displacements\nto locations other than , due to the complex dynamics in both networks that we\nhave described in detail in section \u2018Multiple timescales at the core of short-\nterm sensory history effects\u2019.\n\n### Model predictions\n\n#### The stimulus distribution impacts the pattern of contraction bias through\nits cumulative\n\nIn our model, the pattern of errors is determined by the cumulative\ndistribution of stimuli from the correct decision boundary to the left (right)\nfor pairs of stimuli below (above) the diagonal (Figure 4C and Figure 4\u2014figure\nsupplement 1A). This implies that using a stimulus set in which this\ndistribution is deformed makes different predictions for the gradient of\nperformance across different stimulus pairs. A distribution that is symmetric\n(Figure 4\u2014figure supplement 1A) yields an equal performance for pairs below\nand above the diagonal (blue and red lines) when is at the mean (as well as\nthe median, given the symmetry of the distribution). A distribution that is\nskewed, instead, yields an equal performance when is at the median for both\npairs below and above the diagonal. For a negatively skewed distribution\n(Figure 4\u2014figure supplement 1B) or positively skewed distribution (Figure\n4\u2014figure supplement 1C), the performance curves for pairs of stimuli below and\nabove the diagonal show different concavity. For a distribution that is\nbimodal, the performance as a function of resembles a saddle, with equal\nperformance for intermediate values of (Figure 4\u2014figure supplement 1D). These\nresults indicate that although the performance is quantitatively shaped by the\nform of the stimulus distribution, it persists as a monotonic function of\nunder a wide variety of manipulations of the distributions. This is a result\nof the property of the cumulative function and may underlie the ubiquity of\ncontraction bias under different experimental conditions.\n\nWe compare the predictions from our simple statistical model to the Bayesian\nmodel in Loewenstein et al., 2021, outlined in section \u2018Bayesian description\nof contraction bias\u2019. We compute the predicted performance of an ideal\nBayesian decision maker using a value of the uncertainty in the representation\nof the first stimulus () that yields the best fit with the performance of the\nstatistical model (where the free parameter is ; Figure 4\u2014figure supplement\n1A\u2013D, panel 2). Our model makes different predictions across all types of\ndistributions from that of the Bayesian model. Across all of the distributions\n(used as priors, in the Bayesian model), the main difference is that of a\nmonotonic dependence of performance as a function of for our model (Figure\n4\u2014figure supplement 1A\u2013D, panel 2). The biggest difference can be seen with a\nprior in which pairs of stimuli with extreme values are much more probable\nthan middle-range values. Indeed, in the case of a bimodal prior, for pairs of\nstimuli where our model would predict a worse-than-average performance (Figure\n4\u2014figure supplement 1D, panel 3), the Bayesian model predicts a very good\nperformance (Figure 4\u2014figure supplement 1D, panel 4).\n\nDo human subjects perform as predicted by our model (Figure 6A)? We tested 34\nhuman subjects on the auditory modality of the task. The experimental protocol\nwas identical to the one used in Akrami et al., 2018. Briefly, participants\nwere presented with two sounds separated by a delay interval that varied\nacross trials (randomly selected from 2, 4, and 6 s). After the second sound,\nparticipants were required to decide which sound was louder by pressing the\nappropriate key. We tested two groups of participants on two stimulus\ndistributions: a negatively skewed and a bimodal distribution (Figure 6A, see\nsection \u2018Human auditory experiment: delayed comparison task\u2019 for more\ndetails). Participants performed the task with a mean accuracy of\napproximately 75%, across stimulus distribution groups and across pairs of\nstimuli (Figure 6B). The experimental data was compatible with the predictions\nof our model. First, for the negatively skewed stimulus distribution\ncondition, we observe a shift of the point of equal performance to the right,\nrelative to a symmetric distribution (Figure 6C, left panel). For the bimodal\ncondition, such a shift is not observed, as predicted by our model (Figure 6C,\nright panel). Second, the monotonic behavior of the performance, as a function\nof , also holds across both distributions (Figure 6C). Our model provides a\nsimple explanation: the percent correct on any given pair is given by the\nprobability that, given a shift in the WM representation, this representation\nstill does not affect the outcome of the trial (Figure 4C). This probability\nis given by cumulative of the probability distribution of WM representations\nfor which we assume the marginal distribution of the stimuli to be a good\napproximation (Figure 4A). As a result, performance is a monotonic function of\n, independent of the shape of the distribution, while the same does not always\nhold true for the Bayesian model (Figure 6C).\n\nFigure 6\n\nDownload asset Open asset\n\n###### The stimulus distribution impacts the pattern of contraction bias\nthrough its cumulative.\n\n(A) Left panel: prediction of performance (left y-axis) of our statistical\nmodel (solid lines) and the Bayesian model (dashed lines) for a negatively\nskewed stimulus distribution (gray bars, to be ...\n\nWe further fit the performance of each participant using both our statistical\nmodel and the Bayesian model by minimizing the mean-squared error (MSE) loss\nbetween the empirical curve and the model, with and as free parameters (Figure\n6C), respectively (for the Bayesian model, we used the marginal distribution\nof the stimuli as the prior). Across participants in both distributions, our\nstatistical model yielded a better fit of the performance, relative to the\nBayesian model (Figure 6D, left panel). We further fit the mean performance\nacross all participants within a given distribution group and similarly found\nthat the statistical model yields a better fit using the MSE as a goodness-of-\nfit metric (Figure 6D, right panel).\n\nFinally, in order to better understand the parameters that affect the\noccurrence of errors in human participants, we computed the performance and\nfraction classified as separately for different delay intervals. We found that\nthe larger the delay interval, the lower the average performance (Figure 7A),\naccompanied by a larger contraction bias for larger intervals (Figure 7B). We\nfurther analyzed the fraction of trials in which subjects responded ,\nconditioned on the specific pair of stimuli presented in the current and the\nprevious trials (Figure 7C) for all distributions (one negatively skewed and\ntwo bimodal distributions, of which only one is shown in Figure 6C).\nCompatible with the previous results (Akrami et al., 2018), we found\nattractive history effects that increased with the delay interval (Figure 7D\nand E).\n\nFigure 7\n\nDownload asset Open asset\n\n###### Attractive effects of the previous trials lead to contraction bias in\nhuman subjects, both increasing with delay interval.\n\n(A) The performance (in percentage correct, shown in numbers above each\nstimulus pair) of human subjects is better with lower delay intervals (left, 2\ns) than with higher delay intervals (right, 6 ...\n\n#### A prolonged intertrial interval improves average performance and reduces\nattractive bias\n\nIf errors are due to the persistence of activity resulting from previous\ntrials, what then is the effect of the ITI? In our model, a shorter ITI\n(relative to the default value of 6 s used in Figures 2 and 3) results in a\nworse performance and vice versa (Figure 8A\u2013C). This change in performance is\nreflected in reduced biases toward the previous trial (Figure 8D and E). A\nprolonged ITI allows for a drifting bump to vanish due to the effect of\nadaptation: as a result, the performance improves with increasing ITI and\nconversely worsens with a shorter ITI.\n\nFigure 8\n\nDownload asset Open asset\n\n###### A prolonged intertrial interval (ITI) improves average performance and\nreduces attractive biases.\n\nWorking memory is attracted toward short-term and repelled from long-term\nsensory history. (A) Performance of the network model for the psychometric\nstimuli improves with an increasing ITI. Error ...\n\nDo human subjects express less bias with longer ITIs as predicted by our\nmodel? In our simulations, we set the ITI to either 2.2, 6, or 11 s, whereas\nin the experiment, since it is self-paced, the ITI can vary considerably. In\norder to emulate the simulation setting as closely as possible, we divided\ntrials into two groups: \u2018short\u2019 ITIs (shorter than 3 s) and \u2018long\u2019 ITIs\n(longer than 3 s). This choice was motivated by the shape of the distribution\nof ITIs, which is bimodal, with a peak around 1 s, and another after 3 s\n(Figure 8F). Given the shape of the ITI distribution, we did not divide the\nITIs into smaller intervals as this would result in too little data in some\nintervals. In line with our model, we found a better average performance with\nincreasing ITI accompanied by decreasing contraction bias (Figure 8G). In\norder to quantify one-trial-back effects, we used data pertaining to all of\nthe distributions we tested \u2013 the negatively skewed, and also two bimodal\ndistributions (of which only one is shown in this article, in Figure 6C). This\nallowed us to obtain clear one-trial-back attractive biases, decreasing with\nincreasing ITI (Figure 8H), in line with our model predictions (Figure 8B and\nD).\n\n#### Working memory is attracted toward short-term and repelled from long-term\nsensory history\n\nAlthough contraction bias is robustly found in different contexts,\nsurprisingly similar tasks, such as perceptual estimation tasks, sometimes\nhighlight opposite effects, that is, repulsive effects (Fritsche et al., 2017;\nLi et al., 2017; Hachen et al., 2021). Interestingly, recent studies have\nfound both effects in the same experiment: in a study of visual orientation\nestimation (Fritsche and Spaak, 2020), it has been found that attraction and\nrepulsion have different timescales; while perceptual decisions about\norientation are attracted toward recently perceived stimuli (timescale of a\nfew seconds), they are repelled from stimuli that are shown further back in\ntime (timescale of a few minutes). Moreover, in the same study, they find that\nthe long-term repulsive bias is spatially specific, in line with sensory\nadaptation (Knapen et al., 2010; Boi et al., 2011; Math\u00f4t and Theeuwes, 2013)\nand in contrast to short-term attractive serial dependence (Fritsche and\nSpaak, 2020). Given that adaptation is a main feature of our model of the PPC,\nwe sought to determine whether such repulsive effects can emerge from the\nmodel. We extended the calculation of the bias to up to 10 trials back and\nquantified the slope of the bias as a function of the previous trial stimulus\npair. We observe robust repulsive effects appear after the third trial back in\nhistory and up to six trials back (Figure 8I). In our model, both short-term\nattractive effects and longer-term repulsive effects can be attributed to the\nmultiple timescales over which the networks operate. The short-term attractive\neffects occur due to the long time it takes for the adaptive threshold to\nbuild up in the PPC and the short timescale with which the WM network\nintegrates input from the PPC. The longer-term repulsive effects occur when\nthe activity bump in the PPC persists in one location and causes adaptation to\nslowly build up, effectively increasing the activation threshold. The raised\nthreshold takes equally long to return to baseline, preventing activity bumps\nto form in that location and thereby creating repulsion toward all the other\nlocations in the network. Crucially, however, the amplitude of such effects\ndepends on the ITI; in particular, for shorter ITIs, the repulsive effects are\nless observable.\n\n### The timescale of adaptation in the PPC network can control perceptual\nbiases similar to those observed in dyslexia and autism\n\nIn a recent study (Lieder et al., 2019), a similar PWM task with auditory\nstimuli was studied in human neurotypical (NT), autistic spectrum (ASD) and\ndyslexic (DYS) subjects. Based on an analysis using a generalized linear model\n(GLM), a double dissociation between different subject groups was suggested:\nASD subjects exhibit a stronger bias toward long-term statistics \u2013 compared to\nNT subjects \u2013 while for DYS subjects, a higher bias is present toward short-\nterm statistics.\n\nWe investigated our model to see whether it is able to show similar\nphenomenology, and if so, what are the relevant parameters controlling the\ntimescale of the biases in behavior? We identified the adaptation timescale in\nthe PPC as the parameter that affects the extent of the short-term bias,\nconsistent with previous literature (Jaffe-Dax et al., 2018; Jaffe-Dax et al.,\n2017). Calculating the mean bias toward the previous trial stimulus pair\n(Figure 9A), we find that a shorter-than-NT adaptation timescale yields a\nlarger bias toward the previous trial stimulus. Indeed, a shorter timescale\nfor neuronal adaptation implies a faster process for the extinction of the\nbump in PPC \u2013 and the formation of a new bump that remains stable for a few\ntrials \u2013 producing \u2018jumpier\u2019 dynamics that lead to a larger number of one-\ntrial-back errors. In contrast, increasing this timescale with respect to NT\ngives rise to a stable bump for a longer time, ultimately yielding a smaller\nshort-term bias. This can be seen in the detailed breakdown of the network\u2019s\nbehavior on the current trial when conditioned on the stimuli presented at the\nprevious trial (Figure 9B, see also section \u2018Multiple timescales at the core\nof short-term sensory history effects\u2019 for a more detailed explanation of the\ndynamics). We performed a GLM analysis as in Lieder et al., 2019 to the\nnetwork behavior, with stimuli from four trials back and the mean stimulus as\nregressors (see section \u2018Generalized linear model\u2019). This analysis shows that\na reduction in the PPC adaptation timescale with respect to NT produces\nbehavioral changes qualitatively compatible with data from DYS subjects; on\nthe contrary, an increase of this timescale yields results consistent with ASD\ndata (Figure 9C).\n\nFigure 9 with 1 supplement\n\nDownload asset Open asset\n\n###### Apparent tradeoff between short- and long-term biases controlled by the\ntimescale of neural adaptation.\n\n(A) The bias exerted on the current trial by the previous trial (see main text\nfor how it is computed) for three values of the adaptation timescale that\nmimic similar behavior to the three cohorts ...\n\nFigure 9\u2014figure supplement 1\n\nDownload asset Open asset\n\n###### Apparent tradeoff between short- and long-term biases controlled by the\ntimescale of neural adaptation.\n\n(A) Left: generalized linear model (GLM) weight associated with the regressor\ncorresponding to the mean stimulus across trials (value indicated by colorbar)\nas a function of the strength of the ...\n\nThis GLM analysis suggests that dissociable short- and long-term biases may be\npresent in the network behavior. Having access to the full dynamics of the\nnetwork, we sought to determine how it translates into such dissociable short-\nand long-term biases. Given that all the behavior arises from the location of\nthe bump on the attractor, we quantified the fraction of trials in which the\nbump in the WM network, before the onset of the second stimulus, was present\nin the vicinity of any of the previous trial\u2019s stimuli (Figure 9\u2014figure\nsupplement 1B, right panel, and Figure 9\u2014figure supplement 1C), as well as the\nvicinity of the mean over the sensory history (Figure 9\u2014figure supplement 1B,\nleft panel, and Figure 9\u2014figure supplement 1C). While the bump location\ncorrelated well with the GLM weights corresponding to the previous trial\u2019s\nstimuli regressor (comparing the right panels of Figure 9\u2014figure supplement 1A\nand B), surprisingly, it did not correlate with the GLM weights corresponding\nto the mean stimulus regressor (comparing the left panels of Figure 9\u2014figure\nsupplement 1A and B). In fact, we found that the bump was in a location given\nby the stimuli of the past two trials, as well as the mean over the stimulus\nhistory, in a smaller fraction of trials, as the adaptation timescale\nparameter was made larger (Figure 9\u2014figure supplement 1C).\n\nGiven that the weights, after four trials in the past, were still non-zero, we\nextended the GLM regression by including a larger number of past stimuli as\nregressors. We found that doing this greatly reduced the weight of the mean\nstimulus regressor (Figure 9C\u2013E, see section \u2018Generalized linear mode\u2019 for\nmore details). Therefore, we propose an alternative interpretation of the GLM\nresults given in Lieder et al., 2019. In our model, the increased (reduced)\nweight for long-term mean in the ASD (DYS) subjects can be explained as an\neffect of a larger (smaller) window in time of short-term biases without\ninvoking a double dissociation mechanism (Figure 9D and E). In section\n\u2018Generalized linear model\u2019, we provide a mathematical argument for this, which\nis empirically shown by including a large number of individual stimuli from\nprevious trials in the regression analysis.\n\n## Discussion\n\n### Contraction bias in the delayed comparison task: simply a statistical\neffect or more?\n\nContraction bias is an effect emerging in WM tasks, where in the averaged\nbehavior of a subject the magnitude of the item held in memory appears to be\nlarger than it actually is when it is \u2018small\u2019 and, vice versa, it appears to\nbe smaller when it is \u2018large\u2019 (Algom, 1992; Berliner et al., 1977; Hellstr\u00f6m,\n1985; Poulton and Poulton, 1989; Ashourian and Loewenstein, 2011; Preuschhof\net al., 2010; Olkkonen et al., 2014). Recently, Akrami et al., 2018 found that\ncontraction bias as well as short-term history-dependent effects occur in an\nauditory delayed comparison task in rats and humans: the comparison\nperformance in a given trial depends on the stimuli shown in preceding trials\n(up to three trials back) (Akrami et al., 2018), similar to previous findings\nin human 2AFC paradigms (Raviv et al., 2012). These findings raise the\nquestion: does contraction bias occur independently of short-term history\neffects, or does it emerge as a result of the latter?\n\nAkrami et al., 2018 have also found the PPC to be a critical node for the\ngeneration of such effects as its optogenetic inactivation (specifically\nduring the delay interval) greatly attenuated both effects. WM was found to\nremain intact, suggesting that its content was perhaps read-out in another\nregion. Electrophysiological recordings as well as optogenetic inactivation\nresults in the same study suggest that while sensory history information is\nprovided by the PPC, its integration with the WM content must happen somewhere\ndownstream to the PPC. Different brain areas can fit the profile. For\ninstance, there are known projections from the PPC to mPFC in rats (Olsen et\nal., 2019), where neural correlates of PWM have been found (Esmaeili and\nDiamond, 2019). Building on these findings, we suggest a minimal two-module\nmodel aimed at better understanding the interaction between contraction bias\nand short-term history effects. These two modules capture properties of the\nPPC (in providing sensory history signals) and a downstream network holding WM\ncontent. Our WM and PPC networks, despite having different timescales, are\nboth shown to encode information about the marginal distribution of the\nstimuli (Figure 4A). Although they have similar activity distributions to that\nof the external stimuli, they have different memory properties due to the\ndifferent timescales with which they process incoming stimuli. The putative WM\nnetwork, from which information to solve the task is read-out, receives\nadditional input from the PPC network. The PPC is modeled as integrating\ninputs slower relative to the WM network and is also endowed with firing rate\nadaptation, the dynamics of which yield short-term history biases and,\nconsequently, contraction bias.\n\nIt must be noted, however, that short-term history effects (due to firing rate\nadaptation) do not necessarily need to be invoked in order to recover\ncontraction bias: as long as errors are made following random samples from a\ndistribution in the same range as that of the stimuli, contraction bias should\nbe observed (Tong and Dub\u00e9, 2022). Indeed, when we manipulated the parameters\nof the PPC network in such a way that short-term history effects were\neliminated (by removing the firing rate adaptation), contraction bias\npersisted. As a result, our model suggests that contraction bias may not\nsimply be given by a regression toward the mean of the stimuli during the\ninterstimulus interval (Karim et al., 2013; Kerst and Howard, 1978), but\nbrought about by a richer dynamics occurring at the level of individual trials\n(Jou et al., 2004), more in line with the idea of random sampling (Rahnev and\nDenison, 2018).\n\nThe model makes predictions as to how the pattern of errors may change when\nthe distribution of stimuli is manipulated either at the level of the\npresented stimuli or through the network dynamics. When we tested these\npredictions experimentally by manipulating the skewness of the stimulus\ndistribution such that the median and the mean were dissociated (Figure 6A),\nthe results from our human psychophysics experiments were in agreement with\nthe model predictions. In further support of this, in a recent tactile\ncategorization study (Hachen et al., 2021), where rats were trained to\ncategorize tactile stimuli according to a boundary set by the experimenter,\nthe authors have shown that rats set their decision boundary according to the\nstatistical structure of the stimulus set to which they are exposed. More\nstudies are needed to fully verify the extent to which the statistical\nstructure of the stimuli affects the performance. Finally, we note that in our\nmodel the stimulus distribution is not explicitly learned (but see Maes et\nal., 2023): instead, the PPC dynamics follows the input, and its marginal\ndistribution of activity is similar to that of the external input. This is in\nagreement with Hachen et al., 2021, where the authors used different stimulus\nranges across different sessions and noted that rats initiated each session\nwithout any residual influence of the previous session\u2019s range/boundary on the\ncurrent session, ruling out long-term learning of the input structure.\n\nImportantly, our results are not limited to the delayed \u2018comparison\u2019 paradigm,\nwhere binary decision-making occurs. We show that by analyzing the location of\nthe WM bump at the end of the delay interval, similar to the continuous recall\ntasks, we can retrieve the averaged effects of contraction bias, similar to\nprevious reports (Jazayeri and Shadlen, 2010). Such continuous read-out of the\nmemory reveals a rich dynamics of errors at the level of individual trials,\nsimilar to the delayed comparison case, but to our knowledge this has not been\nstudied in previous experimental studies. Papadimitriou et al., 2015 have\ncharacterized residual error distribution in an orientation recall task when\nlimiting previous trials to orientations in the range of +35 to+ 85\u00b0 relative\nto the current trial. This distribution is unimodal, leading the authors to\nconclude that the current trial shows a small but systematic bias toward the\nlocation of the memorandum of the previous trial. It remains to be tested\nwhether the error distribution remains unimodal if conditioned on other values\nof the current and previous orientations, similar to our analysis in Figure\n5C.\n\n### Attractor mechanism riding on multiple timescales\n\nOur model assumes that the stimulus is held in WM through the persistent\nactivity of neurons, building on the discovery of persistent selective\nactivity in a number of cortical areas, including the prefrontal cortex (PFC),\nduring the delay interval (Fuster and Alexander, 1971; Miyashita and Chang,\n1988; Funahashi et al., 1989; Funahashi et al., 1990; Romo et al., 1999;\nSalinas et al., 2000; Zhang et al., 2019). To explain this finding, we have\nused the attractor framework, in which recurrently connected neurons mutually\nexcite one another to form reverberation of activity within populations of\nneurons coding for a given stimulus (Hopfield, 1982; Amit, 1992; Battaglia and\nTreves, 1998). However, subsequent work has shown that persistent activity\nrelated to the stimulus is not always present during the delay period and that\nthe activity of neurons displays far more heterogeneity than previously\nthought (Barak et al., 2013). It has been proposed that short-term synaptic\nfacilitation may dynamically operate to bring a WM network across a phase\ntransition from a silent to a persistently active state (Mongillo et al.,\n2008; Barak and Tsodyks, 2007). Such mechanisms may further contribute to\nshort-term biases (Barbosa et al., 2020), an alternative possibility that we\nhave not specifically considered in this model.\n\nAn important model feature that is crucial in giving rise to all of its\nbehavioral effects is its operation over multiple timescales (Figure 3\u2014figure\nsupplement 2F). Such timescales have been found to govern the processing of\ninformation in different areas of the cortex (Murray et al., 2014; Siegle et\nal., 2021; Gao et al., 2020) and may reflect the heterogeneity of connections\nacross different cortical areas (Stern et al., 2021).\n\n### Relation to other models\n\nIn many early studies, groups of neurons whose activity correlates\nmonotonically with the stimulus feature, known as \u2018plus\u2019 and \u2018minus\u2019 neurons,\nhave been found in the PFC (Romo et al., 1999; Barak et al., 2010). Such\nneurons have been used as the starting point in the construction of many\nmodels (Miller et al., 2003; Machens et al., 2005; Barak et al., 2013; Barak\nand Tsodyks, 2014). It is important, however, to note that depending on the\narea the fraction of such neurons can be small (Esmaeili and Diamond, 2019)\nand that the majority of neurons exhibit firing profiles that vary largely\nduring the delay period (Machens et al., 2010). Such heterogeneity of the PFC\nneurons\u2019 temporal firing profiles has prompted the successful construction of\nmodels that have not included the basic assumption of plus and minus neurons,\nbut these have largely focused on the plausibility of the dynamics of neurons\nobserved, with little connection to behavior (Barak et al., 2013).\n\nA separate line of research has addressed behavior by focusing on normative\nmodels to account for contraction bias (Ashourian and Loewenstein, 2011; Raviv\net al., 2012; Rahnev and Denison, 2018; Salinas, 2011). The abstract\nmathematical model that we present (Figure 4) can be compatible with a\nBayesian framework (Ashourian and Loewenstein, 2011) in the limit of a very\nbroad likelihood for the first stimulus and a very narrow one for the second\nstimulus, and where the prior for the first stimulus is replaced by the\ndistribution of , following the model in Figure 4B (see section \u2018The\nprobability to make errors is proportional to the cumulative distribution of\nthe stimuli, giving rise to contraction bias\u2019 for details). However, it is\nimportant to note that our model is conceptually different, that is, subjects\ndo not have access to the full prior distribution, but only to samples of the\nprior. We show that having full knowledge of the underlying sensory\ndistribution is not needed to present contraction bias effects. Instead, a\npoint estimate of past events that is updated trial to trial suffices to show\nsimilar results. This suggests a possible mechanism for the brain to\napproximate Bayesian inference, and it remains open whether similar mechanisms\n(based on the interaction of networks with different integration timescales)\ncan approximate other Bayesian computations. It is also important to note the\ndifferences between the predictions from the two models. As shown in Figure 6A\nand Figure 4\u2014figure supplement 1, depending on the specific sensory\ndistributions, the two models can have qualitatively different testable\npredictions. Data from our human psychophysical experiments, utilizing\nauditory PWM, show better agreement with our model predictions compared to the\nBayesian model.\n\nMoreover, an ideal Bayesian observer model alone cannot capture the temporal\npattern of short-term attraction and long-term repulsion observed in some\ntasks, and the model has had to be supplemented with efficient encoding and\nBayesian decoding of information in order to capture both effects (Fritsche\nand Spaak, 2020). In our model, both effects emerge naturally as a result of\nneuronal adaptation, but their amplitudes crucially depend on the time\nparameters of the task, perhaps explaining the sometimes contradictory effects\nreported across different tasks.\n\nFinally, while such attractive and repulsive effects in performance may be\nsuboptimal in the context of a task designed in a laboratory setting, this may\nnot be the case in more natural environments. For example, it has been\nsuggested that integrating information over time serves to preserve perceptual\ncontinuity in the presence of noisy and discontinuous inputs (Fischer and\nWhitney, 2014). This continuity of perception may be necessary to solve more\ncomplex tasks or make decisions, particularly in a non-stationary environment,\nor in a noisy environment.\n\n## Methods\n\n### The model\n\nOur model is composed of two populations of neurons, representing the PPC\nnetwork and the putative WM network. We consider that each population is\norganized as a continuous line attractor, with recurrent connectivity\ndescribed by an interaction matrix , whose entries represent the strength of\nthe interaction between neurons and . The activation function of the neurons\nis a logistic function, that is, the output of neuron , given the input , is\n\n(1)\n\nwhere is the neuronal gain. The variables take continuous values between 0 and\n1 and represent the firing rates of the neurons. The input to a neuron is\ngiven by\n\n(2)\n\nwhere is the timescale for the integration of inputs. In the first term on the\nright-hand side, represents the input to neuron from neuron , and corresponds\nto the external inputs. The recurrent connections are given by\n\n(3)\n\nwith\n\n(4)\n\nThe interaction kernel, , is assumed to be the result of a time-averaged\nHebbian plasticity rule: neurons with nearby firing fields will fire\nconcurrently and strengthen their connections, while firing fields far apart\nwill produce weak interactions (Dalgleish et al., 2020). Neuron is associated\nwith the firing field . The form of expresses a connectivity between neurons\nand that is exponentially decreasing with the distance between their\nrespective firing fields, proportional to ; the exponential rate of decrease\nis set by the constant ,that is, the typical range of interaction. The\namplitude of the kernel is also rescaled by in such a way that is constant.\nThe strength of the excitatory weights is set by ; the normalization of ,\ntogether with the sigmoid activation function saturating to 1, implies that is\nalso the maximum possible input received by any neuron due to the recurrent\nconnections. The constant , instead, contributes to a linear global inhibition\nterm. Its value needs to be chosen depending on and , so that the balance\nbetween excitatory and inhibitory inputs ensures that the activity remains\nlocalized along the attractor, that is, it does not either vanish or equal 1\neverywhere; together, these three constants set the width of the bump of\nactivity.\n\nThe two networks in our model are coupled through excitatory connections from\nthe PPC to the WM network. Therefore, we introduce two equations analogous to\nEquation 2, one for each network. The coupling between the two will enter as a\nfiring rate-dependent input, in addition to . The dynamics of the input to a\nneuron in the WM network writes\n\n(5)\n\nwhere indexes neurons in the WM network, and is the timescale for the\nintegration of inputs in the WM network. The first term in the right-hand side\ncorresponds to inputs from recurrent connections within the WM network. The\nsecond term corresponds to inputs from the PPC network. Finally, the last term\ncorresponds to the external inputs used to give stimuli to the network.\nSimilarly, for the PPC network we have\n\n(6)\n\nwhere indexes neurons in the PPC, and is the timescale for the integration of\ninputs in the PPC network; importantly, we set this to be longer than the\nanalogous quantity for the WM network, (see Appendix 1\u2014table 1). The first and\nthird terms in the right-hand side are analogous to the corresponding ones for\nthe WM network: inputs from within the network and from the stimuli. The\nsecond term instead corresponds to adaptive thresholds with dynamics specified\nby\n\n(7)\n\nmodeling neuronal adaptation, where and set its timescale and its amplitude.\nWe are interested in the condition where the timescale of the evolution of the\ninput current is much smaller relative to that of the adaptation (). For a\nconstant , we find that depending on the value of , the bump of activity shows\ndifferent behaviors. For low values of , the bump remains relatively stable\n(Figure 3\u2014figure supplement 1C; Hollingworth, 1910). Upon increasing , the\nbump gradually starts to drift (Figure 3\u2014figure supplement 1C; Jou et al.,\n2004; Berliner et al., 1977; Romani and Tsodyks, 2015). Upon increasing even\nfurther, a phase transition leads to an abrupt dissipation of the bump (Figure\n3\u2014figure supplement 1C; Hellstr\u00f6m, 1985).\n\nNote that, while the transition from bump stability to drift occurs gradually,\nthe transition from drift to dissipation is abrupt. This abruptness in the\ntransition from the drift to the dissipation regime may imply that only one of\nthe two behaviors is possible in our model of the PPC (section \u2018Multiple\ntimescales at the core of short-term sensory history effects\u2019). In fact, our\nnetwork model of the PPC operates in the \u2018drift\u2019 regime (, ). However, we also\nobserve dissipation of the bump, which is mainly responsible for the jumps\nobserved in the model. This occurs due to the inputs from incoming external\nstimuli that affect the bump via the global inhibition in the model (Figure\n3\u2014figure supplement 1A). Therefore, external stimuli can allow the network to\ntemporarily cross the sharp drift/dissipation boundary shown in Figure\n3\u2014figure supplement 1B. As a result, the combined effects of adaptation,\ntogether with external inputs and global inhibition, result in the drift/jump\ndynamics described in the main text.\n\nFinally, both networks have a linear geometry with free boundary conditions,\nthat is, no condition is imposed on the profile activity at neuron 1 or .\n\n### Simulation\n\nWe performed all the simulations using custom Python code. Differential\nequations were numerically integrated with a time step of using the forward\nEuler method. The activity of neurons in both circuits was initialized to .\nEach stimulus was presented for 400 ms. A stimulus is introduced as a \u2018box\u2019 of\nunit amplitude and of width around in stimulus space: in a network with\nneurons, the stimulus is given by setting in Equation 5 for neurons with index\nwithin , and for all the others. Only the activity in the WM network was used\nto assess performance. To do that, the activity vector was recorded at two\ntimepoints: 200 ms before and after the onset of the second stimulus . Then,\nthe neurons with the maximal activity were identified at both timepoints and\ncompared to make a decision. This procedure was done for 50 different\nsimulations with 1000 consecutive trials in each, with a fixed ITI separating\ntwo consecutive trials, fixed to 5 s. The interstimulus intervals were set\naccording to two different experimental designs, as explained below.\n\n#### Interleaved design\n\nAs in the study in Akrami et al., 2018, an interstimulus interval of either 2,\n6, or 10 s was randomly selected. The delay interval is defined as the time\nelapsed from the end of the first stimulus to the beginning of the second\nstimulus. This procedure was used to produce Figures 1\u20143 and 7, Figure\n2\u2014figure supplement 1, and Figure 3\u2014figure supplement 2.\n\n#### Block design\n\nIn order to provide a comparison to the interleaved design, but also to\nsimulate the design in Lieder et al., 2019, we also ran simulations with a\nblock design, where the interstimulus intervals were kept fixed throughout the\ntrials. Other than this, the procedure and parameters used were exactly the\nsame as in the interleaved case. This procedure was used to produce Figure 9,\nFigure 2\u2014figure supplement 2, and Figure 9\u2014figure supplement 1.\n\n### Human auditory experiment: Delayed comparison task\n\nSubjects received, in each trial, a pair of sounds played from ear-surrounding\nheadphones. The subject self-initiated each trial by pressing the spacebar on\nthe keyboard. The first sound was then presented together with a blue square\non the left side of a computer monitor in front of the subject. This was\nfollowed by a delay period, indicated by \u2018WAIT!\u2019 on the screen, then the\nsecond sound was presented together with a red square on the right side of the\nscreen. At the end of the second stimulus, subjects had 2 s to decide which\none was louder, then indicate their choice by pressing the \u2018s\u2019 key if they\nthought that the first sound was louder, or the \u2018l\u2019 key if they thought that\nthe second sound was louder. Written feedback about the correctness of their\nresponse was provided on the screen for each individual trial. Every 10\ntrials, participants received feedback on their running mean performance\ncalculated up to that trial. Participants then had to press spacebar to go to\nthe next trial (the experiment was hence self-paced).\n\nThe two auditory stimuli, and , separated by a variable delay (of 2, 4, and 6\ns), were played for 400 ms, with short delay periods of 250 ms inserted before\nand after . The stimuli consisted of broadband noise 2000\u201320,000 Hz, generated\nas a series of sound pressure level (SPL) values sampled from a zero-mean\nnormal distribution. The overall mean intensity of sounds varied from 60 to 92\ndB. Participants had to judge which out of the two stimuli, and , was louder\n(had the greater SPL standard deviation). We recruited 10 subjects for the\nnegatively skewed distribution and 24 subjects for the bimodal distribution.\nEach participant performed approximately 400 trials for a given distribution.\nSeveral participants took part in both distributions.\n\nThe study was approved by the University College London (UCL) Research Ethics\nCommittee (16159/001) (London, UK). Before starting the experiment,\nparticipants were provided with an information sheet relevant to the\nexperiment they will be performing and asked to sign an informed written\nconsent. By signing this consent form, the participants consent to freely take\npart in the study and confirm they understand the information they received.\nThe participants additionally confirm they understand that their participation\nis voluntary and that they are allowed to stop the experiment at any moment\nand withdraw the data they provided. Participants consent to allow use of\ntheir personal data only for the purpose of scientific research, as determined\nby applicable law. Identifiable personal data are only available to the\nresearchers and securely stored upon need, while the anonymized version of\nthis data is freely available in a public repository. Once published, the\nparticipants\u2019 contribution will remain non-identifiable.\n\n### Computing bump location\n\nIn order to check whether the bump is in a target location (Figure 3B, Figure\n2\u2014figure supplement 1D, and Figure 3\u2014figure supplement 2B), we check whether\nthe position of the neuron with the maximal firing rate is within a distance\nof \u00b15% of the length of the whole line attractor from the target location\n(Figure 3A, Figure 2\u2014figure supplement 1C, and Figure 3\u2014figure supplement 2A).\nIn these figures, we compare the probability that, in a given trial, the\nactivity of the WM network is localized around one of the previous stimuli\n(estimated from the simulation of the dynamics, histograms) with the\nprobability of this happening due to chance (horizontal dashed line). Here we\ndetail the calculation of the chance probability. In general, if we have two\ndiscrete independent random variables, and , with probability distributions\nand , the probability of them having the same value is\n\nwhere are the indices for different values of the two random variables and\nequals 1 where and 0 otherwise. If the two random variables are identically\ndistributed, the above expression writes\n\nIn our case, the two identically distributed random variables are \u2018bump\nlocation at the current trial\u2019 and the \u2018target bump location\u2019 (that are , , ,\n, and ). With the exception of the mean stimulus , all the other variables are\nidentically distributed, with probability (that is the marginal distribution\nover or ). We note that the bump location in the WM network follows a very\nsimilar distribution to (Figure 4A). Then, we compute the chance probability\nwith the above relationship, where . For the mean stimulus, instead, we have a\nprobability which is simply equal to 1 for and 0 elsewhere; therefore, the\nchance probability for the bump location to be at the mean stimulus then is .\n\nThe excess probability (with respect to chance) for the bump location to equal\none of the previous stimuli gives a measure of the correlation between these\ntwo; in other terms, of the amount of information retained by the network\nabout previous stimuli.\n\n### The probability to make errors is proportional to the cumulative\ndistribution of the stimuli, giving rise to contraction bias\n\nIn order to illustrate the statistical origin of contraction bias consistent\nwith our network model, we consider a simplified mathematical model of its\nperformance (Figure 4B). By definition of the delayed comparison task, the\noptimal decision maker produces a label equal to 1 if , and 0 if ; the\nimpossible cases are excluded from the set of stimuli, but would produce a\nlabel which is either 0 or 1 with 50% probability. That is,\n\n(8)\n\nIn this simplified scheme, at each trial , the two stimuli and are perfectly\nperceived with a finite probability , with . Under the assumption that the\ndecision maker behaves optimally based on the perceived stimuli, a correct\nperception would necessarily lead to the correct label. However, with\nprobability , the first stimulus is randomly selected from a buffer of\nstimuli, that is, is replaced by a random variable that has a probability\ndistribution .\n\nThe probability distribution is the statistics of previously shown stimuli.\nThe information about the previous stimulus is given by the activity of the\n\u2018slower\u2019 PPC network. As shown above, after the presentation of the first\nstimulus of the trial, the bump of activity is seen to jump to the position\nencoding one of the previously presented stimuli, , , , etc., with decreasing\nprobability (Figure 3C). Therefore, in calculating the performance in the\ntask, we can take to be the marginal distribution of the stimulus or across\ntrials, as in the histogram (Figure 4A).\n\nThe probability of a misclassification is then given by the probability that,\ngiven the pair , at trial ,\n\n  1. the first stimulus is replaced by a random value, which happens with probability , and\n\n  2. the value of replaced is larger than when is smaller and vice versa (Figure 4C).\n\nIn summary, the probability of an error at trial is given by\n\n(9)\n\n### Bayesian description of contraction bias\n\nWe reproduce here the theoretical result from Loewenstein et al., 2021, which\nprovides a normative model for contraction bias in the Bayesian inference\nframework, and apply it to the different stimulus distributions described in\nsection \u2018The stimulus distribution impacts the pattern of contraction bias\nthrough its cumulative\u2019.\n\nA stimulus with value is encoded by the agent through a noisy representation .\nBefore the presentation of the stimulus, the agent has an expectation of its\npossible values which is described by the probability . Assuming that it has\naccess to the internal representation , as well as the probability\ndistributions and , the agent can infer the perceived stimulus through Bayes\nrule:\n\n(10)\n\nwhere . In this Bayesian setting, the probability distributions for the noisy\nrepresentation and expected measurement are interpreted as the likelihood and\nthe prior, respectively.\n\nIn the delayed comparison task, at the time of the decision, the two stimuli\nand are assumed to be encoded independently, although with different\nuncertainties, due to the different delays leading to the time of decision: ,\nwith . Similarly, the expected values of the stimuli are assumed to be\nindependent but also identically distributed: .\n\nThe optimal Bayesian decision maker uses the inference of the stimuli through\nEquation 10 to produce an estimate of the probability that , given the\ninternal representations,\n\n(11)\n\nwhere is the Heaviside function and yields a label (truth value of \u2018\u2019) when\nsuch probability is higher than 1/2, and otherwise. Therefore, the probability\nthat the Bayesian decision maker yields the response \u2018\u2019 given the true values\nof the stimuli and are the average of the label over the possible values of\ntheir representations, that is, over the likelihood:\n\n(12)\n\n#### Application to our study\n\nIn modeling our data, we assume that the likelihood functions and are Gaussian\nwith mean equal to the stimulus, but with different standard deviations, and ,\nrespectively, as in Loewenstein et al., 2021. We restrict to the particular\ncase where , that is, there is no uncertainty in the representation of the\nsecond stimulus, since there is negligible delay between its presentation and\nthe decision. We instead assume a finite standard deviation , which we use as\nthe only free parameter of this model to produce Figure 4\u2014figure supplement\n1A\u2013D, panels 2 and 4.\n\nThe prior is chosen to be the marginal distribution of the first stimulus \u2013\nidentical to the marginal of the second stimulus, because of symmetry.\n\nWhen , (Dirac delta), and the predicted response probability, Equation 12,\nreduces to\n\n(13)\n\n### Generalized Linear Model (GLM)\n\n#### GLM as in Lieder et al.\n\nSimilarly to Lieder et al., 2019, we performed a multivariate logistic\nregression (an instance of GLM) to the output of the network in the delayed\ndiscrimination task with recent stimuli values as covariates:\n\n(14)\n\nwhere is the sigmoidal function , is the mean of the stimuli presented at\ntrial , is the number of \u2018history\u2019 terms in the regression, and is the mean of\nthe stimuli within and across trials up to the current one. As in Lieder et\nal., 2019, we choose , that is, we include in the short-term history the four\ntrials prior to the current one. The first term in Equation 14, with weight ,\ncontrols the slope of the psychometric curve. The remaining terms, combined\nlinearly with weights , contribute to biases expressing the long- and short-\nterm memory. In Lieder et al., 2019, it is shown that subjects on the ASD\nconserve the higher long-term weights, , while losing the short-term weights\nexpressed by NT subjects. In contrast, DYS subjects conserve a higher bias\nfrom the recent stimuli, , while losing the higher long-term weights, also\nexpressed by NT subjects.\n\nIn order to gain insight into this regression model in terms of our network,\nwe also performed a linear regression of the bump of activity just before the\nonset of the second stimulus, denoted , versus the same variables:\n\n(15)\n\nIn this case, we see that the weights in the linear regression for have the\nsame qualitative behavior as the weights for the bias term in the GLM\nregression for the performance (not shown). This is expected since the\ndecision-making rule in the network \u2013 based on the bump location just before\nand during the second stimulus, and , respectively \u2013 is deterministic,\nfollowing . Therefore, the bias term in the GLM performed in Lieder et al.,\n2019, Equation 14, corresponds to the displacement of the bump location with\nrespect to the actual stimulus , modeled to be linearly dependent on the\ndisplacement of previous stimuli from .\n\n### Regression model with infinite history\n\nIn the regression formulas in Equations 14 and 15, it is possible to give an\ninterpretation of the parameter , that is, the weight of the contribution from\nthe covariate corresponding to the mean of the past stimuli. Let us consider\ntwo regression models, one in which, in addition to a regressor corresponding\nto the mean stimulus, regressors corresponding to the stimulus history are\nincluded up to trial , and another in which , that is, infinitely many past\nstimuli are included as regressors. In this case, Equation 15 rewrites\n\n(16)\n\nIf we assume that the weights obtained from the regression have roughly an\nexponential dependence on time (Figure 9C and D), we can write\n\n(17)\n\nBy equating Equations 15 and 16, we would find that\n\n(18)\n\nwhere\n\n(19)\n\nthat is, an average over the geometric distribution , from time backward.\nSince for large enough we have , we can identify\n\n(20)\n\nThis derivation indicates that the magnitude in the infinite history model,\ngiven by Equation 15, is a function of the discount factor as well as the\nweight of the first trial left out from the finite history regression (). A\nhigher value, that is, a longer timescale for damping of the weights extending\ninto the stimulus history, yields a higher . We can obtain for each condition\n(NT, ASD, and DYS) by fitting the weights obtained as a function of trials\nextending into the history (Figure 9C and D). As predicted by Equation 20, a\nlarger window for short-term history effects (as in the ASD case relative to\nNT) yields a larger weight for the covariate corresponding to the mean\nstimulus. Finally, Equation 20 also predicts that is proportional to , the\nnumber of trials back we consider in the regression, , implying that the\nnumber of covariates that we choose to include in the model may greatly affect\nthe results. Both of these predictions are corroborated by plotting directly\nthe value of obtained from the regression (Figure 9E).\n\n## Appendix 1\n\n### Parameters\n\nAppendix 1\u2014table 1\n\n###### Simulation parameters, when not explicitly mentioned.\n\nUsed to produce Figures 1\u20145 and 8, Figure 2\u2014figure supplements 1 and 2, and\nFigure 3\u2014figure supplement 2.\n\nParameter| Symbol| Default value  \n---|---|---  \nNumber of neurons| 2000  \nNeuronal gain| 5  \nRange of excitatory interactions (in units of stimulus space length)| 0.02  \nStrength of inhibitory weights| 0.2  \nStrength of excitatory weights| 1  \nTimescale of neuronal integration in WM net (s)| 0.01  \nTimescale of neuronal integration in PPC (s)| 0.5  \nTimescale of neuronal adaptation in PPC (s)| 7.5  \nAmplitude of adaptation current in PPC| 0.3  \nAmplitude of external inputs| 1  \nStrength of weights from PPC to WM net| 0.5  \nDuration of stimuli (s)| 0.4  \nDelay interval (s)| [2, 6, 10]  \nIntertrial interval (s)| 6  \nWidth of box stimulus (in units of stimulus space length)| 0.05  \n  \n  1. WM, working memory; PPC, posterior parietal cortex.\n\n.\n\nAppendix 1\u2014table 2\n\n###### Simulation parameters (Figure 3\u2014figure supplement 1).\n\nParameter| Symbol| Default value  \n---|---|---  \nNumber of neurons| 1000  \nNeuronal gain| 5  \nRange of excitatory interactions (in units of stimulus space length)| 0.02  \nStrength of inhibitory weights| 0.2  \nStrength of excitatory weights| 1  \nTimescale of neuronal integration (s)| 0.01  \nAmplitude of external inputs| 1  \nDuration of stimuli (s)| 0.4  \nWidth of box stimulus (in units of stimulus space length)| 0.05  \n  \nAppendix 1\u2014table 3\n\n###### Simulation parameters (Figure 9 and Figure 9\u2014figure supplement 1).\n\nOther parameters as in Table 1Appendix 1\u2014table 1.\n\nParameter| Symbol| Default value  \n---|---|---  \nTimescale of neuronal adaptation in PPC (s)| 7.5 (DYS), 10 (NT), 15 (ASD)  \nAmplitude of adaptation in PPC| 0.2  \nDelay interval (s)| [2, 6, 10]  \nIntertrial interval (s)| 2.2  \n  \n  1. ASD, autistic spectrum; DYS, dyslexic; NT, neurotypical; PPC, posterior parietal cortex.\n\n## Data availability\n\nThe code used to simulate the network model and analyze the results can be\nfound at https://github.com/vboboeva/ParametricWorkingMemory (copy archived at\nBoboeva, 2023). Data from the human behavioral task and code used to analyze\nit can be found at https://github.com/vboboeva/ParametricWorkingMemory_Data\n(copy archived on Zenodo: https://zenodo.org/records/10592611).\n\nThe following data sets were generated\n\n  1.     1. Boboeva V\n    2. Pezzotta A\n    3. Clopath C\n    4. Akrami A\n\n(2023) Zenodo\n\nParametric Working Memory in humans.\n\nhttps://doi.org/10.5281/zenodo.10592611\n\n## References\n\n  1.     1. Akrami A\n    2. Kopec CD\n    3. Diamond ME\n    4. Brody CD\n\n(2018) Posterior parietal cortex represents sensory history and mediates its\neffects on behaviour\n\nNature 554:368\u2013372.\n\nhttps://doi.org/10.1038/nature25510\n\n     * PubMed\n     * Google Scholar\n\n  2.     1. Alais D\n    2. Kong G\n    3. Palmer C\n    4. Clifford C\n\n(2018) Eye gaze direction shows a positive serial dependency\n\nJournal of Vision 18:11.\n\nhttps://doi.org/10.1167/18.4.11\n\n     * PubMed\n     * Google Scholar\n\n  3.     1. Algom D\n\n(1992) Memory psychophysics: An examination of its perceptual and cognitive\nprospects\n\nAdvances in Psychology 92:441\u2013513.\n\nhttps://doi.org/10.1016/S0166-4115(08)61784-7\n\n     * Google Scholar\n\n  4. Book\n\n    1. Amit DJ\n\n(1992) Modeling Brain Function: The World of Attractor Neural Networks\n\nCambridge University Press.\n\nhttps://doi.org/10.1017/CBO9780511623257\n\n     * Google Scholar\n\n  5.     1. Ashourian P\n    2. Loewenstein Y\n\n(2011) Bayesian inference underlies the contraction bias in delayed comparison\ntasks\n\nPLOS ONE 6:e19551.\n\nhttps://doi.org/10.1371/journal.pone.0019551\n\n     * PubMed\n     * Google Scholar\n\n  6.     1. Barak O\n    2. Tsodyks M\n\n(2007) Persistent activity in neural networks with dynamic synapses\n\nPLOS Computational Biology 3:e35.\n\nhttps://doi.org/10.1371/journal.pcbi.0030035\n\n     * PubMed\n     * Google Scholar\n\n  7.     1. Barak O\n    2. Tsodyks M\n    3. Romo R\n\n(2010) Neuronal population coding of parametric working memory\n\nThe Journal of Neuroscience 30:9424\u20139430.\n\nhttps://doi.org/10.1523/JNEUROSCI.1875-10.2010\n\n     * PubMed\n     * Google Scholar\n\n  8.     1. Barak O\n    2. Sussillo D\n    3. Romo R\n    4. Tsodyks M\n    5. Abbott LF\n\n(2013) From fixed points to chaos: three models of delayed discrimination\n\nProgress in Neurobiology 103:214\u2013222.\n\nhttps://doi.org/10.1016/j.pneurobio.2013.02.002\n\n     * PubMed\n     * Google Scholar\n\n  9.     1. Barak O\n    2. Tsodyks M\n\n(2014) Working models of working memory\n\nCurrent Opinion in Neurobiology 25:20\u201324.\n\nhttps://doi.org/10.1016/j.conb.2013.10.008\n\n     * PubMed\n     * Google Scholar\n\n  10.     1. Barbosa J\n    2. Compte A\n\n(2020) Build-up of serial dependence in color working memory\n\nScientific Reports 10:10959.\n\nhttps://doi.org/10.1038/s41598-020-67861-2\n\n     * PubMed\n     * Google Scholar\n\n  11.     1. Barbosa J\n    2. Stein H\n    3. Martinez RL\n    4. Galan-Gadea A\n    5. Li S\n    6. Dalmau J\n    7. Adam KCS\n    8. Valls-Sol\u00e9 J\n    9. Constantinidis C\n    10. Compte A\n\n(2020) Interplay between persistent activity and activity-silent dynamics in\nthe prefrontal cortex underlies serial biases in working memory\n\nNature Neuroscience 23:1016\u20131024.\n\nhttps://doi.org/10.1038/s41593-020-0644-4\n\n     * PubMed\n     * Google Scholar\n\n  12.     1. Battaglia FP\n    2. Treves A\n\n(1998) Stable and rapid recurrent processing in realistic autoassociative\nmemories\n\nNeural Computation 10:431\u2013450.\n\nhttps://doi.org/10.1162/089976698300017827\n\n     * PubMed\n     * Google Scholar\n\n  13.     1. Berliner JE\n    2. Durlach NI\n    3. Braida LD\n\n(1977) Intensity perception: VII further data on roving-level discrimination\nand the resolution and bias edge effects\n\nThe Journal of the Acoustical Society of America 61:1577\u20131585.\n\nhttps://doi.org/10.1121/1.381471\n\n     * PubMed\n     * Google Scholar\n\n  14. Software\n\n    1. Boboeva V\n\n(2023) Parametric working memory, version\nswh:1:rev:5e23c73f3170af454efdee160d65f31e1b701594\n\nSoftware Heritage.\n\nhttps://archive.softwareheritage.org/swh:1:dir:25663d0731deb224f16a06b6f88c8a75cef41fa0;origin=https://github.com/vboboeva/ParametricWorkingMemory;visit=swh:1:snp:9cad8946e37bf36aab96cad4364a3cf19e905b2a;anchor=swh:1:rev:5e23c73f3170af454efdee160d65f31e1b701594\n\n  15.     1. Boi M\n    2. O\u011fmen H\n    3. Herzog MH\n\n(2011) Motion and tilt aftereffects occur largely in retinal, not in object,\ncoordinates in the Ternus-Pikler display\n\nJournal of Vision 11:7.\n\nhttps://doi.org/10.1167/11.3.7\n\n     * PubMed\n     * Google Scholar\n\n  16.     1. Cicchini GM\n    2. Mikellidou K\n    3. Burr D\n\n(2017) Serial dependencies act directly on perception\n\nJournal of Vision 17:6.\n\nhttps://doi.org/10.1167/17.14.6\n\n     * PubMed\n     * Google Scholar\n\n  17.     1. Czoschke S\n    2. Fischer C\n    3. Beitner J\n    4. Kaiser J\n    5. Bledowski C\n\n(2019) Two types of serial dependence in visual working memory\n\nBritish Journal of Psychology 110:256\u2013267.\n\nhttps://doi.org/10.1111/bjop.12349\n\n     * PubMed\n     * Google Scholar\n\n  18.     1. Dalgleish HW\n    2. Russell LE\n    3. Packer AM\n    4. Roth A\n    5. Gauld OM\n    6. Greenstreet F\n    7. Thompson EJ\n    8. H\u00e4usser M\n\n(2020) How many neurons are sufficient for perception of cortical activity?\n\neLife 9:e58889.\n\nhttps://doi.org/10.7554/eLife.58889\n\n     * PubMed\n     * Google Scholar\n\n  19. Preprint\n\n    1. Ding X\n    2. Froudist-Walsh S\n    3. Jaramillo J\n    4. Jiang J\n    5. Wang X-J\n\n(2022) Predicting distributed working memory activity in a large-scale mouse\nbrain: the importance of the cell type-specific connectome\n\nbioRxiv.\n\nhttps://doi.org/10.1101/2022.12.05.519094\n\n     * Google Scholar\n\n  20.     1. Esmaeili V\n    2. Diamond ME\n\n(2019) Neuronal correlates of tactile working memory in prefrontal and\nvibrissal somatosensory cortex\n\nCell Reports 27:3167\u20133181.\n\nhttps://doi.org/10.1016/j.celrep.2019.05.034\n\n     * PubMed\n     * Google Scholar\n\n  21.     1. Fassihi A\n    2. Akrami A\n    3. Esmaeili V\n    4. Diamond ME\n\n(2014) Tactile perception and working memory in rats and humans\n\nPNAS 111:2331\u20132336.\n\nhttps://doi.org/10.1073/pnas.1315171111\n\n     * Google Scholar\n\n  22.     1. Fassihi A\n    2. Akrami A\n    3. Pulecchi F\n    4. Sch\u00f6nfelder V\n    5. Diamond ME\n\n(2017) Transformation of perception from sensory to motor cortex\n\nCurrent Biology 27:1585\u20131596.\n\nhttps://doi.org/10.1016/j.cub.2017.05.011\n\n     * PubMed\n     * Google Scholar\n\n  23.     1. Fischer J\n    2. Whitney D\n\n(2014) Serial dependence in visual perception\n\nNature Neuroscience 17:738\u2013743.\n\nhttps://doi.org/10.1038/nn.3689\n\n     * PubMed\n     * Google Scholar\n\n  24.     1. Fritsche M\n    2. Mostert P\n    3. de Lange FP\n\n(2017) Opposite effects of recent history on perception and decision\n\nCurrent Biology 27:590\u2013595.\n\nhttps://doi.org/10.1016/j.cub.2017.01.006\n\n     * PubMed\n     * Google Scholar\n\n  25.     1. Fritsche M\n    2. Spaak E\n\n(2020) A Bayesian and efficient observer model explains concurrent attractive\nand repulsive history biases in visual perception\n\neLife 9:e55389.\n\nhttps://doi.org/10.7554/eLife.55389\n\n     * PubMed\n     * Google Scholar\n\n  26.     1. Funahashi S\n    2. Bruce CJ\n    3. Goldman-Rakic PS\n\n(1989) Mnemonic coding of visual space in the monkey\u2019s dorsolateral prefrontal\ncortex\n\nJournal of Neurophysiology 61:331\u2013349.\n\nhttps://doi.org/10.1152/jn.1989.61.2.331\n\n     * Google Scholar\n\n  27.     1. Funahashi S\n    2. Bruce CJ\n    3. Goldman-Rakic PS\n\n(1990) Visuospatial coding in primate prefrontal neurons revealed by\noculomotor paradigms\n\nJournal of Neurophysiology 63:814\u2013831.\n\nhttps://doi.org/10.1152/jn.1990.63.4.814\n\n     * PubMed\n     * Google Scholar\n\n  28.     1. Fung CA\n    2. Wong KM\n    3. Wu S\n\n(2008) Dynamics of neural networks with continuous attractors\n\nEPL 84:18002.\n\nhttps://doi.org/10.1209/0295-5075/84/18002\n\n     * Google Scholar\n\n  29.     1. Fung CA\n    2. Wong KM\n    3. Wu S\n\n(2010) A moving bump in a continuous manifold: a comprehensive study of the\ntracking dynamics of continuous attractor neural networks\n\nNeural Computation 22:752\u2013792.\n\nhttps://doi.org/10.1162/neco.2009.07-08-824\n\n     * Google Scholar\n\n  30.     1. Fuster JM\n    2. Alexander GE\n\n(1971) Neuron activity related to short-term memory\n\nScience 173:652\u2013654.\n\nhttps://doi.org/10.1126/science.173.3997.652\n\n     * PubMed\n     * Google Scholar\n\n  31.     1. Gao R\n    2. van den Brink RL\n    3. Pfeffer T\n    4. Voytek B\n\n(2020) Neuronal timescales are functionally dynamic and shaped by cortical\nmicroarchitecture\n\neLife 9:e61277.\n\nhttps://doi.org/10.7554/eLife.61277\n\n     * PubMed\n     * Google Scholar\n\n  32.     1. Hachen I\n    2. Reinartz S\n    3. Brasselet R\n    4. Stroligo A\n    5. Diamond ME\n\n(2021) Dynamics of history-dependent perceptual judgment\n\nNature Communications 12:6036.\n\nhttps://doi.org/10.1038/s41467-021-26104-2\n\n     * PubMed\n     * Google Scholar\n\n  33.     1. Hellstr\u00f6m \u00c5\n\n(1985) The time-order error and its relatives: Mirrors of cognitive processes\nin comparing\n\nPsychological Bulletin 97:35\u201361.\n\nhttps://doi.org/10.1037//0033-2909.97.1.35\n\n     * Google Scholar\n\n  34.     1. Hern\u00e1ndez A\n    2. Salinas E\n    3. Garc\u0131\u0301a R\n    4. Romo R\n\n(1997) Discrimination in the sense of flutter: new psychophysical measurements\nin monkeys\n\nThe Journal of Neuroscience 17:6391\u20136400.\n\nhttps://doi.org/10.1523/JNEUROSCI.17-16-06391.1997\n\n     * Google Scholar\n\n  35.     1. Hollingworth HL\n\n(1910) The central tendency of judgment\n\nThe Journal of Philosophy, Psychology and Scientific Methods 7:461.\n\nhttps://doi.org/10.2307/2012819\n\n     * Google Scholar\n\n  36.     1. Hopfield JJ\n\n(1982) Neural networks and physical systems with emergent collective\ncomputational abilities\n\nPNAS 79:2554\u20132558.\n\nhttps://doi.org/10.1073/pnas.79.8.2554\n\n     * Google Scholar\n\n  37.     1. Jaffe-Dax S\n    2. Frenkel O\n    3. Ahissar M\n\n(2017) Dyslexics\u2019 faster decay of implicit memory for sounds and words is\nmanifested in their shorter neural adaptation\n\neLife 6:e20557.\n\nhttps://doi.org/10.7554/eLife.20557\n\n     * PubMed\n     * Google Scholar\n\n  38.     1. Jaffe-Dax S\n    2. Kimel E\n    3. Ahissar M\n\n(2018) Shorter cortical adaptation in dyslexia is broadly distributed in the\nsuperior temporal lobe and includes the primary auditory cortex\n\neLife 7:e30018.\n\nhttps://doi.org/10.7554/eLife.30018\n\n     * PubMed\n     * Google Scholar\n\n  39.     1. Jazayeri M\n    2. Shadlen MN\n\n(2010) Temporal context calibrates interval timing\n\nNature Neuroscience 13:1020\u20131026.\n\nhttps://doi.org/10.1038/nn.2590\n\n     * PubMed\n     * Google Scholar\n\n  40.     1. Jou J\n    2. Leka GE\n    3. Rogers DM\n    4. Matus YE\n\n(2004)\n\nContraction bias in memorial quantifying judgment: does it come from a stable\ncompressed memory representation or a dynamic adaptation process?\n\nThe American Journal of Psychology 117:543\u2013564.\n\n     * PubMed\n     * Google Scholar\n\n  41.     1. Karim M\n    2. Harris JA\n    3. Langdon A\n    4. Breakspear M\n\n(2013) The influence of prior experience and expected timing on vibrotactile\ndiscrimination\n\nFrontiers in Neuroscience 7:255.\n\nhttps://doi.org/10.3389/fnins.2013.00255\n\n     * PubMed\n     * Google Scholar\n\n  42.     1. Kerst SM\n    2. Howard JH\n\n(1978) Memory psychophysics for visual area and length\n\nMemory & Cognition 6:327\u2013335.\n\nhttps://doi.org/10.3758/BF03197463\n\n     * Google Scholar\n\n  43.     1. Kiyonaga A\n    2. Scimeca JM\n    3. Bliss DP\n    4. Whitney D\n\n(2017) Serial dependence across perception, attention, and memory\n\nTrends in Cognitive Sciences 21:493\u2013497.\n\nhttps://doi.org/10.1016/j.tics.2017.04.011\n\n     * Google Scholar\n\n  44.     1. Knapen T\n    2. Rolfs M\n    3. Wexler M\n    4. Cavanagh P\n\n(2010) The reference frame of the tilt aftereffect\n\nJournal of Vision 10:8.\n\nhttps://doi.org/10.1167/10.1.8\n\n     * PubMed\n     * Google Scholar\n\n  45.     1. Li L\n    2. Chan A\n    3. Iqbal SM\n    4. Goldreich D\n\n(2017) An adaptation-induced repulsion illusion in tactile spatial perception\n\nFrontiers in Human Neuroscience 11:331.\n\nhttps://doi.org/10.3389/fnhum.2017.00331\n\n     * PubMed\n     * Google Scholar\n\n  46.     1. Lieder I\n    2. Adam V\n    3. Frenkel O\n    4. Jaffe-Dax S\n    5. Sahani M\n    6. Ahissar M\n\n(2019) Perceptual bias reveals slow-updating in autism and fast-forgetting in\ndyslexia\n\nNature Neuroscience 22:256\u2013264.\n\nhttps://doi.org/10.1038/s41593-018-0308-9\n\n     * PubMed\n     * Google Scholar\n\n  47.     1. Loewenstein Y\n    2. Raviv O\n    3. Ahissar M\n\n(2021) Dissecting the roles of supervised and unsupervised learning in\nperceptual discrimination judgments\n\nThe Journal of Neuroscience 41:757\u2013765.\n\nhttps://doi.org/10.1523/JNEUROSCI.0757-20.2020\n\n     * PubMed\n     * Google Scholar\n\n  48.     1. Machens CK\n    2. Romo R\n    3. Brody CD\n\n(2005) Flexible control of mutual inhibition: a neural model of two-interval\ndiscrimination\n\nScience 307:1121\u20131124.\n\nhttps://doi.org/10.1126/science.1104171\n\n     * PubMed\n     * Google Scholar\n\n  49.     1. Machens CK\n    2. Romo R\n    3. Brody CD\n\n(2010) Functional, but not anatomical, separation of 'what' and 'when' in\nprefrontal cortex\n\nThe Journal of Neuroscience 30:350\u2013360.\n\nhttps://doi.org/10.1523/JNEUROSCI.3276-09.2010\n\n     * PubMed\n     * Google Scholar\n\n  50.     1. Maes A\n    2. Barahona M\n    3. Clopath C\n\n(2023) Long- and short-term history effects in a spiking network model of\nstatistical learning\n\nScientific Reports 13:12939.\n\nhttps://doi.org/10.1038/s41598-023-39108-3\n\n     * PubMed\n     * Google Scholar\n\n  51.     1. Manassi M\n    2. Liberman A\n    3. Chaney W\n    4. Whitney D\n\n(2017) The perceived stability of scenes: serial dependence in ensemble\nrepresentations\n\nScientific Reports 7:1971.\n\nhttps://doi.org/10.1038/s41598-017-02201-5\n\n     * PubMed\n     * Google Scholar\n\n  52.     1. Manassi M\n    2. Liberman A\n    3. Kosovicheva A\n    4. Zhang K\n    5. Whitney D\n\n(2018) Serial dependence in position occurs at the time of perception\n\nPsychonomic Bulletin & Review 25:2245\u20132253.\n\nhttps://doi.org/10.3758/s13423-018-1454-5\n\n     * Google Scholar\n\n  53.     1. Math\u00f4t S\n    2. Theeuwes J\n\n(2013) A reinvestigation of the reference frame of the tilt-adaptation\naftereffect\n\nScientific Reports 3:1152.\n\nhttps://doi.org/10.1038/srep01152\n\n     * PubMed\n     * Google Scholar\n\n  54.     1. Mej\u00edas JF\n    2. Wang X-J\n\n(2022) Mechanisms of distributed working memory in a large-scale network of\nmacaque neocortex\n\neLife 11:e72136.\n\nhttps://doi.org/10.7554/eLife.72136\n\n     * PubMed\n     * Google Scholar\n\n  55.     1. Miller P\n    2. Brody CD\n    3. Romo R\n    4. Wang X-J\n\n(2003) A recurrent network model of somatosensory parametric working memory in\nthe prefrontal cortex\n\nCerebral Cortex 13:1208\u20131218.\n\nhttps://doi.org/10.1093/cercor/bhg101\n\n     * PubMed\n     * Google Scholar\n\n  56.     1. Miyashita Y\n    2. Chang HS\n\n(1988) Neuronal correlate of pictorial short-term memory in the primate\ntemporal cortexYasushi Miyashita\n\nNature 331:68\u201370.\n\nhttps://doi.org/10.1038/331068a0\n\n     * Google Scholar\n\n  57.     1. Mongillo G\n    2. Barak O\n    3. Tsodyks M\n\n(2008) Synaptic theory of working memory\n\nScience 319:1543\u20131546.\n\nhttps://doi.org/10.1126/science.1150769\n\n     * PubMed\n     * Google Scholar\n\n  58.     1. Murray JD\n    2. Bernacchia A\n    3. Freedman DJ\n    4. Romo R\n    5. Wallis JD\n    6. Cai X\n    7. Padoa-Schioppa C\n    8. Pasternak T\n    9. Seo H\n    10. Lee D\n    11. Wang X-J\n\n(2014) A hierarchy of intrinsic timescales across primate cortex\n\nNature Neuroscience 17:1661\u20131663.\n\nhttps://doi.org/10.1038/nn.3862\n\n     * PubMed\n     * Google Scholar\n\n  59.     1. Olkkonen M\n    2. McCarthy PF\n    3. Allred SR\n\n(2014) The central tendency bias in color perception: effects of internal and\nexternal noise\n\nJournal of Vision 14:5.\n\nhttps://doi.org/10.1167/14.11.5\n\n     * PubMed\n     * Google Scholar\n\n  60.     1. Olsen GM\n    2. Hovde K\n    3. Kondo H\n    4. Sakshaug T\n    5. S\u00f8mme HH\n    6. Whitlock JR\n    7. Witter MP\n\n(2019) Organization of posterior parietal-frontal connections in the rat\n\nFrontiers in Systems Neuroscience 13:38.\n\nhttps://doi.org/10.3389/fnsys.2019.00038\n\n     * PubMed\n     * Google Scholar\n\n  61.     1. Papadimitriou C\n    2. Ferdoash A\n    3. Snyder LH\n\n(2015) Ghosts in the machine: memory interference from the previous trial\n\nJournal of Neurophysiology 113:567\u2013577.\n\nhttps://doi.org/10.1152/jn.00402.2014\n\n     * PubMed\n     * Google Scholar\n\n  62. Book\n\n    1. Poulton EC\n    2. Poulton S\n\n(1989)\n\nBias in Quantifying Judgements\n\nTaylor & Francis.\n\n     * Google Scholar\n\n  63.     1. Preuschhof C\n    2. Schubert T\n    3. Villringer A\n    4. Heekeren HR\n\n(2010) Prior Information biases stimulus representations during vibrotactile\ndecision making\n\nJournal of Cognitive Neuroscience 22:875\u2013887.\n\nhttps://doi.org/10.1162/jocn.2009.21260\n\n     * PubMed\n     * Google Scholar\n\n  64.     1. Rahnev D\n    2. Denison RN\n\n(2018) Suboptimality in perceptual decision making\n\nThe Behavioral and Brain Sciences 41:e223.\n\nhttps://doi.org/10.1017/S0140525X18000936\n\n     * PubMed\n     * Google Scholar\n\n  65.     1. Raviv O\n    2. Ahissar M\n    3. Loewenstein Y\n\n(2012) How recent history affects perception: the normative approach and its\nheuristic approximation\n\nPLOS Computational Biology 8:e1002731.\n\nhttps://doi.org/10.1371/journal.pcbi.1002731\n\n     * PubMed\n     * Google Scholar\n\n  66.     1. Romani S\n    2. Tsodyks M\n\n(2015) Short-term plasticity based network model of place cells dynamics\n\nHippocampus 25:94\u2013105.\n\nhttps://doi.org/10.1002/hipo.22355\n\n     * PubMed\n     * Google Scholar\n\n  67.     1. Romo R\n    2. Brody CD\n    3. Hern\u00e1ndez A\n    4. Lemus L\n\n(1999) Neuronal correlates of parametric working memory in the prefrontal\ncortex\n\nNature 399:470\u2013473.\n\nhttps://doi.org/10.1038/20939\n\n     * PubMed\n     * Google Scholar\n\n  68.     1. Romo R\n    2. Salinas E\n\n(2003) Flutter discrimination: neural codes, perception, memory and decision\nmaking\n\nNature Reviews. Neuroscience 4:203\u2013218.\n\nhttps://doi.org/10.1038/nrn1058\n\n     * PubMed\n     * Google Scholar\n\n  69.     1. Salinas E\n    2. Hern\u00e1ndez A\n    3. Zainos A\n    4. Romo R\n\n(2000) Periodicity and firing rate as candidate neural codes for the frequency\nof vibrotactile stimuli\n\nThe Journal of Neuroscience 20:5503\u20135515.\n\nhttps://doi.org/10.1523/JNEUROSCI.20-14-05503.2000\n\n     * Google Scholar\n\n  70.     1. Salinas E\n\n(2011) Prior and prejudice\n\nNature Neuroscience 14:943\u2013945.\n\nhttps://doi.org/10.1038/nn.2883\n\n     * Google Scholar\n\n  71.     1. Sebastian Seung H\n\n(1998) Continuous attractors and oculomotor control\n\nNeural Networks 11:1253\u20131258.\n\nhttps://doi.org/10.1016/S0893-6080(98)00064-1\n\n     * Google Scholar\n\n  72.     1. Siegle JH\n    2. Jia X\n    3. Durand S\n    4. Gale S\n    5. Bennett C\n    6. Graddis N\n    7. Heller G\n    8. Ramirez TK\n    9. Choi H\n    10. Luviano JA\n    11. Groblewski PA\n    12. Ahmed R\n    13. Arkhipov A\n    14. Bernard A\n    15. Billeh YN\n    16. Brown D\n    17. Buice MA\n    18. Cain N\n    19. Caldejon S\n    20. Casal L\n    21. Cho A\n    22. Chvilicek M\n    23. Cox TC\n    24. Dai K\n    25. Denman DJ\n    26. de Vries SEJ\n    27. Dietzman R\n    28. Esposito L\n    29. Farrell C\n    30. Feng D\n    31. Galbraith J\n    32. Garrett M\n    33. Gelfand EC\n    34. Hancock N\n    35. Harris JA\n    36. Howard R\n    37. Hu B\n    38. Hytnen R\n    39. Iyer R\n    40. Jessett E\n    41. Johnson K\n    42. Kato I\n    43. Kiggins J\n    44. Lambert S\n    45. Lecoq J\n    46. Ledochowitsch P\n    47. Lee JH\n    48. Leon A\n    49. Li Y\n    50. Liang E\n    51. Long F\n    52. Mace K\n    53. Melchior J\n    54. Millman D\n    55. Mollenkopf T\n    56. Nayan C\n    57. Ng L\n    58. Ngo K\n    59. Nguyen T\n    60. Nicovich PR\n    61. North K\n    62. Ocker GK\n    63. Ollerenshaw D\n    64. Oliver M\n    65. Pachitariu M\n    66. Perkins J\n    67. Reding M\n    68. Reid D\n    69. Robertson M\n    70. Ronellenfitch K\n    71. Seid S\n    72. Slaughterbeck C\n    73. Stoecklin M\n    74. Sullivan D\n    75. Sutton B\n    76. Swapp J\n    77. Thompson C\n    78. Turner K\n    79. Wakeman W\n    80. Whitesell JD\n    81. Williams D\n    82. Williford A\n    83. Young R\n    84. Zeng H\n    85. Naylor S\n    86. Phillips JW\n    87. Reid RC\n    88. Mihalas S\n    89. Olsen SR\n    90. Koch C\n\n(2021) Survey of spiking in the mouse visual system reveals functional\nhierarchy\n\nNature 592:86\u201392.\n\nhttps://doi.org/10.1038/s41586-020-03171-x\n\n     * PubMed\n     * Google Scholar\n\n  73.     1. Sinclair RJ\n    2. Burton H\n\n(1996) Discrimination of vibrotactile frequencies in a delayed pair comparison\ntask\n\nPerception & Psychophysics 58:680\u2013692.\n\nhttps://doi.org/10.3758/BF03213100\n\n     * Google Scholar\n\n  74.     1. Spalla D\n    2. Cornacchia IM\n    3. Treves A\n\n(2021) Continuous attractors for dynamic memories\n\neLife 10:e69499.\n\nhttps://doi.org/10.7554/eLife.69499\n\n     * PubMed\n     * Google Scholar\n\n  75. Preprint\n\n    1. Stern M\n    2. Istrate N\n    3. Mazzucato L\n\n(2021) A Reservoir of Timescales in Random Neural Network\n\nbioRxiv.\n\nhttps://doi.org/10.1101/2021.10.11.463861\n\n     * Google Scholar\n\n  76.     1. Su\u00e1rez-Pinilla M\n    2. Seth AK\n    3. Roseboom W\n\n(2018) Serial dependence in the perception of visual variance\n\nJournal of Vision 18:4.\n\nhttps://doi.org/10.1167/18.7.4\n\n     * PubMed\n     * Google Scholar\n\n  77.     1. Tong K\n    2. Dub\u00e9 C\n\n(2022) A tale of two literatures: a fidelity-based integration account of\ncentral tendency bias and serial dependency\n\nComputational Brain & Behavior 5:103\u2013123.\n\nhttps://doi.org/10.1007/s42113-021-00123-0\n\n     * Google Scholar\n\n  78. Book\n\n    1. Trappenberg TP\n\n(2005) Continuous Attractor neural networks\n\nIn: Trappenberg TP, editors. Recent Developments in Biologically Inspired\nComputing. Igi Global. pp. 398\u2013425.\n\nhttps://doi.org/10.4018/978-1-59140-312-8\n\n     * Google Scholar\n\n  79.     1. Wang X-J\n\n(2001) Synaptic reverberation underlying mnemonic persistent activity\n\nTrends in Neurosciences 24:455\u2013463.\n\nhttps://doi.org/10.1016/S0166-2236(00)01868-3\n\n     * Google Scholar\n\n  80. Preprint\n\n    1. Wang XJ\n    2. Jiang J\n    3. Pereira-Obilinovic U\n\n(2023) Bifurcation in space: emergence of function modularity in the neocortex\n\nbioRxiv.\n\nhttps://doi.org/10.1101/2023.06.04.543639\n\n     * Google Scholar\n\n  81.     1. Wu S\n    2. Amari S\n\n(2005) Computing with continuous attractors: stability and online aspects\n\nNeural Computation 17:2215\u20132239.\n\nhttps://doi.org/10.1162/0899766054615626\n\n     * PubMed\n     * Google Scholar\n\n  82.     1. Wu S\n    2. Wong KYM\n    3. Fung CCA\n    4. Mi Y\n    5. Zhang W\n\n(2016) Continuous attractor neural networks: candidate of a canonical model\nfor neural information representation\n\nF1000Research 5:156.\n\nhttps://doi.org/10.12688/f1000research.7387.1\n\n     * Google Scholar\n\n  83.     1. Zhang X\n    2. Yan W\n    3. Wang W\n    4. Fan H\n    5. Hou R\n    6. Chen Y\n    7. Chen Z\n    8. Ge C\n    9. Duan S\n    10. Compte A\n    11. Li CT\n\n(2019) Active information maintenance in working memory by a sensory cortex\n\neLife 8:e43191.\n\nhttps://doi.org/10.7554/eLife.43191\n\n     * PubMed\n     * Google Scholar\n\n  84.     1. Zhong W\n    2. Lu Z\n    3. Schwab DJ\n    4. Murugan A\n\n(2020) Nonequilibrium statistical mechanics of continuous attractors\n\nNeural Computation 32:1033\u20131068.\n\nhttps://doi.org/10.1162/neco_a_01280\n\n     * PubMed\n     * Google Scholar\n\n## Article and author information\n\n### Author details\n\n  1. #### Vezha Boboeva\n\n    1. Sainsbury Wellcome Centre, University College London, London, United Kingdom\n    2. Department of Bioengineering, Imperial College London, London, United Kingdom\n\n##### Contribution\n\nConceptualization, Data curation, Software, Formal analysis, Validation,\nInvestigation, Visualization, Methodology, Writing \u2013 original draft, Writing \u2013\nreview and editing\n\n##### Competing interests\n\nNo competing interests declared\n\n\"This ORCID iD identifies the author of this article:\" 0000-0002-2476-8714\n\n  2. #### Alberto Pezzotta\n\n    1. Gatsby Computational Neuroscience Unit, University College London, London, United Kingdom\n    2. The Francis Crick Institute, London, United Kingdom\n\n##### Contribution\n\nSoftware, Formal analysis, Methodology, Writing \u2013 review and editing\n\n##### Competing interests\n\nNo competing interests declared\n\n  3. #### Claudia Clopath\n\n    1. Sainsbury Wellcome Centre, University College London, London, United Kingdom\n    2. Department of Bioengineering, Imperial College London, London, United Kingdom\n\n##### Contribution\n\nConceptualization, Supervision, Funding acquisition, Investigation, Writing \u2013\noriginal draft, Project administration, Writing \u2013 review and editing\n\n##### Contributed equally with\n\nAthena Akrami\n\n##### For correspondence\n\nc.clopath@imperial.ac.uk\n\n##### Competing interests\n\nNo competing interests declared\n\n\"This ORCID iD identifies the author of this article:\" 0000-0003-4507-8648\n\n  4. #### Athena Akrami\n\nSainsbury Wellcome Centre, University College London, London, United Kingdom\n\n##### Contribution\n\nConceptualization, Resources, Supervision, Funding acquisition, Investigation,\nWriting \u2013 original draft, Project administration, Writing \u2013 review and editing\n\n##### Contributed equally with\n\nClaudia Clopath\n\n##### For correspondence\n\nathena.akrami@ucl.ac.uk\n\n##### Competing interests\n\nNo competing interests declared\n\n\"This ORCID iD identifies the author of this article:\" 0000-0001-5711-0903\n\n### Funding\n\n#### Wellcome Trust (219627/Z/19/Z)\n\n  * Athena Akrami\n\n#### Gatsby Charitable Foundation (GAT3755)\n\n  * Athena Akrami\n\n#### Biotechnology and Biological Sciences Research Council (BB/N013956/1)\n\n  * Claudia Clopath\n\n#### Biotechnology and Biological Sciences Research Council (BB/N019008/1)\n\n  * Claudia Clopath\n\n#### Wellcome Trust\n\nhttps://doi.org/10.35802/200790\n\n  * Claudia Clopath\n\n#### Simons Foundation (564408)\n\n  * Claudia Clopath\n\n#### Engineering and Physical Sciences Research Council (EP/R035806/1)\n\n  * Claudia Clopath\n\nThe funders had no role in study design, data collection and interpretation,\nor the decision to submit the work for publication. For the purpose of Open\nAccess, the authors have applied a CC BY public copyright license to any\nAuthor Accepted Manuscript version arising from this submission.\n\n### Acknowledgements\n\nWe are grateful to Loreen Hert\u00e4g for helpful comments on our figures and Arash\nFassihi for helpful discussions. We also thank Guilhem Ibos for pointing out a\ntypo in our figure legends in a previous version of this manuscript. This work\nwas supported by BBSRC (BB/N013956/1, BB/N019008/1), Wellcome Trust\n(200790/Z/16/Z), Simons Foundation (564408), EPSRC (EP/R035806/1), Gatsby\nCharitable Foundation (GAT3755), and Wellcome Trust (219627/Z/19/Z).\n\n### Ethics\n\nThe study was approved by the University College London (UCL) Research Ethics\nCommittee [16159/001] (London, UK). Before starting the experiment,\nparticipants were provided with an information sheet relevant to the\nexperiment they will be performing and asked to sign an informed written\nconsent. By signing this consent form, the participants consent to freely take\npart in the study and confirm they understand the information they received.\nThe participants additionally confirm they understand that their participation\nis voluntary and that they are allowed to stop the experiment at any moment\nand withdraw the data they provided. Participants consent to allow use of\ntheir personal data only for the purpose of scientific research, as determined\nby applicable law. Identifiable personal data are only available to the\nresearchers and securely stored upon need, while the anonymized version of\nthis data is freely available in a public repository. Once published, the\nparticipants' contribution will remain non-identifiable.\n\n### Version history\n\n  1. Preprint posted: January 27, 2023 (view preprint)\n  2. Sent for peer review: March 9, 2023\n  3. Preprint posted: August 2, 2023 (view preprint)\n  4. Preprint posted: January 5, 2024 (view preprint)\n  5. Version of Record published: April 24, 2024 (version 1)\n\n### Cite all versions\n\nYou can cite all versions using the DOI https://doi.org/10.7554/eLife.86725.\nThis DOI represents all versions, and will always resolve to the latest one.\n\n### Copyright\n\n\u00a9 2023, Boboeva et al.\n\nThis article is distributed under the terms of the Creative Commons\nAttribution License, which permits unrestricted use and redistribution\nprovided that the original author and source are credited.\n\n## Metrics\n\n  *     1,248\nviews\n\n  *     98\ndownloads\n\n  *     1\ncitations\n\nViews, downloads and citations are aggregated across all versions of this\npaper published by eLife.\n\n## Download links\n\n### Share this article\n\nhttps://doi.org/10.7554/eLife.86725\n\n#### Categories and tags\n\n  * Research Article\n  * Neuroscience\n  * working memory\n  * serial bias\n  * neural network\n  * rat\n  * posterior parietal cortex\n  * contraction bias\n\n#### Research organism\n\n  * Human\n\n### Further reading\n\n  1.     1. Neuroscience\n\n# Cholecystokinin facilitates motor skill learning by modulating\nneuroplasticity in the motor cortex\n\nHao Li, Jingyu Feng ... Jufang He\n\nResearch Article May 3, 2024\n\nCholecystokinin (CCK) is an essential modulator for neuroplasticity in sensory\nand emotional domains. Here, we investigated the role of CCK in motor learning\nusing a single pellet reaching task in mice. Mice with a knockout of Cck gene\n(Cck^\u2212/\u2212) or blockade of CCK-B receptor (CCKBR) showed defective motor\nlearning ability; the success rate of retrieving reward remained at the\nbaseline level compared to the wildtype mice with significantly increased\nsuccess rate. We observed no long-term potentiation upon high-frequency\nstimulation in the motor cortex of Cck^\u2212/\u2212 mice, indicating a possible\nassociation between motor learning deficiency and neuroplasticity in the motor\ncortex. In vivo calcium imaging demonstrated that the deficiency of CCK\nsignaling disrupted the refinement of population neuronal activity in the\nmotor cortex during motor skill training. Anatomical tracing revealed direct\nprojections from CCK-expressing neurons in the rhinal cortex to the motor\ncortex. Inactivation of the CCK neurons in the rhinal cortex that project to\nthe motor cortex bilaterally using chemogenetic methods significantly\nsuppressed motor learning, and intraperitoneal application of CCK4, a\ntetrapeptide CCK agonist, rescued the motor learning deficits of Cck^\u2212/\u2212 mice.\nIn summary, our results suggest that CCK, which could be provided from the\nrhinal cortex, may surpport motor skill learning by modulating neuroplasticity\nin the motor cortex.\n\n  2.     1. Neuroscience\n\n# A dynamic neural resource model bridges sensory and working memory\n\nIvan Tomi\u0107, Paul M Bays\n\nResearch Article May 3, 2024\n\nProbing memory of a complex visual image within a few hundred milliseconds\nafter its disappearance reveals significantly greater fidelity of recall than\nif the probe is delayed by as little as a second. Classically interpreted, the\nformer taps into a detailed but rapidly decaying visual sensory or \u2018iconic\u2019\nmemory (IM), while the latter relies on capacity-limited but comparatively\nstable visual working memory (VWM). While iconic decay and VWM capacity have\nbeen extensively studied independently, currently no single framework\nquantitatively accounts for the dynamics of memory fidelity over these time\nscales. Here, we extend a stationary neural population model of VWM with a\ntemporal dimension, incorporating rapid sensory-driven accumulation of\nactivity encoding each visual feature in memory, and a slower accumulation of\ninternal error that causes memorized features to randomly drift over time.\nInstead of facilitating read-out from an independent sensory store, an early\ncue benefits recall by lifting the effective limit on VWM signal strength\nimposed when multiple items compete for representation, allowing memory for\nthe cued item to be supplemented with information from the decaying sensory\ntrace. Empirical measurements of human recall dynamics validate these\npredictions while excluding alternative model architectures. A key conclusion\nis that differences in capacity classically thought to distinguish IM and VWM\nare in fact contingent upon a single resource-limited WM store.\n\n  3.     1. Neuroscience\n\n# Cognitive Neuroscience: Memorable first impressions\n\nEmilio Salinas, Bashirul I Sheikh\n\nInsight May 3, 2024\n\nOur ability to recall details from a remembered image depends on a single\nmechanism that is engaged from the very moment the image disappears from view.\n\n## Be the first to read new articles from eLife\n\nSign up for email alerts\n\nPrivacy notice\n\nFind us on GitHub\n\neLife is a non-profit organisation inspired by research funders and led by\nscientists. Our mission is to help scientists accelerate discovery by\noperating a platform for research communication that encourages and recognises\nthe most responsible behaviours in science. eLife Sciences Publications, Ltd\nis a limited liability non-profit non-stock corporation incorporated in the\nState of Delaware, USA, with company number 5030732, and is registered in the\nUK with company number FC030576 and branch number BR015634 at the address:\neLife Sciences Publications, Ltd Westbrook Centre, Milton Road Cambridge CB4\n1YG UK\n\n\u00a9 2024 eLife Sciences Publications Ltd. Subject to a Creative Commons\nAttribution license, except where otherwise noted. ISSN: 2050-084X\n\n", "frontpage": false}
