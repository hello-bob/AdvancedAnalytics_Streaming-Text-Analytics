{"aid": "40285062", "title": "Encoding Bitwise Functions as Polynomials", "url": "https://www.leviathansecurity.com/blog/encoding-bitwise-functions-as-polynomials", "domain": "leviathansecurity.com", "votes": 1, "user": "peter_d_sherman", "posted_at": "2024-05-07 13:14:59", "comments": 0, "source_title": "Encoding", "source_text": "Encoding Bitwise Functions as Polynomials \u2014 Leviathan Security Group -\nPenetration Testing, Security Assessment, Risk Advisory\n\n0\n\nSkip to Content\n\nContact Us\n\nContact Us\n\n#\n\nEncoding\n\nBitwise\n\nFunctions\n\nas\n\nPolynomials\n\nApplication Security\n\nMay 2\n\nWritten By Cody Martin\n\nIn this post, we will use polynomials to represent Boolean functions, see how\nthey can be solved using generic math, analyze the properties of the\nrepresentation, and use them to construct an arithmetic logic unit (ALU).\nFollowing these steps, we will create a symbolic solver using algebra.\n\n## Introduction\n\nSymbolic solvers and theorem provers are powerful tools that allow the user to\nprove the satisfiability of a collection of statements and can even provide\nconcrete values that satisfy them. Common applications of solvers are solving\na function for its inputs given its outputs or proving specific properties.\n\nInterestingly, while building this solver, I realized that I had technically\nbuilt an automated theorem prover (ATP). As I was exploring the Wikipedia\nrabbit hole for ATPs, I came across Zhegalkin polynomials^[1], and realized\nthat I had rediscovered them.\n\n## Polynomials\n\n### Definition\n\nPolynomials are expressions consisting of variables and coefficients that\ninvolve only the operations of addition, subtraction, multiplication, and\npositive integer exponentiation of variables^[2]. You may remember them from\nmath class. A few examples are or . Note that is not a polynomial as is a\nvariable and not a positive integer. Each part of the polynomial separated by\nor is called a term. A term can be further broken down into the coefficient,\nvariable (also known as the indeterminate, which I will use interchangeably),\nand degree. For example, in , is the coefficient, is the variable, and is the\ndegree.\n\nAddition is performed by lining up polynomials by the degree of each term and\nthen adding their coefficients. Subtraction works in the same way.\nMultiplication works by multiplying every term of the first polynomial with\nevery term of the second.\n\n### Interpretation as structures\n\nA polynomial can be interpreted as the set of all numbers that satisfy its\nstructure. For example, can be interpreted as all even numbers if we restrict\nthe inputs to integers. This can be seen if we evaluate at different integers:\n. can then be interpreted as all odd numbers.\n\nImagine we have two polynomials, and . Adding them together results in . If we\nevaluate all three polynomials at let's say , we get , , and . You may notice\nthat adding together the results of the first two polynomials, and , is\nequivalent to the result of the third polynomial, . This is no coincidence.\nWhen we added the polynomials together, we added their structures together to\nform the third polynomial.\n\n### Composition\n\nAt this point, you may be thinking, \"Hey, can we evaluate the polynomial's\nindeterminate at another polynomial?\" And the answer is yes! Let's take\nanother look at . If we evaluate it at , we get . This is known as composition\nand is quite powerful. In our framework, it's akin to evaluating a function\nwith the symbolic result of another function.\n\nWe can also use this to create relations between variables in multivariate\npolynomials. A polynomial in one variable is called a univariate polynomial. A\npolynomial in more than one variable is called a multivariate polynomial. For\nexample, is a multivariate polynomial. Imagine we have the multivariate\npolynomial . If we want to set the relation , we can evaluate for to see the\nresult. This would evaluate to .\n\n### Solving with polynomials\n\nImagine we have a complicated function defined as follows:\n\n    \n    \n    def f(a): a -= 53 for i in range(1, 17): if not i % 7: a *= i*a elif i == 5: a += 20 else: a += i+1 a -= 5 return a - 107\n\nAt which integer value of a does ? This would probably be quite difficult to\ndo by hand. However, it's fairly simple if we use polynomials. Using a\ncomputer algebra system (CAS), we can plug in a variable into , and get the\npolynomial result.\n\n    \n    \n    \u2514\u2500$ poly = f(x) <Polynomial: (686)*x^4 + (-115248)*x^3 + (7267288)*x^2 + (-203857248)*x + 2146394847, coeff_ring=ZZ>\n\nNow we just have to get the CAS to solve for , giving us the solutions and .\nInternally, this works by using complicated algebraic algorithms for the\ncommon problem of root finding. The roots of a polynomial are the values for\nwhich it evaluates to zero. However, you may notice that we're not solving for\nzero, but for . This is easily remedied by subtracting from both sides, giving\nus\n\nUsing this method, we can solve certain types of functions for their inputs\ngiven the outputs. Specifically, the functions must only perform operations\nthat can be represented by a polynomial. This hints to us that polynomials may\nbe a viable option for representing and solving functions in general.\n\n## Mathematically representing bitwise functions\n\n### Binary operations over bits\n\nFirst, we must choose a suitable mathematical representation for bits and\ntheir operations. Recall that bits are either or and have various operations\nsuch as , , and . To do this mathematically, I used abstract algebra to\nconstruct a field that could accurately re-create these operations. It's not\nnecessary to understand what that means, but I wanted to include that\nexplanation for the algebraically inclined.\n\nI represented a bit as an element of or \"Z mod 2\". Basically, this means we\nhave two elements and , and we can do normal arithmetic with them, but we must\ntake the result modulo after every operation. For our purposes, modulo is a\nfancy word for \u201cdivide and return only the remainder.\u201d For example, . Using\nthis simple rule, we can re-create every unary and binary bit operation. In\nfact, we only need and because every other bit operation can be constructed\nusing those two.\n\nHere are a few examples:\n\nName| Definition  \n---|---  \n  \n### Symbolic representation using polynomials\n\nSo now we can represent binary operations on concrete values of bits, but how\ncan we represent them symbolically? Well with polynomials, of course!\nSomething not generally discussed in high school math is that polynomials must\nbe defined over algebraic structures such as our field, . This is required\nbecause we need to know how to perform arithmetic with the coefficients and\nwhat values can be substituted for the variable.\n\nFrom here on, all polynomial arithmetic will take place in unless stated\notherwise. For example, if and , then . If we add to , we get the polynomial\nbecause . This should make sense as addition in is , and anything itself is .\nIn addition, when we evaluate a polynomial, it will result in an element of .\n\nThere's another subtle behavior in that might not be immediately apparent.\nLet's look at a degree two polynomial. We know that . But that's just\nredundant because . Thus, any degree greater than one can be reduced to one.\n\nSo far, we've only been working with one variable, . However, only represents\none bit. Let's call the second bit . In order to work with both variables, we\ncan create a multivariate polynomial in indeterminate whose coefficients are\npolynomials in .\n\n## Implementing the symbolic compiler\n\n### Symbolic bits\n\nWe now have enough background information to build the symbolic compiler \ud83c\udf89!\nFirst, we need to lay down some convention. Functions will take in symbits,\nwhich are just wrappers that translate bit operations into polynomial\noperations.\n\nName| Symbol  \n---|---  \n  \nImagine we're working with this function:\n\n    \n    \n    def f(a, b): c = a ^ b return (c | a) & (b @ ~a) & b\n\nThe symbolic compiler reads the function's signature, creates two symbits and\n, and then runs them through the function.\n\nThe result is:\n\n    \n    \n    <SymBit: value=<Polynomial: (a + 1)*b, coeff_ring=ZZ/(ZZ(2))[a]>>\n\nYou can see that the resulting symbit is a simplified version of the function.\nWhat it's saying is that is equivalent to . This simplification is purely a\nresult of our degree reduction and the fact that zero coefficients are\ndeleted.\n\nSince the underlying value is a polynomial, we can evaluate the symbit by\nspecifying the values to use for and . We denote evaluating the symbit for and\nas . We can also perform a partial evaluation of the symbit by not evaluating\nevery variable. For example, we can partially evaluate like . Partial\nevaluation is possible because .\n\n### Bit vectors\n\nLet's define another convenience wrapper called a bit vector. The bit vector\nis just a fixed size vector of symbits. Normal bit operations are possible by\nlining up the bit vectors and performing the operations on each pair of\nelements. Here's an example of with 4-bit bit vectors:\n\nWe can define left shifts and right shifts by shifting the vector whichever\ndirection the designated number of times and padding the opposite side with\nzeroes.\n\nWhen we define a function that uses bit vectors, we'll denote their type and\nsize in the function signature like so:\n\ndef f(a: BitVector[4], b: BitVector[4]): return a & b | (a >> 2)\n\nThe symbolic compiler reads the function signature to determine the name of\nthe bit vectors and their size. For the bit vector of size , the compiler will\ndefine the bit vector's symbits to be . Likewise, will be .\n\nThe result of compiling this function would be another bit vector:\n\nThe bit vector implementation allows us to either evaluate it bit-by-bit\n(e.g., ) or set an entire variable to an integer that it will automatically\ndecompose (e.g., ). Like symbits, bit vectors support partial evaluation and\ncomposition.\n\n## Solving symbolic expressions\n\n### Defining the problem\n\nSolving expressions is a key feature of symbolic solvers... obviously.\nEarlier, we looked at how univariate polynomials can be solved for desired\noutputs using root finding. However, our system is much more complicated. Not\nonly are we using multivariate polynomials with many variables, but we also\nhave several polynomials to solve! Conceptually, this sounds complicated, but\nperhaps we can write a simple algorithm that exploits the structure of the\npolynomials.\n\nWe should first go over how the computer algebra system represents\npolynomials. Let's say we have a polynomial over the integers . Internally,\nthat would be represented as a list, , where index zero is the degree zero\nterm, index one is the degree one term, etc. In our representation, anything\nabove degree one is redundant, so we'd only have a two-element list.\n\nHow does a multivariate polynomial such as get stored? is the variable of a\npolynomial with coefficients in , but is the variable of a polynomial with\ncoefficients in ; its coefficients are polynomials in . That means that looks\nlike\n\nWe know that our multivariate polynomials have a recursive structure\n(polynomials with polynomial coefficients), and that every polynomial in that\nstructure is maximally degree one. Let's generically represent these\npolynomials as , where is the degree one coefficient, is the indeterminate,\nand is the degree zero coefficient. We'll define one more variable, , to\nrepresent the desired output. Altogether, we'll represent any polynomial in\nthis system as .\n\n### Creating the algorithm\n\nNow that we have our representation, we should observe the effect of the\nvariable. Recall that addition in is . If is , then . Therefore, must equal .\nWhen , must not equal . We can use this information to reduce the number of\nstates we explore.\n\nWhat kind of rules we can come up with when we set to or ? Let's start with .\nThis means , so . Let , then . The only way that is if both and . If is also a\npolynomial, then we recursively apply the same idea to and solve it for\nbecause we know must evaluate to .\n\nWe continue recursing until we get to a polynomial with constant coefficients\n(that means they're not variables). For example, imagine we're solving . Here,\nis the coefficient of . Using our above rule of , we find that and solve its\npolynomial coefficient for . We now have the equation . Adding to both sides,\nwe get , so we return the solution .\n\nNow let and , then . This will be true if either or both and . Thus, there are\nthree possible configurations: , , and . If is a polynomial, then we also need\nto solve it for both and . This is a more complicated situation because the\nsolution can be any of these three configurations, and each of the\ncoefficients may have sub-solutions as well. We need to create some sort of\nstructured solution system to handle the layered complexity.\n\n### The constraint system\n\nTo help us with this task, we'll need a recursive solution system with\nstructural and behavioral rules. Let's define a few objects that will help us\nkeep track of the solution space.\n\nThe constraint system, , holds one or more constraints. All of these\nconstraints must be satisfied to be a valid solution. Constraint systems can\nbe seen as performing an between all of the constraints. In order to ensure a\nconsistent and simplified model, a constraint system cannot be a direct child\nof another constraint system, and the topmost object must be a constraint\nsystem. Note that the removal of these rules would allow infinite, different\nrepresentations of the same constraint system, for example, .\n\nThe equals constraint, , specifies that a variable must equal a concrete\nvalue, either or . We'll denote it as .\n\nThe one-of constraint, , specifies that every solution satisfies exactly one\nof its child constraint systems. We denote it as .\n\nThe any constraint, , specifies that a variable can be either or . It's a free\nvariable. We denote it as .\n\nNext, we'll define the interactions between the constraints. Constraint\nsystems act as an , so adding two of them together results in a constraint\nsystem with the union of their constraints.\n\nAdding two constraints together that act on different variables returns a\nconstraint system with both constraints. If they act on the same variable,\nthen they must agree. Otherwise, we throw a NoSolutionException to represent\nunsatisfiability (UNSAT). Adding an to an prunes any constraint systems that\nit makes UNSAT, and then it returns a constraint system with the and pruned .\nAn and an returns a constraint system with both if they act on different\nvariables, and just the if they act on the same variable.\n\nAdding two constraints creates a resultant by taking the product of their\nchild constraint systems and pruning the UNSAT ones. Adding an to an creates a\nconstraint system with both constraints if they act on different variables,\nand returns the unchanged if they act on the same variable.\n\nAdding two constraints creates a constraint system with both if they act on\ndifferent variables, and just one of them if they act on the same variable. An\nimportant property of is that it is equivalent to . Imagine adding of these\nconstraints with different variables. Since we must take the product of them,\nwe would get an with child constraint systems. Using constraints, we can\nrepresent this product with constraints.\n\nFinally, let's discuss solution generation. generates one solution for one\nvariable, . generates two possible solutions for a single variable, , .\ngenerates the product of all solutions generated by its child constraints.\nLastly, generates solutions from each of its child constraint systems and\nreturns those as its solutions.\n\nLet's take another look at . Recall that is the coefficient and is the\nindeterminate. Our constraint system would model this as . This is saying that\nthere are two solution systems that satisfy the equation:\n\n  1. Set to , and let be free\n  2. Let be free, and set to\n\nNote that while there are only two child constraints, together they generate\nthree distinct solutions: .\n\n## Analyzing the symbolic representation\n\nBefore we compile more complex functions, we should analyze the properties of\nour representation to get a good idea of its strengths and limitations.\n\n### Branchless\n\nOne limitation lies in the polynomial structure. Polynomials cannot represent\nall functions simply because their definition does not allow things such as .\nLet's take a closer look at what this means. is really multiplying by itself\ntimes.\n\n    \n    \n    def f(a: BitVector[4], b: BitVector[4]): c = 1 for _ in range(b): c *= a return c\n\nThe obvious problem is that is symbolic and not a concrete value. How do we\nknow when to stop? Maybe this is just a simple programming issue. Of course,\nwe can't give Python a symbolic value to loop over! Let's try it again but try\nto be really careful about how we use the bit vectors.\n\n    \n    \n    def f(a: BitVector[4], b: BitVector[4]): c = 1 d = 1 while True: d += 1 c *= a if b ^ d: # This is not concrete! break return c\n\nHere we define the truthiness of a bit vector to be False for all zeroes and\nTrue otherwise. However, we still don't know when to stop! It's true that\nduring evaluation, could be a concrete value, and this would be valid.\nUnfortunately, this just won't work if is not concrete at compile time. Why?\nBecause we can't branch on a symbolic value. Branching is simply not possible.\nThere are a lot of subtle ways branching happens. In general, deciding \"how\nmany\" or \"when to stop\" causes a branch. This means that symbolic values\ncannot be used as size information.\n\nThe following is a more subtle example of attempting to use a symbolic value\nas size information:\n\n    \n    \n    def f(a: BitVector[4], b: BitVector[4]): d = [a]*b # b is size information! c = 1 for val in d: c *= val return c\n\nA lot of this falls out of the fact that the function f is not being directly\ncompiled. The function is manipulating symbolic objects that record all state\nchanges. As a consequence of being branchless and the fact that we are\nevaluating a finite polynomial, evaluation always halts and thus our\nrepresentation is not Turing complete.\n\n### Timeless\n\nThis representation also has no runtime. In fact, there is no time. Think of\nthe compiled bit vector as every possible answer frozen the moment before\nobservation. When we evaluate it, it instantaneously gives us an answer. The\ncompilation process converts state changes over time into a singular,\nintertwined state. The compiled bit vector is a closed system, unable to\nreceive additional input symbols. All state that is and will be is already\nencoded. That's the reason I did not attempt to make RAM. RAM is for temporary\nstorage. That would only be needed if there were time, change, or other input.\n\n### Cardinality of bitwise functions\n\nWe are also able to determine the exact number of functions that can be\nrepresented with bits of input and bits of output. Recall that polynomials\nover are internally represented as a two-element list. Let's start with\nrepresenting a function using a single symbit. It has a list with indices and\nwhere and . Therefore, there are possible states for .\n\nHow many polynomials can there be with two symbits? Recall that multivariate\npolynomials are represented as a list of lists. Well, each of its indices can\ntake on different values, so . If there was a third symbit, it would be . This\ngrows at a rate of . However, there are also possible outputs, and we can\ncreate a function that maps any input to any output. The final result is that\nthere are or -to- bit functions. No matter how many steps we arbitrarily add\nto the function, it will simplify to one of these functions.\n\nThe number of bit functions is a well-known result^[3], and the fact that our\nrepresentation can encode exactly that many means that we can actually encode\nevery bit function.\n\n## Constructing an ALU\n\n### Introduction\n\nAn arithmetic logic unit (ALU) is the part of a CPU responsible for various\narithmetic operations. Generally, it performs addition, subtraction,\nmultiplication, division, left rotates, right rotates, etc.\n\nConceptually, building the ALU using the symbolic compiler is simple. We just\nneed to implement its hardware algorithms using bitwise functions. I will not\nbe going over implementing each function because the algorithms are well\nknown. However, a problem quickly arises from using the symbolic\nrepresentation: conditionals. Recall from the analysis that we are not allowed\nto branch. Is building the ALU even still possible? Well yes, we just have to\nencode the result of the conditional in a branchless way.\n\n### Branchless conditionals\n\nFirst, we create a function called , short for \"not zero transform.\" It takes\nin a bit vector, performs between all of its bits, and sets each bit to that\nvalue. If the bit vector is all zeroes, the output will be zeroes. Otherwise,\nit will be all ones.\n\n    \n    \n    def NZTRANS(a): \"\"\"Transforms non-zero bitvectors to ALL ones\"\"\" for i in range(a.SIZE.bit_length()-1): a |= a >> 2**i for i in range(a.SIZE.bit_length()-1): a |= a << 2**i return a\n\nNext, we define a function, , short for \"if not zero.\" It takes in a control\nbit vector and two \"statement\" bit vectors and . Recall that we've defined the\ntruthiness of a bit vector to be False if all zeroes and True otherwise. We\nfirst use on , making it either (all zeroes) or (all ones). We then it with ,\nits inverse with , and those results together.\n\n    \n    \n    def IFNZ(c, s1, s2): \"\"\"If `c` is non-zero, s1, else s2\"\"\" d = ADVOP.NZTRANS(c) return (d & s1) ^ (~d & s2)\n\nRemember that for any bit or bit vector , , , and . In the above function, if\nbit vector is non-zero, will be so , but will be so . Finally, , so is\nreturned. If is all zeroes instead, will be zeroed out, and will be returned.\nThis allows us to \"choose\" between and while encoding both results.\n\nThis was pivotal in creating the ALU, and many challenges faced while coding\nthe ALU were due to writing branchless code.\n\n### Implementation\n\nWe're here! Believe it or not, this is going to be the section with the least\ncontent. We've already implemented all of the functionality we're going to\nneed, so this is really just gluing it together! Let's discuss how the ALU\nimplementation works.\n\nThe ALU takes an opcode that we'll call and two operands, and . The opcode\ndecides which operation the ALU should perform on and . For convenience, I've\ndefined an enum with integer values to make which opcode we're using more\napparent. The code for the ALU simply loops over every supported operation and\nits opcode, compares with the opcode's value, and uses to select the correct\nresult for the opcode.\n\n    \n    \n    class ALU(BaseObject): def __init__(self, num_bits: int) -> None: self.n = num_bits OP_CODES = { ALUOP.ADD: ADVOP.ADD, ALUOP.SUB: ADVOP.SUB, ALUOP.AND: lambda a,b: a & b, ALUOP.OR: lambda a,b: a | b, ALUOP.XOR: lambda a,b: a ^ b, ALUOP.TWO_CMPT_A: lambda a,b: ADVOP.TWO_CMPT(a), ALUOP.TWO_CMPT_B: lambda a,b: ADVOP.TWO_CMPT(b), ALUOP.MUL: ADVOP.MUL, ALUOP.DIV: ADVOP.DIV, ALUOP.MIN: ADVOP.MIN, ALUOP.MAX: ADVOP.MAX, ALUOP.GT: ADVOP.GT, ALUOP.LT: ADVOP.LT, ALUOP.EQ: ADVOP.EQ, } def ALU(ctrl: BitVector[self.n]=None, a: BitVector[self.n]=None, b: BitVector[self.n]=None): c = a ^ a for op_code, func in OP_CODES.items(): c = ADVOP.IFNZ(ctrl ^ op_code.value, c, func(a, b)) return c self.alu = BitVector.from_func(ALU) self.func = ALU\n\n### Demonstration\n\nLet's play around with our new toy! Here I define a 4-bit ALU, and show that\nthe operations perform as expected.\n\n    \n    \n    \u2514\u2500$ alu = ALU(num_bits=4) \u2514\u2500$ alu(ctrl=ALUOP.ADD, a=3, b=4).int() 7 \u2514\u2500$ alu(ctrl=ALUOP.SUB, a=5, b=6).int() 15 \u2514\u2500$ alu(ctrl=ALUOP.DIV, a=11, b=2).int() 5 \u2514\u2500$ alu(ctrl=ALUOP.MAX, a=5, b=8).int() 8\n\nRecall that we can also partially evaluate the underlying bit vector bit-by-\nbit. Here I'm partially evaluating the ALU for concrete values of and while\nkeeping symbolic. The resultant bit vector represents the result of every\noperation of the ALU when and .\n\n    \n    \n    \u2514\u2500$ noop = alu(a=5, b=6) \u2514\u2500$ noop(ctrl=ALUOP.ADD.value).int() 11 \u2514\u2500$ noop(ctrl=ALUOP.OR.value).int() 7 \u2514\u2500$ noop(ctrl=ALUOP.MIN.value).int() 5\n\nWe can also force relations between bits by using composition. Imagine we want\nto be equal to . We can simply evaluate at that symbolic value to create the\nrelation. Paired with the solving system, this is extremely powerful. We can\neffectively query the symbolic representation such as, \"Give me all of the\ninputs for the ADD opcode where the result is and .\"\n\n    \n    \n    # This injects the symbits into our local scope as variables \u2514\u2500$ alu.alu.inject_locals(locals()) \u2514\u2500$ add_relation = alu(ctrl=ALUOP.ADD, b0=(a0 & b1) == b2) \u2514\u2500$ sol = add_relation.solve(7) \u2514\u2500$ sol.generate()[0] {'a0': 1, 'a1': 1, 'a2': 0, 'a3': 0, 'b0': 0, 'b1': 0, 'b2': 1, 'b3': 0, 'ctrl0': 0, 'ctrl1': 0, 'ctrl2': 0, 'ctrl3': 0} \u2514\u2500$ all([add_relation(**r).int() == 7 for r in sol.generate()]) True\n\n## Conclusion and closing remarks\n\nPolynomials are a versatile and powerful tool in mathematics. Encoding\nproblems in polynomials allows us to apply centuries of reasoning to them.\nUsing some basic math, we were able to represent an entire ALU and solve it\nfor specific outputs. This is a simple study implementation meant to showcase\none way to solve bitwise functions, so the current implementation is quite\nslow. There are many areas to explore in optimizing both the compiler and\nsolver.\n\n## Footnotes\n\n1\\. https://en.wikipedia.org/wiki/Zhegalkin_polynomial\n\n2\\. https://en.wikipedia.org/wiki/Polynomial\n\n3\\. https://math.stackexchange.com/questions/2418323/how-many-functions-have-\nan-n-bit-input-and-m-bit-output\n\n## Credits\n\nThis article was written by Dani Cronce, a Senior Security Consultant here at\nLeviathan. You can get in touch with Dani on her LinkedIn profile.\n\ndigital logicpolynomialsbitwise functionsapplication\nsecuritycryptographymathematics\n\nCody Martin\n\nPrevious\n\nPrevious\n\n## TunnelVision (CVE-2024-3661): How Attackers Can Decloak Routing-Based VPNs\nFor a Total VPN Leak\n\nNext\n\nNext\n\n## When You Have No Bars\n\ncontact@leviathansecurity.com 866-452-6997\n\nPenetration Testing | Risk And Compliance | Vendor Security | Programs | About Us | Careers | Blog\n\nThis site uses cookies to provide you with a great user experience. By\ncontinuing to use our website, you consent to the use of cookies. To find out\nmore about the cookies we use, please see our Privacy Policy.\n\n", "frontpage": false}
