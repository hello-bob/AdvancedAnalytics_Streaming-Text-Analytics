{"aid": "40274070", "title": "What Every Computer Scientist Should Know About Floating-Point Arithmetic", "url": "https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html", "domain": "oracle.com", "votes": 2, "user": "agomez314", "posted_at": "2024-05-06 12:54:48", "comments": 0, "source_title": "What Every Computer Scientist Should Know About Floating-Point Arithmetic", "source_text": "What Every Computer Scientist Should Know About Floating-Point Arithmetic\n\nNumerical Computation Guide  \n---  \n  \n> # Appendix D\n>\n> # What Every Computer Scientist Should Know About Floating-Point Arithmetic\n>\n> Note \u2013 This appendix is an edited reprint of the paper What Every Computer\n> Scientist Should Know About Floating-Point Arithmetic, by David Goldberg,\n> published in the March, 1991 issue of Computing Surveys. Copyright 1991,\n> Association for Computing Machinery, Inc., reprinted by permission.\n>\n> ## Abstract\n>\n> Floating-point arithmetic is considered an esoteric subject by many people.\n> This is rather surprising because floating-point is ubiquitous in computer\n> systems. Almost every language has a floating-point datatype; computers from\n> PCs to supercomputers have floating-point accelerators; most compilers will\n> be called upon to compile floating-point algorithms from time to time; and\n> virtually every operating system must respond to floating-point exceptions\n> such as overflow. This paper presents a tutorial on those aspects of\n> floating-point that have a direct impact on designers of computer systems.\n> It begins with background on floating-point representation and rounding\n> error, continues with a discussion of the IEEE floating-point standard, and\n> concludes with numerous examples of how computer builders can better support\n> floating-point.\n>\n> Categories and Subject Descriptors: (Primary) C.0 [Computer Systems\n> Organization]: General -- instruction set design; D.3.4 [Programming\n> Languages]: Processors -- compilers, optimization; G.1.0 [Numerical\n> Analysis]: General -- computer arithmetic, error analysis, numerical\n> algorithms (Secondary)\n>\n> D.2.1 [Software Engineering]: Requirements/Specifications -- languages;\n> D.3.4 Programming Languages]: Formal Definitions and Theory -- semantics;\n> D.4.1 Operating Systems]: Process Management -- synchronization.\n>\n> General Terms: Algorithms, Design, Languages\n>\n> Additional Key Words and Phrases: Denormalized number, exception, floating-\n> point, floating-point standard, gradual underflow, guard digit, NaN,\n> overflow, relative error, rounding error, rounding mode, ulp, underflow.\n>\n> ## Introduction\n>\n> Builders of computer systems often need information about floating-point\n> arithmetic. There are, however, remarkably few sources of detailed\n> information about it. One of the few books on the subject, Floating-Point\n> Computation by Pat Sterbenz, is long out of print. This paper is a tutorial\n> on those aspects of floating-point arithmetic (floating-point hereafter)\n> that have a direct connection to systems building. It consists of three\n> loosely connected parts. The first section, Rounding Error, discusses the\n> implications of using different rounding strategies for the basic operations\n> of addition, subtraction, multiplication and division. It also contains\n> background information on the two methods of measuring rounding error, ulps\n> and relative error. The second part discusses the IEEE floating-point\n> standard, which is becoming rapidly accepted by commercial hardware\n> manufacturers. Included in the IEEE standard is the rounding method for\n> basic operations. The discussion of the standard draws on the material in\n> the section Rounding Error. The third part discusses the connections between\n> floating-point and the design of various aspects of computer systems. Topics\n> include instruction set design, optimizing compilers and exception handling.\n>\n> I have tried to avoid making statements about floating-point without also\n> giving reasons why the statements are true, especially since the\n> justifications involve nothing more complicated than elementary calculus.\n> Those explanations that are not central to the main argument have been\n> grouped into a section called \"The Details,\" so that they can be skipped if\n> desired. In particular, the proofs of many of the theorems appear in this\n> section. The end of each proof is marked with the z symbol. When a proof is\n> not included, the z appears immediately following the statement of the\n> theorem.\n>\n> ## Rounding Error\n>\n> Squeezing infinitely many real numbers into a finite number of bits requires\n> an approximate representation. Although there are infinitely many integers,\n> in most programs the result of integer computations can be stored in 32\n> bits. In contrast, given any fixed number of bits, most calculations with\n> real numbers will produce quantities that cannot be exactly represented\n> using that many bits. Therefore the result of a floating-point calculation\n> must often be rounded in order to fit back into its finite representation.\n> This rounding error is the characteristic feature of floating-point\n> computation. The section Relative Error and Ulps describes how it is\n> measured.\n>\n> Since most floating-point calculations have rounding error anyway, does it\n> matter if the basic arithmetic operations introduce a little bit more\n> rounding error than necessary? That question is a main theme throughout this\n> section. The section Guard Digits discusses guard digits, a means of\n> reducing the error when subtracting two nearby numbers. Guard digits were\n> considered sufficiently important by IBM that in 1968 it added a guard digit\n> to the double precision format in the System/360 architecture (single\n> precision already had a guard digit), and retrofitted all existing machines\n> in the field. Two examples are given to illustrate the utility of guard\n> digits.\n>\n> The IEEE standard goes further than just requiring the use of a guard digit.\n> It gives an algorithm for addition, subtraction, multiplication, division\n> and square root, and requires that implementations produce the same result\n> as that algorithm. Thus, when a program is moved from one machine to\n> another, the results of the basic operations will be the same in every bit\n> if both machines support the IEEE standard. This greatly simplifies the\n> porting of programs. Other uses of this precise specification are given in\n> Exactly Rounded Operations.\n>\n> ### Floating-point Formats\n>\n> Several different representations of real numbers have been proposed, but by\n> far the most widely used is the floating-point representation.^1 Floating-\n> point representations have a base (which is always assumed to be even) and a\n> precision p. If = 10 and p = 3, then the number 0.1 is represented as 1.00 \u00d7\n> 10^-1. If = 2 and p = 24, then the decimal number 0.1 cannot be represented\n> exactly, but is approximately 1.10011001100110011001101 \u00d7 2^-4.\n>\n> In general, a floating-point number will be represented as \u00b1 d.dd... d \u00d7 ^e,\n> where d.dd... d is called the significand^2 and has p digits. More precisely\n> \u00b1 d_0 . d_1 d_2 ... d_p-1 \u00d7 ^e represents the number\n>\n> (1) .\n>\n> The term floating-point number will be used to mean a real number that can\n> be exactly represented in the format under discussion. Two other parameters\n> associated with floating-point representations are the largest and smallest\n> allowable exponents, e_max and e_min. Since there are ^p possible\n> significands, and e_max - e_min + 1 possible exponents, a floating-point\n> number can be encoded in\n>\n> bits, where the final +1 is for the sign bit. The precise encoding is not\n> important for now.\n>\n> There are two reasons why a real number might not be exactly representable\n> as a floating-point number. The most common situation is illustrated by the\n> decimal number 0.1. Although it has a finite decimal representation, in\n> binary it has an infinite repeating representation. Thus when = 2, the\n> number 0.1 lies strictly between two floating-point numbers and is exactly\n> representable by neither of them. A less common situation is that a real\n> number is out of range, that is, its absolute value is larger than \u00d7 or\n> smaller than 1.0 \u00d7 . Most of this paper discusses issues due to the first\n> reason. However, numbers that are out of range will be discussed in the\n> sections Infinity and Denormalized Numbers.\n>\n> Floating-point representations are not necessarily unique. For example, both\n> 0.01 \u00d7 10^1 and 1.00 \u00d7 10^-1 represent 0.1. If the leading digit is nonzero\n> (d_0 0 in equation (1) above), then the representation is said to be\n> normalized. The floating-point number 1.00 \u00d7 10^-1 is normalized, while 0.01\n> \u00d7 10^1 is not. When = 2, p = 3, e_min = -1 and e_max = 2 there are 16\n> normalized floating-point numbers, as shown in FIGURE D-1. The bold hash\n> marks correspond to numbers whose significand is 1.00. Requiring that a\n> floating-point representation be normalized makes the representation unique.\n> Unfortunately, this restriction makes it impossible to represent zero! A\n> natural way to represent 0 is with 1.0 \u00d7 , since^ this preserves the fact\n> that the numerical ordering of nonnegative real numbers corresponds to the\n> lexicographic ordering of their floating-point representations.^3 When the\n> exponent is stored in a k bit field, that means that only 2^k - 1 values are\n> available for use as exponents, since one must be reserved to represent 0.\n>\n> Note that the \u00d7 in a floating-point number is part of the notation, and\n> different from a floating-point multiply operation. The meaning of the \u00d7\n> symbol should be clear from the context. For example, the expression (2.5 \u00d7\n> 10^-3) \u00d7 (4.0 \u00d7 10^2) involves only a single floating-point multiplication.\n>\n> FIGURE D-1 Normalized numbers when = 2, p = 3, e_min = -1, e_max = 2\n>\n> ### Relative Error and Ulps\n>\n> Since rounding error is inherent in floating-point computation, it is\n> important to have a way to measure this error. Consider the floating-point\n> format with = 10 and p = 3, which will be used throughout this section. If\n> the result of a floating-point computation is 3.12 \u00d7 10^-2, and the answer\n> when computed to infinite precision is .0314, it is clear that this is in\n> error by 2 units in the last place. Similarly, if the real number .0314159\n> is represented as 3.14 \u00d7 10^-2, then it is in error by .159 units in the\n> last place. In general, if the floating-point number d.d...d \u00d7 ^e is used to\n> represent z, then it is in error by d.d...d - (z/^e)^p-1 units in the last\n> place.^4^, ^5 The term ulps will be used as shorthand for \"units in the last\n> place.\" If the result of a calculation is the floating-point number nearest\n> to the correct result, it still might be in error by as much as .5 ulp.\n> Another way to measure the difference between a floating-point number and\n> the real number it is approximating is relative error, which is simply the\n> difference between the two numbers divided by the real number. For example\n> the relative error committed when approximating 3.14159 by 3.14 \u00d7 10^0 is\n> .00159/3.14159 .0005.\n>\n> To compute the relative error that corresponds to .5 ulp, observe that when\n> a real number is approximated by the closest possible floating-point number\n> d.dd...dd \u00d7 ^e, the error can be as large as 0.00...00' \u00d7 ^e, where ' is the\n> digit /2, there are p units in the significand of the floating-point number,\n> and p units of 0 in the significand of the error. This error is ((/2)^-p) \u00d7\n> ^e. Since numbers of the form d.dd...dd \u00d7 ^e all have the same absolute\n> error, but have values that range between ^e and \u00d7 ^e, the relative error\n> ranges between ((/2)^-p) \u00d7 ^e/^e and ((/2)^-p) \u00d7 ^e/^e+1. That is,\n>\n> (2)\n>\n> In particular, the relative error corresponding to .5 ulp can vary by a\n> factor of . This factor is called the wobble. Setting = (/2)^-p to the\n> largest of the bounds in (2) above, we can say that when a real number is\n> rounded to the closest floating-point number, the relative error is always\n> bounded by e, which is referred to as machine epsilon.\n>\n> In the example above, the relative error was .00159/3.14159 .0005. In order\n> to avoid such small numbers, the relative error is normally written as a\n> factor times , which in this case is = (/2)^-p = 5(10)^-3 = .005. Thus the\n> relative error would be expressed as (.00159/3.14159)/.005) 0.1.\n>\n> To illustrate the difference between ulps and relative error, consider the\n> real number x = 12.35. It is approximated by = 1.24 \u00d7 10^1. The error is 0.5\n> ulps, the relative error is 0.8. Next consider the computation 8 . The exact\n> value is 8x = 98.8, while the computed value is 8 = 9.92 \u00d7 10^1. The error\n> is now 4.0 ulps, but the relative error is still 0.8. The error measured in\n> ulps is 8 times larger, even though the relative error is the same. In\n> general, when the base is , a fixed relative error expressed in ulps can\n> wobble by a factor of up to . And conversely, as equation (2) above shows, a\n> fixed error of .5 ulps results in a relative error that can wobble by .\n>\n> The most natural way to measure rounding error is in ulps. For example\n> rounding to the nearest floating-point number corresponds to an error of\n> less than or equal to .5 ulp. However, when analyzing the rounding error\n> caused by various formulas, relative error is a better measure. A good\n> illustration of this is the analysis in the section Theorem 9. Since can\n> overestimate the effect of rounding to the nearest floating-point number by\n> the wobble factor of , error estimates of formulas will be tighter on\n> machines with a small .\n>\n> When only the order of magnitude of rounding error is of interest, ulps and\n> may be used interchangeably, since they differ by at most a factor of . For\n> example, when a floating-point number is in error by n ulps, that means that\n> the number of contaminated digits is log n. If the relative error in a\n> computation is n, then\n>\n> (3) contaminated digits log_ n.\n>\n> ### Guard Digits\n>\n> One method of computing the difference between two floating-point numbers is\n> to compute the difference exactly and then round it to the nearest floating-\n> point number. This is very expensive if the operands differ greatly in size.\n> Assuming p = 3, 2.15 \u00d7 10^12 - 1.25 \u00d7 10^-5 would be calculated as\n>\n> x = 2.15 \u00d7 10^12 y = .0000000000000000125 \u00d7 10^12 x - y =\n> 2.1499999999999999875 \u00d7 10^12\n>\n> which rounds to 2.15 \u00d7 10^12. Rather than using all these digits, floating-\n> point hardware normally operates on a fixed number of digits. Suppose that\n> the number of digits kept is p, and that when the smaller operand is shifted\n> right, digits are simply discarded (as opposed to rounding). Then 2.15 \u00d7\n> 10^12 - 1.25 \u00d7 10^-5 becomes\n>\n> x = 2.15 \u00d7 10^12 y = 0.00 \u00d7 10^12 x - y = 2.15 \u00d7 10^12\n>\n> The answer is exactly the same as if the difference had been computed\n> exactly and then rounded. Take another example: 10.1 - 9.93. This becomes\n>\n> x = 1.01 \u00d7 10^1 y = 0.99 \u00d7 10^1 x - y = .02 \u00d7 10^1\n>\n> The correct answer is .17, so the computed difference is off by 30 ulps and\n> is wrong in every digit! How bad can the error be?\n>\n> #### Theorem 1\n>\n> Using a floating-point format with parameters and p, and computing\n> differences using p digits, the relative error of the result can be as large\n> as - 1.\n>\n> #### Proof\n>\n> A relative error of - 1 in the expression x - y occurs when x = 1.00...0 and\n> y = ...., where = - 1. Here y has p digits (all equal to ). The exact\n> difference is x - y = ^-^p. However, when computing the answer using only p\n> digits, the rightmost digit of y gets shifted off, and so the computed\n> difference is ^-^p^+1. Thus the error is ^-^p - ^-^p^+1 = ^-^p ( - 1), and\n> the relative error is ^-^p( - 1)/^-^p = - 1. z\n>\n> When =2, the relative error can be as large as the result, and when =10, it\n> can be 9 times larger. Or to put it another way, when =2, equation (3) shows\n> that the number of contaminated digits is log_2(1/) = log_2(2^p) = p. That\n> is, all of the p digits in the result are wrong! Suppose that one extra\n> digit is added to guard against this situation (a guard digit). That is, the\n> smaller number is truncated to p + 1 digits, and then the result of the\n> subtraction is rounded to p digits. With a guard digit, the previous example\n> becomes\n>\n> x = 1.010 \u00d7 10^1 y = 0.993 \u00d7 10^1 x - y = .017 \u00d7 10^1\n>\n> and the answer is exact. With a single guard digit, the relative error of\n> the result may be greater than , as in 110 - 8.59.\n>\n> x = 1.10 \u00d7 10^2 y = .085 \u00d7 10^2 x - y = 1.015 \u00d7 10^2\n>\n> This rounds to 102, compared with the correct answer of 101.41, for a\n> relative error of .006, which is greater than = .005. In general, the\n> relative error of the result can be only slightly larger than . More\n> precisely,\n>\n> #### Theorem 2\n>\n> If x and y are floating-point numbers in a format with parameters and p, and\n> if subtraction is done with p + 1 digits (i.e. one guard digit), then the\n> relative rounding error in the result is less than 2.\n>\n> This theorem will be proven in Rounding Error. Addition is included in the\n> above theorem since x and y can be positive or negative.\n>\n> ### Cancellation\n>\n> The last section can be summarized by saying that without a guard digit, the\n> relative error committed when subtracting two nearby quantities can be very\n> large. In other words, the evaluation of any expression containing a\n> subtraction (or an addition of quantities with opposite signs) could result\n> in a relative error so large that all the digits are meaningless (Theorem\n> 1). When subtracting nearby quantities, the most significant digits in the\n> operands match and cancel each other. There are two kinds of cancellation:\n> catastrophic and benign.\n>\n> Catastrophic cancellation occurs when the operands are subject to rounding\n> errors. For example in the quadratic formula, the expression b^2 - 4ac\n> occurs. The quantities b^2 and 4ac are subject to rounding errors since they\n> are the results of floating-point multiplications. Suppose that they are\n> rounded to the nearest floating-point number, and so are accurate to within\n> .5 ulp. When they are subtracted, cancellation can cause many of the\n> accurate digits to disappear, leaving behind mainly digits contaminated by\n> rounding error. Hence the difference might have an error of many ulps. For\n> example, consider b = 3.34, a = 1.22, and c = 2.28. The exact value of b^2 -\n> 4ac is .0292. But b^2 rounds to 11.2 and 4ac rounds to 11.1, hence the final\n> answer is .1 which is an error by 70 ulps, even though 11.2 - 11.1 is\n> exactly equal to .1^6. The subtraction did not introduce any error, but\n> rather exposed the error introduced in the earlier multiplications.\n>\n> Benign cancellation occurs when subtracting exactly known quantities. If x\n> and y have no rounding error, then by Theorem 2 if the subtraction is done\n> with a guard digit, the difference x-y has a very small relative error (less\n> than 2).\n>\n> A formula that exhibits catastrophic cancellation can sometimes be\n> rearranged to eliminate the problem. Again consider the quadratic formula\n>\n> (4)\n>\n> When , then does not involve a cancellation and\n>\n> .\n>\n> But the other addition (subtraction) in one of the formulas will have a\n> catastrophic cancellation. To avoid this, multiply the numerator and\n> denominator of r_1 by\n>\n> (and similarly for r_2) to obtain\n>\n> (5)\n>\n> If and , then computing r_1 using formula (4) will involve a cancellation.\n> Therefore, use formula (5) for computing r_1 and (4) for r_2. On the other\n> hand, if b < 0, use (4) for computing r_1 and (5) for r_2.\n>\n> The expression x^2 - y^2 is another formula that exhibits catastrophic\n> cancellation. It is more accurate to evaluate it as (x - y)(x + y).^7 Unlike\n> the quadratic formula, this improved form still has a subtraction, but it is\n> a benign cancellation of quantities without rounding error, not a\n> catastrophic one. By Theorem 2, the relative error in x - y is at most 2.\n> The same is true of x + y. Multiplying two quantities with a small relative\n> error results in a product with a small relative error (see the section\n> Rounding Error).\n>\n> In order to avoid confusion between exact and computed values, the following\n> notation is used. Whereas x - y denotes the exact difference of x and y, x y\n> denotes the computed difference (i.e., with rounding error). Similarly , ,\n> and denote computed addition, multiplication, and division, respectively.\n> All caps indicate the computed value of a function, as in LN(x) or SQRT(x).\n> Lowercase functions and traditional mathematical notation denote their exact\n> values as in ln(x) and .\n>\n> Although (x y) (x y) is an excellent approximation to x^2 - y^2, the\n> floating-point numbers x and y might themselves be approximations to some\n> true quantities and . For example, and might be exactly known decimal\n> numbers that cannot be expressed exactly in binary. In this case, even\n> though x y is a good approximation to x - y, it can have a huge relative\n> error compared to the true expression , and so the advantage of (x + y)(x -\n> y) over x^2 - y^2 is not as dramatic. Since computing (x + y)(x - y) is\n> about the same amount of work as computing x^2 - y^2, it is clearly the\n> preferred form in this case. In general, however, replacing a catastrophic\n> cancellation by a benign one is not worthwhile if the expense is large,\n> because the input is often (but not always) an approximation. But\n> eliminating a cancellation entirely (as in the quadratic formula) is\n> worthwhile even if the data are not exact. Throughout this paper, it will be\n> assumed that the floating-point inputs to an algorithm are exact and that\n> the results are computed as accurately as possible.\n>\n> The expression x^2 - y^2 is more accurate when rewritten as (x - y)(x + y)\n> because a catastrophic cancellation is replaced with a benign one. We next\n> present more interesting examples of formulas exhibiting catastrophic\n> cancellation that can be rewritten to exhibit only benign cancellation.\n>\n> The area of a triangle can be expressed directly in terms of the lengths of\n> its sides a, b, and c as\n>\n> (6)\n>\n> (Suppose the triangle is very flat; that is, a b + c. Then s a, and the term\n> (s - a) in formula (6) subtracts two nearby numbers, one of which may have\n> rounding error. For example, if a = 9.0, b = c = 4.53, the correct value of\n> s is 9.03 and A is 2.342.... Even though the computed value of s (9.05) is\n> in error by only 2 ulps, the computed value of A is 3.04, an error of 70\n> ulps.\n>\n> There is a way to rewrite formula (6) so that it will return accurate\n> results even for flat triangles [Kahan 1986]. It is\n>\n> (7)\n>\n> If a, b, and c do not satisfy a b c, rename them before applying (7). It is\n> straightforward to check that the right-hand sides of (6) and (7) are\n> algebraically identical. Using the values of a, b, and c above gives a\n> computed area of 2.35, which is 1 ulp in error and much more accurate than\n> the first formula.\n>\n> Although formula (7) is much more accurate than (6) for this example, it\n> would be nice to know how well (7) performs in general.\n>\n> #### Theorem 3\n>\n> The rounding error incurred when using (7) to compute the area of a triangle\n> is at most 11, provided that subtraction is performed with a guard digit, e\n> .005, and that square roots are computed to within 1/2 ulp.\n>\n> The condition that e < .005 is met in virtually every actual floating-point\n> system. For example when = 2, p 8 ensures that e < .005, and when = 10, p 3\n> is enough.\n>\n> In statements like Theorem 3 that discuss the relative error of an\n> expression, it is understood that the expression is computed using floating-\n> point arithmetic. In particular, the relative error is actually of the\n> expression\n>\n> (8) SQRT((a (b c)) (c (a b)) (c (a b)) (a (b c))) 4\n>\n> Because of the cumbersome nature of (8), in the statement of theorems we\n> will usually say the computed value of E rather than writing out E with\n> circle notation.\n>\n> Error bounds are usually too pessimistic. In the numerical example given\n> above, the computed value of (7) is 2.35, compared with a true value of\n> 2.34216 for a relative error of 0.7, which is much less than 11. The main\n> reason for computing error bounds is not to get precise bounds but rather to\n> verify that the formula does not contain numerical problems.\n>\n> A final example of an expression that can be rewritten to use benign\n> cancellation is (1 + x)^n, where . This expression arises in financial\n> calculations. Consider depositing $100 every day into a bank account that\n> earns an annual interest rate of 6%, compounded daily. If n = 365 and i =\n> .06, the amount of money accumulated at the end of one year is\n>\n> 100\n>\n> dollars. If this is computed using = 2 and p = 24, the result is $37615.45\n> compared to the exact answer of $37614.05, a discrepancy of $1.40. The\n> reason for the problem is easy to see. The expression 1 + i/n involves\n> adding 1 to .0001643836, so the low order bits of i/n are lost. This\n> rounding error is amplified when 1 + i/n is raised to the nth power.\n>\n> The troublesome expression (1 + i/n)^n can be rewritten as e^nln(1 +\n> ^i^/^n^), where now the problem is to compute ln(1 + x) for small x. One\n> approach is to use the approximation ln(1 + x) x, in which case the payment\n> becomes $37617.26, which is off by $3.21 and even less accurate than the\n> obvious formula. But there is a way to compute ln(1 + x) very accurately, as\n> Theorem 4 shows [Hewlett-Packard 1982]. This formula yields $37614.07,\n> accurate to within two cents!\n>\n> Theorem 4 assumes that LN(x) approximates ln(x) to within 1/2 ulp. The\n> problem it solves is that when x is small, LN(1 x) is not close to ln(1 + x)\n> because 1 x has lost the information in the low order bits of x. That is,\n> the computed value of ln(1 + x) is not close to its actual value when .\n>\n> #### Theorem 4\n>\n> If ln(1 + x) is computed using the formula\n>\n> the relative error is at most 5 when 0 x < 3/4, provided subtraction is\n> performed with a guard digit, e < 0.1, and ln is computed to within 1/2 ulp.\n>\n> This formula will work for any value of x but is only interesting for ,\n> which is where catastrophic cancellation occurs in the naive formula ln(1 +\n> x). Although the formula may seem mysterious, there is a simple explanation\n> for why it works. Write ln(1 + x) as\n>\n> .\n>\n> The left hand factor can be computed exactly, but the right hand factor \u03bc(x)\n> = ln(1 + x)/x will suffer a large rounding error when adding 1 to x.\n> However, \u03bc is almost constant, since ln(1 + x) x. So changing x slightly\n> will not introduce much error. In other words, if , computing will be a good\n> approximation to x\u03bc(x) = ln(1 + x). Is there a value for for which and can\n> be computed accurately? There is; namely = (1 x) 1, because then 1 + is\n> exactly equal to 1 x.\n>\n> The results of this section can be summarized by saying that a guard digit\n> guarantees accuracy when nearby precisely known quantities are subtracted\n> (benign cancellation). Sometimes a formula that gives inaccurate results can\n> be rewritten to have much higher numerical accuracy by using benign\n> cancellation; however, the procedure only works if subtraction is performed\n> using a guard digit. The price of a guard digit is not high, because it\n> merely requires making the adder one bit wider. For a 54 bit double\n> precision adder, the additional cost is less than 2%. For this price, you\n> gain the ability to run many algorithms such as formula (6) for computing\n> the area of a triangle and the expression ln(1 + x). Although most modern\n> computers have a guard digit, there are a few (such as Cray systems) that do\n> not.\n>\n> ### Exactly Rounded Operations\n>\n> When floating-point operations are done with a guard digit, they are not as\n> accurate as if they were computed exactly then rounded to the nearest\n> floating-point number. Operations performed in this manner will be called\n> exactly rounded.^8 The example immediately preceding Theorem 2 shows that a\n> single guard digit will not always give exactly rounded results. The\n> previous section gave several examples of algorithms that require a guard\n> digit in order to work properly. This section gives examples of algorithms\n> that require exact rounding.\n>\n> So far, the definition of rounding has not been given. Rounding is\n> straightforward, with the exception of how to round halfway cases; for\n> example, should 12.5 round to 12 or 13? One school of thought divides the 10\n> digits in half, letting {0, 1, 2, 3, 4} round down, and {5, 6, 7, 8, 9}\n> round up; thus 12.5 would round to 13. This is how rounding works on Digital\n> Equipment Corporation's VAX computers. Another school of thought says that\n> since numbers ending in 5 are halfway between two possible roundings, they\n> should round down half the time and round up the other half. One way of\n> obtaining this 50% behavior to require that the rounded result have its\n> least significant digit be even. Thus 12.5 rounds to 12 rather than 13\n> because 2 is even. Which of these methods is best, round up or round to\n> even? Reiser and Knuth [1975] offer the following reason for preferring\n> round to even.\n>\n> #### Theorem 5\n>\n> Let x and y be floating-point numbers, and define x_0 = x, x_1 = (x_0 y) y,\n> ..., x_n = (x_n-1 y) y. If and are exactly rounded using round to even, then\n> either x_n = x for all n or x_n = x_1 for all n 1. z\n>\n> To clarify this result, consider = 10, p = 3 and let x = 1.00, y = -.555.\n> When rounding up, the sequence becomes\n>\n> x_0 y = 1.56, x_1 = 1.56 .555 = 1.01, x_1 y = 1.01 .555 = 1.57,\n>\n> and each successive value of x_n increases by .01, until x_n = 9.45 (n\n> 845)^9. Under round to even, x_n is always 1.00. This example suggests that\n> when using the round up rule, computations can gradually drift upward,\n> whereas when using round to even the theorem says this cannot happen.\n> Throughout the rest of this paper, round to even will be used.\n>\n> One application of exact rounding occurs in multiple precision arithmetic.\n> There are two basic approaches to higher precision. One approach represents\n> floating-point numbers using a very large significand, which is stored in an\n> array of words, and codes the routines for manipulating these numbers in\n> assembly language. The second approach represents higher precision floating-\n> point numbers as an array of ordinary floating-point numbers, where adding\n> the elements of the array in infinite precision recovers the high precision\n> floating-point number. It is this second approach that will be discussed\n> here. The advantage of using an array of floating-point numbers is that it\n> can be coded portably in a high level language, but it requires exactly\n> rounded arithmetic.\n>\n> The key to multiplication in this system is representing a product xy as a\n> sum, where each summand has the same precision as x and y. This can be done\n> by splitting x and y. Writing x = x_h + x_l and y = y_h + y_l, the exact\n> product is\n>\n> xy = x_h y_h + x_h y_l + x_l y_h + x_l y_l.\n>\n> If x and y have p bit significands, the summands will also have p bit\n> significands provided that x_l, x_h, y_h, y_l can be represented using [p/2]\n> bits. When p is even, it is easy to find a splitting. The number x_0.x_1 ...\n> x_p - 1 can be written as the sum of x_0.x_1 ... x_p/2 - 1 and 0.0 ...\n> 0x_p/2 ... x_p - 1. When p is odd, this simple splitting method will not\n> work. An extra bit can, however, be gained by using negative numbers. For\n> example, if = 2, p = 5, and x = .10111, x can be split as x_h = .11 and x_l\n> = -.00001. There is more than one way to split a number. A splitting method\n> that is easy to compute is due to Dekker [1971], but it requires more than a\n> single guard digit.\n>\n> #### Theorem 6\n>\n> Let p be the floating-point precision, with the restriction that p is even\n> when > 2, and assume that floating-point operations are exactly rounded.\n> Then if k = [p/2] is half the precision (rounded up) and m = ^k + 1, x can\n> be split as x = x_h + x_l, where\n>\n> x_h = (m x) (m x x), x_l = x x_h,\n>\n> and each x_i is representable using [p/2] bits of precision.\n>\n> To see how this theorem works in an example, let = 10, p = 4, b = 3.476, a =\n> 3.463, and c = 3.479. Then b^2 - ac rounded to the nearest floating-point\n> number is .03480, while b b = 12.08, a c = 12.05, and so the computed value\n> of b^2 - ac is .03. This is an error of 480 ulps. Using Theorem 6 to write b\n> = 3.5 - .024, a = 3.5 - .037, and c = 3.5 - .021, b^2 becomes 3.5^2 - 2 \u00d7\n> 3.5 \u00d7 .024 + .024^2. Each summand is exact, so b^2 = 12.25 - .168 + .000576,\n> where the sum is left unevaluated at this point. Similarly, ac = 3.5^2 -\n> (3.5 \u00d7 .037 + 3.5 \u00d7 .021) + .037 \u00d7 .021 = 12.25 - .2030 +.000777. Finally,\n> subtracting these two series term by term gives an estimate for b^2 - ac of\n> 0 .0350 .000201 = .03480, which is identical to the exactly rounded result.\n> To show that Theorem 6 really requires exact rounding, consider p = 3, = 2,\n> and x = 7. Then m = 5, mx = 35, and m x = 32. If subtraction is performed\n> with a single guard digit, then (m x) x = 28. Therefore, x_h = 4 and x_l =\n> 3, hence x_l is not representable with [p/2] = 1 bit.\n>\n> As a final example of exact rounding, consider dividing m by 10. The result\n> is a floating-point number that will in general not be equal to m/10. When =\n> 2, multiplying m/10 by 10 will restore m, provided exact rounding is being\n> used. Actually, a more general fact (due to Kahan) is true. The proof is\n> ingenious, but readers not interested in such details can skip ahead to\n> section The IEEE Standard.\n>\n> #### Theorem 7\n>\n> When = 2, if m and n are integers with |m| < 2^p - 1 and n has the special\n> form n = 2^i + 2^j, then (m n) n = m, provided floating-point operations are\n> exactly rounded.\n>\n> #### Proof\n>\n> Scaling by a power of two is harmless, since it changes only the exponent,\n> not the significand. If q = m/n, then scale n so that 2^p^ - 1 n < 2^p and\n> scale m so that 1/2 < q < 1\\. Thus, 2^p^ - 2 < m < 2^p. Since m has p\n> significant bits, it has at most one bit to the right of the binary point.\n> Changing the sign of m is harmless, so assume that q > 0.\n> If = m n, to prove the theorem requires showing that\n>\n> (9)\n>\n> That is because m has at most 1 bit right of the binary point, so n will\n> round to m. To deal with the halfway case when |n - m| = 1/4, note that\n> since the initial unscaled m had |m| < 2^p^ - 1, its low-order bit was 0, so\n> the low-order bit of the scaled m is also 0. Thus, halfway cases will round\n> to m.\n> Suppose that q = .q_1q_2 ..., and let = .q_1q_2 ... q_p1. To estimate |n -\n> m|, first compute\n>\n> | - q| = |N/2^p^ + 1 - m/n|,\n>\n> where N is an odd integer. Since n = 2^i + 2^j and 2^p^ - 1 n < 2^p, it must\n> be that n = 2^p^ - 1 + 2^k for some k p - 2, and thus\n>\n> .\n>\n> The numerator is an integer, and since N is odd, it is in fact an odd\n> integer. Thus,\n>\n> | - q| 1/(n2^p^ + 1 - ^k).\n>\n> Assume q < (the case q > is similar).^10 Then n < m, and\n>\n> |m-n |= m-n = n(q- ) = n(q-( -2^-p-1)) =(2^p^-1+2^k)2^-p-1-2^-p-1+k =\n>\n> This establishes (9) and proves the theorem.^11 z\n>\n> The theorem holds true for any base , as long as 2^i + 2^j is replaced by ^i\n> + ^j. As gets larger, however, denominators of the form ^i + ^j are farther\n> and farther apart.\n>\n> We are now in a position to answer the question, Does it matter if the basic\n> arithmetic operations introduce a little more rounding error than necessary?\n> The answer is that it does matter, because accurate basic operations enable\n> us to prove that formulas are \"correct\" in the sense they have a small\n> relative error. The section Cancellation discussed several algorithms that\n> require guard digits to produce correct results in this sense. If the input\n> to those formulas are numbers representing imprecise measurements, however,\n> the bounds of Theorems 3 and 4 become less interesting. The reason is that\n> the benign cancellation x - y can become catastrophic if x and y are only\n> approximations to some measured quantity. But accurate operations are useful\n> even in the face of inexact data, because they enable us to establish exact\n> relationships like those discussed in Theorems 6 and 7. These are useful\n> even if every floating-point variable is only an approximation to some\n> actual value.\n>\n> ## The IEEE Standard\n>\n> There are two different IEEE standards for floating-point computation. IEEE\n> 754 is a binary standard that requires = 2, p = 24 for single precision and\n> p = 53 for double precision [IEEE 1987]. It also specifies the precise\n> layout of bits in a single and double precision. IEEE 854 allows either = 2\n> or = 10 and unlike 754, does not specify how floating-point numbers are\n> encoded into bits [Cody et al. 1984]. It does not require a particular value\n> for p, but instead it specifies constraints on the allowable values of p for\n> single and double precision. The term IEEE Standard will be used when\n> discussing properties common to both standards.\n>\n> This section provides a tour of the IEEE standard. Each subsection discusses\n> one aspect of the standard and why it was included. It is not the purpose of\n> this paper to argue that the IEEE standard is the best possible floating-\n> point standard but rather to accept the standard as given and provide an\n> introduction to its use. For full details consult the standards themselves\n> [IEEE 1987; Cody et al. 1984].\n>\n> ### Formats and Operations\n>\n> #### Base\n>\n> It is clear why IEEE 854 allows = 10. Base ten is how humans exchange and\n> think about numbers. Using = 10 is especially appropriate for calculators,\n> where the result of each operation is displayed by the calculator in\n> decimal.\n>\n> There are several reasons why IEEE 854 requires that if the base is not 10,\n> it must be 2. The section Relative Error and Ulps mentioned one reason: the\n> results of error analyses are much tighter when is 2 because a rounding\n> error of .5 ulp wobbles by a factor of when computed as a relative error,\n> and error analyses are almost always simpler when based on relative error. A\n> related reason has to do with the effective precision for large bases.\n> Consider = 16, p = 1 compared to = 2, p = 4. Both systems have 4 bits of\n> significand. Consider the computation of 15/8. When = 2, 15 is represented\n> as 1.111 \u00d7 2^3, and 15/8 as 1.111 \u00d7 2^0. So 15/8 is exact. However, when =\n> 16, 15 is represented as F \u00d7 16^0, where F is the hexadecimal digit for 15.\n> But 15/8 is represented as 1 \u00d7 16^0, which has only one bit correct. In\n> general, base 16 can lose up to 3 bits, so that a precision of p hexadecimal\n> digits can have an effective precision as low as 4p - 3 rather than 4p\n> binary bits. Since large values of have these problems, why did IBM choose =\n> 16 for its system/370? Only IBM knows for sure, but there are two possible\n> reasons. The first is increased exponent range. Single precision on the\n> system/370 has = 16, p = 6. Hence the significand requires 24 bits. Since\n> this must fit into 32 bits, this leaves 7 bits for the exponent and one for\n> the sign bit. Thus the magnitude of representable numbers ranges from about\n> to about = . To get a similar exponent range when = 2 would require 9 bits\n> of exponent, leaving only 22 bits for the significand. However, it was just\n> pointed out that when = 16, the effective precision can be as low as 4p - 3\n> = 21 bits. Even worse, when = 2 it is possible to gain an extra bit of\n> precision (as explained later in this section), so the = 2 machine has 23\n> bits of precision to compare with a range of 21 - 24 bits for the = 16\n> machine.\n>\n> Another possible explanation for choosing = 16 has to do with shifting. When\n> adding two floating-point numbers, if their exponents are different, one of\n> the significands will have to be shifted to make the radix points line up,\n> slowing down the operation. In the = 16, p = 1 system, all the numbers\n> between 1 and 15 have the same exponent, and so no shifting is required when\n> adding any of the ( ) = 105 possible pairs of distinct numbers from this\n> set. However, in the = 2, p = 4 system, these numbers have exponents ranging\n> from 0 to 3, and shifting is required for 70 of the 105 pairs.\n>\n> In most modern hardware, the performance gained by avoiding a shift for a\n> subset of operands is negligible, and so the small wobble of = 2 makes it\n> the preferable base. Another advantage of using = 2 is that there is a way\n> to gain an extra bit of significance.^12 Since floating-point numbers are\n> always normalized, the most significant bit of the significand is always 1,\n> and there is no reason to waste a bit of storage representing it. Formats\n> that use this trick are said to have a hidden bit. It was already pointed\n> out in Floating-point Formats that this requires a special convention for 0.\n> The method given there was that an exponent of e_min - 1 and a significand\n> of all zeros represents not , but rather 0.\n>\n> IEEE 754 single precision is encoded in 32 bits using 1 bit for the sign, 8\n> bits for the exponent, and 23 bits for the significand. However, it uses a\n> hidden bit, so the significand is 24 bits (p = 24), even though it is\n> encoded using only 23 bits.\n>\n> #### Precision\n>\n> The IEEE standard defines four different precisions: single, double, single-\n> extended, and double-extended. In IEEE 754, single and double precision\n> correspond roughly to what most floating-point hardware provides. Single\n> precision occupies a single 32 bit word, double precision two consecutive 32\n> bit words. Extended precision is a format that offers at least a little\n> extra precision and exponent range (TABLE D-1).\n>\n> TABLE D-1 IEEE 754 Format ParametersParameter| Format  \n> ---|---  \n> Single| Single-Extended| Double| Double-Extended  \n> p| 24| 32| 53| 64  \n> e_max| +127| 1023| +1023| > 16383  \n> e_min| -126| -1022| -1022| -16382  \n> Exponent width in bits| 8| 11| 11| 15  \n> Format width in bits| 32| 43| 64| 79  \n>  \n> The IEEE standard only specifies a lower bound on how many extra bits\n> extended precision provides. The minimum allowable double-extended format is\n> sometimes referred to as 80-bit format, even though the table shows it using\n> 79 bits. The reason is that hardware implementations of extended precision\n> normally do not use a hidden bit, and so would use 80 rather than 79\n> bits.^13\n>\n> The standard puts the most emphasis on extended precision, making no\n> recommendation concerning double precision, but strongly recommending that\n> Implementations should support the extended format corresponding to the\n> widest basic format supported, ...\n>\n> One motivation for extended precision comes from calculators, which will\n> often display 10 digits, but use 13 digits internally. By displaying only 10\n> of the 13 digits, the calculator appears to the user as a \"black box\" that\n> computes exponentials, cosines, etc. to 10 digits of accuracy. For the\n> calculator to compute functions like exp, log and cos to within 10 digits\n> with reasonable efficiency, it needs a few extra digits to work with. It is\n> not hard to find a simple rational expression that approximates log with an\n> error of 500 units in the last place. Thus computing with 13 digits gives an\n> answer correct to 10 digits. By keeping these extra 3 digits hidden, the\n> calculator presents a simple model to the operator.\n>\n> Extended precision in the IEEE standard serves a similar function. It\n> enables libraries to efficiently compute quantities to within about .5 ulp\n> in single (or double) precision, giving the user of those libraries a simple\n> model, namely that each primitive operation, be it a simple multiply or an\n> invocation of log, returns a value accurate to within about .5 ulp. However,\n> when using extended precision, it is important to make sure that its use is\n> transparent to the user. For example, on a calculator, if the internal\n> representation of a displayed value is not rounded to the same precision as\n> the display, then the result of further operations will depend on the hidden\n> digits and appear unpredictable to the user.\n>\n> To illustrate extended precision further, consider the problem of converting\n> between IEEE 754 single precision and decimal. Ideally, single precision\n> numbers will be printed with enough digits so that when the decimal number\n> is read back in, the single precision number can be recovered. It turns out\n> that 9 decimal digits are enough to recover a single precision binary number\n> (see the section Binary to Decimal Conversion). When converting a decimal\n> number back to its unique binary representation, a rounding error as small\n> as 1 ulp is fatal, because it will give the wrong answer. Here is a\n> situation where extended precision is vital for an efficient algorithm. When\n> single-extended is available, a very straightforward method exists for\n> converting a decimal number to a single precision binary one. First read in\n> the 9 decimal digits as an integer N, ignoring the decimal point. From TABLE\n> D-1, p 32, and since 10^9 < 2^32 4.3 \u00d7 10^9, N can be represented exactly in\n> single-extended. Next find the appropriate power 10^P necessary to scale N.\n> This will be a combination of the exponent of the decimal number, together\n> with the position of the (up until now) ignored decimal point. Compute\n> 10^|^P^|. If |P| 13, then this is also represented exactly, because 10^13 =\n> 2^135^13, and 5^13 < 2^32. Finally multiply (or divide if p < 0) N and\n> 10^|^P^|. If this last operation is done exactly, then the closest binary\n> number is recovered. The section Binary to Decimal Conversion shows how to\n> do the last multiply (or divide) exactly. Thus for |P| 13, the use of the\n> single-extended format enables 9-digit decimal numbers to be converted to\n> the closest binary number (i.e. exactly rounded). If |P| > 13, then single-\n> extended is not enough for the above algorithm to always compute the exactly\n> rounded binary equivalent, but Coonen [1984] shows that it is enough to\n> guarantee that the conversion of binary to decimal and back will recover the\n> original binary number.\n>\n> If double precision is supported, then the algorithm above would be run in\n> double precision rather than single-extended, but to convert double\n> precision to a 17-digit decimal number and back would require the double-\n> extended format.\n>\n> #### Exponent\n>\n> Since the exponent can be positive or negative, some method must be chosen\n> to represent its sign. Two common methods of representing signed numbers are\n> sign/magnitude and two's complement. Sign/magnitude is the system used for\n> the sign of the significand in the IEEE formats: one bit is used to hold the\n> sign, the rest of the bits represent the magnitude of the number. The two's\n> complement representation is often used in integer arithmetic. In this\n> scheme, a number in the range [-2^p-1, 2^p-1 - 1] is represented by the\n> smallest nonnegative number that is congruent to it modulo 2^p.\n>\n> The IEEE binary standard does not use either of these methods to represent\n> the exponent, but instead uses a biased representation. In the case of\n> single precision, where the exponent is stored in 8 bits, the bias is 127\n> (for double precision it is 1023). What this means is that if is the value\n> of the exponent bits interpreted as an unsigned integer, then the exponent\n> of the floating-point number is - 127. This is often called the unbiased\n> exponent to distinguish from the biased exponent .\n>\n> Referring to TABLE D-1, single precision has e_max = 127 and e_min = -126.\n> The reason for having |e_min| < e_max is so that the reciprocal of the\n> smallest number will not overflow. Although it is true that the reciprocal\n> of the largest number will underflow, underflow is usually less serious than\n> overflow. The section Base explained that e_min - 1 is used for representing\n> 0, and Special Quantities will introduce a use for e_max + 1. In IEEE single\n> precision, this means that the biased exponents range between e_min - 1 =\n> -127 and e_max + 1 = 128, whereas the unbiased exponents range between 0 and\n> 255, which are exactly the nonnegative numbers that can be represented using\n> 8 bits.\n>\n> #### Operations\n>\n> The IEEE standard requires that the result of addition, subtraction,\n> multiplication and division be exactly rounded. That is, the result must be\n> computed exactly and then rounded to the nearest floating-point number\n> (using round to even). The section Guard Digits pointed out that computing\n> the exact difference or sum of two floating-point numbers can be very\n> expensive when their exponents are substantially different. That section\n> introduced guard digits, which provide a practical way of computing\n> differences while guaranteeing that the relative error is small. However,\n> computing with a single guard digit will not always give the same answer as\n> computing the exact result and then rounding. By introducing a second guard\n> digit and a third sticky bit, differences can be computed at only a little\n> more cost than with a single guard digit, but the result is the same as if\n> the difference were computed exactly and then rounded [Goldberg 1990]. Thus\n> the standard can be implemented efficiently.\n>\n> One reason for completely specifying the results of arithmetic operations is\n> to improve the portability of software. When a program is moved between two\n> machines and both support IEEE arithmetic, then if any intermediate result\n> differs, it must be because of software bugs, not from differences in\n> arithmetic. Another advantage of precise specification is that it makes it\n> easier to reason about floating-point. Proofs about floating-point are hard\n> enough, without having to deal with multiple cases arising from multiple\n> kinds of arithmetic. Just as integer programs can be proven to be correct,\n> so can floating-point programs, although what is proven in that case is that\n> the rounding error of the result satisfies certain bounds. Theorem 4 is an\n> example of such a proof. These proofs are made much easier when the\n> operations being reasoned about are precisely specified. Once an algorithm\n> is proven to be correct for IEEE arithmetic, it will work correctly on any\n> machine supporting the IEEE standard.\n>\n> Brown [1981] has proposed axioms for floating-point that include most of the\n> existing floating-point hardware. However, proofs in this system cannot\n> verify the algorithms of sections Cancellation and Exactly Rounded\n> Operations, which require features not present on all hardware. Furthermore,\n> Brown's axioms are more complex than simply defining operations to be\n> performed exactly and then rounded. Thus proving theorems from Brown's\n> axioms is usually more difficult than proving them assuming operations are\n> exactly rounded.\n>\n> There is not complete agreement on what operations a floating-point standard\n> should cover. In addition to the basic operations +, -, \u00d7 and /, the IEEE\n> standard also specifies that square root, remainder, and conversion between\n> integer and floating-point be correctly rounded. It also requires that\n> conversion between internal formats and decimal be correctly rounded (except\n> for very large numbers). Kulisch and Miranker [1986] have proposed adding\n> inner product to the list of operations that are precisely specified. They\n> note that when inner products are computed in IEEE arithmetic, the final\n> answer can be quite wrong. For example sums are a special case of inner\n> products, and the sum ((2 \u00d7 10^-30 + 10^30) - 10^30) - 10^-30 is exactly\n> equal to 10^-30, but on a machine with IEEE arithmetic the computed result\n> will be -10^-30. It is possible to compute inner products to within 1 ulp\n> with less hardware than it takes to implement a fast multiplier [Kirchner\n> and Kulish 1987].^14 ^15\n>\n> All the operations mentioned in the standard are required to be exactly\n> rounded except conversion between decimal and binary. The reason is that\n> efficient algorithms for exactly rounding all the operations are known,\n> except conversion. For conversion, the best known efficient algorithms\n> produce results that are slightly worse than exactly rounded ones [Coonen\n> 1984].\n>\n> The IEEE standard does not require transcendental functions to be exactly\n> rounded because of the table maker's dilemma. To illustrate, suppose you are\n> making a table of the exponential function to 4 places. Then exp(1.626) =\n> 5.0835. Should this be rounded to 5.083 or 5.084? If exp(1.626) is computed\n> more carefully, it becomes 5.08350. And then 5.083500. And then 5.0835000.\n> Since exp is transcendental, this could go on arbitrarily long before\n> distinguishing whether exp(1.626) is 5.083500...0ddd or 5.0834999...9ddd.\n> Thus it is not practical to specify that the precision of transcendental\n> functions be the same as if they were computed to infinite precision and\n> then rounded. Another approach would be to specify transcendental functions\n> algorithmically. But there does not appear to be a single algorithm that\n> works well across all hardware architectures. Rational approximation,\n> CORDIC,^16 and large tables are three different techniques that are used for\n> computing transcendentals on contemporary machines. Each is appropriate for\n> a different class of hardware, and at present no single algorithm works\n> acceptably over the wide range of current hardware.\n>\n> ### Special Quantities\n>\n> On some floating-point hardware every bit pattern represents a valid\n> floating-point number. The IBM System/370 is an example of this. On the\n> other hand, the VAX^TM reserves some bit patterns to represent special\n> numbers called reserved operands. This idea goes back to the CDC 6600, which\n> had bit patterns for the special quantities INDEFINITE and INFINITY.\n>\n> The IEEE standard continues in this tradition and has NaNs (Not a Number)\n> and infinities. Without any special quantities, there is no good way to\n> handle exceptional situations like taking the square root of a negative\n> number, other than aborting computation. Under IBM System/370 FORTRAN, the\n> default action in response to computing the square root of a negative number\n> like -4 results in the printing of an error message. Since every bit pattern\n> represents a valid number, the return value of square root must be some\n> floating-point number. In the case of System/370 FORTRAN, is returned. In\n> IEEE arithmetic, a NaN is returned in this situation.\n>\n> The IEEE standard specifies the following special values (see TABLE D-2): \u00b1\n> 0, denormalized numbers, \u00b1 and NaNs (there is more than one NaN, as\n> explained in the next section). These special values are all encoded with\n> exponents of either e_max + 1 or e_min - 1 (it was already pointed out that\n> 0 has an exponent of e_min - 1).\n>\n> TABLE D-2 IEEE 754 Special ValuesExponent| Fraction| Represents  \n> ---|---|---  \n> e = e_min - 1| f = 0| \u00b10  \n> e = e_min - 1| f 0  \n> _ e_min e e_max| \\--| 1.f \u00d7 2^e  \n> e = e_max + 1| f = 0| \u00b1  \n> e = e_max + 1| f 0| NaN  \n>  \n> ### NaNs\n>\n> Traditionally, the computation of 0/0 or has been treated as an\n> unrecoverable error which causes a computation to halt. However, there are\n> examples where it makes sense for a computation to continue in such a\n> situation. Consider a subroutine that finds the zeros of a function f, say\n> zero(f). Traditionally, zero finders require the user to input an interval\n> [a, b] on which the function is defined and over which the zero finder will\n> search. That is, the subroutine is called as zero(f, a, b). A more useful\n> zero finder would not require the user to input this extra information. This\n> more general zero finder is especially appropriate for calculators, where it\n> is natural to simply key in a function, and awkward to then have to specify\n> the domain. However, it is easy to see why most zero finders require a\n> domain. The zero finder does its work by probing the function f at various\n> values. If it probed for a value outside the domain of f, the code for f\n> might well compute 0/0 or , and the computation would halt, unnecessarily\n> aborting the zero finding process.\n>\n> This problem can be avoided by introducing a special value called NaN, and\n> specifying that the computation of expressions like 0/0 and produce NaN,\n> rather than halting. A list of some of the situations that can cause a NaN\n> are given in TABLE D-3. Then when zero(f) probes outside the domain of f,\n> the code for f will return NaN, and the zero finder can continue. That is,\n> zero(f) is not \"punished\" for making an incorrect guess. With this example\n> in mind, it is easy to see what the result of combining a NaN with an\n> ordinary floating-point number should be. Suppose that the final statement\n> of f is return(-b + sqrt(d))/(2*a). If d < 0, then f should return a NaN.\n> Since d < 0, sqrt(d) is a NaN, and -b + sqrt(d) will be a NaN, if the sum of\n> a NaN and any other number is a NaN. Similarly if one operand of a division\n> operation is a NaN, the quotient should be a NaN. In general, whenever a NaN\n> participates in a floating-point operation, the result is another NaN.\n>\n> TABLE D-3 Operations That Produce a NaNOperation| NaN Produced By  \n> ---|---  \n> +| \\+ (- )  \n> \u00d7| 0 \u00d7  \n> /| 0/0, /  \n> REM| x REM 0, REM y  \n> (when x < 0)  \n>  \n> Another approach to writing a zero solver that doesn't require the user to\n> input a domain is to use signals. The zero-finder could install a signal\n> handler for floating-point exceptions. Then if f was evaluated outside its\n> domain and raised an exception, control would be returned to the zero\n> solver. The problem with this approach is that every language has a\n> different method of handling signals (if it has a method at all), and so it\n> has no hope of portability.\n>\n> In IEEE 754, NaNs are often represented as floating-point numbers with the\n> exponent e_max + 1 and nonzero significands. Implementations are free to put\n> system-dependent information into the significand. Thus there is not a\n> unique NaN, but rather a whole family of NaNs. When a NaN and an ordinary\n> floating-point number are combined, the result should be the same as the NaN\n> operand. Thus if the result of a long computation is a NaN, the system-\n> dependent information in the significand will be the information that was\n> generated when the first NaN in the computation was generated. Actually,\n> there is a caveat to the last statement. If both operands are NaNs, then the\n> result will be one of those NaNs, but it might not be the NaN that was\n> generated first.\n>\n> #### Infinity\n>\n> Just as NaNs provide a way to continue a computation when expressions like\n> 0/0 or are encountered, infinities provide a way to continue when an\n> overflow occurs. This is much safer than simply returning the largest\n> representable number. As an example, consider computing , when = 10, p = 3,\n> and e_max = 98. If x = 3 \u00d7 10^70 and y = 4 \u00d7 10^70, then x^2 will overflow,\n> and be replaced by 9.99 \u00d7 10^98. Similarly y^2, and x^2 + y^2 will each\n> overflow in turn, and be replaced by 9.99 \u00d7 10^98. So the final result will\n> be , which is drastically wrong: the correct answer is 5 \u00d7 10^70. In IEEE\n> arithmetic, the result of x^2 is , as is y^2, x^2 + y^2 and . So the final\n> result is , which is safer than returning an ordinary floating-point number\n> that is nowhere near the correct answer.^17\n>\n> The division of 0 by 0 results in a NaN. A nonzero number divided by 0,\n> however, returns infinity: 1/0 = , -1/0 = -. The reason for the distinction\n> is this: if f(x) 0 and g(x) 0 as x approaches some limit, then f(x)/g(x)\n> could have any value. For example, when f(x) = sin x and g(x) = x, then\n> f(x)/g(x) 1 as x 0. But when f(x) = 1 - cos x, f(x)/g(x) 0. When thinking of\n> 0/0 as the limiting situation of a quotient of two very small numbers, 0/0\n> could represent anything. Thus in the IEEE standard, 0/0 results in a NaN.\n> But when c > 0, f(x) c, and g(x)0, then f(x)/g(x) \u00b1, for any analytic\n> functions f and g. If g(x) < 0 for small x, then f(x)/g(x) -, otherwise the\n> limit is +. So the IEEE standard defines c/0 = \u00b1, as long as c 0. The sign\n> of depends on the signs of c and 0 in the usual way, so that -10/0 = -, and\n> -10/-0 = +. You can distinguish between getting because of overflow and\n> getting because of division by zero by checking the status flags (which will\n> be discussed in detail in section Flags). The overflow flag will be set in\n> the first case, the division by zero flag in the second.\n>\n> The rule for determining the result of an operation that has infinity as an\n> operand is simple: replace infinity with a finite number x and take the\n> limit as x . Thus 3/ = 0, because\n>\n> .\n>\n> Similarly, 4 - = -, and = . When the limit doesn't exist, the result is a\n> NaN, so / will be a NaN (TABLE D-3 has additional examples). This agrees\n> with the reasoning used to conclude that 0/0 should be a NaN.\n>\n> When a subexpression evaluates to a NaN, the value of the entire expression\n> is also a NaN. In the case of \u00b1 however, the value of the expression might\n> be an ordinary floating-point number because of rules like 1/ = 0. Here is a\n> practical example that makes use of the rules for infinity arithmetic.\n> Consider computing the function x/(x^2 + 1). This is a bad formula, because\n> not only will it overflow when x is larger than , but infinity arithmetic\n> will give the wrong answer because it will yield 0, rather than a number\n> near 1/x. However, x/(x^2 + 1) can be rewritten as 1/(x + x^-1). This\n> improved expression will not overflow prematurely and because of infinity\n> arithmetic will have the correct value when x = 0: 1/(0 + 0^-1) = 1/(0 + ) =\n> 1/ = 0. Without infinity arithmetic, the expression 1/(x + x^-1) requires a\n> test for x = 0, which not only adds extra instructions, but may also disrupt\n> a pipeline. This example illustrates a general fact, namely that infinity\n> arithmetic often avoids the need for special case checking; however,\n> formulas need to be carefully inspected to make sure they do not have\n> spurious behavior at infinity (as x/(x^2 + 1) did).\n>\n> #### Signed Zero\n>\n> Zero is represented by the exponent e_min - 1 and a zero significand. Since\n> the sign bit can take on two different values, there are two zeros, +0 and\n> -0. If a distinction were made when comparing +0 and -0, simple tests like\n> if (x = 0) would have very unpredictable behavior, depending on the sign of\n> x. Thus the IEEE standard defines comparison so that +0 = -0, rather than -0\n> < +0. Although it would be possible always to ignore the sign of zero, the\n> IEEE standard does not do so. When a multiplication or division involves a\n> signed zero, the usual sign rules apply in computing the sign of the answer.\n> Thus 3\u00b7(+0) = +0, and +0/-3 = -0. If zero did not have a sign, then the\n> relation 1/(1/x) = x would fail to hold when x = \u00b1. The reason is that 1/-\n> and 1/+ both result in 0, and 1/0 results in +, the sign information having\n> been lost. One way to restore the identity 1/(1/x) = x is to only have one\n> kind of infinity, however that would result in the disastrous consequence of\n> losing the sign of an overflowed quantity.\n>\n> Another example of the use of signed zero concerns underflow and functions\n> that have a discontinuity at 0, such as log. In IEEE arithmetic, it is\n> natural to define log 0 = - and log x to be a NaN when x < 0\\. Suppose that\n> x represents a small negative number that has underflowed to zero. Thanks to\n> signed zero, x will be negative, so log can return a NaN. However, if there\n> were no signed zero, the log function could not distinguish an underflowed\n> negative number from 0, and would therefore have to return -. Another\n> example of a function with a discontinuity at zero is the signum function,\n> which returns the sign of a number.\n>\n> Probably the most interesting use of signed zero occurs in complex\n> arithmetic. To take a simple example, consider the equation . This is\n> certainly true when z 0. If z = -1, the obvious computation gives and .\n> Thus, ! The problem can be traced to the fact that square root is multi-\n> valued, and there is no way to select the values so that it is continuous in\n> the entire complex plane. However, square root is continuous if a branch cut\n> consisting of all negative real numbers is excluded from consideration. This\n> leaves the problem of what to do for the negative real numbers, which are of\n> the form -x + i0, where x > 0\\. Signed zero provides a perfect way to\n> resolve this problem. Numbers of the form x + i(+0) have one sign and\n> numbers of the form x + i(-0) on the other side of the branch cut have the\n> other sign . In fact, the natural formulas for computing will give these\n> results.\n>\n> Back to . If z =1 = -1 + i0, then\n>\n> 1/z = 1/(-1 + i0) = [(-1- i0)]/[(-1 + i0)(-1 - i0)] = (-1 -- i0)/((-1)^2 -\n> 0^2) = -1 + i(-0),\n>\n> and so , while . Thus IEEE arithmetic preserves this identity for all z.\n> Some more sophisticated examples are given by Kahan [1987]. Although\n> distinguishing between +0 and -0 has advantages, it can occasionally be\n> confusing. For example, signed zero destroys the relation x = y 1/x = 1/y,\n> which is false when x = +0 and y = -0. However, the IEEE committee decided\n> that the advantages of utilizing the sign of zero outweighed the\n> disadvantages.\n>\n> #### Denormalized Numbers\n>\n> Consider normalized floating-point numbers with = 10, p = 3, and e_min =\n> -98. The numbers x = 6.87 \u00d7 10^-97 and y = 6.81 \u00d7 10^-97 appear to be\n> perfectly ordinary floating-point numbers, which are more than a factor of\n> 10 larger than the smallest floating-point number 1.00 \u00d7 10^-98. They have a\n> strange property, however: x y = 0 even though x y! The reason is that x - y\n> = .06 \u00d7 10^ -97 = 6.0 \u00d7 10^-99 is too small to be represented as a\n> normalized number, and so must be flushed to zero. How important is it to\n> preserve the property\n>\n> (10) x = y x - y = 0 ?\n>\n> It's very easy to imagine writing the code fragment, if (x y) then z =\n> 1/(x-y), and much later having a program fail due to a spurious division by\n> zero. Tracking down bugs like this is frustrating and time consuming. On a\n> more philosophical level, computer science textbooks often point out that\n> even though it is currently impractical to prove large programs correct,\n> designing programs with the idea of proving them often results in better\n> code. For example, introducing invariants is quite useful, even if they\n> aren't going to be used as part of a proof. Floating-point code is just like\n> any other code: it helps to have provable facts on which to depend. For\n> example, when analyzing formula (6), it was very helpful to know that x/2 <\n> y < 2x x y = x - y. Similarly, knowing that (10) is true makes writing\n> reliable floating-point code easier. If it is only true for most numbers, it\n> cannot be used to prove anything.\n>\n> The IEEE standard uses denormalized^18 numbers, which guarantee (10), as\n> well as other useful relations. They are the most controversial part of the\n> standard and probably accounted for the long delay in getting 754 approved.\n> Most high performance hardware that claims to be IEEE compatible does not\n> support denormalized numbers directly, but rather traps when consuming or\n> producing denormals, and leaves it to software to simulate the IEEE\n> standard.^19 The idea behind denormalized numbers goes back to Goldberg\n> [1967] and is very simple. When the exponent is e_min, the significand does\n> not have to be normalized, so that when = 10, p = 3 and e_min = -98, 1.00 \u00d7\n> 10^-98 is no longer the smallest floating-point number, because 0.98 \u00d7\n> 10^-98 is also a floating-point number.\n>\n> There is a small snag when = 2 and a hidden bit is being used, since a\n> number with an exponent of e_min will always have a significand greater than\n> or equal to 1.0 because of the implicit leading bit. The solution is similar\n> to that used to represent 0, and is summarized in TABLE D-2. The exponent\n> e_min is used to represent denormals. More formally, if the bits in the\n> significand field are b_1, b_2, ..., b_p -1, and the value of the exponent\n> is e, then when e > e_min - 1, the number being represented is\n> 1.b_1b_2...b_p - 1 \u00d7 2^e whereas when e = e_min - 1, the number being\n> represented is 0.b_1b_2...b_p - 1 \u00d7 2^e^ + 1. The +1 in the exponent is\n> needed because denormals have an exponent of e_min, not e_min - 1.\n>\n> Recall the example of = 10, p = 3, e_min = -98, x = 6.87 \u00d7 10^-97 and y =\n> 6.81 \u00d7 10^-97 presented at the beginning of this section. With denormals, x\n> - y does not flush to zero but is instead represented by the denormalized\n> number .6 \u00d7 10^-98. This behavior is called gradual underflow. It is easy to\n> verify that (10) always holds when using gradual underflow.\n>\n> FIGURE D-2 Flush To Zero Compared With Gradual Underflow\n>\n> FIGURE D-2 illustrates denormalized numbers. The top number line in the\n> figure shows normalized floating-point numbers. Notice the gap between 0 and\n> the smallest normalized number . If the result of a floating-point\n> calculation falls into this gulf, it is flushed to zero. The bottom number\n> line shows what happens when denormals are added to the set of floating-\n> point numbers. The \"gulf\" is filled in, and when the result of a calculation\n> is less than , it is represented by the nearest denormal. When denormalized\n> numbers are added to the number line, the spacing between adjacent floating-\n> point numbers varies in a regular way: adjacent spacings are either the same\n> length or differ by a factor of . Without denormals, the spacing abruptly\n> changes from to , which is a factor of , rather than the orderly change by a\n> factor of . Because of this, many algorithms that can have large relative\n> error for normalized numbers close to the underflow threshold are well-\n> behaved in this range when gradual underflow is used.\n>\n> Without gradual underflow, the simple expression x - y can have a very large\n> relative error for normalized inputs, as was seen above for x = 6.87 \u00d7\n> 10^-97 and y = 6.81 \u00d7 10^-97. Large relative errors can happen even without\n> cancellation, as the following example shows [Demmel 1984]. Consider\n> dividing two complex numbers, a + ib and c + id. The obvious formula\n>\n> \u00b7 i\n>\n> suffers from the problem that if either component of the denominator c + id\n> is larger than , the formula will overflow, even though the final result may\n> be well within range. A better method of computing the quotients is to use\n> Smith's formula:\n>\n> (11)\n>\n> Applying Smith's formula to (2 \u00b7 10^-98 + i10^-98)/(4 \u00b7 10^-98 + i(2 \u00b7\n> 10^-98)) gives the correct answer of 0.5 with gradual underflow. It yields\n> 0.4 with flush to zero, an error of 100 ulps. It is typical for denormalized\n> numbers to guarantee error bounds for arguments all the way down to 1.0 x .\n>\n> ### Exceptions, Flags and Trap Handlers\n>\n> When an exceptional condition like division by zero or overflow occurs in\n> IEEE arithmetic, the default is to deliver a result and continue. Typical of\n> the default results are NaN for 0/0 and , and for 1/0 and overflow. The\n> preceding sections gave examples where proceeding from an exception with\n> these default values was the reasonable thing to do. When any exception\n> occurs, a status flag is also set. Implementations of the IEEE standard are\n> required to provide users with a way to read and write the status flags. The\n> flags are \"sticky\" in that once set, they remain set until explicitly\n> cleared. Testing the flags is the only way to distinguish 1/0, which is a\n> genuine infinity from an overflow.\n>\n> Sometimes continuing execution in the face of exception conditions is not\n> appropriate. The section Infinity gave the example of x/(x^2 + 1). When x >\n> , the denominator is infinite, resulting in a final answer of 0, which is\n> totally wrong. Although for this formula the problem can be solved by\n> rewriting it as 1/(x + x^-1), rewriting may not always solve the problem.\n> The IEEE standard strongly recommends that implementations allow trap\n> handlers to be installed. Then when an exception occurs, the trap handler is\n> called instead of setting the flag. The value returned by the trap handler\n> will be used as the result of the operation. It is the responsibility of the\n> trap handler to either clear or set the status flag; otherwise, the value of\n> the flag is allowed to be undefined.\n>\n> The IEEE standard divides exceptions into 5 classes: overflow, underflow,\n> division by zero, invalid operation and inexact. There is a separate status\n> flag for each class of exception. The meaning of the first three exceptions\n> is self-evident. Invalid operation covers the situations listed in TABLE\n> D-3, and any comparison that involves a NaN. The default result of an\n> operation that causes an invalid exception is to return a NaN, but the\n> converse is not true. When one of the operands to an operation is a NaN, the\n> result is a NaN but no invalid exception is raised unless the operation also\n> satisfies one of the conditions in TABLE D-3.^20\n>\n> TABLE D-4 Exceptions in IEEE 754*Exception| Result when traps disabled|\n> Argument to trap handler  \n> ---|---|---  \n> overflow| \u00b1 or \u00b1x_max| round(x2^-)  \n> underflow| 0, or denormal| round(x2)  \n> divide by zero| \u00b1| operands  \n> invalid| NaN| operands  \n> inexact| round(x)| round(x)  \n>  \n> *x is the exact result of the operation, = 192 for single precision, 1536\n> for double, and x_max = 1.11 ...11 \u00d7 .\n>\n> The inexact exception is raised when the result of a floating-point\n> operation is not exact. In the = 10, p = 3 system, 3.5 4.2 = 14.7 is exact,\n> but 3.5 4.3 = 15.0 is not exact (since 3.5 \u00b7 4.3 = 15.05), and raises an\n> inexact exception. Binary to Decimal Conversion discusses an algorithm that\n> uses the inexact exception. A summary of the behavior of all five exceptions\n> is given in TABLE D-4.\n>\n> There is an implementation issue connected with the fact that the inexact\n> exception is raised so often. If floating-point hardware does not have flags\n> of its own, but instead interrupts the operating system to signal a\n> floating-point exception, the cost of inexact exceptions could be\n> prohibitive. This cost can be avoided by having the status flags maintained\n> by software. The first time an exception is raised, set the software flag\n> for the appropriate class, and tell the floating-point hardware to mask off\n> that class of exceptions. Then all further exceptions will run without\n> interrupting the operating system. When a user resets that status flag, the\n> hardware mask is re-enabled.\n>\n> #### Trap Handlers\n>\n> One obvious use for trap handlers is for backward compatibility. Old codes\n> that expect to be aborted when exceptions occur can install a trap handler\n> that aborts the process. This is especially useful for codes with a loop\n> like do S until (x >= 100). Since comparing a NaN to a number with <, , >, ,\n> or = (but not ) always returns false, this code will go into an infinite\n> loop if x ever becomes a NaN.\n>\n> There is a more interesting use for trap handlers that comes up when\n> computing products such as that could potentially overflow. One solution is\n> to use logarithms, and compute exp instead. The problem with this approach\n> is that it is less accurate, and that it costs more than the simple\n> expression , even if there is no overflow. There is another solution using\n> trap handlers called over/underflow counting that avoids both of these\n> problems [Sterbenz 1974].\n>\n> The idea is as follows. There is a global counter initialized to zero.\n> Whenever the partial product overflows for some k, the trap handler\n> increments the counter by one and returns the overflowed quantity with the\n> exponent wrapped around. In IEEE 754 single precision, e_max = 127, so if\n> p_k = 1.45 \u00d7 2^130, it will overflow and cause the trap handler to be\n> called, which will wrap the exponent back into range, changing p_k to 1.45 \u00d7\n> 2^-62 (see below). Similarly, if p_k underflows, the counter would be\n> decremented, and negative exponent would get wrapped around into a positive\n> one. When all the multiplications are done, if the counter is zero then the\n> final product is p_n. If the counter is positive, the product overflowed, if\n> the counter is negative, it underflowed. If none of the partial products are\n> out of range, the trap handler is never called and the computation incurs no\n> extra cost. Even if there are over/underflows, the calculation is more\n> accurate than if it had been computed with logarithms, because each p_k was\n> computed from p_k - 1 using a full precision multiply. Barnett [1987]\n> discusses a formula where the full accuracy of over/underflow counting\n> turned up an error in earlier tables of that formula.\n>\n> IEEE 754 specifies that when an overflow or underflow trap handler is\n> called, it is passed the wrapped-around result as an argument. The\n> definition of wrapped-around for overflow is that the result is computed as\n> if to infinite precision, then divided by 2, and then rounded to the\n> relevant precision. For underflow, the result is multiplied by 2. The\n> exponent is 192 for single precision and 1536 for double precision. This is\n> why 1.45 x 2^130 was transformed into 1.45 \u00d7 2^-62 in the example above.\n>\n> #### Rounding Modes\n>\n> In the IEEE standard, rounding occurs whenever an operation has a result\n> that is not exact, since (with the exception of binary decimal conversion)\n> each operation is computed exactly and then rounded. By default, rounding\n> means round toward nearest. The standard requires that three other rounding\n> modes be provided, namely round toward 0, round toward +, and round toward\n> -. When used with the convert to integer operation, round toward - causes\n> the convert to become the floor function, while round toward + is ceiling.\n> The rounding mode affects overflow, because when round toward 0 or round\n> toward - is in effect, an overflow of positive magnitude causes the default\n> result to be the largest representable number, not +. Similarly, overflows\n> of negative magnitude will produce the largest negative number when round\n> toward + or round toward 0 is in effect.\n>\n> One application of rounding modes occurs in interval arithmetic (another is\n> mentioned in Binary to Decimal Conversion). When using interval arithmetic,\n> the sum of two numbers x and y is an interval , where is x y rounded toward\n> -, and is x y rounded toward +. The exact result of the addition is\n> contained within the interval . Without rounding modes, interval arithmetic\n> is usually implemented by computing and , where is machine epsilon.^21 This\n> results in overestimates for the size of the intervals. Since the result of\n> an operation in interval arithmetic is an interval, in general the input to\n> an operation will also be an interval. If two intervals , and , are added,\n> the result is , where is with the rounding mode set to round toward -, and\n> is with the rounding mode set to round toward +.\n>\n> When a floating-point calculation is performed using interval arithmetic,\n> the final answer is an interval that contains the exact result of the\n> calculation. This is not very helpful if the interval turns out to be large\n> (as it often does), since the correct answer could be anywhere in that\n> interval. Interval arithmetic makes more sense when used in conjunction with\n> a multiple precision floating-point package. The calculation is first\n> performed with some precision p. If interval arithmetic suggests that the\n> final answer may be inaccurate, the computation is redone with higher and\n> higher precisions until the final interval is a reasonable size.\n>\n> #### Flags\n>\n> The IEEE standard has a number of flags and modes. As discussed above, there\n> is one status flag for each of the five exceptions: underflow, overflow,\n> division by zero, invalid operation and inexact. There are four rounding\n> modes: round toward nearest, round toward +, round toward 0, and round\n> toward -. It is strongly recommended that there be an enable mode bit for\n> each of the five exceptions. This section gives some simple examples of how\n> these modes and flags can be put to good use. A more sophisticated example\n> is discussed in the section Binary to Decimal Conversion.\n>\n> Consider writing a subroutine to compute x^n, where n is an integer. When n\n> > 0, a simple routine like\n>  \n>  \n>     PositivePower(x,n) {\n>  \n>  \n>     while (n is even) {\n>  \n>  \n>     x = x*x\n>  \n>  \n>     n = n/2\n>  \n>  \n>     }\n>  \n>  \n>     u = x\n>  \n>  \n>     while (true) {\n>  \n>  \n>     n = n/2\n>  \n>  \n>     if (n==0) return u\n>  \n>  \n>     x = x*x\n>  \n>  \n>     if (n is odd) u = u*x\n>  \n>  \n>     }\n>\n> If n < 0, then a more accurate way to compute x^n is not to call\n> PositivePower(1/x, -n) but rather 1/PositivePower(x, -n), because the first\n> expression multiplies n quantities each of which have a rounding error from\n> the division (i.e., 1/x). In the second expression these are exact (i.e.,\n> x), and the final division commits just one additional rounding error.\n> Unfortunately, these is a slight snag in this strategy. If PositivePower(x,\n> -n) underflows, then either the underflow trap handler will be called, or\n> else the underflow status flag will be set. This is incorrect, because if\n> x^-^n underflows, then x^n will either overflow or be in range.^22 But since\n> the IEEE standard gives the user access to all the flags, the subroutine can\n> easily correct for this. It simply turns off the overflow and underflow trap\n> enable bits and saves the overflow and underflow status bits. It then\n> computes 1/PositivePower(x, -n). If neither the overflow nor underflow\n> status bit is set, it restores them together with the trap enable bits. If\n> one of the status bits is set, it restores the flags and redoes the\n> calculation using PositivePower(1/x, -n), which causes the correct\n> exceptions to occur.\n>\n> Another example of the use of flags occurs when computing arccos via the\n> formula\n>\n> arccos x = 2 arctan .\n>\n> If arctan() evaluates to /2, then arccos(-1) will correctly evaluate to\n> 2\u00b7arctan() =, because of infinity arithmetic. However, there is a small\n> snag, because the computation of (1 - x)/(1 + x) will cause the divide by\n> zero exception flag to be set, even though arccos(-1) is not exceptional.\n> The solution to this problem is straightforward. Simply save the value of\n> the divide by zero flag before computing arccos, and then restore its old\n> value after the computation.\n>\n> ## Systems Aspects\n>\n> The design of almost every aspect of a computer system requires knowledge\n> about floating-point. Computer architectures usually have floating-point\n> instructions, compilers must generate those floating-point instructions, and\n> the operating system must decide what to do when exception conditions are\n> raised for those floating-point instructions. Computer system designers\n> rarely get guidance from numerical analysis texts, which are typically aimed\n> at users and writers of software, not at computer designers. As an example\n> of how plausible design decisions can lead to unexpected behavior, consider\n> the following BASIC program.\n>  \n>  \n>     q = 3.0/7.0\n>  \n>  \n>     if q = 3.0/7.0 then print \"Equal\":\n>  \n>  \n>     else print \"Not Equal\"\n>\n> When compiled and run using Borland's Turbo Basic on an IBM PC, the program\n> prints Not Equal! This example will be analyzed in the next section\n>\n> Incidentally, some people think that the solution to such anomalies is never\n> to compare floating-point numbers for equality, but instead to consider them\n> equal if they are within some error bound E. This is hardly a cure-all\n> because it raises as many questions as it answers. What should the value of\n> E be? If x < 0 and y > 0 are within E, should they really be considered to\n> be equal, even though they have different signs? Furthermore, the relation\n> defined by this rule, a ~ b |a - b| < E, is not an equivalence relation\n> because a ~ b and b ~ c does not imply that a ~ c.\n>\n> ### Instruction Sets\n>\n> It is quite common for an algorithm to require a short burst of higher\n> precision in order to produce accurate results. One example occurs in the\n> quadratic formula ( )/2a. As discussed in the section Proof of Theorem 4,\n> when b^2 4ac, rounding error can contaminate up to half the digits in the\n> roots computed with the quadratic formula. By performing the subcalculation\n> of b^2 - 4ac in double precision, half the double precision bits of the root\n> are lost, which means that all the single precision bits are preserved.\n>\n> The computation of b^2 - 4ac in double precision when each of the quantities\n> a, b, and c are in single precision is easy if there is a multiplication\n> instruction that takes two single precision numbers and produces a double\n> precision result. In order to produce the exactly rounded product of two\n> p-digit numbers, a multiplier needs to generate the entire 2p bits of\n> product, although it may throw bits away as it proceeds. Thus, hardware to\n> compute a double precision product from single precision operands will\n> normally be only a little more expensive than a single precision multiplier,\n> and much cheaper than a double precision multiplier. Despite this, modern\n> instruction sets tend to provide only instructions that produce a result of\n> the same precision as the operands.^23\n>\n> If an instruction that combines two single precision operands to produce a\n> double precision product was only useful for the quadratic formula, it\n> wouldn't be worth adding to an instruction set. However, this instruction\n> has many other uses. Consider the problem of solving a system of linear\n> equations,\n>\n> a_11x_1 + a_12x_2 +_ \u00b7 \u00b7 \u00b7 + a_1nx_n= b_1 a_21x_1 + a_22x_2 +_ \u00b7 \u00b7 \u00b7 +\n> a_2nx_n= b_2 \u00b7 \u00b7 \u00b7 a_n1x_1 + a_n2x_2 +_ \u00b7 \u00b7 \u00b7+ a_nnx_n= b_n\n>\n> which can be written in matrix form as Ax = b, where\n>\n> Suppose that a solution x^(1) is computed by some method, perhaps Gaussian\n> elimination. There is a simple way to improve the accuracy of the result\n> called iterative improvement. First compute\n>\n> (12) = Ax^(1) - b\n>\n> and then solve the system\n>\n> (13) Ay =\n>\n> Note that if x^(1) is an exact solution, then is the zero vector, as is y.\n> In general, the computation of and y will incur rounding error, so Ay Ax^(1)\n> - b = A(x^(1) - x), where x is the (unknown) true solution. Then y x^(1) -\n> x, so an improved estimate for the solution is\n>\n> (14) x^(2) = x^(1) - y\n>\n> The three steps (12), (13), and (14) can be repeated, replacing x^(1) with\n> x^(2), and x^(2) with x^(3). This argument that x^(^i^ + 1) is more accurate\n> than x^(^i^) is only informal. For more information, see [Golub and Van Loan\n> 1989].\n>\n> When performing iterative improvement, is a vector whose elements are the\n> difference of nearby inexact floating-point numbers, and so can suffer from\n> catastrophic cancellation. Thus iterative improvement is not very useful\n> unless = Ax^(1) - b is computed in double precision. Once again, this is a\n> case of computing the product of two single precision numbers (A and x^(1)),\n> where the full double precision result is needed.\n>\n> To summarize, instructions that multiply two floating-point numbers and\n> return a product with twice the precision of the operands make a useful\n> addition to a floating-point instruction set. Some of the implications of\n> this for compilers are discussed in the next section.\n>\n> ### Languages and Compilers\n>\n> The interaction of compilers and floating-point is discussed in Farnum\n> [1988], and much of the discussion in this section is taken from that paper.\n>\n> #### Ambiguity\n>\n> Ideally, a language definition should define the semantics of the language\n> precisely enough to prove statements about programs. While this is usually\n> true for the integer part of a language, language definitions often have a\n> large grey area when it comes to floating-point. Perhaps this is due to the\n> fact that many language designers believe that nothing can be proven about\n> floating-point, since it entails rounding error. If so, the previous\n> sections have demonstrated the fallacy in this reasoning. This section\n> discusses some common grey areas in language definitions, including\n> suggestions about how to deal with them.\n>\n> Remarkably enough, some languages don't clearly specify that if x is a\n> floating-point variable (with say a value of 3.0/10.0), then every\n> occurrence of (say) 10.0*x must have the same value. For example Ada, which\n> is based on Brown's model, seems to imply that floating-point arithmetic\n> only has to satisfy Brown's axioms, and thus expressions can have one of\n> many possible values. Thinking about floating-point in this fuzzy way stands\n> in sharp contrast to the IEEE model, where the result of each floating-point\n> operation is precisely defined. In the IEEE model, we can prove that\n> (3.0/10.0)*10.0 evaluates to 3 (Theorem 7). In Brown's model, we cannot.\n>\n> Another ambiguity in most language definitions concerns what happens on\n> overflow, underflow and other exceptions. The IEEE standard precisely\n> specifies the behavior of exceptions, and so languages that use the standard\n> as a model can avoid any ambiguity on this point.\n>\n> Another grey area concerns the interpretation of parentheses. Due to\n> roundoff errors, the associative laws of algebra do not necessarily hold for\n> floating-point numbers. For example, the expression (x+y)+z has a totally\n> different answer than x+(y+z) when x = 10^30, y = -10^30 and z = 1 (it is 1\n> in the former case, 0 in the latter). The importance of preserving\n> parentheses cannot be overemphasized. The algorithms presented in theorems\n> 3, 4 and 6 all depend on it. For example, in Theorem 6, the formula x_h = mx\n> - (mx - x) would reduce to x_h = x if it weren't for parentheses, thereby\n> destroying the entire algorithm. A language definition that does not require\n> parentheses to be honored is useless for floating-point calculations.\n>\n> Subexpression evaluation is imprecisely defined in many languages. Suppose\n> that ds is double precision, but x and y are single precision. Then in the\n> expression ds + x*y is the product performed in single or double precision?\n> Another example: in x + m/n where m and n are integers, is the division an\n> integer operation or a floating-point one? There are two ways to deal with\n> this problem, neither of which is completely satisfactory. The first is to\n> require that all variables in an expression have the same type. This is the\n> simplest solution, but has some drawbacks. First of all, languages like\n> Pascal that have subrange types allow mixing subrange variables with integer\n> variables, so it is somewhat bizarre to prohibit mixing single and double\n> precision variables. Another problem concerns constants. In the expression\n> 0.1*x, most languages interpret 0.1 to be a single precision constant. Now\n> suppose the programmer decides to change the declaration of all the\n> floating-point variables from single to double precision. If 0.1 is still\n> treated as a single precision constant, then there will be a compile time\n> error. The programmer will have to hunt down and change every floating-point\n> constant.\n>\n> The second approach is to allow mixed expressions, in which case rules for\n> subexpression evaluation must be provided. There are a number of guiding\n> examples. The original definition of C required that every floating-point\n> expression be computed in double precision [Kernighan and Ritchie 1978].\n> This leads to anomalies like the example at the beginning of this section.\n> The expression 3.0/7.0 is computed in double precision, but if q is a\n> single-precision variable, the quotient is rounded to single precision for\n> storage. Since 3/7 is a repeating binary fraction, its computed value in\n> double precision is different from its stored value in single precision.\n> Thus the comparison q = 3/7 fails. This suggests that computing every\n> expression in the highest precision available is not a good rule.\n>\n> Another guiding example is inner products. If the inner product has\n> thousands of terms, the rounding error in the sum can become substantial.\n> One way to reduce this rounding error is to accumulate the sums in double\n> precision (this will be discussed in more detail in the section Optimizers).\n> If d is a double precision variable, and x[] and y[] are single precision\n> arrays, then the inner product loop will look like d = d + x[i]*y[i]. If the\n> multiplication is done in single precision, than much of the advantage of\n> double precision accumulation is lost, because the product is truncated to\n> single precision just before being added to a double precision variable.\n>\n> A rule that covers both of the previous two examples is to compute an\n> expression in the highest precision of any variable that occurs in that\n> expression. Then q = 3.0/7.0 will be computed entirely in single\n> precision^24 and will have the boolean value true, whereas d = d + x[i]*y[i]\n> will be computed in double precision, gaining the full advantage of double\n> precision accumulation. However, this rule is too simplistic to cover all\n> cases cleanly. If dx and dy are double precision variables, the expression y\n> = x + single(dx-dy) contains a double precision variable, but performing the\n> sum in double precision would be pointless, because both operands are single\n> precision, as is the result.\n>\n> A more sophisticated subexpression evaluation rule is as follows. First\n> assign each operation a tentative precision, which is the maximum of the\n> precisions of its operands. This assignment has to be carried out from the\n> leaves to the root of the expression tree. Then perform a second pass from\n> the root to the leaves. In this pass, assign to each operation the maximum\n> of the tentative precision and the precision expected by the parent. In the\n> case of q = 3.0/7.0, every leaf is single precision, so all the operations\n> are done in single precision. In the case of d = d + x[i]*y[i], the\n> tentative precision of the multiply operation is single precision, but in\n> the second pass it gets promoted to double precision, because its parent\n> operation expects a double precision operand. And in y = x + single(dx-dy),\n> the addition is done in single precision. Farnum [1988] presents evidence\n> that this algorithm in not difficult to implement.\n>\n> The disadvantage of this rule is that the evaluation of a subexpression\n> depends on the expression in which it is embedded. This can have some\n> annoying consequences. For example, suppose you are debugging a program and\n> want to know the value of a subexpression. You cannot simply type the\n> subexpression to the debugger and ask it to be evaluated, because the value\n> of the subexpression in the program depends on the expression it is embedded\n> in. A final comment on subexpressions: since converting decimal constants to\n> binary is an operation, the evaluation rule also affects the interpretation\n> of decimal constants. This is especially important for constants like 0.1\n> which are not exactly representable in binary.\n>\n> Another potential grey area occurs when a language includes exponentiation\n> as one of its built-in operations. Unlike the basic arithmetic operations,\n> the value of exponentiation is not always obvious [Kahan and Coonen 1982].\n> If ** is the exponentiation operator, then (-3)**3 certainly has the value\n> -27. However, (-3.0)**3.0 is problematical. If the ** operator checks for\n> integer powers, it would compute (-3.0)**3.0 as -3.0^3 = -27. On the other\n> hand, if the formula x^y = e^ylog^x is used to define ** for real arguments,\n> then depending on the log function, the result could be a NaN (using the\n> natural definition of log(x) = NaN when x < 0). If the FORTRAN CLOG function\n> is used however, then the answer will be -27, because the ANSI FORTRAN\n> standard defines CLOG(-3.0) to be i + log 3 [ANSI 1978]. The programming\n> language Ada avoids this problem by only defining exponentiation for integer\n> powers, while ANSI FORTRAN prohibits raising a negative number to a real\n> power.\n>\n> In fact, the FORTRAN standard says that\n>\n> Any arithmetic operation whose result is not mathematically defined is\n> prohibited...\n>\n> Unfortunately, with the introduction of \u00b1 by the IEEE standard, the meaning\n> of not mathematically defined is no longer totally clear cut. One definition\n> might be to use the method shown in section Infinity. For example, to\n> determine the value of a^b, consider non-constant analytic functions f and g\n> with the property that f(x) a and g(x) b as x 0. If f(x)^g^(^x^) always\n> approaches the same limit, then this should be the value of a^b. This\n> definition would set 2^ = which seems quite reasonable. In the case of 1.0^,\n> when f(x) = 1 and g(x) = 1/x the limit approaches 1, but when f(x) = 1 - x\n> and g(x) = 1/x the limit is e^-1. So 1.0^, should be a NaN. In the case of\n> 0^0, f(x)^g^(^x^) = e^g(^x^)log ^f^(^x^). Since f and g are analytic and\n> take on the value 0 at 0, f(x) = a_1x^1 + a_2x^2 + ... and g(x) = b_1x^1 +\n> b_2x^2 + .... Thus lim_x_ 0g(x) log f(x) = lim_x_ 0x log(x(a_1 + a_2x +\n> ...)) = lim_x_ 0x log(a_1x) = 0. So f(x)^g^(^x^) e^0 = 1 for all f and g,\n> which means that 0^0 = 1.^25 ^26 Using this definition would unambiguously\n> define the exponential function for all arguments, and in particular would\n> define (-3.0)**3.0 to be -27.\n>\n> #### The IEEE Standard\n>\n> The section The IEEE Standard,\" discussed many of the features of the IEEE\n> standard. However, the IEEE standard says nothing about how these features\n> are to be accessed from a programming language. Thus, there is usually a\n> mismatch between floating-point hardware that supports the standard and\n> programming languages like C, Pascal or FORTRAN. Some of the IEEE\n> capabilities can be accessed through a library of subroutine calls. For\n> example the IEEE standard requires that square root be exactly rounded, and\n> the square root function is often implemented directly in hardware. This\n> functionality is easily accessed via a library square root routine. However,\n> other aspects of the standard are not so easily implemented as subroutines.\n> For example, most computer languages specify at most two floating-point\n> types, while the IEEE standard has four different precisions (although the\n> recommended configurations are single plus single-extended or single,\n> double, and double-extended). Infinity provides another example. Constants\n> to represent \u00b1 could be supplied by a subroutine. But that might make them\n> unusable in places that require constant expressions, such as the\n> initializer of a constant variable.\n>\n> A more subtle situation is manipulating the state associated with a\n> computation, where the state consists of the rounding modes, trap enable\n> bits, trap handlers and exception flags. One approach is to provide\n> subroutines for reading and writing the state. In addition, a single call\n> that can atomically set a new value and return the old value is often\n> useful. As the examples in the section Flags show, a very common pattern of\n> modifying IEEE state is to change it only within the scope of a block or\n> subroutine. Thus the burden is on the programmer to find each exit from the\n> block, and make sure the state is restored. Language support for setting the\n> state precisely in the scope of a block would be very useful here. Modula-3\n> is one language that implements this idea for trap handlers [Nelson 1991].\n>\n> There are a number of minor points that need to be considered when\n> implementing the IEEE standard in a language. Since x - x = +0 for all x,^27\n> (+0) - (+0) = +0. However, -(+0) = -0, thus -x should not be defined as 0 -\n> x. The introduction of NaNs can be confusing, because a NaN is never equal\n> to any other number (including another NaN), so x = x is no longer always\n> true. In fact, the expression x x is the simplest way to test for a NaN if\n> the IEEE recommended function Isnan is not provided. Furthermore, NaNs are\n> unordered with respect to all other numbers, so x y cannot be defined as not\n> x > y. Since the introduction of NaNs causes floating-point numbers to\n> become partially ordered, a compare function that returns one of <, =, >, or\n> unordered can make it easier for the programmer to deal with comparisons.\n>\n> Although the IEEE standard defines the basic floating-point operations to\n> return a NaN if any operand is a NaN, this might not always be the best\n> definition for compound operations. For example when computing the\n> appropriate scale factor to use in plotting a graph, the maximum of a set of\n> values must be computed. In this case it makes sense for the max operation\n> to simply ignore NaNs.\n>\n> Finally, rounding can be a problem. The IEEE standard defines rounding very\n> precisely, and it depends on the current value of the rounding modes. This\n> sometimes conflicts with the definition of implicit rounding in type\n> conversions or the explicit round function in languages. This means that\n> programs which wish to use IEEE rounding can't use the natural language\n> primitives, and conversely the language primitives will be inefficient to\n> implement on the ever increasing number of IEEE machines.\n>\n> #### Optimizers\n>\n> Compiler texts tend to ignore the subject of floating-point. For example Aho\n> et al. [1986] mentions replacing x/2.0 with x*0.5, leading the reader to\n> assume that x/10.0 should be replaced by 0.1*x. However, these two\n> expressions do not have the same semantics on a binary machine, because 0.1\n> cannot be represented exactly in binary. This textbook also suggests\n> replacing x*y-x*z by x*(y-z), even though we have seen that these two\n> expressions can have quite different values when y z. Although it does\n> qualify the statement that any algebraic identity can be used when\n> optimizing code by noting that optimizers should not violate the language\n> definition, it leaves the impression that floating-point semantics are not\n> very important. Whether or not the language standard specifies that\n> parenthesis must be honored, (x+y)+z can have a totally different answer\n> than x+(y+z), as discussed above. There is a problem closely related to\n> preserving parentheses that is illustrated by the following code\n>  \n>  \n>     eps = 1;\n>  \n>  \n>     do eps = 0.5*eps; while (eps + 1 > 1);\n>\n> :\n>\n> This is designed to give an estimate for machine epsilon. If an optimizing\n> compiler notices that eps + 1 > 1 eps > 0, the program will be changed\n> completely. Instead of computing the smallest number x such that 1 x is\n> still greater than x (x e ), it will compute the largest number x for which\n> x/2 is rounded to 0 (x ). Avoiding this kind of \"optimization\" is so\n> important that it is worth presenting one more very useful algorithm that is\n> totally ruined by it.\n>\n> Many problems, such as numerical integration and the numerical solution of\n> differential equations involve computing sums with many terms. Because each\n> addition can potentially introduce an error as large as .5 ulp, a sum\n> involving thousands of terms can have quite a bit of rounding error. A\n> simple way to correct for this is to store the partial summand in a double\n> precision variable and to perform each addition using double precision. If\n> the calculation is being done in single precision, performing the sum in\n> double precision is easy on most computer systems. However, if the\n> calculation is already being done in double precision, doubling the\n> precision is not so simple. One method that is sometimes advocated is to\n> sort the numbers and add them from smallest to largest. However, there is a\n> much more efficient method which dramatically improves the accuracy of sums,\n> namely\n>\n> #### Theorem 8 (Kahan Summation Formula)\n>\n> Suppose that is computed using the following algorithm\n>  \n>  \n>     S = X[1];\n>  \n>  \n>     C = 0;\n>  \n>  \n>     for j = 2 to N {\n>  \n>  \n>     Y = X[j] - C;\n>  \n>  \n>     T = S + Y;\n>  \n>  \n>     C = (T - S) - Y;\n>  \n>  \n>     S = T;\n>  \n>  \n>     }\n>\n> Then the computed sum S is equal to where .\n>\n> Using the naive formula , the computed sum is equal to where |_j| < (n -\n> j)e. Comparing this with the error in the Kahan summation formula shows a\n> dramatic improvement. Each summand is perturbed by only 2e, instead of\n> perturbations as large as ne in the simple formula. Details are in, Errors\n> In Summation.\n>\n> An optimizer that believed floating-point arithmetic obeyed the laws of\n> algebra would conclude that C = [T-S] - Y = [(S+Y)-S] - Y = 0, rendering the\n> algorithm completely useless. These examples can be summarized by saying\n> that optimizers should be extremely cautious when applying algebraic\n> identities that hold for the mathematical real numbers to expressions\n> involving floating-point variables.\n>\n> Another way that optimizers can change the semantics of floating-point code\n> involves constants. In the expression 1.0E-40*x, there is an implicit\n> decimal to binary conversion operation that converts the decimal number to a\n> binary constant. Because this constant cannot be represented exactly in\n> binary, the inexact exception should be raised. In addition, the underflow\n> flag should to be set if the expression is evaluated in single precision.\n> Since the constant is inexact, its exact conversion to binary depends on the\n> current value of the IEEE rounding modes. Thus an optimizer that converts\n> 1.0E-40 to binary at compile time would be changing the semantics of the\n> program. However, constants like 27.5 which are exactly representable in the\n> smallest available precision can be safely converted at compile time, since\n> they are always exact, cannot raise any exception, and are unaffected by the\n> rounding modes. Constants that are intended to be converted at compile time\n> should be done with a constant declaration, such as const pi = 3.14159265.\n>\n> Common subexpression elimination is another example of an optimization that\n> can change floating-point semantics, as illustrated by the following code\n>  \n>  \n>     C = A*B;\n>  \n>  \n>     RndMode = Up\n>  \n>  \n>     D = A*B;\n>\n> Although A*B can appear to be a common subexpression, it is not because the\n> rounding mode is different at the two evaluation sites. Three final\n> examples: x = x cannot be replaced by the boolean constant true, because it\n> fails when x is a NaN; -x = 0 - x fails for x = +0; and x < y is not the\n> opposite of x y, because NaNs are neither greater than nor less than\n> ordinary floating-point numbers.\n>\n> Despite these examples, there are useful optimizations that can be done on\n> floating-point code. First of all, there are algebraic identities that are\n> valid for floating-point numbers. Some examples in IEEE arithmetic are x + y\n> = y + x, 2 \u00d7 x = x + x, 1 \u00d7 x = x, and 0.5\u00d7 x = x/2. However, even these\n> simple identities can fail on a few machines such as CDC and Cray\n> supercomputers. Instruction scheduling and in-line procedure substitution\n> are two other potentially useful optimizations.^28\n>\n> As a final example, consider the expression dx = x*y, where x and y are\n> single precision variables, and dx is double precision. On machines that\n> have an instruction that multiplies two single precision numbers to produce\n> a double precision number, dx = x*y can get mapped to that instruction,\n> rather than compiled to a series of instructions that convert the operands\n> to double and then perform a double to double precision multiply.\n>\n> Some compiler writers view restrictions which prohibit converting (x + y) +\n> z to x + (y + z) as irrelevant, of interest only to programmers who use\n> unportable tricks. Perhaps they have in mind that floating-point numbers\n> model real numbers and should obey the same laws that real numbers do. The\n> problem with real number semantics is that they are extremely expensive to\n> implement. Every time two n bit numbers are multiplied, the product will\n> have 2n bits. Every time two n bit numbers with widely spaced exponents are\n> added, the number of bits in the sum is n + the space between the exponents.\n> The sum could have up to (e^max - e^min) + n bits, or roughly 2\u00b7e^max + n\n> bits. An algorithm that involves thousands of operations (such as solving a\n> linear system) will soon be operating on numbers with many significant bits,\n> and be hopelessly slow. The implementation of library functions such as sin\n> and cos is even more difficult, because the value of these transcendental\n> functions aren't rational numbers. Exact integer arithmetic is often\n> provided by lisp systems and is handy for some problems. However, exact\n> floating-point arithmetic is rarely useful.\n>\n> The fact is that there are useful algorithms (like the Kahan summation\n> formula) that exploit the fact that (x + y) + z x + (y + z), and work\n> whenever the bound\n>\n> a b = (a + b)(1 + )\n>\n> holds (as well as similar bounds for -, \u00d7 and /). Since these bounds hold\n> for almost all commercial hardware, it would be foolish for numerical\n> programmers to ignore such algorithms, and it would be irresponsible for\n> compiler writers to destroy these algorithms by pretending that floating-\n> point variables have real number semantics.\n>\n> ### Exception Handling\n>\n> The topics discussed up to now have primarily concerned systems implications\n> of accuracy and precision. Trap handlers also raise some interesting systems\n> issues. The IEEE standard strongly recommends that users be able to specify\n> a trap handler for each of the five classes of exceptions, and the section\n> Trap Handlers, gave some applications of user defined trap handlers. In the\n> case of invalid operation and division by zero exceptions, the handler\n> should be provided with the operands, otherwise, with the exactly rounded\n> result. Depending on the programming language being used, the trap handler\n> might be able to access other variables in the program as well. For all\n> exceptions, the trap handler must be able to identify what operation was\n> being performed and the precision of its destination.\n>\n> The IEEE standard assumes that operations are conceptually serial and that\n> when an interrupt occurs, it is possible to identify the operation and its\n> operands. On machines which have pipelining or multiple arithmetic units,\n> when an exception occurs, it may not be enough to simply have the trap\n> handler examine the program counter. Hardware support for identifying\n> exactly which operation trapped may be necessary.\n>\n> Another problem is illustrated by the following program fragment.\n>  \n>  \n>     x = y*z;\n>  \n>  \n>     z = x*w;\n>  \n>  \n>     a = b + c;\n>  \n>  \n>     d = a/x;\n>\n> Suppose the second multiply raises an exception, and the trap handler wants\n> to use the value of a. On hardware that can do an add and multiply in\n> parallel, an optimizer would probably move the addition operation ahead of\n> the second multiply, so that the add can proceed in parallel with the first\n> multiply. Thus when the second multiply traps, a = b + c has already been\n> executed, potentially changing the result of a. It would not be reasonable\n> for a compiler to avoid this kind of optimization, because every floating-\n> point operation can potentially trap, and thus virtually all instruction\n> scheduling optimizations would be eliminated. This problem can be avoided by\n> prohibiting trap handlers from accessing any variables of the program\n> directly. Instead, the handler can be given the operands or result as an\n> argument.\n>\n> But there are still problems. In the fragment\n>  \n>  \n>     x = y*z;\n>  \n>  \n>     z = a + b;\n>\n> the two instructions might well be executed in parallel. If the multiply\n> traps, its argument z could already have been overwritten by the addition,\n> especially since addition is usually faster than multiply. Computer systems\n> that support the IEEE standard must provide some way to save the value of z,\n> either in hardware or by having the compiler avoid such a situation in the\n> first place.\n>\n> W. Kahan has proposed using presubstitution instead of trap handlers to\n> avoid these problems. In this method, the user specifies an exception and\n> the value he wants to be used as the result when the exception occurs. As an\n> example, suppose that in code for computing (sin x)/x, the user decides that\n> x = 0 is so rare that it would improve performance to avoid a test for x =\n> 0, and instead handle this case when a 0/0 trap occurs. Using IEEE trap\n> handlers, the user would write a handler that returns a value of 1 and\n> install it before computing sin x/x. Using presubstitution, the user would\n> specify that when an invalid operation occurs, the value 1 should be used.\n> Kahan calls this presubstitution, because the value to be used must be\n> specified before the exception occurs. When using trap handlers, the value\n> to be returned can be computed when the trap occurs.\n>\n> The advantage of presubstitution is that it has a straightforward hardware\n> implementation.^29 As soon as the type of exception has been determined, it\n> can be used to index a table which contains the desired result of the\n> operation. Although presubstitution has some attractive attributes, the\n> widespread acceptance of the IEEE standard makes it unlikely to be widely\n> implemented by hardware manufacturers.\n>\n> ## The Details\n>\n> A number of claims have been made in this paper concerning properties of\n> floating-point arithmetic. We now proceed to show that floating-point is not\n> black magic, but rather is a straightforward subject whose claims can be\n> verified mathematically. This section is divided into three parts. The first\n> part presents an introduction to error analysis, and provides the details\n> for the section Rounding Error. The second part explores binary to decimal\n> conversion, filling in some gaps from the section The IEEE Standard. The\n> third part discusses the Kahan summation formula, which was used as an\n> example in the section Systems Aspects.\n>\n> ### Rounding Error\n>\n> In the discussion of rounding error, it was stated that a single guard digit\n> is enough to guarantee that addition and subtraction will always be accurate\n> (Theorem 2). We now proceed to verify this fact. Theorem 2 has two parts,\n> one for subtraction and one for addition. The part for subtraction is\n>\n> #### Theorem 9\n>\n> If x and y are positive floating-point numbers in a format with parameters\n> and p, and if subtraction is done with p + 1 digits (i.e. one guard digit),\n> then the relative rounding error in the result is less than\n>\n> e 2e.\n>\n> #### Proof\n>\n> Interchange x and y if necessary so that x > y. It is also harmless to scale\n> x and y so that x is represented by x_0.x_1 ... x_p - 1 \u00d7 ^0. If y is\n> represented as y_0.y_1 ... y_p-1, then the difference is exact. If y is\n> represented as 0.y_1 ... y_p, then the guard digit ensures that the computed\n> difference will be the exact difference rounded to a floating-point number,\n> so the rounding error is at most e. In general, let y = 0.0 ... 0y_k + 1 ...\n> y_k + _p and be y truncated to p + 1 digits. Then\n>\n> (15) y - < ( - 1)(^-^p^ - 1 + ^-^p^ - 2 + ...^ + ^-^p^ - ^k).\n>\n> From the definition of guard digit, the computed value of x - y is x -\n> rounded to be a floating-point number, that is, (x - ) + , where the\n> rounding error satisfies\n>\n> (16) || (/2)^-^p.\n>\n> The exact difference is x - y, so the error is (x - y) - (x - + ) = - y + .\n> There are three cases. If x - y 1 then the relative error is bounded by\n>\n> (17) ^-^p [( - 1)(^-1 + ... + ^-^k) + /2] < ^-^p(1 + /2) .\n>\n> Secondly, if x - < 1, then = 0. Since the smallest that x - y can be is\n>\n> > (- 1)(^-1 + ... + ^-^k), where = - 1,\n>\n> in this case the relative error is bounded by\n>\n> (18) .\n>\n> The final case is when x - y < 1 but x - 1. The only way this could happen\n> is if x - = 1, in which case = 0. But if = 0, then (18) applies, so that\n> again the relative error is bounded by ^-^p < ^-^p(1 + /2). z\n>\n> When = 2, the bound is exactly 2e, and this bound is achieved for x= 1 + 2^2\n> - ^p and y = 2^1 - ^p - 2^1 - 2^p in the limit as p . When adding numbers of\n> the same sign, a guard digit is not necessary to achieve good accuracy, as\n> the following result shows.\n>\n> #### Theorem 10\n>\n> If x 0 and y 0, then the relative error in computing x + y is at most 2,\n> even if no guard digits are used.\n>\n> #### Proof\n>\n> The algorithm for addition with k guard digits is similar to that for\n> subtraction. If x y, shift y right until the radix points of x and y are\n> aligned. Discard any digits shifted past the p + k position. Compute the sum\n> of these two p + k digit numbers exactly. Then round to p digits.\n> We will verify the theorem when no guard digits are used; the general case\n> is similar. There is no loss of generality in assuming that x y 0 and that x\n> is scaled to be of the form d.dd...d \u00d7 ^0. First, assume there is no carry\n> out. Then the digits shifted off the end of y have a value less than ^-^p^ +\n> 1, and the sum is at least 1, so the relative error is less than ^-^p^+1/1 =\n> 2e. If there is a carry out, then the error from shifting must be added to\n> the rounding error of\n>\n> .\n>\n> The sum is at least , so the relative error is less than\n>\n> 2\\. z\n>\n> It is obvious that combining these two theorems gives Theorem 2. Theorem 2\n> gives the relative error for performing one operation. Comparing the\n> rounding error of x^2 - y^2 and (x + y) (x - y) requires knowing the\n> relative error of multiple operations. The relative error of x y is _1 = [(x\n> y) - (x - y)] / (x - y), which satisfies |_1| 2e. Or to write it another way\n>\n> (19) x y = (x - y) (1 + _1), |_1| 2e\n>\n> Similarly\n>\n> (20) x y = (x + y) (1 + _2), |_2| 2e\n>\n> Assuming that multiplication is performed by computing the exact product and\n> then rounding, the relative error is at most .5 ulp, so\n>\n> (21) u v = uv (1 + _3), |_3| e\n>\n> for any floating-point numbers u and v. Putting these three equations\n> together (letting u = x y and v = x y) gives\n>\n> (22) (x y) (x y) = (x - y) (1 + _1) (x + y) (1 + _2) (1 + _3)\n>\n> So the relative error incurred when computing (x - y) (x + y) is\n>\n> (23)\n>\n> This relative error is equal to _1 + _2 + _3 + _1_2 + _1_3 + _2_3 + _1_2_3,\n> which is bounded by 5 + 8^2. In other words, the maximum relative error is\n> about 5 rounding errors (since e is a small number, e^2 is almost\n> negligible).\n>\n> A similar analysis of (x x) (y y) cannot result in a small value for the\n> relative error, because when two nearby values of x and y are plugged into\n> x^2 - y^2, the relative error will usually be quite large. Another way to\n> see this is to try and duplicate the analysis that worked on (x y) (x y),\n> yielding\n>\n> (x x) (y y) = [x^2(1 + _1) - y^2(1 + _2)] (1 + _3) = ((x^2 - y^2) (1 + _1) +\n> (_1 - _2)y^2) (1 + _3)\n>\n> When x and y are nearby, the error term (_1 - _2)y^2 can be as large as the\n> result x^2 - y^2. These computations formally justify our claim that (x - y)\n> (x + y) is more accurate than x^2 - y^2.\n>\n> We next turn to an analysis of the formula for the area of a triangle. In\n> order to estimate the maximum error that can occur when computing with (7),\n> the following fact will be needed.\n>\n> #### Theorem 11\n>\n> If subtraction is performed with a guard digit, and y/2 x 2y, then x - y is\n> computed exactly.\n>\n> #### Proof\n>\n> Note that if x and y have the same exponent, then certainly x y is exact.\n> Otherwise, from the condition of the theorem, the exponents can differ by at\n> most 1. Scale and interchange x and y if necessary so that 0 y x, and x is\n> represented as x_0.x_1 ... x_p - 1 and y as 0.y_1 ... y_p. Then the\n> algorithm for computing x y will compute x - y exactly and round to a\n> floating-point number. If the difference is of the form 0.d_1 ... d_p, the\n> difference will already be p digits long, and no rounding is necessary.\n> Since x 2y, x - y y, and since y is of the form 0.d_1 ... d_p, so is x - y.\n> z\n>\n> When > 2, the hypothesis of Theorem 11 cannot be replaced by y/x y; the\n> stronger condition y/2 x 2y is still necessary. The analysis of the error in\n> (x - y) (x + y), immediately following the proof of Theorem 10, used the\n> fact that the relative error in the basic operations of addition and\n> subtraction is small (namely equations (19) and (20)). This is the most\n> common kind of error analysis. However, analyzing formula (7) requires\n> something more, namely Theorem 11, as the following proof will show.\n>\n> #### Theorem 12\n>\n> If subtraction uses a guard digit, and if a,b and c are the sides of a\n> triangle (a b c), then the relative error in computing (a + (b + c))(c - (a\n> - b))(c + (a - b))(a +(b - c)) is at most 16, provided e < .005.\n>\n> #### Proof\n>\n> Let's examine the factors one by one. From Theorem 10, b c = (b + c) (1 +\n> _1), where _1 is the relative error, and |_1| 2. Then the value of the first\n> factor is\n>\n> (a (b c)) = (a + (b c)) (1 + _2) = (a + (b + c) (1 + _1))(1 + _2),\n>\n> and thus\n>\n> (a + b + c) (1 - 2)^2 [a + (b + c) (1 - 2)] \u00b7 (1-2) a (b c) [a + (b + c) (1\n> + 2)] (1 + 2) (a + b + c) (1 + 2)^2\n>\n> This means that there is an _1 so that\n>\n> (24) (a (b c)) = (a + b + c) (1 + _1)^2, |_1| 2.\n>\n> The next term involves the potentially catastrophic subtraction of c and a\n> b, because a b may have rounding error. Because a, b and c are the sides of\n> a triangle, a b+ c, and combining this with the ordering c b a gives a b + c\n> 2b 2a. So a - b satisfies the conditions of Theorem 11. This means that a -\n> b = a b is exact, hence c (a - b) is a harmless subtraction which can be\n> estimated from Theorem 9 to be\n>\n> (25) (c (a b)) = (c - (a - b)) (1 + _2), |_2| 2\n>\n> The third term is the sum of two exact positive quantities, so\n>\n> (26) (c (a b)) = (c + (a - b)) (1 + _3), |_3| 2\n>\n> Finally, the last term is\n>\n> (27) (a (b c)) = (a + (b - c)) (1 + _4)^2, |_4| 2,\n>\n> using both Theorem 9 and Theorem 10. If multiplication is assumed to be\n> exactly rounded, so that x y = xy(1 + ) with || , then combining (24), (25),\n> (26) and (27) gives\n>\n> (a (b c)) (c (a b)) (c (a b)) (a (b c)) (a + (b + c)) (c - (a - b)) (c + (a\n> - b)) (a + (b - c)) E\n>\n> where\n>\n> E = (1 + _1)^2 (1 + _2) (1 + _3) (1 +_4)^2 (1 + _1)(1 + _2) (1 + _3)\n>\n> An upper bound for E is (1 + 2)^6(1 + )^3, which expands out to 1 + 15 +\n> O(^2). Some writers simply ignore the O(e^2) term, but it is easy to account\n> for it. Writing (1 + 2)^6(1 + )^3 = 1 + 15 + R(), R() is a polynomial in e\n> with positive coefficients, so it is an increasing function of . Since\n> R(.005) = .505, R() < 1 for all < .005, and hence E (1 + 2)^6(1 + )^3 < 1 +\n> 16. To get a lower bound on E, note that 1 - 15 - R() < E, and so when <\n> .005, 1 - 16 < (1 - 2)^6(1 - )^3. Combining these two bounds yields 1 - 16 <\n> E < 1 + 16. Thus the relative error is at most 16. z\n>\n> Theorem 12 certainly shows that there is no catastrophic cancellation in\n> formula (7). So although it is not necessary to show formula (7) is\n> numerically stable, it is satisfying to have a bound for the entire formula,\n> which is what Theorem 3 of Cancellation gives.\n>\n> #### Proof of Theorem 3\n>\n> Let\n>\n> q = (a + (b + c)) (c - (a - b)) (c + (a - b)) (a + (b - c))\n>\n> and\n>\n> Q = (a (b c)) (c (a b)) (c (a b)) (a (b c)).\n>\n> Then, Theorem 12 shows that Q = q(1 + ), with 16. It is easy to check that\n>\n> (28)\n>\n> provided .04/(.52)^2 .15, and since || 16 16(.005) = .08, does satisfy the\n> condition. Thus\n>\n> ,\n>\n> with |_1| .52|| 8.5. If square roots are computed to within .5 ulp, then the\n> error when computing is (1 + _1)(1 + _2), with |_2| . If = 2, then there is\n> no further error committed when dividing by 4. Otherwise, one more factor 1\n> + _3 with |_3| is necessary for the division, and using the method in the\n> proof of Theorem 12, the final error bound of (1 +_1) (1 + _2) (1 + _3) is\n> dominated by 1 + _4, with |_4| 11. z\n>\n> To make the heuristic explanation immediately following the statement of\n> Theorem 4 precise, the next theorem describes just how closely \u03bc(x)\n> approximates a constant.\n>\n> #### Theorem 13\n>\n> If \u03bc(x) = ln(1 + x)/x, then for 0 x , \u03bc(x) 1 and the derivative satisfies\n> |\u03bc'(x)| .\n>\n> #### Proof\n>\n> Note that \u03bc(x) = 1 - x/2 + x^2/3 - ... is an alternating series with\n> decreasing terms, so for x 1, \u03bc(x) 1 - x/2 1/2. It is even easier to see\n> that because the series for \u03bc is alternating, \u03bc(x) 1. The Taylor series of\n> \u03bc'(x) is also alternating, and if x has decreasing terms, so - \u03bc'(x) - +\n> 2x/3, or - \u03bc'(x) 0, thus |\u03bc'(x)| . z\n>\n> #### Proof of Theorem 4\n>\n> Since the Taylor series for ln\n>\n> is an alternating series, 0 < x - ln(1 + x) < x^2/2, the relative error\n> incurred when approximating ln(1 + x) by x is bounded by x/2. If 1 x = 1,\n> then |x| < , so the relative error is bounded by /2.\n> When 1 x 1, define via 1 x = 1 + . Then since 0 x < 1, (1 x) 1 = . If\n> division and logarithms are computed to within ulp, then the computed value\n> of the expression ln(1 + x)/((1 + x) - 1) is\n>\n> (29) (1 + _1) (1 + _2) = (1 + _1) (1 + _2) = \u03bc( ) (1 + _1) (1 + _2)\n>\n> where |_1| and |_2| . To estimate \u03bc( ), use the mean value theorem, which\n> says that\n>\n> (30) \u03bc( ) - \u03bc(x) = ( - x)\u03bc'()\n>\n> for some between x and . From the definition of , it follows that | - x| , and combining this with Theorem 13 gives |\u03bc( ) - \u03bc(x)| /2, or |\u03bc( )/\u03bc(x) - 1| /(2|\u03bc(x)|) which means that \u03bc( ) = \u03bc(x) (1 + _3), with |_3| . Finally, multiplying by x introduces a final _4, so the computed value of\n>\n> x\u00b7ln(1 x)/((1 x) 1)\n>\n> is\n>\n> It is easy to check that if < 0.1, then\n>\n> (1 + _1) (1 + _2) (1 + _3) (1 + _4) = 1 + ,\n>\n> with || 5. z\n>\n> An interesting example of error analysis using formulas (19), (20), and (21)\n> occurs in the quadratic formula . The section Cancellation, explained how\n> rewriting the equation will eliminate the potential cancellation caused by\n> the \u00b1 operation. But there is another potential cancellation that can occur\n> when computing d = b_2 - 4ac. This one cannot be eliminated by a simple\n> rearrangement of the formula. Roughly speaking, when b^2 4ac, rounding error\n> can contaminate up to half the digits in the roots computed with the\n> quadratic formula. Here is an informal proof (another approach to estimating\n> the error in the quadratic formula appears in Kahan [1972]).\n>\n> If b^2 4ac, rounding error can contaminate up to half the digits in the\n> roots computed with the quadratic formula .\n>\n> Proof: Write (b b) (4a c) = (b^2(1 + _1) - 4ac(1 + _2)) (1 + _3), where |_i|\n> . ^30 Using d = b^2 - 4ac, this can be rewritten as (d(1 + _1) - 4ac(_2 -\n> _1)) (1 + _3). To get an estimate for the size of this error, ignore second\n> order terms in _i, in which case the absolute error is d(_1 + _3) - 4ac_4,\n> where |_4| = |_1 - _2| 2. Since , the first term d(_1 + _3) can be ignored.\n> To estimate the second term, use the fact that ax^2 + bx + c = a(x - r_1) (x\n> - r_2), so ar_1r_2 = c. Since b^2 4ac, then r_1 _r_2, so the second error\n> term is . Thus the computed value of is\n>\n> .\n>\n> The inequality\n>\n> shows that\n>\n> ,\n>\n> where\n>\n> ,\n>\n> so the absolute error in a is about . Since _4 ^-^p, , and thus the absolute\n> error of destroys the bottom half of the bits of the roots r_1 r_2. In other\n> words, since the calculation of the roots involves computing with , and this\n> expression does not have meaningful bits in the position corresponding to\n> the lower order half of r_i, then the lower order bits of r_i cannot be\n> meaningful. z\n>\n> Finally, we turn to the proof of Theorem 6. It is based on the following\n> fact, which is proven in the section Theorem 14 and Theorem 8.\n>\n> #### Theorem 14\n>\n> Let 0 < k < p, and set m = ^k + 1, and assume that floating-point operations\n> are exactly rounded. Then (m x) (m x x) is exactly equal to x rounded to p -\n> k significant digits. More precisely, x is rounded by taking the significand\n> of x, imagining a radix point just left of the k least significant digits\n> and rounding to an integer.\n>\n> #### Proof of Theorem 6\n>\n> By Theorem 14, x_h is x rounded to p - k = places. If there is no carry out,\n> then certainly x_h can be represented with significant digits. Suppose there\n> is a carry-out. If x = x_0.x_1 ... x_p - 1 \u00d7 ^e, then rounding adds 1 to x_p\n> - _k_ - 1, and the only way there can be a carry-out is if x_p - _k_ - 1 = -\n> 1, but then the low order digit of x_h is 1 + x_p - _k_- 1 = 0, and so again\n> x_h is representable in digits.\n> To deal with x_l, scale x to be an integer satisfying ^p^ - 1 x ^p - 1. Let\n> where is the p - k high order digits of x, and is the k low order digits.\n> There are three cases to consider. If , then rounding x to p - k places is\n> the same as chopping and , and . Since has at most k digits, if p is even,\n> then has at most k = = digits. Otherwise, = 2 and is representable with k -\n> 1 significant bits. The second case is when , and then computing x_h\n> involves rounding up, so x_h = + ^k, and x_l = x - x_h = x - -^k = - ^k.\n> Once again, has at most k digits, so is representable with p/2 digits.\n> Finally, if = (/2)^k^ - 1, then x_h = or + ^k depending on whether there is\n> a round up. So x_l is either (/2)^k^ - 1 or (/2)^k^ - 1 - ^k = -^k/2, both\n> of which are represented with 1 digit. z\n>\n> Theorem 6 gives a way to express the product of two working precision\n> numbers exactly as a sum. There is a companion formula for expressing a sum\n> exactly. If |x| |y| then x + y = (x y) + (x (x y)) y [Dekker 1971; Knuth\n> 1981, Theorem C in section 4.2.2]. However, when using exactly rounded\n> operations, this formula is only true for = 2, and not for = 10 as the\n> example x = .99998, y = .99997 shows.\n>\n> ### Binary to Decimal Conversion\n>\n> Since single precision has p = 24, and 2^24 < 10^8, you might expect that\n> converting a binary number to 8 decimal digits would be sufficient to\n> recover the original binary number. However, this is not the case.\n>\n> #### Theorem 15\n>\n> When a binary IEEE single precision number is converted to the closest eight\n> digit decimal number, it is not always possible to uniquely recover the\n> binary number from the decimal one. However, if nine decimal digits are\n> used, then converting the decimal number to the closest binary number will\n> recover the original floating-point number.\n>\n> #### Proof\n>\n> Binary single precision numbers lying in the half open interval [10^3, 2^10)\n> = [1000, 1024) have 10 bits to the left of the binary point, and 14 bits to\n> the right of the binary point. Thus there are (2^10 - 10^3)2^14 = 393,216\n> different binary numbers in that interval. If decimal numbers are\n> represented with 8 digits, then there are (2^10 - 10^3)10^4 = 240,000\n> decimal numbers in the same interval. There is no way that 240,000 decimal\n> numbers could represent 393,216 different binary numbers. So 8 decimal\n> digits are not enough to uniquely represent each single precision binary\n> number.\n> To show that 9 digits are sufficient, it is enough to show that the spacing\n> between binary numbers is always greater than the spacing between decimal\n> numbers. This will ensure that for each decimal number N, the interval\n>\n> [N - ulp, N + ulp]\n>\n> contains at most one binary number. Thus each binary number rounds to a\n> unique decimal number which in turn rounds to a unique binary number.\n> To show that the spacing between binary numbers is always greater than the\n> spacing between decimal numbers, consider an interval [10^n, 10^n^ + 1]. On\n> this interval, the spacing between consecutive decimal numbers is 10^(^n^ +\n> 1) - 9. On [10^n, 2^m], where m is the smallest integer so that 10^n < 2^m,\n> the spacing of binary numbers is 2^m^ - 24, and the spacing gets larger\n> further on in the interval. Thus it is enough to check that 10^(^n^ + 1) - 9\n> < 2^m^ - 24. But in fact, since 10^n < 2^m, then 10^(^n^ + 1) - 9 =\n> 10^n10^-8 < 2^m10^-8 < 2^m2^-24. z\n>\n> The same argument applied to double precision shows that 17 decimal digits\n> are required to recover a double precision number.\n>\n> Binary-decimal conversion also provides another example of the use of flags.\n> Recall from the section Precision, that to recover a binary number from its\n> decimal expansion, the decimal to binary conversion must be computed\n> exactly. That conversion is performed by multiplying the quantities N and\n> 10^|^P^| (which are both exact if p < 13) in single-extended precision and\n> then rounding this to single precision (or dividing if p < 0; both cases are\n> similar). Of course the computation of N \u00b7 10^|^P^| cannot be exact; it is\n> the combined operation round(N \u00b7 10^|^P^|) that must be exact, where the\n> rounding is from single-extended to single precision. To see why it might\n> fail to be exact, take the simple case of = 10, p = 2 for single, and p = 3\n> for single-extended. If the product is to be 12.51, then this would be\n> rounded to 12.5 as part of the single-extended multiply operation. Rounding\n> to single precision would give 12. But that answer is not correct, because\n> rounding the product to single precision should give 13. The error is due to\n> double rounding.\n>\n> By using the IEEE flags, double rounding can be avoided as follows. Save the\n> current value of the inexact flag, and then reset it. Set the rounding mode\n> to round-to-zero. Then perform the multiplication N \u00b7 10^|^P^|. Store the\n> new value of the inexact flag in ixflag, and restore the rounding mode and\n> inexact flag. If ixflag is 0, then N \u00b7 10^|^P^| is exact, so round(N \u00b7\n> 10^|^P^|) will be correct down to the last bit. If ixflag is 1, then some\n> digits were truncated, since round-to-zero always truncates. The significand\n> of the product will look like 1.b_1...b_22b_23...b_31. A double rounding\n> error may occur if b_23 ...b_31 = 10...0. A simple way to account for both\n> cases is to perform a logical OR of ixflag with b_31. Then round(N \u00b7\n> 10^|^P^|) will be computed correctly in all cases.\n>\n> ### Errors In Summation\n>\n> The section Optimizers, mentioned the problem of accurately computing very\n> long sums. The simplest approach to improving accuracy is to double the\n> precision. To get a rough estimate of how much doubling the precision\n> improves the accuracy of a sum, let s_1 = x_1, s_2 = s_1 x_2..., s_i = s_i -\n> 1 x_i. Then s_i = (1 + _i) (s_i - 1 + x_i), where _i , and ignoring second\n> order terms in _i gives\n>\n> (31)\n>\n> The first equality of (31) shows that the computed value of is the same as\n> if an exact summation was performed on perturbed values of x_j. The first\n> term x_1 is perturbed by n, the last term x_n by only . The second equality\n> in (31) shows that error term is bounded by . Doubling the precision has the\n> effect of squaring . If the sum is being done in an IEEE double precision\n> format, 1/ 10^16, so that for any reasonable value of n. Thus, doubling the\n> precision takes the maximum perturbation of n and changes it to . Thus the 2\n> error bound for the Kahan summation formula (Theorem 8) is not as good as\n> using double precision, even though it is much better than single precision.\n>\n> For an intuitive explanation of why the Kahan summation formula works,\n> consider the following diagram of the procedure.\n>\n> Each time a summand is added, there is a correction factor C which will be\n> applied on the next loop. So first subtract the correction C computed in the\n> previous loop from X_j, giving the corrected summand Y. Then add this\n> summand to the running sum S. The low order bits of Y (namely Y_l) are lost\n> in the sum. Next compute the high order bits of Y by computing T - S. When Y\n> is subtracted from this, the low order bits of Y will be recovered. These\n> are the bits that were lost in the first sum in the diagram. They become the\n> correction factor for the next loop. A formal proof of Theorem 8, taken from\n> Knuth [1981] page 572, appears in the section Theorem 14 and Theorem 8.\"\n>\n> ## Summary\n>\n> It is not uncommon for computer system designers to neglect the parts of a\n> system related to floating-point. This is probably due to the fact that\n> floating-point is given very little (if any) attention in the computer\n> science curriculum. This in turn has caused the apparently widespread belief\n> that floating-point is not a quantifiable subject, and so there is little\n> point in fussing over the details of hardware and software that deal with\n> it.\n>\n> This paper has demonstrated that it is possible to reason rigorously about\n> floating-point. For example, floating-point algorithms involving\n> cancellation can be proven to have small relative errors if the underlying\n> hardware has a guard digit, and there is an efficient algorithm for binary-\n> decimal conversion that can be proven to be invertible, provided that\n> extended precision is supported. The task of constructing reliable floating-\n> point software is made much easier when the underlying computer system is\n> supportive of floating-point. In addition to the two examples just mentioned\n> (guard digits and extended precision), the section Systems Aspects of this\n> paper has examples ranging from instruction set design to compiler\n> optimization illustrating how to better support floating-point.\n>\n> The increasing acceptance of the IEEE floating-point standard means that\n> codes that utilize features of the standard are becoming ever more portable.\n> The section The IEEE Standard, gave numerous examples illustrating how the\n> features of the IEEE standard can be used in writing practical floating-\n> point codes.\n>\n> ## Acknowledgments\n>\n> This article was inspired by a course given by W. Kahan at Sun Microsystems\n> from May through July of 1988, which was very ably organized by David Hough\n> of Sun. My hope is to enable others to learn about the interaction of\n> floating-point and computer systems without having to get up in time to\n> attend 8:00 a.m. lectures. Thanks are due to Kahan and many of my colleagues\n> at Xerox PARC (especially John Gilbert) for reading drafts of this paper and\n> providing many useful comments. Reviews from Paul Hilfinger and an anonymous\n> referee also helped improve the presentation.\n>\n> ## References\n>\n> Aho, Alfred V., Sethi, R., and Ullman J. D. 1986. Compilers: Principles,\n> Techniques and Tools, Addison-Wesley, Reading, MA.\n>\n> ANSI 1978. American National Standard Programming Language FORTRAN, ANSI\n> Standard X3.9-1978, American National Standards Institute, New York, NY.\n>\n> Barnett, David 1987. A Portable Floating-Point Environment, unpublished\n> manuscript.\n>\n> Brown, W. S. 1981. A Simple but Realistic Model of Floating-Point\n> Computation, ACM Trans. on Math. Software 7(4), pp. 445-480.\n>\n> Cody, W. J et. al. 1984. A Proposed Radix- and Word-length-independent\n> Standard for Floating-point Arithmetic, IEEE Micro 4(4), pp. 86-100.\n>\n> Cody, W. J. 1988. Floating-Point Standards -- Theory and Practice, in\n> \"Reliability in Computing: the role of interval methods in scientific\n> computing\", ed. by Ramon E. Moore, pp. 99-107, Academic Press, Boston, MA.\n>\n> Coonen, Jerome 1984. Contributions to a Proposed Standard for Binary\n> Floating-Point Arithmetic, PhD Thesis, Univ. of California, Berkeley.\n>\n> Dekker, T. J. 1971. A Floating-Point Technique for Extending the Available\n> Precision, Numer. Math. 18(3), pp. 224-242.\n>\n> Demmel, James 1984. Underflow and the Reliability of Numerical Software,\n> SIAM J. Sci. Stat. Comput. 5(4), pp. 887-919.\n>\n> Farnum, Charles 1988. Compiler Support for Floating-point Computation,\n> Software-Practice and Experience, 18(7), pp. 701-709.\n>\n> Forsythe, G. E. and Moler, C. B. 1967. Computer Solution of Linear Algebraic\n> Systems, Prentice-Hall, Englewood Cliffs, NJ.\n>\n> Goldberg, I. Bennett 1967. 27 Bits Are Not Enough for 8-Digit Accuracy,\n> Comm. of the ACM. 10(2), pp 105-106.\n>\n> Goldberg, David 1990. Computer Arithmetic, in \"Computer Architecture: A\n> Quantitative Approach\", by David Patterson and John L. Hennessy, Appendix A,\n> Morgan Kaufmann, Los Altos, CA.\n>\n> Golub, Gene H. and Van Loan, Charles F. 1989. Matrix Computations, 2nd\n> edition,The Johns Hopkins University Press, Baltimore Maryland.\n>\n> Graham, Ronald L. , Knuth, Donald E. and Patashnik, Oren. 1989. Concrete\n> Mathematics, Addison-Wesley, Reading, MA, p.162.\n>\n> Hewlett Packard 1982. HP-15C Advanced Functions Handbook.\n>\n> IEEE 1987. IEEE Standard 754-1985 for Binary Floating-point Arithmetic,\n> IEEE, (1985). Reprinted in SIGPLAN 22(2) pp. 9-25.\n>\n> Kahan, W. 1972. A Survey Of Error Analysis, in Information Processing 71,\n> Vol 2, pp. 1214 - 1239 (Ljubljana, Yugoslavia), North Holland, Amsterdam.\n>\n> Kahan, W. 1986. Calculating Area and Angle of a Needle-like Triangle,\n> unpublished manuscript.\n>\n> Kahan, W. 1987. Branch Cuts for Complex Elementary Functions, in \"The State\n> of the Art in Numerical Analysis\", ed. by M.J.D. Powell and A. Iserles (Univ\n> of Birmingham, England), Chapter 7, Oxford University Press, New York.\n>\n> Kahan, W. 1988. Unpublished lectures given at Sun Microsystems, Mountain\n> View, CA.\n>\n> Kahan, W. and Coonen, Jerome T. 1982. The Near Orthogonality of Syntax,\n> Semantics, and Diagnostics in Numerical Programming Environments, in \"The\n> Relationship Between Numerical Computation And Programming Languages\", ed.\n> by J. K. Reid, pp. 103-115, North-Holland, Amsterdam.\n>\n> Kahan, W. and LeBlanc, E. 1985. Anomalies in the IBM Acrith Package, Proc.\n> 7th IEEE Symposium on Computer Arithmetic (Urbana, Illinois), pp. 322-331.\n>\n> Kernighan, Brian W. and Ritchie, Dennis M. 1978. The C Programming Language,\n> Prentice-Hall, Englewood Cliffs, NJ.\n>\n> Kirchner, R. and Kulisch, U. 1987. Arithmetic for Vector Processors, Proc.\n> 8th IEEE Symposium on Computer Arithmetic (Como, Italy), pp. 256-269.\n>\n> Knuth, Donald E., 1981. The Art of Computer Programming, Volume II, Second\n> Edition, Addison-Wesley, Reading, MA.\n>\n> Kulisch, U. W., and Miranker, W. L. 1986. The Arithmetic of the Digital\n> Computer: A New Approach, SIAM Review 28(1), pp 1-36.\n>\n> Matula, D. W. and Kornerup, P. 1985. Finite Precision Rational Arithmetic:\n> Slash Number Systems, IEEE Trans. on Comput. C-34(1), pp 3-18.\n>\n> Nelson, G. 1991. Systems Programming With Modula-3, Prentice-Hall, Englewood\n> Cliffs, NJ.\n>\n> Reiser, John F. and Knuth, Donald E. 1975. Evading the Drift in Floating-\n> point Addition, Information Processing Letters 3(3), pp 84-87.\n>\n> Sterbenz, Pat H. 1974. Floating-Point Computation, Prentice-Hall, Englewood\n> Cliffs, NJ.\n>\n> Swartzlander, Earl E. and Alexopoulos, Aristides G. 1975. The Sign/Logarithm\n> Number System, IEEE Trans. Comput. C-24(12), pp. 1238-1242.\n>\n> Walther, J. S., 1971. A unified algorithm for elementary functions,\n> Proceedings of the AFIP Spring Joint Computer Conf. 38, pp. 379-385.\n>\n> ## Theorem 14 and Theorem 8\n>\n> This section contains two of the more technical proofs that were omitted\n> from the text.\n>\n> ### Theorem 14\n>\n> Let 0 < k < p, and set m = ^k + 1, and assume that floating-point operations\n> are exactly rounded. Then (m x) (m x x) is exactly equal to x rounded to p -\n> k significant digits. More precisely, x is rounded by taking the significand\n> of x, imagining a radix point just left of the k least significant digits,\n> and rounding to an integer.\n>\n> ### Proof\n>\n> The proof breaks up into two cases, depending on whether or not the\n> computation of mx = ^kx + x has a carry-out or not.\n>\n> Assume there is no carry out. It is harmless to scale x so that it is an\n> integer. Then the computation of mx = x + ^kx looks like this:\n>\n> aa...aabb...bb + aa...aabb...bb zz...zzbb...bb\n> where x has been partitioned into two parts. The low order k digits are\n> marked b and the high order p - k digits are marked a. To compute m x from\n> mx involves rounding off the low order k digits (the ones marked with b) so\n>\n> (32) m x = mx - x mod(^k) + r^k\n>\n> The value of r is 1 if .bb...b is greater than and 0 otherwise. More\n> precisely\n>\n> (33) r = 1 if a.bb...b rounds to a + 1, r = 0 otherwise.\n>\n> Next compute m x - x = mx - x mod(^k) + r^k - x = ^k(x + r) - x mod(^k). The\n> picture below shows the computation of m x - x rounded, that is, (m x) x.\n> The top line is ^k(x + r), where B is the digit that results from adding r\n> to the lowest order digit b.\n>\n> aa...aabb...bB00...00 - bb...bb zz... zzZ00...00\n> If .bb...b < then r = 0, subtracting causes a borrow from the digit marked\n> B, but the difference is rounded up, and so the net effect is that the\n> rounded difference equals the top line, which is ^kx. If .bb...b > then r =\n> 1, and 1 is subtracted from B because of the borrow, so the result is ^kx.\n> Finally consider the case .bb...b = . If r = 0 then B is even, Z is odd, and\n> the difference is rounded up, giving ^kx. Similarly when r = 1, B is odd, Z\n> is even, the difference is rounded down, so again the difference is ^kx. To\n> summarize\n>\n> (34) (m x) x = ^kx\n>\n> Combining equations (32) and (34) gives (m x) - (m x x) = x - x mod(^k) +\n> \u00b7^k. The result of performing this computation is\n>\n> r00...00 + aa...aabb...bb - bb...bb aa...aA00...00\n> The rule for computing r, equation (33), is the same as the rule for\n> rounding a... ab...b to p - k places. Thus computing mx - (mx - x) in\n> floating-point arithmetic precision is exactly equal to rounding x to p - k\n> places, in the case when x + ^kx does not carry out.\n>\n> When x + ^kx does carry out, then mx = ^kx + x looks like this:\n>\n> aa...aabb...bb + aa...aabb...bb zz...zZbb...bb\n>\n> Thus, m x = mx - x mod(^k) + w^k, where w = -Z if Z < /2, but the exact\n> value of w is unimportant. Next, m x - x = ^kx - x mod(^k) + w^k. In a\n> picture\n>\n> aa...aabb...bb00...00 - bb... bb + w zz ... zZbb ...bb^31\n> Rounding gives (m x) x = ^kx + w^k - r^k, where r = 1 if .bb...b > or if\n> .bb...b = and b_0 = 1.^32 Finally,\n>\n> (m x) - (m x x) = mx - x mod(^k) + w^k - (^kx + w^k - r^k) = x - x mod(^k) +\n> r^k.\n>\n> And once again, r = 1 exactly when rounding a...ab...b to p - k places\n> involves rounding up. Thus Theorem 14 is proven in all cases. z\n>\n> #### Theorem 8 (Kahan Summation Formula)\n>\n> Suppose that is computed using the following algorithm\n>  \n>  \n>     S = X [1];\n>\n> Then the computed sum S is equal to S = x_j (1 + _j) + O(N^2) |x_j|, where\n> |_j| 2.\n>\n> #### Proof\n>\n> First recall how the error estimate for the simple formula x_i went.\n> Introduce s_1 = x_1, s_i = (1 + _i) (s_i - 1 + x_i). Then the computed sum\n> is s_n, which is a sum of terms, each of which is an x_i multiplied by an\n> expression involving _j's. The exact coefficient of x_1 is (1 + _2)(1 + _3)\n> ... (1 + _n), and so by renumbering, the coefficient of x_2 must be (1 +\n> _3)(1 + _4) ... (1 + _n), and so on. The proof of Theorem 8 runs along\n> exactly the same lines, only the coefficient of x_1 is more complicated. In\n> detail s_0 = c_0 = 0 and\n>\n> y_k = x_k c_k - 1 = (x_k - c_k - 1) (1 + _k)\n> s_k = s_k - 1 y_k = (s_k-1 + y_k) (1 + _k)\n> c_k = (s_k s_k - 1) y_k= [(s_k - s_k - 1) (1 + _k) - y_k] (1 + _k)\n> where all the Greek letters are bounded by . Although the coefficient of x_1\n> in s_k is the ultimate expression of interest, in turns out to be easier to\n> compute the coefficient of x_1 in s_k - c_k and c_k.\n>\n> When k = 1,\n>\n> c_1 = (s_1(1 + _1) - y_1) (1 + d_1)\n> = y_1((1 + s_1) (1 + _1) - 1) (1 + d_1)\n> = x_1(s_1 +_1 + s_1g_1) (1 + d_1) (1 + h_1)\n> s_1 - c_1 = x_1[(1 + s_1) - (s_1 + g_1 + s_1g_1) (1 + d_1)](1 + h_1)\n> = x_1[1 - g_1 - s_1d_1 - s_1g_1 - d_1g_1 - s_1g_1d_1](1 + h_1)\n> Calling the coefficients of x_1 in these expressions C_k and S_k\n> respectively, then\n>\n> C_1 = 2 + O(^2) S_1 = + _1 - _1 + 4^2 + O(^3)\n>\n> To get the general formula for S_k and C_k, expand the definitions of s_k\n> and c_k, ignoring all terms involving x_i with i > 1 to get\n>\n> s_k = (s_k - 1 + y_k)(1 + _k)\n> = [s_k - 1 + (x_k - c_k - 1) (1 + _k)](1 + _k)\n> = [(s_k - 1 - c_k - 1) - _kc_k - 1](1+_k)\n> c_k = [{s_k - s_k - 1}(1 + _k) - y_k](1 + _k)\n> = [{((s_k - 1 - c_k - 1) - _kc_k - 1)(1 + _k) - s_k - 1}(1 + _k) + c_k - 1(1\n> + _k)](1 + _k)\n> = [{(s_k - 1 - c_k - 1)_k - _kc_k_-1(1 + _k) - c_k - 1}(1 + _k) + c_k_ - 1(1\n> + _k)](1 + _k)\n> = [(s_k - 1 - c_k - 1)_k(1 + _k) - c_k - 1(_k + _k(_k + _k + _k_k))](1 +\n> _k),\n> s_k - c_k = ((s_k - 1 - c_k - 1) - _kc_k - 1) (1 + _k)\n> \\- [(s_k - 1 - c_k - 1)_k(1 + _k) - c_k - 1(_k + _k(_k + _k + _k_k)](1 + _k)\n> = (s_k- 1 - c_k - 1)((1 + _k) - _k(1 + _k)(1 + _k))\n> \\+ c_k - 1(-_k(1 + _k) + (_k + _k(_k + _k + _k_k)) (1 + _k))\n> = (s_- 1 - c_k - 1) (1 - _k(_k + _k + _k_k))\n> \\+ c_k - 1 - [_k + _k + _k(_k + _k_k) + (_k + _k(_k + _k + _k_k))_k]\n>\n> Since S_k and C_k are only being computed up to order ^2, these formulas can\n> be simplified to\n>\n> C_k= (_k + O(^2))S_k - 1 + (-_k + O(^2))C_k - 1\n> S_k= ((1 + 2^2 + O(^3))S_k - 1 + (2 + (^2))C_k - 1\n> Using these formulas gives\n>\n> C_2 = _2 + O(^2) S_2 = 1 + _1 - _1 + 10^2 + O(^3)\n>\n> and in general it is easy to check by induction that\n>\n> C_k = _k + O(^2) S_k = 1 + _1 - _1 + (4_k+2)^2 + O(^3)\n>\n> Finally, what is wanted is the coefficient of x_1 in s_k. To get this value,\n> let x_n + 1 = 0, let all the Greek letters with subscripts of n + 1 equal 0,\n> and compute s_n + 1. Then s_n + 1 = s_n - c_n, and the coefficient of x_1 in\n> s_n is less than the coefficient in s_n + 1, which is S_n = 1 + _1 - _1 +\n> (4n + 2)^2 = (1 + 2 + (n^2)). z\n>\n> ## Differences Among IEEE 754 Implementations\n>\n> Note \u2013 This section is not part of the published paper. It has been added to\n> clarify certain points and correct possible misconceptions about the IEEE\n> standard that the reader might infer from the paper. This material was not\n> written by David Goldberg, but it appears here with his permission.\n>\n> The preceding paper has shown that floating-point arithmetic must be\n> implemented carefully, since programmers may depend on its properties for\n> the correctness and accuracy of their programs. In particular, the IEEE\n> standard requires a careful implementation, and it is possible to write\n> useful programs that work correctly and deliver accurate results only on\n> systems that conform to the standard. The reader might be tempted to\n> conclude that such programs should be portable to all IEEE systems. Indeed,\n> portable software would be easier to write if the remark \"When a program is\n> moved between two machines and both support IEEE arithmetic, then if any\n> intermediate result differs, it must be because of software bugs, not from\n> differences in arithmetic,\" were true.\n>\n> Unfortunately, the IEEE standard does not guarantee that the same program\n> will deliver identical results on all conforming systems. Most programs will\n> actually produce different results on different systems for a variety of\n> reasons. For one, most programs involve the conversion of numbers between\n> decimal and binary formats, and the IEEE standard does not completely\n> specify the accuracy with which such conversions must be performed. For\n> another, many programs use elementary functions supplied by a system\n> library, and the standard doesn't specify these functions at all. Of course,\n> most programmers know that these features lie beyond the scope of the IEEE\n> standard.\n>\n> Many programmers may not realize that even a program that uses only the\n> numeric formats and operations prescribed by the IEEE standard can compute\n> different results on different systems. In fact, the authors of the standard\n> intended to allow different implementations to obtain different results.\n> Their intent is evident in the definition of the term destination in the\n> IEEE 754 standard: \"A destination may be either explicitly designated by the\n> user or implicitly supplied by the system (for example, intermediate results\n> in subexpressions or arguments for procedures). Some languages place the\n> results of intermediate calculations in destinations beyond the user's\n> control. Nonetheless, this standard defines the result of an operation in\n> terms of that destination's format and the operands' values.\" (IEEE\n> 754-1985, p. 7) In other words, the IEEE standard requires that each result\n> be rounded correctly to the precision of the destination into which it will\n> be placed, but the standard does not require that the precision of that\n> destination be determined by a user's program. Thus, different systems may\n> deliver their results to destinations with different precisions, causing the\n> same program to produce different results (sometimes dramatically so), even\n> though those systems all conform to the standard.\n>\n> Several of the examples in the preceding paper depend on some knowledge of\n> the way floating-point arithmetic is rounded. In order to rely on examples\n> such as these, a programmer must be able to predict how a program will be\n> interpreted, and in particular, on an IEEE system, what the precision of the\n> destination of each arithmetic operation may be. Alas, the loophole in the\n> IEEE standard's definition of destination undermines the programmer's\n> ability to know how a program will be interpreted. Consequently, several of\n> the examples given above, when implemented as apparently portable programs\n> in a high-level language, may not work correctly on IEEE systems that\n> normally deliver results to destinations with a different precision than the\n> programmer expects. Other examples may work, but proving that they work may\n> lie beyond the average programmer's ability.\n>\n> In this section, we classify existing implementations of IEEE 754 arithmetic\n> based on the precisions of the destination formats they normally use. We\n> then review some examples from the paper to show that delivering results in\n> a wider precision than a program expects can cause it to compute wrong\n> results even though it is provably correct when the expected precision is\n> used. We also revisit one of the proofs in the paper to illustrate the\n> intellectual effort required to cope with unexpected precision even when it\n> doesn't invalidate our programs. These examples show that despite all that\n> the IEEE standard prescribes, the differences it allows among different\n> implementations can prevent us from writing portable, efficient numerical\n> software whose behavior we can accurately predict. To develop such software,\n> then, we must first create programming languages and environments that limit\n> the variability the IEEE standard permits and allow programmers to express\n> the floating-point semantics upon which their programs depend.\n>\n> ### Current IEEE 754 Implementations\n>\n> Current implementations of IEEE 754 arithmetic can be divided into two\n> groups distinguished by the degree to which they support different floating-\n> point formats in hardware. Extended-based systems, exemplified by the Intel\n> x86 family of processors, provide full support for an extended double\n> precision format but only partial support for single and double precision:\n> they provide instructions to load or store data in single and double\n> precision, converting it on-the-fly to or from the extended double format,\n> and they provide special modes (not the default) in which the results of\n> arithmetic operations are rounded to single or double precision even though\n> they are kept in registers in extended double format. (Motorola 68000 series\n> processors round results to both the precision and range of the single or\n> double formats in these modes. Intel x86 and compatible processors round\n> results to the precision of the single or double formats but retain the same\n> range as the extended double format.) Single/double systems, including most\n> RISC processors, provide full support for single and double precision\n> formats but no support for an IEEE-compliant extended double precision\n> format. (The IBM POWER architecture provides only partial support for single\n> precision, but for the purpose of this section, we classify it as a\n> single/double system.)\n>\n> To see how a computation might behave differently on an extended-based\n> system than on a single/double system, consider a C version of the example\n> from the section Systems Aspects:\n>  \n>  \n>     int main() {\n>  \n>  \n>     double q;\n>  \n>  \n>     q = 3.0/7.0;\n>  \n>  \n>     if (q == 3.0/7.0) printf(\"Equal\\n\");\n>  \n>  \n>     else printf(\"Not Equal\\n\");\n>  \n>  \n>     return 0;\n>  \n>  \n>     }\n>\n> Here the constants 3.0 and 7.0 are interpreted as double precision floating-\n> point numbers, and the expression 3.0/7.0 inherits the double data type. On\n> a single/double system, the expression will be evaluated in double precision\n> since that is the most efficient format to use. Thus, q will be assigned the\n> value 3.0/7.0 rounded correctly to double precision. In the next line, the\n> expression 3.0/7.0 will again be evaluated in double precision, and of\n> course the result will be equal to the value just assigned to q, so the\n> program will print \"Equal\" as expected.\n>\n> On an extended-based system, even though the expression 3.0/7.0 has type\n> double, the quotient will be computed in a register in extended double\n> format, and thus in the default mode, it will be rounded to extended double\n> precision. When the resulting value is assigned to the variable q, however,\n> it may then be stored in memory, and since q is declared double, the value\n> will be rounded to double precision. In the next line, the expression\n> 3.0/7.0 may again be evaluated in extended precision yielding a result that\n> differs from the double precision value stored in q, causing the program to\n> print \"Not equal\". Of course, other outcomes are possible, too: the compiler\n> could decide to store and thus round the value of the expression 3.0/7.0 in\n> the second line before comparing it with q, or it could keep q in a register\n> in extended precision without storing it. An optimizing compiler might\n> evaluate the expression 3.0/7.0 at compile time, perhaps in double precision\n> or perhaps in extended double precision. (With one x86 compiler, the program\n> prints \"Equal\" when compiled with optimization and \"Not Equal\" when compiled\n> for debugging.) Finally, some compilers for extended-based systems\n> automatically change the rounding precision mode to cause operations\n> producing results in registers to round those results to single or double\n> precision, albeit possibly with a wider range. Thus, on these systems, we\n> can't predict the behavior of the program simply by reading its source code\n> and applying a basic understanding of IEEE 754 arithmetic. Neither can we\n> accuse the hardware or the compiler of failing to provide an IEEE 754\n> compliant environment; the hardware has delivered a correctly rounded result\n> to each destination, as it is required to do, and the compiler has assigned\n> some intermediate results to destinations that are beyond the user's\n> control, as it is allowed to do.\n>\n> ### Pitfalls in Computations on Extended-Based Systems\n>\n> Conventional wisdom maintains that extended-based systems must produce\n> results that are at least as accurate, if not more accurate than those\n> delivered on single/double systems, since the former always provide at least\n> as much precision and often more than the latter. Trivial examples such as\n> the C program above as well as more subtle programs based on the examples\n> discussed below show that this wisdom is naive at best: some apparently\n> portable programs, which are indeed portable across single/double systems,\n> deliver incorrect results on extended-based systems precisely because the\n> compiler and hardware conspire to occasionally provide more precision than\n> the program expects.\n>\n> Current programming languages make it difficult for a program to specify the\n> precision it expects. As the section Languages and Compilers mentions, many\n> programming languages don't specify that each occurrence of an expression\n> like 10.0*x in the same context should evaluate to the same value. Some\n> languages, such as Ada, were influenced in this respect by variations among\n> different arithmetics prior to the IEEE standard. More recently, languages\n> like ANSI C have been influenced by standard-conforming extended-based\n> systems. In fact, the ANSI C standard explicitly allows a compiler to\n> evaluate a floating-point expression to a precision wider than that normally\n> associated with its type. As a result, the value of the expression 10.0*x\n> may vary in ways that depend on a variety of factors: whether the expression\n> is immediately assigned to a variable or appears as a subexpression in a\n> larger expression; whether the expression participates in a comparison;\n> whether the expression is passed as an argument to a function, and if so,\n> whether the argument is passed by value or by reference; the current\n> precision mode; the level of optimization at which the program was compiled;\n> the precision mode and expression evaluation method used by the compiler\n> when the program was compiled; and so on.\n>\n> Language standards are not entirely to blame for the vagaries of expression\n> evaluation. Extended-based systems run most efficiently when expressions are\n> evaluated in extended precision registers whenever possible, yet values that\n> must be stored are stored in the narrowest precision required. Constraining\n> a language to require that 10.0*x evaluate to the same value everywhere\n> would impose a performance penalty on those systems. Unfortunately, allowing\n> those systems to evaluate 10.0*x differently in syntactically equivalent\n> contexts imposes a penalty of its own on programmers of accurate numerical\n> software by preventing them from relying on the syntax of their programs to\n> express their intended semantics.\n>\n> Do real programs depend on the assumption that a given expression always\n> evaluates to the same value? Recall the algorithm presented in Theorem 4 for\n> computing ln(1 + x), written here in Fortran:\n>  \n>  \n>     real function log1p(x)\n>  \n>  \n>     real x\n>  \n>  \n>     if (1.0 + x .eq. 1.0) then\n>  \n>  \n>     log1p = x\n>  \n>  \n>     else\n>  \n>  \n>     log1p = log(1.0 + x) * x / ((1.0 + x) - 1.0)\n>  \n>  \n>     endif\n>  \n>  \n>     return\n>\n> On an extended-based system, a compiler may evaluate the expression 1.0 + x\n> in the third line in extended precision and compare the result with 1.0.\n> When the same expression is passed to the log function in the sixth line,\n> however, the compiler may store its value in memory, rounding it to single\n> precision. Thus, if x is not so small that 1.0 + x rounds to 1.0 in extended\n> precision but small enough that 1.0 + x rounds to 1.0 in single precision,\n> then the value returned by log1p(x) will be zero instead of x, and the\n> relative error will be one--rather larger than 5. Similarly, suppose the\n> rest of the expression in the sixth line, including the reoccurrence of the\n> subexpression 1.0 + x, is evaluated in extended precision. In that case, if\n> x is small but not quite small enough that 1.0 + x rounds to 1.0 in single\n> precision, then the value returned by log1p(x) can exceed the correct value\n> by nearly as much as x, and again the relative error can approach one. For a\n> concrete example, take x to be 2^-24 + 2^-47, so x is the smallest single\n> precision number such that 1.0 + x rounds up to the next larger number, 1 +\n> 2^-23. Then log(1.0 + x) is approximately 2^-23. Because the denominator in\n> the expression in the sixth line is evaluated in extended precision, it is\n> computed exactly and delivers x, so log1p(x) returns approximately 2^-23,\n> which is nearly twice as large as the exact value. (This actually happens\n> with at least one compiler. When the preceding code is compiled by the Sun\n> WorkShop Compilers 4.2.1 Fortran 77 compiler for x86 systems using the -O\n> optimization flag, the generated code computes 1.0 + x exactly as described.\n> As a result, the function delivers zero for log1p(1.0e-10) and 1.19209E-07\n> for log1p(5.97e-8).)\n>\n> For the algorithm of Theorem 4 to work correctly, the expression 1.0 + x\n> must be evaluated the same way each time it appears; the algorithm can fail\n> on extended-based systems only when 1.0 + x is evaluated to extended double\n> precision in one instance and to single or double precision in another. Of\n> course, since log is a generic intrinsic function in Fortran, a compiler\n> could evaluate the expression 1.0 + x in extended precision throughout,\n> computing its logarithm in the same precision, but evidently we cannot\n> assume that the compiler will do so. (One can also imagine a similar example\n> involving a user-defined function. In that case, a compiler could still keep\n> the argument in extended precision even though the function returns a single\n> precision result, but few if any existing Fortran compilers do this,\n> either.) We might therefore attempt to ensure that 1.0 + x is evaluated\n> consistently by assigning it to a variable. Unfortunately, if we declare\n> that variable real, we may still be foiled by a compiler that substitutes a\n> value kept in a register in extended precision for one appearance of the\n> variable and a value stored in memory in single precision for another.\n> Instead, we would need to declare the variable with a type that corresponds\n> to the extended precision format. Standard FORTRAN 77 does not provide a way\n> to do this, and while Fortran 95 offers the SELECTED_REAL_KIND mechanism for\n> describing various formats, it does not explicitly require implementations\n> that evaluate expressions in extended precision to allow variables to be\n> declared with that precision. In short, there is no portable way to write\n> this program in standard Fortran that is guaranteed to prevent the\n> expression 1.0 + x from being evaluated in a way that invalidates our proof.\n>\n> There are other examples that can malfunction on extended-based systems even\n> when each subexpression is stored and thus rounded to the same precision.\n> The cause is double-rounding. In the default precision mode, an extended-\n> based system will initially round each result to extended double precision.\n> If that result is then stored to double precision, it is rounded again. The\n> combination of these two roundings can yield a value that is different than\n> what would have been obtained by rounding the first result correctly to\n> double precision. This can happen when the result as rounded to extended\n> double precision is a \"halfway case\", i.e., it lies exactly halfway between\n> two double precision numbers, so the second rounding is determined by the\n> round-ties-to-even rule. If this second rounding rounds in the same\n> direction as the first, the net rounding error will exceed half a unit in\n> the last place. (Note, though, that double-rounding only affects double\n> precision computations. One can prove that the sum, difference, product, or\n> quotient of two p-bit numbers, or the square root of a p-bit number, rounded\n> first to q bits and then to p bits gives the same value as if the result\n> were rounded just once to p bits provided q 2p + 2. Thus, extended double\n> precision is wide enough that single precision computations don't suffer\n> double-rounding.)\n>\n> Some algorithms that depend on correct rounding can fail with double-\n> rounding. In fact, even some algorithms that don't require correct rounding\n> and work correctly on a variety of machines that don't conform to IEEE 754\n> can fail with double-rounding. The most useful of these are the portable\n> algorithms for performing simulated multiple precision arithmetic mentioned\n> in the section Exactly Rounded Operations. For example, the procedure\n> described in Theorem 6 for splitting a floating-point number into high and\n> low parts doesn't work correctly in double-rounding arithmetic: try to split\n> the double precision number 2^52 + 3 \u00d7 2^26 - 1 into two parts each with at\n> most 26 bits. When each operation is rounded correctly to double precision,\n> the high order part is 2^52 + 2^27 and the low order part is 2^26 - 1, but\n> when each operation is rounded first to extended double precision and then\n> to double precision, the procedure produces a high order part of 2^52 + 2^28\n> and a low order part of -2^26 - 1. The latter number occupies 27 bits, so\n> its square can't be computed exactly in double precision. Of course, it\n> would still be possible to compute the square of this number in extended\n> double precision, but the resulting algorithm would no longer be portable to\n> single/double systems. Also, later steps in the multiple precision\n> multiplication algorithm assume that all partial products have been computed\n> in double precision. Handling a mixture of double and extended double\n> variables correctly would make the implementation significantly more\n> expensive.\n>\n> Likewise, portable algorithms for adding multiple precision numbers\n> represented as arrays of double precision numbers can fail in double-\n> rounding arithmetic. These algorithms typically rely on a technique similar\n> to Kahan's summation formula. As the informal explanation of the summation\n> formula given on Errors In Summation suggests, if s and y are floating-point\n> variables with |s| |y| and we compute:\n>  \n>  \n>     t = s + y;\n>  \n>  \n>     e = (s - t) + y;\n>\n> then in most arithmetics, e recovers exactly the roundoff error that\n> occurred in computing t. This technique doesn't work in double-rounded\n> arithmetic, however: if s = 2^52 + 1 and y = 1/2 - 2^-54, then s + y rounds\n> first to 2^52 + 3/2 in extended double precision, and this value rounds to\n> 2^52 + 2 in double precision by the round-ties-to-even rule; thus the net\n> rounding error in computing t is 1/2 + 2^-54, which is not representable\n> exactly in double precision and so can't be computed exactly by the\n> expression shown above. Here again, it would be possible to recover the\n> roundoff error by computing the sum in extended double precision, but then a\n> program would have to do extra work to reduce the final outputs back to\n> double precision, and double-rounding could afflict this process, too. For\n> this reason, although portable programs for simulating multiple precision\n> arithmetic by these methods work correctly and efficiently on a wide variety\n> of machines, they do not work as advertised on extended-based systems.\n>\n> Finally, some algorithms that at first sight appear to depend on correct\n> rounding may in fact work correctly with double-rounding. In these cases,\n> the cost of coping with double-rounding lies not in the implementation but\n> in the verification that the algorithm works as advertised. To illustrate,\n> we prove the following variant of Theorem 7:\n>\n> #### Theorem 7'\n>\n> If m and n are integers representable in IEEE 754 double precision with |m|\n> < 2^52 and n has the special form n = 2^i + 2^j, then (m n) n = m, provided\n> both floating-point operations are either rounded correctly to double\n> precision or rounded first to extended double precision and then to double\n> precision.\n>\n> #### Proof\n>\n> Assume without loss that m > 0\\. Let q = m n. Scaling by powers of two, we\n> can consider an equivalent setting in which 2^52 m < 2^53 and likewise for\n> q, so that both m and q are integers whose least significant bits occupy the\n> units place (i.e., ulp(m) = ulp(q) = 1). Before scaling, we assumed m <\n> 2^52, so after scaling, m is an even integer. Also, because the scaled\n> values of m and q satisfy m/2 < q < 2m, the corresponding value of n must\n> have one of two forms depending on which of m or q is larger: if q < m, then\n> evidently 1 < n < 2, and since n is a sum of two powers of two, n = 1 +\n> 2^-^k for some k; similarly, if q > m, then 1/2 < n < 1, so n = 1/2 +\n> 2^-(^k^ + 1). (As n is the sum of two powers of two, the closest possible\n> value of n to one is n = 1 + 2^-52. Because m/(1 + 2^-52) is no larger than\n> the next smaller double precision number less than m, we can't have q = m.)\n>\n> Let e denote the rounding error in computing q, so that q = m/n + e, and the\n> computed value q n will be the (once or twice) rounded value of m + ne.\n> Consider first the case in which each floating-point operation is rounded\n> correctly to double precision. In this case, |e| < 1/2. If n has the form\n> 1/2 + 2^-(^k^ + 1), then ne = nq - m is an integer multiple of 2^-(^k^ + 1)\n> and |ne| < 1/4 + 2^-(^k^ + 2). This implies that |ne| 1/4. Recall that the\n> difference between m and the next larger representable number is 1 and the\n> difference between m and the next smaller representable number is either 1\n> if m > 2^52 or 1/2 if m = 2^52. Thus, as |ne| 1/4, m + ne will round to m.\n> (Even if m = 2^52 and ne = -1/4, the product will round to m by the round-\n> ties-to-even rule.) Similarly, if n has the form 1 + 2^-^k, then ne is an\n> integer multiple of 2^-^k and |ne| < 1/2 + 2^-(^k^ + 1); this implies |ne|\n> 1/2. We can't have m = 2^52 in this case because m is strictly greater than\n> q, so m differs from its nearest representable neighbors by \u00b11. Thus, as\n> |ne| 1/2, again m + ne will round to m. (Even if |ne| = 1/2, the product\n> will round to m by the round-ties-to-even rule because m is even.) This\n> completes the proof for correctly rounded arithmetic.\n>\n> In double-rounding arithmetic, it may still happen that q is the correctly\n> rounded quotient (even though it was actually rounded twice), so |e| < 1/2\n> as above. In this case, we can appeal to the arguments of the previous\n> paragraph provided we consider the fact that q n will be rounded twice. To\n> account for this, note that the IEEE standard requires that an extended\n> double format carry at least 64 significant bits, so that the numbers m \u00b1\n> 1/2 and m \u00b1 1/4 are exactly representable in extended double precision.\n> Thus, if n has the form 1/2 + 2^-(^k^ + 1), so that |ne| 1/4, then rounding\n> m + ne to extended double precision must produce a result that differs from\n> m by at most 1/4, and as noted above, this value will round to m in double\n> precision. Similarly, if n has the form 1 + 2^-^k, so that |ne| 1/2, then\n> rounding m + ne to extended double precision must produce a result that\n> differs from m by at most 1/2, and this value will round to m in double\n> precision. (Recall that m > 2^52 in this case.)\n>\n> Finally, we are left to consider cases in which q is not the correctly\n> rounded quotient due to double-rounding. In these cases, we have |e| < 1/2 +\n> 2^-(^d^ + 1) in the worst case, where d is the number of extra bits in the\n> extended double format. (All existing extended-based systems support an\n> extended double format with exactly 64 significant bits; for this format, d\n> = 64 - 53 = 11.) Because double-rounding only produces an incorrectly\n> rounded result when the second rounding is determined by the round-ties-to-\n> even rule, q must be an even integer. Thus if n has the form 1/2 + 2^-(^k^ +\n> 1), then ne = nq - m is an integer multiple of 2^-^k, and\n>\n> |ne| < (1/2 + 2^-(^k^ + 1))(1/2 + 2^-(^d^ + 1)) = 1/4 + 2^-(^k^ + 2) +\n> 2^-(^d^ + 2) + 2^-(^k^ + ^d^ + 2).\n>\n> If k d, this implies |ne| 1/4. If k > d, we have |ne| 1/4 + 2^-(^d^ + 2). In\n> either case, the first rounding of the product will deliver a result that\n> differs from m by at most 1/4, and by previous arguments, the second\n> rounding will round to m. Similarly, if n has the form 1 + 2^-^k, then ne is\n> an integer multiple of 2^-(^k^ - 1), and\n>\n> |ne| < 1/2 + 2^-(^k^ + 1) + 2^-(^d^ + 1) + 2^-(^k^ + ^d^ + 1).\n>\n> If k d, this implies |ne| 1/2. If k > d, we have |ne| 1/2 + 2^-(^d^ + 1). In\n> either case, the first rounding of the product will deliver a result that\n> differs from m by at most 1/2, and again by previous arguments, the second\n> rounding will round to m. z\n>\n> The preceding proof shows that the product can incur double-rounding only if\n> the quotient does, and even then, it rounds to the correct result. The proof\n> also shows that extending our reasoning to include the possibility of\n> double-rounding can be challenging even for a program with only two\n> floating-point operations. For a more complicated program, it may be\n> impossible to systematically account for the effects of double-rounding, not\n> to mention more general combinations of double and extended double precision\n> computations.\n>\n> ### Programming Language Support for Extended Precision\n>\n> The preceding examples should not be taken to suggest that extended\n> precision per se is harmful. Many programs can benefit from extended\n> precision when the programmer is able to use it selectively. Unfortunately,\n> current programming languages do not provide sufficient means for a\n> programmer to specify when and how extended precision should be used. To\n> indicate what support is needed, we consider the ways in which we might want\n> to manage the use of extended precision.\n>\n> In a portable program that uses double precision as its nominal working\n> precision, there are five ways we might want to control the use of a wider\n> precision:\n>\n>   1. Compile to produce the fastest code, using extended precision where\n> possible on extended-based systems. Clearly most numerical software does not\n> require more of the arithmetic than that the relative error in each\n> operation is bounded by the \"machine epsilon\". When data in memory are\n> stored in double precision, the machine epsilon is usually taken to be the\n> largest relative roundoff error in that precision, since the input data are\n> (rightly or wrongly) assumed to have been rounded when they were entered and\n> the results will likewise be rounded when they are stored. Thus, while\n> computing some of the intermediate results in extended precision may yield a\n> more accurate result, extended precision is not essential. In this case, we\n> might prefer that the compiler use extended precision only when it will not\n> appreciably slow the program and use double precision otherwise.\n>\n>   2. Use a format wider than double if it is reasonably fast and wide\n> enough, otherwise resort to something else. Some computations can be\n> performed more easily when extended precision is available, but they can\n> also be carried out in double precision with only somewhat greater effort.\n> Consider computing the Euclidean norm of a vector of double precision\n> numbers. By computing the squares of the elements and accumulating their sum\n> in an IEEE 754 extended double format with its wider exponent range, we can\n> trivially avoid premature underflow or overflow for vectors of practical\n> lengths. On extended-based systems, this is the fastest way to compute the\n> norm. On single/double systems, an extended double format would have to be\n> emulated in software (if one were supported at all), and such emulation\n> would be much slower than simply using double precision, testing the\n> exception flags to determine whether underflow or overflow occurred, and if\n> so, repeating the computation with explicit scaling. Note that to support\n> this use of extended precision, a language must provide both an indication\n> of the widest available format that is reasonably fast, so that a program\n> can choose which method to use, and environmental parameters that indicate\n> the precision and range of each format, so that the program can verify that\n> the widest fast format is wide enough (e.g., that it has wider range than\n> double).\n>\n>   3. Use a format wider than double even if it has to be emulated in\n> software. For more complicated programs than the Euclidean norm example, the\n> programmer may simply wish to avoid the need to write two versions of the\n> program and instead rely on extended precision even if it is slow. Again,\n> the language must provide environmental parameters so that the program can\n> determine the range and precision of the widest available format.\n>\n>   4. Don't use a wider precision; round results correctly to the precision\n> of the double format, albeit possibly with extended range. For programs that\n> are most easily written to depend on correctly rounded double precision\n> arithmetic, including some of the examples mentioned above, a language must\n> provide a way for the programmer to indicate that extended precision must\n> not be used, even though intermediate results may be computed in registers\n> with a wider exponent range than double. (Intermediate results computed in\n> this way can still incur double-rounding if they underflow when stored to\n> memory: if the result of an arithmetic operation is rounded first to 53\n> significant bits, then rounded again to fewer significant bits when it must\n> be denormalized, the final result may differ from what would have been\n> obtained by rounding just once to a denormalized number. Of course, this\n> form of double-rounding is highly unlikely to affect any practical program\n> adversely.)\n>\n>   5. Round results correctly to both the precision and range of the double\n> format. This strict enforcement of double precision would be most useful for\n> programs that test either numerical software or the arithmetic itself near\n> the limits of both the range and precision of the double format. Such\n> careful test programs tend to be difficult to write in a portable way; they\n> become even more difficult (and error prone) when they must employ dummy\n> subroutines and other tricks to force results to be rounded to a particular\n> format. Thus, a programmer using an extended-based system to develop robust\n> software that must be portable to all IEEE 754 implementations would quickly\n> come to appreciate being able to emulate the arithmetic of single/double\n> systems without extraordinary effort.\n>\n>\n\n>\n> No current language supports all five of these options. In fact, few\n> languages have attempted to give the programmer the ability to control the\n> use of extended precision at all. One notable exception is the ISO/IEC\n> 9899:1999 Programming Languages - C standard, the latest revision to the C\n> language, which is now in the final stages of standardization.\n>\n> The C99 standard allows an implementation to evaluate expressions in a\n> format wider than that normally associated with their type, but the C99\n> standard recommends using one of only three expression evaluation methods.\n> The three recommended methods are characterized by the extent to which\n> expressions are \"promoted\" to wider formats, and the implementation is\n> encouraged to identify which method it uses by defining the preprocessor\n> macro FLT_EVAL_METHOD: if FLT_EVAL_METHOD is 0, each expression is evaluated\n> in a format that corresponds to its type; if FLT_EVAL_METHOD is 1, float\n> expressions are promoted to the format that corresponds to double; and if\n> FLT_EVAL_METHOD is 2, float and double expressions are promoted to the\n> format that corresponds to long double. (An implementation is allowed to set\n> FLT_EVAL_METHOD to -1 to indicate that the expression evaluation method is\n> indeterminable.) The C99 standard also requires that the <math.h> header\n> file define the types float_t and double_t, which are at least as wide as\n> float and double, respectively, and are intended to match the types used to\n> evaluate float and double expressions. For example, if FLT_EVAL_METHOD is 2,\n> both float_t and double_t are long double. Finally, the C99 standard\n> requires that the <float.h> header file define preprocessor macros that\n> specify the range and precision of the formats corresponding to each\n> floating-point type.\n>\n> The combination of features required or recommended by the C99 standard\n> supports some of the five options listed above but not all. For example, if\n> an implementation maps the long double type to an extended double format and\n> defines FLT_EVAL_METHOD to be 2, the programmer can reasonably assume that\n> extended precision is relatively fast, so programs like the Euclidean norm\n> example can simply use intermediate variables of type long double (or\n> double_t). On the other hand, the same implementation must keep anonymous\n> expressions in extended precision even when they are stored in memory (e.g.,\n> when the compiler must spill floating-point registers), and it must store\n> the results of expressions assigned to variables declared double to convert\n> them to double precision even if they could have been kept in registers.\n> Thus, neither the double nor the double_t type can be compiled to produce\n> the fastest code on current extended-based hardware.\n>\n> Likewise, the C99 standard provides solutions to some of the problems\n> illustrated by the examples in this section but not all. A C99 standard\n> version of the log1p function is guaranteed to work correctly if the\n> expression 1.0 + x is assigned to a variable (of any type) and that variable\n> used throughout. A portable, efficient C99 standard program for splitting a\n> double precision number into high and low parts, however, is more difficult:\n> how can we split at the correct position and avoid double-rounding if we\n> cannot guarantee that double expressions are rounded correctly to double\n> precision? One solution is to use the double_t type to perform the splitting\n> in double precision on single/double systems and in extended precision on\n> extended-based systems, so that in either case the arithmetic will be\n> correctly rounded. Theorem 14 says that we can split at any bit position\n> provided we know the precision of the underlying arithmetic, and the\n> FLT_EVAL_METHOD and environmental parameter macros should give us this\n> information.\n>\n> The following fragment shows one possible implementation:\n>  \n>  \n>     #include <math.h>\n>  \n>  \n>     #include <float.h>\n>  \n>  \n>     #if (FLT_EVAL_METHOD==2)\n>  \n>  \n>     #define PWR2 LDBL_MANT_DIG - (DBL_MANT_DIG/2)\n>  \n>  \n>     #elif ((FLT_EVAL_METHOD==1) || (FLT_EVAL_METHOD==0))\n>  \n>  \n>     #define PWR2 DBL_MANT_DIG - (DBL_MANT_DIG/2)\n>  \n>  \n>     #else\n>  \n>  \n>     #error FLT_EVAL_METHOD unknown!\n>  \n>  \n>     #endif\n>  \n>  \n>     ...\n>  \n>  \n>     double x, xh, xl;\n>  \n>  \n>     double_t m;\n>  \n>  \n>     m = scalbn(1.0, PWR2) + 1.0; // 2**PWR2 + 1\n>  \n>  \n>     xh = (m * x) - ((m * x) - x);\n>  \n>  \n>     xl = x - xh;\n>\n> Of course, to find this solution, the programmer must know that double\n> expressions may be evaluated in extended precision, that the ensuing double-\n> rounding problem can cause the algorithm to malfunction, and that extended\n> precision may be used instead according to Theorem 14. A more obvious\n> solution is simply to specify that each expression be rounded correctly to\n> double precision. On extended-based systems, this merely requires changing\n> the rounding precision mode, but unfortunately, the C99 standard does not\n> provide a portable way to do this. (Early drafts of the Floating-Point C\n> Edits, the working document that specified the changes to be made to the C90\n> standard to support floating-point, recommended that implementations on\n> systems with rounding precision modes provide fegetprec and fesetprec\n> functions to get and set the rounding precision, analogous to the fegetround\n> and fesetround functions that get and set the rounding direction. This\n> recommendation was removed before the changes were made to the C99\n> standard.)\n>\n> Coincidentally, the C99 standard's approach to supporting portability among\n> systems with different integer arithmetic capabilities suggests a better way\n> to support different floating-point architectures. Each C99 standard\n> implementation supplies an <stdint.h> header file that defines those integer\n> types the implementation supports, named according to their sizes and\n> efficiency: for example, int32_t is an integer type exactly 32 bits wide,\n> int_fast16_t is the implementation's fastest integer type at least 16 bits\n> wide, and intmax_t is the widest integer type supported. One can imagine a\n> similar scheme for floating-point types: for example, float53_t could name a\n> floating-point type with exactly 53 bit precision but possibly wider range,\n> float_fast24_t could name the implementation's fastest type with at least 24\n> bit precision, and floatmax_t could name the widest reasonably fast type\n> supported. The fast types could allow compilers on extended-based systems to\n> generate the fastest possible code subject only to the constraint that the\n> values of named variables must not appear to change as a result of register\n> spilling. The exact width types would cause compilers on extended-based\n> systems to set the rounding precision mode to round to the specified\n> precision, allowing wider range subject to the same constraint. Finally,\n> double_t could name a type with both the precision and range of the IEEE 754\n> double format, providing strict double evaluation. Together with\n> environmental parameter macros named accordingly, such a scheme would\n> readily support all five options described above and allow programmers to\n> indicate easily and unambiguously the floating-point semantics their\n> programs require.\n>\n> Must language support for extended precision be so complicated? On\n> single/double systems, four of the five options listed above coincide, and\n> there is no need to differentiate fast and exact width types. Extended-based\n> systems, however, pose difficult choices: they support neither pure double\n> precision nor pure extended precision computation as efficiently as a\n> mixture of the two, and different programs call for different mixtures.\n> Moreover, the choice of when to use extended precision should not be left to\n> compiler writers, who are often tempted by benchmarks (and sometimes told\n> outright by numerical analysts) to regard floating-point arithmetic as\n> \"inherently inexact\" and therefore neither deserving nor capable of the\n> predictability of integer arithmetic. Instead, the choice must be presented\n> to programmers, and they will require languages capable of expressing their\n> selection.\n>\n> ### Conclusion\n>\n> The foregoing remarks are not intended to disparage extended-based systems\n> but to expose several fallacies, the first being that all IEEE 754 systems\n> must deliver identical results for the same program. We have focused on\n> differences between extended-based systems and single/double systems, but\n> there are further differences among systems within each of these families.\n> For example, some single/double systems provide a single instruction to\n> multiply two numbers and add a third with just one final rounding. This\n> operation, called a fused multiply-add, can cause the same program to\n> produce different results across different single/double systems, and, like\n> extended precision, it can even cause the same program to produce different\n> results on the same system depending on whether and when it is used. (A\n> fused multiply-add can also foil the splitting process of Theorem 6,\n> although it can be used in a non-portable way to perform multiple precision\n> multiplication without the need for splitting.) Even though the IEEE\n> standard didn't anticipate such an operation, it nevertheless conforms: the\n> intermediate product is delivered to a \"destination\" beyond the user's\n> control that is wide enough to hold it exactly, and the final sum is rounded\n> correctly to fit its single or double precision destination.\n>\n> The idea that IEEE 754 prescribes precisely the result a given program must\n> deliver is nonetheless appealing. Many programmers like to believe that they\n> can understand the behavior of a program and prove that it will work\n> correctly without reference to the compiler that compiles it or the computer\n> that runs it. In many ways, supporting this belief is a worthwhile goal for\n> the designers of computer systems and programming languages. Unfortunately,\n> when it comes to floating-point arithmetic, the goal is virtually impossible\n> to achieve. The authors of the IEEE standards knew that, and they didn't\n> attempt to achieve it. As a result, despite nearly universal conformance to\n> (most of) the IEEE 754 standard throughout the computer industry,\n> programmers of portable software must continue to cope with unpredictable\n> floating-point arithmetic.\n>\n> If programmers are to exploit the features of IEEE 754, they will need\n> programming languages that make floating-point arithmetic predictable. The\n> C99 standard improves predictability to some degree at the expense of\n> requiring programmers to write multiple versions of their programs, one for\n> each FLT_EVAL_METHOD. Whether future languages will choose instead to allow\n> programmers to write a single program with syntax that unambiguously\n> expresses the extent to which it depends on IEEE 754 semantics remains to be\n> seen. Existing extended-based systems threaten that prospect by tempting us\n> to assume that the compiler and the hardware can know better than the\n> programmer how a computation should be performed on a given system. That\n> assumption is the second fallacy: the accuracy required in a computed result\n> depends not on the machine that produces it but only on the conclusions that\n> will be drawn from it, and of the programmer, the compiler, and the\n> hardware, at best only the programmer can know what those conclusions may\n> be.\n>\n> ^1 Examples of other representations are floating slash and signed logarithm\n> [Matula and Kornerup 1985; Swartzlander and Alexopoulos 1975].\n>\n> ^2 This term was introduced by Forsythe and Moler [1967], and has generally\n> replaced the older term mantissa.\n>\n> ^3 This assumes the usual arrangement where the exponent is stored to the\n> left of the significand.\n>\n> ^4 Unless the number z is larger than +1 or smaller than . Numbers which are\n> out of range in this fashion will not be considered until further notice.\n>\n> ^5 Let z' be the floating-point number that approximates z. Then d.d...d -\n> (z/^e)^p-1 is equivalent to z'-z/ulp(z'). A more accurate formula for\n> measuring error is z'-z/ulp(z). - Ed.\n>\n> ^6 700, not 70. Since .1 - .0292 = .0708, the error in terms of ulp(0.0292)\n> is 708 ulps. - Ed.\n>\n> ^7 Although the expression (x - y)(x + y) does not cause a catastrophic\n> cancellation, it is slightly less accurate than x^2 - y^2 if or . In this\n> case, (x - y)(x + y) has three rounding errors, but x^2 - y^2 has only two\n> since the rounding error committed when computing the smaller of x^2 and y^2\n> does not affect the final subtraction.\n>\n> ^8 Also commonly referred to as correctly rounded. - Ed.\n>\n> ^9 When n = 845, x_n= 9.45, x_n + 0.555 = 10.0, and 10.0 - 0.555 = 9.45.\n> Therefore, x_n = x_845 for n > 845.\n>\n> ^10 Notice that in binary, q cannot equal . - Ed.\n>\n> ^11 Left as an exercise to the reader: extend the proof to bases other than\n> 2. - Ed.\n>\n> ^12 This appears to have first been published by Goldberg [1967], although\n> Knuth ([1981], page 211) attributes this idea to Konrad Zuse. ^13 According\n> to Kahan, extended precision has 64 bits of significand because that was the\n> widest precision across which carry propagation could be done on the Intel\n> 8087 without increasing the cycle time [Kahan 1988]. ^14 Some arguments\n> against including inner product as one of the basic operations are presented\n> by Kahan and LeBlanc [1985].\n>\n> ^15 Kirchner writes: It is possible to compute inner products to within 1\n> ulp in hardware in one partial product per clock cycle. The additionally\n> needed hardware compares to the multiplier array needed anyway for that\n> speed. ^16 CORDIC is an acronym for Coordinate Rotation Digital Computer and\n> is a method of computing transcendental functions that uses mostly shifts\n> and adds (i.e., very few multiplications and divisions) [Walther 1971]. It\n> is the method additionally needed hardware compares to the multiplier array\n> needed anyway for that speed. d used on both the Intel 8087 and the Motorola\n> 68881. ^17 Fine point: Although the default in IEEE arithmetic is to round\n> overflowed numbers to , it is possible to change the default (see Rounding\n> Modes)\n>\n> ^18 They are called subnormal in 854, denormal in 754.\n>\n> ^19 This is the cause of one of the most troublesome aspects of the\n> standard. Programs that frequently underflow often run noticeably slower on\n> hardware that uses software traps. ^20 No invalid exception is raised unless\n> a \"trapping\" NaN is involved in the operation. See section 6.2 of IEEE Std\n> 754-1985. - Ed.\n>\n> ^21 may be greater than if both x and y are negative. - Ed.\n>\n> ^22 It can be in range because if x < 1, n < 0 and x^-^n is just a tiny bit\n> smaller than the underflow threshold , then , and so may not overflow, since\n> in all IEEE precisions, -e_min < e_max.\n>\n> ^23 This is probably because designers like \"orthogonal\" instruction sets,\n> where the precisions of a floating-point instruction are independent of the\n> actual operation. Making a special case for multiplication destroys this\n> orthogonality.\n>\n> ^24 This assumes the common convention that 3.0 is a single-precision\n> constant, while 3.0D0 is a double precision constant.\n>\n> ^25 The conclusion that 0^0 = 1 depends on the restriction that f be\n> nonconstant. If this restriction is removed, then letting f be the\n> identically 0 function gives 0 as a possible value for lim _x 0\n> f(x)^g^(^x^), and so 0^0 would have to be defined to be a NaN.\n>\n> ^26 In the case of 0^0, plausibility arguments can be made, but the\n> convincing argument is found in \"Concrete Mathematics\" by Graham, Knuth and\n> Patashnik, and argues that 00 = 1 for the binomial theorem to work. - Ed.\n> ^27 Unless the rounding mode is round toward -, in which case x - x = -0.\n>\n> ^28 The VMS math libraries on the VAX use a weak form of in-line procedure\n> substitution, in that they use the inexpensive jump to subroutine call\n> rather than the slower CALLS and CALLG instructions.\n>\n> ^29 The difficulty with presubstitution is that it requires either direct\n> hardware implementation, or continuable floating-point traps if implemented\n> in software. - Ed.\n>\n> ^30 In this informal proof, assume that = 2 so that multiplication by 4 is\n> exact and doesn't require a _i.\n>\n> ^31 This is the sum if adding w does not generate carry out. Additional\n> argument is needed for the special case where adding w does generate carry\n> out. - Ed.\n>\n> ^32 Rounding gives ^kx + w^k - r^k only if^ (^kx + w^k^) keeps the form of^\n> ^kx^. -^ Ed.\n\nSun Microsystems, Inc. Copyright information. All rights reserved. Feedback| Library | Contents | Previous | Next | Index  \n---|---\n\n", "frontpage": false}
