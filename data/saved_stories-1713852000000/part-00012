{"aid": "40117251", "title": "MongoDB change data capture: An in-depth guide", "url": "https://www.propeldata.com/blog/mongodb-change-data-capture-an-in-depth-guide", "domain": "propeldata.com", "votes": 3, "user": "acossta", "posted_at": "2024-04-22 18:28:29", "comments": 0, "source_title": "MongoDB change data capture: An in-depth guide", "source_text": "MongoDB change data capture: An in-depth guide\n\n\u00d7\n\nThis website stores cookies on your computer. These cookies are used to\ncollect information about how you interact with our website and allow us to\nremember you. We use this information in order to improve and customize your\nbrowsing experience and for analytics and metrics about our visitors both on\nthis website and other media. To find out more about the cookies we use, see\nour Privacy Policy\n\nIf you decline, your information won\u2019t be tracked when you visit this website.\nA single cookie will be used in your browser to remember your preference not\nto be tracked.\n\nAccept Decline\n\nBlog\n\nData Engineering\n\nEstimated Read Time: 10 minutes\n\n# MongoDB change data capture: An in-depth guide\n\n## Learn how to implement Change Data Capture (CDC) for a MongoDB database\nusing Debezium and Kafka for real-time analytics.\n\nArtem Oppermann\n\n\u2022\n\nPublished:\n\nApril 22, 2024\n\n\u2022\n\nLast updated:\n\nApril 22, 2024\n\nGraphic: Propel Data\n\nChange data capture (CDC) is a technique to observe and capture changes in\ndatabases like MongoDB. It's particularly useful for monitoring changes such\nas insertions, updates, and deletions in real time. Once detected, these data\nmodifications can be transformed into a continuous stream of change events,\nwhich you can integrate into various systems and applications for further\nprocessing.\n\nCDC is utilized in many different fields and applications because its real-\ntime data synchronization and analysis ensure that the systems have access to\nthe latest information and data. For example, in event-driven architectures\nsuch as microservices, CDC is a useful component for data exchange between\nmicroservices. It also enables real-time updating of materialized views within\nthese architectures. In data warehouses, CDC can be used to capture data\nchanges in different source systems and use this information to keep the data\nwarehouse up to date. For cache invalidation, CDC can track modifications to\ndata in order to keep cached data consistent. CDC is also very useful in audit\nlogging. It can document every data modification and keep a comprehensive\nrecord of transactions, which is important for compliance and security.\n\nIn this article, you'll learn about the importance of CDC, become familiar\nwith some of its use cases, and explore how you can implement CDC with a\nMongoDB database.\n\n## How to Implement CDC with MongoDB\n\nImplementing CDC with MongoDB can be accomplished using several different\ntools and techniques. The following sections will discuss several prominent\nsolutions, including using Debezium, MongoDB CDC handlers, Qlik Replicate CDC,\nand Airbyte CDC.\n\n### Debezium\n\nDebezium is an open-source platform that allows users to perform CDC with\nMongoDB. Debezium establishes a direct connection to the MongoDB database to\nmonitor document-level changes like inserts, updates, and deletes. Debezium\nchecks the operations log (oplog) in MongoDB for any modifications to the\ndata. The detected data changes are then streamed to a Kafka topic.\nImplementing Debezium with MongoDB involves configuring the Debezium connector\nto tap into MongoDB's operations log and setting up all necessary\nconfigurations for integration with Kafka.\n\n### MongoDB CDC Handlers\n\nAnother option for performing CDC with MongoDB is to use the MongoDB CDC\nhandlers. This involves streaming change events from MongoDB to a Kafka topic\nusing the MongoDB Kafka Connector. To implement CDC using this method, you'll\nneed to configure source and sink connectors. The source connector captures\ndata changes in the MongoDB collection and streams them to a Kafka topic. The\nsink connector then consumes these messages and applies the changes to the\ntarget MongoDB collection. This process ensures data consistency across\ndifferent collections.\n\nMongoDB Atlas users also have the option of using the MongoDB Atlas Source\nConnector for Confluent Cloud to establish a fully managed solution for CDC\nwith Kafka. This connector specifically targets MongoDB Atlas databases and\nautomatically captures and publishes change events to Kafka topics, thus\nproviding an efficient way to integrate MongoDB Atlas with Kafka to perform\nCDC.\n\n### Fivetran\n\nFivetran is a data integration tool that supports CDC. It is able to capture\nand replicate data changes from a source database such as MongoDB to a variety\nof target systems or platforms. To set up MongoDB for CDC using Fivetran, you\nmust configure a new connector in the Fivetran dashboard, where MongoDB is\nselected as the source. Additionally, you have to specify which databases and\ncollections should be monitored for changes and how these changes should be\ncaptured and replicated. This includes setting the synchronization frequency\nand determining how much historical data should be captured.\n\n### Airbyte CDC\n\nAirbyte offers another alternative to enable CDC with MongoDB. Airbyte is an\nopen source data integration platform that can extract data from a source\nsystem (like a MongoDB database) and load it into a target system (like a data\nwarehouse or another database). This process involves configuring MongoDB as\nthe data source and selecting a target system, such as a data warehouse, where\nthe captured changes will be streamed.\n\nAirbyte then tracks the oplog of the MongoDB database for data modifications.\nAs soon as a modification is captured in the source system, Airbyte streams\nthe change event to the target system.\n\n### Use Cases for CDC with Debezium and MongoDB\n\nDebezium is a CDC platform that enables the synchronization of data across\nvarious systems by detecting and streaming real-time changes from databases\nlike MongoDB. Using Debezium, organizations can implement efficient CDC in a\nwide range of different use cases, including:\n\n  * Microservices: In the context of microservices, Debezium can be used to observe changes in the MongoDB database of a microservice. Consider a customer support system where a new support ticket is created. Debezium captures this data change and streams it to a predefined Kafka topic. Other microservices subscribed to this topic are then informed of this change event and can update their internal data states or initiate certain actions based on the change. This ensures data consistency across microservices without the need for direct communication between them. This method is known as the outbox pattern.\n  * Data warehouses: By capturing changes in operational databases, such as patient records in a healthcare provider's MongoDB database, Debezium ensures that data warehouses remain synchronized with the latest updates, like new patient entries.\n  * Cache validation: For systems that require high responsiveness, like a content management system that caches data in Redis, Debezium can help to keep the cache updated by detecting and streaming database changes. This ensures that users access only the most current information.\n  * Audit logging: In environments where it's important to track data modifications for compliance, such as banking transactions, Debezium provides a mechanism to monitor and record all changes made to the data. This creates a comprehensive audit trail for enhanced security and compliance.\n  * Analytics: Debezium can enhance analytics by providing a real-time data feed into analytical platforms such as Propel. For example, in e-commerce, user interactions with products stored in MongoDB can be tracked and streamed to an analytics platform. This would allow you to perform immediate analysis of consumer behavior and their preferences to get a deeper understanding of customer dynamics. As a result, this would enable organizations to better adjust their strategies and enhance user experiences based on fresh insights.\n\n## Implementing CDC with MongoDB\n\nTo illustrate how to implement CDC with MongoDB, imagine you're part of the IT\nor data management team at a financial institution that uses MongoDB to store\nall data related to transactions, account balance adjustments, and customer\nprofile updates. Until recently, the lack of real-time data capture caused\ndifficulties in generating an audit trail for compliance or security purposes.\nThis resulted in delayed insights and potential gaps in monitoring data\nchanges. Your task is to employ Debezium to implement CDC, which connects to\nthe MongoDB banking database and monitors changes within it. For example, when\na transaction is processed, or an account balance is updated in the\n\nThe technical architecture requires setting up a Debezium MongoDB connector\nfor the relevant collections in the MongoDB database. This connector then\nlistens for changes in the oplog of MongoDB and captures any data\nmodifications.\n\nA rough architecture diagram of the application is illustrated below:\n\n### Prerequisites\n\nTo get started, you'll need the following:\n\n  * A recent version of Docker (this article uses Docker 24.0.7)\n  * A recent version of Docker Compose (this article uses Docker Compose 1.29.2)\n  * ZooKeeper\n  * Kafka\n  * Kafka Connect\n  * Debezium's MongoDB connector (this article uses connector version 0.9.4)\n  * A running instance of a MongoDB database\n\n### Starting Kafka and Debezium's MongoDB Connector\n\nFirst, you'll need to download Debezium's MongoDB connector. The connector\nmonitors a replica set or a sharded cluster of MongoDB for any data changes in\nthe databases and collections. These changes are then recorded as events in\nKafka topics. In particular, the Debezium MongoDB connector uses the oplog of\nMongoDB to detect the data changes.\n\nDownload version 0.9.4 of Debezium's MongoDB connector and extract the folder\ninto the directory PROJECT_DIR/kafka-connect/plugins, where PROJECT_DIR is the\nmain directory of your project.\n\nIn the next step, you can start the Kafka services and the Debezium connector.\nThe most convenient and efficient way to deploy and manage this stack is\nthrough the use of Docker and Docker Compose. Download the docker-compose.yml\nfile and save it in PROJECT_DIR. Then, open a terminal in this directory and\nrun the following command:\n\n    \n    \n    docker-compose up -d\n\nIn the terminal, you should see that the Kafka services have started:\n\n    \n    \n    Starting kafka_zookeeper_1 ... done Starting kafka_kafka_1 ... done Starting kafka_kafka-connect_1 ... done\n\nTo confirm that Debezium's MongoDB connector is installed correctly and is\npicked up by the plugin loader, run the following command in the terminal:\n\n    \n    \n    curl -sS localhost:8083/connector-plugins | jq '.[].class' | grep mongodb\n\nIf it's been installed successfully, you'll see the following output in the\nterminal:\n\n    \n    \n    \"io.debezium.connector.mongodb.MongoDbConnector\"\n\n### Setting Up MongoDB Using Docker\n\nIf you don't have a native installation of MongoDB, you can follow the\ninstructions in this section to launch MongoDB in a Docker container.\n\nIn a new terminal, execute the command below to pull a Docker image for\nMongoDB:\n\n    \n    \n    docker pull mongo\n\nNext, in the same terminal, run the container with the command below, where\nmongodb-1 specifies the name of the container:\n\n    \n    \n    docker run --name mongodb-1 -p 27017:27017 -d mongo --replSet debezium\n\nHere,--replSet debezium is passed as an argument to the MongoDB instance. This\ntells MongoDB to initiate itself as a member of a replica set named debezium.\n\nYou also need to configure the running container, but you'll first need to\nexecute the following command in your terminal to get access to run commands\ninside the Docker container:\n\n    \n    \n    docker exec -it mongodb-1 bash\n\nOnce inside the container, execute mongosh to start a mongosh process. You\nshould get the following output:\n\n    \n    \n    Connecting to: mongodb://127.0.0.1:27017/?directConnection=true&serverSelectionTimeoutMS=2000&appName=mongosh+2.1.3 Using MongoDB: 7.0.5 Using Mongosh: 2.1.3 For mongosh info see: ...output omitted...\n\nNow, execute the command below within the mongosh process to configure the\nrunning MongoDB instance as a member of a replica set:\n\n    \n    \n    rs.initiate({_id: \"debezium\", members:[{_id: 0, host: \"localhost:27017\"}]})\n\nMongoDB is now configured to be a member of a replica set named debezium with\na single member (itself) at the specified host and port. After initiating the\nreplica set with this command, the MongoDB instance starts behaving as part of\na replica set. It's ready to replicate data to any additional members that\nmight be added or to support operations that require a replica set, such as\ndata change streams.\n\nYou can now create a user profile for the MongoDB instance. Inside the mongosh\nprocess, execute the command below to create a user, role, and password:\n\n    \n    \n    use admin db.createUser( { user: \"debezium\", pwd: \"dbz\", roles: [\"dbOwner\"] } )\n\nNext, create a database and a collection within MongoDB by running the\nfollowing command in the mongosh process:\n\n    \n    \n    use banking db.transactions.insert({ _id: 1, type: 'deposit', amount: 1000, currency: 'USD', account_id: 123456, date: new Date() })\n\nThis command creates a database called banking and a collection named\ntransactions. It also inserts a new transaction record into the collection.\n\nYou can execute the command db.transactions.find().pretty() in the same\nmongosh process to verify that the database was created successfully. If it\nwas successful, you should see the following output:\n\n    \n    \n    [ { _id: 1, type: 'deposit', amount: 1000, currency: 'USD', account_id: 123456, date: ISODate('2024-02-08T07:59:14.916Z') } ]\n\n### Starting the Debezium MongoDB Connector\n\nTo provide Kafka Connect with the necessary configuration to instantiate and\nrun the connector as a task within the cluster, you must first create a\nconfiguration file at register-mongodb.json to store the following connector\nconfigurations:\n\n    \n    \n    { \"name\": \"transactions-connector\", \"config\": { \"connector.class\" : \"io.debezium.connector.mongodb.MongoDbConnector\", \"tasks.max\" : \"1\", \"mongodb.hosts\" : \"debezium/localhost:27017\", \"mongodb.name\" : \"dbserver1\", \"mongodb.user\" : \"debezium\", \"mongodb.password\" : \"dbz\", } }\n\nEach attribute within the config object specifies how the connector should\noperate:\n\n  * connector.class specifies the class of the connector to be used (in this case, io.debezium.connector.mongodb.MongoDbConnector).\n  * tasks.max defines the maximum number of tasks that the connector should use to perform its work.\n  * mongodb.hosts lists the MongoDB server or replica set members that the connector will connect to.\n  * mongodb.name is an identifier for the MongoDB server or replica set that is used internally by Debezium. It labels the origin of the captured changes.\n  * The mongodb.user and mongodb.password fields contain the credentials that Debezium will use to authenticate with MongoDB.\n\nIn the next step, open a new terminal in the directory where register-\nmongodb.json is saved and execute this command:\n\n    \n    \n    curl -i -X POST -H \"Accept:application/json\" -H \"Content-Type:application/json\" <http://localhost:8083/connectors/> -d @register-mongodb.json\n\nIf the connector is successfully instantiated, you should see the following\noutput in the terminal:\n\n    \n    \n    HTTP/1.1 201 Created Date: Thu, 08 Feb 2024 08:05:57 GMT Location: <http://localhost:8083/connectors/transactions-connector> Content-Type: application/json Content-Length: 307 ...output omitted...\n\n### Starting the Kafka Consumer to Observe the Change Events\n\nYou can utilize Kafka to display all captured data changes in the terminal. To\ndo this, you should first open an interactive bash shell inside the running\nKafka Docker container. Execute the following command in the terminal:\n\n    \n    \n    docker exec -it kafka_kafka_1 bash\n\nOnce inside the container, execute the following command to start a Kafka\nconsumer that displays all captured data changes in the terminal:\n\n    \n    \n    kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic dbserver1.banking.transactions --from-beg\n\nThis command tells the Confluent CLI to consume messages from a Kafka topic\ncalled dbserver1.banking.transactions. This topic contains change events for\nthe transactions collection in the banking database of the MongoDB server.\n\nTo verify that CDC is working, go back to the terminal where the mongosh\nprocess is running and insert some new data into the collection:\n\nIn the terminal where the consumer is executed, you should see the displayed\nchange event:\n\n    \n    \n    {\"_id\" : 2,\"type\" : \"deposit\",\"amount\" : 596,\"currency\" : \"EU\",\"account_id\" : 654123,\"date\" : {\"$date\" : 1707402585158}}\n\nLastly, you can verify that the data in the database is consistent with the\nchanges made. In the terminal with the mongosh process, execute the command\nbelow to display the entire transactions collection:\n\n    \n    \n    db.transactions.find().pretty();\n\nUpon executing the command, you should expect to see the following output:\n\n    \n    \n    [ { _id: 1, type: 'deposit', amount: 1000, currency: USD, account_id: 123456, date: ISODate('2024-02-08T07:59:14.916Z') }, { _id: 2, type: 'deposit', amount: 596, currency: 'EU', account_id: 654123, date: ISODate('2024-02-08T14:29:45.158Z') } ]\n\nThis output indicates that a new customer was successfully added to the\ndatabase. It confirms the successful registration of the connector\nconfiguration with the Kafka Connect cluster as well as the initiation of data\ncapture from MongoDB, reflecting recent changes such as the addition of a new\ncustomer.\n\n## Conclusion\n\nIn this article, you learned the role change data capture (CDC) plays ensuring\ndata across different systems and applications remains synchronized and up to\ndate. CDC preserves data integrity, enhances decision-making capabilities, and\nfacilitates real-time data analysis and reporting. In the practical part, you\nfocused on deploying CDC within a MongoDB database environment by utilizing\nDebezium and Kafka.\n\nIf you need to power analytics from your MongoDB data, consider exploring\nPropel. For example, you to directly ingest your MongoDB CDC to a Propel\nWebhook Data and expose it via low-latency analytics APIs to power dashboards\nand reports. You can also enrich and transform your MongoDB data for analytics\nusing Propel\u2019s Materialized Views. Think of it as gathering data in one main\nspot for different uses.\n\nAlternatively, Propel also integrates with ELT platforms like Fivetran and\nAirbyte, as well as data warehouses like BigQuery and Snowflake. This allows\nyou to ingest data from either of these platforms and then use it to Propel to\npower customer-facing analytics.\n\nArtem Oppermann\n\n\u2022\n\nResearch Engineer (AI & Data Science)\n\n## Related posts\n\nData Engineering\n\n### MySQL Change Data Capture: A Definitive Guide\n\nThis is some text inside of a div block.\n\n# Heading 1\n\n## Heading 2\n\n### Heading 3\n\n#### Heading 4\n\n##### Heading 5\n\n###### Heading 6\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor\nincididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis\nnostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore\neu fugiat nulla pariatur.\n\n> Block quote\n\nOrdered list\n\n  1. Item 1\n  2. Item 2\n  3. Item 3\n\nUnordered list\n\n  * Item A\n  * Item B\n  * Item C\n\nText link\n\nBold text\n\nEmphasis\n\n^Superscript\n\n_Subscript\n\nArtem Oppermann\n\nResearch Engineer (AI & Data Science)\n\nMarch 14, 2024\n\nData Engineering\n\n### DynamoDB Change Data Capture: A Definitive Guide\n\nThis is some text inside of a div block.\n\n# Heading 1\n\n## Heading 2\n\n### Heading 3\n\n#### Heading 4\n\n##### Heading 5\n\n###### Heading 6\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor\nincididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis\nnostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore\neu fugiat nulla pariatur.\n\n> Block quote\n\nOrdered list\n\n  1. Item 1\n  2. Item 2\n  3. Item 3\n\nUnordered list\n\n  * Item A\n  * Item B\n  * Item C\n\nText link\n\nBold text\n\nEmphasis\n\n^Superscript\n\n_Subscript\n\nLucien Chemaly\n\nTech Lead | Instructor | Technical Writer\n\nMarch 6, 2024\n\nData Engineering\n\n### How to Move Data from MongoDB to Amazon S3 in Parquet\n\nThis is some text inside of a div block.\n\n# Heading 1\n\n## Heading 2\n\n### Heading 3\n\n#### Heading 4\n\n##### Heading 5\n\n###### Heading 6\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor\nincididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis\nnostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\nDuis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore\neu fugiat nulla pariatur.\n\n> Block quote\n\nOrdered list\n\n  1. Item 1\n  2. Item 2\n  3. Item 3\n\nUnordered list\n\n  * Item A\n  * Item B\n  * Item C\n\nText link\n\nBold text\n\nEmphasis\n\n^Superscript\n\n_Subscript\n\nRajkumar Venkatasamy\n\nPrincipal Architect at an MNC\n\nFebruary 16, 2024\n\nReturn to blog\n\nStart shipping today\n\n### Deliver the analytics your customers have been asking for.\n\nCreate a free account\n\nBook a live demo\n\nTrust & Compliance\n\nPlatform\n\nReal-Time Analytics Cloud\n\nSecure Data Sharing\n\nSemantic Layer\n\nMulti-tenant Access Controls\n\nSQL\n\nData APIs\n\nReact UI Components\n\nLLM Analytics\n\nSnowflake\n\nDevelopers\n\nQuickstart\n\nGraphQL API\n\nDocumentation\n\nAdmin API\n\nAccess Control\n\nTerraform Provider\n\nResources\n\nBlog\n\nPodcast\n\nCustomer Stories\n\nCompany\n\nAbout\n\nContact Us\n\nContact Sales\n\nLegal\n\nFollow us\n\nTwitter\n\nLinkedIn\n\nGitHub\n\nReddit\n\n\u00a9 2023 Propel Data Cloud Inc. All rights reserved.\n\n", "frontpage": false}
