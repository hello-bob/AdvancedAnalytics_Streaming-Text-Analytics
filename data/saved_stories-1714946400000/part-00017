{"aid": "40263819", "title": "Healthcare Needs Trustworthy LLMs: Introducing Deterministic Quoting", "url": "https://mattyyeung.github.io/deterministic-quoting", "domain": "mattyyeung.github.io", "votes": 1, "user": "mattyyeung", "posted_at": "2024-05-05 10:47:36", "comments": 0, "source_title": "Healthcare needs trustworthy LLMs: Introducing Deterministic Quoting", "source_text": "Healthcare needs trustworthy LLMs: Introducing Deterministic Quoting | Matt Yeung\n\nMatt Yeung\n\n# Healthcare needs trustworthy LLMs: Introducing Deterministic Quoting\n\nApr 2024\n\nLLMs have the potential to revolutionise healthcare, but the fear and reality\nof hallucinations prevent adoption in most applications.\n\nAt Invetech, we\u2019re working on \u201cDeterministic Quoting\u201d, a new technique that\nensures quotations from source material are verbatim, not hallucinated.\n\nIn this example, everything displayed with a blue background is guaranteed to\nbe verbatim from source material. No hallucinations. LLMs remain imperfect, so\nit may still choose to quote the wrong part of the source material, but only\n\u201creal\u201d quotations are displayed on blue - they are deterministically\ngenerated.\n\nWe think Deterministic Quoting is an \u201cenabler\u201d for deploying LLMs where there\nare serious consequences to incorrect information, such as:\n\n  * AIs that process medical records\n  * AIs that assist in diagnosis\n  * AIs which reference medical guidelines\n\nMany LLM systems can be designed to deterministically quote. This article\nprovides motivation and explains a basic implementation.\n\n#### Contents:\n\n  1. Hallucinations Matter\n  2. Introducing Deterministic Quoting\n  3. How Well Does It Work?\n  4. Applications\n  5. Technical Details: How is it Implemented?\n  6. Beyond the Minimalist Implementation\n  7. Conclusion: Is This Really Ready For Healthcare?\n\n## 1\\. Hallucinations Matter\n\nHallucinations are endemic to LLM systems.\n\n  * The biggest models from OpenAI, Google, Anthropic etc. hallucinate more than 20% of the time in some use-cases\n  * It\u2019s safe to assume that the next generation (ChatGPT \u201c5\u201d, Gemini 1.5 Ultra, etc.) will continue to hallucinate, albeit at a lower rate\n  * Some LLMs are trained/prompted to cite sources... but these citations themselves can be hallucinated! This can be particularly problematic: users are more likely to trust authoritative-looking citations\n  * \u201cCheck your own answer\u201d iterative methods can reduce the rate of hallucinations, but only partially.\n  * Attaching relevant source material to the query (RAG) is better than training/fine-tuning/LoRA (see catastrophic forgetting), but even still the material is sometimes transformed when quoted in the output.\n\nIn short: any information that passes through an LLM is potentially \u201ctainted\u201d.\n\n### So what?\n\nIn naturally conservative fields like healthcare we do not deploy systems that\nare reliable \u201cmost of the time\u201d. Even a low rate of hallucination is enough to\nprevent adoption at scale for most of our use-cases. We simply can\u2019t burden\nusers with the task of independent verification \u2013 mistakes and their\nconsequences are inevitable.\n\nEventually, LLM quality may be high enough to be trustworthy as-is, but this\nis not currently within sight. Until then...\n\n## 2\\. Introducing: Deterministic Quoting\n\nDeterministic Quoting techniques bridge the LLM \u201ctrust gap\u201d.\n\nApplications with Deterministic Quoting provide verbatim \u201cground-truth\u201d\ninformation interspersed with LLM commentary. It combines the convenience and\nflexibility of the LLM with trustworthy data that is guaranteed to be\nhallucination-free. Users benefit from the framing and commentary but can\neasily verify the underlying assumptions without extra action.\n\nThe \u201challucination-free\u201d guarantee is achieved by ensuring that the data\ndisplayed on the blue background has never passed through an LLM (or any non-\ndeterministic AI model). The AI chooses which section of source material to\nquote, but the retrieval of that text is a traditional non-AI database lookup.\nThat\u2019s the only way to guarantee that an LLM has not transformed text: don\u2019t\nsend it through the LLM in the first place.\n\nThe approach is imperfect: the surrounding text (white background) has come\ndirectly from an LLM and therefore may still be hallucinated. Or, the AI can\nchoose an irrelevant (but still verbatim) quote to display. Still, the result\nis a significant improvement: users report intuitively grasping the difference\nbetween trusted quotations and prose generated by the LLM.\n\n## 3\\. How well does it work?\n\nIn practice, we meet the stated goal of \u201czero false positives\u201d for any quoted\ntext. That is, 100% of all text displayed in the special quote box (the blue\nbackground in the examples) is indeed verbatim, not hallucinated.\n\nIn addition, several other metrics are useful:\n\n  * Are there hallucinations in the non-DQ prose? (ie. white background)\n  * Was the right quote/data chosen by the LLM? Is it relevant to the question?\n  * Is user\u2019s query answered?\n\nHere, the underlying LLM remains the limiting factor for quality, DQ is only\npart of the story. We don\u2019t expect DQ to improve these metrics, our goal is to\navoid regressions.\n\nHere are some preliminary results comparing a standard RAG pipeline\n(Llamaindex + ChatGPT 4) with a modified version with a minimalistic\nimplementation of DQ. N=60, non-public dataset.\n\nBaseline| Goal| With DQ  \n---|---|---  \nHallucinations in blue box:| N/A| 0| 0  \nHallucinations outside blue box:| 12%| Better than baseline| 2%  \nWas the quote/data relevant?| 90%| Baseline| 92%  \nIs the user\u2019s query answered?| 83%| Baseline| 88%  \n  \nWhile preliminary, this data is consistent with other experiments we have run:\nadding DQ does not appear to degrade the overall quality of answers. If\nanything, answer quality may slightly improve.\n\n## 4\\. Applications\n\nDQ techniques provide benefit wherever hallucinations are problematic \u2013\ntypically Information Retrieval (eg RAG) and related systems.\n\n  * a pharmacist discussing drug interactions and side effects with an AI assistant\n  * an anxious patient doesn\u2019t get through all of their questions in the 7 minutes they have with their surgeon but has all the time in the world to discuss with a knowledgable AI assistant\n  * a doctor asks the medical records system if a patient has a history of heart conditions - it responds with a summarised list\n  * an assistant to front-line medical staff with knowledge of best-practice diagnosis and treatment for common conditions\n  * an educational aid that allows students to query textbooks or other study material\n\nProof-of-concept demonstations already exist - organisations are clearly\nexcited about the potential for these systems. But in most of these, there are\nserious consequences to hallucinations. Few are currently reliable enough to\ndeploy at scale.\n\nOf course, DQ may be applied outside healthcare too - eg. systems with\nknowledge of legisliation, financial regulation or works of literature. But\nhere, we constrain the discussion to healtcare.\n\n## 5\\. Technical Details: How is it Implemented?\n\nWhile implementations will vary, DQ fundamentally involves a sending LLM\noutputs to a separate module that replaces potentially-hallucinated quotations\nwith verbatim copies direct from the source material. This replacement is a\ntraditional non-AI database query, that is, it\u2019s \u201cdeterministic\u201d.\n\nBefore discussing advanced implementations, let\u2019s build a proof-of-concept.\nThe simplest way is to take a typical RAG pipeline and make some\nmodifications.\n\n### A \u201cMinimalist Implementation\u201d of DQ: a modified RAG Pipeline\n\nA typical RAG pipeline:\n\nNote how the retrieved source material passes through the LLM - and is\ntherefore liable to be transformed (hallucinated) before it is shown to the\nuser.\n\nWe want to fix this by adding a \u201cdeterministic lookup\u201d of quotes after all\ncalls to the LLM are complete. Note how the new modules are added after the\nLLM.\n\nTo achieve this, we make six changes to the original:\n\n  1. Chunker: modify to suit in-line quotation\n  2. Generate a unique reference string for each chunk\n  3. Retrieval: wrap chunks in a structured format - including a unique reference - before passing to the LLM.\n  4. Prompt: instruct the LLM to cite references for all claims and output a structured format for these references\n  5. Deterministic lookup: use those references to loop back to the source material to find the original quote\n  6. Add support to GUI\n\n### Chunking Approach\n\nWe add two constraints to the document \u201cchunker\u201d - the module that splits our\ndataset up into indexable pieces.\n\nFirst, we prefer smaller chunks that are easily displayed in and around the\nLLM\u2019s commentary. In RAG with current-generation LLMs, chunks are often quite\nlarge - a page of text or more. But this simple implementation of DQ only\ndisplays whole chunks, not parts. A whole page for each quote would often be\ntoo much for users to conveniently parse, so paragraph-sized are preferred. Of\ncourse, the LLM can still quote several consecutive paragraphs where relevant.\n\nSecond, we want chunk boundaries to be logical to a user: displaying cut-off\npieces of a section could be confusing. Such \u201csemantic chunking\u201d can be\ntedious, depending heavily on the source material structure and format.\nHowever, it does seem to provide an improvement in the quality of some\nanswers, presumably because there is semantic value in the structure of the\ndocument.\n\nLike most ML systems, source data preparation and chunking are often the most\ntime consuming to implement.\n\n### Give Each Chunk a Unique Reference String\n\nWe create a unique reference string for each chunk. This string is used in\nseveral ways:\n\n  * stored in the datastore as metadata for each chunk\n  * passed into the LLM context as a \u201cheader\u201d for its chunk inside a <title> tag\n  * output by the LLM alongside anything it wants to quote \u2013 again in a <title> tag\n  * (ideally, if meaningful to humans) displayed in the \u201cblue box\u201d as a link to the original source.\n\nThis is not always straightforward. Difficulties include:\n\n  * unstructured documents. Humans would say \u201chalf-way down page 32\u201d\n  * dealing with duplicates: eg \u201cSection 1: Introduction\u201d may appear in multiple documents, 2 documents may have the same name, or the corpus may contain multiple revisions of the same document\n  * the stretch goal of making the string meaningful to humans can be difficult, depending on the document set\n\n### Prompt Engineering\n\nHere is an example system prompt for a system with access to our internal\nmedical device design standards:\n\n    \n    \n    System prompt: You are an expert Q&A system with excellent knowledge of the internal documentation of our company, Invetech. Invetech designs medical devices, so it is critical that you always accurately quote the reference information you use to answer the query.\n\nLike typical RAG systems, we instruct the LLM to only answer from the source\nmaterial, not its learned \u201cmemory\u201d. Unlike many RAG systems, however, we\ninstruct the LLM to always cite sources, keeping \u201ccommentary\u201d separate from\nthe quotes.\n\n    \n    \n    Always answer the query using the provided reference information, and not prior knowledge. The reference information is provided below as a list of \"sections\". Each section is encapsulated by a <quote> tag. Each <quote> contains a <title> tag at the very beginning. Use this title when quoting. Some rules to follow: 1. Always use the appropriate <quote>s to answer the question. 2. Include a plain English summary in answer to the query, based on the text in the relevant <quote>s. There is no need to simply repeat the quoted text, however.\n\nWhen citing, we instruct the LLM to start with the unique reference string in\na structured format. XML-style works well, but so do json or others.\n\n    \n    \n    3. Quote the whole section so the user can see where the information has come from. Always include the <quote> tags, including the title, when quoting. 4. Always start quotes with \"<quote>\" and end quotes with \"</quote>\" 5. Quote multiple sections if relevant, but always quote whole sections using the correct format. 6. Use the term \"Reference information\" instead of \"context information\" 7. Always quote whole sections verbatim, not a subset.\n\n### Retrieval of Top-K Chunks\n\nThe only change to retrieval is the format used when inserting into LLM\ncontext. We use the same format we want the LLM to output \u2013 including the\nunique reference string from above inside <title> tags. This provides the LLM\nwith examples of the preferred format without eating up valuable context\nspace. An example:\n\n    \n    \n    <quote> <title>ICD-10 Version:2019 - Chapter VI Diseases of the nervous system - G43.1</title> G43.1 Migraine with aura [classical migraine] <link>https://icd.who.int/browse10/2019/en#G43.1</link> </quote>\n\n### Deterministic Lookup\n\nAfter the LLM output has been returned, quotations matching the above format\nare extracted. In the minimal implementation of DQ, only the unique reference\nstring is kept. The actual quote text is discarded because it may contain\nhallucinations.\n\nThe application looks up the unique reference string in the chunk index. If it\nmatches, the true quotation text is inserted into the <quote> tag text.\nOtherwise, the unique reference string has been hallucinated \u2013 the quotation\nis invalid.\n\nWhen this step is complete, everything contained within a <quote> tag is\nguaranteed to be a valid quotation directly from the source material.\n\n### GUI Changes\n\nTo complete our minimalist implementation of DQ, we modify the GUI to clearly\ndistinguish between quotations and LLM prose. In the examples above, we:\n\n  * put the text in a blue box\n  * add a user-readable reference (ideally the unique reference string)\n  * label it as deterministically generated\n  * add a link to the source\n\n## 6\\. Beyond the Minimalist Implementation\n\nWhile the implementation above is a useful explainer there are many\nopportunities for improvement. Our tests have shown significant improvement\nbeyond the proof-of-principle implementation:\n\n#### Reducing the rate of irrelevant answers and omissions\n\n  * Detect & reject answers that fail to use deterministic quoting\n  * Detect if the answer fails to answer the question (imperfect, because an LLM is used to do this)\n  * Replace the \u201cunique reference string matching\u201d with a more sophisticated method of matching a selected quote to the ground-truth database.\n  * Prompt engineering to encourage \u201cI don\u2019t know\u201d responses\n\n#### Presenting information the LLMs in a more machine-understandable manner\n\n  * Improved versions of semantic chunking - eg overlapping chunks or processing documents multiple times with different chunk sizes\n  * Adding more meta-data to chunks - For example, sometimes nearby subheading will provide important context\n  * Diagrams, charts and tables are notoriously difficult for AI systems to understand. DQ has its own challenges with these, though they are much easier to solve than the general problem faced by all RAG systems\n  * Iterative calls to the LLM to narrow down source material\n  * Customising parsing and chunking to the dataset, or, where possible, changing dataset format to suit the LLM\n\nGenerally, these techniques are not spcific to DQ, but often provide more\nbenefit when used with DQ.\n\n#### Non-RAG Applications\n\nDQ isn\u2019t limited to RAG systems. However, implementation can be more\ncumbersome because some parts of the RAG pipeline (eg chunking) are required\nfor DQ.\n\nFor example, upcoming models from Google et al. can fit whole books in context\n- enough to make RAG unnecessary for small corpuses. They still hallucinate,\nso Deterministic Quoting remains beneficial, but the source data must be\nchunked (ideally semantically) and indexed as it was in RAG.\n\n## 7\\. Conclusion: Is this Really Ready for Healthcare?\n\nDeterministic Quoting helps to bridge the LLM \u201ctrust gap\u201d. Combining the\nflexibility of LLMs with the reliability of non-AI quote lookups, DQ gives\nusers confidence in the \u201cground truth\u201d data assumed by the AI.\n\nAdvanced versions of the technique are still under development, but even basic\nimplementations show significant improvement over the current state-of-the-\nart. Future versions can provide further improvements to quality of answers\nand flexibility when parsing a wide range of input documentation.\n\nFor some applications, a basic DQ implementation may be the difference between\na proof-of-concept and a system with enough trustworthiness to deploy into\nproduction. For others, there is still work to be done before we can\ndemonstrate sufficient safety. In all cases, it is clear that some variation\nof DQ will remain useful as long as models continue to hallucinate.\n\nContributors: Chris Herring, Lars Katzfey & I. We work at Invetech Contact:\nmatthew.yeungZZZZZ@invetech.com.au or LinkedIn\n\n  * Matt Yeung\n  * Melbourne, Australia\n  * matt.yyeung at gmail dot com\n\nCC BY-NC 4.0 Int\n\nRSS\n\n", "frontpage": false}
