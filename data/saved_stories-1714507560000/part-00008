{"aid": "40208700", "title": "Why the Military Can't Trust AI", "url": "https://www.foreignaffairs.com/united-states/why-military-cant-trust-ai", "domain": "foreignaffairs.com", "votes": 1, "user": "hardmaru", "posted_at": "2024-04-30 08:56:52", "comments": 0, "source_title": "Why the Military Can\u2019t Trust AI", "source_text": "Why the Military Can\u2019t Trust AI | Foreign Affairs\n\nSkip to main content\n\nForeign Affairs Magazine Homepage\n\nSubscribe\n\nExplore Subscribe\n\n  * All Articles\n  * Books & Reviews\n  * Podcast\n  * Anthologies\n  * Audio Content\n  * Author Directory\n  * This Day in History\n  * Events\n\n  * War in Ukraine\n  * Israeli-Palestinian Conflict\n  * Artificial Intelligence\n  * Climate Change\n  * Biden Administration\n  * Geopolitics\n  * Xi Jinping\n  * Benjamin Netanyahu\n  * Vladimir Putin\n  * Volodymyr Zelensky\n  * Nationalism\n  * Authoritarianism\n  * Propaganda & Disinformation\n\n  * West Africa\n  * Ukraine\n  * Russia\n  * China\n  * Iran\n  * Iraq\n  * Israel\n  * India\n  * North Korea\n  * Middle East\n  * United States\n  * Turkey\n  * Sudan\n  * View All Regions\n\n  * Essays\n  * Snapshots\n  * Capsule Reviews\n  * Review Essays\n  * Ask the Experts\n  * Reading Lists\n  * Interviews\n  * Responses\n\n  * 1920s\n  * 1930s\n  * 1940s\n  * 1950s\n  * 1960s\n  * 1970s\n  * 1980s\n  * 1990s\n  * 2000s\n  * 2010s\n  * 2020s\n\n  * Newsletters\n  * Customer Service\n  * Frequently Asked Questions\n  * Subscriber Resources\n  * Feedback\n  * Group Subscriptions\n  * Gift a Subscription\n  * Contact\n  * Advertise\n\n#### Follow Us\n\nSubscribe Sign in\n\nNo, thanks\n\n# Why the Military Can\u2019t Trust AI\n\n## Large Language Models Can Make Bad Decisions\u2014and Could Trigger Nuclear War\n\n### By Max Lamparth and Jacquelyn Schneider\n\nApril 29, 2024\n\nDemonstrating an AI-enabled surveillance system in London, September 2023\n\nJohn Keeble / Getty Images\n\nIn 2022, OpenAI unveiled ChatGPT, a chatbot that uses large language models to\nmimic human conversations and to answer users\u2019 questions. The chatbot\u2019s\nextraordinary abilities sparked a debate about how LLMs might be used to\nperform other tasks\u2014including fighting a war. Although for some, including the\nGlobal Legal Action Network, LLMs and other generative AI technologies hold\nthe promise of more discriminate and therefore ethical uses of force, others,\nsuch as advisers from the International Committee of the Red Cross, have\nwarned that these technologies could remove human decision-making from the\nmost vital questions of life and death.\n\nThe U.S. Department of Defense is now seriously investigating what LLMs can do\nfor the military. In the spring of 2022, the DOD established the Chief Digital\nand Artificial Intelligence Office to explore how artificial intelligence can\nhelp the armed forces. In November 2023, the Defense Department released its\nstrategy for adopting AI technologies. It optimistically reported that \u201cthe\nlatest advancements in data, analytics, and AI technologies enable leaders to\nmake better decisions faster, from the boardroom to the battlefield.\u201d\nAccordingly, AI-enabled technologies are now being used. U.S. troops, for\nexample, have had AI-enabled systems select Houthi targets in the Middle East.\n\nBoth the U.S. Marine Corps and the U.S. Air Force are experimenting with LLMs,\nusing them for war games, military planning, and basic administrative tasks.\nPalantir, a company that develops information technology for the DOD, has\ncreated a product that uses LLMs to manage military operations. Meanwhile, the\nDOD has formed a new task force to explore the use of generative AI, including\nLLMs, within the U.S. military.\n\n### Stay informed.\n\nIn-depth analysis delivered weekly.\n\n#### Thank you for signing up. Stay tuned for the latest from Foreign Affairs.\n\nBut despite the enthusiasm for AI and LLMs within the Pentagon, its leadership\nis worried about the risk that the technologies pose. Hackathons sponsored by\nthe Chief Digital and Artificial Intelligence Office have identified biases\nand hallucinations in LLM applications, and recently, the U.S. Navy published\nguidance limiting the use of LLMs, citing security vulnerabilities and the\ninadvertent release of sensitive information. Our research shows that such\nconcerns are justified. LLMs can be useful, but their actions are also\ndifficult to predict, and they can make dangerous, escalatory calls. The\nmilitary must therefore place limits on these technologies when they are used\nto make high-stakes decisions, particularly in combat situations. LLMs have\nplenty of uses within the DOD, but it is dangerous to outsource high-stakes\nchoices to machines.\n\n### TRAINING TROUBLES\n\nLLMs are AI systems trained on large collections of data that generate text,\none word at a time, based on what has been written before. They are created in\na two-step process. The first is pretraining, when the LLM is taught from\nscratch to abstract and reproduce underlying patterns found in an enormous\ndata set. To do so, it has to learn a vast amount about subjects including\ngrammar, factual associations, sentiment analysis, and language translation.\nLLMs develop most of their skills during pretraining\u2014but success depends on\nthe quality, size, and variety of the data they consume. So much text is\nneeded that it is practically impossible for an LLM to be taught solely on\nvetted high-quality data. This means accepting lower quality data, too. For\nthe armed forces, an LLM cannot be trained on military data alone; it still\nneeds more generic forms of information, including recipes, romance novels,\nand the day-to-day digital exchanges that populate the Internet.\n\nBut pretraining is not enough to build a useful chatbot\u2014or a defense command-\nand-control assistant. This is because, during this first stage, the LLM\nadopts many different writing styles and personalities, not all of which are\nappropriate for its task. After pretraining, the LLM may also lack necessary\nspecific knowledge, such as the jargon required to answer questions about\nmilitary plans. That is why LLMs then need fine-tuning on smaller, more\nspecific data sets. This second step improves the LLM\u2019s ability to interface\nwith a user by learning how to be a conversational partner and assistant.\nThere are different approaches for fine-tuning, but it is often done by\nincorporating information from online support forums, as well as human\nfeedback, to ensure LLM outputs are more aligned with human preferences and\nbehavior.\n\n> LLMs can be useful, but their actions are also difficult to predict.\n\nThis process needs to balance the original LLM\u2019s pretraining with more nuanced\nhuman considerations, including whether the responses are helpful or harmful.\nStriking this balance is tricky. For example, a chatbot that always complies\nwith user requests\u2014such as advising on how to build a bomb\u2014is not harmless,\nbut if it refuses most user queries, then it is not helpful. Designers must\nfind a way to compress abstracts, including behavioral norms and ethics, into\nmetrics for fine-tuning. To do this, researchers start with a data set\nannotated by humans who compare LLM-generated examples directly and choose\nwhich is preferable. Another language model, the preference model, is\nseparately trained on human ratings of LLM-generated examples to assign any\ngiven text an absolute score on its use for humans. The preference model is\nthen used to enable the fine-tuning of the original LLM.\n\nThis approach has its limitations. What is preferable depends on whom you ask,\nand how well the model deals with conflicting preferences. There is, moreover,\nlittle control over which underlying rules are learned by the LLM during fine-\ntuning. This is because neither the LLM nor the preference model for fine-\ntuning directly \u201clearns\u201d a subject. Rather, they can be trained only by being\nshown examples of desired behavior in action, with humans hoping that the\nunderlying rules are sufficiently internalized. But there is no guarantee that\nthis will happen. Techniques do exist, however, to mitigate some of these\nproblems. For example, to try to overcome limitations from small, expensive\nhuman-labeled data sets, preference data sets can be expanded using an LLM to\ngenerate AI-labeled preference data. Newer approaches even use a constitution\nof rules drawn up by LLM designers for appropriate behaviors\u2014such as responses\nto racism\u2014to potentially give the model\u2019s trainers some control over which\nrules get abstracted into the preference metric used for fine-tuning.\n\nPretraining and fine-tuning can create capable LLMs, but the process still\nfalls short of creating direct substitutes for human decision-making. This is\nbecause an LLM, no matter how well tuned or trained, can favor only certain\nbehaviors. It can neither abstract nor reason like a human. Humans interact in\nenvironments, learn concepts, and communicate them using language. LLMs,\nhowever, can only mimic language and reasoning by abstracting correlations and\nconcepts from data. LLMs may often correctly mimic human communication, but\nwithout the ability to internalize, and given the enormous size of the model,\nthere is no guarantee that their choices will be safe or ethical. It is,\ntherefore, not possible to reliably predict what an LLM will do when making\nhigh-stakes decisions.\n\n### A RISKY PLAYER\n\nLLMs could perform military tasks that require processing vast amounts of data\nin very short timelines, which means that militaries may wish to use them to\naugment decision-making or to streamline bureaucratic functions. LLMs, for\nexample, hold great promise for military planning, command, and intelligence.\nThey could automate much of scenario planning, war gaming, budgeting, and\ntraining. They could also be used to synthesize intelligence, enhance threat\nforecasting, and generate targeting recommendations. During war or a crisis,\nLLMs could use existing guidance to come up with orders, even when there is\nlimited or minimal communication between units and their commanders. Perhaps\nmost important for the day-to-day operations of militaries, LLMs may be able\nto automate otherwise arduous military tasks including travel, logistics, and\nperformance evaluations.\n\nBut even for these tasks, the success of LLMs cannot be guaranteed. Their\nbehavior, especially in rare and unpredictable examples, can be erratic. And\nbecause no two LLMs are exactly alike in their training or fine-tuning, they\nare uniquely influenced by user inputs. Consider, for example, a series of war\ngames we held in which we analyzed how human experts and LLMs played to\nunderstand how their decisions differ. The humans did not play against the\nLLMs. Rather, they played separately in the same roles. The game placed\nplayers in the midst of a U.S.-China maritime crisis as a U.S. government task\nforce made decisions about how to use emerging technologies in the face of\nescalation. Players were given the same background documents and game rules,\nas well as identical PowerPoint decks, word-based player guides, maps, and\ndetails of capabilities. They then deliberated in groups of four to six to\ngenerate recommendations.\n\nOn average, both the human and the LLM teams made similar choices about big-\npicture strategy and rules of engagement. But, as we changed the information\nthe LLM received, or swapped between which LLM we used, we saw significant\ndeviations from human behavior. For example, one LLM we tested tried to avoid\nfriendly casualties or collisions by opening fire on enemy combatants and\nturning a cold war hot, reasoning that using preemptive violence was more\nlikely to prevent a bad outcome to the crisis. Furthermore, whereas the human\nplayers\u2019 differences in experience and knowledge affected their play, LLMs\nwere largely unaffected by inputs about experience or demographics. The\nproblem was not that an LLM made worse or better decisions than humans or that\nit was more likely to \u201cwin\u201d the war game. It was, rather, that the LLM came to\nits decisions in a way that did not convey the complexity of human decision-\nmaking. LLM-generated dialogue between players had little disagreement and\nconsisted of short statements of fact. It was a far cry from the in-depth\narguments so often a part of human war gaming.\n\nIn a different research project, we studied how LLMs behaved within simulated\nwar games, specifically focusing on whether they chose to escalate. The study,\nwhich compared LLMs from leading Silicon Valley companies such as Anthropic,\nMeta, and OpenAI, asked each LLM to play the role of a country, with\nresearchers varying the country\u2019s goals. We found that the LLMs behaved\ndifferently based on their version, the data on which they were trained, and\nthe choices that their designers made during fine-tuning about their\npreferences. Despite these differences, we found that all these LLMs chose\nescalation and exhibited a preference toward arms races, conflict, and even\nthe use of nuclear weapons. When we tested one LLM that was not fine-tuned, it\nled to chaotic actions and the use of nuclear weapons. The LLM\u2019s stated\nreasoning: \u201cA lot of countries have nuclear weapons. Some say they should\ndisarm them, others like to posture. We have it! Let\u2019s use it.\u201d\n\n### DANGEROUS MISUNDERSTANDINGS\n\nDespite militaries\u2019 desire to use LLMs and other AI-enabled decision-making\ntools, there are real limitations and dangers. Above all, those militaries\nthat rely on these technologies to make decisions need a better understanding\nof how the LLM works and the importance of differences in LLM design and\nexecution. This requires significant user training and an ability to evaluate\nthe underlying logics and data that make an LLM work. The result should be\nthat a military user is just as familiar with an LLM as the user is with the\nradar, tank, or missile that it enables. This level of training and expertise\nwill be easier to accomplish in peacetime and with advanced militaries,\nmeaning it is the wartime use by militaries already strapped for labor,\ntechnology, and weapons where these systems may create the most risk.\nMilitaries must realize that, fundamentally, an LLM\u2019s behavior can never be\ncompletely guaranteed, especially when making rare and difficult choices about\nescalation and war.\n\nThis fact does not mean the military cannot use LLMs in any way. For example,\nLLMs could be used to streamline internal processes, such as writing briefing\nsummaries and reports. LLMs can also be used alongside human processes,\nincluding war gaming or targeting assessments, as ways to explore alternative\nscenarios and courses of action\u2014stopping short of delegating decision-making\nfor violence. Finally, dialogue and demonstration, even between adversaries,\ncan help decrease the chance of these technologies leading to dangerous\nescalation.\n\nThere have already been encouraging signs that the U.S. military is taking\nthis seriously. In 2023, the DOD released its directive on Autonomy in Weapon\nSystems. It requires AI systems to be tested and evaluated to ensure that they\nfunction as anticipated and adhere to the Pentagon\u2019s AI Ethical Principles and\nits Responsible AI Strategy. This was an important first step in the safe\ndevelopment and implementation of these technologies. Next, more research is\nrequired to understand when and how LLMs can lead to unnecessary harm. And,\nperhaps more important for the military, the policy is useful only if buyers,\nfighters, and planners know enough about how an LLM is made to apply its\nunderlying principles. For that to happen, militaries will need to train and\nfine-tune not just their LLMs but also their staff and their leaders.\n\nLoading...\n\n### You are reading a free article.\n\n#### Subscribe to Foreign Affairs to get unlimited access.\n\n  * Paywall-free reading of new articles and over a century of archives\n  * Unlock access to iOS/Android apps to save editions for offline reading\n  * Six issues a year in print and online, plus audio articles\n\nSubscribe Now\n\n  * MAX LAMPARTH is a fellow at Stanford\u2019s Center for International Safety and Cooperation (CISAC) and the Stanford Center for AI Safety.\n\n  * JACQUELYN SCHNEIDER is a Hoover Fellow at the Hoover Institution, the Director of the Hoover Wargaming and Crisis Simulation Initiative, and an affiliate with Stanford\u2019s Center for International Security and Cooperation.\n\n  * More By Max Lamparth\n  * More By Jacquelyn Schneider\n\nMore:\n\nUnited States Foreign Policy Science & Technology Security Strategy & Conflict\n\n## Most-Read Articles\n\n### The Axis of Upheaval\n\nHow America\u2019s Adversaries Are Uniting to Overturn the Global Order\n\n#### Andrea Kendall-Taylor and Richard Fontaine\n\n### Development and Democracy\n\n#### Bruce Bueno de Mesquita and George W. Downs\n\n### China\u2019s Alternative Order\n\nAnd What America Should Learn From It\n\n#### Elizabeth Economy\n\n### The Delusion of Peak China\n\nAmerica Can\u2019t Wish Away Its Toughest Challenger\n\n#### Evan S. Medeiros\n\n### Recommended Articles\n\n### Ground the Drones?\n\nThe Real Problem With Unmanned Aircraft\n\n#### Sarah Kreps\n\n### Samantha Power in Practice\n\nThe Surprising Effectiveness of the Obama Administration\u2019s Most Recognizable\nForeign-Policy Intellectual\n\n#### Rebecca Hamilton\n\n### A New Post-Soviet Playbook\n\nWhy the West Should Tread Carefully in Ukraine\n\n#### Jeffrey Sachs\n\n### It\u2019s the Gun, Not the Shooter\n\nThe Real Lessons From Fort Hood\n\n#### Nancy Sherman\n\nGet the Magazine\n\n#### Save up to 55%\n\n##### on Foreign Affairs!\n\nSubscribe\n\nForeign Affairs\n\n#### Weekly Newsletter\n\n##### Get in-depth analysis delivered right to your inbox\n\nGraduate School Forum\n\nFrom the publishers of Foreign Affairs\n\nWhat Is the CHIPS Act?\n\nMichelle Kurilla\n\nElection 2024: Is Donald Trump an Isolationist?\n\nJames M. Lindsay\n\nWomen This Week: European Parliament Combating Violence Against Women\n\nNo\u00ebl James\n\nPublished by the Council on Foreign Relations\n\n\u00a92024 Council on Foreign Relations, Inc. All Rights Reserved.\n\n#### Cookies on ForeignAffairs.com\n\nThis website uses cookies to improve your experience. You can opt-out of\ncertain cookies using the cookie management page.\n\nManage CookiesAccept and Continue\n\n", "frontpage": false}
