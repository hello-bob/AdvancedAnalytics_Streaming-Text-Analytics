{"aid": "40285823", "title": "Ways to Think about AGI", "url": "https://www.ben-evans.com/benedictevans/2024/5/4/ways-to-think-about-agi", "domain": "ben-evans.com", "votes": 2, "user": "bookofjoe", "posted_at": "2024-05-07 14:22:18", "comments": 0, "source_title": "Ways to think about AGI", "source_text": "Ways to think about AGI \u2014 Benedict Evans\n\nBenedict Evans\n\nBenedict Evans\n\n#\n\nWays to think about AGI\n\nHow do we think about a fundamentally unknown and unknowable risk, when the\nexperts agree only that they have no idea?\n\nThe manuscript for \u2018A Logic Named Joe\u2019\n\nIn 1946, my grandfather, writing as \u2018Murray Leinster\u2019, published a science\nfiction story called \u2018A Logic Named Joe\u2019. Everyone has a computer (a \u2018logic\u2019)\nconnected to a global network that does everything from banking to newspapers\nand video calls. One day, one of these logics, \u2018Joe\u2019, starts giving helpful\nanswers to any request, anywhere on the network: invent an undetectable\npoison, say, or suggest the best way to rob a bank. Panic ensues - \u2018Check your\ncensorship circuits!\u2019 - until they work out what to unplug. (My other\ngrandfather, meanwhile, was using computers to spy on the Germans, and then\nthe Russians.)\n\nFor as long as we\u2019ve thought about computers, we\u2019ve wondered if they could\nmake the jump from mere machines, shuffling punch-cards and databases, to some\nkind of \u2018artificial intelligence\u2019, and wondered what that would mean, and\nindeed, what we\u2019re trying to say with the word \u2018intelligence\u2019. There\u2019s an old\njoke that \u2018AI\u2019 is whatever doesn\u2019t work yet, because once it works, people say\n\u2018that\u2019s not AI - it\u2019s just software\u2019. Calculators do super-human maths, and\ndatabases have super-human memory, but they can\u2019t do anything else, and they\ndon\u2019t understand what they\u2019re doing, any more than a dishwasher understands\ndishes, or a drill understands holes. A drill is just a machine, and databases\nare \u2018super-human\u2019 but they\u2019re just software. Somehow, people have something\ndifferent, and so, on some scale, do dogs, chimpanzees and octopuses and many\nother creatures. AI researchers have come to talk about this as \u2018general\nintelligence\u2019 and hence making it would be \u2018artificial general intelligence\u2019 -\nAGI.\n\nIf we really could create something in software that was meaningfully\nequivalent to human intelligence, it should be obvious that this would be a\nvery big deal. Can we make software that can reason, plan, and understand? At\nthe very least, that would be a huge change in what we could automate, and as\nmy grandfather and a thousand other science fiction writers have pointed out,\nit might mean a lot more.\n\nEvery few decades since 1946, there\u2019s been a wave of excitement that sometime\nlike this might be close, each time followed by disappointment and an \u2018AI\nWinter\u2019, as the technology approach of the day slowed down and we realised\nthat we needed an unknown number of unknown further breakthroughs. In 1970 the\nAI pioneer Marvin Minsky claimed that in \u201cfrom three to eight years we will\nhave a machine with the general intelligence of an average human being\u201d, but\neach time we thought we had an approach that would produce that, it turned out\nthat it was just more software (or just didn\u2019t work).\n\nAs we all know, the Large Language Models (LLMs) that took off 18 months ago\nhave driven another such wave. Serious AI scientists who previously thought\nAGI was probably decades away now suggest that it might be much closer. At the\nextreme, the so-called \u2018doomers\u2019 argue there is a real risk of AGI emerging\nspontaneously from current research and that this could be a threat to\nhumanity, and calling for urgent government action. Some of this comes from\nself-interested companies seeking barriers to competition (\u2018This is very\ndangerous and we are building it as fast as possible, but don\u2019t let anyone\nelse do it\u2019), but plenty of it is sincere.\n\n(I should point out, incidentally, that the doomers\u2019 \u2018existential risk\u2019\nconcern that an AGI might want to and be able to destroy or control humanity,\nor treat us as pets, is quite independent of more quotidian concerns about,\nfor example, how governments will use AI for face recognition, or talking\nabout AI bias, or AI deepfakes, and all the other ways that people will abuse\nAI or just screw up with it, just as they have with every other technology.)\n\nHowever, for every expert that thinks that AGI might now be close, there\u2019s\nanother who doesn\u2019t. There are some who think LLMs might scale all the way to\nAGI, and others who think, again, that we still need an unknown number of\nunknown further breakthroughs.\n\nMore importantly, they would all agree that they don\u2019t actually know. This is\nwhy I used terms like \u2018might\u2019 or \u2018may\u2019 - our first stop is an appeal to\nauthority (often considered a logical fallacy, for what that\u2019s worth), but the\nauthorities tell us that they don\u2019t know, and don\u2019t agree.\n\nThey don\u2019t know, either way, because we don\u2019t have a coherent theoretical\nmodel of what general intelligence really is, nor why people seem to be better\nat it than dogs, nor how exactly people or dogs are different to crows or\nindeed octopuses. Equally, we don\u2019t know why LLMs seem to work so well, and we\ndon\u2019t know how much they can improve. We know, at a basic and mechanical\nlevel, about neurons and tokens, but we don\u2019t know why they work. We have many\ntheories for parts of these, but we don\u2019t know the system. Absent an appeal to\nreligion, we don\u2019t know of any reason why AGI cannot be created (it doesn\u2019t\nappear to violate any law of physics), but we don\u2019t know how to create it or\nwhat it is, except as a concept.\n\nAnd so, some experts look at the dramatic progress of LLMs and say \u2018perhaps!\u2019\nand other say \u2018perhaps, but probably not!\u2019, and this is fundamentally an\nintuitive and instinctive assessment, not a scientific one.\n\nIndeed, \u2018AGI\u2019 itself is a thought experiment, or, one could suggest, a place-\nholder. Hence, we have to be careful of circular definitions, and of defining\nsomething into existence, certainty or inevitably.\n\nIf we start by defining AGI as something that is in effect a new life form,\nequal to people in \u2018every\u2019 way (barring some sense of physical form), even\ndown to concepts like \u2018awareness\u2019, emotions and rights, and then presume that\ngiven access to more compute it would be far more intelligent (and that there\neven is a lot more spare compute available on earth), and presume that it\ncould immediately break out of any controls, then that sounds dangerous, but\nreally, you\u2019ve just begged the question.\n\nAs Anselm demonstrated, if you define God as something that exists, then\nyou\u2019ve proved that God exists, but you won\u2019t persuade anyone. Indeed, a lot of\nAGI conversations sound like the attempts by some theologians and philosophers\nof the past to deduce the nature of god by reasoning from first principles.\nThe internal logic of your argument might be very strong (it took centuries\nfor philosophers to work out why Anselm\u2019s proof was invalid) but you cannot\ncreate knowledge like that.\n\nEqually, you can survey lots of AI scientists about how uncertain they feel,\nand produce a statistically accurate average of the result, but that doesn\u2019t\nof itself create certainty, any more than surveying a statistically accurate\nsample of theologians would produce certainty as to the nature of god, or,\nperhaps, bundling enough sub-prime mortgages together can produce AAA bonds,\nanother attempt to produce certainty by averaging uncertainty. One of the most\nbasic fallacies in predicting tech is to say \u2018people were wrong about X in the\npast so they must be wrong about Y now\u2019, and the fact that leading AI\nscientists were wrong before absolutely does not tell us they\u2019re wrong now,\nbut it does tell us to hesitate. They can all be wrong at the same time.\n\nMeanwhile, how do you know that\u2019s what general intelligence would be like?\nIsaiah Berlin once suggested that even presuming there is in principle a\npurpose to the universe, and that it is in principle discoverable, there\u2019s no\na priori reason why it must be interesting. \u2018God\u2019 might be real, and boring,\nand not care about us, and we don\u2019t know what kind of AGI we would get. It\nmight scale to 100x more intelligent than a person, or it might be much faster\nbut no more intelligent (is intelligence \u2018just\u2019 about speed?). We might\nproduce general intelligence that\u2019s hugely useful but no more clever than a\ndog, which, after all, does have general intelligence, and, like databases or\ncalculators, a super-human ability (scent). We don\u2019t know.\n\nTaking this one step further, as I listened to Mark Zuckerberg talking about\nLlama 3, it struck me that he talks about \u2018general intelligence\u2019 as something\nthat will arrive in stages, with different modalities a little at at a time.\nMaybe people will point at the \u2018general intelligence\u2019 of Llama 6 or ChatGPT 7\nand say \u201cThat\u2019s not AGI, it\u2019s just software!\u201d We created the term AGI because\nAI came just to mean software, and perhaps \u2018AGI\u2019 will be the same, and we\u2019'll\nneed to invent another term.\n\nThis fundamental uncertainty, even at the level of what we\u2019re talking about,\nis perhaps why all conversations about AGI seem to turn to analogies. If you\ncan compare this to nuclear fission then you know what to expect, and you know\nwhat to do. But this isn\u2019t fission, or a bioweapon, or a meteorite. This is\nsoftware, that might or might not turn into AGI, that might or might not have\ncertain characteristics, some of which might be bad, and we don\u2019t know. And\nwhile a giant meteorite hitting the earth could only be bad, software and\nautomation are tools, and over the last 200 years automation has sometimes\nbeen bad for humanity, but mostly it\u2019s been a very good thing that we should\nwant much more of.\n\nHence, I\u2019ve already used theology as an analogy, but my preferred analogy is\nthe Apollo Program. We had a theory of gravity, and a theory of the\nengineering of rockets. We knew why rockets didn\u2019t explode, and how to model\nthe pressures in the combustion chamber, and what would happen if we made them\n25% bigger. We knew why they went up, and how far they needed to go. You could\nhave given the specifications for the Saturn rocket to Isaac Newton and he\ncould have done the maths, at least in principle: this much weight, this much\nthrust, this much fuel... will it get there? We have no equivalents here. We\ndon\u2019t know why LLMs work, how big they can get, or how far they have to go.\nAnd yet, we keep making them bigger, and they do seem to be getting close.\nWill they get there? Maybe, yes!\n\nOn this theme, some people suggest that we are in the empirical stage of AI or\nAGI: we are building things and making observations without knowing why they\nwork, and the theory can come later, a little as Galileo came before Newton\n(there\u2019s an old English joke about a Frenchman who says \u2018that\u2019s all very well\nin practice, but does it work in theory\u2019). Yet while we can, empirically, see\nthe rocket going up, we don\u2019t know how far away the moon is. We can\u2019t plot\npeople and ChatGPT on a chart and draw a line to say when one will reach the\nother, even just extrapolating the current rate of growth.\n\nAll analogies have flaws, and the flaw in my analogy, of course, is that if\nthe Apollo program went wrong the downside was not, even theoretically, the\nend of humanity. A little before my grandfather, here\u2019s another magazine\nwriter on unknown risks:\n\nI was reading in the paper the other day about those birds who are trying to\nsplit the atom, the nub being that they haven't the foggiest as to what will\nhappen if they do. It may be all right. On the other hand, it may not be all\nright. And pretty silly a chap would feel, no doubt, if, having split the\natom, he suddenly found the house going up in smoke and himself torn limb from\nlimb.\n\nRight ho, Jeeves, PG Wodehouse, 1934\n\nWhat then, is your preferred attitude to risks that are real but unknown??\nWhich thought experiment do you prefer? We can return to half-forgotten\nundergraduate philosophy (Pascals\u2019s Wager! Anselm\u2019s Proof!), but if you can\u2019t\nknow, do you worry, or shrug? How do we think about other risks? Meteorites\nare a poor analogy for AGI because we know they\u2019re real, we know they could\ndestroy mankind, and they have no benefits at all (unless they\u2019re very very\nsmall). And yet, we\u2019re not really looking for them.\n\nPresume, though, you decide the doomers are right: what can you do? The\ntechnology is in principle public. Open source models are proliferating. For\nnow, LLMs need a lot of expensive chips (Nvidia sold $47.5bn in the last 12\nmonths and can\u2019t meet demand), but on a decade\u2019s view the models will get more\nefficient and the chips will be everywhere. In the end, you can\u2019t ban\nmathematics. On a scale of decades, it will happen anyway. If you must use\nanalogies to nuclear fission, imagine if we discovered a way that anyone could\nbuild a bomb in their garage with household materials - good luck preventing\nthat. (A doomer might respond that this answers the Fermi paradox: at a\ncertain point every civilisation creates AGI and it turns them into\npaperclips.)\n\nBy default, though, this will follow all the other waves of AI, and become\n\u2018just\u2019 more software and more automation. Automation has always produced\nfrictional pain, back to the Luddites, and the UK\u2019s Post Office scandal\nreminds us that you don\u2019t need AGI for software to ruin people\u2019s lives. LLMs\nwill produce more pain and more scandals, but life will go on. At least,\nthat\u2019s the answer I prefer myself.\n\nArtificial IntelligenceBenedict Evans4 May 2024\n\n## Subscribe\n\nWhat mattered in tech this week?\n\nOnce a week, I send an email newsletter to over 150,000 people - what happened\nin tech that actually mattered, and what it means. I pick out the changes and\nideas you don\u2019t want to miss in all the noise, and give them context and\nanalysis.\n\nSubscribe\n\n\u00a9 Benedict Evans\n\nHours\n\nNewsletter\n\n2024\n\nWhat mattered in tech this week?\n\nOnce a week, I send a newsletter to 175,000 people - what happened in tech\nthat actually mattered, and what it means. I pick out the changes and ideas\nyou don\u2019t want to miss in all the noise, and give them context and analysis.\n\nSUBSCRIBE\n\n\u00a9 BENEDICT EVANS\n\nThis site uses cookies to function, and for anonymous analytics. You can read\nthe privacy policy here.\n\n", "frontpage": false}
