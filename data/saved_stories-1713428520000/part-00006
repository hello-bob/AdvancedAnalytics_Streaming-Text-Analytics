{"aid": "40071940", "title": "Nvidia Speech and Translation AI Models Set Records for Speed and Accuracy", "url": "https://developer.nvidia.com/blog/nvidia-speech-and-translation-ai-models-set-records-for-speed-and-accuracy/", "domain": "nvidia.com", "votes": 6, "user": "belter", "posted_at": "2024-04-18 01:07:12", "comments": 0, "source_title": "NVIDIA Speech and Translation AI Models Set Records for Speed and Accuracy | NVIDIA Technical Blog", "source_text": "NVIDIA Speech and Translation AI Models Set Records for Speed and Accuracy | NVIDIA Technical Blog\n\nDEVELOPER\n\nRelated Resources\n\nConversational AI\n\n# NVIDIA Speech and Translation AI Models Set Records for Speed and Accuracy\n\nMar 19, 2024\n\nBy Gordana Neskovic, Somshubra Majumdar, Nithin Rao Koluguri, Akshit Arora,\nHainan Xu and Elena Rastorgueva\n\n+13\n\nLike\n\nDiscuss (0)\n\n  * L\n  * T\n  * F\n  * R\n  * E\n\nSpeech and translation AI models developed at NVIDIA are pushing the\nboundaries of performance and innovation. The NVIDIA Parakeet automatic speech\nrecognition (ASR) family of models and the NVIDIA Canary multilingual,\nmultitask ASR and translation model currently top the Hugging Face Open ASR\nLeaderboard. In addition, a multilingual P-Flow-based text-to-speech (TTS)\nmodel won the LIMMITS \u201924 challenge by synthesizing a speaker\u2019s voice into\nseven languages using a short audio clip.\n\nThis post details how several of these best-in-the-world models are breaking\nnew ground in speech and translation AI, from speech recognition to custom\nvoice creation.\n\nFigure 1. NVIDIA speech AI models dominate the Hugging Face Open ASR\nLeaderboard\n\n## NVIDIA Parakeet speech recognition models\n\nThe NVIDIA Parakeet family of models includes Parakeet CTC 1.1B, Parakeet CTC\n0.6B, Parakeet RNNT 1.1B, Parakeet RNNT 0.6B, and Parakeet-TDT 1.1B. These\nmodels provide robust English speech transcription with a variety of options\nfor different customer applications, accuracy, speed, and other requirements.\nThe models come in two sizes: 0.6 billion and 1.1 billion parameters.\n\nKey advantages of Parakeet models include:\n\n  * State-of-the-art accuracy delivered with superior word error rate (WER) across diverse audio sources and domains.\n  * Fast inference speed of 1,336 hours of audio in one hour of real time for transcription with Parakeet CTC 1.1 B, 1,212 with Parakeet TDT 1.1B, and 1,120 with Parakeet RNN-T 1.1B. Note measurements conducted outside of Hugging Face leaderboard evaluations with the additional algorithmic and CUDA-level optimizations.\n  * Superior noise robustness to background speech and non-speech segments.\n  * Seamless integration and customization as models come with ready-to-use pretrained checkpoints, facilitating ease of deployment for inference and fine-tuning tasks.\n\nThe effectiveness of the Parakeet CTC and RNNT models lies in end-to-end\ntraining using the fast conformer (FC) encoder, recurrent neural network\ntransducer (RNNT) and connectionist temporal classification (CTC) decoders.\nFor more details, see Investigating End-to-End ASR Architectures for Long Form\nAudio Transcription and Fast Conformer with Linearly Scalable Attention for\nEfficient Speech Recognition.\n\nFigure 2. Architecture of the NVIDIA Parakeet encoder with blocks of\ndownsampling and subsampling, conformer encoder blocks with limited context\nattention (LCA), and global token (GT)\n\n## Parakeet-TDT model for turbocharging speech recognition\n\nThe Parakeet-TDT (token-and-duration transducer) 1.1B model achieves the best\naccuracy among the Parakeet family when transcribing spoken English while\nrunning 64% faster than the second-best Parakeet model at the Hugging Face\nleaderboard evaluations.\n\nWhat makes Parakeet-TDT 1.1B stand out in terms of speed and accuracy is its\nTDT model architecture, which is a novel sequence modeling architecture\ndeveloped by NVIDIA. To learn more, see Efficient Sequence Transduction by\nJointly Predicting Tokens and Durations.\n\nParakeet-TDT 1.1B decouples token and duration predictions and uses duration\noutput to skip majority blank predictions. The reduction of wasteful\ncomputation during the recognition process significantly accelerates inference\nand enhances robustness to noisy speech, compared to traditional transducer\nmodels.\n\nFigure 3. The Parakeet-TDT architecture mitigates wasted computations by\nintelligently detecting and skipping blank frames during recognition\n\n## Canary multilingual model for speech recognition and translation\n\nCanary 1B is a multilingual multitasking model with state-of-the-art accuracy\non multiple benchmarks. It transcribes English, German, French, and Spanish\nspeech both with and without punctuation and capitalization (PnC). It also\nsupports bidirectional English translation from and to German, French, and\nSpanish. With an average WER of 6.67%, Canary is currently the most accurate\nmodel on the Hugging Face Open ASR Leaderboard, outperforming all other models\nby a wide margin.\n\nVideo 1. Watch a demo of the Canary multilingual model, which transcribes and\ntranslates English, German, Spanish, and French\n\nCanary is an encoder-decoder model built on several innovations. The encoder\nis a fast conformer, optimized for ~3x compute savings and ~4x memory savings\ncompared to the conformer encoder.\n\nThe Canary encoder processes audio in the form of a log-Mel spectrogram and\nthe transformer decoder generates output text tokens in an auto-regressive\nmanner. The decoder is prompted with unique tokens to control whether Canary\nperforms transcription or translation. Canary also incorporates the\nconcatenated tokenizer, offering explicit control of the output token space.\nFor more information, see Unified Model for Code-Switching Speech Recognition\nand Language Identification Based on Concatenated Tokenizer.\n\n## P-Flow for custom voice creation\n\nNVIDIA won the LIMMITS \u201824 voice challenge by harnessing the P-Flow zero-shot\nTTS model to create a customized high-quality personalized voice for a\nspeaker. P-Flow can use a speech prompt as short as three seconds. Zero-shot\nrefers to generating speech with the voice characteristics of a speaker not in\nthe model training data. The speech created by the P-Flow model matches the\nvoice in the speech prompt, and is preferred in human likeness and voice\nsimilarity compared to state-of-the-art counterparts.\n\nP-Flow consists of a speech-prompted text encoder for speaker voice\nadaptation, and a flow-matching generative decoder for fast, high-quality\nspeech synthesis. The encoder uses speech prompts and text inputs to generate\na speaker-conditional text representation. The decoder uses this speaker-\nconditional text representation to synthesize high-quality personalized speech\nsignificantly faster than in real time. To learn more, see P-Flow: A Fast and\nData-Efficient Zero-Shot TTS through Speech Prompting.\n\nFigure 4. P-Flow architecture with speech-prompted text encoder for speaker\nadaptation and flow matching generative model for fast and high-quality speech\nsynthesis\n\nP-Flow creates high-quality voices without requiring super-large datasets,\ncomplex training setups, representation quantization steps, pretraining tasks,\nor slow autoregressive formulations.\n\nFor the voice challenge, the NVIDIA team extended the zero-shot TTS ability of\nP-Flow from its original language, English, to include seven additional Indic\nlanguages. This effectively creates a multilingual TTS system such that an\nunseen speaker can speak in any of seven targeted languages with a native\naccent, given just a three-second speech prompt.\n\nThe following samples showcase the NVIDIA-built multilingual P-Flow-based TTS.\nStarting with a three-second voice sample of a Kannada speaker, the model\nproduces the same vocal qualities with native Hindi and English accents.\n\n### Input\n\nThe input is a three-second sample of a native Kannada speaker saying, \u201cThanks\nto their work driving AI forward, Akshit Arora and Rafael Valle could someday\nspeak to their spouses\u2019 families in their native languages.\u201d\n\n### Output\n\nThe output is a similar-sounding synthesized voice reading input text in Hindi\nand English.\n\nHindi:\n\nEnglish:\n\n## Conclusion\n\nNVIDIA speech and translation AI models are pushing the boundaries of\nperformance and innovation. With its RNNT and CTC variants, the Parakeet\nfamily of models offers a spectrum of options, balancing accuracy and speed to\nsuit diverse deployment needs. The Parakeet-TDT model sets a new benchmark by\ncoupling superior accuracy with unprecedented speed, epitomizing efficiency in\nspeech recognition.\n\nThe Canary multilingual model emerges as the new standard, excelling in speech\nrecognition and translation across multiple languages with unparalleled\naccuracy.\n\nThe P-Flow multilingual model enables the creation of custom voices, offering\na fast and data-efficient solution for personalized speech synthesis. By\nharnessing P-Flow, NVIDIA synthesized speaker voices across multiple languages\nwith remarkable fidelity and efficiency.\n\nThe groundbreaking Parakeet-CTC and P-Flow models are available now,\nexclusively for enterprises. This restriction is to prevent potential misuse\nof the P-Flow zero-shot TTS in the form of, for example, voice impersonation\nof public figures and non-consenting individuals.\n\nParakeet-RNNT, Parakeet-TDT, and the Canary models will be available soon as\npart of NVIDIA Riva. Experience AI models through the NVIDIA API catalog and\naccess to run them anywhere with NVIDIA NIM\u2014cloud, data center, workstation,\nor PC. NVIDIA LaunchPad provides the necessary hardware and software stacks on\nprivate-hosted infrastructure for additional exploration.\n\nJoin us in person or virtually for these NVIDIA GTC 2024 sessions to learn\nmore about the potential of AI-driven communication:\n\n  * Speech AI Demystified\n  * Speaking in Every Language: A Quick Start Guide to TTS Models for Accented, Multilingual Communication\n  * Build a RAG-Powered Application with a Human Voice Interface\n  * Mastering Speech AI for Multilingual Multimedia Transformation\n  * Secure AI-Driven Translation in Video Conferencing\n  * Adapting Conformer-Based ASR Models for Conversations Over the Phone\n  * Behind the Scenes of Running a Conversational Character in a 3D Scene\n  * Multi-Speaker ASR with NVIDIA NeMo Toolkit \u2014Training & Inference\n\n## Related resources\n\n  * GTC session: AI/ML Speech Recognition/Inferencing: NVIDIA Riva on Red Hat OpenShift with PowerFlex\n  * GTC session: Adapting Conformer-Based ASR Models for Conversations Over the Phone\n  * GTC session: Throughput Performance Benchmarking: Pre-Training Foundational Large Language Models on Kubernetes\n  * NGC Containers: NVIDIA NIM for LLMs\n  * SDK: Riva\n  * SDK: NGC Models\n\nDiscuss (0)\n\n+13\n\nLike\n\n## Tags\n\nConversational AI | General | Academia / Education | Aerospace | Agriculture | Architecture / Engineering / Construction | Automotive / Transportation | Cloud Services | Consumer Internet | Energy | Financial Services | Gaming | Hardware / Semiconductor | Healthcare & Life Sciences | HPC / Scientific Computing | Manufacturing | Media & Entertainment | Public Sector | Restaurant / Quick-Service | Retail / Consumer Packaged Goods | Smart Cities / Spaces | Telecommunications | LaunchPad | Riva | Intermediate Technical | Benchmark | News | GTC March 2024 | featured | Pretrained Models | Speech & Audio Processing | Speech AI | Speech Recognition / Diarization | Translation\n\n## About the Authors\n\nAbout Gordana Neskovic Gordana Neskovic is on the AI / DL product marketing\nteam responsible for NVIDIA Maxine. Gordana has held various product\nmarketing, data scientist, AI architect, and engineering roles at VMware,\nWells Fargo, Pinterest, SFO-ITT, and KLA-Tencor before joining NVIDIA. She\nholds a Ph.D. from Santa Clara University and master\u2019s and bachelor's degrees\nin electrical engineering from the University of Belgrade, Serbia.\n\nView all posts by Gordana Neskovic\n\nAbout Somshubra Majumdar Somshubra Majumdar is a senior research scientist\nworking on the NVIDIA NeMo toolkit. He received a bachelor's degree in\nComputer Engineering from the University of Mumbai in 2016, and a master's\ndegree in Computer Science from the University of Illinois at Chicago in 2018.\nHis research interests include automatic speech recognition, speech\nclassification, time series classification, and practical applications of deep\nlearning.\n\nView all posts by Somshubra Majumdar\n\nAbout Nithin Rao Koluguri Nithin Rao Koluguri is a senior research scientist\non the NVIDIA Conversational AI team, focusing on advancing speech and speaker\nrecognition models. As a key contributor to the NVIDIA NeMo toolkit, he plays\na vital role in enhancing features for conversational AI model development.\n\nView all posts by Nithin Rao Koluguri\n\nAbout Akshit Arora Akshit Arora is a senior data scientist at NVIDIA, where he\nworks on deploying conversational AI models on GPUs at scale. He's a graduate\nof University of Colorado at Boulder, where he applied deep learning to\nimprove knowledge tracing on a K-12 online tutoring platform. His work spans\nmultilingual text-to-speech, ed-tech, and practical applications of deep\nlearning.\n\nView all posts by Akshit Arora\n\nAbout Hainan Xu Hainan Xu is a staff research scientist at NVIDIA where he\nfocuses on developing new methods and architectures for sequence modeling in\nthe realm of speech and natural language processing. He obtained his PhD in\nComputer Science from Johns Hopkins University, advised by Daniel Povey and\nSanjeev Khudanpur.\n\nView all posts by Hainan Xu\n\nAbout Elena Rastorgueva Elena Rastorgueva is a senior applied research\nscientist at NVIDIA. She has worked on a range of projects to improve the\naccuracy of the speech recognition and translation models in NVIDIA NeMo\ntoolkit. Before joining NVIDIA, she interned at Amazon Alexa and Mila. Elena\nhas a master\u2019s degree in Engineering from the University of Cambridge.\n\nView all posts by Elena Rastorgueva\n\n## Comments\n\n### Start the discussion at forums.developer.nvidia.com\n\n## Related posts\n\n### New Languages, Enhanced Cybersecurity, and Medical AI Frameworks Unveiled\nat GTC\n\nNew Languages, Enhanced Cybersecurity, and Medical AI Frameworks Unveiled at\nGTC\n\n### Create Speech AI Applications in Multiple Languages and Customize Text-to-\nSpeech with Riva\n\nCreate Speech AI Applications in Multiple Languages and Customize Text-to-\nSpeech with Riva\n\n### NVIDIA Announces Riva Speech AI and Large Language Modeling Software For\nEnterprise\n\nNVIDIA Announces Riva Speech AI and Large Language Modeling Software For\nEnterprise\n\n### NVIDIA at INTERSPEECH 2021\n\nNVIDIA at INTERSPEECH 2021\n\n### Develop Smaller Speech Recognition Models with the NVIDIA NeMo Framework\n\nDevelop Smaller Speech Recognition Models with the NVIDIA NeMo Framework\n\n## Related posts\n\n### Mastering LLM Techniques: Training\n\nMastering LLM Techniques: Training\n\n### Elevate Enterprise Generative AI App Development with NVIDIA AI on Azure\nMachine Learning\n\nElevate Enterprise Generative AI App Development with NVIDIA AI on Azure\nMachine Learning\n\n### Build Custom Enterprise-Grade Generative AI with NVIDIA AI Foundation\nModels\n\nBuild Custom Enterprise-Grade Generative AI with NVIDIA AI Foundation Models\n\n### How to Deploy NVIDIA Riva Speech and Translation AI in the Public Cloud\n\nHow to Deploy NVIDIA Riva Speech and Translation AI in the Public Cloud\n\n### How Language Neutralization Is Transforming Customer Service Contact\nCenters\n\nHow Language Neutralization Is Transforming Customer Service Contact Centers\n\nSign up for NVIDIA News\n\nSubscribe\n\nFollow NVIDIA Developer\n\nFind more news and tutorials on NVIDIA Technical Blog\n\n  * Privacy Policy\n  * Manage My Privacy\n  * Do Not Sell or Share My Data\n  * Terms of Use\n  * Cookie Policy\n  * Contact\n\nCopyright \u00a9 2024 NVIDIA Corporation\n\n  * L\n  * T\n  * F\n  * R\n  * E\n\n  * Join\n\n  * Home\n  * Blog\n  * Forums\n  * Docs\n  * Downloads\n  * Training\n\nNVIDIA uses cookies to enable and improve the use of the website. Please see\nour Cookie Policy for more information.\n\nNVIDIA uses cookies to enable and improve the use of the website. GPC signal\ndetected and only \u2018Required\u2019 cookies have been enabled. To update your\ncommunication preferences please visit the Preference Center. Please see our\nCookie Policy for more information.\n\n## Cookie Settings\n\nNVIDIA websites use cookies to deliver and improve the visitor experience.\nLearn more about the cookies we use on our Cookie Policy page.\n\n#### Required Cookies\n\nThese cookies are required for our sites to function and cannot be turned off.\n\n#### Performance Cookies\n\nThese cookies provide information to help us improve your web experience by\nmonitoring the performance of our website and collecting anonymous data on how\nyou use it.\n\n#### Advertising Cookies\n\nSet by our advertising partners, these cookies are used to build a profile of\nyour interests and show you relevant ads on other sites. They do not store\npersonal information, but are based on uniquely identifying your browser and\ninternet device.\n\n  * ##### Personalization Cookies\n\nlabel\n\nThese cookies are used to better understand and optimize your web experience,\nsuch as pages visited or purchases made through our e-store. These cookies and\nthe information they collect may be managed by other companies, and the\ninformation collected by these cookies may be used to build a profile of your\ninterests and show you relevant advertising on other sites. They do not store\ndirect personally identifiable information, but are based on uniquely\nidentifying your browser and internet device. Cookie Details\n\n### Cookie List\n\nlabel\n\nConsent Leg.Interest\n\nlabel\n\nlabel\n\nlabel\n\nSearch\n\n", "frontpage": true}
