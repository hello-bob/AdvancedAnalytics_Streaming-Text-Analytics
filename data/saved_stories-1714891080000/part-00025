{"aid": "40261315", "title": "Transformers Dashboard", "url": "https://v4nn4.github.io/posts/transformers-dashboard/", "domain": "v4nn4.github.io", "votes": 1, "user": "v4nn4", "posted_at": "2024-05-05 00:03:19", "comments": 0, "source_title": "Transformers Dashboard \ud83e\udd16\ud83d\udcc8", "source_text": "Transformers Dashboard \ud83e\udd16\ud83d\udcc8 | v4nn4's blog\n\n# Transformers Dashboard \ud83e\udd16\ud83d\udcc8\n\nMay 4, 2024 \u00b7 4 min \u00b7 684 words \u00b7 v4nn4 | Suggest Changes\n\n\u27a1\ufe0f https://transformers-dashboard.vercel.app\n\nSince the publication of the now famous 2017 paper Attention is All You\nNeed^1, many large language models based on the transformer architecture have\nemerged. Fortunately, some studies ^2 ^3 have compiled extensive data on many\npublished models, including the dimensions of their transformers.\n\nMuch like my experience learning about CNNs and their growth in complexity, I\nwanted to analyze LLM transformers. Which models are the largest? What is the\noptimal size for the feed-forward layer? Is it better to add more embeddings\nor more attention heads? Can we easily derive the total number of parameters\nfrom the network dimensions?\n\n## Transformer model parameters#\n\nI will use the notations from the original Attention is All You Need ^1 paper.\n\n  * : the number of layers\n  * : the number of attention heads\n  * : the size of the embeddings\n  * : the size of the hidden FFN layer\n  * : the vocabulary size, that is the number of tokens used\n\nIn order to count model parameters, we need break the model down into building\nblocks:\n\n  * Multi-head attention block : trainable parameters are contained in weight matrices , for , as well as and their associated biaises. We then multiply the added number of parameters by , the number of heads. Using the relationship ^1 we get\n\n  * Feed-forward block : in both the encoder and the decoder, the output of size is passed throught a feed-forward block ^1 : . This leads to the following number of parameters\n\n  * Layer normalization block : gain and bias with dimension\n\n  * Encoder : the encoder has one MHA and one FFN. Each one has a norm layer.\n\n  * Decoder : the decoder has two MHA and one FFN. Each one has a norm layer.\n\n  * Linear block : the linear block outputs as many logits as the vocabulary size, hence the dimension of its matrix and bias is\n\nFinally the total number of parameters is\n\n## Gathering data#\n\nAlthough the aforementioned studies ^2 ^3 are invaluable and packed with\nuseful information, they\u2019ve become quickly outdated given the pace of model\nreleases these days. I decided to collect my own data from original research\npapers, announcement posts, as well as some Hugging Face configuration files.\nI focused on models published by large research teams and/or that had\nsignificant impact.\n\nHere are my findings:\n\n  * GPT ^4 used a causal decoder-only transformer, which many models have adopted. This means the encoder block is not present in most models\n  * GPT used\n  * According to ^2, sometimes biases are omitted in the model\n  * Sometimes, some parameters are omitted in the paper and implied from previous version of the model\n  * Closed-source models rarely disclose detailed architecture information\n  * Hugging Face configuration files generally display one version (size) from a family of models, potentially leading to misleading interpretations\n\n## Publishing a dashboard#\n\nOnce the data started to look interesting, I put together a small Next.js app\nusing shadcn/ui data tables. A dashboard is available at https://transformers-\ndashboard.vercel.app.\n\n  1. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. \u21a9\ufe0e \u21a9\ufe0e \u21a9\ufe0e \u21a9\ufe0e\n\n  2. Naveed, H., Khan, A. U., Qiu, S., Saqib, M., Anwar, S., Usman, M., ... & Mian, A. (2023). A comprehensive overview of large language models. arXiv preprint arXiv:2307.06435. \u21a9\ufe0e \u21a9\ufe0e \u21a9\ufe0e\n\n  3. Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., ... & Wen, J. R. (2023). A survey of large language models. arXiv preprint arXiv:2303.18223. \u21a9\ufe0e \u21a9\ufe0e\n\n  4. Radford, A., & Narasimhan, K. (2018). Improving Language Understanding by Generative Pre-Training. \u21a9\ufe0e\n\n\u00a9 2024 v4nn4's blog Powered by Hugo & PaperMod\n\n", "frontpage": false}
