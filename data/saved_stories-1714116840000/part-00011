{"aid": "40162161", "title": "Pgcopydb: Copy a Postgres database to a target Postgres server", "url": "https://github.com/dimitri/pgcopydb", "domain": "github.com/dimitri", "votes": 4, "user": "thunderbong", "posted_at": "2024-04-25 19:48:40", "comments": 0, "source_title": "GitHub - dimitri/pgcopydb: Copy a Postgres database to a target Postgres server (pg_dump | pg_restore on steroids)", "source_text": "GitHub - dimitri/pgcopydb: Copy a Postgres database to a target Postgres server (pg_dump | pg_restore on steroids)\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\ndimitri / pgcopydb Public\n\n  * Notifications\n  * Fork 68\n  * Star 993\n\nCopy a Postgres database to a target Postgres server (pg_dump | pg_restore on steroids)\n\n### License\n\nView license\n\n993 stars 68 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# dimitri/pgcopydb\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n10 Branches\n\n26 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nkbarberFix infinite recursion for schema_list_extensions() catalog query\n(#752)Apr 20, 20249a151a6 \u00b7 Apr 20, 2024Apr 20, 2024\n\n## History\n\n546 Commits  \n  \n### .github/workflows\n\n|\n\n### .github/workflows\n\n| Problem: Post merge docker push fails (#724)| Mar 27, 2024  \n  \n### ci\n\n|\n\n### ci\n\n| Overhaul of EditorConfig rules (#652)| Jan 31, 2024  \n  \n### debian\n\n|\n\n### debian\n\n| Vendor SQLite amalgamation distribution. (#660)| Jan 31, 2024  \n  \n### docs\n\n|\n\n### docs\n\n| Add skip tablespaces in restore options. (#683)| Mar 4, 2024  \n  \n### src/bin\n\n|\n\n### src/bin\n\n| Fix infinite recursion for schema_list_extensions() catalog query (#752)|\nApr 20, 2024  \n  \n### tests\n\n|\n\n### tests\n\n| Fix infinite recursion for schema_list_extensions() catalog query (#752)|\nApr 20, 2024  \n  \n### .dockerignore\n\n|\n\n### .dockerignore\n\n| Use same connection in table copy worker. (#542)| Nov 29, 2023  \n  \n### .editorconfig\n\n|\n\n### .editorconfig\n\n| Overhaul of EditorConfig rules (#652)| Jan 31, 2024  \n  \n### .gitattributes\n\n|\n\n### .gitattributes\n\n| Vendor SQLite amalgamation distribution. (#660)| Jan 31, 2024  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| Improve documentation templates (#618)| Jan 24, 2024  \n  \n### .readthedocs.yaml\n\n|\n\n### .readthedocs.yaml\n\n| Overhaul of EditorConfig rules (#652)| Jan 31, 2024  \n  \n### CHANGELOG.md\n\n|\n\n### CHANGELOG.md\n\n| Release v0.15. (#630)| Jan 10, 2024  \n  \n### CONTRIBUTING.md\n\n|\n\n### CONTRIBUTING.md\n\n| Introduce CONTRIBUTING.md (#653)| Jan 30, 2024  \n  \n### Dockerfile\n\n|\n\n### Dockerfile\n\n| Dockerfile: make pg_dump more interoperable (#735)| Apr 4, 2024  \n  \n### Dockerfile.debian\n\n|\n\n### Dockerfile.debian\n\n| Fix make deb target, improve build-from-sources docs. (#670)| Feb 5, 2024  \n  \n### Dockerfile.debian-qa\n\n|\n\n### Dockerfile.debian-qa\n\n| Overhaul of EditorConfig rules (#652)| Jan 31, 2024  \n  \n### GIT-VERSION-GEN\n\n|\n\n### GIT-VERSION-GEN\n\n| Release v0.15. (#630)| Jan 10, 2024  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Update the LICENSE document.| Jan 13, 2022  \n  \n### Makefile\n\n|\n\n### Makefile\n\n| Vendor SQLite amalgamation distribution. (#660)| Jan 31, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| Introduce CONTRIBUTING.md (#653)| Jan 30, 2024  \n  \n## Repository files navigation\n\n# pgcopydb\n\n## Introduction\n\npgcopydb is a tool that automates running pg_dump | pg_restore between two running Postgres servers. To make a copy of a database to another server as quickly as possible, one would like to use the parallel options of pg_dump and still be able to stream the data to as many pg_restore jobs.\n\nThe idea would be to use pg_dump --jobs=N --format=directory postgres://user@source/dbname | pg_restore --jobs=N --format=directory -d postgres://user@target/dbname in a way. This command line can't be made to work, unfortunately, because pg_dump --format=directory writes to local files and directories first, and then later pg_restore --format=directory can be used to read from those files again.\n\nGiven that, pgcopydb then uses pg_dump and pg_restore for the schema parts of\nthe process, and implements its own data copying multi-process streaming\nparts. Also, pgcopydb bypasses pg_restore index building and drives that\ninternally so that all indexes may be built concurrently.\n\n## Base Copy and Change Data Capture\n\npgcopydb implements both the base copy of a database and also Change Data\nCapture to allow replay of changes from the source database to the target\ndatabase. The Change Data Capture facility is implemented using Postgres\nLogical Decoding infrastructure and the wal2json plugin.\n\nThe pgcopydb follow command implements a logical replication client for the\nlogical decoding plugin wal2json.\n\nThe pgcopydb clone --follow command implements a full solution for online\nmigration. Beware that online migrations involve a lot more complexities when\ncompared to offline migration. It is always a good idea to first implement\noffline migration first. The command pgcopydb clone is used to implement the\noffline migration approach.\n\n## Documentation\n\nFull documentation is available online, including manual pages of all the\npgcopydb sub-commands. Check out https://pgcopydb.readthedocs.io/.\n\n    \n    \n    $ pgcopydb help pgcopydb clone Clone an entire database from source to target fork Clone an entire database from source to target follow Replay changes from the source database to the target database snapshot Create and export a snapshot on the source database + compare Compare source and target databases + copy Implement the data section of the database copy + dump Dump database objects from a Postgres instance + restore Restore database objects into a Postgres instance + list List database objects from a Postgres instance + stream Stream changes from the source database ping Attempt to connect to the source and target instances help Print help message version Print pgcopydb version pgcopydb compare schema Compare source and target schema data Compare source and target data pgcopydb copy db Copy an entire database from source to target roles Copy the roles from the source instance to the target instance extensions Copy the extensions from the source instance to the target instance schema Copy the database schema from source to target data Copy the data section from source to target table-data Copy the data from all tables in database from source to target blobs Copy the blob data from the source database to the target sequences Copy the current value from all sequences in database from source to target indexes Create all the indexes found in the source database in the target constraints Create all the constraints found in the source database in the target pgcopydb dump schema Dump source database schema as custom files in work directory pre-data Dump source database pre-data schema as custom files in work directory post-data Dump source database post-data schema as custom files in work directory roles Dump source database roles as custom file in work directory pgcopydb restore schema Restore a database schema from custom files to target database pre-data Restore a database pre-data schema from custom file to target database post-data Restore a database post-data schema from custom file to target database roles Restore database roles from SQL file to target database parse-list Parse pg_restore --list output from custom file pgcopydb list databases List databases extensions List all the source extensions to copy collations List all the source collations to copy tables List all the source tables to copy data from table-parts List a source table copy partitions sequences List all the source sequences to copy data from indexes List all the indexes to create again after copying the data depends List all the dependencies to filter-out schema List the schema to migrate, formatted in JSON progress List the progress pgcopydb stream setup Setup source and target systems for logical decoding cleanup Cleanup source and target systems for logical decoding prefetch Stream JSON changes from the source database and transform them to SQL catchup Apply prefetched changes from SQL files to the target database replay Replay changes from the source to the target database, live + sentinel Maintain a sentinel table on the source database receive Stream changes from the source database transform Transform changes from the source database into SQL commands apply Apply changes from the source database into the target database pgcopydb stream sentinel create Create the sentinel table on the source database drop Drop the sentinel table on the source database get Get the sentinel table values on the source database + set Maintain a sentinel table on the source database pgcopydb stream sentinel set startpos Set the sentinel start position LSN on the source database endpos Set the sentinel end position LSN on the source database apply Set the sentinel apply mode on the source database prefetch Set the sentinel prefetch mode on the source database\n\n## Example\n\nWhen using pgcopydb it is possible to achieve the result outlined before with\nthis simple command line:\n\n    \n    \n    $ export PGCOPYDB_SOURCE_PGURI=\"postgres://user@source.host.dev/dbname\" $ export PGCOPYDB_TARGET_PGURI=\"postgres://role@target.host.dev/dbname\" $ pgcopydb clone --table-jobs 8 --index-jobs 2\n\nA typical output from the command would contain lots of lines of logs, and\nthen a table summary with a line per table and some information (timing for\nthe table COPY, cumulative timing for the CREATE INDEX commands), and then an\noverall summary that looks like the following:\n\n    \n    \n    18:26:35 77615 INFO [SOURCE] Copying database from \"port=54311 host=localhost dbname=pgloader\" 18:26:35 77615 INFO [TARGET] Copying database into \"port=54311 dbname=plop\" 18:26:35 77615 INFO STEP 1: dump the source database schema (pre/post data) 18:26:35 77615 INFO /Applications/Postgres.app/Contents/Versions/12/bin/pg_dump -Fc --section pre-data --file /tmp/pgcopydb/schema/pre.dump 'port=54311 host=localhost dbname=pgloader' 18:26:35 77615 INFO /Applications/Postgres.app/Contents/Versions/12/bin/pg_dump -Fc --section post-data --file /tmp/pgcopydb/schema/post.dump 'port=54311 host=localhost dbname=pgloader' 18:26:36 77615 INFO STEP 2: restore the pre-data section to the target database 18:26:36 77615 INFO /Applications/Postgres.app/Contents/Versions/12/bin/pg_restore --dbname 'port=54311 dbname=plop' /tmp/pgcopydb/schema/pre.dump 18:26:36 77615 INFO STEP 3: copy data from source to target in sub-processes 18:26:36 77615 INFO STEP 4: create indexes and constraints in parallel 18:26:36 77615 INFO STEP 5: vacuum analyze each table 18:26:36 77615 INFO Listing ordinary tables in \"port=54311 host=localhost dbname=pgloader\" 18:26:36 77615 INFO Fetched information for 56 tables ... 18:26:37 77615 INFO STEP 6: restore the post-data section to the target database 18:26:37 77615 INFO /Applications/Postgres.app/Contents/Versions/12/bin/pg_restore --dbname 'port=54311 dbname=plop' --use-list /tmp/pgcopydb/schema/post.list /tmp/pgcopydb/schema/post.dump OID | Schema | Name | copy duration | indexes | create index duration ------+----------+-----------------+---------------+---------+---------------------- 17085 | csv | track | 62ms | 1 | 24ms ... ... Step Connection Duration Concurrency --------------------------------------------- ---------- ---------- ------------ Dump Schema source 884ms 1 Prepare Schema target 405ms 1 COPY, INDEX, CONSTRAINTS, VACUUM (wall clock) both 1s281 8 + 2 COPY (cumulative) both 2s040 8 CREATE INDEX (cumulative) target 381ms 2 Finalize Schema target 29ms 1 --------------------------------------------- ---------- ---------- ------------ Total Wall Clock Duration both 2s639 8 + 2 --------------------------------------------- ---------- ---------- ------------\n\n## Installing pgcopydb\n\nSee our documentation.\n\n## Design Considerations (why oh why)\n\nThe reason why pgcopydb has been developed is mostly to allow two aspects that\nare not possible to achieve directly with pg_dump and pg_restore, and that\nrequires just enough fiddling around that not many scripts have been made\navailable to automate around.\n\n### Bypass intermediate files for the TABLE DATA\n\nFirst aspect is that for pg_dump and pg_restore to implement concurrency they\nneed to write to an intermediate file first.\n\nThe docs for pg_dump say the following about the --jobs parameter:\n\n> You can only use this option with the directory output format because this\n> is the only output format where multiple processes can write their data at\n> the same time.\n\nThe docs for pg_restore say the following about the --jobs parameter:\n\n> Only the custom and directory archive formats are supported with this\n> option. The input must be a regular file or directory (not, for example, a\n> pipe or standard input).\n\nSo the first idea with pgcopydb is to provide the --jobs concurrency and\nbypass intermediate files (and directories) altogether, at least as far as the\nactual TABLE DATA set is concerned.\n\nThe trick to achieve that is that pgcopydb must be able to connect to the\nsource database during the whole operation, when pg_restore may be used from\nan export on-disk, without having to still be able to connect to the source\ndatabase. In the context of pgcopydb requiring access to the source database\nis fine. In the context of pg_restore, it would not be acceptable.\n\n### For each table, build all indexes concurrently\n\nThe other aspect that pg_dump and pg_restore are not very smart about is how\nthey deal with the indexes that are used to support constraints, in particular\nunique constraints and primary keys.\n\nThose indexes are exported using the ALTER TABLE command directly. This is\nfine because the command creates both the constraint and the underlying index,\nso the schema in the end is found as expected.\n\nThat said, those ALTER TABLE ... ADD CONSTRAINT commands require a level of\nlocking that prevents any concurrency. As we can read on the docs for ALTER\nTABLE:\n\n> Although most forms of ADD table_constraint require an ACCESS EXCLUSIVE\n> lock, ADD FOREIGN KEY requires only a SHARE ROW EXCLUSIVE lock. Note that\n> ADD FOREIGN KEY also acquires a SHARE ROW EXCLUSIVE lock on the referenced\n> table, in addition to the lock on the table on which the constraint is\n> declared.\n\nThe trick is then to first issue a CREATE UNIQUE INDEX statement and when the\nindex has been built then issue a second command in the form of ALTER TABLE\n... ADD CONSTRAINT ... PRIMARY KEY USING INDEX ..., as in the following\nexample taken from the logs of actually running pgcopydb:\n\n    \n    \n    ... 21:52:06 68898 INFO COPY \"demo\".\"tracking\"; 21:52:06 68899 INFO COPY \"demo\".\"client\"; 21:52:06 68899 INFO Creating 2 indexes for table \"demo\".\"client\" 21:52:06 68906 INFO CREATE UNIQUE INDEX client_pkey ON demo.client USING btree (client); 21:52:06 68907 INFO CREATE UNIQUE INDEX client_pid_key ON demo.client USING btree (pid); 21:52:06 68898 INFO Creating 1 indexes for table \"demo\".\"tracking\" 21:52:06 68908 INFO CREATE UNIQUE INDEX tracking_pkey ON demo.tracking USING btree (client, ts); 21:52:06 68907 INFO ALTER TABLE \"demo\".\"client\" ADD CONSTRAINT \"client_pid_key\" UNIQUE USING INDEX \"client_pid_key\"; 21:52:06 68906 INFO ALTER TABLE \"demo\".\"client\" ADD CONSTRAINT \"client_pkey\" PRIMARY KEY USING INDEX \"client_pkey\"; 21:52:06 68908 INFO ALTER TABLE \"demo\".\"tracking\" ADD CONSTRAINT \"tracking_pkey\" PRIMARY KEY USING INDEX \"tracking_pkey\"; ...\n\nThis trick is worth a lot of performance gains on its own, as has been\ndiscovered and experienced and appreciated by pgloader users already.\n\n## Dependencies\n\nAt run-time pgcopydb depends on the pg_dump and pg_restore tools being\navailable in the PATH. The tools version should match the Postgres version of\nthe target database.\n\nWhen you have multiple versions of Postgres installed, consider exporting the\nPG_CONFIG environment variable to the version you want to use. pgcopydb then\nuses the PG_CONFIG from the path and runs ${PG_CONFIG} --bindir to find the\npg_dump and pg_restore binaries it needs.\n\n## Manual Steps\n\nThe pgcopydb command line also includes entry points that allows implementing\nany step on its own.\n\n  1. pgcopydb snapshot &\n  2. pgcopydb dump schema\n  3. pgcopydb restore pre-data\n  4. pgcopydb copy table-data\n  5. pgcopydb copy blobs\n  6. pgcopydb copy sequences\n  7. pgcopydb copy indexes\n  8. pgcopydb copy constraints\n  9. pgcopydb vacuumdb\n  10. pgcopydb restore post-data\n  11. kill %1\n\nUsing individual commands fails to provide the advanced concurrency\ncapabilities of the main pgcopydb clone command, so it is strongly advised to\nprefer that main command.\n\nAlso when using separate commands, one has to consider the --snapshot option\nthat allows for consistent operations. A background process should then export\nthe snapshot and maintain a transaction opened for the duration of the\noperations. See documentation for pgcopydb snapshot.\n\n## Authors\n\n  * Dimitri Fontaine\n\n## License\n\nCopyright (c) The PostgreSQL Global Development Group.\n\nThis project is licensed under the PostgreSQL License, see LICENSE file for\ndetails.\n\nThis project includes bundled third-party dependencies, see NOTICE file for\ndetails.\n\n## About\n\nCopy a Postgres database to a target Postgres server (pg_dump | pg_restore on steroids)\n\n### Resources\n\nReadme\n\n### License\n\nView license\n\nActivity\n\n### Stars\n\n993 stars\n\n### Watchers\n\n15 watching\n\n### Forks\n\n68 forks\n\nReport repository\n\n## Releases 15\n\nv0.15 Latest\n\nJan 10, 2024\n\n\\+ 14 releases\n\n## Packages 1\n\n  * pgcopydb\n\n## Contributors 25\n\n\\+ 11 contributors\n\n## Languages\n\n  * C 99.1%\n  * Other 0.9%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
