{"aid": "40163994", "title": "BitNetMCU: Neural Networks on the \"10-cent\" RISC-V MCU without Multiplier", "url": "https://cpldcpu.wordpress.com/2024/04/24/implementing-neural-networks-on-the-10-cent-risc-v-mcu-without-multiplier/", "domain": "cpldcpu.wordpress.com", "votes": 3, "user": "kken", "posted_at": "2024-04-25 22:43:42", "comments": 0, "source_title": "Implementing Neural Networks on the \u201c10-cent\u201d RISC-V MCU without Multiplier", "source_text": "Implementing Neural Networks on the \u201c10-cent\u201d RISC-V MCU without Multiplier \u2013\nTim's Blog\n\nSkip to content\n\nTim's Blog\n\n# Implementing Neural Networks on the \u201c10-cent\u201d RISC-V MCU without Multiplier\n\nI have been meaning for a while to establish a setup to implement neural\nnetwork based algorithms on smaller microcontrollers. After reviewing existing\nsolutions, I felt there is no solution that I really felt comfortable with.\nOne obvious issue is that often flexibility is traded for overhead. As always,\nfor a really optimized solution you have to roll your own. So I did. You can\nfind the project here and a detailed writeup here.\n\nIt is always easier to work with a clear challenge: I picked the CH32V003 as\nmy target platform. This is the smallest RISC-V microcontroller on the market\nright now, addressing a $0.10 price point. It sports 2kb of SRAM and 16kb of\nflash. It is somewhat unique in implementing the RV32EC instruction set\narchitecture, which does not even support multiplications. In other words, for\nmany purposes this controller is less capable than an Arduino UNO.\n\nAs a test subject I chose the well-known MNIST dataset, which consists of\nimages of hand written numbers which need to be classified from 0 to 9. Many\ninspiring implementation on Arduino exist for MNIST, for example here. In this\ncase, the inference time was 7 seconds and 82% accuracy was achieved.\n\nThe idea is to train a neural network on a PC and optimize it for inference on\nteh CH32V003 while meetings these criteria:\n\n  1. Be as fast and as accurate as possible\n  2. Low SRAM footprint during inference to fit into 2kb sram\n  3. Keep the weights of the neural network as small as possible\n  4. No multiplications!\n\nThese criteria can be addressed by using a neural network with quantized\nweights, were each weight is represented with as few bits as possible. The\nbest possible results are achieved when training the network already on\nquantized weights (Quantization Aware Training) as opposed to quantized a\nmodel that was trained with high accuracy weights. There is currently some\nhype around using Binary and Ternary weights for large language models. But\nindeed, we can also use these approaches to fit a neural network to a small\nmicrocontroller.\n\nThe benefit of only using a few bits to represent each weight is that the\nmemory footprint is low and we do not need a real multiplication instruction \u2013\ninference can be reduced to additions only.\n\n## Model structure and optimization\n\nFor simplicity reasons, I decided to go for a e network architecture based on\nfully-connected layers instead of convolutional neural networks. The input\nimages are reduced to a size of 16\u00d716=256 pixels and are then fed into the\nnetwork as shown below.\n\nThe implementation of the inference enginge is straightforward since only\nfully connected layers are used. The code snippet below shows the innerloop,\nwhich implements multiplication of 4 bit weights by using adds and shifts. The\nweights us a one-complement encoding without zero, which help with efficiency.\nOne bit, ternary, and 2 bit quantization was implemented in a similar way.\n\n    \n    \n    int32_t sum = 0; for (uint32_t k = 0; k < n_input; k+=8) { uint32_t weightChunk = *weightidx++;\n    \n    for (uint32_t j = 0; j < 8; j++) { int32_t in=*activations_idx++; int32_t tmpsum = (weightChunk & 0x80000000) ? -in : in; sum += tmpsum; // sign*in*1 if (weightChunk & 0x40000000) sum += tmpsum<<3; // sign*in*8 if (weightChunk & 0x20000000) sum += tmpsum<<2; // sign*in*4 if (weightChunk & 0x10000000) sum += tmpsum<<1; // sign*in*2 weightChunk <<= 4; } } output[i] = sum;\n\nIn addition the fc layers also normalization and ReLU operators are required.\nI found that it was possible to replace a more complex RMS normalization with\nsimple shifts in the inference. Not a single full 32\u00d732 multiplication is\nneeded for the inference! Having this simple structure for inference means\nthat we have to focus the effort on the training part.\n\nI studied variations of the network with different numbers of bits and\ndifferent sizes by varying the numer of hidden activiations. To my surprise I\nfound that the accuracy of the prediction is proportional to the total number\nof bits used to store the weights. For example, when 2 bits are used for each\nweight, twice the numbers of weights is needed to achieve the same perforemnce\nas a 4 bit weight network.8 The plot below shows training loss vs. total\nnumber of bits. We can see that for 1-4 bits, we can basically trade more\nweights for less bits. This trade-off is less efficient for 8 bits and no\nquantization (fp32).\n\nI further optimized the training by using data augmentation, a cosine schedule\nand more epochs. It seems that 4 bit weights offered the best trade off.\n\nMore than 99% accuracy was achieved for 12 kbyte model size! While it is\npossible to achiever better accuracy with much larger models, it is\nsignificantly more accurate than other on-MCU implementations of MNIST.\n\n## Implementation on the Microcontroller\n\nThe model data is exported to a c-header file for inclusion into the inference\ncode. I used the excellent ch32v003fun environment, which allowed me to reduce\noverhead to be able to store 12kb of weights plus the inference engine in only\n16kb of flash.\n\nThere was still enough free flash to include 4 sample images. The inference\noutput is shown above. Execution time for one inference is 13.7 ms which would\nactually allow to model to process moving image input in real time!\n\nAlternatively, I also tested a smaller model with 4512 2-bit parameters and\nonly 1kb of flash memory footprintg. Despite its size, it still achieves a\n94.22% test accuracy and it executes in only 1.88ms.\n\n## Conclusions\n\nThis was quite a tedious projects, hunting many lost bits and rounding errors.\nI am quite pleased with the outcome as it shows that it is possible to\ncompress neural networks very significantly with dedicated effort. I learned a\nlot and am planning to use the data pipeline for more interesting\napplications.\n\n### Share this:\n\n  * Twitter\n  * Facebook\n\nLike Loading...\n\n### Related\n\nLight_WS2812 library V2.0 \u2013 Part II: The CodeJanuary 19, 2014In \"AVR\"\n\nUltra Low Power LED Flasher using the Padauk PFS154February 7, 2021In \"LED\"\n\nControlling RGB LEDs With Only the Powerlines: Anatomy of a Christmas Light\nStringJanuary 23, 2022In \"Hardware\"\n\nAuthor cpldcpuPosted on April 24, 2024April 25, 2024Categories\nUncategorizedTags ai, artificial-intelligence, CH32V003, data-science, deep-\nlearning, Inference, machine learning, ML, MNIST, Optimization\n\n## Leave a comment Cancel reply\n\n## Top Posts & Pages\n\n  * Implementing Neural Networks on the \"10-cent\" RISC-V MCU without Multiplier\n  * Light_WS2812 library V2.0 - Part I: Understanding the WS2812\n  * Ultra Low Power LED Flasher using the Padauk PFS154\n  * The \"terrible\" 3 cent MCU - a short survey of sub $0.10 microcontrollers.\n  * The SK6812 - another intelligent RGB LED\n\n## Categories\n\n## Pages\n\nTim's Blog Blog at WordPress.com.\n\n  * Comment\n  * Reblog\n  * Subscribe Subscribed\n\n    * Tim's Blog\n    * Already have a WordPress.com account? Log in now.\n\n  * Privacy\n  *     * Tim's Blog\n    * Customize\n    * Subscribe Subscribed\n    * Sign up\n    * Log in\n    * Copy shortlink\n    * Report this content\n    * View post in Reader\n    * Manage subscriptions\n    * Collapse this bar\n\nLoading Comments...\n\n%d\n\n", "frontpage": false}
