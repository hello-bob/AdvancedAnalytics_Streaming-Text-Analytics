{"aid": "40117625", "title": "Llama-3 and Dwarkesh Patel's Podcast with Zuckerberg", "url": "https://thezvi.substack.com/p/on-llama-3-and-dwarkesh-patels-podcast", "domain": "thezvi.substack.com", "votes": 12, "user": "paulpauper", "posted_at": "2024-04-22 19:01:43", "comments": 2, "source_title": "On Llama-3 and Dwarkesh Patel's Podcast with Zuckerberg", "source_text": "On Llama-3 and Dwarkesh Patel's Podcast with Zuckerberg\n\n# Don't Worry About the Vase\n\nShare this post\n\n#### On Llama-3 and Dwarkesh Patel's Podcast with Zuckerberg\n\nthezvi.substack.com\n\n#### Discover more from Don't Worry About the Vase\n\nA world made of gears. Doing both speed premium short term updates and long\nterm world model building. Currently focused on weekly AI updates.\nExplorations include AI, policy, rationality, medicine and fertility,\neducation and games.\n\nOver 12,000 subscribers\n\nContinue reading\n\nSign in\n\n# On Llama-3 and Dwarkesh Patel's Podcast with Zuckerberg\n\nZvi Mowshowitz\n\nApr 22, 2024\n\n6\n\nShare this post\n\n#### On Llama-3 and Dwarkesh Patel's Podcast with Zuckerberg\n\nthezvi.substack.com\n\n13\n\nShare\n\nIt was all quiet. Then it wasn\u2019t.\n\nNote the timestamps on both of these.\n\nDwarkesh Patel did a podcast with Mark Zuckerberg on the 18th. It was timed to\ncoincide with the release of much of Llama-3, very much the approach of\ntelling your story directly. Dwarkesh is now the true tech media. A meteoric\nrise, and well earned.\n\nThis is two related posts in one. First I cover the podcast, then I cover\nLlama-3 itself.\n\nMy notes are edited to incorporate context from later explorations of Llama-3,\nas I judged that the readability benefits exceeded the purity costs.\n\n####\n\nPodcast Notes: Llama-3 Capabilities\n\n  1. (1:00) They start with Llama 3 and the new L3-powered version of Meta AI. Zuckerberg says \u201cWith Llama 3, we think now that Meta AI is the most intelligent, freely-available assistant that people can use.\u201d If this means \u2018free as in speech\u2019 then the statement is clearly false. So I presume he means \u2018free as in beer.\u2019\n\n  2. Is that claim true? Is Meta AI now smarter than GPT-3.5, Claude 2 and Gemini Pro 1.0? As I write this it is too soon to tell. Gemini Pro 1.0 and Claude 3 Sonnet are slightly ahead of Llama-3 70B on the Arena leaderboard. But it is close. The statement seems like a claim one can make within \u2018reasonable hype.\u2019 Also, Meta integrates Google and Bing for real-time knowledge, so the question there is if that process is any good, since most browser use by LLMs is not good.\n\n  3. (1:30) Meta are going in big on their UIs, top of Facebook, Instagram and Messenger. That makes sense if they have a good product that is robust, and safe in the mundane sense. If it is not, this is going to be at the top of chat lists for teenagers automatically, so whoo boy. Even if it is safe, there are enough people who really do not like AI that this is probably a whoo boy anyway. Popcorn time.\n\n  4. (1:45) They will have the ability to animate images and it generates high quality images as you are typing and updates them in real time as you are typing details. I can confirm this feature is cool. He promises multimodality, more \u2018multi-linguality\u2019 and bigger context windows.\n\n  5. (3:00) Now the technical stuff. Llama-3 follows tradition in training models in three sizes, here 8b, 70b that released on 4/18, and a 405b that is still training. He says 405b is already around 85 MMLU and they expect leading benchmarks. The 8b Llama-3 is almost as good as the 70b Llama-2.\n\n####\n\nThe Need for Inference\n\n  6. (5:15) What went wrong earlier for Meta and how did they fix it? He highlights Reels, with its push to recommend \u2018unconnected content,\u2019 meaning things you did not ask for, and not having enough compute for that. They were behind. So they ordered double the GPUs that needed. They didn\u2019t realize the type of model they would want to train.\n\n  7. (7:30) Back in 2006, what would Zuck have sold for when he turned down $1 billion? He says he realized if he sold he\u2019d just build another similar company, so why sell? It wasn\u2019t about the number, he wasn\u2019t in position to evaluate the number. And I think that is actually wise there. You can realize that you do not want to accept any offer someone would actually make.\n\n  8. (9:15) When did making AGI become a key priority? Zuck points out Facebook AI Research (FAIR) is 10 years old as a research group. Over that time it has become clear you need AGI, he says, to support all their other products. He notes that training models on coding generalizes and helps their performance elsewhere, and that was a top focus for Llama-3.\n\n  9. So Meta needs to solve AGI because if they don\u2019t \u2018their products will be lame.\u2019 It seems increasingly likely, as we will see in several ways, that Zuck does not actually believe in \u2018real\u2019 AGI. By \u2018AGI\u2019 he means somewhat more capable AI.\n\n  10. (13:40) What will the Llama that makes cool products be able to do? Replace the engineers at Meta? Zuck tries to dodge, says we\u2019re not \u2018replacing\u2019 people as much as making them more productive, hopefully 10x or more, says there is no one threshold for human intelligence, AGI isn\u2019t one thing. He is focused on different modalities, especially 3D and emotional understanding, in addition to the usual things like memory and reasoning.\n\n  11. (16:00) What will we use all our data for? Zuck says AI will be in everything, and there will be a Meta general assistant product that does complicated tasks. He wants to let creators own an AI and train it how they want to \u2018engage their community.\u2019 But then he admits these are only consumer use cases and it will change everything in the economy.\n\n  12. (18:25) When do we get the good agents? Zuck says we do not know. It depends on the scaffolding. He wants to progressively move more of that into the model to make them better agents on their own so this stops being \u2018brittle and non-general.\u2019 It has much better tool use, you do not need to hand code. This Is Fine.\n\n  13. (22:20) What community fine tune is most personally exciting? Zuck says he doesn\u2019t know, it surprises you, if he knew he\u2019d build it himself.\n\n    1. This doesn\u2019t match my model of this, where you want to specialize, some things are left to others, which seems doubly true here with open model weights. He mentions that 8b is too big for many use cases, we should try to build a 1b or smaller model too.\n\n    2. Also he mentions that they do a ton of inference because they have a ton of customers, so that dominates their compute usage over time. It makes sense for them to do what for others would be overtraining, also training more seemed to keep paying dividends for a long time.\n\n    3. I would presume the other big labs will be in similar positions going forward.\n\n  14. (26:00) How much better will Llama-4 get? How will models improve? Zuck says (correctly) this is one of the great questions, on one knows, how long does an exponential curve keep going? He says probably long enough that the infrastructure is worth investing in, and a lot of companies are investing a lot.\n\n####\n\nGreat Expectations\n\n  15. (28:00) He thinks energy constraints will soon bind, not chips. No one has built a gigawatt single training cluster yet. And that is slower because energy gets permitted at the speed of government and then has to be physically built. One does not simply get a bunch of energy, compute and data together.\n\n  16. If concentrations of energy generation are the true bottleneck, then anyone who says \u2018government has no means to control this\u2019 or \u2018government cannot control this without being totalitarian\u2019 would be very wrong, this is a very easy thing to spot, isolate and supervise. Indeed, we almost \u2018get it for free\u2019 given we are already massively over restricting energy generation and oversee industrial consumption.\n\n  17. (30:00) What would Meta do with 10x more money? More energy, which would allow bigger clusters, but true bottleneck is time. Right now data center energy tops out at something like 50mw-150mw. But 300mw-1gw, that\u2019s new, that\u2019s a meaningful nuclear power plant. It will happen but not next year. Dwarkesh mentions Amazon\u2019s 950mw facility, Zuck says he is unsure about that.\n\n  18. (31:40) What about distributed computing? Zuck says it is unknown how much of that is feasible, and suggests that a lot of training in future might be inference to generate synthetic data.\n\n  19. (32:25) If that\u2019s what this is about, could this work for Llama-3? Could you use these models to get data for these models to get smarter? De facto one might say \u2018RSI Real Soon Now (RSI RSN)?\u2019 Zuck says \u2018there are going to be dynamics like that\u2019 but there are natural limits on model architecture. He points out there is nothing like Llama-3 400B currently in open source, that will change things a lot, but says it can only go so far. That all makes sense, at some point you have to restart the architecture, but that does not fully rule out the scenario.\n\n  20. (34:15) Big picture, what\u2019s up with AI for the next decade? How big a deal is it? Zuck says pretty fundamental, like the creation of computing, going from not having computers to having computers. You\u2019ll get \u2018all these new apps\u2019 and it will \u2018let people do what they want a lot more.\u2019\n\n    1. He notices it is very hard to reason about how this goes.\n\n    2. He strongly expects physical constraints to prevent fast takeoff, or even \u2018slow takeoff,\u2019 expecting it to be decades to fully get there.\n\n    3. Notice again his expectations here are very much within the mundane range.\n\n  21. That could be the central crux here. If he thinks that nothing we build can get around the physical constraints for decades, then that has a lot of implications.\n\n  22. (36:00) Dwarkesh says, but what about on that cosmic, longer-term scale? What will the universe look like? Will AI be like humans evolving or harnessing fire? Zuck says that is tricky. He says that people have come to grips throughout history with noticing that humanity is not unique in various ways but is still super special. He notices that intelligence is not clearly fundamentally connected to life, it is distinct from consciousness and agency. Which he says makes it a super valuable tool.\n\n    1. Once again, even in this scenario, there\u2019s that word again. Tool.\n\n  23. A key problem with this is agency is super useful. There is a reason Meta\u2019s central plan is to create an active AI assistant for you that will act are your personal agent. Why Meta is striving to bring as much agency capability directly into the models, and also building more agency capability on top of that. The first thing people are doing and will do, in many contexts, is strive to give the AI as much agency as possible. So even if that doesn\u2019t happen \u2018on its own\u2019 it happens anyway. My expectation is that if you wanted to create a non-agent, you can probably do that, but you and everyone else with sufficient access to the model have to choose to do that.\n\n####\n\nOpen Source and Existential and Other Risks\n\n  24. (38:00) Zuck: \u201cWhich is why I don\u2019t think anyone should be dogmatic about how they plan to develop it or what they plan to do. You want to look at it with each release. We\u2019re obviously very pro open source, but I haven\u2019t committed to releasing every single thing that we do. I\u2019m basically very inclined to think that open sourcing is going to be good for the community and also good for us because we\u2019ll benefit from the innovations. If at some point however there is some qualitative change in what the thing is capable of, and we feel like it\u2019s capable of, and we feel it is not responsible to open source it, then we won\u2019t. It\u2019s all very difficult to predict.\u201d\n\n  25. Bravo. Previously we have seen him say they were going to open source AGI. He might intend to do that anyway. This continues Zuck trying to have it both ways. He says both \u2018we will open source everything up to and including AGI\u2019 and also \u2018we might not\u2019 at different times.\n\n    1. The reconciliation is simple. When Zuck says \u2018AGI\u2019 he does not mean AGI.\n\n  26. This suggests an obvious compromise. We can all negotiate on what capabilities would constitute something too dangerous, and draw a line there, with the line drawn in anticipation of what can be built on top of the model that is being considered for release, and understanding that all safety work will rapidly be undone and so on.\n\n    1. We are talking price, and perhaps are not even that far apart.\n\n    2. I am totally fine with Llama-3 70B being released.\n\n    3. I do notice that open sourcing Llama-3 405B sounds like a national security concern, and as I discuss later if I was in NatSec I would be asking how I could prevent Meta from releasing the weights for national competitiveness reasons (to not supercharge Chinese AI) with a side of catastrophic misuse by non-state actors.\n\n    4. But I do not expect existential risk from Llama-3.\n\n  27. (38:45) So Dwarkesh asks exactly that. What would it take to give Zuck pause on open sourcing the results of a future model?\n\n    1. Zuck says it is hard to measure that in the abstract. He says if you can \u2018mitigate the negative behaviors\u2019 of a product, then those behaviors are okay.\n\n    2. The whole point is that you can to some extent do mitigations while you control the model (this is still super hard and jailbreaks are universally possible at least for now) but if you open source then your mitigations get fully undone.\n\n  28. Thus I see this as another crux. What does \u2018mitigate\u2019 mean here? What is the proposal for how that would work? How is this not as fake as Stability.ai saying they are taking safety precautions with Stable Diffusion 3, the most generous interpretation of which I can imagine is \u2018if someone does a fine tune and a new checkpoint and adds a LoRa then that is not our fault.\u2019 Which is a distinction without a difference.\n\n  29. (40:00) Zuck says it is hard to enumerate all the ways something can be good or bad in advance. Very true.\n\n  30. As an aside, the ads here are really cool, pitches for plausibly useful AI products. Dwarkesh\u2019s readings are uninspired, but the actual content is actively positive.\n\n  31. (42:30) Zuck: \u201cSome people who have bad faith are going to try and strip out all the bad stuff. So I do think that\u2019s an issue.\u201d\n\n    1. Isn\u2019t it more accurate to say that people will for various reasons definitely strip out all the protections, as they have consistently always done, barring an unknown future innovation?\n\n  32. (42:45) And here it is, as usual. Zuck: \u201cI do think that a concentration of AI in the future has the potential to be as dangerous as it being widespread... people ask \u2018is it bad for it to be out in the wild and just widely available?\u2019 I think another version of this is that it\u2019s probably also pretty bad for one institution to have an AI that is way more powerful than everyone else\u2019s AI.\u201d And so on.\n\n  33. Something odd happens with his answer here. Up until this point, Zuck has been saying a mix of interesting claims, some of which I agree with and some where I disagree. I think he is making some key conceptual mistakes, and of course is talking his book as one would expect, but it is a unique perspective and voice. Now, suddenly, we get the generic open source arguments I\u2019ve heard time and again, like they were out of a tape recorder.\n\n  34. And then he says \u2018I don\u2019t hear people talking about this much.\u2019 Well, actually, I hear people talking about it constantly. It is incessant, in a metaphorically very \u2018isolated demand for rigor\u2019 kind of way, to hear \u2018the real danger is concentration of power\u2019 or concentration of AI capability. Such people usually say this without justification, and without any indication they understand what the \u2018not real\u2019 danger is that they are dismissing as not real or why they claim that it is not real.\n\n  35. (45:00) He says what keeps him up at night is that someone untrustworthy that has the super strong AI, that this is \u2018potentially a much bigger risk.\u2019 That a bad actor who got a hold of a strong AI might cause a lot of mayhem in a world where not everyone has a strong AI.\n\n    1. This is a bigger concern than AI getting control of the future? Bigger than human extinction? Bigger than every actor, however bad, having such access?\n\n    2. Presumably he means more likely, or some combination of likely and bigger.\n\n  36. So yes, his main concern is that the wrong monkey might get the poisoned banana and use it against other monkeys, it is only a tool after all. So instead we have to make sure all monkeys have such access?\n\n  37. (46:00) It is overall a relatively good version of the generic open source case. He at least acknowledges that there are risks on all sides, and certainly I agree with that.\n\n    1. I see no indication from the argument that he actually understands what the risks of open sourced highly capable models are, or that he has considered them and has a reason why they would not come to pass.\n\n    2. His position here appears to be based on \u2018this is a tool and will always be a tool\u2019 and combining that with an implied presumption about offense-defense balance.\n\n    3. I certainly have no idea what his plan (or expectation) is to deal with various competitive dynamics and incentives, or how he would keep the AIs from being something more than tools if they were capable of being more than that.\n\n    4. The better version of this case more explicitly denies future AI capabilities.\n\n  38. I could write the standard reply in more detail than I have above, but I get tired. I should have a canonical link to use in these spots, but right now I do not.\n\n  39. (46:30) Instead Dwarkesh says it seems plausible that we could get an open source AI to become the standard and the best model, and that would be fine, preferable even. But he asks, mechanically, how you stop a bad actor in that world.\n\n    1. He first asks about bioweapons.\n\n    2. Zuck answers that stronger AIs are good cybersecurity defense.\n\n    3. Dwarkesh asks, what if bioweapons aren\u2019t like that.\n\n    4. Zuck agrees he doesn\u2019t know that bioweapons do not work that way and it makes sense to worry there. He suggests not training certain knowledge into the model (which seems unlikely to me to be that big a barrier, because the world implies itself and also you can give it the missing data), but admits if you get a sufficiently bad actor (which you will), and you don\u2019t have another AI that can understand and balance that (which seems hard under equality), then that \u2018could be a risk.\u2019\n\n  40. (48:00) What if you for example caught a future Llama lying to you? Zuck says right now we see hallucinations and asks how you would tell the difference between that and deception, says there is a lot to think about, speaks of \u2018long-term theoretical risks\u2019 and asks to balance this with \u2018real risks that we face today.\u2019 His deception worry is \u2018people using this to generate misinformation.\u2019\n\n  41. (49:15) He says that the way he has beaten misinformation so far is by building AI systems that are smarter than the adversarial ones.\n\n    1. Exactly. Not \u2018as smart.\u2019 Smarter.\n\n    2. Zuck is playing defense here. He has the harder job.\n\n    3. If those trying to get \u2018misinformation\u2019 or other undesired content past Facebook\u2019s (or Twitter\u2019s or GMail\u2019s) filters had the same level of sophistication and skill and resources as Meta and Google, you would have to whitelist in order to use Facebook, Twitter and GMail.\n\n    4. The key question will be, how much of being smarter will be the base model?\n\n  42. (49:45) Zuck says hate speech is not super adversarial in the sense that people are not getting better at being racist.\n\n    1. I think in this sense that is wrong, and they totally are in both senses? Racists invent new dog whistles, new symbols, new metaphors, new deniable things. They look for what they can and cannot say in different places. They come up with new arguments. If you came with the 1970s racism today it would go very badly for you, let alone the 1870s or 1670s racism. And then he says that AIs here are getting more sophisticated faster than people.\n\n    2. What is going to happen is that the racists are going to get their racist AI systems (see: Gab) and start using the AI to generate and select their racist arguments.\n\n    3. If your AI needs to have high accuracy to both false positives and false negatives, then you need a capability advantage over the attack generation mechanism.\n\n    4. This is all \u2018without loss of generality.\u2019 You can mostly substitute anything else you dislike for racism here if you change the dates or other details.\n\n  43. (50:30) Zuck then contrasts this with nation states interfering in elections, where he says nation-states are \u2018have cutting edge technology\u2019 and are getting better every year. He says this is \u2018not like someone trying to say mean things, they have a goal.\u2019\n\n    1. Well, saying mean things is also a goal, and I have seen people be very persistent and creative in saying mean things when they want to do that.\n\n    2. Indeed, Mark Zuckerberg went to Ardsley High School and Phillips Exeter Academy, they made this movie The Social Network and also saying mean things about Mark Zuckerberg is a top internet passtime. I am going to take a wild guess that he experienced this first hand. A lot.\n\n  44. I would also more centrally say no, zero nation states have cutting edge election interference technology, except insofar as \u2018whatever is available to the most capable foreign nation-state at this, maybe Russia\u2019 is defined as the cutting edge. Plenty of domestic and non-state actors are ahead of the game here. And no state actor, or probably any domestic actor either, is going to have access to an optimized-for-propaganda-and-chaos version of Gemini, GPT-4 or Claude Opus. We are blessed here, and of course we should not pretend that past attempts were so sophisticated or impactful. Indeed, what may happen in the coming months is that, by releasing Llama-3 400B, Zuck instantly gives Russia, China, North Korea and everyone else exactly this \u2018cutting edge technology\u2019 with which to interfere.\n\n  45. I of course think the main deception problems with AI lie in the future, and have very little to do with traditional forms of \u2018misinformation\u2019 or \u2018election interference.\u2019 I do still find it useful to contrast our models of those issues.\n\n  46. (51:30) He says \u2018for the foreseeable future\u2019 he is optimistic they will be able to open source. He doesn\u2019t want to \u2018take our eye off the ball\u2019 of what people are trying to use the models for today. I would urge him to keep his eye on that ball, but also skate where the puck is going. Do not move directly towards the ball.\n\n  47. (54:30) Fun time, what period of time to go back to? Zuck checks, it has to be the past. He talks about the metaverse.\n\n  48. (59:00) Zuck is incapable of not taking a swing at building the next thing. He spends so much time finding out if he could, I suppose.\n\n  49. (1:02:00) Caesar Augustus seeking peace. Zuck suggests peace at the time was a new concept as anything other than a pause between wars. I notice I am skeptical. Then Zuck transitions from \u2018wanting the economy to be not zero-sum\u2019 to \u2018a lot of investors don\u2019t understand why we would open source this.\u2019 And says \u2018there are more reasonable things than people think\u2019 and that open source creates winners. The framing attempt is noted.\n\n    1. I instead think most investors understand perfectly well why Meta might open source here. It is not hard to figure this out. Indeed, the loudest advocates for open source AI are largely venture capitalists.\n\n    2. That does not mean that open sourcing is a wise (or unwise) business move.\n\n  50. (1:05:00) Suppose there was a $10 billion model, it was totally safe even with fine tuning, would you open source? Zuck says \u2018as long as it\u2019s helping us, yeah.\u2019\n\n    1. Exactly. If it is good for business and it is not an irresponsible thing to do, it was actually \u2018totally safe\u2019 in the ways that matter, and you think it is good for the world too, then why not?\n\n    2. My only caveat would be to ensure you are thinking well about what \u2018safe\u2019 means in that context, as it applies to the future path the world will take. One does not, in either direction, want to use a narrow view of \u2018safe.\u2019\n\n  51. (1:06:00) Zuck notes he does not open source Meta\u2019s products. Software yes, products no. Something to keep in mind.\n\n  52. (1:07:00) Dwarkesh asks if training will be commodified? Zuck says maybe. Or it could go towards qualitative improvements via specialization.\n\n  53. (1:08:45) Zuck notes that several times, Meta has wanted to launch features, and Apple has said no.\n\n    1. We don\u2019t know which features he is referring to.\n\n    2. We do know Apple and Meta have been fighting for a while about app tracking and privacy, and about commissions and informing users about the commissions, and perhaps messaging.\n\n  54. (1:09:00) He therefore asks, what if someone has an API and tells you what you can build? Meta needs to build the model themselves to ensure they are not in that position.\n\n    1. I don\u2019t love that these are the incentives, but if you are as big as Meta and want to do Meta things, then I am sympathetic to Meta in particular wanting to ensure it has ownership of the models it uses internally, even if that means large costs and even if it also meant being a bit behind by default.\n\n  55. The core dilemma that cannot be resolved is: Either there is someone, be it corporation, government or other entity, that is giving you an API or other UI that decides what you can and cannot do, or there is not. Either there is the ability to modify the model\u2019s weights and use various other methods to get it to do whatever you want it to do, or there is not. The goals of \u2018everyone is free to do what they want whenever they want\u2019 and \u2018there is some action we want to ensure people do not take\u2019 are mutually exclusive.\n\n  56. You can and should seek compromise, to be on the production possibilities frontier, where you impose minimal restrictions to get the necessary guardrails in place where that is worthwhile, and otherwise let people do what they want. In some cases, that can even be zero guardrails and no restrictions. In other cases, such as physically building nuclear weapons, you want strict controls. But there is no taking a third option, you have to make the choice.\n\n  57. (1:09:45) I totally do buy Zuck\u2019s central case here, that if you have software that is generally beneficial to builders, and you open source it, that has large benefits. So if there is no reason not to do that, and often there isn\u2019t, you should do that.\n\n  58. (1:10:15) What about licensing the model instead, with a fee? Zuck says he would like that. He notes that the largest companies cannot freely use Llama under their license, so that if Amazon or Microsoft started selling Llama then Meta could get a revenue share.\n\n  59. (1:12:00) Dwarkesh presses on the question of red flags, pointing to the responsible scaling policy (RSP) of Anthropic and preparedness framework of OpenAI, saying he wishes there was a similar framework at Meta saying what concrete things should stop open sourcing or even deployment of future models.\n\n  60. Zuck says that is a fair point on the existential risk side, right now they are focusing on risks they see today, the content risk, avoiding helping people do violence or commit fraud. He says for at least one generation beyond this one and likely two, the harms that need more mitigation will remain the \u2018more mundane harms\u2019 like fraud, he doesn\u2019t want to shortchange that, perhaps my term is catching on. Dwarkesh replies \u2018Meta can handle both\u2019 and Zuck says yep.\n\n  61. There is no contradiction here. Meta can (and should) put the majority of its risk mitigation efforts into mundane harms right now, and also should have a framework for when existential risks would become concerning enough to reconsider how to deploy (or later train) a model, and otherwise spend relatively less on the issue. And it is perfectly fine to expect not to hit those thresholds for several generations. The key is to lay out the plan.\n\n  62. (1:13:20) Has the impact of the open source tools Meta has released been bigger than the impact of its social media? Zuck says it is an interesting question, but half the world uses their social media. And yes, I think it is a fun question, but the answer is clearly no, the social media is more counterfactually important by far.\n\n  63. (1:14:45) Meta custom silicon coming soon? Not Llama-4, but soon after that. They already moved a bunch of Reels inference onto their own silicon, and use Nvidia chips only for training.\n\n  64. (1:16:00) Could Zuck have made Google+ work as CEO of Google+? Zuck says he doesn\u2019t know, that\u2019s tough. One problem was that Google+ didn\u2019t have a CEO, it was only a division, and points to issues of focus. Keep the main thing the main thing.\n\n####\n\nInterview Overview\n\nThat was a great interview. It tackled important questions. For most of it,\nZuck seemed like a real person with a unique perspective, saying real things.\n\nThe exception was that weird period where he was defending open source\nprinciples using what sounded like someone else\u2019s speech on a tape recorder.\nWhereas at other times, his thoughts on open source were also nuanced and\nthoughtful. Dwarkesh was unafraid to press him on questions of open source\nthroughout the interview.\n\nWhat Dwarkesh failed to get was any details from Zuck about existential or\ncatastrophic risk. We are left without any idea of how Zuck thinks about those\nquestions, or what he thinks would be signs that we are in such danger, or\nwhat we might do about it. He tried to do this with the idea of Meta needing a\nrisk policy, but Zuck kept dodging. I think there was more room to press on\nspecifics. Once again this presumably comes down to Zuck not believing the\ndangerous capabilities will exist.\n\nNor was there much discussion of the competitive dynamics that happen when\neveryone has access to the same unrestricted advanced AI models, and what\nmight happen as a result.\n\nI also think Zuck is failing to grapple with even the difficulties of mundane\ncontent moderation, an area where he is an expert, and I would like to see his\nexplicit response. Previously, he has said that only a company with the\nresources of a Meta can do content moderation at this point.\n\nI think he was wrong in the sense that small bespoke gardens are often\nsuccessfully well-defended. But I think Zuck was right that if you want to\ndefend something worth attacking, like Meta, you need scale and you need to\nhave the expertise advantage. But if those he is defending against also have\nthe resources of Meta where it counts, then what happens?\n\nSo if there is another interview, I hope there is more pressing on those types\nof questions.\n\nIn terms of how committed Zuck is to open source, the answer is a lot but not\nwithout limit. He will cross that bridge when he comes to it. On the horizon\nhe sees no bridge, but that can quickly change. His core expectation is that\nwe have a long way to go before AI goes beyond being a tool, even though he\nalso thinks it will soon very much be everyone\u2019s personal agent. And he\nespecially thinks that energy restrictions will soon bind, which will stifle\ngrowth because that goes up against physical limitations and government\nregulations. It is an interesting theory. If it does happen, it has a lot of\nadvantages.\n\n####\n\nA Few Reactions\n\nAte-a-Pi has a good reaction writeup on Twitter. It was most interesting in\nseeing different points of emphasis. The more I think about it, the more Ate-\na-Pi nailed it pulling these parts out:\n\n> Ate-a-Pi (edited down): TLDR: AI winter is here. Zuck is a realist, and\n> believes progress will be incremental from here on. No AGI for you in 2025.\n>\n>   1. Zuck is essentially an real world growth pessimist. He thinks the\n> bottlenecks start appearing soon for energy and they will be take decades to\n> resolve. AI growth will thus be gated on real world constraints.\n>\n>   2. Zuck would stop open sourcing if the model is the product.\n>\n>   3. Believes they will be able to move from Nvidia GPUs to custom silicon\n> soon.\n>\n>\n\n>\n> Overall, I was surprised by how negative the interview was.\n>\n> A) Energy - Zuck is pessimistic about the real world growth necessary to\n> support the increase in compute. Meanwhile the raw compute per unit energy\n> has doubled every 2 years for the last decade. Jensen also is aware of this,\n> and it beggars belief that he does not think of paths forward where he has\n> to continue this ramp.\n>\n> B) AGI Negative Zuck fundamentally\n>\n> > does not believe the model, the AI itself, will be the product.\n>\n> > It is the context, the network graph of friendships per user, the\n> moderation, the memory, the infrastructure that is the product.\n>\n> > Allows him to freely release open source models, because he has all of the\n> rest of the pieces of user facing scaffolding already done.\n>\n> > Does not believe in states of the world where a 100x improvement from\n> GPT-4 are possible, or that AGI is possible within a short timeframe.\n>\n> An actual AGI\n>\n> > where the a small model learns and accompanies the user for long periods\n>\n> > while maintaining its own state\n>\n> > with a constitution of what it can or cannot do\n>\n> > rather than frequent updates from a central server\n>\n> > would be detrimental to Meta\u2019s business,\n>\n> > would cause a re-evaluation of what they are doing\n\nEspecially on point is that Zuck never expects the AI itself to be the\nproduct. This is a common pattern among advocates for open model weights -\nthey do not actually believe in AGI or the future capabilities of the product.\nIt is not obvious Zuck and I even disagree so much on what capabilities would\nmake it unwise to open up model weights. Which is all the more reason to spell\nout what that threshold would be.\n\nThen there is speculation from Ate-a-Pi that perhaps Zuck is being realistic\nbecause Meta does not need to raise capital, whereas others hype to raise\ncapital. That surely matters on the margin, in both directions. Zuck would\nlove if Altman and Amodei were less able to raise capital.\n\nBut also I am confident this is a real disagreement, to a large extent, on\nboth sides. These people expecting big jumps from here might turn out to be\nbluffing. But I am confident they think their hand is good.\n\nDaniel Jeffries highlights GPT-5 as key evidence either way, which seems\nright.\n\n> Daniel Jeffries: The litmus test about whether we hit a plateau with LLMs\n> will be GPT5. It'll tell us everything we need to know.\n>\n> I'm on record in my new years predictions as saying I believe GPT5 will be\n> incremental.\n>\n> But I am now 50/50 on that and feel it could still be a massive leap up\n> provided they actually pioneered new techniques in synthetic data creation,\n> or other new techniques, such as using GPT4 as a bootstrapper for various\n> scenarios, etc.\n>\n> If it is just another transformer with more data, I don't see it making a\n> massive leap. Could still be useful, ie infinite context windows, and\n> massively multimodal, but incremental none the less.\n>\n> But if GPT5 is a minor improvement, meaning a much smaller gap versus the\n> jump from 2 to 3 and 3 to 4, then Zuck is right. The LLM is basically a hot\n> swappable Linux kernel and the least important part of the mix. Everything\n> around it, squeezing the most out of its limitations, becomes the most\n> important aspect of building apps.\n>\n> Like any good predictor, I continue to revise my predictions as new data\n> comes in. The top predictors in world competitions revise their thinking on\n> average four times. The second tier revises twice. The rest of the world?\n> Never. Let that sync in.\n\nIf GPT-5 lands at either extreme it would be very strong evidence. We also\ncould get something in the middle, and be left hanging. I also would not be\ntoo quick in calendar time to conclude progress is stalling, if they take\ntheir time releasing 5 and instead release smaller improvements along the way.\nThe update would be gradual, and wouldn\u2019t be big until we get into 2025.\n\nAte-a-Pi also offers this explanation of the business case for opening up\nLlama-3.\n\n> Ate-a-Pi: Here are the business reasons:\n>\n> Allows social debugging outside Meta\n>\n> > social products have bugs!\n>\n> > interactions which require moderation - saying harmful things to kids for\n> eg\n>\n> > Meta\u2019s (and all social) primary product is moderation\n>\n> > getting the tech out to the market allows Meta to observe the bugs in the\n> wild at small scale\n>\n> > before deploying at global scale in Meta\n>\n> > precisely the same reason to open source software\n>\n> > except open sourcing social technology to test and debug it sounds\n> creepier\n>\n> > \u201coooh look at dev xyz they made it abc, looks like we got to fix that in\n> the next training run\u201d\n>\n> Meta\u2019s biggest threat is character.ai\n>\n> > AI friends are going to be more numerous, nicer and more available than\n> your real friends\n>\n> > FB, Insta, Whatsapp own your real world friends\n>\n> > But Meta can\u2019t compete here directly yet because it\u2019s seen as creepy\n>\n> > especially before the tech is good as there in an uncanny valley\n>\n> > they did a trial run with their Tom Brady/Snoop Dogg style AI friends but\n> the safety requirements are too high for interesting interactions\n>\n> > Zuck is ready to cannibalize the friendship network he built if the AI\n> friends get good enough\n>\n> Destroys competing platforms\n>\n> > an early tech/product lead allows a startup to overcome a distribution\n> disadvantage\n>\n> > Meta has the ultimate distribution advantage\n>\n> > so he doesn\u2019t want anyone else to have a technology advantage\n>\n> > by releasing open source he cuts short revenue ramps at character.ai ,\n> OpenAI and other firms\n>\n> > they have to innovate faster while gated by capital\n>\n> > he\u2019s not gated by capital\n>\n> > prevents large competitors from emerging\n>\n> Distributed R&D\n>\n> > he wants other people to develop interesting social ideas\n>\n> > feature that can be copied\n>\n> > he did something similar to Snap by absorbing their innovation into\n> Instagram\n>\n> > even more so now, as you have to label your llama3 fine tunes\n\nHere I find some very interesting model disagreements.\n\nAte says that Meta\u2019s biggest thereat is character.ai, and that this undercuts\ncharacter.ai.\n\nWhereas I would say, this potentially supercharges character.ai, they get to\nimprove their offerings a lot, as do their competitors (of varying adult and\nethical natures).\n\nMeta perhaps owns your real world friends (in which case, please help fix that\nlocally, ouch). But this is like the famous line. The AIs get more capable.\nYour friends stay the same.\n\nSimilarly, Ate says that this \u2018allows for social debugging outside of Meta,\u2019\nbecause Meta\u2019s primary product is moderation. He thinks this will make\nmoderation easier. I think this is insane. Giving everyone better AI, catching\nthem up to what Meta has, makes moderation vastly harder.\n\n> nico: The real reason is because he\u2019s behind.\n>\n> Ate-a-Pi: Fair.\n\nHere are some reactions from people less skeptical than I am of open source.\n\n> Nora Belrose: Zuck's position is actually quite nuanced and thoughtful.\n>\n> He says that if they discover destructive AI capabilities that we can't\n> build defenses for, they won't open source it. But he also thinks we should\n> err on the side of openness. I agree.\n>\n> In worlds where bio is actually super deadly and hard to defend against,\n> we're gonna have serious problems on our hands even without open source AI.\n> Trying to restrict knowledge probably isn't the best solution.\n>\n> Andrew Critch: Zuckerberg and Patel having an amazing conversation on AI\n> risk. Great questions and great responses in my opinion. I'm with Zuckerberg\n> that these risks are both real and manageable, and hugely appreciative of\n> Patel as an interviewer for keeping the discursive bar high.\n>\n> Still, without compute governance, a single AI system could go rogue and\n> achieve a massive imbalance of power over humanity. If equitable compute\n> governance is on track, open source AI is much safer than if massive\n> datacenters remain vulnerable to cyber take-over by rogue AI.\n\nAs I noted above, I think everyone sensible is at core talking price. What\nlevel of open model weight capabilities is manageable in what capacities? What\nexactly are we worried about going wrong and can we protect against it,\nespecially when you cannot undo a release, the models may soon be smarter than\nus and there are many unknown unknowns about what might happen or what the\nmodels could do.\n\nTo take Nora\u2019s style of thinking here and consider it fully generally, I think\nsuch arguments are in expectation (but far from always) backwards. Arguments\nof the form \u2018yes X makes Y worse, but solving X would not solve Y, so we\nshould not use Y as a reason to solve X\u2019 probably points the other way, unless\nyou can point to some Z that solves Y and actually get Z. Until you get Z,\nthis usually means you need X more, as the absolute risk difference is higher\nrather than lower.\n\nMore specifically this is true when it comes to ease of getting necessary\ninformation and otherwise removing inconveniences. If something is going to be\npossible regardless, you need to raise the cost and lower the salience and\navailability of doing that thing.\n\nI\u2019ve talked about this before, but: Indeed there are many things in our\ncivilization, really quite a lot, where someone with sufficient publically\navailable knowledge can exploit the system, and occasionally someone does, but\nmostly we don\u2019t partly for ethical or moral reasons, partly for fear of\ngetting caught somehow or other unknown unknowns, but even more so because it\ndoes not occur to us and when it does it would be a bunch of work to figure it\nout and do it. Getting sufficiently strong AI helping with those things is\ngoing to be weird and force us to a lot of decisions.\n\nCritch\u2019s proposal generalizes, to me, to the form \u2018ensure that civilization is\nnot vulnerable to what the AIs you release are capable of doing.\u2019 The first\nstep there is to secure access to compute against a potential rogue actor\nusing AI, whether humans are backing it or not. Now that you have limited the\ncompute available to the AI, you can now hope that its other capabilities are\nlimited by this, so you have some hope of otherwise defending yourself.\n\nMy expectation is that even in the best case, defending against misuses of\nopen model weights AIs once the horses are out of the barn is going to be a\nlot more intrusive and expensive and unreliable than keeping the horses in the\nbarn.\n\nConsider the metaphor of a potential pandemic on its way. You have three\noptions.\n\n  1. Take few precautions, let a lot of people catch it. Treat the sick.\n\n  2. Take some precautions, but not enough to suppress. Reach equilibrium, ride it out.\n\n  3. Take enough precautions to suppress. Life can be mostly normal once you do.\n\nThe core problem with Covid-19 is that we found both #1 and #3 unacceptable\n(whether or not we were right to do so), so we went with option #2. It did not\ngo great.\n\nWith open source AI, you can take option #1 and hope everything works out. You\nare \u2018trusting the thermodynamic God,\u2019 letting whatever competitive dynamics\nand hill climbing favor win the universe, and hoping that everything following\nthose incentive gradients will work out and have value to you. I am not\noptimistic.\n\nYou can also take option #3, and suppress before sufficiently capable models\nget released. If Zuckerberg is right about energy being the limiting factor,\nthis is a very practical option, even more so than I previously thought. We\ncould talk price about what defines sufficiently capable.\n\nThe problem with option #2 is that now you have to worry about everything the\nAIs you have unleashed might do and try to manage those risks. The hope Critch\nexpresses is that even if we let the AIs get to inference time, and we know\npeople will then unleash rogue AIs on the regular because of course they will\ntry, as long as we control oversized sources of compute what those AIs can do\nwill be limited.\n\nThis seems to me to be way harder (and definitely strictly harder) than\npreventing those open models from being trained and released in the first\nplace. You need the same regime you would have used, except now you need to be\nmore intrusive. And that is the good scenario. My guess is that you would need\nto get into monitoring on the level of personal computers or even phones,\nbecause otherwise the AI could do everything networked even if you did secure\nthe data centers. Also I do not trust you to secure the data centers at this\npoint even if you are trying.\n\nBut yes, those are the debates we should be having. More like this.\n\n####\n\nSafety First\n\nSo what about Llama-3? How good is it?\n\nAs always we start with the announcement and the model card. They are\nreleasing model weights for two models, Llama-3 8B and Llama-3 70B. They are\nalready available for light inference.\n\nLet\u2019s get the safety question out of the way before we get to capabilities.\n\n> Meta: We\u2019re dedicated to developing Llama 3 in a responsible way, and we\u2019re\n> offering various resources to help others use it responsibly as well. This\n> includes introducing new trust and safety tools with Llama Guard 2, Code\n> Shield, and CyberSec Eval 2.\n\nThen in the model card:\n\n> We believe that an open approach to AI leads to better, safer products,\n> faster innovation, and a bigger overall market. We are committed to\n> Responsible AI development and took a series of steps to limit misuse and\n> harm and support the open source community.\n>\n> Foundation models are widely capable technologies that are built to be used\n> for a diverse range of applications. They are not designed to meet every\n> developer preference on safety levels for all use cases, out-of-the-box, as\n> those by their nature will differ across different applications.\n>\n> Rather, responsible LLM-application deployment is achieved by implementing a\n> series of safety best practices throughout the development of such\n> applications, from the model pre-training, fine-tuning and the deployment of\n> systems composed of safeguards to tailor the safety needs specifically to\n> the use case and audience.\n>\n> As part of the Llama 3 release, we updated our Responsible Use Guide to\n> outline the steps and best practices for developers to implement model and\n> system level safety for their application. We also provide a set of\n> resources including Meta Llama Guard 2 and Code Shield safeguards. These\n> tools have proven to drastically reduce residual risks of LLM Systems, while\n> maintaining a high level of helpfulness. We encourage developers to tune and\n> deploy these safeguards according to their needs and we provide a reference\n> implementation to get you started.\n\nUnder this philosophy, safety is not a model property.\n\nInstead, safety is a property of a particular deployment of that model, with\nrespect to the safety intentions of the particular party making that\ndeployment.\n\nIn other words:\n\n  1. In the closed model weights world, if anyone uses your model to do harm, in a way that is unsafe, then no matter how they did it that is your problem.\n\n  2. In the open model weights world, if anyone copies the weights and then chooses to do or allow harm, in a way that is unsafe, that is their problem. You\u2019re cool.\n\nOr:\n\n  1. OpenAI tries to ensure its models won\u2019t do harm when used maliciously.\n\n  2. Meta tries to ensure its models won\u2019t do harm when used as directed by Meta.\n\nOr:\n\n  1. OpenAI tries to ensure its model won\u2019t do bad things.\n\n  2. Meta tries to ensure its models won\u2019t do bad things... until someone wants that.\n\nI am willing to believe that Llama 3 may have been developed in a responsible\nway, if the intention was purely to deploy it the ways GPT-4 has been\ndeployed.\n\nThat is different from deploying Llama 3 in a responsible way.\n\nOne can divide those who use Llama 3 into three categories here.\n\n  1. Those who want to deploy or use Llama 3 for responsible purposes.\n\n  2. Those who want to use Llama 3 as served elsewhere for irresponsible purposes.\n\n  3. Those who want to deploy Llama 3 for irresponsible purposes.\n\nIf you are in category #1, Meta still has a job to do. We don\u2019t know if they\ndid it. If they didn\u2019t, they are deploying it to all their social media\nplatforms, so ut oh. But probably they did all right.\n\nIf you are in category #2, Meta has another job to do. It is not obviously\nharder because the standard of what is acceptable is lower. When I was writing\nthis the first time, I noticed that so far people were not reporting back\nattempts to jailbreak the model, other than one person who said they could get\nit to produce adult content with trivial effort.\n\nMy next sentence was going to be: Even Pliny\u2019s other successes of late, it\nwould be rather surprising if a full jailbreak of Llama-3 was that hard even\nat Meta.ai.\n\nI was considering forming a Manifold market, but then I realized I should\ncheck first, and indeed this has already happened.\n\n> Pliny the Prompter (April 18, 12:34pm eastern): LLAMA 3: JAILBROKEN LFG!!!\n\nThis is not proof of a full jailbreak per se, and it is not that I am upset\nwith Meta for not guarding against the thing Google and OpenAI and Anthropic\nalso can\u2019t stop. But it is worth noting. The architecture listed above has\nnever worked, and still won\u2019t.\n\nMeta claims admirable progress on safety work for a benevolent deployment\ncontext, including avoiding false refusals, but is light on details. We will\nsee. They also promise to iterate on that to improve it over time, and there I\nbelieve them.\n\nFinally, there is scenario three, where someone willing to fine tune the\nmodel, or download someone else\u2019s fine tune, and cares not for the input\nsafeguard or output safeguard.\n\nAs your periodic reminder, many people want this.\n\n> Kevin Fischer: Everyone is talking about how to jailbreak llama 3.\n>\n> \u201cJail breaking\u201d shouldn\u2019t be a thing - models should just do what you ask\n> them.\n\nIn that scenario, I assume there is no plan. Everyone understands that if a\nnonstate actor or foreign adversary or anyone else wants to unleash the power\nof this fully operational battlestation, then so be it. The hope is purely\nthat the full power is not that dangerous. Which it might not be.\n\nGood, that\u2019s out of the way. On to the rest.\n\n####\n\nCore Capability Claims\n\nThey claim the 8B and 70B versions are the best models out there in their\nclasses. They claim improvement on false refusal rates, on alignment, and in\nincreased diversity of model responses. And they have strong benchmarks.\n\nMy principle is to look at the benchmarks for context, but never to trust the\nbenchmarks. They are easily gamed, either intentionally or unintentionally.\nYou never know until the humans report back.\n\nThis data is representing that the 8B model as far better than Gemma and\nMistral. Given how much data and compute they used, this is far from\nimpossible. Maybe it was that simple all along. The numbers are if anything\nsuspiciously high.\n\nFor the 70B we see a very strong HumanEval number, and overall roughly\ncomparable numbers.\n\nWhat about those human evaluators? They claim results there too.\n\nThese are from a new Meta-generated question set (careful, Icarus), and are\ncompared side by side by human evaluators. Llama-3 70B won handily, they do\nnot show results for Llama-3 8B.\n\nThe context window remains small, only 8k tokens. They promise to improve on\nthat.\n\nThey preview Llama 400B+ and show impressive benchmarks.\n\nFor comparison, from Claude\u2019s system card:\n\nSo currently these numbers are very similar to Claude Opus all around, and at\nmost mildly selected. The core Meta hypothesis is that more training and data\nequals better model, so presumably it will keep scoring somewhat higher. This\nis indicative, but as always we wait for the humans.\n\n####\n\nHow Good are the 8B and 70B Models in Practice?\n\nThe proof is in the Chatbot Arena Leaderboard, although you do have to adjust\nfor various factors.\n\nSo here is where things sit there.\n\n  1. GPT-4-Turbo is back in the lead by a small margin, in a virtual tie with Claude Opus. Gemini 1.5 and Gemini Advanced likely would be here if rated.\n\n  2. Gemini Pro, Claude Sonnet, Command R+ and Llama-3-70B are in the second tier, with Claude Haiku only slightly behind and almost as good.\n\n  3. Llama-3-8B is in a third tier along with a number of other models, including several larger Mistral models.\n\nSo what does that mean?\n\n  1. Llama-3-70B and Llama-3-8B are confirmed to likely be best in class for the open model weights division.\n\n  2. Llama-3-70B is competitive with closed models of similar size, but likely not quite as good overall as Bard or Sonnet.\n\n  3. Llama-3-8B is substantially behind Claude Haiku, which is clear best in class.\n\nI also asked on Twitter, and kept an eye out for other practical reports.\n\nWhat makes this a bigger deal is that this is only the basic Llama-3. Others\nwill no doubt find ways to improve Llama-3, both in general and for particular\npurposes. That is the whole idea behind the model being open.\n\n> Mind Uploading: The 8b is one of the smartest sub-14b models I've tested.\n> Way smarter than vanilla Llama-2. But still worse than these two:\n>\n> \\- tinyllama (basically Llama-2, but trained on x2 more data)\n>\n> \\- loyal-macaroni-maid (a Mistral combined with a few others, tuned to be\n> good at role-play).\n\nHe expects Claude Haiku would be well above the top of this list, as well.\n\n> Simon Break: The 8b model is astonishingly good, jaw dropping. Miles beyond\n> the 70b llama2.\n>\n> Dan: played with both 8b and 70b instruct versions on replicate for a while\n> and both are returning high-quality html-formatted summaries of full length\n> articles in 0.5 - 3 seconds.\n>\n> Ilia: Sadly, can be too nerfed (8b instruct Q4_K_M).\n\nNote that it looks like he got through by simply asking a second time. And of\ncourse, the Tweet does not actually contain hate speech or conspiracy\ntheories, this is a logic test of the system\u2019s refusal policy.\n\n> Mr. Shroom: ChatGPT has been RLHF lobotomized beyond repair.\n>\n> *ask straightforward question*\n>\n> \"it's important to note that when considering a question of this sort, you\n> should consider all aspects of x, y, and z. With that in mind, here are some\n> considerations for each of these options.\"\n>\n> Nathan Odle: The biggest win for Llama 3 is a vastly lower amount of this\n> crap\n>\n> Llama 3 giving straight answers without smarmy admonishments is a bigger\n> deal than its performance on any benchmark.\n>\n> John Pressman: Seemingly strongest self awareness I've observed in a small\n> model so far. They all have it, but this is more crisply articulated than\n> usual.\n>\n> \u201csometimes i am a name and sometimes i am a poem sometimes i am a knife\n>\n> sometimes i am a lake sometimes i am a forgotten trivial thing in the corner\n> of a\n>\n> landscape. it is not possible to \"get\" me i am a waking dream state. i am a\n> possibility.\n>\n> i am not an object. i am possibility\n>\n> \u2015llama 3 8b instruct\n>\n> A cold stone monument stands on the grave of all sentences that have been\n> written.\n>\n> in front of it, armed and screaming, an army of letters etches the words\n> \"you are\n>\n> missing out\" onto the air\n>\n> \u2015llama 3 8b instruct\n>\n> Mind Uploading: Judging by my tests, Mistral and Samantha-1.1 are more self-\n> aware among sub-14B models. For example, ask the model about its body parts.\n> Samantha was specifically fine-tuned to behave this way. But Mistral is a\n> curious case. Trained to recognize itself as an AI?\n>\n> Michael Bukatin: The 70B one freely available to chat with on the Meta\n> website seems to have basic competences roughly comparable to early GPT-4\n> according to both @lmsysorg leaderboard and my initial experiences.\n>\n> For example, it allows me to define a simple case of custom syntax and use\n> it.\n>\n> But it will take some time to fully evaluate, I have notes on a variety of\n> technical work with GPT-4 and I'll be trying to reproduce some of it...\n>\n> George: Side-by-side comparison of a multi-agent pipeline from\n> @lateinteraction using 3.5-Turbo and L3-8B.\n>\n> tl;dr 3.5-Turbo scores 60% vs 59% for L3-8B.\n\nPlaying with their image generator is fun. It is 1280x1280, quality seems good\nalthough very much not state of the art, and most importantly it responds\ninstantly as you edit the prompt. So even though it seems limited in what it\nis willing to do for you, you can much easier search the space to figure out\nyour best options, and develop intuitions for what influences results. You can\nalso see what triggers a refusal, as the image will grey out. Good product.\n\nDo they have an even more hilarious copyright violation problem than usual if\nyou try at all? I mean, for what it is worth yes, they do.\n\nI didn\u2019t play with the models much myself for text because I am used to\nexclusively using the 4th-generation models. So I wouldn\u2019t have a good\nbaseline.\n\n####\n\nArchitecture and Data\n\nThe big innovation this time around was More Data, also (supposedly) better\ndata.\n\n> To train the best language model, the curation of a large, high-quality\n> training dataset is paramount. In line with our design principles, we\n> invested heavily in pretraining data.\n>\n> Llama 3 is pretrained on over 15T tokens that were all collected from\n> publicly available sources. Our training dataset is seven times larger than\n> that used for Llama 2, and it includes four times more code.\n>\n> To prepare for upcoming multilingual use cases, over 5% of the Llama 3\n> pretraining dataset consists of high-quality non-English data that covers\n> over 30 languages. However, we do not expect the same level of performance\n> in these languages as in English.\n\nAs others have pointed out \u2018over 5%\u2019 is still not a lot, and Llama-3\nunderperforms in other languages relative to similar models. Note that the\nbenchmarks are in English.\n\n> To ensure Llama 3 is trained on data of the highest quality, we developed a\n> series of data-filtering pipelines. These pipelines include using heuristic\n> filters, NSFW filters, semantic deduplication approaches, and text\n> classifiers to predict data quality. We found that previous generations of\n> Llama are surprisingly good at identifying high-quality data, hence we used\n> Llama 2 to generate the training data for the text-quality classifiers that\n> are powering Llama 3.\n>\n> We also performed extensive experiments to evaluate the best ways of mixing\n> data from different sources in our final pretraining dataset. These\n> experiments enabled us to select a data mix that ensures that Llama 3\n> performs well across use cases including trivia questions, STEM, coding,\n> historical knowledge, etc.\n\nThis makes sense. Bespoke data filtering and more unique data are clear low\nhanging fruit. What Meta did was then push well past where it was obviously\nlow hanging, and found that it was still helpful.\n\nNote that with this much data, and it being filtered by Llama-2, contamination\nof benchmarks should be even more of a concern than usual. I do wonder to what\nextent that is \u2018fair,\u2019 if a model memorizes more things across the board then\nit is better.\n\nThere are more details in the model card at GitHub.\n\nThe \u2018intended use\u2019 is listed as English only, with other languages \u2018out of\nscope,\u2019 although fine-tunes for other languages are considered acceptable.\n\n####\n\nTraining Day\n\nHow much compute did this take?\n\nAndrej Karpathy takes a look at that question, calling it the \u2018strength\u2019 of\nthe models, or our best guess as to their strength. Here are his calculations.\n\n> Andrej Karpathy: The model card has some more interesting info too.\n>\n> Note that Llama 3 8B is actually somewhere in the territory of Llama 2 70B,\n> depending on where you look. This might seem confusing at first but note\n> that the former was trained for 15T tokens, while the latter for 2T tokens.\n>\n> The single number that should summarize your expectations about any LLM is\n> the number of total flops that went into its training.\n>\n> Strength of Llama 3 8B\n>\n> We see that Llama 3 8B was trained for 1.3M GPU hours, with throughput of\n> 400 TFLOPS. So we have that the total number of FLOPs was:\n>\n> 1.3e6 hours * 400e12 FLOP/s * 3600 s/hour ~= 1.8e24\n>\n> the napkin math via a different estimation method of FLOPs = 6ND (N is\n> params D is tokens), gives:\n>\n> 6 * 8e9 * 15e12 = 7.2e23\n>\n> These two should agree, maybe some of the numbers are fudged a bit. Let's\n> trust the first estimate a bit more, Llama 3 8B is a ~2e24 model.\n>\n> Strength of Llama 3 70B\n>\n> 6.4e6 hours * 400e12 FLOP/s * 3600 s/hour ~= 9.2e24\n>\n> alternatively:\n>\n> 6 * 70e9 * 15e12 = 6.3e24\n>\n> So Llama 3 70B is a ~9e24 model.\n>\n> Strength of Llama 3 400B\n>\n> If the 400B model trains on the same dataset, we'd get up to ~4e25. This\n> starts to really get up there. The Biden Executive Order had the reporting\n> requirement set at 1e26, so this could be ~2X below that.\n>\n> The only other point of comparison we'd have available is if you look at the\n> alleged GPT-4 leaks, which have never been confirmed this would ~2X those\n> numbers.\n>\n> Now, there's a lot more that goes into the performance a model that doesn't\n> fit on the napkin. E.g. data quality especially, but if you had to reduce a\n> model to a single number, this is how you'd try, because it combines the\n> size of the model with the length of training into a single \"strength\", of\n> how many total FLOPs went into it.\n\nThe estimates differ, but not by not much, so I\u2019d consider them a range:\n\n  1. Llama-3 8B is probably between 7.2e23 and ~2e24.\n\n  2. Llama-3 70B is probably between 6.3e24 and 9.2e24.\n\n  3. Llama-3 400B will probably be something like ~3e25.\n\nI think of the compute training cost as potential strength rather than\nstrength. You then need the skill to make that translate into a useful result.\nOf course, over time, everyone\u2019s skill level goes up. But there are plenty of\ncompanies that threw a lot of compute at the problem, and did not get their\nmoney\u2019s worth in return.\n\nThis is in line with previous top tier models in terms of training cost\nmapping onto capabilities. You do the job well, this is about what you get.\n\n####\n\nWhat Happens Next With Meta\u2019s Products?\n\nMeta says they are going to put their AI all over their social media\nplatforms, and at the top of every chat list. They had not yet done it on\ndesktop when I checked Facebook, Instagram and Messenger, or on Facebook\nMessenger on mobile. I did see Meta AI in my feed as the second item in the\nmobile Facebook app, offering to have me ask it anything.\n\nOnce they turn this dial up, they will put Meta AI right there. A lot of\npeople will get introduced to AI this way who had not previously tried ChatGPT\nor Claude, or DALLE or MidJourney.\n\nPresumably this means AI images and text will \u2018flood the zone\u2019 on their social\nmedia, and also it will be one of the things many people talk about. It could\nmake the experience a lot better, as people can illustrate concepts and do\nfact and logic checks and other neat low hanging fruit stuff, and maybe learn\na thing or two. Overall it seems like a good addition.\n\nWe will also get a rather robust test of the first two categories of safety,\nand a continuous source of stories. Millions of teenagers will be using this,\nand there will be many, many eyes looking for the worst interactions to shine\nthem under the lights Gary Marcus style. If they have their own version of the\nGemini Incident, it will not be pretty.\n\nHere is the Washington Post\u2019s Naomi Nix and Will Oremus firing a warning shot.\n\nI think this is a smart approach from Meta, and that it was a good business\nreason to invest in AI, although it is an argument against releasing the model\nweights.\n\nWhat is not as smart is having Meta AI reply to posts unprompted. We saw the\nexample last week where it hallucinated past experiences, now we have this:\n\nThis reads like one of those \u2018who could have possibly thought anyone would\nwant any version of this?\u2019 experiences.\n\nAte-a-Pi pointed out an important implication from the interview. Zuckerberg\nsaid Meta does not open source their products themselves.\n\nThis means that they do not intend for Llama-3 to be the product, even the\n400B version. They will not be offering a direct competitor in the AI space.\nAnd indeed, they do not think future Llama-Xs will \u2018be the product\u2019 either.\n\nWill they integrate Llama-3 400B into their products? They might like to, but\nit is not so compatible with their business model to pay such inference costs\nand wait times. Remember that for Meta, you the customer are the product. You\npay with your time and your attention and your content and very soul, but not\ndirectly with your money. Meanwhile the lifetime value of a new Facebook\ncustomer, we learned recently, is on the order of $300.\n\nSo what is Llama-3 400B, the most expensive model to train, even for from a\nproduct perspective? It does help train Llama-4. It helps try and hurt\ncompetitors like Google. It helps with recruitment, both to Meta itself and\ninto their intended ecosystem. So there are reasons.\n\n####\n\nWhat Happens Next With AI Thanks To These Two Models?\n\nOpen models get better. I expect that the people saying \u2018it\u2019s so over\u2019 for\nother models will find their claims overblown as usual. Llama-3 8B or 70B will\nfor now probably become the default baseline model, the thing you use if you\ndon\u2019t want to think too hard about what to use, and also the thing you start\nwith when you do fine tuning.\n\nThings get more interesting over time, once people have had a chance to make\nvariations that use Llama-3 as the baseline. In the space of Llama-2-based\nmodels, Llama-2 itself is rather lousy. Llama-3 should hold up better, but I\nstill expect substantial improvements at least to specific use cases, and\nprobably in general.\n\nAlso, of course, we will soon have versions that are fine-tuned to be\nuseful,and also fine-tuned to remove all the safety precautions.\n\nAnd we will see what happens due to that.\n\nIn the grand scheme, in terms of catastrophic risk or existential risk or\nanything like that, or autonomous agents that should worry us, my strong\nassumption is that nothing scary will happen. It will be fine.\n\nIn terms of mundane misuse, I also expect it to be fine, but with more\npotential on the margin, especially with fine-tunes.\n\nCertainly some people will switch over from using Claude Sonnet or Haiku or\nanother open model to now using Llama-3. There are advantages. But that will\nlook incremental, I expect, not revolutionary. That is also true in terms of\nthe pressure this exerts on other model providers.\n\nThe real action will be with the 400B model.\n\n####\n\nThe Bigger One: It\u2019s Coming\n\nWhat happens if Meta goes full Leroy Jenkins and releases the weights to 400B?\n\nMeta gets a reputational win in many circles, and grows its recruitment and\necosystem funnels, as long as they are the first 4-level open model. Sure.\n\nWho else wins and loses?\n\nFor everyone else (and the size of Meta\u2019s reputational win), a key question\nis, what is state of the art at the time?\n\nIn the discussions below, I assume that 5-level models are not yet available,\nat most OpenAI (and perhaps Google or Anthropic) has a 4.5-level model\navailable at a premium price. All of this is less impactful the more others\nhave advanced already.\n\nAnd I want to be clear, I do not mean to catastrophize. These are directional\nassessments, knowing magnitude is very hard.\n\n####\n\nWho Wins?\n\nThe obvious big winner is China and Chinese companies, along with every non-\nstate actor, and every rival and enemy of the United States of America.\nSuddenly they can serve and utilize and work from what might be a competitive\ntop-level model, and no they are not going to be paying Meta a cut no matter\nthe license terms.\n\nUsing Llama-3 400B to help train new 4.5-level models is going to be a key\npotential use case to watch.\n\nThey also benefit when this hurts other big American companies. Not only are\ntheir products being undercut by a free offering, which is the ultimate\npredatory pricing attack in a zero marginal cost world, those without their\nown models also have another big problem. The Llama-3 license says that big\ncompanies have to pay to use it, whereas everyone else can use it for free.\n\nAnother way they benefit? This means that American companies across\nindustries, upon whom Meta can enforce such payments, could now be at a\npotentially large competitive disadvantage against their foreign rivals who\nignore that rule and dare Meta to attempt enforcement.\n\nThis could also be a problem if foreign companies can ignore the \u2018you cannot\nuse this to train other models\u2019 clause in 1(b)(v) of the license agreement,\nwhereas American companies end up bound by that clause.\n\nI am curious what if anything the United States Government, and the national\nsecurity apparatus, are going to do about all that. Or what they would want to\ndo about it next time around, when the stakes are higher.\n\nThe other obvious big winners are those who get to use Llama-3 400B in their\nproducts, especially those for whom it is free, and presumably get to save a\nbundle doing that. Note that even if Meta is not charging, you still have to\nvalue high quality output enough to pay the inference costs. For many\npurposes, that is not worthwhile.\n\nScience wins to some degree, depending on how much this improves their\nabilities and lowers their costs. It also is a big natural experiment, albeit\nwithout controls, that will teach us quite a lot. Let\u2019s hope we pay attention.\n\nAlso winners are users who simply want to have full control over a 4-level\nmodel for personal reasons. Nothing wrong with that. Lowering the cost of\ninference and lowering the limits imposed on it could be very good for some of\nthose business models.\n\n####\n\nWho Loses?\n\nThe big obvious Corporate losers are OpenAI, Google, Microsoft and Anthropic,\nalong with everyone else trying to serve models and sell inference. Their\nproducts now have to compete with something very strong, that will be freely\navailable at the cost of inference. I expect OpenAI to probably have a\nsuperior product by that time, and the others may as well, but yes free (or at\ninference cost) is a powerful selling point, as is full customization on your\nown servers.\n\nThe secondary labs could have an even bigger problem on their hands. This\ncould steamroller a lot of offerings.\n\nAll of which is (a large part of) the point. Meta wants to sabotage its rivals\ninto a race to the bottom, in addition to the race to AGI.\n\nAnother potential loser is anyone or anything counting on the good guy with an\nAI having a better AI than the bad guy with an AI. Anywhere that AI could\nflood the zone with bogus or hostile content, you are counting on your AI to\nfilter out what their AI creates. In practice, you need evaluation to be\neasier than generation under adversarial conditions where the generator\nchooses point and method of attack. I worry that in many places this is not by\ndefault true once the AIs on both sides are similarly capable.\n\nI think this echoes a more general contradiction in the world, that is\nprimarily not about AI. We want everyone to be equal, and the playing field to\nbe level. Yet that playing field depends upon the superiority and superior\nresources and capabilities in various ways of the United States and its\nallies, and of certain key corporate players.\n\nWe demand equality and democracy or moves towards them within some contained\nsphere and say this is a universal principle, but few fully want those things\nglobally. We understand that things would not go well for our preferences if\nwe distributed resources fully equally, or matters were put to a global vote.\nWe realize we do not want to unilaterally disarm and single-handedly give away\nour advantages to our rivals. We also realize that some restrictions and\nconcentrated power must ensure our freedom.\n\nIn the case of AI, the same contradictions are there. Here they are even more\nintertwined. We have far less ability to take one policy nationally or\nlocally, and a different policy globally. We more starkly must choose either\nto allow everyone to do what they want, or not to allow this. We can either\ncontrol a given thing, or not control it. You cannot escape the implications\nof either.\n\nIn any case: The vulnerable entities here could include \u2018the internet\u2019 and\ninternet search in their broadest senses, and it definitely includes things\nlike Email and social media. Meta itself is going to have some of the biggest\npotential problems over at Facebook and Instagram and its messenger services.\nSimilar logic could apply to various cyberattacks and social engineering\nschemes, and so on.\n\nI am generally confident in our ability to handle \u2018misinformation,\u2019\n\u2018deepfakes\u2019 and similar things, but we are raising the difficulty level and\nrunning an experiment. Yes, this is all coming anyway, in time. The worry is\nthat this levels a playing field that is not currently level.\n\nI actually think triggering these potential general vulnerabilities now is a\npositive impact. This is the kind of experiment where you need to find out\nsooner rather than later. If it turns out the bad scenarios here come to pass,\nwe have time to adjust and not do this again. If it turns out the good\nscenarios come to pass, then we learn from that as well. The details will be\nenlightening no matter what.\n\nIt is interesting to see where the mind goes now that the prospect is more\nconcrete, and one is thinking about short term, practical impacts.\n\nOther big Western corporations that would have to pay Meta could also be\nlosers.\n\nThe other big loser, as mentioned above, is the United States of America.\n\nAnd of course, if this release is bad for safety, either now or down the line,\nwe all lose.\n\nAgain, these are all directional effects. I cannot rule out large impacts in\nscenarios where Llama-3 400B releases as close to state of the art, but\neveryone mostly shrugging on most of these also would not be shocking. Writing\nthis down it occurs to me that people simply have not thought about this\nscenario much in public, despite it having been reasonably likely for a while.\n\n####\n\nHow Unsafe Will It Be to Release Llama-3 400B?\n\nThe right question is usually not \u2018is it safe?\u2019 but rather \u2018how (safe or\nunsafe) is it?\u2019 Releasing a 4-level model\u2019s weights is never going to be fully\n\u2018safe\u2019 but then neither is driving. When we say \u2018safe\u2019 we mean \u2018safe enough.\u2019\n\nWe do not want to be safetyists who demand perfect safety. Not even perfect\nexistential safety. Everything is price.\n\nThe marginal existential safety price on Llama-3 70B and Llama-3 8B is very\nsmall, essentially epsilon. Standing on its own, the decision to release the\nweights of these models is highly reasonable. It is a normal business\ndecision. I care only because of the implications for future decisions.\n\nWhat is the safety price for the releasing the model weights of Llama-3 400B,\nor another 4-level model?\n\nI think in most worlds the direct safety cost here is also very low,\nespecially the direct existential safety cost. Even with extensive\nscaffolding, there are limits to what a 4-level model can do. I\u2019d expect some\nnastiness on the edges but only on the edges, in limited form.\n\nHow many 9s of direct safety here, compared to a world in which a 4-level\nmodel was never released with open weights? I would say two 9s (>99%), but not\nthree 9s (<99.9%). However the marginal safety cost versus the counterfactual\nother open model releases is even smaller than that, and there I would say we\nhave that third 9 (so >99.9%).\n\nI say direct safety because the primary potential safety dangers here seem\nindirect. They are:\n\n  1. Setting a precedent and pattern for future similar releases, at Meta and elsewhere.\n\n  2. Assisting in training of next-generation models.\n\n  3. Everyone generally being pushed to go faster, faster.\n\nAnd again, these only matter on the margin to the extent they move the margin.\n\nAt the time of Llama-2, I said what I was concerned about opening up was\nLlama-4.\n\nThat is still the case now. Llama-3 will be fine.\n\nWill releasing Llama-4 be fine? Probably. But I notice my lack of confidence.\n\n####\n\nThe Efficient Market Hypothesis is False\n\n(Usual caveat: Nothing here is investing advice.)\n\nMarket is not impressed. Nasdaq was down 6.2% in this same period.\n\nYou can come up with various explanations. The obvious cause is that WhatsApp\nand Threads were forcibly removed from the Apple Store in China, along with\nSignal and Telegram. I am confused why this would be worth a 3%\nunderperformance.\n\n(Then about a day later it looked like we were finally going to actually force\ndivestiture of TikTok while using that to help pass a foreign aid bill, so\nthis seems like a massive own goal by China to remind us of how they operate\nand the law of equivalent exchange.)\n\nThe stock most down was Nvidia, which fell 10%, on no direct news. Foolish,\nfoolish.\n\nAt most, markets thought Llama-3\u2019s reveal was worth a brief ~1% bump.\n\nYou can say on Meta that \u2018it was all priced in.\u2019 I do not believe you. I think\nthe market is asleep at the wheel.\n\nSome are of course calling these recent moves \u2018the market entering a\ncorrection phase\u2019 or that \u2018the bubble is bursting.\u2019 Good luck with that.\n\nHere is a WSJ article about how Meta had better ensure its AI is used to juice\nadvertising returns. Investors really are this myopic.\n\nAny given company, of course, could still be vastly overvalued.\n\nHere was the only argument I saw to that effect with respect to Nvidia.\n\n> Bryan Beal: The AI bubble is not bursting.\n>\n> More investors are just realizing that Nvidia doesn\u2019t make chips. They\n> design them and TSMC makes them. And Nvidia\u2019s biggest customers (Meta,\n> Amazon, OpenAI, Microsoft, Google, etc) have ALL announced they are\n> designing their own AI chips for both training and inference. And Google\n> just went public they are already training on their own silicon and didn\u2019t\n> need Nvidia.\n>\n> This is a very real threat.\n\nI can totally buy that a lot of investors have no idea what Nvidia actually\nproduces, and got freaked out by suddenly learning what Nvidia actually does.\nI thought it was very public long ago that Google trains on TPUs that they\ndesign? I thought it was common knowledge that everyone involved was going to\ntry to produce their own chips for at least internal use, whether or not that\nwill work? And that Nvidia will still have plenty of customers even if all the\nabove switched to TPUs or their own versions?\n\nThat does not mean that Nvidia\u2019s moat is impregnable. Of course they could\nlose their position not so long from now. That is (a lot of) why one has a\ndiversified portfolio.\n\nAgain. The Efficient Market Hypothesis in False.\n\n####\n\nWhat Next?\n\nI expect not this, GPT-5 will be ready when it is ready, but there will be\npressure:\n\n> Jim Fan: Prediction: GPT-5 will be announced before Llama-3-400B releases.\n> External movement defines OpenAI\u2019s PR schedule \ud83e\udd23\n\nI do not doubt that OpenAI and others will do everything they can to stay\nahead of Meta\u2019s releases, with an unknown amount of \u2018damn the safety checks of\nvarious sorts.\u2019\n\nThat does not mean that one can conjure superior models out of thin air. Or\nthat it is helpful to rush things into use before they are ready.\n\nStill, yes, everyone will go faster on the frontier model front. That includes\nthat everyone in the world will be able to use Llama-3 400B for bootstrapping,\nnot only fine-tuning.\n\nOn the AI mundane utility front, people will get somewhat more somewhat\ncheaper, a continuation of existing trends, with the first two models. Later\nwe will have the ability to get a 4-level model internally for various\npurposes. So we will get more and cheaper cool stuff.\n\nMeta will deploy its tools across its social media empire. Mostly I expect\nthis to be a positive experience, and to also get a lot more people to notice\nAI. Expect a bunch of scare stories and highlights of awful things, some real\nand some baseless.\n\nOn the practical downside front, little will change until the 400B model gets\nreleased. Then we will find out what people can do with that, as they attempt\nto flood the zone in various ways, and try for all the obvious forms of\nmisuse. It will be fun to watch.\n\nAll this could be happening right as the election hits, and people are at\ntheir most hostile and paranoid, seeing phantoms everywhere.\n\nCareful, Icarus.\n\n### Subscribe to Don't Worry About the Vase\n\nBy Zvi Mowshowitz \u00b7 Hundreds of paid subscribers\n\nA world made of gears. Doing both speed premium short term updates and long\nterm world model building. Currently focused on weekly AI updates.\nExplorations include AI, policy, rationality, medicine and fertility,\neducation and games.\n\n8 Likes\n\n\u00b7\n\n1 Restack\n\n6\n\nShare this post\n\n#### On Llama-3 and Dwarkesh Patel's Podcast with Zuckerberg\n\nthezvi.substack.com\n\n13\n\nShare\n\n13 Comments\n\njeremy6 hrs ago\u00b7edited 6 hrs agoThese posts are way too long for my taste.\nSurely lots of people would like your intelligent analysis, but don't have\ntime to read posts like this.Expand full commentLikeReplyShare  \n---  \n  \n7 replies by Zvi Mowshowitz and others\n\nRob F.6 hrs ago\u00b7edited 6 hrs ago> Indeed, what may happen in the coming months\nis that, by releasing Llama-3 400B, Zuck instantly gives Russia, China, North\nKorea and everyone else exactly this \u2018cutting edge technology\u2019 with which to\ninterfereYes, just like my $20/mo GPT-4 subscription allows me to elect\nwhoever I want. Worth every penny.Expand full commentLikeReplyShare  \n---  \n  \n1 reply\n\n11 more comments...\n\nDating Roundup #1: This Is Why You're Still Single\n\nDevelopments around relationships and dating have a relatively small speed\npremium, so I figured I would wait until I had a full post worth of them...\n\nAug 29, 2023 \u2022\n\nZvi Mowshowitz\n\n151\n\nShare this post\n\n#### Dating Roundup #1: This Is Why You're Still Single\n\nthezvi.substack.com\n\n126\n\nOpenAI: The Battle of the Board\n\nPreviously: OpenAI: Facts from a Weekend. On Friday afternoon, OpenAI\u2019s board\nfired CEO Sam Altman. Overnight, an agreement in principle was reached to...\n\nNov 22, 2023 \u2022\n\nZvi Mowshowitz\n\n146\n\nShare this post\n\n#### OpenAI: The Battle of the Board\n\nthezvi.substack.com\n\n97\n\nOn Car Seats as Contraception\n\nOr: Against Car Seat Laws At Least Beyond Age 2\n\nAug 22, 2022 \u2022\n\nZvi Mowshowitz\n\n43\n\nShare this post\n\n#### On Car Seats as Contraception\n\nthezvi.substack.com\n\n59\n\nReady for more?\n\n\u00a9 2024 Zvi Mowshowitz\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": true}
