{"aid": "40069793", "title": "Kafka API is the HTTP protocol for data streaming", "url": "https://tributarydata.substack.com/p/a-primer-on-kafka-api", "domain": "tributarydata.substack.com", "votes": 1, "user": "jovezhong", "posted_at": "2024-04-17 20:35:13", "comments": 0, "source_title": "A Gentle Introduction to Kafka API", "source_text": "A Primer on Kafka API - by Dunith Danushka - Tributary Data\n\n# Tributary Data\n\nShare this post\n\n#### A Primer on Kafka API\n\ntributarydata.substack.com\n\n#### Discover more from Tributary Data\n\nYour thoughtful guide to the realms of data engineering, real-time analytics,\nstreaming data, and AI.\n\nContinue reading\n\nSign in\n\n# A Primer on Kafka API\n\n### A vendor-neutral and language-agnostic beginners guide to fundamentals of\nKafka API and its information hierarchy.\n\nDunith Danushka\n\nApr 16, 2024\n\nShare this post\n\n#### A Primer on Kafka API\n\ntributarydata.substack.com\n\nShare\n\nKafka is a project that initially began within LinkedIn to build a robust,\nscalable message bus. It played a pivotal role in LinkedIn\u2019s data\ninfrastructure and was widely adopted due to its unique features and\ncapabilities. Recognizing its potential, LinkedIn donated Kafka to the Apache\nSoftware Foundation in 2016, where it evolved into Apache Kafka, retaining its\noriginal functionality while gaining additional features and improvements.\n\nAs Kafka gained popularity and its codebase matured, several vendors forked it\nto provide their own Kafka distributions. Companies like Redpanda and\nWarpstream even went as far as to rewrite the entire codebase in different\nprogramming languages other than Java.\n\nThanks for reading Tributary Data! Subscribe for free to receive new posts and\nsupport my work.\n\nHowever, amidst all these changes, these companies strived to keep one crucial\nthing unchanged \u2014 the Kafka API. This standard interface plays a key role in\nensuring compatibility and interoperability across different Kafka\ndistributions.\n\n#\n\nApache Kafka vs the Kafka API\n\nIt\u2019s important to understand the distinction between Apache Kafka and the\nKafka API, as they are two separate entities.\n\nApache Kafka is a distributed append-only commit log. It receives messages\nfrom producers and stores them in a fault-tolerant way, allowing consumers to\nfetch these messages in the order they were received. Due to its design, Kafka\nhas been established as a reliable and highly scalable publish-subscribe\nmessaging system for handling real-time data feeds.\n\nYou will work with Kafka through Kafka API. As a developer, you will utilize\nthe Kafka API to write client applications that can read and write data to\nKafka. As an operator, you will utilize the Kafka API to perform\nadministrative operations on Kafka. Regardless of whether your Kafka\ndistribution is open-source or commercial, you will still need to work with\nthe same Kafka API. Therefore, understanding Kafka API is crucial.\n\nWhile the Broker can be any Kafka distribution, the Kafka API always remains\nthe same.\n\nIn this post, we will cover the high-level concepts you would encounter when\nworking with any Kafka distribution. This content is independent of specific\nprogramming languages and vendors. We will discuss topics conceptually without\ndelving into the details of the Kafka protocol or API methods.\n\n#\n\nBrokers and clusters\n\nKafka is a distributed system where a Broker represents a single node within\nthis system.\n\nA broker can take on various form factors, from physical machines to virtual\nmachines and containers. When multiple brokers are configured together, they\nform a Kafka Cluster. Having multiple brokers ensures load balancing and fault\ntolerance.\n\nEach broker in the cluster runs the Kafka daemon as well as manages the\nstorage.\n\nHow would you configure a client application to initially connect to one\nbroker when there are several brokers in a cluster? You\u2019ll need to use the\nBootstrap Server address. This is the first broker your application interacts\nwith to acquire cluster metadata, a topic that we\u2019ll discuss in detail later.\n\nThe number of brokers in a cluster typically takes an odd number to help\nachieve a quorum.\n\n#\n\nMessages\n\nIn the Kafka world, message, record, and event all refer to the same concept \u2014\na unit of data transferred between your application and the broker.\n\nA message consists of a key and a value. The value contains the actual data or\nthe payload you want to send or receive. The key can take any value, including\nnull. Typically, an attribute related to the value is nominated as the key.\nFor example, if the value is an order object, the key can be the customer ID.\nThe purpose of having a key is to route the message to specific partitions\nwithin a topic. We will learn about that when we get to the partitions.\n\nIn Kafka world, message, record, and event refers to the same.\n\nBoth the key and the value are represented as a byte array of a variable\nlength. That allows Kafka to handle a diverse range of data types, from plain\ntext to serialized objects.\n\n#\n\nTopics\n\nA topic is a logical grouping of messages. It\u2019s analogous to a table from the\nrelational database world, which keeps related records together. You can have\ndifferent topics for different purposes.\n\nTopic sits at the highest level of Kafka\u2019s information hierarchy. As a\ndeveloper, you will write client applications that produce data to and consume\ndata from, various topics. Topic borrows the pub-sub semantics from message\nbrokers. The topic supports concurrent data writing and reading by multiple\nproducers and consumers. It also supports broadcast/fanout style messaging\nwhere a message produced on a topic can be consumed by multiple consumers.\n\nTopics only permit append operations, not random mutations. Meaning, that once\na message is written to a topic, you can\u2019t go back and update it. In addition,\nreading from a topic is a non-destructive operation. The same consumer can\ncome back later and re-read the message as needed. We will learn about this\nwhen we talk about consumer offsets.\n\nA topic spreads across the cluster\n\n#\n\nPartitions and offsets\n\nTopics are not continuous; rather, they are composed of partitions.\n\nA topic is a logical concept, while a partition is a more tangible entity.\n\nA topic partition is an append-only ordered log file that stores a subset of\ndata belonging to one topic. A topic can have more than one partition and\nthese partitions are scattered across different brokers of a cluster to\nprovide load balancing and fault tolerance.\n\n##\n\nWhy partitions?\n\nWhat would happen if there\u2019s no partition concept and Kafka keeps the topic\u2019s\ndata as a monolithic block? First, the topic will grow in size as more data\ncomes and soon it will exceed the storage limits of a single machine. You can\nalways attach more storage and make the machine taller. However, there will be\na limit to that at some point.\n\nSecondly, all consumers must consume from the broker holding that giant topic.\nThat will increase the load on that broker because there\u2019s no way for consumer\nload balancing. Moreover, backing up such a vast topic is time-consuming, and\nthere\u2019s a high risk of losing all its data if the broker storing it crashes.\n\nTo sum it up, having topic partitions in Kafka is beneficial because it\nenables the distribution of data across multiple brokers in a cluster. This\ndistribution allows for improved load balancing and fault tolerance. It also\nmakes the system more scalable, as the topic\u2019s data can grow beyond the\nstorage limits of a single machine. Additionally, partitions allow for\nconsumer load balancing, as consumers can consume from different brokers. This\nmakes the system more efficient and reliable, as there\u2019s less risk of losing\nall data if one broker crashes.\n\n##\n\nPartition offsets\n\nEach message in a partition gets a unique offset \u2014 a monotonically increasing\ninteger indicating a message\u2019s position in the partition log file. In simple\nterms, an offset says how far a message is located from the start of the log\nfile.\n\nWhen a message is written to a partition, it is appended to the end of the\nlog, assigning the next sequential offset. Offsets are especially helpful for\nconsumers to keep track of the messages they have consumed from a partition.\n\n##\n\nMessage ordering and partition routing\n\nMessages written to a partition are always ordered by the time they arrive.\nBut message ordering across a topic is not guaranteed. If you need strict\nordering within a partition, you must use a partition key properly. But how?\n\nAs we learned above, you can include a key with every message. Upon receiving\na message, Kafka uses a hash function on the key to determine the partition to\nwhich the message should be written. That assures that all records produced\nwith the same key will arrive at the same partition in the exact order they\nwere sent.\n\n#\n\nPartition replication \u2014 leaders and follows\n\nA partition can have more than one copy. They are replicated for two main\nreasons: fault tolerance and high availability.\n\nBy maintaining multiple copies of the same data, Kafka ensures that if one\nbroker fails, another broker can serve the data. This redundancy allows the\nsystem to continue functioning even in the face of failures. Additionally, by\nhaving replicas spread across multiple brokers, Kafka can balance the load of\nread and write requests, improving the system\u2019s performance.\n\nWhen you create a topic, you can optionally specify the partition count as\nwell as the replication factor. If a topic has 10 partitions with a\nreplication factor set to 3, there will be a total of 10x3=30 partitions\nstored across the cluster.\n\n##\n\nPartition leaders and followers\n\nEach partition replica is either a leader or a follower. The leader replica\nhandles all read and write requests for the partition, while the follower\nreplicas passively replicate the leader. If the leader fails, one of the\nfollower replicas will automatically become the new leader.\n\nHow does Kafka determine the partition leader? Kafka depends on a distributed\nconsensus algorithm implementation, such as Apache Zookeeper, to handle\nleadership elections for partitions. When a broker fails and comes back\nonline, or when a new broker is added to the cluster, ZooKeeper helps in\nelecting the new leader for each partition. The election process ensures that\nat any given time, only one broker acts as the leader for a particular\npartition.\n\nHowever, Kafka\u2019s dependency on Zookeeper is being deprecated and replaced with\nKRaft. Some brokers, such as Redpanda, incorporate a native Raft\nimplementation into the broker.\n\n#\n\nSegments\n\nWhile I mentioned that partitions are tangible, that\u2019s not exactly the case. A\npartition is further broken down into segments.\n\nA Segment is the smallest unit of data containment of Kafka storage, which is\nessentially an append-only ordered log file that holds a subset of messages\nbelonging to a partition. Multiple segments get together to form a partition.\n\nFor each partition, there is only one active segment that always receives\ndata. Once enough messages accumulate in the active segment, it is closed or\n\u201crolls over\u201d to the next active segment. This segment size is configurable.\n\nMessages in closed segments can be deleted or compacted to save disk space.\nThis is also configurable. Moreover, with Tiered Storage, you can archive\nolder log segments into cost-efficient storage, like an S3 bucket to reduce\nthe storage cost.\n\nUnlike partitions, segments are not visible and accessible to developers. They\nparticularly belong to the storage and operations side of things.\n\n#\n\nKafka clients\n\nWe say Kafka is a dumb pipe while producers and consumers are smart endpoints.\nThey are thick clients with lots of smart logic embedded in the client SDK.\n\nAs a developer, you can write a Kafka client in any programming language where\na corresponding Kafka client SDK is available. Java and Scala being the\ndefault, Kafka client SDKs are available for languages, including Python,\n.NET, Go, Rust, C++, etc.\n\nA Producer is a client application that generates messages and sends them to a\nKafka topic. The SDK exposes the send() method for producing, which is an\noverloaded method allowing topic name, key, value, and partition ID as\nparameters. The SDK groups messages by partition, batches them together, and\nsends each batch to the broker when the batch size reaches a certain\nthreshold.\n\nA Consumer is a client application that reads messages from a Kafka topic. The\nKafka client SDK provides methods to consume messages either one by one or in\nbatches. Consumers can subscribe to one or more topics and consume messages in\nthe order they were written.\n\nA Consumer Group is a feature that allows a pool of consumers to divide up the\nwork of processing records. When multiple consumers are subscribed to a topic\nand belong to the same consumer group, each consumer in the group will receive\na subset of the records. Kafka ensures that a message is only consumed by one\nconsumer in the group and balances the consumers in case of failure, making it\na useful feature for both scalability and fault tolerance.\n\nIf you need to scale up processing, the only solution is to increase the\npartition count. Not the consumers.\n\n#\n\nWrap up\n\nIn conclusion, we\u2019ve covered the fundamental concepts and components of Kafka\nand the Kafka API. We discussed how Kafka functions as a distributed append-\nonly commit log, and how the Kafka API offers a standard interface for\ninteracting with it.\n\nWe explored the concepts of brokers and clusters, messages, topics,\npartitions, offsets, and replication. We also delved into the importance of\nsegments and the role of Kafka clients. Understanding these concepts is\ncrucial for anyone working with Kafka, regardless of the specific distribution\nor programming language used.\n\nFor a hands-on learning experience, beginners can follow the Kafka Building\nBlocks course that Redpanda University offers.\n\nThanks for reading Tributary Data! Subscribe for free to receive new posts and\nsupport my work.\n\nShare this post\n\n#### A Primer on Kafka API\n\ntributarydata.substack.com\n\nShare\n\nComments\n\nIn-game Analytics Pipeline with Redpanda, ClickHouse, and Streamlit\n\nHow to build a scalable and low-latency leaderboard for video games.\n\nMar 13 \u2022\n\nDunith Danushka\n\nShare this post\n\n#### In-game Analytics Pipeline with Redpanda, ClickHouse, and Streamlit\n\ntributarydata.substack.com\n\nHow Does Throttling Work?\n\nExplains throttling in the context of streaming data, how it can be\nimplemented, and how it differs from rate limiting Continue reading on Level\nUp...\n\nJan 10 \u2022\n\nDunith Danushka\n\nShare this post\n\n#### How Does Throttling Work?\n\ntributarydata.substack.com\n\nOperational Use case Patterns for Apache Kafka and Flink \u2014 Part 1\n\nOperational Use case Patterns for Apache Kafka and Flink \u2014 Part 1 This is the\nfirst post of the series that shows building operational use cases with...\n\nJan 3, 2023 \u2022\n\nDunith Danushka\n\nShare this post\n\n#### Operational Use case Patterns for Apache Kafka and Flink \u2014 Part 1\n\ntributarydata.substack.com\n\nReady for more?\n\n\u00a9 2024 Dunith Danushka\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
