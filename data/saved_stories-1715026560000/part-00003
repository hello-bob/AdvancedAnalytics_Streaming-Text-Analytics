{"aid": "40274429", "title": "Scaling hierarchical agglomerative clustering to trillion-edge graphs", "url": "https://research.google/blog/scaling-hierarchical-agglomerative-clustering-to-trillion-edge-graphs/", "domain": "research.google", "votes": 1, "user": "Anon84", "posted_at": "2024-05-06 13:29:39", "comments": 0, "source_title": "Scaling hierarchical agglomerative clustering to trillion-edge graphs", "source_text": "Scaling hierarchical agglomerative clustering to trillion-edge graphs\n\nresearch.google uses cookies from Google to deliver and enhance the quality of\nits services and to analyze traffic. Learn more.\n\nJump to Content\n\nResearch\n\nResearch\n\n# Scaling hierarchical agglomerative clustering to trillion-edge graphs\n\nMay 1, 2024\n\nLaxman Dhulipala and Jakub \u0141\u0105cki, Research Scientists, Google Research, Graph\nMining Team\n\nWe describe a series of our recent works on building more scalable graph\nclustering, culminating in our \u201cTeraHAC: Hierarchical Agglomerative Clustering\nof Trillion-Edge Graphs\u201d paper. We cover some of the key ideas behind this\nwork and explain how to scale a high-quality clustering algorithm that can\ncluster trillion-edge graphs.\n\n## Quick links\n\n  * TeraHAC paper\n  *     * \u00d7\n\nIn large-scale prediction and information retrieval tasks, we frequently\nencounter various applications of clustering, a type of unsupervised machine\nlearning that aims to group similar items together. Examples include de-\nduplication, anomaly detection, privacy, entity matching, compression, and\nmany more (see more examples in our posts on balanced partitioning and\nhierarchical clustering at scale, scalability and quality in text clustering,\nand practical differentially private clustering).\n\nA major challenge is clustering extremely large datasets containing hundreds\nof billions of datapoints. While multiple high-quality clustering algorithms\nexist, many of them are not easy to run at scale, often due to their high\ncomputational costs or not being designed for parallel multicore or\ndistributed environments. The Graph Mining team within Google Research has\nbeen working for many years to develop highly scalable clustering algorithms\nthat deliver high quality.\n\nTen years ago, our colleagues developed a scalable, high-quality algorithm,\ncalled affinity clustering, using the simple idea that each datapoint should\nbe in the same cluster as the datapoint it\u2019s most similar to. One can imagine\nthis process as drawing an edge from each point to its nearest neighbor and\ncontracting the connected components. This simple approach was amenable to a\nscalable MapReduce implementation. However, the guiding principle for the\nalgorithm was not always correct as similarity is not a transitive relation,\nand the algorithm can erroneously merge clusters due to chaining, i.e.,\nmerging long paths of pairwise related concepts to place two unrelated\nconcepts in the same cluster, as illustrated below.\n\nA big improvement to affinity clustering came with the development of the Sub-\nCluster Component (SCC) algorithm, which places a point in the same cluster as\nits most similar neighbor, only if their similarity is at least a given\nthreshold. Put another way, the algorithm merges connected components induced\nby edges with similarity that is at least the threshold. Then, the algorithm\nrepeats this approach, and gradually decreases the threshold at each step.\nInspired by this line of work on affinity clustering and SCC, we decided to\ntake a fresh look at designing high-quality extremely large-scale clustering\nalgorithms.\n\nIn this blogpost, we describe a series of our recent works on building more\nscalable graph clustering, culminating in our SIGMOD 2024 paper on \u201cTeraHAC:\nHierarchical Agglomerative Clustering of Trillion-Edge Graphs\u201d. We cover some\nof the key ideas behind this work and explain how to scale a high-quality\nclustering algorithm that can cluster trillion-edge graphs.\n\n## Hierarchical agglomerative clustering (HAC)\n\nBoth affinity clustering and SCC are inspired by the popular hierarchical\nagglomerative clustering (HAC) algorithm, a greedy clustering algorithm that\nis well known to deliver very good clustering quality. The HAC algorithm\ngreedily merges the two closest clusters into a new cluster, proceeding until\nthe desired level of dissimilarity between all pairs of remaining clusters is\nachieved. HAC can be configured using a linkage function that computes the\nsimilarity between two clusters; for example, the popular average-linkage\nmeasure computes the similarity of (A, B) as the average similarity between\nthe two sets of points. Average-linkage is a good choice of linkage function\nas it takes into account the sizes of the clusters that are being merged.\nHowever, because HAC performs merges greedily, there is no obvious way to\nparallelize the algorithm. Moreover, the best previous implementations had\nquadratic complexity, that is the number of operations they performed was\nproportional to n2, where n is the number of input data points. As a result,\nHAC could only be applied to moderately sized datasets.\n\n## Speeding up HAC\n\nIn a line of work starting in 2021, we carefully investigated the complexity\nand parallel scalability of HAC, from a sparse graph-theoretic perspective. We\nobserved that in practice, only m << n2 of the most similar entries in the\nsimilarity matrix were actually relevant for computing high-quality clusters.\nFocusing on this sparse setting, we set out to determine if we could design\nHAC algorithms that exploit sparsity to achieve two key goals:\n\n  1. a number of computation steps proportional to the number of similarities (m), rather than n2, and\n  2. high parallelism and scalability.\n\nWe started by carefully exploring the first goal, since achieving this would\nmean that HAC could be solved much faster on sparse graphs, thus presenting a\npath for scaling to extremely large datasets. We proved this possibility by\npresenting an efficient sub-quadratic work algorithm for the problem on sparse\ngraphs which runs in sub-quadratic work (specifically, O(nm^1\u20442) work, up to\npoly-logarithmic factors). We obtained the algorithm by designing a careful\naccounting scheme that combines the classic nearest-neighbor chain algorithm\nfor HAC with a dynamic edge-orientation algorithm.\n\nHowever, the sub-quadratic work algorithm requires maintaining a complicated\ndynamic edge-orientation data structure and is not easy to implement. We\ncomplemented our exact algorithm with a simple algorithm for approximate\naverage-linkage HAC, which runs in nearly-linear work, i.e., O(m + n) up to\npoly-logarithmic factors, and is a natural relaxation of the greedy algorithm.\nLet \u03b5 be an accuracy parameter. Then, more formally, a (1+\u03b5)-approximation of\nHAC is a sequence of cluster merges, where the similarity of each merge is\nwithin a factor of (1+\u03b5) of the highest similarity edge in the graph at that\ntime (i.e., the merge that the greedy exact algorithm will perform).\n\nExperimentally, this notion of approximation (say for \u03b5=0.1) incurs a minimal\nquality loss over the exact algorithm on the same graph. Furthermore, the\napproximate algorithm also yielded large speedups of over 75\u00d7 over the\nquadratic-work algorithms, and could scale to inputs with tens of millions of\npoints. However, our implementations were slow for inputs with more than a few\nhundred million edges as they were entirely sequential.\n\nThe next step was to attempt to design a parallel algorithm that had the same\nwork and a provably low number of sequential dependencies (formally, low\ndepth, i.e., longest critical path). In a pair of papers from NeurIPS 2022 and\nICALP 2024, we studied how to obtain good parallel algorithms for HAC. First,\nwe confirmed the common wisdom that exact average-linkage HAC is hard to\nparallelize due to the sequential dependencies between successive greedy\nmerges. Formally, we showed that the problem is as hard to parallelize as any\nother problem solvable in polynomial time. Thus, barring a breakthrough in\ncomplexity theory, average-linkage HAC is unlikely to admit fast parallel\nalgorithms.\n\nOn the algorithmic side, we developed a parallel approximate HAC algorithm,\ncalled ParHAC, that we show is highly scalable and runs in near-linear work\nand poly-logarithmic depth. ParHAC works by grouping the edges into O(log n)\nweight classes, and processing each class using a carefully designed low-depth\nsymmetry-breaking algorithm. ParHAC enabled the clustering of the\nWebDataCommons hyperlink graph, one of the largest publicly available graph\ndatasets with over 100 billion edges, in just a few hours using an inexpensive\nmulticore machine.\n\nplay silent looping video pause silent looping video\n\nSequential HAC (on the left) merges one pair of clusters per step. The\nparallel algorithm (on the right) can merge multiple pairs of clusters at each\nstep, which results in fewer steps overall.\n\n## The TeraHAC algorithm\n\nThe final frontier is to obtain scalable and high-quality HAC algorithms when\na graph is so large that it no longer fits within the memory of a single\nmachine. In our most recent paper, we designed a new algorithm, called\nTeraHAC, that scales approximate HAC to trillion-edge graphs. TeraHAC is based\non a new approach to computing approximate HAC that is a natural fit for\nMapReduce-style algorithms. The algorithm proceeds in rounds. In each round,\nit partitions the graph into subgraphs and then runs on each subgraph\nindependently to make merges. The tricky question is what merges is the\nalgorithm allowed to make?\n\nThe main idea behind TeraHAC is to identify a way to perform merges based\nsolely on information local to the subgraphs, while guaranteeing that the\nmerges can be reordered into an approximate merge sequence. The paradox rests\nin the fact that a cluster in some subgraph may make a merge that is far from\nbeing approximate. However, the merges satisfy a certain condition, which\nallows us to show that the final result of the algorithm is still the same as\nwhat a proper (1+\u03b5)-approximate HAC algorithm would have computed.\n\nTeraHAC uses at most a few dozen rounds on all of the graphs used for\nevaluation, including the largest inputs; our round-complexity is over 100\u00d7\nlower than the best previous approaches to distributed HAC. Our implementation\nof TeraHAC can compute a high-quality approximate HAC solution on a graph with\nseveral trillion edges in under a day using a modest amount of cluster\nresources (800 hyper-threads over 100 machines). The quality results on this\nextremely large dataset are shown in the figure below; we see that TeraHAC\nachieves the best precision-recall tradeoff compared to other scalable\nclustering algorithms, and is likely the algorithm of choice for very large-\nscale graph clustering.\n\n## Conclusion\n\nWe demonstrate that despite being a classic method, HAC still has many more\nsecrets to reveal. Interesting directions for future work include designing\ngood dynamic algorithms for HAC (some recent work by our colleagues show\npromising results in this direction) and understanding the complexity of HAC\nin low-dimensional metric spaces. Aside from theory, we are hopeful that our\nteam\u2019s approach to scalable and sparse algorithms for hierarchical clustering\nfinds wider use in practice.\n\n## Acknowledgements\n\nThis blog post reports joint work with our colleagues including\nMohammadHossein Bateni, David Eisenstat, Rajesh Jayaram, Jason Lee, Vahab\nMirrokni and a former intern, Jessica Shi. We also thank our academic\ncollaborators Kishen N Gowda and D Ellis Hershkowitz. Lastly, we thank Tom\nSmall for his valuable help with making the animation in this blog post.\n\nLabels:\n\n  * Algorithms & Theory\n\n  * Machine Intelligence\n\n## Quick links\n\n  * TeraHAC paper\n  *     * \u00d7\n\n### Other posts of interest\n\n  * April 23, 2024\n\nSafely repairing broken builds with ML\n\n    * Machine Intelligence \u00b7\n    * Software Systems & Engineering\n\n  * April 19, 2024\n\nImproving Gboard language models via private federated analytics\n\n    * Algorithms & Theory \u00b7\n    * Distributed Systems & Parallel Computing \u00b7\n    * Product \u00b7\n    * Security, Privacy and Abuse Prevention\n\n  * April 16, 2024\n\nSolving the minimum cut problem for undirected graphs\n\n    * Algorithms & Theory\n\nFollow us\n\n", "frontpage": false}
