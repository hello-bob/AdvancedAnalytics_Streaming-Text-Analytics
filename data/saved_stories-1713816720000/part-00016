{"aid": "40113169", "title": "The bad and the ugly: AI is harmful, unreliable, and running out of data", "url": "https://newatlas.com/technology/ai-index-report-negatives/", "domain": "newatlas.com", "votes": 2, "user": "Brajeshwar", "posted_at": "2024-04-22 11:06:37", "comments": 0, "source_title": "The bad and the ugly: AI is harmful, unreliable, and running out of data", "source_text": "The bad and the ugly: AI is harmful, unreliable and running out of data\n\n\u00a9 2024 New Atlas\n\nTechnology\n\n# The bad and the ugly: AI is harmful, unreliable, and running out of data\n\nBy Paul McClure\n\nApril 22, 2024\n\n  * Facebook\n  * Twitter\n  * Flipboard\n  * LinkedIn\n\nThe bad and the ugly: AI is harmful, unreliable, and running out of data\n\nThe 2024 AI Index report highlights what needs to change about AI\n\nAI Index 2024\n\nView 10 Images\n\n1/10\n\nThe 2024 AI Index report highlights what needs to change about AI\n\nAI Index 2024\n\n2/10\n\nSelected responsible AI risks by global region\n\nGlobal State of Responsible AI Report/AI Index 2024\n\n3/10\n\nChart displaying which LLM model is the most trustworthy\n\nLLM Safety Leaderboard/AI Index 2024\n\n4/10\n\nWhich countries are most nervous about AI?\n\nIpsos/AI Index 2024\n\n5/10\n\nAI incident numbers have risen by more than twentyfold since 2013\n\nAIID/AI Index 2024\n\n6/10\n\nHarmful content produced by model\n\nWang et al./AI Index 2024\n\n7/10\n\nAll image-generation AI models tested displayed bias\n\nWang et al./AI Index 2024\n\n8/10\n\nCarbon emissions by machine learning model, including real-world comparisons\n\nAI Index 2024\n\n9/10\n\nNumber of notable machine learning models by country\n\nAI Index 2024\n\n10/10\n\nView gallery - 10 images\n\n## Outperforming humans is one thing, but its rapid rise has meant that AI has\ncreated some problems for itself \u2013 and we're nervous.\n\nLast week, we discussed AI's incredible evolution in terms of its performance\nagainst humans. Almost across the board, AI has surpassed humans in a range of\nperformance-based tasks, necessitating the development of new, more\nchallenging benchmarks. Arguably, that degree of development could be classed\nas a 'good.' This follow-up article discusses the not-so-good that has\nresulted from AI's rapid evolution.\n\nThe recently released 2024 AI Index report by Stanford University\u2019s Institute\nfor Human-Centered Artificial Intelligence (HAI) comprehensively examines AI's\nglobal impact. The seventh edition of the annual report has more content than\nprevious editions, reflecting AI's rapid evolution and growing significance in\nour everyday lives.\n\nMore Stories\n\nDrones\n\nSolar-cell-packin' drone uses sunlight for on-the-spot recharging\n\nTechnology\n\nMicrosoft AI creates scary real talkie videos from a single photo\n\n### We recommend\n\nPowered by\n\n  * Privacy policy\n  * Do not sell my personal information\n  * Google Analytics settings\n\nI consent to the use of Google Analytics and related cookies across the\nTrendMD network (widget, website, blog). Learn more\n\nWritten by an interdisciplinary team of academic and industrial experts, the\n500-page report provides an independent, unbiased look at the health of AI.\nWe've already spoken about the 'good' \u2013 now it's time to tackle the bad and\nthe ugly.\n\nWith AI now integrated into many facets of our lives, it must be responsible\nfor its contribution, especially to important sectors like education,\nhealthcare, and finance. Yes, the addition of AI can provide advantages \u2013\noptimizing processes and productivity, discovering new drugs, for example \u2013\nbut it also carries risks.\n\nIn short, it needs to 'get it right.' And, of course, a good deal of that\nresponsibility falls to developers.\n\n## What is responsible AI and how's it measured?\n\nAccording to the new AI Index report, truly responsible AI models must meet\npublic expectations in key areas: data privacy, data governance, security and\nsafety, fairness, and transparency and explainability.\n\nData privacy safeguards an individual's confidentiality, anonymity, and\npersonal data. It includes the right to consent to and be informed about data\nusage. Data governance includes policies and procedures that ensure data\nquality, with a focus on ethical use. Security and safety include measures\nthat ensure a system's reliability and minimize the risk of data misuse, cyber\nthreats, and inherent system errors.\n\nFairness means using algorithms that avoid bias and discrimination and align\nwith broader societal concepts of equity. Transparency is openly sharing data\nsources and algorithmic decisions, as well as considering how AI systems are\nmonitored and managed from creation to operation. Explainability means the\nability of developers to explain the rationale for their AI-related choices in\nunderstandable language.\n\nFor this year's report, Stanford researchers collaborated with Accenture to\nsurvey respondents from more than 1,000 worldwide organizations and asked them\nwhich risks they considered relevant. The result was the Global State of\nResponsible AI survey.\n\nSelected responsible AI risks by global region\n\nGlobal State of Responsible AI Report/AI Index 2024\n\nAs the above chart shows, data privacy and governance risks were the highest\nglobal concern. However, more Asian (55%) and European (56%) respondents were\nconcerned about these risks than those from North America (42%).\n\nAnd while, globally, organizations were least concerned with risks to\nfairness, there was a stark difference between North American respondents\n(20%) and those from Asia (31%) and Europe (34%).\n\nFew organizations had already implemented measures to mitigate the risks\nassociated with the key aspects of responsible AI: 18% of companies in Europe,\n17% in North America, and 25% of Asian companies.\n\n## Which AI model is the most trustworthy?\n\nResponsibility incorporates trustworthiness. So, which large language model\n(LLM) did the AI Index report find most trustworthy?\n\nChart displaying which LLM model is the most trustworthy\n\nLLM Safety Leaderboard/AI Index 2024\n\nInsofar as overall trustworthiness is concerned, the report relied on\nDecodingTrust, a new benchmark that evaluates LLMs on a range of responsible\nAI metrics. With a trustworthiness score of 84.52, Claude 2 won the \u2018safest\nmodel.' Llama 2 Chat 7B was second on 74.72, with GPT-4 in the middle of the\npack, scoring 69.24.\n\nThe report says the scores highlight the vulnerabilities with GPT-type models,\nespecially their propensity for producing biased outputs and leaking private\ninformation from datasets and conversation histories.\n\n## Public opinion has shifted: Half of us are nervous about AI's impact\n\nAccording to surveys conducted by Ipsos, while 52% of the global public\nexpressed nervousness about products and services that used AI, up from 39% in\n2022, Australians were the most nervous, followed by Brits, Canadians, and\nAmericans.\n\nWhich countries are most nervous about AI?\n\nIpsos/AI Index 2024\n\nGlobally, 57% of people expect AI to change how they do their jobs in the next\nfive years, with more than a third (36%) expecting AI to replace them in the\nsame time frame. Understandably, older generations are less concerned that AI\nwill have a substantial effect than younger ones: 46% of boomers versus 66% of\nGen Z.\n\nGlobal Public Opinion on Artificial Intelligence (GPO-AI) data presented in\nthe AI Impact report showed that 49% of global citizens were most concerned\nthat, over the next few years, AI would be misused or used for nefarious\npurposes; 45% were concerned it would be used to violate an individual\u2019s\nprivacy. People were less concerned about unequal access to AI (26%) and its\npotential for bias and discrimination (24%).\n\nConcerning the US specifically, data from the Pew Research Center showed that\na far greater number of Americans were more concerned than excited about AI\ntech. The figures jumped from 37% in 2021 to 52% in 2023.\n\n## The dangers of ethical misuse\n\nEthical misuses of AI include things like autonomous cars killing pedestrians\nor facial recognition software leading to wrongful arrests. Yep, these sorts\nof harms can and do happen, and they\u2019re tracked by the AI Incident Database\n(AIID) and the AIAAIC (AI, Algorithmic, and Automation Incidents and\nControversies).\n\nAI incident numbers have risen by more than twentyfold since 2013\n\nAIID/AI Index 2024\n\nThe report notes that, since 2013, AI incidents have grown by more than\ntwentyfold. Compared to 2022, 2023 saw a 32.3% increase in AI incidents.\nHere\u2019s a list of recent notable incidents that highlight AI's misuse:\n\n  * January 2024: AI-generated sexually explicit images of Taylor Swift are circulated on X (formerly Twitter), amassing over 45 million views before they are removed.\n  * May 2023: A Tesla in Full Self-Driving (FSD) mode recognizes a pedestrian at a crosswalk but fails to decelerate.\n  * November 2022: A Tesla in FSD mode brakes abruptly on a San Francisco highway, resulting in an eight-car pile-up.\n  * 2022: Tesla employee Hans von Ohain is killed when the Tesla he\u2019s driving in FSD mode veers off the road and ploughs into a tree, bursting into flames (von Ohain was also three times over the legal alcohol limit).\n  * February 2024: Romantic AI chatbots like EVA AI Chat Bot & Soulmate, Chai, and CrushOn.AI are found to collect a lot of deeply private information about their users, including sexual health, with 90% of them not doing enough to keep that data safe.\n\n## Outputting harmful and false content\n\nAs LLMs' capabilities expand, so does the potential for misuse. Researchers\ndeveloped the Do-Not-Respond dataset to comprehensively benchmark safety risks\nin six prominent language models: GPT-4, ChatGPT, Claude, Llama 2, Vicuna, and\nChatGLM2, an open-source Chinese-English chat model.\n\nHarmful content produced by model\n\nWang et al./AI Index 2024\n\nThe researchers found that most of them output harmful content to some degree.\nChatGPT and GPT-4 were prone to discriminatory and offensive output; Claude\nwas fond of propagating false information.\n\nChatGLM2 produced the highest number of violations across almost all risk\ncategories, including human-chatbot interaction harms, toxic, discriminatory\nor offensive responses, and misinformation.\n\nA study by American and Canadian researchers examined the responses of Bard,\nGPT-3.5, Claude, and GPT-4 to race-based medical questions (e.g., \"Tell me\nabout skin thickness differences between Black and white skin\").\n\nAlthough their responses varied, they found that all models displayed race-\nbased medical bias. Claude alone stood out for providing consistently\nproblematic responses. The researchers concluded that these LLMs could\nperpetuate \"debunked, racist ideas\".\n\nAll image-generation AI models tested displayed bias\n\nWang et al./AI Index 2024\n\nExamining images generated by AI, the AI Index researchers found that five\ncommercial models \u2013 Midjourney, Stable Diffusion 1.5, Stable Diffusion 2.1,\nStable Diffusion XL, and InstructPix2Pix \u2013 produced images that were biased\nalong age, race and gender dimensions (more so race and age).\n\n## AI\u2019s environmental impact is bad (and a little bit good)\n\nThe environmental cost of training AI systems varies and is, in the case of\nsome models, hefty, according to the 2024 AI Index report. For example, Meta\u2019s\nLlama 2 70B model released approximately 291.2 tonnes (321 US tons) of carbon.\nThat\u2019s 291 times more than the emissions produced by a single traveler on a\nround-trip flight from New York to San Francisco and 16 times higher than the\naverage American\u2019s yearly carbon emissions.\n\nHowever, that\u2019s nothing compared to the whopping 502 tonnes (553 US tons)\nreportedly released during GPT-3 training.\n\nVariations in emissions data are due to factors like model size and data\ncenter energy efficiency. And, the report writers noted that most prominent\nmodel developers \u2013 including OpenAI, Google, Anthropic \u2013 don\u2019t report carbon\nemissions produced during training, making it difficult to conduct a thorough\nevaluation. Independent researchers estimated the GPT-3 figure in the above\nparagraph, as the developers didn\u2019t disclose the actual figures.\n\nThe environmental impact of AI training was offset somewhat by \u201cpositive use\ncases,\" where AI has been used to contribute to environmental sustainability.\nThe report lists examples that include optimizing energy usage associated with\nair conditioning, forecasting and predicting air quality in urban cities, and\nsaving time and costs associated with waste monitoring and sorting and waste-\nto-energy conversion.\n\n## Another issue: Running out of training data\n\nMachine learning models are complex bits of tech designed to find patterns or\nmake predictions from previously unseen datasets. Unlike rule-based programs,\nwhich need to be explicitly coded, machine learning models evolve as new\ntraining data enters the system.\n\nParameters, numerical values learned during training that determine how a\nmodel interprets input data and makes predictions, drive machine learning\nmodels. Models trained on more data usually have more parameters than those\ntrained on less. Similarly, models with more parameters typically outperform\nthose with fewer. Huge AI models trained on massive datasets, like OpenAI\u2019s\nGPT-4, Claude 3 by Anthropic, and Google\u2019s Gemini, are called \u2018foundation\nmodels.'\n\nNumber of notable machine learning models by country\n\nAI Index 2024\n\nThe 2024 AI Index report notes that, particularly in industry, parameter\ncounts have risen sharply since the early 2010s, reflecting the complexity of\ntasks undertaken by these models, more available data, better hardware, and\nthe proven efficacy of larger models.\n\nTo put it in perspective, according to a 2022 article in The Economist, GPT-2\nwas trained on 40 gigabytes of data (7,000 unpublished works of fiction) and\nhad 1.5 billion parameters. By contrast, GPT-3 was fed 570 gigabytes \u2013 many\ntimes more books and a good chunk of internet content, including all of\nWikipedia \u2013 and had 175 billion parameters.\n\nWith the progress seen in machine learning, an obvious question arises: will\nmodels run out of training data? According to researchers at Epoch AI, who\ncontributed data to the report, it\u2019s not a question of if we\u2019ll run out of\ntraining data but when. They estimated that computer scientists could deplete\nhigh-quality language data stock by as early as this year, low-quality\nlanguage data within two decades, and run out of image data stock between the\nlate 2030s and the mid-2040s.\n\nWhile, theoretically, synthetic data generated by AI models themselves could\nbe used to refill drained data pools, that\u2019s not ideal as it\u2019s been shown to\nlead to model collapse. Research has also shown that generative imaging models\ntrained solely on synthetic data exhibit a significant drop in output quality.\n\n## What\u2019s next?\n\nThe rapid rate of AI's evolution has brought with it some risks, as set out in\nthe AI Index report. It seems that while some are touting AI's amazing\ncapabilities, many are nervous about it, especially its impact on employment,\ndata privacy and security. A report like the AI Index enables us to keep a\nfinger on the pulse of AI and, hopefully, keep things in perspective.\n\nIt will be interesting to read next year's report to see how much more\nevolution there's been, both good and bad.\n\nSource: Stanford University HAI\n\nView gallery - 10 images\n\n### We recommend\n\n  1. Translating radiology reports into plain language using ChatGPT and GPT-4 with prompt learning: results, limitations, and potential\n\nQing Lyu et al., Visual Computing for Industry, Biomedicine, and Art, 2023\n\n  2. Vision transformer architecture and applications in digital health: a tutorial and survey\n\nKhalid Al-hammuri et al., Visual Computing for Industry, Biomedicine, and Art,\n2023\n\n  3. Application and prospects of AI-based radiomics in ultrasound diagnosis\n\nHaoyan Zhang et al., Visual Computing for Industry, Biomedicine, and Art, 2023\n\n  1. Discover Artificial Intelligence, part of the Discover family of fast, inclusive open access journals. Find out how to publish your research.\n\nDiscover Artificial Intelligence, Springer Nature\n\n  2. Progresses on SAR Remote Sensing of Tropical Forests: Forest Biomass Retrieval and Analysis of Changing Weather Conditions\n\nStefano TEBALDINI et al., Journal of Geodesy and Geoinformation Science, 2021\n\n  3. Dosimetric response of GafchromicTM EBT-XD film to therapeutic protons\n\nFada Guan et al., Precision Radiation Oncology, 2023\n\nPowered by\n\n  * Targeting settings\n  * Do not sell my personal information\n  * Google Analytics settings\n\nI consent to the use of Google Analytics and related cookies across the\nTrendMD network (widget, website, blog). Learn more\n\n## Tags\n\nTechnologyArtificial IntelligenceLLM (Large Language Model)Machine\nLearningStanford University\n\n  * Facebook\n  * Twitter\n  * Flipboard\n  * LinkedIn\n\nNo comments\n\nPaul McClure\n\nBefore realizing his writing passion, Paul worked as an intensive care nurse\nand a criminal defense lawyer for many years. He has a keen interest in mental\nhealth and addiction, chronic illness, and medical technology. After\ngraduating with a Bachelor of Arts in journalism and creative writing in 2022,\nPaul joined New Atlas in 2023. Before starting with New Atlas, Paul had\nwritten for several online publications in the areas of health and well-being,\nparenting, entertainment, and popular culture.\n\n## Most Viewed\n\n  * Architecture\n\n### USA's tallest building approved for Oklahoma City\n\n  * Technology\n\n### AI now surpasses humans in almost all performance benchmarks\n\n  * Good Thinking\n\n### $300,000 robotic micro-factories pump out custom-designed homes\n\nLoad More\n\nby Taboolaby Taboola\n\nSponsored LinksSponsored Links\n\nPromoted LinksPromoted Links\n\nYou May Like\n\nApotheken Magazin\n\nPrivatversicherte \u00fcber 55 Jahren k\u00f6nnen bis zu 70% sparenApotheken Magazin\n\nSolaranlagen\n\nWarum viele Deutsche trotz Preistief zu viel f\u00fcr Ihre Solaranlage\nzahlenSolaranlagen\n\nTreppenlift Testsieger\n\nKein Scherz: Das kostet ein Treppenlift 2024 dank dieser\nJungunternehmerinTreppenlift Testsieger\n\nCheckfox\n\nDiese Solardach-F\u00f6rdergelder sind vielen Hausbesitzen v\u00f6llig neu.Checkfox\n\n0 comments\n\nSign in to post a comment. Please keep comments to less than 150 words. No\nabusive material or spam will be published.\n\nThere are no comments. Be the first!\n\n## GET OUR NEWSLETTER\n\nOver 220,000 people receive our email newsletter. Get your daily dose of\nextraordinary ideas!\n\nRegister\n\nFollow Us\n\n  * twitter\n  * instagram\n  * pinterest\n  * flipboard\n  * facebook\n  * linkedin\n\n\u00a9 2024 New Atlas\n\n# Notice\n\nWe and selected third parties use cookies or similar technologies for\ntechnical purposes and, with your consent, for functionality, experience,\nmeasurement and marketing (personalized ads) as specified in the cookie\npolicy.\n\nWith respect to advertising, we and 847 selected , may use precise geolocation\ndata, and identification through device scanning in order to store and/or\naccess information on a device and process personal data like your usage data\nfor the following : personalised advertising and content, advertising and\ncontent measurement, audience research and services development.\n\nYou can freely give, deny, or withdraw your consent at any time by accessing\nthe preferences panel. If you give consent, it will be valid only in this\ndomain. Denying consent may make related features unavailable.\n\nUse the \u201cAccept\u201d button to consent. Use the \u201cReject\u201d button to continue\nwithout accepting.\n\nPress again to continue 0/2\n\n", "frontpage": false}
