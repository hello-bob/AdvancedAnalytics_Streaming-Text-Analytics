{"aid": "40058716", "title": "From Zero to Fineturning with Axolotl on ROCm", "url": "https://erichartford.com/from-zero-to-fineturning-with-axolotl-on-rocm", "domain": "erichartford.com", "votes": 1, "user": "belter", "posted_at": "2024-04-16 23:40:27", "comments": 0, "source_title": "From Zero to Fineturning with Axolotl on ROCm", "source_text": "From Zero to Fineturning with Axolotl on ROCm\n\n#\n\nCognitive Computations\n\n# Cognitive Computations\n\n# From Zero to Fineturning with Axolotl on ROCm\n\nEric Hartford\n\n\u00b7Mar 11, 2024\u00b7\n\n7 min read\n\nGratitude to https://tensorwave.com/ for giving me access to their excellent\nservers!\n\nFew have tried this and fewer have succeeded. I've been marginally successful\nafter a significant amount of effort, so it deserves a blog post.\n\nKnow that you are in for rough waters. And even when you arrive - There are\nlots of optimizations tailored for nVidia GPUs so, even though the hardware\nmay be just as strong spec-wise, in my experience so far, it still may take\n2-3 times as long to train on equivalient AMD hardware. (though if you are a\nsuper hacker maybe you can fix it!)\n\nRight now I'm using Axolotl. Though I am probably going to give LlamaFactory a\nsolid try in the near future. There's also LitGpt and TRL. But I kind of rely\non the dataset features and especially the sample packing of Axolotl. But more\nand more LlamaFactory is interesting me, it supports new features really fast.\n(like GaLore is the new hotness at the moment). This blog post will be about\ngetting Axolotl up and running in AMD, and I may do one about LlamaFactory if\nthere is demand.\n\nI am using Ubuntu 22.04 LTS, and you should too. (unless this blog post is\nreally old by the time you read it). Otherwise you can use this post as a\ngeneral guide.\n\nHere are all the environment variables I ended up setting in my .bashrc and\nI'm not exactly sure which ones are needed. You better set them all just in\ncase.\n\n    \n    \n    export GPU_ARCHS=\"gfx90a\" # mi210 - use the right code for your GPU export ROCM_TARGET=\"gfx90a\" export HIP_PATH=\"/opt/rocm-6.0.0\" export ROCM_PATH=\"/opt/rocm-6.0.0\" export ROCM_HOME=\"/opt/rocm-6.0.0\" export HIP_PLATFORM=amd export DS_BUILD_CPU_ADAM=1 export TORCH_HIP_ARCH_LIST=\"gfx90a\"\n\n# Part 1: Driver, ROCm, HIP\n\n## Clean everything out.\n\nThere shouldn't be any trace of nvidia, cuda, amd, hip, rocm, anything like\nthat. This is not necessarily a simple task, and of course it totally depends\non the current state of your system. and I had to use like 4 of my daily\nClaude Opus questions to accomplish this. (sad face) By the way Anthropic\nClaude Opus is the new king of interactive troubleshooting. By far. Bravo.\nDon't nerf it pretty please!\n\nHere are some things I had to do, that might help you:\n\n  * sudo apt autoremove rocm-core\n\n  * sudo apt remove amdgpu-dkms\n\n  * sudo dpkg --remove --force-all amdgpu-dkms\n\n  * sudo apt purge amdgpu-dkms\n\n  * sudo apt remove --purge nvidia*\n\n  * sudo apt remove --purge cuda*\n\n  * sudo apt remove --purge rocm-* hip-*\n\n  * sudo apt remove --purge amdgpu-* xserver-xorg-video-amdgpu\n\n  * sudo apt clean\n\n  * sudo reboot\n\n  * sudo dpkg --remove amdgpu-install\n\n  * sudo apt remove --purge amdgpu-* xserver-xorg-video-amdgpu\n\n  * sudo apt autoremove\n\n  * sudo apt clean\n\n  * rm ~/amdgpu-install_*.deb\n\n  * sudo reboot\n\n  * sudo rm /etc/apt/sources.list.d/amdgpu.list\n\n  * sudo rm /etc/apt/sources.list.d/rocm.list\n\n  * sudo rm /etc/apt/sources.list.d/cuda.list\n\n  * sudo apt-key del A4B469963BF863CC\n\n  * sudo apt update\n\n  * sudo apt remove --purge nvidia-* cuda-* rocm-* hip-* amdgpu-*\n\n  * sudo apt autoremove\n\n  * sudo apt clean\n\n  * sudo rm -rf /etc/OpenCL /etc/OpenCL.conf /etc/amd /etc/rocm.d /usr/lib/x86_64-linux-gnu/amdgpu /usr/lib/x86_64-linux-gnu/rocm /opt/rocm-* /opt/amdgpu-pro-* /usr/lib/x86_64-linux-gnu/amdvlk\n\n  * sudo reboot\n\n  * I love Linux (smile with tear)\n\n  * Now finally do like sudo apt-get updatesudo apt-get upgrade and sudo apt-get dist-upgrade and make sure there's no errors or warnings! You should be good to begin your journey.\n\n## Install AMD drivers, ROCm, HIP\n\n  * wgethttps://repo.radeon.com/amdgpu-install/23.40.2/ubuntu/jammy/amdgpu-install_6.0.60002-1_all.deb\n\n  * (at time of this writing). But you should double check here. And the install instructions here.\n\n  * sudo apt-get install ./amdgpu-install_6.0.60002-1_all.deb\n\n  * sudo apt-get update\n\n  * sudo amdgpu-install -y --accept-eula --opencl=rocr --vulkan=amdvlk --usecase=workstation,rocm,rocmdev,rocmdevtools,lrt,opencl,openclsdk,hip,hiplibsdk,mllib,mlsdk\n\n  * If you get error messages (I did) try to fix them. I had to do this:\n\n    * sudo dpkg --remove --force-all libvdpau1\n\n    * sudo apt clean\n\n    * sudo apt update\n\n    * sudo apt --fix-broken install\n\n    * sudo apt upgrade\n\n    * and then, again, I had to run sudo amdgpu-install -y --accept-eula --opencl=rocr --vulkan=amdvlk --usecase=workstation,rocm,rocmdev,rocmdevtools,lrt,opencl,openclsdk,hip,hiplibsdk,mllib,mlsdk\n\n## Check Installation\n\n    \n    \n    rocm-smi rocminfo /opt/rocm/bin/hipconfig --full\n\nI hope that worked for you - if not, I suggest asking Claude Opus about the\nerror messages to help you figure it out. If that doesn't work, reach out to\nthe community.\n\n# Part 2: Pytorch, BitsAndBytes, Flash Attention, DeepSpeed, Axolotl\n\n## Conda\n\n    \n    \n    mkdir -p ~/miniconda3 wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh bash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3 rm -rf ~/miniconda3/miniconda.sh ~/miniconda3/bin/conda init bash\n\nExit your shell and enter it again.\n\n    \n    \n    conda create -n axolotl python=3.12 conda activate axolotl\n\n## Pytorch\n\nI tried the official install command from pytorch's website, and it didn't\nwork for me.\n\nHere is what did work:\n\n    \n    \n    pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/rocm6.0 python -c \"import torch; print(torch.version.hip)\"\n\nThis tests both Torch, and Torch's ability to interface with HIP. If it\nworked, it will print HIP version. Otherwise, it will print None.\n\n## BitsAndBytes\n\nBitsAndBytes is by Tim Dettmers, an absolute hero among men. It lets us\nfinetune in 4-bits. It gives us qLoRA. It brings AI to the masses.\n\nThere is a fork of BitsAndBytes that supports ROCm. This is provided not by\nTim Dettmers, and not by AMD, but by a vigilante superhero, Arlo-Phoenix.\n\nIn appreciation, here is a portrait ChatGPT made for Arlo-Phoenix, vigilante\nsuperhero. I hope you like it, if you see this Arlo-Phoenix. <3\n\n    \n    \n    git clone https://github.com/arlo-phoenix/bitsandbytes-rocm-5.6 cd bitsandbytes-rocm-5.6 git checkout rocm ROCM_TARGET=gfx90a make hip # use the ROCM_TARGET for your GPU pip install .\n\n## Flash Attention\n\nThis fork is maintained by AMD\n\n    \n    \n    git clone --recursive https://github.com/ROCmSoftwarePlatform/flash-attention.git cd flash-attention export GPU_ARCHS=\"gfx90a\" # use the GPU_ARCHS for your GPU pip install packaging pip install ninja pip install .\n\n## DeepSpeed\n\nMicrosoft included AMD support in DeepSpeed proper, but there's still some\nundocumented fussiness to get it working, and there is a bug I found with\nDeepSpeed, I had to modify it to get it to work.\n\n    \n    \n    git clone https://github.com/microsoft/DeepSpeed cd DeepSpeed git checkout v0.14.0 # but check the tags for newer version\n\nNow, you gotta modify this file: vim op_builder/builder.py\n\nReplace the function assert_no_cuda_mismatch with this: (unless they fixed it\nyet)\n\n    \n    \n    def assert_no_cuda_mismatch(name=\"\"): cuda_available = torch.cuda.is_available() if not cuda_available and not torch.version.hip: # Print a warning message indicating no CUDA or ROCm support print(f\"Warning: {name} requires CUDA or ROCm support, but neither is available.\") return False else: # Check CUDA version if available if cuda_available: cuda_major, cuda_minor = installed_cuda_version(name) sys_cuda_version = f'{cuda_major}.{cuda_minor}' torch_cuda_version = torch.version.cuda if torch_cuda_version is not None: torch_cuda_version = \".\".join(torch_cuda_version.split('.')[:2]) if sys_cuda_version != torch_cuda_version: if (cuda_major in cuda_minor_mismatch_ok and sys_cuda_version in cuda_minor_mismatch_ok[cuda_major] and torch_cuda_version in cuda_minor_mismatch_ok[cuda_major]): print(f\"Installed CUDA version {sys_cuda_version} does not match the \" f\"version torch was compiled with {torch.version.cuda} \" \"but since the APIs are compatible, accepting this combination\") return True elif os.getenv(\"DS_SKIP_CUDA_CHECK\", \"0\") == \"1\": print( f\"{WARNING} DeepSpeed Op Builder: Installed CUDA version {sys_cuda_version} does not match the \" f\"version torch was compiled with {torch.version.cuda}.\" \"Detected `DS_SKIP_CUDA_CHECK=1`: Allowing this combination of CUDA, but it may result in unexpected behavior.\" ) return True raise CUDAMismatchException( f\">- DeepSpeed Op Builder: Installed CUDA version {sys_cuda_version} does not match the \" f\"version torch was compiled with {torch.version.cuda}, unable to compile \" \"cuda/cpp extensions without a matching cuda version.\") else: print(f\"Warning: {name} requires CUDA support, but torch.version.cuda is None.\") return False return True\n    \n    \n    pip install -r requirements/requirements.txt HIP_PLATFORM=\"amd\" DS_BUILD_CPU_ADAM=1 TORCH_HIP_ARCH_LIST=\"gfx90a\" python setup.py install\n\n## Axolotl\n\nInstalling Axolotl might overwrite BitsAndBytes, DeepSpeed, and PyTorch. Be\nprepared for things to break, they do often.\n\nYour choice is either modify the setup.py and requirements.txt (if you are\nconfident to change those things) or pay attention to what libraries get\ndeleted and reinstalled, and just delete them again and reinstall the correct\nROCm version that you installed earlier. If Axolotl complains about incorrect\nversions - just ignore it, you know better than Axolotl.\n\nRight now, Axolotl's Flash Attention implementation has a hard dependency on\nXformers for its SwiGLU implementation, and Xformers doesn't work with ROCm,\nyou can't even install it. So, we are gonna have to hack axolotl to remove\nthat dependency.\n\n    \n    \n    https://github.com/OpenAccess-AI-Collective/axolotl.git cd axolotl\n\nfrom requirements.txt remove xformers==0.0.22\n\nfrom setup.py make this change (remove any mention of xformers)\n\n    \n    \n    $ git diff setup.py diff --git a/setup.py b/setup.py index 40dd0a6..235f1d0 100644 --- a/setup.py +++ b/setup.py @@ -30,7 +30,7 @@ def parse_requirements(): try: if \"Darwin\" in platform.system(): - _install_requires.pop(_install_requires.index(\"xformers==0.0.22\")) + print(\"hi\") else: torch_version = version(\"torch\") _install_requires.append(f\"torch=={torch_version}\") @@ -45,9 +45,6 @@ def parse_requirements(): else: raise ValueError(\"Invalid version format\") - if (major, minor) >= (2, 1): - _install_requires.pop(_install_requires.index(\"xformers==0.0.22\")) - _install_requires.append(\"xformers>=0.0.23\") except PackageNotFoundError: pass\n\nAnd then in src/axolotl/monkeypatch/llama_attn_hijack_flash.py make this\nchange:\n\n    \n    \n    --- a/src/axolotl/monkeypatch/llama_attn_hijack_flash.py +++ b/src/axolotl/monkeypatch/llama_attn_hijack_flash.py @@ -22,7 +22,9 @@ from transformers.models.llama.modeling_llama import ( apply_rotary_pos_emb, repeat_kv, ) -from xformers.ops import SwiGLU +class SwiGLU: + def __init__(): + print(\"hi\") from axolotl.monkeypatch.utils import get_cu_seqlens_from_pos_ids, set_module_name @@ -45,15 +47,7 @@ LOG = logging.getLogger(\"axolotl\") def is_xformers_swiglu_available() -> bool: - from xformers.ops.common import get_xformers_operator - - try: - get_xformers_operator(\"swiglu_packedw\")() - return True - except RuntimeError as exc: - if \"No such operator xformers::swiglu_packedw \" in str(exc): - return False - return True + return False\n\nNow you can install axolotl\n\n    \n    \n    pip install -e . accelerate launch -m axolotl.cli.train examples/openllama-3b/lora.yml\n\nWelcome to finetuning on ROCm!\n\n## Subscribe to my newsletter\n\nRead articles from Cognitive Computations directly inside your inbox.\nSubscribe to the newsletter, and don't miss out.\n\nAIArtificial Intelligencefinetuningamd\n\n### Written by\n\n# Eric Hartford\n\nI make AI models like Dolphin and Samantha https://ko-fi.com/erichartford BTC\n3ENBV6zdwyqieAXzZP2i3EjeZtVwEmAuo4 ETH\n0xcac74542A7fF51E2fb03229A5d9D0717cB6d70C9\n\nI make AI models like Dolphin and Samantha https://ko-fi.com/erichartford BTC\n3ENBV6zdwyqieAXzZP2i3EjeZtVwEmAuo4 ETH\n0xcac74542A7fF51E2fb03229A5d9D0717cB6d70C9\n\nShare this\n\n### More articles\n\nEric Hartford\n\n# dolphin-mixtral-8x7b\n\nhttps://huggingface.co/cognitivecomputations/dolphin-2.6-mixtral-8x7b Please\nNote, that this model i...\n\nEric Hartford\n\n# Built with Dolphin\n\nI started to understand that a lot of people are using and enjoying Dolphin -\nso I decided to put a ...\n\nEric Hartford\n\n# Running Dolphin Locally with Ollama\n\nWanna chat with Dolphin locally? (no internet connection needed) Here is the\neasy way - Ollama. in...\n\n\u00a92024 Cognitive Computations\n\nArchive\u00b7Privacy policy\u00b7Terms\n\nWrite on Hashnode\n\nPowered by Hashnode - Home for tech writers and readers\n\n", "frontpage": false}
