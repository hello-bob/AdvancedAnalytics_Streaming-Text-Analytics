{"aid": "40097317", "title": "MeshLRM: Large Reconstruction Model for High-Quality Meshes", "url": "https://sarahweiii.github.io/meshlrm/", "domain": "sarahweiii.github.io", "votes": 1, "user": "GaggiX", "posted_at": "2024-04-20 13:43:50", "comments": 0, "source_title": "MeshLRM", "source_text": "MeshLRM\n\n# MeshLRM: Large Reconstruction Model for High-Quality Meshes\n\nXinyue Wei*^1, Kai Zhang*^2, Sai Bi^2, Hao Tan^2, Fujun Luan^2, Valentin\nDeschaintre^2, Kalyan Sunkavalli^2, Hao Su^1, Zexiang Xu^2\n\n^1UC San Diego ^2Adobe Research * denotes equal contribution\n\narXiv\n\n## Abstract\n\nWe propose MeshLRM, a novel LRM-based approach that can reconstruct a high-\nquality mesh from merely four input images in less than one second. Different\nfrom previous large reconstruction models (LRMs) that focus on NeRF-based\nreconstruction, MeshLRM incorporates differentiable mesh extraction and\nrendering within the LRM framework. This allows for end-to-end mesh\nreconstruction by fine-tuning a pre-trained NeRF LRM with mesh rendering.\nMoreover, we improve the LRM architecture by simplifying several complex\ndesigns in previous LRMs. MeshLRM's NeRF initialization is sequentially\ntrained with low- and high-resolution images; this new LRM training strategy\nenables significantly faster convergence and thereby leads to better quality\nwith less compute. Our approach achieves state-of-the-art mesh reconstruction\nfrom sparse-view inputs and also allows for many downstream applications,\nincluding text-to-3D and single-image-to-3D generation.\n\nPipeline: The model architecture of MeshLRM. The images are first patchified\nto tokens. The transformer takes the concatenated image and triplane tokens as\ninput. The output triplane tokens are upsampled with the unpatchifying\noperator while the output image tokens are dropped (not drawn in figure). With\ntwo tiny MLPs for density and color decoding, this model supports both the\nvolumetric rendering and the DiffMC fine-tuning. We render our final mesh\noutput on the rightmost of the figure.\n\n## GSO dataset\n\nInteractive Viewer | Download Mesh\n\nInteractive Viewer | Download Mesh\n\nInteractive Viewer | Download Mesh\n\nInteractive Viewer | Download Mesh\n\nInteractive Viewer | Download Mesh\n\nInteractive Viewer | Download Mesh\n\n## ABO dataset\n\nInteractive Viewer | Download Mesh\n\nInteractive Viewer | Download Mesh\n\nInteractive Viewer | Download Mesh\n\nInteractive Viewer | Download Mesh\n\nInteractive Viewer | Download Mesh\n\nInteractive Viewer | Download Mesh\n\n## OpenIllumination dataset\n\nInteractive Viewer | Download Mesh\n\nInteractive Viewer | Download Mesh\n\nInteractive Viewer | Download Mesh\n\n## Text-to-3D\n\nNote: We use Instant3D for text-to-multiview generation.\n\nInteractive Viewer | Download Mesh\n\nInteractive Viewer | Download Mesh\n\nInteractive Viewer | Download Mesh\n\n## Image-to-3D\n\nNote: We use Zero123++ for image-to-multiview generation.\n\nInteractive Viewer | Download Mesh\n\nInteractive Viewer | Download Mesh\n\nInteractive Viewer | Download Mesh\n\nWe thank the Nerfies team for providing this amazing website template.\n\n", "frontpage": false}
