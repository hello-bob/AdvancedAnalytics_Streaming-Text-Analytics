{"aid": "40196694", "title": "What Computers Cannot Do: The Consequences of Turing-Completeness", "url": "https://gavinhoward.com/2024/03/what-computers-cannot-do-the-consequences-of-turing-completeness/", "domain": "gavinhoward.com", "votes": 1, "user": "lemper", "posted_at": "2024-04-29 10:44:14", "comments": 0, "source_title": "What Computers Cannot Do: The Consequences of Turing-Completeness | Gavin D. Howard", "source_text": "What Computers Cannot Do: The Consequences of Turing-Completeness | Gavin D. Howard\n\n# About\n\n# Contact\n\n# Archive\n\n# Categories\n\n# Tags\n\n# Subscribe\n\n# What Computers Cannot Do: The Consequences of Turing-Completeness\n\n26 Mar 2024\n\nYzena\n\n|\n\nTech: Programming\n\n  * Alan Turing\n  * Computability\n  * Decidability\n  * Entscheidungsproblem\n  * Halting Problem\n  * Programming\n  * Rig\n  * Tech\n  * Turing Machines\n  * Universal Turing Machines\n  * Yao\n  * Zig\n\nToggle Warnings Toggle Notes\n\nPlease see the disclaimer.\n\nAssumed Audience: Programmers. All programmers.\n\nDiscuss on Hacker News and Reddit.\n\nEpistemic Status: Confident with few doubts. Minor mistakes may exist, though.\nCorrections welcome.\n\nIf you want to skip to the actual material, use this link.\n\nThis post is meant to be informative, but it does have an ad at the end, which\nwill be clearly marked. You have been warned!\n\n## Introduction\n\nI used to think that my Bachelor\u2019s Degree in Computer Science was useless,\nthat I didn\u2019t learn much that would help me in programming.\n\nI was wrong.\n\nThere was at least one class that was well worth it: Theory of Computation.\n\nI also consider a few others worth it:\n\n  * The class about linear algebra.\n  * The class about discrete math.\n  * The class about different kinds of algorithms.\n  * The classes on computer graphics.\n  * The class on cryptography. I wish they had taught us more about elliptic curves.\n  * A class I audited about signals processing. I wish I had had the time to take this class for real.\n\nI have used material from all of these classes. I encourage programmers to\nstudy these if they haven\u2019t.\n\nWhen I showed up to the Theory of Computation class, I was solidly unhappy. I\nwas certain that this would be a worthless class; I held the opinion that\ntheory was practically meaningless.\n\nAnd then I show up to class and found out that it was really an applied math\nclass; there was little theory and a lot of proofs.\n\nI struggled. I hated it.\n\nBut I came out of that class with two things: a problem that annoyed me and an\nunderstanding of what computers cannot do.\n\nThe problem that annoyed me is about the soft limits of what computers can do;\nit turns out that there are some algorithms that can be quick most of the\ntime, but can also be impossibly slow in certain cases.\n\nAnd as far as we know, those algorithms are optimal.\n\nIt makes sense that there are some things that are so expensive to compute\nthat they are not worth it.\n\nI had always assumed there were, but it annoyed me that we didn\u2019t even know if\nthat limit existed.\n\nSo computers may have soft limits.\n\nSurely, they won\u2019t have hard limits?\n\nSurprisingly, we know the answer to this question: they do!\n\nIn other words, there are some problems computers cannot solve, even if given\ninfinite time and space.\n\nWhen I attended that class, this blew me away. I had never considered that\nthere were things computers could never do.\n\nI still had the mainstream belief that AI would be just as capable as humans.\n\nWhy I now believe otherwise, and how people can measure AI \u201cintelligence,\u201d is\na subject for another post.\n\nWell, time passed, and I graduated. While I knew those hard limits were\nimportant, I didn\u2019t think I would run into them.\n\nI was wrong again; I run into them all the time!\n\nBeyond that, I have run into a slew of programmers, on Hacker News and other\nforums, that seem to not understand these hard limits and what they mean.\n\nAn example is this exchange, started by a user asking me to explain why I\ndon\u2019t like Zig.\n\nSo because I believe that these hard limits are essential to our programming,\nhere is a free class on the most important programming knowledge that\nprogrammers don\u2019t understand.\n\nI may not use completely mainstream terminology in order to make this more\naccessible. I may also skip a lot of formality and some nuance.\n\n## The Entscheidungsproblem\n\nWe start by going back to the 1930\u2019s. At King\u2019s College, Cambridge, there is a\nexceptionally bright student named Alan Turing working on his Master\u2019s Course\nand writing a paper.\n\nUnknown to everyone at the time, this brilliant man would someday save the\nBritish Empire, and maybe Western Civilization.\n\nAlso unknown to everyone, this paper would be the most important scientific\nresult of the 20th century!\n\nAnd that is despite the fact that Turing was beat to the punch!\n\nThe man who beat him was Alonzo Church, and their results were proven\nequivalent.\n\nSoon after finishing his paper, but before it was published, Turing went to\nPrinceton to study under his rival.\n\nWhat a marvelous partnership!\n\nThey used very different models. Church used something he called the \u201clamda\ncalculus,\u201d and Turing used hypothetical machines that came to be called\n\u201cTuring Machines.\u201d\n\nBut what problem were they tackling?\n\nIt\u2019s called the Entscheidungsproblem, a name that, to this day, I can\u2019t\nremember how to spell or say.\n\nIt means \u201cdecision problem\u201d according to Wikipedia, and that agrees with what\nI know.\n\nAs far as I know, this is the same \u201cdecision\u201d in the \u201cdecision\u201d problems that\nmake up the P and NP complexity classes.\n\nThis is also why \u201ccomputability\u201d is called \u201cdecidability\u201d. I just use\n\u201ccomputable\u201d and \u201ccomputability\u201d for accessibility; all of the literature uses\n\u201cdecidable\u201d and \u201cdecidability.\u201d\n\nThis \u201cdecision problem\u201d was defined as giving a (mathematical) statement to an\nalgorithm and to have that algorithm return a decision about whether that\nstatement was true or not.\n\nThis was a big deal at the time; mathematicians would have loved to prove\nthings automatically.\n\nAnd then along came Alonzo Church and Alan Turing and proved that solving the\nEntscheidungsproblem is impossible.\n\nWell, there goes that dream.\n\nBut if Alonzo Church proved it first, why is Alan Turing the central character\nof this story?\n\nBecause there is one aspect of Turing\u2019s model that makes it more useful:\nUniversal Turing Machines (UTM\u2019s).\n\n## Universal Turing Machines\n\nThe idea of UTM\u2019s is that you can have Turing Machines running other Turing\nMachines.\n\nThis idea is so basic to computer science and the world in general that this\nis the biggest reason I say that Turing\u2019s result, while later than Church\u2019s,\nis still more important!\n\nHow important is it?\n\nWithout this concept, would we understand the idea of interpreters?\n\nYou run interpreters every day, even if you don\u2019t know it. Python is an\ninterpreter. Your shell is an interpreter. Your text editor is running\ninterpreters. Your web browser runs an interpreter.\n\nIt goes even further!\n\nIt means that Turing Machines can take other Turing Machines as input and do\nstuff to them. So even compilers depend on this concept!\n\nAnd it goes even deeper!\n\nSay you are running a Python program that is implementing a DSL. How many\nlayers of compilers and interpreters are you running?\n\n  1. Your Python program is an interpreter of the DSL.\n  2. Python itself is an interpreter of your Python program.\n  3. Python is implemented in C, which is implemented by a C compiler.\n  4. The Python C program has been translated to machine code, which is implemented by microcode in your hardware which acts as a compiler to translate the machine code into something even closer to the hardware.\n  5. The microcode interpreter is implemented by actual hardware, which acts as an interpreter of the translated machine code.\n\nYep. It\u2019s that central; it goes all the way down to hardware.\n\nImagine how many levels deep you can get running your Python program in the\nweb browser on something like repl.it!\n\n## The Halting Problem\n\nSo what did Alan Turing prove, exactly?\n\nI\u2019m going to let Mark Jago and Computerphile explain that with a wonderful\nanimation.\n\n## Generalizing\n\nThat may have been all Turing proved, but people absolutely built on the\nshoulders of that giant and proved stronger results.\n\nThis was \u201ceasy\u201d because the other major thing about UTM\u2019s is how well they\ngeneralize to proving that just about any property of an algorithm is not\ncomputable, in the general case.\n\nYou want to know that a function is never called? Nope.\n\nYou want to know that a line of code is never executed? Sorry.\n\nYou want to ensure a red function is never passed to a blue function? Futile.\n\nYou want to check if a program is a virus? Fruitless.\n\nYep, virus scanners are almost completely useless. They can only detect known\nviruses, and only if those viruses are not changed.\n\nYou want to ensure a certain kind of bug never exists in your program?\nImpossible.\n\nThis is crucial to emphasize: The true importance of Turing-completeness is\nthat there is at least one property that cannot be proven!\n\nYou may be able to prove some properties of a program, but if you cannot prove\nthem all, that is the fault of Turing-completeness. And sometimes, you may be\nable to prove a property you care about, but if you cannot prove it every\ntime, that is the fault of Turing-completeness.\n\n\u201cBut Gavin, we have safe languages! They ensure that memory bugs never exist,\nright?\u201d\n\nNo, they just crash if one exists. They are using runtime checks to stop a\nmemory bug from happening. The bug still existed.\n\nMore on the power of runtime checks later.\n\nThis generalization is pervasive and pernicious. Without assumptions (that are\noften wrong), proving the correctness of programs is hopeless.\n\nYou can prove certain properties if you make assumptions, like the seL4\nproject did, but if any one of those assumptions is false, your entire proof\nis garbage.\n\nOver time, people began to realize the importance of what Church and Turing\nhad proved and just how pervasive the concept would be.\n\nSo they gave it a name in honor of the man that fought and won a war with just\nnumbers and his piercing intellect.\n\nThey called it \u201cTuring-complete.\u201d\n\n## Turing-Completeness\n\nTo be Turing-complete, a thing needs to be able to accomplish what any other\nTuring-complete thing can do.\n\nYes, that definition is recursive, but we can adjust it.\n\nUniversal Turing Machines are Turing-complete. Church\u2019s lamda calculus is\nTuring-complete.\n\nSo we can change the definition to this:\n\nTuring-complete\n\n    \n\nAn adjective that applies to a thing that can do anything a Universal Turing\nMachine can do.\n\n### Requirements\n\nBut that brings up a question: what exactly can a UTM do?\n\nLet\u2019s first describe what a UTM is.\n\nA UTM has four things: a set of states and a way to transition between them\n(the program), an unbounded tape with distinct slots (memory), something to\nread and write symbols in a slot on the tape (reading and writing memory), and\nsomething to move the tape.\n\nOkay, cool. What it is also tells us what it can do: it can read and write\nstuff on a tape and move that tape.\n\nBut there\u2019s a missing detail: how can it move the tape?\n\nMost people would assume that it can move the tape both backwards and\nforwards, and they are correct.\n\nIt turns out that moving the tape both directions is important! Without it, a\nUTM would not be Turing-complete!\n\nUnderstanding this was a mind-blowing moment for me in class.\n\nAnd it goes even further: the machine must be able to move the tape backwards\nand forwards without any restriction. If it can only move the tape 5 spots\nbackwards before it must move the tape at least 5 spots forward, it is no\nlonger Turing-complete.\n\nConversely, this means that Turing-completeness only needs four simple things:\n\n  * Memory;\n  * An ability to read anywhere in memory;\n  * An ability to write anywhere in memory;\n  * A program that can leverage those abilities.\n\n\u201cThat\u2019s nice, Gavin, but what does that mean in practice?\u201d\n\nIn practice, assuming that reading and writing memory is possible, then if\nyour compiler or interpreter has, or can implement, only two keywords, then it\nis Turing-complete.\n\nWhat are these two keywords?\n\nIn programming languages, they are usually if and while.\n\nYep. That\u2019s it.\n\nif gives you the ability to move forwards any amount.\n\nwhile gives you the ability to move backwards any amount, and unlike\nrestricted forms of for, it doesn\u2019t prevent you from moving backwards after a\ncertain amount of moves.\n\nWhat I mean by \u201crestricted forms of for\u201d is for loops that only iterate over a\nfixed list or iterator, such as a foreach.\n\nC\u2019s for loop, because you can change the loop index, and/or use a condition\nthat has nothing to do with the loop index, is unrestricted, and therefore, is\nequivalent to a while loop with extra steps.\n\nFor me, this was astonishing. My teacher kept talking about how powerful\nTuring-completeness was; I expected that it would be difficult to implement\nit.\n\nNope. It just takes if and while. It\u2019s so easy that people accidentally make\nthings Turing-complete all the time.\n\nI have an acquaintance. He made a calculator language for himself and tried to\nrestrict it so that it wasn\u2019t Turing-complete.\n\nI asked him if it had if statements and while loops. He said yes, and I told\nhim that it was Turing-complete.\n\nHis reply was classic. \u201chorsefeathers. i accidentally a language\u201d\n\n### Power\n\nOf course, it may seem that Turing-completeness should be shunned if it makes\nthings impossible to prove.\n\nUnfortunately, there are algorithms we want to run for which Turing-\ncompleteness is required. Even worse, it turns out that applies to most\ninteresting algorithms.\n\nThis is the Faustian Bargain of Turing-completeness: you need it, even if you\ndon\u2019t want it.\n\nDon\u2019t believe me?\n\nWell, the astute among you noticed that I have mentioned \u201cpower\u201d twice and\nhave talked about what machines can do.\n\nSurely there is a power spectrum!\n\nYes, there is. And Turing-completeness is on that spectrum.\n\nAnd at each level are two things: a type of (hypothetical) machine, and a type\nof language.\n\nObviously, Turing machines and Turing-complete languages are in the Turing-\ncomplete level.\n\nIf Turing-completeness is on that power spectrum, then surely there are things\nthat are less powerful and more powerful, right?\n\nYes and no. Turing-completeness is as powerful as we can implement.\n\nSure, there are general recursive functions, as opposed to primitive recursive\nfunctions, but it turns out that they correspond directly to Turing-\ncompleteness and the next power level down.\n\nSo what is at that next power level down, the one with primitive recursive\nfunctions?\n\nThe machines that implement that power level are called pushdown automata, and\nthe languages are context-free languages.\n\nIf the machine is a deterministic pushdown automaton, it only needs a\ndeterministic context-free language, but it turns out that the non-\ndeterministic versions have more power than the deterministic ones.\n\nEssentially, pushdown automata have a stack, and they can use the stack.\n\nCrucially, though, they can only use the top of the stack.\n\nThe next level down has finite-state machines and regular languages.\n\nThese have a fixed set of states, and their memory consists solely of the\nstates, their relationships, and the current state.\n\nIt turns out that there are deterministic and non-deterministic versions of\nfinite-state machines as well, but they are actually equivalent.\n\nAnd then, at the bottom, you have combinational logic. It is mostly\nuninteresting.\n\n## Consequences of Power\n\nAnd why is combinational logic uninteresting? Because the same input always\nresults in the same output.\n\nThere\u2019s not much power in that, nor many consequences; you can design your\ncombinational logic to always return the desired results.\n\nBored!\n\nFinite-state machines can do more stuff. They are useful for things like\nvending machines, turnstiles, traffic lights, elevators, and anything that is\nonly in one fixed state at a time.\n\nIf a programmer is smart, he will turn his code into a finite-state machine\nwhenever he can because finite-state machines can be proven correct.\n\nPushdown automata can compute, well, context-free languages.\n\nContext-free languages are useful in things like file formats; if a format is\neasily computed/processed, then it will likely have less bugs.\n\nPushdown automata can be tricky to prove correct, but as far as I know, it can\nbe done.\n\nAnd then there\u2019s Turing machines.\n\n### Pervasiveness\n\nAs already mentioned, Turing-completeness is pervasive, so pervasive that it\nhappens by accident.\n\nThis is no accident!\n\nI said above that most interesting algorithms need Turing-completeness. Here\nare some examples:\n\n  * Languages that are not context-free. Yep! Some languages need full power to parse!\n  * Schedules. Any kind of scheduling, really.\n  * Google Maps and how it finds the way to your destination.\n  * Facebook and how it maps relationships between people.\n  * Recommendation algorithms.\n  * Compression of data. If you\u2019ve ever watched video or listened to music, you have used this.\n  * Newton\u2019s method, useful for computing square roots, any kind of root, and lots of other nice things.\n  * Gradient descents. You like AI? At its heart is gradient descent.\n  * Internet search. Google, Bing, etc.\n  * Image rendering. This happens in every movie with CGI.\n  * Physics simulation. This also happens in every movie with CGI.\n  * Any algorithm to make sure computers agree.\n  * Laying out text on a sheet of paper, like in your word processor.\n  * Processing changes to a spreadsheet.\n  * Video games. Every video game literally has a while loop at the top. This loop renders every frame, simulates all physics per frame, emits sound, takes your input, everything.\n  * Operating systems. Every operating system has a while loop at the top. This loop continuously dispatches processes, gives out resources, and takes them back, until the user shuts down.\n\nYou get the idea; Turing-completeness is necessary for anything interesting\nenough to not have fixed processing sizes.\n\nAnd as we will see, fixed processing sizes can sometimes be effectively\nTuring-complete.\n\n### Static vs. Dynamic\n\nNote that I said fixed processing sizes; if you have a fixed size, you\u2019re\noften okay.\n\nThat is an example of something that is static.\n\nFor example, say you limit files for your newest app to 1 MB. Is that enough\nto work around Turing-completeness?\n\nIt might be. As long as you do a fixed amount of processing per unit of data,\nand that fixed amount multiplied by the amount of data does not take too long.\n\nBut here\u2019s the kicker: anything that varies at runtime cannot be statically\ndetermined, in general. That\u2019s why you need to have a fixed amount of\nprocessing. If one type of input causes your runtime to explode, you just ran\ninto a Turing bomb.\n\nAnd in general, things can can be completely statically determined are not as\npowerful as Turing-complete things.\n\nSay you statically determined that your physics simulator will have a fixed\nprocessing time for every frame. Say you statically determined that it will\nonly ever run for a fixed set of frames.\n\nWould your physics simulator be as powerful as one that could run however long\nthe user wanted? Of course not.\n\nThis is why one of the most reliable pieces of software in the world, SQLite,\ndoes not use static analysis.\n\nYup! In this era when Rust and its static analysis is all the rage, SQLite\neschews those techniques. In fact, they say,\n\n> More bugs have been introduced into SQLite while trying to get it to compile\n> without warnings than have been found by static analysis.\n\nIn other words, static analysis has been a net negative for SQLite.\n\nWhy is this?\n\nI think one reason is that SQLite runs on different platforms, and a lot of\nthem.\n\nAs said in that link, \u201cformal verification requires a formal specification,\u201d\nand building a formal specification for SQLite would be far too much work\nsimply because all of the platforms are different.\n\n### Comptime vs. Runtime\n\nThat said, SQLite, and most reliable software I know about, love fuzzing.\n\nWhy do they love it? Because fuzzing is to runtime what static analysis is to\ncompile time.\n\nSee, if you have something that\u2019s Turing-complete, the only way to tell what\nit will do on certain input is to run it on that input!\n\nThat\u2019s essentially what fuzzing is: finding as many \u201cinteresting\u201d inputs as\npossible and seeing how the software acts with those inputs.\n\nAnd even then, it might never halt; the Halting Problem bites again. This is\nwhy fuzzers use timeouts.\n\nIn fact, there is a company pushing what they call \u201cinvariant development as a\nservice.\u201d\n\nTheir technique? Fuzzing. And they explicitly chose it over formal\nverification, which is \u00fcber static analysis.\n\nAnd they used fuzzers they wrote to easily find bugs in programs that were\nfound first by formal verification.\n\nSomeone even offered more context:\n\n> This article is target at proving programs that run on blockchain based\n> virtual machines. These are deterministic to their core. This is quite\n> different the envirnoments that than most programs run in. For example every\n> time code on the EVM runs in production, it is being runn [sic] in parallel\n> on hundreds to hundreds of thousands of systems, and the outputs/storage\n> writes/events all have to agree exactly after execution, or bad things\n> things happen. The EVM is also single threaded, reasonably well defined, and\n> with multiple different VM implementations in many different languages. So\n> programs here are much easier to verify.\n>\n> In addition, program execution costs are paid per opcode executed, so\n> program sizes range from hundreds of lines to about thirty thousand lines\n> (with the high side being considered excessive and insane). It\u2019s again quite\n> different than the average execution environment.\n>\n> \u2013 danielvf on Hacker News\n\nIn short, this is the perfect environment for formal verification, with small\nprograms and fully deterministic behavior, and a hard requirement to get\nthings right the first time, and fuzzers still beat formal verification.\n\nOr would have, if they had been used.\n\n\u201cBut isn\u2019t the number of possible inputs for a program so large that fuzzing\nwould be mostly useless?\u201d\n\nAh, good question.\n\nThe answer is yes and no.\n\nYes, fuzzing can be useless in the naive case.\n\nBut in the hands of smart and experienced people like the SQLite authors,\nfuzzing can be guided to yield more useful inputs.\n\nAnd in the experience of at least one person, specifications needed for formal\nverification were far more complex than the code itself.\n\nThis holds with seL4, a formally proven operating system, which had about 10k\nLoC of C and around 200k LoC of proof.\n\nAnother way to make fuzzing better is asserts, which are notoriously useful\nbefore code is tested, especially while fuzzing.\n\n\u201cNotorious, Gavin?\u201d\n\nAbsolutely. You will hate them because they will cause so many crashes. Or\nrather, a fuzzer will deliberately try to trip them and succeed.\n\nBut this is a good thing! It means:\n\n  * Bugs are found closer to the actual source.\n  * Bugs are more easily found because fuzzers detect crashes more easily than bugs that don\u2019t cause crashes.\n  * If you build software with assertions, then even if there are bugs, a lot of those bugs do not turn into bigger problems.\n  * And finally, asserts constrain behavior in a way that culls the state space exponentially.\n\n### \u201cInfinite\u201d State\n\nI want to talk about that last point.\n\nHave you wondered how much your programs can do? Or rather, how many states it\ncan be in while doing its work?\n\nIf you haven\u2019t, it\u2019s time to level up, so here\u2019s a speedrun masterclass.\n\nEvery bit your program has in memory contributes to its state. Some contribute\nmore than others. (For example, a bit that controls a branch contributes more\nthan a plain old bit used for data.)\n\nEvery bit, or set of bits, that contributes a whole independent bit of state,\nmeaning that it can control something, we\u2019ll call control bits.\n\nYour program has 2^N states, where N is the number of control bits it has.\n\nYes, I\u2019m handwaving cases where programs have more or less bits depending on\ninput. Yes, sometimes bits may not be independent of each other.\n\nWork with me here.\n\nHow big does N have to be before you can never hit every possible state?\n\nFirst, let\u2019s define never. You may think never means \u201cinfinite time and\nspace.\u201d Let\u2019s shrink that ever so slightly to \u201cthe time and space in this\nuniverse.\u201d\n\nIn other words, we can assume that we can use all of the energy in the\nuniverse, and all of the time, and all of the space. But we do not have an\ninfinite supply.\n\nOkay, with that out of the way, how far could we go with our universe? How big\ncould N be before it\u2019s impossible to hit every state?\n\nYou ready for this? The answer is a mere 300.\n\nThat\u2019s it. If you have 300 control bits in your program, you cannot reach\nevery possible state.\n\nDon\u2019t believe me? Take it from a physicist:\n\n> The universe could currently register \u2248 10^90 bits. To register this amount\n> of information requires every degree of freedom of every particle in the\n> universe.\n>\n> \u2013 Dr. Seth Lloyd\n\nIn other words, you\u2019d have to use the whole universe to be able to reach 10^90\nstates. And converting from 10^90 to a power of 2 yields about 2^299, so 300\nbits.\n\nIn other words, you don\u2019t need a program that deals with an infinite amount of\ndata; you just need enough program to have 300 control bits.\n\nAnd if we were to be more realistic and restrict ourselves to the resources we\nhave on Earth, it\u2019s even less.\n\nThis is actually the entire deal behind cryptography: good cryptography is\ndesigned such that every single bit of a key is a control bit. But 300 bits is\n\u201cToo Much Crypto,\u201d so cryptographers \u201csettled\u201d on 256 because we only have\nEarth; we don\u2019t need to worry about the entire universe!\n\nYou remember how I said I use asserts? I use them because they help me remove\nunnecessary control bits, which I define as the bits that create accidental\ncomplexity.\n\n### Mathematical vs. Practical\n\nNow, the mathematicians in the audience might be like:\n\nHere is why I am wrong in their eyes: because the definition of a Turing\nmachine is one that is infinite. So as soon as I talk about finite stuff, it\nis not a Turing machine, and it is not Turing-complete.\n\nThey are technically correct. And if I were trying to prove a solution to the\nmost annoying match problem, I would use the mathematical definition of a\nTuring machine.\n\nBut when it comes to real programs running on real computers, I don\u2019t.\n\nThere is a difference between mathematical Turing-completeness and practical\nTuring-completeness, and for physical humans and machines, we should care\nabout practical Turing-completeness.\n\nWhy? Because the only thing we should care about actual software is how good\nit is, and it is only as good as the lack of bugs and other defects.\n\nSure, our machines may not be mathematically Turing-complete, but if we still\ncannot prove properties about those programs, they may as well be Turing-\ncomplete.\n\nThe rule of thumb that I use: if you cannot prove even one substantial\nproperty of the program, it is Turing-complete.\n\nAs an example, take Starlark and Meson.\n\nOne is a non-Turing-complete language for build systems (used by Bazel and\nBuck2), and the other is a build system with a bespoke non-Turing-complete\nlanguage.\n\nOstensibly, both are not Turing-complete, and that is sort of true.\n\nBut they both made one crucial mistake in their language design: loops can\nbreak out early with the break keyword.\n\nSure, they don\u2019t have a while loop, and their for loops are restricted to\nranges with upper limits.\n\nBut with break keywords, I can simulate a while loop.\n\nI wrote a program in Starlark that could either halt or not on a practical\nTuring machine:\n\n    \n    \n    chrs = [ \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"a\", \"b\", \"c\", \"d\", \"e\", \"f\" ] def hxn(n): n7 = chrs[n % 16] n6 = chrs[n // 16 % 16] n5 = chrs[n // 256 % 16] n4 = chrs[n // 4096 % 16] n3 = chrs[n // 65536 % 16] n2 = chrs[n // 1048576 % 16] n1 = chrs[n // 16777216 % 16] n0 = chrs[n // 268435456] return str(n0) + str(n1) + str(n2) + str(n3) + \\ str(n4) + str(n5) + str(n6) + str(n7) def test(stop): x = 4294967295 do_break = False for i in range(0, x): for j in range(0, x): for k in range(0, x): for l in range(0, x): for m in range(0, x): for n in range(0, x): for o in range(0, x): for p in range(0, x): num_str = hxn(i) + hxn(j) + \\ hxn(k) + hxn(l) + \\ hxn(m) + hxn(n) + \\ hxn(o) + hxn(p) print(num_str) if num_str == stop: do_break = True break if do_break: break if do_break: break if do_break: break if do_break: break if do_break: break if do_break: break if do_break: break return x\n\nYes, it\u2019s ugly. But it does run; you just need to add a call to test() at the\nbottom.\n\nNow, why is it ugly?\n\nBecause the Starlark designers did do one clever thing: they limited integers\nto 2^32-1. This means that in order to get enough iterations to span the life\nof the universe, I needed 8 nested loops at that limit.\n\nThis gives me 2^256 iterations, plenty of time to wait for death.\n\nAnd depending on the value of the string I pass in, I could make it go as many\niterations as I want. I could even make it impossible to stop by passing in\nthe string \"GO\".\n\nOr even \"STOP\", just to be cheeky.\n\nIn addition, while Starlark can prove that the code above doesn\u2019t iterate a\nfixed, although huge, number of times, it cannot prove the opposite.\n\nIn other words, it can prove the maximum number of iterations, but it cannot\nprove the minimum.\n\nThat looks like an unprovable property, which is exactly what you would have\nin a Turing-complete language.\n\nI could go further and implement math on 256-bit integers disguised as\nstrings, but that is just rubbing salt in the wounds of a dead horse.\n\nAnyway, what I have done in that twisted code is implement something that can\nact like a while loop on a practical Turing machine, and I could extend that\nloop to any size by just adding more nested loops and bigger strings.\n\nSo while the author of Meson doth protest, as well as a user of Starlark, and\nboth are technically correct, in this case, if users are nesting for loops to\nsimulate while loops to make a ray tracer, you have lost.\n\nIn essence, your language is still Turing-complete in practice.\n\nAt that point, just give your users a while loop for usability\u2019s sake.\n\n### Program Size\n\nBut if our physical machines don\u2019t have to be infinite to be effectively\nTuring-complete, how big a program has to be before it is large enough to also\nbe effectively Turing-complete, i.e., have at least 2^300 states?\n\nThat I don\u2019t know, because it depends on the program, but I think I have a\ngood starting point.\n\nI wrote an implementation of POSIX bc and Unix dc. It\u2019s more sophisticated\nthan other implementations, and most importantly, it has zero dependencies.\n\nAt least one person thinks bc is too small to compare against a server\nprogram, but I believe my bc can be compared to servers in one thing: like\nservers, it has more than 2^300 states.\n\nI think.\n\nAnd how big is bc? 14.3k LoC. That\u2019s not including comments or blank lines. Or\nheaders.\n\nbc is tiny compared to the software that companies produce, and it doesn\u2019t\neven touch the outside world except through stdin, stdout, stderr, and reading\nfiles on disk! Yep, not even writing files!\n\nMine will also touch the kernel random number generator.\n\nNevertheless, it is still big enough to have states that will never be\nreached.\n\nAnother example: Zig at comptime. They use a branch limiter to make sure\ncomptime halts.\n\nThis begs the question: how many branches are okay before the program could\nhave unreachable states?\n\nWell, it could be effectively infinite if there is only one branch and it\u2019s\nfor a limited foreach loop. That\u2019s only one control bit.\n\nBut it could be as low as 300 if all branches depend on different control\nbits!\n\nAnother example: last I heard, seL4 was around 10k LoC. They managed to prove\nit correct at that size, but it was an enormous effort. I bet it was, and is,\non the very edge of what humans can prove.\n\nAnd I bet it was because they culled control bits. You should too.\n\n### Sources of State\n\nSo where can control bits come from?\n\nUnfortunately, they can come from anywhere!\n\nMemory? Yep.\n\nFiles on disk? Sure.\n\nRandom number generator? Most definitely.\n\nStuff from the network? Oh, yeah.\n\nSharp rocks at the bottom? Most likely.\n\nProgrammers are bad at managing state, so the best programming systems try to\neliminate sources of state, or at least reduce the amount of state those\nsources can contribute.\n\nThis is what Rust does and why people like it so much.\n\nI wish our operating systems were designed this way; Rust and friends can only\ndo so much.\n\nBut that\u2019s a post for another day.\n\nAnd the worst ones? They add it. Deliberately.\n\nGood C and C++ programmers have the scars to prove it.\n\nWhy? Because the king of state explosion is Undefined Behavior (UB), and C and\nC++ have a lot of it.\n\nTo boil UB down, whenever system designers say, \u201c...the behavior is\nundefined,\u201d they are really saying, \u201cThe sources of state for the behavior in\nthis situation are as close infinite as possible.\u201d\n\nYep, that\u2019s right: UB makes state explode.\n\nThat is because the true behavior of stuff subject to UB depends on the state\nof some esoteric stuff that we don\u2019t usually consider:\n\n  * The timing and order of the scheduler.\n  * The contents of the caches, even for other programs!\n  * Timing of user inputs.\n  * In short, the entire state of the machine!\n\nAnd maybe the Internet!\n\nYep! In the presence of UB, you have to worry about everything else that the\ncomputer has been running!\n\nThis is why I hate UB and why I hate compiler developers who exploit UB.\n\n\u201cBut Gavin, aren\u2019t those compiler developers really reducing the state from\nUB?\u201d\n\nSometimes, but they are doing at the cost of always taking away from the\nprogrammer the tools they need to manage that state.\n\nSure, your program may have less states, but those states that are left are\nall evil, so while you have less state space, more of it is not just bad, but\nactively terrible!\n\n## Conclusion\n\nSo there you have it. That is why Turing-completeness is important to\nunderstand, what it is, and its consequences for our finite machines, which\nare technically not Turing-complete.\n\nI hope this was useful.\n\nAnd remember: the magic measure is 300 control bits; once you go beyond that,\nyou best consider your program effectively infinite.\n\nBelow is the ad mentioned at the top!\n\n## Rig\n\nI have spent more than 10 years learning about Turing-completeness, including\nbeing embarrassed live in front of an audience at a conference.\n\nThe guy was not trying to embarrass me; my own ignorance did.\n\nWith all of that knowledge, you might assume that I would try to avoid Turing-\ncompleteness wherever possible.\n\nAnd the answer is no, I don\u2019t.\n\nI even built a build system called Rig that has a Turing-complete language\nbacking it.\n\nThere is a reason it\u2019s Turing-complete; well, 300 reasons: control bits.\n\nThat is such a small number that, even though builds should not be big\nprograms, I have to assume that builds can have that many control bits.\n\nJust take it from users of build systems:\n\n> [A]ny sufficiently complex build system needs a Turing complete language for\n> configuration.\n>\n> \u2013 obsidian_golem on Reddit\n\n> Meson is quite good. It\u2019s biggest issue is it is not Turing complete so it\n> has certain limits of extensibility.\n>\n> \u2013 marcthe12 on Hacker News\n\nRig is meant to not have limits of extensibility, and it is meant to have a\nbetter user experience, so it will be Turing-complete and provide a while\nloop, among other things.\n\nThough Rig may not be able to parse while loops at release; it didn\u2019t need\nthem to bootstrap itself.\n\nBut that is not all; despite embracing Turing-completeness, I know there must\nbe a way to avoid it in the default case.\n\nSo there will be: Rig will be able to selectively turn off keywords, and by\ndefault, it will. So it will not be Turing-complete by default.\n\nThough not at the initial release.\n\nIn addition, because Rig will be intended to build code from outside sources,\nwhich may not be trustworthy, it will be able to check permissions at runtime\nusing capabilities.\n\nWhy runtime? The same reason fuzzing is better than formal verification:\nbecause running something is the only way to tell if it goes haywire.\n\nThis will go so far as to check which commands to run; if you are building a\nthird-party C project, you will be able to restrict it to only call the C\ncompiler of your choice. Or only connect to certain IP address/domains. Or not\nconnect to the Internet at all.\n\nYou will even be able to restrict it from touching disk, or only touching disk\nin certain directories.\n\nThat is the power of runtime: checks can be as coarse or as detailed as you\nwant, and as long as there aren\u2019t bugs in the check code (Turing-completeness\nstrikes again!), you are guaranteed that those bad things cannot happen!\n\nYes, it will require a lot of work, but the plumbing already exists.\n\nWhy do this? Because I plan to have commercial customers, who depend on\nsoftware supply chains. My hope is that supply chain attacks on my customers\ncan be curtailed, and that I will be paid for doing so.\n\nI don\u2019t even care about being paid handsomely; I just want enough.\n\nAnd while runtime capabilities will start in Yao and Rig scripts, run by an\ninterpreter, once Yao\u2019s compiler can generate actual machine code, I will add\nruntime capability checks to compiled Yao code.\n\nIn fact, Yao is actually using my own LLVM-like compiler backend, and that\nbackend is where runtime capabilities exist, so any language that targets my\nbackend would get those runtime checks for free.\n\nYes, these runtime capabilities are not as advanced as CHERI, but it is\nsomething that will exist on any platform for any piece of software.\n\nAnd finally, before we part ways, I would like to publicly answer a question I\ngot:\n\n> If turing-completeness is necessary, why should there even exist a build\n> system independent from a programming language now?\n>\n> \u2013 choeger on Hacker News\n\nBesides the answer given on Hacker News (mult-language builds) and the\ncapabilities system I just mentioned, I have one good reason: those languages\nare not designed for builds.\n\nRig\u2019s language, Yao, is my own creation. I have been designing it for 12\nyears.\n\nAnd it has one trick: it is extensible in ways other languages are not.\n\nYes, Rust and Lisp have macros. Lisp has reader macros.\n\nBut Yao has lex modes and user-defined keywords.\n\nLex modes are like reader macros; in fact, they may be equivalent.\n\nIn Yao, they let you do this:\n\n    \n    \n    $ clang -o test test.c;\n\nWhich will run Clang on test.c.\n\nIn short, lex modes let Yao have a first-class shell-like syntax for running\ncommands that can take Yao expressions as arguments. And that is not all they\ncan do.\n\nBut user-defined keywords are even better.\n\nThey are like macros, except that they are run directly by the compiler at\nparse time, as though they are part of the compiler.\n\nAll of Yao\u2019s keywords are defined as user-defined keywords, and all of Rig\u2019s\nare too. Since Rig can have its own, it can define its own build DSL.\n\nAnd that\u2019s exactly what it does. In fact, the first Yao program to ever run is\nits own build script, which is written in the Rig DSL.\n\nSo why should a build system exist independently of the language it builds? So\nit can have a build language more fit for purpose.\n\nAnd all of the other reasons too.\n\nIf all of that sounds interesting to you, wait one more week; Rig and its\nlanguage, Yao, will be publicly released on April 2, 2024.\n\nLook for an announcement on Hacker News, Reddit, and this blog!\n\nOlder Post\n\n# Recent Posts\n\nWhat Computers Cannot Do: The Consequences of Turing-Completeness\n\nBuild System Schism: The Curse of Meta Build Systems\n\nHow Yzena Versions Software and Interfaces\n\n# Subscribe\n\n  * Twitter\n  * GitHub\n  * Atom\n\n\u00a92018-2024 Gavin D. Howard. All rights reserved. 100% AI-free organic content.\n\n", "frontpage": false}
