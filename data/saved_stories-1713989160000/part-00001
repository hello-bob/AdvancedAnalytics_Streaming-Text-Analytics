{"aid": "40144554", "title": "VideoGigaGAN: Towards Detail-Rich Video Super-Resolution", "url": "https://videogigagan.github.io/", "domain": "videogigagan.github.io", "votes": 63, "user": "bookofjoe", "posted_at": "2024-04-24 14:04:36", "comments": 29, "source_title": "VideoGigaGAN", "source_text": "VideoGigaGAN\n\n# VideoGigaGAN: Towards Detail-rich Video Super-Resolution\n\nYiran Xu^^\ud83d\udc22 Taesung Park^ Richard Zhang^ Yang Zhou^ Eli Shechtman^ Feng Liu^\nJia-Bin Huang^\ud83d\udc22 Difan Liu^\n\n^\ud83d\udc22University of Maryland, College Park ^Adobe Research\n\narXiv Supplementary PDF BibTex\n\n## 8\u00d7 Upsampling results (128\u00d7128\u21921024\u00d71024)\n\nOur model is able to upsample a video up to 8\u00d7 with rich details.\n\n## Abstract\n\nVideo super-resolution (VSR) approaches have shown impressive temporal\nconsistency in upsampled videos. However, these approaches tend to generate\nblurrier results than their image counterparts as they are limited in their\ngenerative capability. This raises a fundamental question: can we extend the\nsuccess of a generative image upsampler to the VSR task while preserving the\ntemporal consistency? We introduce VideoGigaGAN, a new generative VSR model\nthat can produce videos with high-frequency details and temporal consistency.\nVideoGigaGAN builds upon a large-scale image upsampler -- GigaGAN. Simply\ninflating GigaGAN to a video model by adding temporal modules produces severe\ntemporal flickering. We identify several key issues and propose techniques\nthat significantly improve the temporal consistency of upsampled videos. Our\nexperiments show that, unlike previous VSR methods, VideoGigaGAN generates\ntemporally consistent videos with more fine-grained appearance details. We\nvalidate the effectiveness of VideoGigaGAN by comparing it with state-of-the-\nart VSR models on public datasets and showcasing video results with 8\u00d7 super-\nresolution.\n\n## Overview: Why is it challenging?\n\n## Method Overview\n\nOur Video Super-Resolution (VSR) model is built upon the asymmetric U-Net\narchitecture of the image GigaGAN upsampler. To enforce temporal consistency,\nwe first inflate the image upsampler into a video upsampler by adding temporal\nattention layers into the decoder blocks. We also enhance consistency by\nincorporating the features from the flow-guided propagation module. To\nsuppress aliasing artifacts, we use Anti-aliasing block in the downsampling\nlayers of the encoder. Lastly, we directly shuttle the high frequency features\nvia skip connection to the decoder layers to compensate for the loss of\ndetails in the BlurPool process.\n\n## Ablation study\n\nStrong hallucination capability of image GigaGAN results in temporally\nflickering artifacts, especially aliasing caused by the artifacted LR input.\n\n1\n\nSlide to switch between different examples\n\nWe progressively add components to the base model to handle these artifacts \u2192\n\nImage GigaGAN\n\nGT\n\nInput\n\nComparison with GT\n\n## Comparison with previous methods\n\nCompared to previous models, our models provides a detail-rich result with\ncomparable temporal consistency.\n\n1\n\nInput\n\nBasicVSR++\n\nOurs\n\nComparison\n\nGT\n\n## Results on generic videos (128\u00d7128\u2192512\u00d7512)\n\nOur model is able to handle generic videos of different categories.\n\n## BibTeX\n\n    \n    \n    @article{xu2024videogigagan, title={VideoGigaGAN: Towards Detail-rich Video Super-Resolution}, author={Yiran Xu and Taesung Park and Richard Zhang and Yang Zhou and Eli Shechtman and Feng Liu and Jia-Bin Huang and Difan Liu}, year={2024}, eprint={2404.12388}, archivePrefix={arXiv}, primaryClass={cs.CV} }\n\nWe thank Nerfies and Upscale-A-Video for its template.\n\n", "frontpage": true}
