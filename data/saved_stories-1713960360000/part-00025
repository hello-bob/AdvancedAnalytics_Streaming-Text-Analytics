{"aid": "40139398", "title": "Apple releases CoreNet, a library for training deep neural networks", "url": "https://github.com/apple/corenet", "domain": "github.com/apple", "votes": 31, "user": "rocauc", "posted_at": "2024-04-24 01:26:48", "comments": 9, "source_title": "GitHub - apple/corenet: CoreNet: A library for training deep neural networks", "source_text": "GitHub - apple/corenet: CoreNet: A library for training deep neural networks\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\napple / corenet Public\n\n  * Notifications\n  * Fork 1\n  * Star 91\n\nCoreNet: A library for training deep neural networks\n\n### License\n\nView license\n\n91 stars 1 fork Branches Tags Activity\n\nStar\n\nNotifications\n\n# apple/corenet\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nsacmehtaOpenELM arxiv link addedApr 24, 20245b50eca \u00b7 Apr 24, 2024Apr 24, 2024\n\n## History\n\n2 Commits  \n  \n### assets\n\n|\n\n### assets\n\n| CoreNet v0.1.0| Apr 23, 2024  \n  \n### corenet\n\n|\n\n### corenet\n\n| CoreNet v0.1.0| Apr 23, 2024  \n  \n### mlx_examples\n\n|\n\n### mlx_examples\n\n| CoreNet v0.1.0| Apr 23, 2024  \n  \n### projects\n\n|\n\n### projects\n\n| OpenELM arxiv link added| Apr 24, 2024  \n  \n### tests\n\n|\n\n### tests\n\n| CoreNet v0.1.0| Apr 23, 2024  \n  \n### tools\n\n|\n\n### tools\n\n| CoreNet v0.1.0| Apr 23, 2024  \n  \n### tutorials\n\n|\n\n### tutorials\n\n| CoreNet v0.1.0| Apr 23, 2024  \n  \n### .dockerignore\n\n|\n\n### .dockerignore\n\n| CoreNet v0.1.0| Apr 23, 2024  \n  \n### .flake8\n\n|\n\n### .flake8\n\n| CoreNet v0.1.0| Apr 23, 2024  \n  \n### .gitattributes\n\n|\n\n### .gitattributes\n\n| CoreNet v0.1.0| Apr 23, 2024  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| CoreNet v0.1.0| Apr 23, 2024  \n  \n### ACKNOWLEDGEMENTS\n\n|\n\n### ACKNOWLEDGEMENTS\n\n| CoreNet v0.1.0| Apr 23, 2024  \n  \n### CODE_OF_CONDUCT.md\n\n|\n\n### CODE_OF_CONDUCT.md\n\n| CoreNet v0.1.0| Apr 23, 2024  \n  \n### CONTRIBUTING.md\n\n|\n\n### CONTRIBUTING.md\n\n| CoreNet v0.1.0| Apr 23, 2024  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| CoreNet v0.1.0| Apr 23, 2024  \n  \n### Makefile\n\n|\n\n### Makefile\n\n| CoreNet v0.1.0| Apr 23, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| OpenELM arxiv link added| Apr 24, 2024  \n  \n### conftest.py\n\n|\n\n### conftest.py\n\n| CoreNet v0.1.0| Apr 23, 2024  \n  \n### pyproject.toml\n\n|\n\n### pyproject.toml\n\n| CoreNet v0.1.0| Apr 23, 2024  \n  \n### requirements-optional.txt\n\n|\n\n### requirements-optional.txt\n\n| CoreNet v0.1.0| Apr 23, 2024  \n  \n### requirements.txt\n\n|\n\n### requirements.txt\n\n| CoreNet v0.1.0| Apr 23, 2024  \n  \n### setup.py\n\n|\n\n### setup.py\n\n| CoreNet v0.1.0| Apr 23, 2024  \n  \n### tox.ini\n\n|\n\n### tox.ini\n\n| CoreNet v0.1.0| Apr 23, 2024  \n  \n## Repository files navigation\n\n# CoreNet: A library for training deep neural networks\n\nCoreNet is a deep neural network toolkit that allows researchers and engineers\nto train standard and novel small and large-scale models for variety of tasks,\nincluding foundation models (e.g., CLIP and LLM), object classification,\nobject detection, and semantic segmentation.\n\n## Table of contents\n\n  * What's new?\n  * Research efforts at Apple using CoreNet\n  * Installation\n  * Directory Structure\n  * Maintainers\n  * Contributing to CoreNet\n  * License\n  * Relationship with CVNets\n  * Citation\n\n## What's new?\n\n  * April 2024: Version 0.1.0 of the CoreNet library includes\n\n    * OpenELM\n    * CatLIP\n    * MLX examples\n\n## Research efforts at Apple using CoreNet\n\nBelow is the list of publications from Apple that uses CoreNet:\n\n  * OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework\n\n  * CatLIP: CLIP-level Visual Recognition Accuracy with 2.7x Faster Pre-training on Web-scale Image-Text Data\n  * Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement\n  * CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement\n  * FastVit: A Fast Hybrid Vision Transformer using Structural Reparameterization\n  * Bytes Are All You Need: Transformers Operating Directly on File Bytes\n  * MobileOne: An Improved One millisecond Mobile Backbone\n  * RangeAugment: Efficient Online Augmentation with Range Learning\n  * Separable Self-attention for Mobile Vision Transformers (MobileViTv2)\n  * CVNets: High performance library for Computer Vision, ACM MM'22\n  * MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer, ICLR'22\n\n## Installation\n\nYou will need Git LFS (instructions below) to run tests and Jupyter notebooks\n(instructions) in this repository, and to contribute to it so we recommend\nthat you install and activate it first.\n\nOn Linux we recommend to use Python 3.10+ and PyTorch (version >= v2.1.0), on\nmacOS system Python 3.9+ should be sufficient.\n\nNote that the optional dependencies listed below are required if you'd like to\nmake contributions and/or run tests.\n\nFor Linux (substitute apt for your package manager):\n\n    \n    \n    sudo apt install git-lfs git clone git@github.com:apple/corenet.git cd corenet git lfs install git lfs pull # The following venv command is optional, but recommended. Alternatively, you can create and activate a conda environment. python3 -m venv venv && source venv/bin/activate python3 -m pip install --editable .\n\nTo install optional dependencies for audio and video processing:\n\n    \n    \n    sudo apt install libsox-dev ffmpeg\n\nFor macOS, assuming you use Homebrew:\n\n    \n    \n    brew install git-lfs git clone git@github.com:apple/corenet.git cd corenet cd \\$(pwd -P) # See the note below. git lfs install git lfs pull # The following venv command is optional, but recommended. Alternatively, you can create and activate a conda environment. python3 -m venv venv && source venv/bin/activate python3 -m pip install --editable .\n\nTo install optional dependencies for audio and video processing:\n\n    \n    \n    brew install sox ffmpeg\n\nNote that on macOS the file system is case insensitive, and case sensitivity\ncan cause issues with Git. You should access the repository on disk as if the\npath were case sensitive, i.e. with the same capitalization as you see when\nyou list the directories ls. You can switch to such a path with the cd $(pwd\n-P) command.\n\n## Directory Structure\n\nThis section provides quick access and a brief description for important\nCoreNet directories.\n\nDescription| Quick Access  \n---|---  \n  \n### Getting Started\n\nWorking with the examples is an easy way to get started with CoreNet.|\n\n    \n    \n    \u2514\u2500\u2500 tutorials \u251c\u2500\u2500 train_a_new_model_on_a_new_dataset_from_scratch.ipynb \u251c\u2500\u2500 guide_slurm_and_multi_node_training.md \u251c\u2500\u2500 clip.ipynb \u251c\u2500\u2500 semantic_segmentation.ipynb \u2514\u2500\u2500 object_detection.ipynb  \n  \n### Training Recipes\n\nCoreNet provides reproducible training recipes, in addition to the pretrained\nmodel weights and checkpoints for the publications that are listed in\nprojects/ directory.Publication project directories generally contain the\nfollowing contents:\n\n  * README.md provides documentation, links to the pretrained weights, and citations.\n  * <task_name>/<model_name>.yaml provides configuration for reproducing the trainings and evaluations.\n\n|\n\n    \n    \n    \u2514\u2500\u2500 projects \u251c\u2500\u2500 byteformer \u251c\u2500\u2500 catlip (*) \u251c\u2500\u2500 clip \u251c\u2500\u2500 fastvit \u251c\u2500\u2500 mobilenet_v1 \u251c\u2500\u2500 mobilenet_v2 \u251c\u2500\u2500 mobilenet_v3 \u251c\u2500\u2500 mobileone \u251c\u2500\u2500 mobilevit \u251c\u2500\u2500 mobilevit_v2 \u251c\u2500\u2500 openelm (*) \u251c\u2500\u2500 range_augment \u251c\u2500\u2500 resnet \u2514\u2500\u2500 vit (*) Newly released.  \n  \n### MLX Examples\n\nMLX examples demonstrate how to run CoreNet models efficiently on Apple\nSilicon. Please find further information in the README.md file within the\ncorresponding example directory.|\n\n    \n    \n    \u2514\u2500\u2500mlx_example \u251c\u2500\u2500 clip \u2514\u2500\u2500 open_elm  \n  \n### Model Implementations\n\nModels are organized by tasks (e.g. \"classification\"). You can find all model\nimplementations for each task in the corresponding task folder.Each model\nclass is decorated by a @MODEL_REGISTRY.register(name=\"<model_name>\",\ntype=\"<task_name>\") decorator. To use a model class in CoreNet training or\nevaluation, assign moels.<task_name>.name = <model_name> in the YAML\nconfiguration.|\n\n    \n    \n    \u2514\u2500\u2500 corenet \u2514\u2500\u2500 modeling \u2514\u2500\u2500 models \u251c\u2500\u2500 audio_classification \u251c\u2500\u2500 classification \u251c\u2500\u2500 detection \u251c\u2500\u2500 language_modeling \u251c\u2500\u2500 multi_modal_img_text \u2514\u2500\u2500 segmentation  \n  \n### Datasets\n\nSimilarly to the models, datasets are also categorized by tasks.|\n\n    \n    \n    \u2514\u2500\u2500 corenet \u2514\u2500\u2500 data \u2514\u2500\u2500 datasets \u251c\u2500\u2500 audio_classification \u251c\u2500\u2500 classification \u251c\u2500\u2500 detection \u251c\u2500\u2500 language_modeling \u251c\u2500\u2500 multi_modal_img_text \u2514\u2500\u2500 segmentation  \n  \n### Other key directories\n\nIn this section, we have highlighted the rest of the key directories that\nimplement classes corresponding to the names that are referenced in the YAML\nconfigurations.|\n\n    \n    \n    \u2514\u2500\u2500 corenet \u251c\u2500\u2500 loss_fn \u251c\u2500\u2500 metrics \u251c\u2500\u2500 optims \u2502 \u2514\u2500\u2500 scheduler \u251c\u2500\u2500 train_eval_pipelines \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 collate_fns \u2502 \u251c\u2500\u2500 sampler \u2502 \u251c\u2500\u2500 text_tokenizer \u2502 \u251c\u2500\u2500 transforms \u2502 \u2514\u2500\u2500 video_reader \u2514\u2500\u2500 modeling \u251c\u2500\u2500 layers \u251c\u2500\u2500 modules \u251c\u2500\u2500 neural_augmentor \u2514\u2500\u2500 text_encoders  \n  \n## Maintainers\n\nThis code is developed by Sachin, and is now maintained by Sachin, Maxwell\nHorton, Mohammad Sekhavat, and Yanzi Jin.\n\n### Previous Maintainers\n\n  * Farzad\n\n## Contributing to CoreNet\n\nWe welcome PRs from the community! You can find information about contributing\nto CoreNet in our contributing document.\n\nPlease remember to follow our Code of Conduct.\n\n## License\n\nFor license details, see LICENSE.\n\n## Relationship with CVNets\n\nCoreNet evolved from CVNets, to encompass a broader range of applications\nbeyond computer vision. Its expansion facilitated the training of foundational\nmodels, including LLMs.\n\n## Citation\n\nIf you find our work useful, please cite the following paper:\n\n    \n    \n    @inproceedings{mehta2022cvnets, author = {Mehta, Sachin and Abdolhosseini, Farzad and Rastegari, Mohammad}, title = {CVNets: High Performance Library for Computer Vision}, year = {2022}, booktitle = {Proceedings of the 30th ACM International Conference on Multimedia}, series = {MM '22} }\n\n## About\n\nCoreNet: A library for training deep neural networks\n\n### Resources\n\nReadme\n\n### License\n\nView license\n\n### Code of conduct\n\nCode of conduct\n\nActivity\n\nCustom properties\n\n### Stars\n\n91 stars\n\n### Watchers\n\n7 watching\n\n### Forks\n\n1 fork\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Languages\n\n  * Python 99.7%\n  * Other 0.3%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": true}
