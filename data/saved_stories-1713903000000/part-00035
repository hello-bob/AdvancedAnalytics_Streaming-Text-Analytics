{"aid": "40131022", "title": "Virtual reality-empowered deep-learning analysis of brain cells", "url": "https://www.nature.com/articles/s41592-024-02245-2", "domain": "nature.com", "votes": 2, "user": "bookofjoe", "posted_at": "2024-04-23 12:13:39", "comments": 0, "source_title": "Virtual reality-empowered deep-learning analysis of brain cells", "source_text": "Virtual reality-empowered deep-learning analysis of brain cells | Nature Methods\n\nLoading [MathJax]/jax/output/HTML-CSS/config.js\n\nSkip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n\n## Search\n\nAdvanced search\n\n### Quick links\n\n    * Explore articles by subject\n    * Find a job\n    * Guide to authors\n    * Editorial policies\n\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n\n  * Sign up for alerts\n  * RSS feed\n\nVirtual reality-empowered deep-learning analysis of brain cells\n\nDownload PDF\n\nDownload PDF\n\n  * Article\n  * Open access\n  * Published: 22 April 2024\n\n# Virtual reality-empowered deep-learning analysis of brain cells\n\n  * Doris Kaltenecker^1,2,3,4^ na1,\n  * Rami Al-Maskari^4,5,6,7^ na1,\n  * Moritz Negwer^5^ na1,\n  * Luciano Hoeher^5,\n  * Florian Kofler^6,7,8,9,\n  * Shan Zhao^4,5,\n  * Mihail Todorov ORCID: orcid.org/0000-0002-8627-1260^4,5,\n  * Zhouyi Rong^4,5,\n  * Johannes Christian Paetzold^5,7,10,\n  * Benedikt Wiestler ORCID: orcid.org/0000-0002-2963-7772^8,\n  * Marie Piraud ORCID: orcid.org/0000-0002-4917-2458^9,\n  * Daniel Rueckert ORCID: orcid.org/0000-0002-5683-5889^10,\n  * Julia Geppert^1,2,3,\n  * Pauline Morigny^1,2,3,\n  * Maria Rohm ORCID: orcid.org/0000-0003-3926-1534^1,2,3,\n  * Bjoern H. Menze^6,11,\n  * Stephan Herzig ORCID: orcid.org/0000-0003-3950-3652^1,2,3,12,\n  * Mauricio Berriel Diaz ORCID: orcid.org/0000-0003-4670-919X^1,2,3 &\n  * ...\n  * Ali Ert\u00fcrk ORCID: orcid.org/0000-0001-5163-5100^4,5,13,14,15\n\nNature Methods (2024)Cite this article\n\n  * 128 Altmetric\n\n  * Metrics details\n\n## Abstract\n\nAutomated detection of specific cells in three-dimensional datasets such as\nwhole-brain light-sheet image stacks is challenging. Here, we present DELiVR,\na virtual reality-trained deep-learning pipeline for detecting c-Fos^+ cells\nas markers for neuronal activity in cleared mouse brains. Virtual reality\nannotation substantially accelerated training data generation, enabling DELiVR\nto outperform state-of-the-art cell-segmenting approaches. Our pipeline is\navailable in a user-friendly Docker container that runs with a standalone Fiji\nplugin. DELiVR features a comprehensive toolkit for data visualization and can\nbe customized to other cell types of interest, as we did here for microglia\nsomata, using Fiji for dataset-specific training. We applied DELiVR to\ninvestigate cancer-related brain activity, unveiling an activation pattern\nthat distinguishes weight-stable cancer from cancers associated with weight\nloss. Overall, DELiVR is a robust deep-learning tool that does not require\nadvanced coding skills to analyze whole-brain imaging data in health and\ndisease.\n\n### Similar content being viewed by others\n\n### Dense 4D nanoscale reconstruction of living brain tissue\n\nArticle Open access 10 July 2023\n\nPhilipp Velicky, Eder Miguel, ... Johann G. Danzl\n\n### Whole-brain tissue mapping toolkit using large-scale highly multiplexed\nimmunofluorescence imaging and deep neural networks\n\nArticle Open access 10 March 2021\n\nDragan Maric, Jahandar Jahanipour, ... Badrinath Roysam\n\n### SimpylCellCounter: an automated solution for quantifying cells in brain\ntissue\n\nArticle Open access 28 July 2020\n\nAneesh Bal, Fidel Maureira & Amy A. Arguello\n\n## Main\n\nAnalyzing the expression of proteins is essential to understand cellular and\nmolecular processes in physiology and disease. While standard\nimmunohistochemistry is useful for validating protein expression on tissue\nsections, it does not provide a holistic view of expression patterns in larger\nsamples and information can be lost during slicing^1,2. Tissue clearing and\nfluorescent imaging solve many of these restrictions and allow unbiased\nprotein expression analysis up to the whole-organism scale^1,3,4,5.\n\nWhole-brain analysis is essential for detecting areas involved in specific\nbehaviors or conditions. A brain-wide snapshot of the neuronal activity of an\nanimal can be obtained by immunostaining for the expression of immediate early\ngenes such as c-Fos. Unbiased quantification methods for system-level\nexamination at the single-cell resolution are essential to interpret those\nbrain-wide findings^6, but current automated methods for cell detection and\nregistration to the Allen Mouse Brain Atlas^7,8,9 are difficult to apply\nconsistently to three-dimensional (3D) whole-brain datasets. Variations in\nimage acquisitions between samples, uneven signal-to-noise ratios across the\ntissue or low abundance of the target protein limit detection sensitivity and\nspecificity. This requires manual adjustments such as setting sample and\nvolume-specific thresholds or using conservative thresholds that will not\ncapture all information in each sample. Deep-learning-based cell detection\nmethods offer a promising solution to address these challenges; however, their\nimplementation typically demands advanced coding skills, presenting a\nchallenge for users lacking computational expertise.\n\nHere, we developed DELiVR (deep learning and virtual reality mesoscale\nannotation pipeline), a virtual reality (VR)-aided deep-learning pipeline for\ndetecting c-Fos^+ cells in cleared mouse brains (Fig. 1a) that can be extended\nto other cell types. We generated high-quality annotations of light-sheet\nmicroscopy data of cleared whole mouse brains stained for c-Fos in a VR\nenvironment. Next, we trained a deep neural network on these data to identify\nc-Fos^+ cells across the brain and mapped them automatically to the Allen\nBrain Atlas. To increase the usability of DELiVR, we packaged it into a single\nDocker container that runs via a plugin for the open-source software Fiji.\nDELiVR can also be trained with custom data via Fiji to adapt DELiVR to\nspecific datasets. We used DELiVR to study cancer-related cachexia and found\nincreased neuronal activity in mice with weight-stable cancer in brain areas\nrelated to sensory processing and foraging. In contrast, this increase was\nlost in cachectic animals, suggesting a weight-stable cancer-specific\nneurophysiological hyperactivation phenotype.\n\nFig. 1: Virtual reality-aided annotation is faster than 2D-slice annotation.\n\na, Summary of VR-aided deep learning for antibody-labeled cell segmentation in\nmouse brains. (i) Fixed mouse brains are subjected to SHANEL-based antibody\nlabeling, tissue clearing and fluorescent light-sheet imaging. (ii) Volumes of\nraw data are labeled in VR to generate reference annotations. (iii) The DELiVR\npipeline was packaged in a Docker container, controlled via a Fiji plugin.\nDELiVR segments cells using deep learning and registers them to the Allen\nBrain Atlas. DELiVR produces per-region cell counts and generates\nvisualizations with all detected cells color coded by atlas region. b, Patch\nvolume of raw data (c-Fos-labeled brain imaged with LSFM) and loaded into\nArivis VisionVR. Volume size represents 200^3 voxel, rendered isotropically.\nc, Illustration of VR goggles and VR zoomed-in view of the same data as in b.\nd\u2013f, Using Arivis VisionVR, individual cells were annotated by placing a\nselection cube on the cell (d), fitting the cube to the size of the cell (e)\nand filling (f). Scale bar, 10 \u03bcm. g,h, Zoomed-in view of raw data (same\nvolume as in b) (g) and annotation overlay generated in VR (h). Scale bar, 10\n\u03bcm. i, Time spent for annotating a test patch using 2D-slice (n = 7) and VR\nannotation (n = 12 with n = 6 annotations performed with Arivis VisonVR and n\n= 6 annotations performed with syGlass). Data are presented as mean \u00b1 s.e.m.\n***P = 0.0005, two-sided Mann\u2013Whitney U-test. j, Instance Dice of 2D-slice\nannotation (n = 7) versus VR annotation (n = 12 with n = 6 annotations\nperformed with Arivis VisonVR and n = 6 annotations performed with syGlass).\nData are presented as mean \u00b1 s.e.m. *P = 0.0445, two-sided unpaired t-test.\nA.U., arbitrary units.\n\nSource data\n\nFull size image\n\n## Results\n\n### Reference annotation is faster in VR compared to 2D slices\n\nWe used the SHANEL protocol^10 for whole-brain c-Fos immunostaining, tissue\nclearing and light-sheet fluorescence microscopy (LSFM). To train deep-\nlearning segmentation models in a supervised manner, substantial amounts of\nhigh-quality expert annotations are crucial. As common annotation approaches\nsuch as ITK-SNAP^11 rely on time-consuming sequential two-dimensional (2D)\nslice-by-slice annotation, we used a VR approach that allows for full\nimmersion into 3D volumetric data (Fig. 1b,c). We used two commercial VR\nannotation software packages (Arivis VisionVR and syGlass^12) to evaluate the\nspeed and accuracy of VR in comparison to 2D slice-based annotation in ITK-\nSNAP.\n\nFor annotation using Arivis VisionVR, the annotator defined a region of\ninterest (ROI) in which an adaptive thresholding function was applied,\naccording to the annotator\u2019s input (Fig. 1d\u2013h and Supplementary Video 1). In\nsyGlass, the annotation tool allowed the annotator to draw simple 3D shapes as\nROIs and adjust a threshold until the annotation was acceptable to the\nannotator (Extended Data Fig. 1a\u2013d and Supplementary Video 2). In ITK-SNAP,\nindividual c-Fos^+ cells were segmented in each plane of the image stack\n(Extended Data Fig. 1e and Supplementary Video 3). We evaluated the time spent\nby the annotators for a 1003 voxel sub-volume (depicting 83 c-Fos^+ cells) as\nwell as the annotation quality of cell instances using the F1 score. We found\nthat VR annotation was significantly (P = 0.0005, two-sided Mann\u2013Whitney\nU-test) faster than 2D-slice annotation (Fig. 1i) and improved annotation\nquality (increase in F1 score from 0.7383 to 0.8032 (Fig. 1j)). Thus, we\ndecided to generate reference data in VR for our deep-learning algorithm for\nc-Fos activity mapping.\n\n### DELiVR outperforms threshold-based c-Fos segmentation\n\nTo comprehensively analyze neuronal activity across the entire brain, DELiVR\ndetects and aligns the cells to the Allen Brain Atlas. DELiVR then visualizes\nthe segmentation in both image and atlas space. Therefore, DELiVR consists of\nmultiple steps (Fig. 2a). First, the pipeline downsamples the raw image stack\nand generates ventricle masks (Extended Data Fig. 2a\u2013c). It then upscales the\nmasks and uses them to mask the ventricles in the raw image input. DELiVR then\nutilizes a customized sliding-window inferer to identify potential cells.\nAfterwards, we conduct a connected component analysis^13 to identify\nindividual cells in the masked images and filter by size. DELiVR then aligns\nthe previously downsampled brain to the Allen Brain Atlas (CCF3, 50 \u03bcm per\nvoxel) with mBrainAligner^14 and assigns the corresponding atlas region to\neach detected cell. The connected component analysis returns a set of center-\npoint coordinates and volume for each segmented cell, which DELiVR then\nautomatically maps to the Allen Brain Atlas with mBrainAligner.\n\nFig. 2: DELiVR\u2019s UNet outperforms current methods for c-Fos^+ cell detection.\n\na, Scheme of the DELiVR inference pipeline. All components are packaged in a\nsingle Docker container. Raw image stacks serve as input. They are downsampled\nfor atlas alignment and optionally masked (to exclude detection on\nventricles). The masked images are then passed on to deep-learning cell\ndetection (inference), which produces binary segmentations. The binarized\ncell\u2019s center points are subsequently transformed to the Allen Brain Atlas\nCCF3 space. The cells are visualized in atlas space as (group-wise) heat maps\nand in image space as color-coded tiff stacks. b, Quantitative comparison of\nsegmentation performance based on instance Dice (F1 score) between different\ndeep-learning architectures and DELiVR. c, F1 scores for non-deep-learning\nmethods (gray) and DELiVR (the same F1 score for DELiVR is used as in b). d,\n3D qualitative comparison between ClearMap, ClearMap2, \u2018Optimized\u2019 ClearMap,\nIlastik and DELiVR on instance basis. Predicted cells with overlap in\nreference annotations (TP) are masked in green, predicted cells with no\noverlap in reference annotations (FP) are masked in red. Undetected reference\nannotation cells (FN) are marked in blue. TP, true positive; FP, false\npositive; FN, false negative. Scale bar, 100 \u03bcm. e, Whole-brain segmentation\noutput of the detected cells is visualized in atlas space using BrainRender.\nScale bar, 1 mm in CCF3 atlas space.\n\nSource data\n\nFull size image\n\nTo train and validate our model, we randomly sampled and VR-annotated 48 \u00d7\n1003 voxel patches (referring to 5,889 cells) from a c-Fos-labeled brain. From\nthese we trained a 3D BasicUNet (Extended Data Fig. 2d). In addition, we\ntrained recent larger segmentation models, such as transformers^15,\nSegResNet^16 and the MONAI DynUnet^17 to determine which model was best suited\nfor our data. Assessing the instance performance by calculating the overlap\nbetween individual cells, the 3D BasicUNet architecture showed the best\nperformance (based on F1 score) (Fig. 2b and Extended Data Fig. 2e).\nTherefore, we chose the 3D BasicUNet for our DELiVR pipeline.\n\nWe also compared DELiVR with previously published non-deep-learning models\nthat are applicable to cell detection in 3D images and had code available\n(ClearMap^7, ClearMap2 (ref. ^18) and Ilastik^19). Our performance on the test\nset shows an F1 score of 0.7918 (+89.03% increase), instance sensitivity of\n0.8470 (+181.64% increase), instance precision of 0.7434 (+7.74% increase) and\na volumetric Dice of 0.6739 (+581,39% increase) compared to the second-best\nperforming method, ClearMap2 (Fig. 2c,d, Extended Data Fig. 2f and\nSupplementary Table 1). We increased the performance of ClearMap based on the\nF1 score to 0.65 by manually pre-processing image stacks and optimizing\nparameters for cell detection^3; however, DELiVR still had a superior\nperformance. These scores demonstrate a clear improvement over filter and\nthreshold-based segmentation methods as the deep-learning model captures 84.8\ntimes more cells (1,611 true positives) than ClearMap (19 true positives), 2.8\ntimes more cells than ClearMap2 (572 true positives) and 31.2% more than the\noptimized version of ClearMap (1,228 true positives) while not over-\nsegmenting. For visualization, DELiVR generates a whole-brain segmentation\noutput that exists in the original image space. Here, each segmented cell\ncorresponds to a threshold value fitting to an Area ID of the Allen Brain\nAtlas and was colored according to the brain region that it belongs to\n(Extended Data Fig. 3a,b and Supplementary Video 4). In addition, we used\nBrainRender^20 to plot and visualize the detected cells in the atlas space\n(Fig. 2e and Extended Data Fig. 3c).\n\nTo increase usability, the entire DELiVR pipeline, encompassing atlas\nalignment, cell detection and visualization, is available as a single, user-\nfriendly Docker container for both Linux and Windows. Docker is a software\nplatform that allows to bundle and distribute applications, along with their\nrequired components, in a uniform container format^21. We also developed a\ndedicated Fiji plugin to seamlessly run the DELiVR Docker (Fig. 3a\u2013c and\nSupplementary Video 5).\n\nFig. 3: DELiVR runs end to end and can be adapted to other cell types.\n\na\u2013c, The DELiVR plugin will appear in Fiji upon installation. It can launch\nDELiVR for inference (b) or launch the training Docker to train on domain-\nspecific training data (c). d,e, Zoomed-in Arivis VisionVR view of raw data\nfrom a CX3CR1^GFP/+ microglia reporter mouse (d) and annotation overlay of\ncell bodies generated in VR (e). Scale bar, 10 \u03bcm. f, 3D representation of the\ntraining evaluation on instance basis; predicted cells with overlap in\nreference annotations are masked in green (TP), predicted cells with no\noverlap in reference annotations are masked in red (FP) and reference\nannotation cells with no corresponding prediction are marked in blue (FN).\nFollowing training, DELiVR segments microglia cell bodies with a Dice (F1)\nscore of 0.92. Scale bar, 10 \u03bcm. g, Optical section of a CX3CR1^GFP/+\nmicroglia reporter mouse brain hemisphere (n = 1, sagittal), scanned at \u00d712\nmagnification and with inversed brightness (microglia indicates black spots).\nScale bar, 1 mm. h, Zoomed-in view of the cortex (red inset in g), with\noverlaid segmented cells detected by whole-hemisphere DELiVR analysis shown in\ngreen (n = 1). Scale bar, 100 \u03bcm. i, Visualization of 12.2 million\nCX3CR1^GFP/+ microglia across one hemisphere, generated by DELiVR and\nvisualized with Imaris. Color-coding per Allen Brain Atlas CCF3 regions. Scale\nbar, 1 mm.\n\nFull size image\n\nMoreover, we provide a Docker container for training that integrates with the\nDELiVR Fiji plugin (Fig. 3a,c). This feature allows users to (re-)train DELiVR\non other datasets, thereby enhancing DELiVR\u2019s precision and adaptability.\nUsers can choose to fine-tune the existing c-Fos model or train their own\nmodel from scratch. For this, one can adjust hyperparameters such as the\nnumber of epochs and learning rate. The user-trained model can then be used in\nthe DELiVR pipeline as the inference model. For a comprehensive guide, please\nconsult our \u2018DELiVR handbook\u2019 (Supplementary Note).\n\nWe used our DELiVR training to annotate microglial cell bodies, the brain\u2019s\nresident macrophages^22. We performed whole-brain nanobody labeling in\nCX3CR1^GFP/+ reporter mice, clearing and LSFM, and annotated microglia somata\nin VR (Fig. 3d,e). Training used a dataset of 161 VR-annotated 100^3 voxel\npatches with a total of 3,798 annotated microglia somata. The newly trained\nmodel had an F1 score of 0.92, indicating robust performance^23 (Fig. 3f). We\napplied this model in the DELiVR pipeline and could detect and map microglia\ncell bodies throughout the brain. Using DELiVR\u2019s visualization tool, we\nevaluated the microglia cell body segmentation output generated by DELiVR in\nour original images (Fig. 3g\u2013i) and mapped the segmented cells to region IDs\nof the Allen Brain Atlas (Fig. 3i). Thereby, DELiVR allows to find and confirm\nan anatomical or functional sub-area in the original image stack of the brain.\n\n### DELiVR identifies activation patterns in tumor-bearing mice\n\nCancer affects normal physiology locally in the surrounding tissue but can\nalso lead to profound changes in the systemic metabolism of the patient. This\nis exemplified by the wasting syndrome cancer-associated cachexia (CAC)\ncharacterized by involuntary loss of body weight^24,25,26 and specific changes\nin brain activity^27.\n\nTo identify brain regions affecting body weight maintenance in cancer, we used\nDELiVR to compare the neuronal activity patterns between weight-stable cancer\nand CAC. We subcutaneously transplanted NC26 colon cancer cells that give rise\nto weight-stable cancer or C26 colon cancer cells that induce weight loss\n(Fig. 4a). As expected^28, no changes in body weight were observed in NC26\ntumor-bearing mice compared to controls, whereas C26 tumor-bearing mice showed\nsignificant (P < 0.0001, one-way analysis of variance (ANOVA) with Sidak post\nhoc analysis) reductions (Fig. 4b). The differences in body weight were not\ndue to differences in tumor mass (Fig. 4c). C26 tumor-bearing mice displayed\nreduced weights of the gastrocnemius muscle and white adipose tissue depots\n(Extended Data Fig. 4a\u2013c). We observed a small but statistically significant\n(P = 0.0479, one-way ANOVA with Sidak post hoc analysis) decrease in brain\nweights of cachectic C26 versus weight-stable NC26 tumor-bearing mice\n(Extended Data Fig. 4d). We performed c-Fos antibody labeling, clearing and\nimaging of whole brains of these mice and applied DELiVR for whole-brain\nmapping of neuronal activity. c-Fos^+ density maps indicated an increase in\nbrain activity in weight-stable NC26 tumor-bearing mice compared to phosphate-\nbuffered saline (PBS) controls, whereas this increase was not present in\ncachectic C26 tumor-bearing mice (Fig. 4d).\n\nFig. 4: DELiVR identifies changes in neuronal activity in weight-stable\ncancer.\n\na, Experimental setup. Adult mice were subcutaneously injected with PBS as\ncontrol; NC26 cells that lead to a weight-stable cancer or cachexia-inducing\nC26 cancer cells. b, Body weight change of mice at the end of the experiment\ncompared to starting body weight. Tumor weight was subtracted from the final\nbody weight. n(PBS) = 12, n(NC26) = 8, n(C26) = 12. Data are presented as mean\n\u00b1 s.e.m. ****P < 0.0001, one-way ANOVA with Sidak post hoc analysis c, Tumor\nweight at the end of the experiment. n(NC26) = 8, n(C26) = 12. Data are\npresented as mean \u00b1 s.e.m. d, Normalized c-Fos^+ cell density in brains of PBS\ncontrols, mice with weight-stable cancer (NC26) and mice with cancer-\nassociated weight loss (C26), visualized in CCF3 atlas space. n(PBS) = 12,\nn(NC26) = 8, n(C26) = 12. Scale bars, 2 mm.\n\nSource data\n\nFull size image\n\nThe increase in brain activity in NC26 tumor-bearing mice was most pronounced\nthroughout the cortical plate and in the lateral septal complex (Fig. 5a,b).\nOverall, we identified 19 areas in NC26 tumor-bearing mice that showed\nstatistically significantly (Padj < 0.1, two-sided unpaired t-tests with\nBenjamini\u2013Hochberg multiple-testing correction with family-wise error rate\n(FWER) = 0.1) increased c-Fos expression compared to PBS controls after\nmultiple-testing correction (Fig. 5a). We found that NC26-bearing mice also\nhave more c-Fos^+ cells in the cortical plate, with the most pronounced\ndifferences observed in the somatomotor areas (Fig. 5a,b). NC26 tumors notably\nincreased c-Fos^+ density in the somatosensory cortex related to the snout,\nspecifically in the mouth region (layer 2/3 and 4) and barrel field layer 4.\nFurthermore, NC26-bearing mice showed more c-Fos^+ cells than PBS controls in\nthe primary (layers 1 and 5) and secondary motor areas (layers 2/3 and 5; Fig.\n5a,b). The primary motor cortex layer 5 is especially interesting because it\ncontains extratelencephalic projection neurons that project as far as the\nspinal cord, among others^29.\n\nFig. 5: DELiVR identifies cancer-related brain activation patterns.\n\na, Brain-region-wise c-Fos^+ cell density log_2(fold change) compared between\nthe three groups. *Padj < 0.1 (two-sided unpaired t-tests with\nBenjamini\u2013Hochberg multiple-testing correction with FWER = 0.1, n(PBS) = 12,\nn(NC26) = 8, n(C26) = 12). b, Brain areas with significantly different (Padj <\n0.1) c-Fos expression between NC26/C26 (top) or NC26/PBS (bottom) visualized\nusing BrainRender. Red indicates significantly (*Padj < 0.1) more c-Fos^+\ncells in NC26 in both cases. Two-sided unpaired t-tests with\nBenjamini\u2013Hochberg multiple-testing correction with FWER = 0.1, n(PBS) = 12,\nn(NC26) = 8, n(C26) = 12. Scale bars, 1 mm. c, Flattened-cortex visualizations\nof normalized c-Fos^+ cell density for PBS control mice (n = 12), NC26 (n = 8)\nand C26 tumor-bearing mice (n = 12), Scale bars, 1 mm in flattened-cortex\nprojection space (flattened from CCF3 atlas space). d, c-Fos^+ cell density in\ncortical subregions that were statistically significantly (*Padj < 0.1)\ndifferent after multiple-testing correction. Two-sided unpaired t-tests with\nBenjamini\u2013Hochberg multiple-testing correction with FWER = 0.1, n(PBS) = 12,\nn(NC26) = 8, n(C26) = 12.\n\nSource data\n\nFull size image\n\nWe found seven areas that were significantly altered between NC26 and\ncachectic C26 tumor-bearing mice, whereas we did not observe significant\nchanges (Padj > 0.1, two-sided unpaired t-tests with Benjamini\u2013Hochberg\nmultiple-testing correction with FWER = 0.1) in c-Fos^+ expression when\ncomparing PBS and cachectic C26 tumor-bearing mice after correcting for\nmultiple testing (Fig. 5a). When comparing NC26 to C26 tumor-bearing mice, we\nfound that NC26 mice had more c-Fos^+ cells overall in the cortical plate,\nwith the differences clustering in the dorsal and agranular lateral\nretrosplenial cortex as well as a subset of entorhinal cortex (Fig. 5a,c).\nEvaluation of c-Fos^+ density heat maps offer additional details (Fig. 5d).\nEvaluation of layer 2/3 retrosplenial cortex shows that activity clusters in\nthe anterior third of retrosplenial cortex in PBS and C26, but not NC26\nretrosplenial cortex, where it is both stronger and spread further to the\nback. The retrosplenial cortex is thought to be a site of multisensory\nintegration, spatial integration and environment mapping^30 and is crucially\ninvolved in foraging behavior^31.\n\nOverall, our findings showed that brain activity in weight-stable NC26 cancer-\nbearing mice are markedly different from both cachectic C26 cancer-bearing\nmice and PBS controls. Specifically, we find a consistent hyperactivation\nphenotype in NC26 brains that is detectable at the whole-cortex level, but\nmost pronounced in areas relevant to somatosensation at the snout and motor\nplanning, as well as spatial navigation (all of which would be consistent with\na foraging-related brain activation pattern). Thus, with DELiVR we were able\nto identify a neuronal activity pattern specific to the NC26 cancer model.\n\n## Discussion\n\nHere, we present DELiVR, an end-to-end VR-enabled deep-learning-based\nquantification pipeline for whole-brain cell mapping in cleared mouse brains.\nWe designed it to make deep learning accessible to most biologists via a Fiji\nfront end, not requiring coding skills. We leveraged VR technology to generate\nreference annotations for training a deep-learning-based segmentation network.\nDELiVR improves segmentation accuracy compared to current cell detection\nmethods and generates a registered segmentation output that can be examined in\nthe original image and in the atlas spaces. In addition, our Fiji training\nfeature enables users to adapt DELiVR to their specific datasets, increasing\nits versatility and usability.\n\nTraditional, non-machine-learning solutions for large-scale analysis of c-Fos\ncell detection, such as ClearMap^7,18 rely on a sophisticated system of\nthresholding and filtering to detect small structures and classify them as\ncells. While such approaches generated valuable information^6, their\nperformance is limited for data with variable signal-to-noise ratios, as is\nthe case when imaging large volumes such as the entire mouse brain. Though\nparameters can be adjusted, it is difficult to find a setting that accounts\nfor all cells. Hence, the thresholds tend to be set conservatively, meaning\nthat subtle differences may be lost during threshold-based analysis. A trained\ndeep-learning model learns these local variances, thus providing more accurate\ncell number estimates than threshold-based methods, as exemplified by DELiVR\u2019s\nhigh instance F1 score. Previous approaches for segmenting cells in mouse\nbrains ranged from deep learning^32,33,34, random forest algorithms^19,35 and\nthreshold-based solutions^7,18,36; however, only a subset of studies published\ntheir analysis pipeline and model weights in a working package that makes it\napplicable for other datasets. We found that DELiVR\u2019s 3D BasicUNet\nconsistently outperformed all other approaches with available code. In\naddition to providing highly accurate AI-based cell detection, DELiVR provides\na unique and accessible open-source tool, functioning seamlessly within Fiji.\nIt encompasses all steps of brain activity mapping, including cell detection,\natlas alignment and visualization, in an easily accessible environment without\nthe need for writing additional computer code.\n\nOur experiments showed that VR is a superior means of annotation and data\nexploration for volumetric data analysis. Non-VR methods show orthogonal\nslices, which allows an annotator to outline the shape of individual cells in\n2D; however, it obfuscates necessary volumetric information, making annotation\nchallenging and time consuming; an annotator never sees the whole cell, only a\ncross-section and must scroll through slices to ensure that it is in fact a\ncell and not background noise. In contrast, VR allows the annotator to capture\n3D structures in their entirety, enabling the fast generation of more reliable\nannotated data.\n\nIn future work, it will be interesting to explore the possibility of\nperforming active learning in a VR environment. Active learning is a combined\nmachine-learning training and annotation approach, where a model selectively\nchooses the most informative or uncertain data points for manual annotation,\nallowing for efficient model improvement with fewer labeled data points^37.\nThis approach is currently limited by the possibilities of the VR annotation\nsoftware application programming interfaces. Using an ensemble of networks or\na test time augmentation uncertainty map as well as methods such as Monte-\nCarlo-based sampling using dropout layers^38 to highlight areas that are\nambiguous to the network, the annotator can be guided to even more efficient\ntime use in VR annotation. The annotators\u2019 choices can then be fed into a\nfine-tuning step to improve the model while annotating.\n\nWe used DELiVR to profile the brain activation patterns of cancer-bearing mice\nthat were either weight-stable or displayed CAC. A mix of reduced food intake,\nelevated catabolism, increased energy expenditure and inflammation drives\nweight loss in cancer^26. The brain was shown to contribute to anorexia in\nCAC, as it responds to inflammatory cytokines that modulate the activity of\nneuronal populations that regulate appetite^39. In addition, activation of\nneurons in the parabrachial nucleus was shown to suppress appetite in mouse\nmodels of CAC^40. The reduction in brain weight among cachectic C26 tumor-\nbearing mice aligns with prior reports of decreased brain weight in mice with\ncachexia-inducing pancreatic tumors^41. It is currently unclear whether this\nthis volume reduction is due to cell death, or white matter loss.\n\nNotably, we found a substantial increase in c-Fos^+ expression in the brains\nof weight-stable NC26 tumor-bearing mice, especially in motor and sensory\nareas, and higher-order regions such as the retrosplenial cortex. Those\nregions are linked to sensorimotor control, motor sequencing and\nforaging^30,42,43. The abundance of sensory-related regions suggests cancer-\nspecific impairment in GABAergic inhibition^44, driving a hyperactivation\nphenotype via disinhibition. If and how these increases in neuronal activation\nin weight-stable cancer-bearing mice affect body weight maintenance will be of\na high interest to explore in future studies.\n\nIn conclusion, we present DELiVR: an integrated, easy-to-use pipeline to\nlabel, scan and analyze neuronal activity markers across the entire mouse\nbrain and show how VR increases the speed and accuracy of generating reference\nannotations. Using DELiVR, we find differences in c-Fos expression between\ncachectic and non-cachectic cancer mouse brains, pointing us to a previously\nunknown neurophysiological phenotype in cancer-related weight control.\n\n## Methods\n\n### Whole-brain immunolabeling and clearing\n\nImmunostaining for c-Fos was performed using a modified version of SHANEL^10.\nAll incubation steps were carried out under moderate shaking (300 rpm). For\nthe pretreatment, samples were dehydrated with an ethanol/water series (50%,\n70% and 100% ethanol) at room temperature for 3 h per step. Next, samples were\nincubated in dichloromethane (DCM)/methanol (2:1 v/v) at room temperature for\n1 day. Brains were rehydrated with an ethanol/water series (100%, 70% and 50%\nethanol and diH_2O) at room temperature for 3 h per step. Samples were\nincubated in 0.5 M acetic acid at room temperature for 5 h followed by washing\nwith diH_2O. Next, brains were incubated in 4 M guanidine HCl, 0.05 M sodium\nacetate, 2% v/v Triton X-100, pH 6.0, at room temperature for 5 h followed by\nwashing with diH_2O. Brains were incubated in a mix of 10% CHAPS and 25%\nN-methyldiethanolamine at 37 \u00b0C for 12 h before washing with diH_2O. Blocking\nwas performed by incubating the brains in 0.2% Triton X-100, 10%\ndimethylsulfoxide and 10% goat serum in PBS shaking at 37 \u00b0C for 2 days.\nSamples were incubated with c-Fos primary antibody (Cell Signaling Technology,\n2250, 1:1,000 dilution) in primary antibody buffer (0.2% Tween-20, 5%\ndimethylsulfoxide, 3% goat serum and 100 \u03bcl heparin per 100 ml PBS) shaking at\n37 \u00b0C for 7 days. The antibody solution was filtered (22-\u03bcm pore size) before\nuse. Samples were washed in washing solution (0.2% Tween-20 and 100 \u03bcl heparin\nin 100 ml PBS) shaking at 37 \u00b0C for 1 day at which the washing solution was\nrefreshed five times. Brains were incubated with the secondary antibody (Alexa\nFluor 647 and goat anti-rabbit IgG (H + L) from Invitrogen, A-21245, 1:500\ndilution) in secondary antibody buffer (0.2% Tween-20, 3% goat serum and 100\n\u03bcl heparin per 100 ml PBS) shaking at 37 \u00b0C for 7 days followed by incubating\nin washing solution shaking at 37 \u00b0C for 1 day at which the washing solution\nwas refreshed five times. Brains were dehydrated using 3DISCO^2 with a\nTHF/H_2O series (50%, 70%, 90% and 100% THF) for 12 h per step followed by an\nincubation in DCM for 1 h. Tissues were incubated in benzyl alcohol/benzyl\nbenzoate (1:2 v/v) until tissue transparency was reached (>4 h).\n\nFor microglia labeling, brains of CX3CR1^GFP/+ mice were pretreated via the\nmodified SHANEL protocol as described above and incubated with\nAtto647N-conjugated anti-GFP nanobooster (Chromotek, gba647n-100, 1:1,000\ndilution) with 5% 2-hydroxypropyl-\u03b2-cyclodextrin, 0.2% Tween-20 and 6% goat\nserum in PBS for 5 days at 37 \u00b0C. Brains were washed as described in washing\nsolution shaking at 37 \u00b0C for 1 day at which the washing solution was\nrefreshed five times. Brains were dehydrated with an ethanol/dH_2O series\n(50%, 70%, 90% and 100% ethanol) at room temperature for 2 h each step and\nincubated in 100% ethanol overnight. Subsequently, brains were incubated in\nDCM for 1 h before incubation in benzyl alcohol/benzyl benzoate until tissue\ntransparency was reached.\n\n### Light-sheet imaging\n\nLight-sheet imaging for c-Fos labeled brains was conducted through a \u00d74\nobjective lens (Olympus XLFLUOR 340) equipped with an immersion-corrected\ndipping cap mounted on an UltraMicroscope II (LaVision BioTec) coupled to a\nwhite light laser module (NKT SuperK Extreme EXW-12). The antibody signal was\nvisualized using a 640/40 nm excitation and 690/50 nm emission filter. Tiling\nscans (3 \u00d7 3 tiles) were acquired with a 15\u201320% overlap, 60% sheet width and\n0.027 NA. The images were taken in 16-bit depth and at a nominal resolution of\n1.625 \u03bcm per voxel on the xy axes. In the z dimension we took images in 6-\u03bcm\nsteps using left- and right-sided illumination. Whole-brain scans for\nmicroglia-labeled CX3CR1^GFP/+ brains were generated with the LaVision BioTec\nUltramicroscope Blaze coupled with LaVision BioTec MI PLAN \u00d712 objective (0.53\nNA (WD = 10 mm), nominal pixel size of 0.54 \u03bcm in xy). Stitching of tile scans\nwas carried out using Fiji\u2019s stitching plugin, using the \u2018Stitch Sequence of\nGrids of Images\u2019 plugin^45 and custom Python scripts.\n\n### ClearMap\n\nClearMap^7 and the CellMap portion^18 of ClearMap2 were used with adapted\nsettings for thresholds and cell sizes that fitted to the higher resolution\nand different signal-to-noise ratios in our dataset. Segmentation masks were\nsaved as tiff stacks by toggling the \u2018save\u2019 option in the last segmentation\nstep. ClearMap was ported to Python (v.3.5) before use, but functioned\nidentically^46. We only used the cell segmentation portions, no pre-processing\n(for example ClearMap2\u2019s flat-field correction) or post-processing, such as\natlas alignment, were performed. Both pipelines were run for an entire brain\nand subsequently subdivided into test patches that we used for the comparisons\nwith DELiVR. For \u2018optimized ClearMap\u2019^3, we performed the following pre-\nprocessing steps on our image stack: (1) Background equalization to homogenize\nintensity distribution and appearance of the c-Fos^+ cells over different\nregions of the brain, using pseudo-flat-field correction function from Bio-\nVoxxel toolbox (https://doi.org/10.5281/zenodo.5986129). (2) Convoluted\nbackground removal, to remove all particles bigger than relevant cells. This\nwas performed with the median option in the Bio-Voxxel toolbox. (3) A 2D\nmedian filter to remove remaining noise after background removal. (4)\nUnsharpen mask to amplify the high-frequency components of a signal and\nincrease overall accuracy of the cell detection algorithm of ClearMap. (5) A\nz-wise removal of artifacts by manually selecting ROIs in Fiji. After pre-\nprocessing, ClearMap^7 was applied by following the original publication and\nconsidering the threshold levels that we obtained from the pre-processing\nsteps.\n\n### Ventricle masking\n\nWe wrote an automated pre-processing script that downsamples the image stack\nto an isotropic 25 \u00d7 25 \u00d7 25 \u03bcm per voxel and then applies a custom-trained\nrandom forest to identify ventricles. Specifically, we integrated Ilastik^19\n(v.1.4.0b8) with a 3D pixel classifier, which we trained on several\ndownsampled brain image stacks to differentiate between ventricles and brain\nparenchyma. The pre-processing script then generates a 3D mask stack that our\nscript upsamples to the original image stack dimensions, using bicubic\ninterpolation to avoid aliasing artifacts at ventricle edges. It then masks\neach original z-plane image with the respective mask, pads it and returns a\n16-bit image stack (saved as one big .npy file that can be read via\nnp.memmap).\n\n### Annotation\n\nVR annotation for c-Fos^+ cells was carried out using Arivis VisionVR\n(v.3.4.0, Carl Zeiss Microscopy Software Center Rostock) or syGlass (v.1.7.2,\nref. ^12). For this purpose, the annotator was wearing a VR headset (Oculus\nRift S) and carried out annotations in VR using hand controllers (Oculus\nTouch). Slice-by-slice annotation was carried out using ITK-SNAP (v.3.8, ref.\n^11). For comparing VR and 2D-sliced based annotation, a 100^3-voxel volume of\nc-Fos labeled brain was annotated by the participants and the time was\nrecorded until the annotation task was finished. For training and testing our\ndeep-learning network, we annotated a total of 48 \u00d7 1003 voxel patches in VR.\nAll of our training and test patches were furthermore vetted by an expert\nbiologist in ITK-SNAP to ensure that only cells were annotated. We evaluated\nthe annotation quality using the formula of Dice as described below. For more\ndetails about the annotation process in VR, please see our \u2018DELiVR handbook\u2019\nprovided as a Supplementary Note. Microglia cell bodies were annotated in VR\nsimilar to c-Fos^+ cells using Arivis VisionVR. Only the somata were\nannotated, while the microglia processes were excluded.\n\n### Deep learning\n\nTo automatically segment the cells in all brains, we trained a 3D BasicUNet^47\nfor DELiVR from the MONAI library^48. The annotated dataset of 48 \u00d7 1003\npatches was split into nine patches for testing and 39 patches for training\nstratified by signal after manual ventricle masking. As an activation\nfunction, we chose Mish^49 and as optimizer Ranger21 (ref. ^50). As a loss\nfunction, we used binary cross-entropy loss^17. For the training of 500\nepochs, we set the initial learning rate to 1 \u00d7 10^\u22123 and the batch size to\nfour. The network was then trained on a single GPU (NVIDIA RTX8000). Instead\nof conducting model selection, we selected the last checkpoint after 500\nepochs of training. To compare the DELiVR 3D BasicUNet with other segmentation\nmodels, we trained UNETR^15, SegResNet^16and MONAI DynUNET^17 with similar\nspecifications.\n\nThe microglia 3D BasicUNet model was trained in a similar fashion for 500\nepochs using 161 patches containing 3,798 cells. These were split into 129\npatches for training and 32 patches for testing. Training was performed on an\nNVIDIA A100 GPU.\n\n### Evaluation of the segmentation model\n\nEvaluation of the deep-learning model was conducted in a twofold manner.\nFirst, we evaluated the volumetric segmentation quality by assessing, for each\nvoxel, whether it was correctly classified as foreground or background using\npymia^51. A volumetric quality assessment gave us TPs, FPs, FNs and true\nnegatives by comparing every prediction voxel with the reference annotation\nvoxel. Additionally, we conducted an instance-wise assessment of the\nsegmentation quality. Therefore, we assess detection rates on a single-cell\n(instance) level^52. To fairly evaluate every cell irrespective of the patch,\nwe aggregated the counts across all patches and computed the instance metrics\nglobally^53.\n\nVolumetric and instance scores were calculated according to the following\nequations:\n\n$${\\mathrm{Dice}}=\\frac{2\\mathrm{TP}}{2\\mathrm{TP}+\\mathrm{FP}+\\mathrm{FN}}\\qquad{\\mathrm{Sensitvity}}=\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FN}}\\qquad{\\mathrm{Precision}}=\\frac{\\mathrm{TP}}{\\mathrm{TP}+\\mathrm{FP}}$$\n\nComparison with ClearMap, ClearMap2, \u2018Optimized ClearMap\u2019 and Ilastik was\nperformed on a test brain to generate segmentations from which we cropped\n1003-voxel patches to avoid artifacts that occur when the methods are applied\nat the patch level. These patches were then compared to our reference\nannotation using the same metrics as described above.\n\n### Atlas registration and statistical analysis\n\nFor atlas registration, we used mBrainAligner^14, which worked well with our\ndatasets (Supplementary Fig. 1). We manually saved the downsampled isotropic\n25 \u00d7 25 \u00d7 25 \u03bcm per voxel stacks as .v3draw using Vaa3d^54. Subsequently, we\nwrote an automated script that aligned the image stacks to mBrainAligner\u2019s 50\n\u00d7 50 \u00d7 50 \u03bcm per voxel version of the Allen Brain Atlas CCF3 reference atlas,\nusing the LSFM example settings with minor adaptations. Subsequently, we used\nmBrainAligner\u2019s swc transformation tool to map the center-point coordinates of\nour c-Fos^+ cells into atlas space.\n\nFurthermore, we wrote a custom cell-to-atlas script (reusing parser code from\nVeSSAP^55 and the Allen Brain Atlas CCF3 atlas file as provided by the\nScalable Brain Atlas^56) that filters the cells by size, with a user-defined\nupper and lower limit and returns two tables: a table with each cell as a row,\nincluding the region and Allen Brain Atlas color code, etc. and a region table\nwith one region per row, in which the number of c-Fos^+ cells per region is\nsummarized. For all datasets, the post-processing script generates overview\ntables that contain cell counts for all regions. We used the latter for\nuncorrected Student\u2019s t-tests. Finally, we implemented a level-aware multiple-\ntesting script that compares groups at the Allen Brain Atlas\u2019s 11 structure\nlevels. We excluded the fiber tracts from our statistical comparisons.\n\n### Visualization\n\nFor visualizing the cells and regions in atlas space, we used BrainRender^20\n(v.2) with a modified density plot function^46. To visualize the segmented\ncells in the original image space, we combined the area-wise color code from\nthe Allen Brain Atlas with the 3D segment mask output by the connected\ncomponent analysis. The result is a cell mask file with each cell being color\ncoded according to the brain area that it belongs to, which makes overlaying\nwith the original image data in for example Fiji easy and allows for direct\nvisual inspection of the segmentation results. Finally, we used the Allen\nInstitute for Brain Science\u2019s cortical flat-map code (https://github.com/int-\nbrain-lab/atlas) with adaptions^46 to include our heat maps.\n\n### DELiVR Docker and Fiji plugin\n\nWe packaged the DELiVR pipeline as provided in GitHub\n(https://github.com/erturklab/delivr_cfos) into a Docker container (base,\nnvidia/cuda:11.7.2-runtime-ubuntu22.04) including mBrainAligner^14\n(https://github.com/Vaa3D/vaa3d_tools/tree/master/hackathon/mBrainAligner),\nIlastik (https://www.ilastik.org/download.html, v.1.4.0b8) and TeraStitcher\nportable^57 (https://github.com/abria/TeraStitcher/wiki/Binary-\npackages#terastitcher-portable-no-gui-only-command-line-tools, v.1.11.10). The\ncode included Python (v.3.8), PyTorch (v.1.11), PyTorch Lightning (v.2.0.5),\nNibabel (v.5.1.0), MONAI (v.1.2.0), SciPy (v.1.8.1), NumPy (v.1.24.4), Pandas\n(v.1.4.3), imglib2 (https://github.com/imglib/imglib2) and cc3d\n(https://github.com/seung-lab/connected-components-3d). For details, please\nsee the Docker file on GitHub\n(https://github.com/erturklab/delivr_cfos/blob/main/Dockerfile).\n\nWe wrote the Fiji^58 (v.1.52p) plugin in Java (v.1.8, using Maven (v.3.9.5)\nand Jackson, https://github.com/FasterXML/jackson) as a front end. This\nprovides a graphical user interface that compiles a config.json with path\nnames and analysis parameters. Subsequently, the plugin calls the Docker\ncontainer via a shell command and displays the progress of the pipeline. For a\nmore detailed description, please see our \u2018DELiVR handbook\u2019 provided as a\nSupplementary Note.\n\n### Docker for training and Fiji plugin\n\nWe packaged the training code (https://github.com/erturklab/delivr_train) as a\nseparate Docker container, which is also accessible via the Fiji plugin. The\ntraining plugin accepts annotated patches and trains a model specifically for\nthis dataset. This model can then be imported into the inference pipeline for\ndataset-specific inference for any cell type. The Fiji training plugin\ncompiles a config_train.json and arranges the file layout for the training\nDocker. It displays the training progress and shows the final test scores at\nthe end.\n\n### Cell culture\n\nC26 and NC26 colon cancer cells were cultured in high-glucose DMEM with\npyruvate (Life Technologies, 41966052), supplemented with 10% fetal bovine\nserum (Sigma-Aldrich, F7524) and 1% penicillin-streptomycin (Thermo Fisher,\n15140122) as described previously^28,59. Before using the cells for\ntransplantation, cells had a confluence of 80%. Cells were trypsinized,\ncounted and required cell numbers were suspended in Dulbecco\u2019s PBS (Thermo\nFisher, 14190250).\n\n### Animal experimentation\n\nExperiments were carried out with male BALB/c mice aged 10\u201312 weeks. They were\npurchased from Charles River Laboratories, maintained on a 12-h light\u2013dark\ncycle and fed a regular unrestricted chow diet. The set points in the animal\nroom were set to 20\u201324 \u00b0C temperature and 45\u201365% humidity. The mice were\ninjected with 1 \u00d7 10^6 C26 or 1.5 \u00d7 10^6 NC26 colon cancer cells^28,59 in 50\n\u03bcl PBS subcutaneously into the right flank. Control mice were injected with 50\n\u03bcl PBS. After 5 days from cell implantation, mice were monitored daily for\ntumor growth and body weight. Cachectic C26 tumor-bearing mice were considered\ncachectic when they had lost 10\u201315% of body weight. Mice were killed following\ndeep anesthesia with a mix of ketamine/xylazine, followed by intracardiac\nperfusion with heparinized PBS (10 U ml^\u22121 heparin) and by a perfusion with 4%\nparaformaldehyde (PFA). Tissues and organs were dissected, weighed and post-\nfixed at 4 \u00b0C overnight. Animal experimentation was performed in accordance\nwith European Union directives and the German Animal Welfare Act\n(Tierschutzgesetz) and approved by the state ethics committee and the\nGovernment of Upper Bavaria (ROB-55.2-2532.Vet_02-18-93).\n\nThe 6\u20138-week-old CX3CR1^GFP/+ (B6.129P-Cx3cr1tm1Litt/J) mice were purchased\nfrom The Jackson Laboratory (strain code 005582). They were deeply\nanesthetized using a combination of midazolam, medetomidine and fentanyl,\nintracardially perfused with 15 ml 0.01 M PBS solution (10 U ml^\u22121 heparin)\nand 15 ml 4% PFA solution. The brain was dissected, post-fixed in 4% PFA for 6\nh, then proceeded for staining and clearing following the SHANEL protocol.\nCX3CR1^GFP/+ mice were killed for organ withdrawal (T\u00f6tung zu\nWissenschaftlichen Zwecken/Organentnahme) in accordance with the German law\nfor animal experiments (Tierschutzgesetz), paragraph 4, section 3.\n\n### Statistical analysis\n\nResults from biological replicates were expressed as mean \u00b1 s.e.m. Statistical\nanalysis was performed using GraphPad Prism (v.9). Normality was tested using\nShapiro\u2013Wilk normality tests. To compare two conditions, unpaired Student\u2019s\nt-tests or Mann\u2013Whitney U-tests were performed. A one-way ANOVA with Sidak\u2019s\npost hoc test or Kruskal\u2013Wallis tests with Dunn\u2019s multiple comparison test\nwere used to compare three groups. For the c-Fos^+ density comparison between\nareas, we used two-sided t-tests followed by Benjamini\u2013Hochberg multiple-\ntesting correction with a false discovery rate (FWER) of 0.1, as implemented\nin SciPy statsmodels.stats.multitest.multipletests module\n(https://www.statsmodels.org/dev/generated/statsmodels.stats.multitest.multipletests.html).\n\n### Reporting summary\n\nFurther information on research design is available in the Nature Portfolio\nReporting Summary linked to this article.\n\n## Data availability\n\nAll data that support the findings of this study are available from the\ncorresponding author. We provide the numerical source files of all figures in\nthe supplementary material. Our training and test data as well as the trained\nnetwork is available in GitHub at https://github.com/erturklab/delivr_cfos\n(ref. ^60) and https://github.com/erturklab/delivr_train (ref. ^61). A subset\nof representative whole-brain scans is available at the EBI Bioimage\nRepository (accession code S-BIAD1019). Due to limitations to share large\nimaging data online, additional whole-brain scans (n = 27 whole brains, ~2 TB\ndata) will be made available upon reasonable request. The Allen Brain Atlas\n(CCF3) was downloaded from the Scalable Brain Atlas repository at\nhttps://scalablebrainatlas.incf.org/mouse/ABA_v3. Source data are provided\nwith this paper.\n\n## Code availability\n\nAll code to run our pipeline end-to-end is available in GitHub at\nhttps://github.com/erturklab/delivr_cfos (ref. ^60). Training code is\navailable in GitHub at https://github.com/erturklab/delivr_train (ref. ^61).\nDocker containers and the plugin can be obtained from\nhttps://discotechnologies.org/DELiVR. The code is released under the MIT\nlicense.\n\n## References\n\n  1. Ueda, H. R. et al. Tissue clearing and its applications in neuroscience. Nat. Rev. Neurosci. 21, 61\u201379 (2020).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  2. Erturk, A. et al. Three-dimensional imaging of solvent-cleared organs using 3DISCO. Nat. Protoc. 7, 1983\u20131995 (2012).\n\nArticle CAS PubMed Google Scholar\n\n  3. Cai, R. et al. Panoptic imaging of transparent mice reveals whole-body neuronal projections and skull-meninges connections. Nat. Neurosci. 22, 317\u2013327 (2019).\n\nArticle CAS PubMed Google Scholar\n\n  4. Belle, M. et al. Tridimensional visualization and analysis of early human development. Cell 169, 161\u2013173 (2017).\n\nArticle CAS PubMed Google Scholar\n\n  5. Bhatia, H. S. et al. Spatial proteomics in three-dimensional intact specimens. Cell 185, 5040\u20135058 (2022).\n\nArticle CAS PubMed Google Scholar\n\n  6. Molbay, M., Kolabas, Z. I., Todorov, M. I., Ohn, T. L. & Erturk, A. A guidebook for DISCO tissue clearing. Mol. Syst. Biol. 17, e9807 (2021).\n\nArticle PubMed PubMed Central Google Scholar\n\n  7. Renier, N. et al. Mapping of brain activity by automated volume analysis of immediate early genes. Cell 165, 1789\u20131802 (2016).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  8. Nectow, A. R. et al. Identification of a brainstem circuit controlling feeding. Cell 170, 429\u2013442 (2017).\n\nArticle CAS PubMed Google Scholar\n\n  9. Topilko, T. et al. Edinger-Westphal peptidergic neurons enable maternal preparatory nesting. Neuron 110, 1385\u20131399 (2022).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  10. Zhao, S. et al. Cellular and molecular probing of intact human organs. Cell 180, 796\u2013812 (2020).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  11. Yushkevich, P. A. & Gerig, G. ITK-SNAP: An intractive medical image segmentation tool to meet the need for expert-guided segmentation of complex medical images. IEEE Pulse 8, 54\u201357 (2017).\n\nArticle PubMed Google Scholar\n\n  12. Pidhorskyi, S., Morehead, M., Jones, Q., Spirou, G., & Doretto, G. syGlass: interactive exploration of multidimensional images using virtual reality head-mounted displays. Preprint at arXiv https://doi.org/10.48550/arXiv.1804.08197 (2018).\n\n  13. Silversmith, W. cc3d: Connected components on multilabel 3D & 2D images. Version 3.2.1. Zenodo https://doi.org/10.5281/zenodo.5719536 (2021).\n\n  14. Qu, L. et al. Cross-modal coherent registration of whole mouse brains. Nat. Methods 19, 111\u2013118 (2022).\n\nArticle CAS PubMed Google Scholar\n\n  15. Hatamizadeh, A. et al. UNETR: Transformers for 3D Medical Image Segmentation. In IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 1748\u20131758 (2022)\n\n  16. Myronenko, A. 3D MRI brain tumor segmentation using autoencoder regularization. in Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries: 4th International Workshop, BrainLes 2018, Part II, pp. 311\u2013320 (Springer International Publishing, 2018).\n\n  17. Isensee, F., Jaeger, P. F., Kohl, S. A. A., Petersen, J. & Maier-Hein, K. H. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nat. Methods 18, 203\u2013211 (2021).\n\nArticle CAS PubMed Google Scholar\n\n  18. Kirst, C. et al. Mapping the fine-scale organization and plasticity of the brain vasculature. Cell 180, 780\u2013795 (2020).\n\nArticle CAS PubMed Google Scholar\n\n  19. Berg, S. et al. ilastik: interactive machine learning for (bio)image analysis. Nat. Methods 16, 1226\u20131232 (2019).\n\nArticle CAS PubMed Google Scholar\n\n  20. Claudi, F. et al. Visualizing anatomically registered data with brainrender. eLife 10, e65751 (2021).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  21. Merkel, D. Docker: lightweight Linux containers for consistent development and deployment. Linux J. 2014, 2 (2014).\n\n  22. Salter, M. W. & Stevens, B. Microglia emerge as central players in brain disease. Nat. Med. 23, 1018\u20131027 (2017).\n\nArticle CAS PubMed Google Scholar\n\n  23. Kofler, F. et al. Approaching peak ground truth. in 2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI), 1\u20136 (IEEE, 2023).\n\n  24. Porporato, P. E. Understanding cachexia as a cancer metabolism syndrome. Oncogenesis 5, e200 (2016).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  25. Baracos, V. E., Martin, L., Korc, M., Guttridge, D. C. & Fearon, K. C. H. Cancer-associated cachexia. Nat. Rev. Dis. Prim. 4, 17105 (2018).\n\nArticle PubMed Google Scholar\n\n  26. Schmidt, S. F., Rohm, M., Herzig, S. & Berriel Diaz, M. Cancer cachexia: more than skeletal muscle wasting. Trends Cancer 4, 849\u2013860 (2018).\n\nArticle CAS PubMed Google Scholar\n\n  27. Argiles, J. M., Stemmler, B., Lopez-Soriano, F. J. & Busquets, S. Inter-tissue communication in cancer cachexia. Nat. Rev. Endocrinol. 15, 9\u201320 (2018).\n\nArticle PubMed Google Scholar\n\n  28. Morigny, P. et al. High levels of modified ceramides are a defining feature of murine and human cancer cachexia. J. Cachexia Sarcopenia Muscle 11, 1459\u20131475 (2020).\n\nArticle PubMed PubMed Central Google Scholar\n\n  29. Baker, A. et al. Specialized subpopulations of deep-layer pyramidal neurons in the neocortex: bridging cellular properties to functional consequences. J. Neurosci. 38, 5441\u20135455 (2018).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  30. Alexander, A. S. et al. Egocentric boundary vector tuning of the retrosplenial cortex. Sci. Adv. 6, eaaz2322 (2020).\n\nArticle PubMed PubMed Central Google Scholar\n\n  31. Carstensen, L. C., Alexander, A. S., Chapman, G. W., Lee, A. J. & Hasselmo, M. E. Neural responses in retrosplenial cortex associated with environmental alterations. iScience 24, 103377 (2021).\n\nArticle PubMed PubMed Central Google Scholar\n\n  32. Kim, Y. et al. Mapping social behavior-induced brain activation at cellular resolution in the mouse. Cell Rep. 10, 292\u2013305 (2015).\n\nArticle CAS PubMed Google Scholar\n\n  33. Jager, P. et al. Dual midbrain and forebrain origins of thalamic inhibitory interneurons. eLife https://doi.org/10.7554/eLife.59272 (2021).\n\n  34. Tyson, A. L. et al. A deep learning algorithm for 3D cell detection in whole mouse brain image datasets. PLoS Comput. Biol. 17, e1009074 (2021).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  35. Menegas, W. et al. Dopamine neurons projecting to the posterior striatum form an anatomically distinct subclass. eLife 4, e10032 (2015).\n\nArticle PubMed PubMed Central Google Scholar\n\n  36. Matsumoto, K. et al. Advanced CUBIC tissue clearing for whole-organ cell profiling. Nat. Protoc. 14, 3506\u20133537 (2019).\n\nArticle CAS PubMed Google Scholar\n\n  37. Nath, V., Yang, D., Landman, B. A., Xu, D. & Roth, H. R. Diminishing uncertainty within the training pool: active learning for medical image segmentation. IEEE Trans. Med. Imaging 40, 2534\u20132547 (2021).\n\nArticle PubMed Google Scholar\n\n  38. Gal, Y. & Ghahramani, Z. Dropout as a Bayesian approximation: representing model uncertainty in deep learning. in International Conference on Machine Learning, 1050\u20131059. (PMLR, 2015).\n\n  39. Burfeind, K. G., Michaelis, K. A. & Marks, D. L. The central role of hypothalamic inflammation in the acute illness response and cachexia. Semin. Cell Dev. Biol. 54, 42\u201352 (2016).\n\nArticle PubMed Google Scholar\n\n  40. Campos, C. A. et al. Cancer-induced anorexia and malaise are mediated by CGRP neurons in the parabrachial nucleus. Nat. Neurosci. 20, 934\u2013942 (2017).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  41. Winnard, P. T. Jr. et al. Brain metabolites in cholinergic and glutamatergic pathways are altered by pancreatic cancer cachexia. J. Cachexia Sarcopenia Muscle 11, 1487\u20131500 (2020).\n\nArticle PubMed PubMed Central Google Scholar\n\n  42. Rolls, E. T., Cheng, W. & Feng, J. The orbitofrontal cortex: reward, emotion and depression. Brain Commun. 2, fcaa196 (2020).\n\nArticle PubMed PubMed Central Google Scholar\n\n  43. Basu, R. et al. The orbitofrontal cortex maps future navigational goals. Nature 599, 449\u2013452 (2021).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  44. Kann, O. The interneuron energy hypothesis: Implications for brain disease. Neurobiol. Dis. 90, 75\u201385 (2016).\n\nArticle CAS PubMed Google Scholar\n\n  45. Preibisch, S., Saalfeld, S. & Tomancak, P. Globally optimal stitching of tiled 3D microscopic image acquisitions. Bioinformatics 25, 1463\u20131465 (2009).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  46. Negwer, M. et al. FriendlyClearMap: an optimized toolkit for mouse brain mapping and analysis. Gigascience 12, giad035 (2022).\n\nArticle PubMed Google Scholar\n\n  47. Falk, T. et al. U-Net: deep learning for cell counting, detection, and morphometry. Nat. Methods 16, 67\u201370 (2019).\n\nArticle CAS PubMed Google Scholar\n\n  48. Cardoso, J. M. et al. MONAI: an open-source framework for deep learning in healthcare. Preprint at arXiv https://doi.org/10.48550/arXiv.2211.02701 (2022).\n\n  49. Misra, D. A self regularized non-monotonic neural activation function. Preprint at arXiv https://doi.org/10.48550/arXiv.1908.08681 (2019).\n\n  50. Wright, L. & Demeure, N. Ranger21: a synergistic deep learning optimizer. Preprint at arXiv https://doi.org/10.48550/arXiv.2106.13731 (2021).\n\n  51. Jungo, A., Scheidegger, O., Reyes, M. & Balsiger, F. pymia: a Python package for data handling and evaluation in deep learning-based medical image analysis. Comput. Methods Prog. Biomed. 198, 105796 (2021).\n\nArticle Google Scholar\n\n  52. Pan, C. et al. Deep learning reveals cancer metastasis and therapeutic antibody targeting in the entire body. Cell 179, 1661\u20131676 (2019).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  53. Kofler, F. et al. blob loss: instance imbalance aware loss functions for semantic segmentation. in Information Processing in Medical Imaging (Springer, 2023).\n\n  54. Peng, H., Ruan, Z., Long, F., Simpson, J. H. & Myers, E. W. V3D enables real-time 3D visualization and quantitative analysis of large-scale biological image data sets. Nat. Biotechnol. 28, 348\u2013353 (2010).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  55. Todorov, M. I. et al. Machine learning analysis of whole mouse brain vasculature. Nat. Methods 17, 442\u2013449 (2020).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  56. Bakker, R., Tiesinga, P. & Kotter, R. The scalable brain atlas: instant web-based access to public brain atlases and related content. Neuroinformatics 13, 353\u2013366 (2015).\n\nArticle PubMed PubMed Central Google Scholar\n\n  57. Bria, A. & Iannello, G. TeraStitcher: a tool for fast automatic 3D-stitching of teravoxel-sized microscopy images. BMC Bioinform. 13, 316 (2012).\n\nArticle Google Scholar\n\n  58. Schindelin, J. et al. Fiji: an open-source platform for biological-image analysis. Nat. Methods 9, 676\u2013682 (2012).\n\nArticle CAS PubMed Google Scholar\n\n  59. Morigny, P. et al. Association of circulating PLA2G7 levels with cancer cachexia and assessment of darapladib as a therapy. J. Cachexia Sarcopenia Muscle 12, 1333\u20131351 (2021).\n\nArticle PubMed PubMed Central Google Scholar\n\n  60. DELiVR pipeline. (GitHub, 2024); https://zenodo.org/doi/10.5281/zenodo.10908720\n\n  61. Training code. (GitHub, 2024); https://zenodo.org/doi/10.5281/zenodo.10909998\n\nDownload references\n\n## Acknowledgements\n\nWe thank AIME (Berlin) for providing the processing time on their servers. We\nthank I. Horvath, H. Mai and L. K\u00fcmmerle from the Institute for Tissue\nEngineering and Regenerative Medicine (iTERM, Helmholtz Munich) for annotation\ntasks. We thank M. Ali and I. Horvath (iTERM) for helping with initial Docker\ncontainer setup and slurm scripting. We thank L. Harrison from the Institute\nfor Diabetes and Cancer (Helmholtz Munich) for editing the graphical summary.\nFigures 1a and 3a were generated with BioRender.com. We thank R. Zechner and\nM. Schweiger from the Institute of Molecular Biosciences (University of Graz)\nfor kindly providing NC26 cancer cell lines. We thank M. Elsner and F. Hellal\n(iTERM) for proofreading and editing this manuscript. This work was supported\nby the Vascular Dementia Research Foundation, Deutsche Forschungsgemeinschaft\n(DFG, German Research Foundation) under Germany\u2019s Excellence Strategy within\nthe framework of the Munich Cluster for Systems Neurology (EXC 2145 SyNergy,\ngrant no. ID 390857198) and DFG (grant nos. SFB 1052, project A9; TR 296\nproject 03) as well as the German Federal Ministry of Education and Research\n(Bundesministerium f\u00fcr Bildung und Forschung) within the NATON collaboration\n(grant no. 01KX2121) and the HIVacToGC collaboration. This work was also\nsupported by the European Research Council Consolidator grant (CALVARIA, grant\nno. GA 865323 to A.E.) and Nomis Heart Atlas Project Grant (Nomis Foundation).\nThis work was supported by the European Research Council under the European\nUnion\u2019s Horizon 2020 research and innovation program (949017 to M.R.) and a\ngrant from the Else-Kr\u00f6ner-Fresenius-Stiftung (2020 EKSE.23 to S.H.), as well\nas the Edith-Haberland-Wagner Stiftung. B.M., B.W. and F.K. are supported\nthrough the SFB 824, subproject B12, DFG through TUM International Graduate\nSchool of Science and Engineering, GSC 81. B.M. acknowledges support by the\nHelmut Horten Foundation.\n\n## Funding\n\nOpen access funding provided by Helmholtz Zentrum M\u00fcnchen - Deutsches\nForschungszentrum f\u00fcr Gesundheit und Umwelt (GmbH).\n\n## Author information\n\nAuthor notes\n\n  1. These authors contributed equally: Doris Kaltenecker, Rami Al-Maskari, Moritz Negwer.\n\n### Authors and Affiliations\n\n  1. Institute for Diabetes and Cancer (IDC), Helmholtz Munich, Neuherberg, Germany\n\nDoris Kaltenecker, Julia Geppert, Pauline Morigny, Maria Rohm, Stephan Herzig\n& Mauricio Berriel Diaz\n\n  2. Joint Heidelberg-IDC Translational Diabetes Program, Heidelberg University Hospital, Heidelberg, Germany\n\nDoris Kaltenecker, Julia Geppert, Pauline Morigny, Maria Rohm, Stephan Herzig\n& Mauricio Berriel Diaz\n\n  3. German Center for Diabetes Research (DZD), Neuherberg, Germany\n\nDoris Kaltenecker, Julia Geppert, Pauline Morigny, Maria Rohm, Stephan Herzig\n& Mauricio Berriel Diaz\n\n  4. Institute for Stroke and Dementia Research, Klinikum der Universit\u00e4t M\u00fcnchen, Ludwig-Maximilians-Universit\u00e4t LMU, Munich, Germany\n\nDoris Kaltenecker, Rami Al-Maskari, Shan Zhao, Mihail Todorov, Zhouyi Rong &\nAli Ert\u00fcrk\n\n  5. Institute for Tissue Engineering and Regenerative Medicine, Helmholtz Munich, Neuherberg, Germany\n\nRami Al-Maskari, Moritz Negwer, Luciano Hoeher, Shan Zhao, Mihail Todorov,\nZhouyi Rong, Johannes Christian Paetzold & Ali Ert\u00fcrk\n\n  6. Department of Computer Science, TUM Computation, Information and Technology, Technical University of Munich (TUM), Munich, Germany\n\nRami Al-Maskari, Florian Kofler & Bjoern H. Menze\n\n  7. Center for Translational Cancer Research of the TUM (TranslaTUM), Munich, Germany\n\nRami Al-Maskari, Florian Kofler & Johannes Christian Paetzold\n\n  8. Department of Diagnostic and Interventional Neuroradiology, School of Medicine, Klinikum rechts der Isar, Technical University of Munich, Munich, Germany\n\nFlorian Kofler & Benedikt Wiestler\n\n  9. Helmholtz AI, Helmholtz Munich, Neuherberg, Germany\n\nFlorian Kofler & Marie Piraud\n\n  10. Department of Computing, Imperial College London, London, United Kingdom\n\nJohannes Christian Paetzold & Daniel Rueckert\n\n  11. Department for Quantitative Biomedicine, University of Zurich, Zurich, Switzerland\n\nBjoern H. Menze\n\n  12. Chair Molecular Metabolic Control, TU Munich, Munich, Germany\n\nStephan Herzig\n\n  13. School of Medicine, Ko\u00e7 University, \u0130stanbul, Turkey\n\nAli Ert\u00fcrk\n\n  14. Munich Cluster for Systems Neurology (SyNergy), Munich, Germany\n\nAli Ert\u00fcrk\n\n  15. Deep Piction, Munich, Germany\n\nAli Ert\u00fcrk\n\nAuthors\n\n  1. Doris Kaltenecker\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  2. Rami Al-Maskari\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  3. Moritz Negwer\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  4. Luciano Hoeher\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  5. Florian Kofler\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  6. Shan Zhao\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  7. Mihail Todorov\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  8. Zhouyi Rong\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  9. Johannes Christian Paetzold\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  10. Benedikt Wiestler\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  11. Marie Piraud\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  12. Daniel Rueckert\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  13. Julia Geppert\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  14. Pauline Morigny\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  15. Maria Rohm\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  16. Bjoern H. Menze\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  17. Stephan Herzig\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  18. Mauricio Berriel Diaz\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  19. Ali Ert\u00fcrk\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n### Contributions\n\nD.K., R.A., S.H., M.B.D. and A.E. conceptualized the study. A.E. supervised\nand managed the entire project. D.K. performed in vivo work, antibody\nlabeling, imaging and data processing. R.A. performed VR and deep-learning\nanalysis. M.N. performed atlas registration, data analysis and interpreted the\nresults. L.H. performed VR annotations. F.K. trained the deep-learning c-Fos\nmodels and wrote the initial inference pipeline. R.A., M.N. and F.K. improved\nthe pipeline and ran data inference. M.N. and R.A. added visualization. R.A.\nand F.K. compared model performance metrics. R.A. wrote the Fiji plugin. M.N.,\nR.A. and D.K. tested the Fiji plugin. M.N. and F.K. packaged the pipeline in a\nDocker container. D.K., R.A., M.N. and L.H. wrote the DELiVR handbook. S.Z.\nand M.T. supported imaging and antibody labeling. J.G. and P.M. supported in\nvivo experiments. Z.R. provided microglia data. R.A. trained the deep-learning\nmicroglia models. J.C.P. supported data interpretation. M.R. provided funding\nand useful discussions. A.E., M.R.B., S.H., M.R., B.W., D.R., B.M. and M.P.\nprovided funding. S.H. supported supervision of the study. A.E. and M.B.D.\nsupervised the study. D.K., R.A. und M.N. drafted the manuscript. A.E., D.K.,\nM.N. and R.A. revised the manuscript. F.K. provided critical feedback on the\nrevised manuscript.\n\n### Corresponding authors\n\nCorrespondence to Mauricio Berriel Diaz or Ali Ert\u00fcrk.\n\n## Ethics declarations\n\n### Competing interests\n\nA.E. is a co-founder of Deep Piction. The remaining authors declare no\ncompeting interests related to this work.\n\n## Peer review\n\n### Peer review information\n\nNature Methods thanks Hirofumi Kobayashi and the other, anonymous, reviewer(s)\nfor their contribution to the peer review of this work. Primary Handling\nEditor: Nina Vogt, in collaboration with the Nature Methods team.\n\n## Additional information\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional\nclaims in published maps and institutional affiliations.\n\n## Extended data\n\n### Extended Data Fig. 1 VR Segmentation in syGlass and 2D-slice-based\nsegmentation using ITK-SNAP.\n\na, Volume of raw data (c-Fos labeled brain) that was generated by light-sheet\nmicroscopy and loaded into syGlass. Volume size represents 200^3 voxel,\nrendered isotropically. b-d, Using VR, individual cells were segmented in\nsyGlass by using three-dimensional euclidean shapes as ROI and adjusting a\nthreshold until the segmentation was acceptable. Scale bar indicates 5 \u03bcm. e,\nITK-SNAP view of a single plane of the image stack. Cells were labeled in 2D,\nslice by slice. Segmentations are color coded by cell ID.\n\n### Extended Data Fig. 2 DELiVR pre-processing automatically removes\nartefacts.\n\na-c, Horizontal view of an original image slice (a), the proposed mask (b) and\nthe masked image slice generated (c). Scale bar = 1 mm. d, Architecture of the\nc-Fos deep-learning network; a MONAI 3D BasicUNet. e, Quantitative comparison\n(instance precision and sensitivity) of segmentation performance between deep-\nlearning architectures and DELiVR\u2019s 3D BasicUNet. f, Segmentation performance\nof non-deep-learning methods and DELiVR (Scores for DELiVR are the same as\nused in e).\n\nSource data\n\n### Extended Data Fig. 3 Whole-brain segmentation output generated with\nDELiVR.\n\na, 3D visualization of a whole raw light-sheet image stack. b, 3D view of\nwhole-brain segmentation output of detected cells by DELiVR. The area-wise\ncolor code from the Allen Brain Atlas was combined with the 3D segmentation.\nThereby each cell is color coded according to the brain area it was detected\nin. The segmentation of cells is shown in the original image space. Scale bar\n= 500 \u03bcm. c, Visualization of the detected cells in CCF3 atlas space using\nBrainRender (same image as in Fig. 2e). Scale bar = 1 mm.\n\n### Extended Data Fig. 4 Tissue weights of mice with weight-stable cancer\n(NC26) and cancer-associated weight loss (C26).\n\na, Gastrocnemius (GC) muscle weight. n(PBS) = 12, n(NC26) = 8, n(C26) = 12,\n****p < 0.0001,***p = 0.0004, One-way ANOVA with Sidak post hoc analysis. b,\nEpididymal white adipose tissue (eWAT) weight. n(PBS) = 12, n(NC26) = 8,\nn(C26) = 12, **p(PBS vs C26) = 0.0040,**p(NC26 vs C26) = 0.0015, Kruskal-\nWallis test with Dunn \u0301s multiple comparison test. c, Subcutaneous WAT (scWAT)\nweight. ***p = 0.0003, **p = 0.0019, Kruskal-Wallis test with Dunn \u0301s multiple\ncomparison test. d, Brain weight. n(PBS) = 12, n(NC26) = 7, n(C26) = 12, *p =\n0.0479, One-way ANOVA with Sidak post hoc analysis. All data are presented as\nmean values +/\u2212 SEM.\n\nSource data\n\n## Supplementary information\n\n### Supplementary Information\n\nSupplementary Fig.1, Supplementary Table 1 and DELiVR handbook.\n\n### Reporting Summary\n\n### Supplementary Video 1\n\nAnnotation of cells in virtual reality using Arivis VisionVR.\n\n### Supplementary Video 2\n\nAnnotation of cells in VR using syGlass.\n\n### Supplementary Video 3\n\n2D-slice-based annotation using ITK-snap.\n\n### Supplementary Video 4\n\nWhole-brain with c-Fos^+ cells detected by DELiVR depicted in the original\nimage space. Cells are color coded by the area they belong to in the Allen\nBrain Atlas.\n\n### Supplementary Video 5\n\nExample run of the ImageJ plugin on Ubuntu Linux.\n\n## Source data\n\n### Source Data Fig. 1\n\nNumerical source data.\n\n### Source Data Fig. 2\n\nNumerical source data.\n\n### Source Data Fig. 4\n\nNumerical source data.\n\n### Source Data Fig. 5\n\nNumerical source data.\n\n### Source Data Extended Data Fig. 2\n\nNumerical source data.\n\n### Source Data Extended Data Fig. 4\n\nNumerical source data.\n\n## Rights and permissions\n\nOpen Access This article is licensed under a Creative Commons Attribution 4.0\nInternational License, which permits use, sharing, adaptation, distribution\nand reproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the\nCreative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article\u2019s\nCreative Commons licence, unless indicated otherwise in a credit line to the\nmaterial. If material is not included in the article\u2019s Creative Commons\nlicence and your intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this licence, visit\nhttp://creativecommons.org/licenses/by/4.0/.\n\nReprints and permissions\n\n## About this article\n\n### Cite this article\n\nKaltenecker, D., Al-Maskari, R., Negwer, M. et al. Virtual reality-empowered\ndeep-learning analysis of brain cells. Nat Methods (2024).\nhttps://doi.org/10.1038/s41592-024-02245-2\n\nDownload citation\n\n  * Received: 03 June 2022\n\n  * Accepted: 12 March 2024\n\n  * Published: 22 April 2024\n\n  * DOI: https://doi.org/10.1038/s41592-024-02245-2\n\n### Share this article\n\nAnyone you share the following link with will be able to read this content:\n\nSorry, a shareable link is not currently available for this article.\n\nProvided by the Springer Nature SharedIt content-sharing initiative\n\n### Subjects\n\n  * Fluorescence imaging\n  * Machine learning\n  * Neuroscience\n  * Software\n\nDownload PDF\n\n## Associated content\n\n### Simplifying deep learning to enhance accessibility of large-scale 3D brain\nimaging analysis\n\nNature Methods Research Briefing 22 Apr 2024\n\nAdvertisement\n\nNature Methods (Nat Methods) ISSN 1548-7105 (online) ISSN 1548-7091 (print)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n  * Cancel contracts here\n\n\u00a9 2024 Springer Nature Limited\n\nSign up for the Nature Briefing: AI and Robotics newsletter \u2014 what matters in\nAI and robotics research, free to your inbox weekly.\n\nGet the most important science stories of the day, free in your inbox. Sign up\nfor Nature Briefing: AI and Robotics\n\n", "frontpage": false}
