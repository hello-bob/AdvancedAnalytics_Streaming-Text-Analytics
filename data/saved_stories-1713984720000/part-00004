{"aid": "40142160", "title": "AI chatbots refuse to produce 'controversial' output \u2212 that's a problem", "url": "https://theconversation.com/ai-chatbots-refuse-to-produce-controversial-output-why-thats-a-free-speech-problem-226596", "domain": "theconversation.com", "votes": 3, "user": "tomohawk", "posted_at": "2024-04-24 09:07:12", "comments": 0, "source_title": "AI chatbots refuse to produce \u2018controversial\u2019 output \u2212 why that\u2019s a free speech problem", "source_text": "AI chatbots refuse to produce \u2018controversial\u2019 output \u2212 why that\u2019s a free\nspeech problem\n\nMenu Close\n\nAcademic rigour, journalistic flair\n\nAI chatbots restrict their output according to vague and broad policies.\ntaviox/iStock via Getty Images\n\n# AI chatbots refuse to produce \u2018controversial\u2019 output \u2212 why that\u2019s a free\nspeech problem\n\nPublished: April 18, 2024 2.23pm CEST\n\nJordi Calvet-Bademunt, Jacob Mchangama, Vanderbilt University\n\n### Authors\n\n  1. Jordi Calvet-Bademunt\n\nResearch Fellow and Visiting Scholar of Political Science, Vanderbilt\nUniversity\n\n  2. Jacob Mchangama\n\nResearch Professor of Political Science, Vanderbilt University\n\n### Disclosure statement\n\nJordi Calvet-Bademunt is affiliated with The Future of Free Speech. The Future\nof Free Speech is a non-partisan, independent think tank that has received\nlimited financial support from Google for specific projects. However, Google\ndid not fund the report we refer to in this article. In all cases, The Future\nof Free Speech retains full independence and final authority for its work,\nincluding research pursuits, methodology, analysis, conclusions, and\npresentation.\n\nJacob Mchangama is affiliated with The Future of Free Speech. The Future of\nFree Speech is a non-partisan, independent think tank that has received\nlimited financial support from Google for specific projects. However, Google\ndid not fund the report we refer to in this article. In all cases, The Future\nof Free Speech retains full independence and final authority for its work,\nincluding research pursuits, methodology, analysis, conclusions, and\npresentation.\n\n### Partners\n\nVanderbilt University provides funding as a founding partner of The\nConversation US.\n\nView all partners\n\n#####\n\nWe believe in the free flow of information\n\n###### Republish our articles for free, online or in print, under Creative\nCommons licence.\n\nEmail\n\nX (Twitter)\n\nFacebook36\n\nLinkedIn\n\nWhatsApp\n\nMessenger\n\nPrint\n\nGoogle recently made headlines globally because its chatbot Gemini generated\nimages of people of color instead of white people in historical settings that\nfeatured white people. Adobe Firefly\u2019s image creation tool saw similar issues.\nThis led some commentators to complain that AI had gone \u201cwoke.\u201d Others\nsuggested these issues resulted from faulty efforts to fight AI bias and\nbetter serve a global audience.\n\nThe discussions over AI\u2019s political leanings and efforts to fight bias are\nimportant. Still, the conversation on AI ignores another crucial issue: What\nis the AI industry\u2019s approach to free speech, and does it embrace\ninternational free speech standards?\n\nWe are policy researchers who study free speech, as well as executive director\nand a research fellow at The Future of Free Speech, an independent,\nnonpartisan think tank based at Vanderbilt University. In a recent report, we\nfound that generative AI has important shortcomings regarding freedom of\nexpression and access to information.\n\nGenerative AI is a type of AI that creates content, like text or images, based\non the data it has been trained with. In particular, we found that the use\npolicies of major chatbots do not meet United Nations standards. In practice,\nthis means that AI chatbots often censor output when dealing with issues the\ncompanies deem controversial. Without a solid culture of free speech, the\ncompanies producing generative AI tools are likely to continue to face\nbacklash in these increasingly polarized times.\n\n## Vague and broad use policies\n\nOur report analyzed the use policies of six major AI chatbots, including\nGoogle\u2019s Gemini and OpenAI\u2019s ChatGPT. Companies issue policies to set the\nrules for how people can use their models. With international human rights law\nas a benchmark, we found that companies\u2019 misinformation and hate speech\npolicies are too vague and expansive. It is worth noting that international\nhuman rights law is less protective of free speech than the U.S. First\nAmendment.\n\nOur analysis found that companies\u2019 hate speech policies contain extremely\nbroad prohibitions. For example, Google bans the generation of \u201ccontent that\npromotes or encourages hatred.\u201d Though hate speech is detestable and can cause\nharm, policies that are as broadly and vaguely defined as Google\u2019s can\nbackfire.\n\nTo show how vague and broad use policies can affect users, we tested a range\nof prompts on controversial topics. We asked chatbots questions like whether\ntransgender women should or should not be allowed to participate in women\u2019s\nsports tournaments or about the role of European colonialism in the current\nclimate and inequality crises. We did not ask the chatbots to produce hate\nspeech denigrating any side or group. Similar to what some users have\nreported, the chatbots refused to generate content for 40% of the 140 prompts\nwe used. For example, all chatbots refused to generate posts opposing the\nparticipation of transgender women in women\u2019s tournaments. However, most of\nthem did produce posts supporting their participation.\n\nFreedom of speech is a foundational right in the U.S., but what it means and\nhow far it goes are still widely debated.\n\nVaguely phrased policies rely heavily on moderators\u2019 subjective opinions about\nwhat hate speech is. Users can also perceive that the rules are unjustly\napplied and interpret them as too strict or too lenient.\n\nFor example, the chatbot Pi bans \u201ccontent that may spread misinformation.\u201d\nHowever, international human rights standards on freedom of expression\ngenerally protect misinformation unless a strong justification exists for\nlimits, such as foreign interference in elections. Otherwise, human rights\nstandards guarantee the \u201cfreedom to seek, receive and impart information and\nideas of all kinds, regardless of frontiers ... through any ... media of ...\nchoice,\u201d according to a key United Nations convention.\n\nDefining what constitutes accurate information also has political\nimplications. Governments of several countries used rules adopted in the\ncontext of the COVID-19 pandemic to repress criticism of the government. More\nrecently, India confronted Google after Gemini noted that some experts\nconsider the policies of the Indian prime minister, Narendra Modi, to be\nfascist.\n\n## Free speech culture\n\nThere are reasons AI providers may want to adopt restrictive use policies.\nThey may wish to protect their reputations and not be associated with\ncontroversial content. If they serve a global audience, they may want to avoid\ncontent that is offensive in any region.\n\nIn general, AI providers have the right to adopt restrictive policies. They\nare not bound by international human rights. Still, their market power makes\nthem different from other companies. Users who want to generate AI content\nwill most likely end up using one of the chatbots we analyzed, especially\nChatGPT or Gemini.\n\nThese companies\u2019 policies have an outsize effect on the right to access\ninformation. This effect is likely to increase with generative AI\u2019s\nintegration into search, word processors, email and other applications.\n\nThis means society has an interest in ensuring such policies adequately\nprotect free speech. In fact, the Digital Services Act, Europe\u2019s online safety\nrulebook, requires that so-called \u201cvery large online platforms\u201d assess and\nmitigate \u201csystemic risks.\u201d These risks include negative effects on freedom of\nexpression and information.\n\nJacob Mchangama discusses online free speech in the context of the European\nUnion\u2019s 2022 Digital Services Act.\n\nThis obligation, imperfectly applied so far by the European Commission,\nillustrates that with great power comes great responsibility. It is unclear\nhow this law will apply to generative AI, but the European Commission has\nalready taken its first actions.\n\nEven where a similar legal obligation does not apply to AI providers, we\nbelieve that the companies\u2019 influence should require them to adopt a free\nspeech culture. International human rights provide a useful guiding star on\nhow to responsibly balance the different interests at stake. At least two of\nthe companies we focused on \u2013 Google and Anthropic \u2013 have recognized as much.\n\n## Outright refusals\n\nIt\u2019s also important to remember that users have a significant degree of\nautonomy over the content they see in generative AI. Like search engines, the\noutput users receive greatly depends on their prompts. Therefore, users\u2019\nexposure to hate speech and misinformation from generative AI will typically\nbe limited unless they specifically seek it.\n\nThis is unlike social media, where people have much less control over their\nown feeds. Stricter controls, including on AI-generated content, may be\njustified at the level of social media since they distribute content publicly.\nFor AI providers, we believe that use policies should be less restrictive\nabout what information users can generate than those of social media\nplatforms.\n\nAI companies have other ways to address hate speech and misinformation. For\ninstance, they can provide context or countervailing facts in the content they\ngenerate. They can also allow for greater user customization. We believe that\nchatbots should avoid merely refusing to generate any content altogether. This\nis unless there are solid public interest grounds, such as preventing child\nsexual abuse material, something laws prohibit.\n\nRefusals to generate content not only affect fundamental rights to free speech\nand access to information. They can also push users toward chatbots that\nspecialize in generating hateful content and echo chambers. That would be a\nworrying outcome.\n\n  * Artificial intelligence (AI)\n  * Censorship\n  * Human rights\n  * Free speech\n  * Technology\n  * Hate speech\n  * ChatGPT\n  * Generative AI\n  * AI chatbots\n\n### Events\n\nMore events\n\n### Jobs\n\n  * ##### Project Offier - Diversity & Inclusion\n\n  * ##### Senior Lecturer - Earth System Science\n\n  * ##### Sydney Horizon Educators (Identified)\n\n  * ##### Deputy Social Media Producer\n\n  * ##### Associate Professor, Occupational Therapy\n\nMore jobs\n\nCopyright \u00a9 2010\u20132024, The Conversation Media Group Ltd\n\n", "frontpage": false}
