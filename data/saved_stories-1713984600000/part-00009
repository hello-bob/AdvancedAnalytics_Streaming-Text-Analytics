{"aid": "40141938", "title": "Self-Host LLMs with VLLM and BentoML", "url": "https://github.com/bentoml/BentoVLLM", "domain": "github.com/bentoml", "votes": 1, "user": "sherlockxu", "posted_at": "2024-04-24 08:26:46", "comments": 0, "source_title": "GitHub - bentoml/BentoVLLM: Self-host LLMs with vLLM and BentoML", "source_text": "GitHub - bentoml/BentoVLLM: Self-host LLMs with vLLM and BentoML\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nbentoml / BentoVLLM Public\n\n  * Notifications\n  * Fork 6\n  * Star 16\n\nSelf-host LLMs with vLLM and BentoML\n\n16 stars 6 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# bentoml/BentoVLLM\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n7 Branches\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nSherlock113Add Llama 3 requirements installation command (#33)Apr 24,\n202491421f3 \u00b7 Apr 24, 2024Apr 24, 2024\n\n## History\n\n62 Commits  \n  \n### llama2-7b-chat\n\n|\n\n### llama2-7b-chat\n\n| doc: update readme with guided_json and other extra parameters (#29)| Apr\n12, 2024  \n  \n### llama3-8b-instruct\n\n|\n\n### llama3-8b-instruct\n\n| Add Llama 3 requirements installation command (#33)| Apr 24, 2024  \n  \n### mistral-7b-instruct\n\n|\n\n### mistral-7b-instruct\n\n| doc: update readme with guided_json and other extra parameters (#29)| Apr\n12, 2024  \n  \n### mixtral-8x7b-instruct\n\n|\n\n### mixtral-8x7b-instruct\n\n| doc: update readme with guided_json and other extra parameters (#29)| Apr\n12, 2024  \n  \n### outlines-integration\n\n|\n\n### outlines-integration\n\n| docs: Update readmes (#25)| Apr 4, 2024  \n  \n### solar-10.7b-instruct\n\n|\n\n### solar-10.7b-instruct\n\n| doc: update readme with guided_json and other extra parameters (#29)| Apr\n12, 2024  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| Ignore .whl extensions| Feb 21, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| docs: Update readmes (#25)| Apr 4, 2024  \n  \n## Repository files navigation\n\n# Self-host LLMs with vLLM and BentoML\n\nThis is a BentoML example project, showing you how to serve and deploy open-\nsource Large Language Models using vLLM, a high-throughput and memory-\nefficient inference engine.\n\nSee here for a full list of BentoML example projects.\n\n\ud83d\udca1 This example is served as a basis for advanced code customization, such as\ncustom model, inference logic or vLLM options. For simple LLM hosting with\nOpenAI compatible endpoint without writing any code, see OpenLLM.\n\n## Prerequisites\n\n  * You have installed Python 3.8+ and pip. See the Python downloads page to learn more.\n  * You have a basic understanding of key concepts in BentoML, such as Services. We recommend you read Quickstart first.\n  * If you want to test the Service locally, you need a Nvidia GPU with at least 16G VRAM.\n  * (Optional) We recommend you create a virtual environment for dependency isolation for this project. See the Conda documentation or the Python documentation for details.\n\n## Install dependencies\n\n    \n    \n    git clone https://github.com/bentoml/BentoVLLM.git cd BentoVLLM/mistral-7b-instruct pip install -r requirements.txt && pip install -f -U \"pydantic>=2.0\"\n\n## Run the BentoML Service\n\nWe have defined a BentoML Service in service.py. Run bentoml serve in your\nproject directory to start the Service.\n\n    \n    \n    $ bentoml serve . 2024-01-18T07:51:30+0800 [INFO] [cli] Starting production HTTP BentoServer from \"service:VLLM\" listening on http://localhost:3000 (Press CTRL+C to quit) INFO 01-18 07:51:40 model_runner.py:501] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. INFO 01-18 07:51:40 model_runner.py:505] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. INFO 01-18 07:51:46 model_runner.py:547] Graph capturing finished in 6 secs.\n\nThe server is now active at http://localhost:3000. You can interact with it\nusing the Swagger UI or in other different ways.\n\nFor detailed explanations of the Service code, see vLLM inference.\n\n## Deploy to BentoCloud\n\nAfter the Service is ready, you can deploy the application to BentoCloud for\nbetter management and scalability. Sign up if you haven't got a BentoCloud\naccount.\n\nMake sure you have logged in to BentoCloud, then run the following command to\ndeploy it.\n\n    \n    \n    bentoml deploy .\n\nOnce the application is up and running on BentoCloud, you can access it via\nthe exposed URL.\n\nNote: For custom deployment in your own infrastructure, use BentoML to\ngenerate an OCI-compliant image.\n\n## Different LLM Models\n\nBesides the mistral-7b-instruct model, we have examples for other models in\nsubdirectories of this repository. Below is a list of these models and links\nto the example subdirectories.\n\n  * Mistral-7B-Instruct-v0.2\n  * Mixtral-8x7B-Instruct-v0.1 with gptq quantization\n  * Llama-2-7b-chat-hf\n  * SOLAR-10.7B-v1.0\n\n## LLM tools integration examples\n\n  * Every model directory contains codes to add OpenAI compatible endpoints to the BentoML service.\n  * outlines-integration/ contains the code to integrate with outlines for structured generation.\n\n## About\n\nSelf-host LLMs with vLLM and BentoML\n\n### Resources\n\nReadme\n\nActivity\n\nCustom properties\n\n### Stars\n\n16 stars\n\n### Watchers\n\n6 watching\n\n### Forks\n\n6 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Contributors 8\n\n## Languages\n\n  * Python 99.9%\n  * Shell 0.1%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
