{"aid": "40091859", "title": "Simulation framework for accelerating research in Private Federated Learning", "url": "https://apple.github.io/pfl-research/", "domain": "apple.github.io", "votes": 2, "user": "aeontech", "posted_at": "2024-04-19 21:02:47", "comments": 0, "source_title": "pfl 0.1.0 documentation", "source_text": "pfl 0.1.0 documentation\n\npfl 0.1.0 documentation\n\npfl 0.1.0 documentation\n\nGetting Started:\n\n  * Federated learning with pfl\n  * Fast distributed simulations\n\nSupport\n\n  * Installation\n  * Contributing\n\nReference\n\n  * Algorithms\n  * Backend\n  * Aggregator\n  * Data transport\n  * Weighting\n  * Callbacks\n  * Types\n  * Context\n  * Data\n  * Exception\n  * Hyperparameters\n  * Metrics\n  * Models\n  * Postprocessor\n  * Differential privacy\n  * Training statistics\n  * Gradient boosted decision trees\n  * Environment variables\n  * Internal API\n\n    * Bisect\n    * Bridges\n    * Distribution\n    * Ops\n    * Platform\n    * Privacy loss bound\n    * Tree\n\nBack to top\n\n# pfl: Python framework for Private Federated Learning simulations#\n\npfl is a Python framework developed at Apple to enable researchers to run\nefficient simulations with privacy-preserving federated learning (FL) and\ndisseminate the results of their research in FL. The framework is not intended\nto be used for third-party FL deployments but the results of the simulations\ncan be tremendously useful in actual FL deployments. We hope that pfl will\npromote open research in FL and its effective dissemination.\n\npfl provides several useful features, including the following:\n\n  * Get started quickly trying out PFL for your use case with your existing model and data.\n\n  * Iterate quickly with fast simulations utilizing multiple levels of distributed training (multiple processes, GPUs and machines).\n\n  * Flexibility and expressiveness - when a researcher has a PFL idea to try, pfl has flexible APIs to express these ideas and promote their dissemination (e.g. models, algorithms, federated datasets, privacy mechanisms).\n\n  * Fast, scalable simulations for large experiments with state-of-the-art algorithms and models.\n\n  * Support of both PyTorch and TensorFlow. This is great for groups that use both, e.g. other large companies.\n\n  * Unified benchmarks for datasets that has been vetted for both TensorFlow and PyTorch. Current FL benchmarks are made for one or the other.\n\n  * Support of other models in addition to neural networks, e.g. GBDTs. Switching between types of models while keeping the remaining setup fixed is seamless.\n\n  * Tight integration with privacy features, including common mechanisms for local and central differential privacy.\n\nResearchers are invited to contribute to the framework. Please, see\nContributing for more details.\n\nGetting Started:\n\n  * Federated learning with pfl\n\n    * Cross-device federated learning\n    * Preparing data\n    * Defining a model\n    * FL algorithms in pfl\n    * From FL to PFL: Incorporating Privacy\n  * Fast distributed simulations\n\n    * Quickstart\n    * Distributed simulation with Horovod\n    * Distributed simulation with native TF/PyTorch libraries\n    * Central evaluation\n    * Native datasets\n\nSupport\n\n  * Installation\n\n    * Install pfl from PyPi\n    * Install pfl from source\n  * Contributing\n\n    * Setting up development environment\n    * Compiling the documentation\n    * Contributing to code\n\n      * Development process\n      * Standardizing the code\n      * Testing\n      * Package dependencies\n      * Code structure\n      * Making a pull request\n\nReference\n\n  * Algorithms\n\n    * Abstract base classes\n    * Federated learning\n    * Meta-learning / personalisation\n    * Expectation maximization GMM\n    * Utils\n  * Backend\n\n    * Backend\n    * SimulatedBackend\n  * Aggregator\n\n    * Aggregator\n    * SumAggregator\n  * Data transport\n\n    * DataTransport\n    * Float32DataTransport\n    * BFloat16DataTransport\n  * Weighting\n\n    * WeightingStrategy\n    * WeightByUser\n    * WeightByDatapoints\n  * Callbacks\n\n    * TrainingProcessCallback\n    * RestoreTrainingCallback\n    * CentralEvaluationCallback\n    * CentralEvaluationWithEMACallback\n    * ConvergenceCallback\n    * EarlyStoppingCallback\n    * StopwatchCallback\n    * TensorBoardCallback\n    * ModelCheckpointingCallback\n    * ProfilerCallback\n    * AggregateMetricsToDisk\n    * TrackBestOverallMetrics\n    * WandbCallback\n  * Types\n\n    * Population\n    * Saveable\n  * Context\n\n    * UserContext\n    * CentralContext\n  * Data\n\n    * User dataset\n    * Federated dataset\n    * Sampling\n    * Partitioning\n    * User state\n  * Exception\n\n    * PFLError\n    * UserNotFoundError\n    * CheckpointNotFoundError\n    * MatrixFactorizationError\n  * Hyperparameters\n\n    * typing.ParameterType\n    * HyperParam\n    * get_param_value()\n    * HyperParams\n    * AlgorithmHyperParams\n    * ModelHyperParams\n    * NNEvalHyperParams\n    * NNTrainHyperParams\n  * Metrics\n\n    * StringMetricName\n    * MetricName\n    * TrainMetricName\n    * ComposableMetricName\n    * MetricNamePostfix\n    * SkipSerialization\n    * MetricValue\n    * user_average()\n    * get_overall_value()\n    * Weighted\n    * Summed\n    * Histogram\n    * MetricsZero\n    * Metrics\n  * Models\n\n    * Abstract base classes\n    * TensorFlow\n    * PyTorch\n    * Gaussian mixture model\n    * Exponential moving average\n  * Postprocessor\n\n    * Postprocessor\n    * SummaryMetrics\n  * Differential privacy\n\n    * Abstract base classes\n    * Privacy mechanisms\n    * Privacy accountants\n    * DP with adaptive clipping\n    * Approximate local DP with central DP\n    * DP metrics\n    * DP utilities\n  * Training statistics\n\n    * TensorLike\n    * TrainingStatistics\n    * WeightedStatistics\n    * MappedVectorStatistics\n    * ElementWeightedMappedVectorStatistics\n  * Gradient boosted decision trees\n\n    * Algorithms\n    * Models\n    * Utils\n  * Environment variables\n  * Internal API\n\n    * Bisect\n    * Bridges\n    * Distribution\n    * Ops\n    * Platform\n    * Privacy loss bound\n    * Tree\n\n# Indices and tables#\n\n  * Index\n\n  * Module Index\n\n  * Search Page\n\nNext\n\nFederated learning with pfl\n\nCopyright \u00a9 2024 Apple Inc.\n\nMade with Sphinx and @pradyunsg's Furo\n\nLast updated on Jan 26, 2024\n\nOn this page\n\n  * pfl: Python framework for Private Federated Learning simulations\n\n  * Indices and tables\n\n", "frontpage": false}
