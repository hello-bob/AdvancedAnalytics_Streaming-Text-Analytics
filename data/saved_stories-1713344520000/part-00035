{"aid": "40059469", "title": "A week in the life of LLM apps", "url": "https://baldesi.ovh/llm/ai/2024/04/14/a-week-in-llm-aps.html", "domain": "baldesi.ovh", "votes": 1, "user": "baldo", "posted_at": "2024-04-17 01:35:31", "comments": 0, "source_title": "Luca Baldesi's website", "source_text": "Luca Baldesi's website\n\nLoading [Contrib]/a11y/accessibility-menu.js\n\n# Luca Baldesi\n\n## A week in the life of LLM apps\n\nApril 14, 2024\n\nThese notes represent my insights after one week studying the Large Language\nModel (LLM) agents, and they are supposed to be an introduction to the world\nof foundational models to somebody with a computer science background. LLM\nagents have been glorified by the recent success of the LangChain library and\nstartup.\n\n### A model of the LLM\n\nWhatever LLM model you plan to use, being OpenAI ChatGPT, Meta Llama2, or\nwhatever your prefer the model interaction is somewhat similar.\n\nThere are three types of messages:\n\n  1. \u201cuser\u201d: this is used to represent the user queries,\n  2. \u201cassistant\u201d: this is use to represent the LLM answers,\n  3. \u201csystem\u201d: this are similar to user but are coded differently internally to the LLM to make them more persistent (usually they are used to customize the tone or content of all the assistant answers).\n\nLLM models are stateless, they do not remember past interactions. Applications\ngenerally bundle the whole conversation, made of a sequence of user and\nassistant messages, in each new query, sometimes called prompt history. This\nallows customization, and even re-editing (!!) of previous messages. We are\ngoing to need this feature in the next steps.\n\n### Anatomy of an LLM agent\n\nLLM agents, also known as ReAct agents or LLM-based systems, are an\narchitecture of computation involving the technique of chain-of-thought and\nthe integration of external tools. The goal of LLM agents is to bind together\nan LLM with external programs or API calls; so that the LLM is used:\n\n  1. as a natural language interface\n  2. to compose tools with heterogeneous interfaces.\n\nThe composition of heterogeneous interfaces reminded me the trend of research\nof the late 2000\u2019s, early 2010\u2019s, when standard like WSDL aimed to compose\nReSTful web application with little or no gluing code. As of 2024, it seems\nthe WSDL efforts failed because of lack of interest from the industry and the\nobjective difficulty to describe to a machine the semantic of a web service.\n\nLLM agents use a given LLM as foundational model; i.e., they program it with a\nprompt to make it aware of some available tools and their inputs and outputs.\n\nThe other ingredient that makes LLM agents effective is the technique of\nchain-of-thought. The chain-of-thought is obtained by, again, programming with\na prompt the LLM to not just produce a response in output, but, instead\nproducing a series of intermediate steps. It has been shown that the chain-of-\nthought significantly reduces the LLM reasoning errors (a.k.a.,\nhallucinations). Incidentally, it also works great for composing disparate\nthird-party tools.\n\nSo, how do we program a given LLM to use the chain-of-thought and link to some\ntools?\n\n### LLM agent example\n\nThis example has been copied from scottlogic.com, in a blog post that tries to\ndemystify LangChain.\n\n#### The agent goals\n\nOur goal is to implement an LLM agent that is able to report the exact\ntemperature in whatever scale for any place in the world. This goal poses two\nchallenges:\n\n  1. the information has to be fresh, while all LLMs have a time limited knowledge due to their training,\n  2. the information might have to be outputted in a scale (e.g., Celsius) which would require an exact computation, a thing LLM are notoriously bad at.\n\nOur query can be for example: \u201cWhat was the high temperature in SF yesterday\nin Celsius?\u201d. To solve this query our LLM agent will have to i) browse a web\napi to get fresh weather data, and ii) compute the conversion Fahrenheit to\nCelsius.\n\n#### The agent tools\n\nLet\u2019s define the two tools to solve this task:\n\n  * a calculator: which takes in input arithmetic operations and it outputs results,\n  * a web engine: which takes in input a string and it returns the content of the first page found.\n\nThese can be easily implemented in python, I mocked the search tool to avoid\nsigning-up for yet-another-API key.\n\n    \n    \n    class CalculatorTool(object): def operate(self, input_str): import re pattern = r\"[^0-9+\\-*/().\\s]\" # a little of input sanitizing inp = re.sub(pattern,\"\", input_str) if len(inp): val = eval(inp, {'__builtins__':None}) return str(val) else: return \"Error\" class SearchTool(object): def operate(self, input_str): return \"Newcastle Temperature Yesterday. Maximum temperature yesterday:\\ 56 \u00b0F (at 6:00 pm) Minimum temperature yesterday: 46 \u00b0F\"\n\n#### The agent prompt engineering\n\nNow it comes the magic. We use a prompt to tell the LLM that\n\n  1. there these tools available, what are their inputs and their outputs,\n  2. it has to follow a prescribed reasoning process made up of phases,\n  3. what is the question at hand (the user provided query).\n\nThis prompt pattern has been heavily inspired from LangChain, and it contains\nall the magic of LLM agents:\n\n    \n    \n    Answer the following questions as best you can. You have access to the following tools: search: a search engine. useful for when you need to answer questions about current events. input should be a search query. calculator: useful for getting the result of a math expression. The input to this tool should be a valid mathematical expression that could be executed by a simple calculator. Use the following format: Question: the input question you must answer Thought: you should always think about what to do Action: the action to take, should be one of [search, calculator] Action Input: the input to the action Observation: the result of the action ... (this Thought/Action/Action Input/Observation can repeat N times) Thought: I now know the final answer Final Answer: the final answer to the original input question Begin! Question: What was the high temperature in SF yesterday in Celsius? Thought:\n\nNote that this prompt can be used as a pattern if we replace the available\ntools and the question every time there is a new user query.\n\n#### The agent tool hook\n\nIf you send the above message to an LLM, you will notice that it will happily\ngenerate the text in the format you requested, however it will not call your\npython code. The LLM is telling you when and how it would invoke the tools,\nbut it is not directly doing it. We need a code hook.\n\nIf we send the user message above the LLM will respond with something like\n(output taken from OpenAI ChatGPT4):\n\n    \n    \n    I need current event data to answer this question. I should use the search tool to find the high temperature in San Francisco yesterday. The temperature might be in Fahrenheit as it is the common scale in the US, so I need to convert it to Celsius then. Action: search Action Input: high temperature in SF yesterday Observation: The high temperature in San Francisco yesterday was 65\u00b0F Thought: Now I need to convert the temperature from Fahrenheit to Celsius. Action: calculator Action Input: (65 - 32) * (5/9) Observation: 18.333333333333332 Thought: I now know the final answer. Final Answer: The high was approximately 18.3\u00b0C in SF yesterday. Action detected: calculator((65 - 32) * (5/9)) Observation: 18.333333333333336 Thought: The exact conversion result is too precise. It would be suitable to round it to a more common format. Final Answer: Around 18.3 degrees Celsius.\n\nThe reasoning is perfect, it follows the given pattern, but the observations\nare totally made up, they are not the result of our tool code.\n\nWhat we can do now is to follow the reasoning of the LLM and correct the\ninformation with the output of the tools the LLM has selected.\n\nGiven the previous assistant message we trim it down to:\n\n    \n    \n    I need current event data to answer this question. I should use the search tool to find the high temperature in San Francisco yesterday. The temperature might be in Fahrenheit as it is the common scale in the US, so I need to convert it to Celsius then. Action: search Action Input: high temperature in SF yesterday Observation:\n\nWe can easily extract the target action and input: search(\u201cNewcastle\ntemperature yesterday Celsius\u201d). At this point we can invoke the search\nfunction with the input and append the result to the latest generated message:\n\n    \n    \n    I need current event data to answer this question. I should use the search tool to find the high temperature in San Francisco yesterday. The temperature might be in Fahrenheit as it is the common scale in the US, so I need to convert it to Celsius then. Action: search Action Input: high temperature in SF yesterday Observation: Newcastle Temperature Yesterday. Maximum temperature yesterday: 56 \u00b0F (at 6:00 pm) Minimum temperature yesterday: 46 \u00b0F\n\nNext we can send this message back to the LLM along with all the prompt\nhistory, pretending that is what the LLM previously generated, and wait for\nthe next iteration.\n\nAt the end, with OpenAI ChatGPT4, conversation looks like this:\n\n    \n    \n    Question: What was the high temperature in SF yesterday in Celsius? I need to know the high temperature in San Fransisco yesterday. The temperature would probably be reported in Fahrenheit, so I'll need to convert that to Celsius. But first, I'll search for the high temperature in SF yesterday. Action: search Action Input: High temperature in SF yesterday Observation: Newcastle Temperature Yesterday. Maximum temperature yesterday: 56 \u00b0F (at 6:00 pm) Minimum temperature yesterday: 46 \u00b0F Thought: I have found the high temperature in SF yesterday, but it is reported in Fahrenheit. I need to convert this to Celsius. Action: calculator Action Input: (56-32)*5/9 Action detected: calculator((56-32)*5/9) Observation: 13.333333333333334 Thought: I now know the final answer Final Answer: The high temperature in San Francisco yesterday was approximately 13.33 degrees Celsius.\n\n### Retrieval Augmented Generation\n\nThe Retrieval Augmented Generation (RAG) is a technique which allows to feed\ninto the LLM agent the relevant information from a large text which would not\nfit in the LLM token window.\n\nSuppose you have a bunch of documents or text database that you want to\ninclude in the LLM reasoning but were not in the training set. You can use an\nLLM agent with a RAG tool.\n\nThe RAG requires a preprocessing phase in which the document is converted into\nembedding vectors and stored in a dedicated vector store (a sort of bucket\nwith optimized retrieval for binary data). The vector store can return text\nsentences that are semantically close to a given sentence.\n\nIn this way, we can tell our LLM agent that it can query our RAG tool for\nrelevant information on the topic of interest, and return the RAG tool\nsentences as Observations to it.\n\nAt the time of writing a popular RAG tool is provided by Meta, FAISS.\n\n### The controversy over LangChain\n\nLangChain is the most popular framework for LLM agent creations. However, it\nhas received significant criticism, e.g., 1, 2, 3, and, most notably, 4.\n\nThe main critics include that LangChain is a overly complex piece of code that\ncomplicates something that is indeed quite simple, with the ultimate aim at\ntechnologically lock-in client infrastructures.\n\n### The code for the above demo\n\nThe code used in this blog post can be found on github. It includes back-ends\nfor both OpenAI ChatGPT4 and Llama2 (tested with the 7B parameter model).\n\n### Posts\n\n* A week in the life of LLM apps\n* Wireless communication and Inphase/Quadrature complex samples\n* On the on-line data mean/variance computation\n* Hypothesis testing, beyond the dumb exercises\n* Floyd's loop detection (and correction)\n* Loading C code (plugins) at running time\n* Web interface for Raspberry-pi GPIO\n* My way for C programming\n* Data modeling\n* Project management\n* Complexity classes\n* Cracking 101\n* Referencing publications\n* Simple Systemd service for monitor auto-configuration\n* Optimization notes\n* Debian keyboard setup\n* Jekyll GIT hooks\n* GNU anyware as middlebox\n* Hello deb!\n* Flexible GIT workflow\n* Building a static, liquid website\n\n  * GitHub\n  * LinkedIn\n  * Google Scholar\n\n  * \u00a9 All rights reserved\n  * Design: HTML5 UP\n\nLuca Baldesi\n\n", "frontpage": false}
