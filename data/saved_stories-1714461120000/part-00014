{"aid": "40203044", "title": "Show HN: Attorch \u2013 PyTorch's nn module written in Python using OpenAI's Triton", "url": "https://github.com/BobMcDear/attorch", "domain": "github.com/bobmcdear", "votes": 1, "user": "bornaahz", "posted_at": "2024-04-29 19:37:19", "comments": 0, "source_title": "GitHub - BobMcDear/attorch: A subset of PyTorch's neural network modules, written in Python using OpenAI's Triton.", "source_text": "GitHub - BobMcDear/attorch: A subset of PyTorch's neural network modules,\nwritten in Python using OpenAI's Triton.\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nBobMcDear / attorch Public\n\n  * Notifications\n  * Fork 15\n  * Star 371\n\nA subset of PyTorch's neural network modules, written in Python using OpenAI's\nTriton.\n\n### License\n\nMIT license\n\n371 stars 15 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# BobMcDear/attorch\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nBobMcDearFeat: Add fused dropout support to activation layersApr 29,\n202415c84c7 \u00b7 Apr 29, 2024Apr 29, 2024\n\n## History\n\n111 Commits  \n  \n### attorch\n\n|\n\n### attorch\n\n| Feat: Add fused dropout support to activation layers| Apr 29, 2024  \n  \n### examples\n\n|\n\n### examples\n\n| Docs: Add Imagenette training example| Apr 1, 2024  \n  \n### tests\n\n|\n\n### tests\n\n| Test: Add tests for multi-headed attention layer| Apr 15, 2024  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Docs: Add MIT license| Aug 3, 2023  \n  \n### README.md\n\n|\n\n### README.md\n\n| Docs: Add multi-headed attention layer to docs| Apr 22, 2024  \n  \n## Repository files navigation\n\n# attorch\n\n\u2022 Introduction \u2022 Installation \u2022 Layers \u2022 PyTorch Fallback \u2022 Tests\n\n## Introduction\n\nattorch is a subset of PyTorch's nn module, written purely in Python using\nOpenAI's Triton. Its goal is to be an easily hackable, self-contained, and\nreadable collection of neural network modules whilst maintaining or improving\nupon the efficiency of PyTorch. In other words, it intends to be a forkable\nproject endowed with a simple, intuitive design that can serve as an\naccessible starting point for those who are seeking to develop custom deep\nlearning operations but are not satisfied with the speed of a pure PyTorch\nimplementation and do not have the technical expertise or resources to write\nCUDA kernels.\n\nThere already exist a number of wonderful PyTorch-like frameworks powered by\nTriton, but most concentrate solely on Transformers and NLP applications,\nwhereas attorch aims to be more inclusive by also presenting a variety of\nlayers pertaining to areas besides NLP such as computer vision. Moreover,\nattorch is not an inference-only package and fully supports both forward and\nbackward passes, meaning it can be used during training as well as inference,\nthough its performance for the latter is generally not on par with dedicated\ninference engines.\n\n## Installation\n\nThe only dependencies of attorch are torch==2.2.0 and triton==2.2.0. Please\ninstall the specified versions of these two libraries and clone this\nrepository to get started.\n\n## Layers\n\nCurrently implemented layers, with automatic mixed precision (AMP) support,\nare,\n\n  * attorch.Conv2d: 2D-convolves over the input using weights, optionally adding bias.\n  * attorch.MultiheadAttention: Applies multi-headed scaled dot-product attention to the inputs.\n  * attorch.GELU: Applies GELU to the input.\n  * attorch.ReLU: Applies ReLU to the input.\n  * attorch.SiLU: Applies SiLU to the input.\n  * attorch.Sigmoid: Applies sigmoid to the input.\n  * attorch.Tanh: Applies tanh to the input.\n  * attorch.LogSoftmax: Normalizes the input using softmax and takes its log.\n  * attorch.Softmax: Normalizes the input using softmax.\n  * attorch.Softmin: Normalizes the input using softmin.\n  * attorch.BatchNorm1d: Batch-normalizes the 2D or 3D input, optionally fusing an activation function and adding a residual to the pre-activation result.\n  * attorch.BatchNorm2d: Batch-normalizes the 4D input, optionally fusing an activation function and adding a residual to the pre-activation result.\n  * attorch.LayerNorm: Layer-normalizes the input.\n  * attorch.Linear: Linearly transforms the input using weights, optionally adding bias and fusing an activation function.\n  * attorch.Dropout: Randomly zeroes elements in the input during training.\n  * attorch.L1Loss: Measures the mean absolute error between the input and target.\n  * attorch.MSELoss: Measures the mean squared error between the input and target.\n  * attorch.CrossEntropyLoss: Measures the mean cross entropy loss between the input and target, with optional reweighing of each class.\n  * attorch.NLLLoss: Measures the negative log likelihood loss between the input and target, with optional reweighing of each class.\n\nUnless otherwise noted in their docstrings, the aforementioned layers behave\nidentically to their PyTorch equivalents.\n\n## PyTorch Fallback\n\nTo enable easier integration of attorch and PyTorch layers, attorch.nn is\noffered, which provides an interface to attorch's modules with PyTorch\nfallback should a desired layer not be available, as seen below.\n\n    \n    \n    from attorch import nn lin = nn.Linear(10, 20) # Uses attorch's linear layer gap = nn.AdaptiveAvgPool2d(1) # Uses PyTorch's global pooling since GAP is not available in attorch\n\n## Tests\n\nEach module can be tested against its PyTorch counterpart to ensure\ncorrectness. These tests are included under tests/ and can be executed using\npytest. It should be noted that some might fail owing to numerical precision\nissues, but in most practical use cases, that should not be a problem.\n\n## About\n\nA subset of PyTorch's neural network modules, written in Python using OpenAI's\nTriton.\n\n### Topics\n\nmachine-learning deep-learning cuda pytorch openai triton openai-triton\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\nActivity\n\n### Stars\n\n371 stars\n\n### Watchers\n\n7 watching\n\n### Forks\n\n15 forks\n\nReport repository\n\n## Languages\n\n  * Python 100.0%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
