{"aid": "40216758", "title": "Scalable AI Architectures", "url": "https://www.8vc.com/resources/scalable-ai-architectures", "domain": "8vc.com", "votes": 1, "user": "mehulashah", "posted_at": "2024-04-30 21:45:41", "comments": 0, "source_title": "Scalable AI Architectures | Posts | 8VC", "source_text": "Scalable AI Architectures | Posts | 8VC\n\nPortfolio\n\nTeam\n\nResources\n\nAbout\n\nJOBS\n\nback to resources\n\n##### Share\n\nTwitter\n\nLinkedIn\n\nEmail\n\nEmail\n\nLinkedIn\n\nTwitter\n\nClose\n\n# Scalable AI Architectures\n\nPosts\n\nApr 30, 2024\n\n##### Share\n\nKevin Niechen\n\nInvestment Team\n\nPosts\n\nApr 30, 2024\n\n##### Share\n\nToday, two neural architectures scale to the limits of available data and\ncompute:\n\n  1. Transformers\n  2. Diffusion\n\nThere\u2019s a myth spreading that scalable architectures aren\u2019t rare, and that\nevery architecture scales with enough optimization. However, decades of\nresearch have revealed countless architectures \u2013 inspired by all manner of\nphysics, neuroscience, fruit fly mating habits, and mathematics \u2013 that don\u2019t\nscale.\n\nEven if data and compute are the most urgent bottlenecks in advancing today\u2019s\ncapabilities, we shouldn\u2019t take scalable architectures for granted. We should\nvalue them, advance them, and find better architectures that address their\nlimitations.\n\nNeural architectures that don\u2019t (black) and do (blue) scale\n\nFinding new scalable architectures doesn\u2019t necessarily require massive\ntraining runs, thanks to scaling laws. Scaling laws are relationships that\npredict how model performance will change as dataset size, model size, and\ntraining costs vary. In other words, if we plot an architecture\u2019s performance\nat enough small training runs along those dimensions, we can predict its\nperformance at much larger runs.\n\nFor many neural architectures, scaling laws level off well below the 1\nbillion-parameter mark. Kaplan\u2019s 2020 paper, \u201cScaling Laws for AI Language\nModels\u201d, shows the scaling laws of traditional LSTMs plateauing at just 10\nmillion parameters. Tay\u2019s 2022 paper produces scaling laws for ten different\narchitectures, and shows five non-transformer architectures\u2019 scaling laws\nplateauing well before the 1 billion-parameter mark.\n\nAs a result, ruling out new architectures by examining their scaling laws can\nbe quite cheap. According to Databricks, a 1 billion-parameter training run\ncosts only $2,000 (source).\n\nTraditional LSTM\u2019s scaling law plateauing in \u201cScaling Laws for AI Language\nModels\u201d (Kaplan, 2020)Non-transformers\u2019 scaling laws plateauing in \u201cScaling\nLaws vs Model Architectures\u201d (Yi Tay, 2022)\n\nIn fact, it was OpenAI\u2019s 2019 GPT- 2 transformer training run at 1.5 billion\nparameters that gave them the confidence to invest in a GPT-3 training run at\n175 billion parameters, over two orders of magnitude larger:\n\n  1. Scaling laws: \u201cby the time we made GPT-2... you could look at the scaling laws and see what was going to happen.\u201d - Sam Altman, On with Kara Swisher, 3/23/23\n  2. Benchmarks: \u201cstate-of-the-art on Winograd Schema, LAMBADA\u201d - OpenAI Announcement\n  3. Emergent capabilities: \u201ccapable of generating samples... that feel close to human quality\u201d - OpenAI Announcement\n\nGPT scaling laws in \u201dScaling Laws for AI Language Models\u201d (Kaplan, 2020)\n\nThe straightforward next step to improve model capabilities is continuing to\nscale the architectures that work, by increasing the size of our data centers\nand datasets. However, these architectures have major limitations, such as\nlogarithmic scaling, and limited context, which means there's still an\nopportunity to innovate in core architecture research.\n\nPositively bending our best scaling laws would help not only at the top end,\nby increasing the capabilities of models trained at the largest data centers\non the largest datasets, but also at the middle and low ends, by allowing\ncapable models to be used at affordable prices.\n\nSome of the most exciting lines of work to improve existing scalable\narchitectures include:\n\n  1. Planning (e.g., tree search, reinforcement learning)\n  2. Longer context (e.g., Stanford\u2019s FlashAttention, Google\u2019s RingAttention)\n  3. Ensembling (e.g., Google\u2019s mixture of experts, Sakana\u2019s evolutionary model merging)\n\nThere are also early candidates for new scalable architectures:\n\n  1. State-space models (e.g., Stanford\u2019s S4, Hyena, Cartesia\u2019s Mamba, AI2\u2019s Jamba)\n  2. RNN variants (e.g., Bo Peng\u2019s RKVW)\n  3. Striped variants of the above with existing architectures\n\nNotably, these new architectures are mostly driven by small teams outside the\nmajor industry labs. For example, S4 has three authors, Mamba has two authors,\nand RKVW is primarily developed by Bo Peng.\n\nThese emerging architectures are already proving useful in real-world domains.\nFor example, Arc Institute's Evo, a 7B biological model, is based on an\narchitecture that avoids the quadratic complexity of attention, allowing it to\nprocess vast volumes of biological sequence data within a 131K context window.\n\nWe're still early in the history of AI, and our architectures are far from\nphysics-limit optimal. While scaling via data center buildouts and data\nacquisition is the clear next step in advancing model capabilities, there are\nstill major breakthroughs to come from the scalable architectures themselves.\n\n\u2013\n\nNotes\n\n  1. I opted for simplicity over accuracy in the discussion of scaling laws. In reality, they\u2019re not as well-understood as many believe. We only know a small sliver of how models scale above a certain level, and it\u2019s hard to answer questions beyond \u201cWhat is the best model I can train with a fixed budget of $__?\u201d. Factors like data quality are also hard to quantify.\n  2. Scaling law papers often prioritize speed over rigor (see failed replications), because the field is moving so fast.\n  3. What will be the real bottlenecks in scaling known architectures? Data? GPU supply? Energy? Dollars? Data centers? Researchers?\n  4. What should small teams outside the major industry labs work on?\n  5. There are many other promising approaches to improving models that don't involve modifying the architecture, like novel training methods (e.g. UL2R), better optimization, synthetic data, data curation, compression, and alignment.\n\n\u2013\n\nThanks to friends and colleagues who provided feedback on early drafts of\nthis.\n\n##### Share\n\n## Continue Reading\n\nPosts\n\nInterview\n\nApr 25, 2024\n\n#### Karim Atiyeh and Nik Koblov (Ramp) Fireside Chat\n\nToday, we\u2019re pleased to share the transcript from that conversation. Karim and\nNik share numerous insights on Ramp\u2019s journey, which recently marked five\nyears and ...\n\nPosts\n\nNews\n\nApr 11, 2024\n\n#### Announcing Our Investment in PeerDB\n\nWhile models command the spotlight, the AI age ultimately depends on real-\ntime, reliable data access - and this is true of countless traditional\nbusiness processes...\n\nPosts\n\nNews\n\nFeb 27, 2024\n\n#### OpenGov: A Changing Guard; A Continuing Mission\n\nToday, OpenGov announced its acquisition by Cox Enterprises for $1.8 billion\nUSD. ...\n\nPosts\n\nInterview\n\nFeb 7, 2024\n\n#### Nik Spirin (NVIDIA) Fireside Chat\n\nFor our final Chat8VC of 2023, we hosted Dr. Nik Spirin, Director of\nGenerative AI and LLMOps at NVIDIA. He sits at a very compelling vantage point\nin this role. P...\n\nPosts\n\nNews\n\nJan 30, 2024\n\n#### A Survey Of Large Language Models Accelerating Healthcare Businesses\nToday\n\n...\n\nPosts\n\nInterview\n\nNov 28, 2023\n\n#### Michel Tricot (Airbyte) Fireside Chat\n\nOur story is certainly full of ups and downs. Airbyte today is often\nassociated with our open source data infrastructure project and product. It\nreally started tho...\n\n[mORE RESOURCES]\n\nback to RESOURCES\n\n#### Links\n\nhomeportfolioResources8VC angelcontact\n\n#### Company\n\naboutteamjobs\n\n#### Programs\n\nfellowshipBio-IT fellowshipbuildChat 8vc community\n\nprivacycookies\n\n#### Contact\n\n907 South Congress Avenue, Austin, TX 78704\n\ntwitterInvestors\n\n\u00a92024 8VC. All Rights Reserved.\n\n# Notice\n\nWe and selected third parties use cookies or similar technologies for\ntechnical purposes and, with your consent, for experience, measurement and\nmarketing (personalized ads) as specified in the cookie policy.\n\nWith respect to advertising, we and 851 selected , may use precise geolocation\ndata, and identification through device scanning in order to store and/or\naccess information on a device and process personal data like your usage data\nfor the following : personalised advertising and content, advertising and\ncontent measurement, audience research and services development.\n\nYou can freely give, deny, or withdraw your consent at any time by accessing\nthe preferences panel. If you give consent, it will be valid only in this\ndomain. Denying consent may make related features unavailable.\n\nUse the \u201cAccept\u201d button to consent. Use the \u201cReject\u201d button to continue\nwithout accepting.\n\nPress again to continue 0/1\n\n", "frontpage": false}
