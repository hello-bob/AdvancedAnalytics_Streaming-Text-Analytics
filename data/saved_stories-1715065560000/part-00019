{"aid": "40278452", "title": "Google \u2013 Site Reliability Engineering", "url": "https://sre.google/sre-book/addressing-cascading-failures/#xref_cascading-failure_load-shed-graceful-degredation", "domain": "sre.google", "votes": 2, "user": "hui-zheng", "posted_at": "2024-05-06 19:18:39", "comments": 0, "source_title": "Google - Site Reliability Engineering", "source_text": "Google - Site Reliability Engineering\n\n## Chapter 22 - Addressing Cascading Failures\n\n  1. Table of Contents\n  2. Foreword\n  3. Preface\n  4. Part I - Introduction\n  5. 1\\. Introduction\n  6. 2\\. The Production Environment at Google, from the Viewpoint of an SRE\n  7. Part II - Principles\n  8. 3\\. Embracing Risk\n  9. 4\\. Service Level Objectives\n  10. 5\\. Eliminating Toil\n  11. 6\\. Monitoring Distributed Systems\n  12. 7\\. The Evolution of Automation at Google\n  13. 8\\. Release Engineering\n  14. 9\\. Simplicity\n  15. Part III - Practices\n  16. 10\\. Practical Alerting\n  17. 11\\. Being On-Call\n  18. 12\\. Effective Troubleshooting\n  19. 13\\. Emergency Response\n  20. 14\\. Managing Incidents\n  21. 15\\. Postmortem Culture: Learning from Failure\n  22. 16\\. Tracking Outages\n  23. 17\\. Testing for Reliability\n  24. 18\\. Software Engineering in SRE\n  25. 19\\. Load Balancing at the Frontend\n  26. 20\\. Load Balancing in the Datacenter\n  27. 21\\. Handling Overload\n  28. 22\\. Addressing Cascading Failures\n  29. 23\\. Managing Critical State: Distributed Consensus for Reliability\n  30. 24\\. Distributed Periodic Scheduling with Cron\n  31. 25\\. Data Processing Pipelines\n  32. 26\\. Data Integrity: What You Read Is What You Wrote\n  33. 27\\. Reliable Product Launches at Scale\n  34. Part IV - Management\n  35. 28\\. Accelerating SREs to On-Call and Beyond\n  36. 29\\. Dealing with Interrupts\n  37. 30\\. Embedding an SRE to Recover from Operational Overload\n  38. 31\\. Communication and Collaboration in SRE\n  39. 32\\. The Evolving SRE Engagement Model\n  40. Part V - Conclusions\n  41. 33\\. Lessons Learned from Other Industries\n  42. 34\\. Conclusion\n  43. Appendix A. Availability Table\n  44. Appendix B. A Collection of Best Practices for Production Services\n  45. Appendix C. Example Incident State Document\n  46. Appendix D. Example Postmortem\n  47. Appendix E. Launch Coordination Checklist\n  48. Appendix F. Example Production Meeting Minutes\n  49. Bibliography\n\n# Addressing Cascading Failures\n\nWritten by Mike Ulrich\n\n> If at first you don't succeed, back off exponentially.\n>\n> Dan Sandler, Google Software Engineer\n\n> Why do people always forget that you need to add a little jitter?\n>\n> Ade Oshineye, Google Developer Advocate\n\nA cascading failure is a failure that grows over time as a result of positive\nfeedback.^107 It can occur when a portion of an overall system fails,\nincreasing the probability that other portions of the system fail. For\nexample, a single replica for a service can fail due to overload, increasing\nload on remaining replicas and increasing their probability of failing,\ncausing a domino effect that takes down all the replicas for a service.\n\nWe\u2019ll use the Shakespeare search service discussed in Shakespeare: A Sample\nService as an example throughout this chapter. Its production configuration\nmight look something like Figure 22-1.\n\nFigure 22-1. Example production configuration for the Shakespeare search\nservice\n\n# Causes of Cascading Failures and Designing to Avoid Them\n\nWell-thought-out system design should take into account a few typical\nscenarios that account for the majority of cascading failures.\n\n## Server Overload\n\nThe most common cause of cascading failures is overload. Most cascading\nfailures described here are either directly due to server overload, or due to\nextensions or variations of this scenario.\n\nSuppose the frontend in cluster A is handling 1,000 requests per second (QPS),\nas in Figure 22-2.\n\nFigure 22-2. Normal server load distribution between clusters A and B\n\nIf cluster B fails (Figure 22-3), requests to cluster A increase to 1,200 QPS.\nThe frontends in A are not able to handle requests at 1,200 QPS, and therefore\nstart running out of resources, which causes them to crash, miss deadlines, or\notherwise misbehave. As a result, the rate of successfully handled requests in\nA dips well below 1,000 QPS.\n\nFigure 22-3. Cluster B fails, sending all traffic to cluster A\n\nThis reduction in the rate of useful work being done can spread into other\nfailure domains, potentially spreading globally. For example, local overload\nin one cluster may lead to its servers crashing; in response, the load\nbalancing controller sends requests to other clusters, overloading their\nservers, leading to a service-wide overload failure. It may not take long for\nthese events to transpire (e.g., on the order of a couple minutes), because\nthe load balancer and task scheduling systems involved may act very quickly.\n\n## Resource Exhaustion\n\nRunning out of a resource can result in higher latency, elevated error rates,\nor the substitution of lower-quality results. These are in fact desired\neffects of running out of resources: something eventually needs to give as the\nload increases beyond what a server can handle.\n\nDepending on what resource becomes exhausted in a server and how the server is\nbuilt, resource exhaustion can render the server less efficient or cause the\nserver to crash, prompting the load balancer to distribute the resource\nproblems to other servers. When this happens, the rate of successfully handled\nrequests can drop and possibly send the cluster or an entire service into a\ncascade failure.\n\nDifferent types of resources can be exhausted, resulting in varying effects on\nservers.\n\n### CPU\n\nIf there is insufficient CPU to handle the request load, typically all\nrequests become slower. This scenario can result in various secondary effects,\nincluding the following:\n\nIncreased number of in-flight requests\n\n    Because requests take longer to handle, more requests are handled concurrently (up to a possible maximum capacity at which queuing may occur). This affects almost all resources, including memory, number of active threads (in a thread-per-request server model), number of file descriptors, and backend resources (which in turn can have other effects).\nExcessively long queue lengths\n\n    If there is insufficient capacity to handle all the requests at steady state, the server will saturate its queues. This means that latency increases (the requests are queued for longer amounts of time) and the queue uses more memory. See Queue Management for a discussion of mitigation strategies.\nThread starvation\n\n    When a thread can\u2019t make progress because it\u2019s waiting for a lock, health checks may fail if the health check endpoint can\u2019t be served in time.\nCPU or request starvation\n\n    Internal watchdogs^108 in the server detect that the server isn\u2019t making progress, causing the servers to crash due to CPU starvation, or due to request starvation if watchdog events are triggered remotely and processed as part of the request queue.\nMissed RPC deadlines\n\n    As a server becomes overloaded, its responses to RPCs from its clients arrive later, which may exceed any deadlines those clients set. The work the server did to respond is then wasted, and clients may retry the RPCs, leading to even more overload.\nReduced CPU caching benefits\n\n    As more CPU is used, the chance of spilling on to more cores increases, resulting in decreased usage of local caches and decreased CPU efficiency.\n\n### Memory\n\nIf nothing else, more in-flight requests consume more RAM from allocating the\nrequest, response, and RPC objects. Memory exhaustion can cause the following\neffects:\n\nDying tasks\n\n    For example, a task might be evicted by the container manager (VM or otherwise) for exceeding available resource limits, or application-specific crashes may cause tasks to die.\nIncreased rate of garbage collection (GC) in Java, resulting in increased CPU\nusage\n\n    A vicious cycle can occur in this scenario: less CPU is available, resulting in slower requests, resulting in increased RAM usage, resulting in more GC, resulting in even lower availability of CPU. This is known colloquially as the \u201cGC death spiral.\u201d\nReduction in cache hit rates\n\n    Reduction in available RAM can reduce application-level cache hit rates, resulting in more RPCs to the backends, which can possibly cause the backends to become overloaded.\n\n### Threads\n\nThread starvation can directly cause errors or lead to health check failures.\nIf the server adds threads as needed, thread overhead can use too much RAM. In\nextreme cases, thread starvation can also cause you to run out of process IDs.\n\n### File descriptors\n\nRunning out of file descriptors can lead to the inability to initialize\nnetwork connections, which in turn can cause health checks to fail.\n\n### Dependencies among resources\n\nNote that many of these resource exhaustion scenarios feed from one another\u2014a\nservice experiencing overload often has a host of secondary symptoms that can\nlook like the root cause, making debugging difficult.\n\nFor example, imagine the following scenario:\n\n  1. A Java frontend has poorly tuned garbage collection (GC) parameters.\n  2. Under high (but expected) load, the frontend runs out of CPU due to GC.\n  3. CPU exhaustion slows down completion of requests.\n  4. The increased number of in-progress requests causes more RAM to be used to process the requests.\n  5. Memory pressure due to requests, in combination with a fixed memory allocation for the frontend process as a whole, leaves less RAM available for caching.\n  6. The reduced cache size means fewer entries in the cache, in addition to a lower hit rate.\n  7. The increase in cache misses means that more requests fall through to the backend for servicing.\n  8. The backend, in turn, runs out of CPU or threads.\n  9. Finally, the lack of CPU causes basic health checks to fail, starting a cascading failure.\n\nIn situations as complex as the preceding scenario, it\u2019s unlikely that the\ncausal chain will be fully diagnosed during an outage. It might be very hard\nto determine that the backend crash was caused by a decrease in the cache rate\nin the frontend, particularly if the frontend and backend components have\ndifferent owners.\n\n## Service Unavailability\n\nResource exhaustion can lead to servers crashing; for example, servers might\ncrash when too much RAM is allocated to a container. Once a couple of servers\ncrash on overload, the load on the remaining servers can increase, causing\nthem to crash as well. The problem tends to snowball and soon all servers\nbegin to crash-loop. It\u2019s often difficult to escape this scenario because as\nsoon as servers come back online they\u2019re bombarded with an extremely high rate\nof requests and fail almost immediately.\n\nFor example, if a service was healthy at 10,000 QPS, but started a cascading\nfailure due to crashes at 11,000 QPS, dropping the load to 9,000 QPS will\nalmost certainly not stop the crashes. This is because the service will be\nhandling increased demand with reduced capacity; only a small fraction of\nservers will usually be healthy enough to handle requests. The fraction of\nservers that will be healthy depends on a few factors: how quickly the system\nis able to start the tasks, how quickly the binary can start serving at full\ncapacity, and how long a freshly started task is able to survive the load. In\nthis example, if 10% of the servers are healthy enough to handle requests, the\nrequest rate would need to drop to about 1,000 QPS in order for the system to\nstabilize and recover.\n\nSimilarly, servers can appear unhealthy to the load balancing layer, resulting\nin reduced load balancing capacity: servers may go into \u201clame duck\u201d state (see\nA Robust Approach to Unhealthy Tasks: Lame Duck State) or fail health checks\nwithout crashing. The effect can be very similar to crashing: more servers\nappear unhealthy, the healthy servers tend to accept requests for a very brief\nperiod of time before becoming unhealthy, and fewer servers participate in\nhandling requests.\n\nLoad balancing policies that avoid servers that have served errors can\nexacerbate problems further\u2014a few backends serve some errors, so they don\u2019t\ncontribute to the available capacity for the service. This increases the load\non the remaining servers, starting the snowball effect.\n\n# Preventing Server Overload\n\nThe following list presents strategies for avoiding server overload in rough\npriority order:\n\nLoad test the server\u2019s capacity limits, and test the failure mode for overload\n\n    This is the most important exercise you should conduct in order to prevent server overload. Unless you test in a realistic environment, it\u2019s very hard to predict exactly which resource will be exhausted and how that resource exhaustion will manifest. For details, see Testing for Cascading Failures.\nServe degraded results\n\n    Serve lower-quality, cheaper-to-compute results to the user. Your strategy here will be service-specific. See Load Shedding and Graceful Degradation.\nInstrument the server to reject requests when overloaded\n\n    Servers should protect themselves from becoming overloaded and crashing. When overloaded at either the frontend or backend layers, fail early and cheaply. For details, see Load Shedding and Graceful Degradation.\nInstrument higher-level systems to reject requests, rather than overloading\nservers\n\n    Note that because rate limiting often doesn\u2019t take overall service health into account, it may not be able to stop a failure that has already begun. Simple rate-limiting implementations are also likely to leave capacity unused. Rate limiting can be implemented in a number of places:\n    \n\n  * At the reverse proxies, by limiting the volume of requests by criteria such as IP address to mitigate attempted denial-of-service attacks and abusive clients.\n  * At the load balancers, by dropping requests when the service enters global overload. Depending on the nature and complexity of the service, this rate limiting can be indiscriminate (\u201cdrop all traffic above X requests per second\u201d) or more selective (\u201cdrop requests that aren\u2019t from users who have recently interacted with the service\u201d or \u201cdrop requests for low-priority operations like background synchronization, but keep serving interactive user sessions\u201d).\n  * At individual tasks, to prevent random fluctuations in load balancing from overwhelming the server.\n\nPerform capacity planning\n\n    Good capacity planning can reduce the probability that a cascading failure will occur. Capacity planning should be coupled with performance testing to determine the load at which the service will fail. For instance, if every cluster\u2019s breaking point is 5,000 QPS, the load is evenly spread across clusters,^109 and the service\u2019s peak load is 19,000 QPS, then approximately six clusters are needed to run the service at N + 2.\n\nCapacity planning reduces the probability of triggering a cascading failure,\nbut it is not sufficient to protect the service from cascading failures. When\nyou lose major parts of your infrastructure during a planned or unplanned\nevent, no amount of capacity planning may be sufficient to prevent cascading\nfailures. Load balancing problems, network partitions, or unexpected traffic\nincreases can create pockets of high load beyond what was planned. Some\nsystems can grow the number of tasks for your service on demand, which may\nprevent overload; however, proper capacity planning is still needed.\n\n## Queue Management\n\nMost thread-per-request servers use a queue in front of a thread pool to\nhandle requests. Requests come in, they sit on a queue, and then threads pick\nrequests off the queue and perform the actual work (whatever actions are\nrequired by the server). Usually, if the queue is full, the server will reject\nnew requests.\n\nIf the request rate and latency of a given task is constant, there is no\nreason to queue requests: a constant number of threads should be occupied.\nUnder this idealized scenario, requests will only be queued if the steady\nstate rate of incoming requests exceeds the rate at which the server can\nprocess requests, which results in saturation of both the thread pool and the\nqueue.\n\nQueued requests consume memory and increase latency. For example, if the queue\nsize is 10x the number of threads, the time to handle the request on a thread\nis 100 milliseconds. If the queue is full, then a request will take 1.1\nseconds to handle, most of which time is spent on the queue.\n\nFor a system with fairly steady traffic over time, it is usually better to\nhave small queue lengths relative to the thread pool size (e.g., 50% or less),\nwhich results in the server rejecting requests early when it can\u2019t sustain the\nrate of incoming requests. For example, Gmail often uses queueless servers,\nrelying instead on failover to other server tasks when the threads are full.\nOn the other end of the spectrum, systems with \u201cbursty\u201d load for which traffic\npatterns fluctuate drastically may do better with a queue size based on the\ncurrent number of threads in use, processing time for each request, and the\nsize and frequency of bursts.\n\n## Load Shedding and Graceful Degradation\n\nLoad shedding drops some proportion of load by dropping traffic as the server\napproaches overload conditions. The goal is to keep the server from running\nout of RAM, failing health checks, serving with extremely high latency, or any\nof the other symptoms associated with overload, while still doing as much\nuseful work as it can.\n\nOne straightforward way to shed load is to do per-task throttling based on\nCPU, memory, or queue length; limiting queue length as discussed in Queue\nManagement is a form of this strategy. For example, one effective approach is\nto return an HTTP 503 (service unavailable) to any incoming request when there\nare more than a given number of client requests in flight.\n\nChanging the queuing method from the standard first-in, first-out (FIFO) to\nlast-in, first-out (LIFO) or using the controlled delay (CoDel) algorithm\n[Nic12] or similar approaches can reduce load by removing requests that are\nunlikely to be worth processing [Mau15]. If a user\u2019s web search is slow\nbecause an RPC has been queued for 10 seconds, there\u2019s a good chance the user\nhas given up and refreshed their browser, issuing another request: there\u2019s no\npoint in responding to the first one, since it will be ignored! This strategy\nworks well when combined with propagating RPC deadlines throughout the stack,\ndescribed in Latency and Deadlines.\n\nMore sophisticated approaches include identifying clients to be more selective\nabout what work is dropped, or picking requests that are more important and\nprioritizing. Such strategies are more likely to be needed for shared\nservices.\n\nGraceful degradation takes the concept of load shedding one step further by\nreducing the amount of work that needs to be performed. In some applications,\nit\u2019s possible to significantly decrease the amount of work or time needed by\ndecreasing the quality of responses. For instance, a search application might\nonly search a subset of data stored in an in-memory cache rather than the full\non-disk database or use a less-accurate (but faster) ranking algorithm when\noverloaded.\n\nWhen evaluating load shedding or graceful degradation options for your\nservice, consider the following:\n\n  * Which metrics should you use to determine when load shedding or graceful degradation should kick in (e.g,. CPU usage, latency, queue length, number of threads used, whether your service enters degraded mode automatically or if manual intervention is necessary)?\n  * What actions should be taken when the server is in degraded mode?\n  * At what layer should load shedding and graceful degradation be implemented? Does it make sense to implement these strategies at every layer in the stack, or is it sufficient to have a high-level choke-point?\n\nAs you evaluate options and deploy, keep the following in mind:\n\n  * Graceful degradation shouldn\u2019t trigger very often\u2014usually in cases of a capacity planning failure or unexpected load shift. Keep the system simple and understandable, particularly if it isn\u2019t used often.\n  * Remember that the code path you never use is the code path that (often) doesn\u2019t work. In steady-state operation, graceful degradation mode won\u2019t be used, implying that you\u2019ll have much less operational experience with this mode and any of its quirks, which increases the level of risk. You can make sure that graceful degradation stays working by regularly running a small subset of servers near overload in order to exercise this code path.\n  * Monitor and alert when too many servers enter these modes.\n  * Complex load shedding and graceful degradation can cause problems themselves\u2014excessive complexity may cause the server to trip into a degraded mode when it is not desired, or enter feedback cycles at undesired times. Design a way to quickly turn off complex graceful degradation or tune parameters if needed. Storing this configuration in a consistent system that each server can watch for changes, such as Chubby, can increase deployment speed, but also introduces its own risks of synchronized failure.\n\n## Retries\n\nSuppose the code in the frontend that talks to the backend implements retries\nnaively. It retries after encountering a failure and caps the number of\nbackend RPCs per logical request to 10. Consider this code in the frontend,\nusing gRPC in Go:\n\n    \n    \n    func exampleRpcCall(client pb.ExampleClient, request pb.Request) *pb.Response { // Set RPC timeout to 5 seconds. opts := grpc.WithTimeout(5 * time.Second) // Try up to 10 times to make the RPC call. attempts := 10 for attempts > 0 { conn, err := grpc.Dial(*serverAddr, opts...) if err != nil { // Something went wrong in setting up the connection. Try again. attempts-- continue } defer conn.Close() // Create a client stub and make the RPC call. client := pb.NewBackendClient(conn) response, err := client.MakeRequest(context.Background, request) if err != nil { // Something went wrong in making the call. Try again. attempts-- continue } return response } grpclog.Fatalf(\"ran out of attempts\") }\n\nThis system can cascade in the following way:\n\n  1. Assume our backend has a known limit of 10,000 QPS per task, after which point all further requests are rejected in an attempt at graceful degradation.\n  2. The frontend calls MakeRequest at a constant rate of 10,100 QPS and overloads the backend by 100 QPS, which the backend rejects.\n  3. Those 100 failed QPS are retried in MakeRequest every 1,000 ms, and probably succeed. But the retries are themselves adding to the requests sent to the backend, which now receives 10,200 QPS\u2014200 QPS of which are failing due to overload.\n  4. The volume of retries grows: 100 QPS of retries in the first second leads to 200 QPS, then to 300 QPS, and so on. Fewer and fewer requests are able to succeed on their first attempt, so less useful work is being performed as a fraction of requests to the backend.\n  5. If the backend task is unable to handle the increase in load\u2014which is consuming file descriptors, memory, and CPU time on the backend\u2014it can melt down and crash under the sheer load of requests and retries. This crash then redistributes the requests it was receiving across the remaining backend tasks, in turn further overloading those tasks.\n\nSome simplifying assumptions were made here to illustrate this scenario,^110\nbut the point remains that retries can destabilize a system. Note that both\ntemporary load spikes and slow increases in usage can cause this effect.\n\nEven if the rate of calls to MakeRequest decreases to pre-meltdown levels\n(9,000 QPS, for example), depending on how much returning a failure costs the\nbackend, the problem might not go away. Two factors are at play here:\n\n  * If the backend spends a significant amount of resources processing requests that will ultimately fail due to overload, then the retries themselves may be keeping the backend in an overloaded mode.\n  * The backend servers themselves may not be stable. Retries can amplify the effects seen in Server Overload.\n\nIf either of these conditions is true, in order to dig out of this outage, you\nmust dramatically reduce or eliminate the load on the frontends until the\nretries stop and the backends stabilize.\n\nThis pattern has contributed to several cascading failures, whether the\nfrontends and backends communicate via RPC messages, the \u201cfrontend\u201d is client\nJavaScript code issuing XmlHttpRequest calls to an endpoint and retries on\nfailure, or the retries originate from an offline sync protocol that retries\naggressively when it encounters a failure.\n\nWhen issuing automatic retries, keep in mind the following considerations:\n\n  * Most of the backend protection strategies described in Preventing Server Overload apply. In particular, testing the system can highlight problems, and graceful degradation can reduce the effect of the retries on the backend.\n  * Always use randomized exponential backoff when scheduling retries. See also \"Exponential Backoff and Jitter\" in the AWS Architecture Blog [Bro15]. If retries aren\u2019t randomly distributed over the retry window, a small perturbation (e.g., a network blip) can cause retry ripples to schedule at the same time, which can then amplify themselves [Flo94].\n  * Limit retries per request. Don\u2019t retry a given request indefinitely.\n  * Consider having a server-wide retry budget. For example, only allow 60 retries per minute in a process, and if the retry budget is exceeded, don\u2019t retry; just fail the request. This strategy can contain the retry effect and be the difference between a capacity planning failure that leads to some dropped queries and a global cascading failure.\n  * Think about the service holistically and decide if you really need to perform retries at a given level. In particular, avoid amplifying retries by issuing retries at multiple levels: a single request at the highest layer may produce a number of attempts as large as the product of the number of attempts at each layer to the lowest layer. If the database can\u2019t service requests because it\u2019s overloaded, and the backend, frontend, and JavaScript layers all issue 3 retries (4 attempts), then a single user action may create 64 attempts (4^3) on the database. This behavior is undesirable when the database is returning those errors because it\u2019s overloaded.\n  * Use clear response codes and consider how different failure modes should be handled. For example, separate retriable and nonretriable error conditions. Don\u2019t retry permanent errors or malformed requests in a client, because neither will ever succeed. Return a specific status when overloaded so that clients and other layers back off and do not retry.\n\nIn an emergency, it may not be obvious that an outage is due to bad retry\nbehavior. Graphs of retry rates can be an indication of bad retry behavior,\nbut may be confused as a symptom instead of a compounding cause. In terms of\nmitigation, this is a special case of the insufficient capacity problem, with\nthe additional caveat that you must either fix the retry behavior (usually\nrequiring a code push), reduce load significantly, or cut requests off\nentirely.\n\n## Latency and Deadlines\n\nWhen a frontend sends an RPC to a backend server, the frontend consumes\nresources waiting for a reply. RPC deadlines define how long a request can\nwait before the frontend gives up, limiting the time that the backend may\nconsume the frontend\u2019s resources.\n\n### Picking a deadline\n\nIt\u2019s usually wise to set a deadline. Setting either no deadline or an\nextremely high deadline may cause short-term problems that have long since\npassed to continue to consume server resources until the server restarts.\n\nHigh deadlines can result in resource consumption in higher levels of the\nstack when lower levels of the stack are having problems. Short deadlines can\ncause some more expensive requests to fail consistently. Balancing these\nconstraints to pick a good deadline can be something of an art.\n\n### Missing deadlines\n\nA common theme in many cascading outages is that servers spend resources\nhandling requests that will exceed their deadlines on the client. As a result,\nresources are spent while no progress is made: you don\u2019t get credit for late\nassignments with RPCs.\n\nSuppose an RPC has a 10-second deadline, as set by the client. The server is\nvery overloaded, and as a result, it takes 11 seconds to move from a queue to\na thread pool. At this point, the client has already given up on the request.\nUnder most circumstances, it would be unwise for the server to attempt to\nhandle this request, because it would be doing work for which no credit will\nbe granted\u2014the client doesn\u2019t care what work the server does after the\ndeadline has passed, because it\u2019s given up on the request already.\n\nIf handling a request is performed over multiple stages (e.g., there are a few\ncallbacks and RPC calls), the server should check the deadline left at each\nstage before attempting to perform any more work on the request. For example,\nif a request is split into parsing, backend request, and processing stages, it\nmay make sense to check that there is enough time left to handle the request\nbefore each stage.\n\n### Deadline propagation\n\nRather than inventing a deadline when sending RPCs to backends, servers should\nemploy deadline propagation.\n\nWith deadline propagation, a deadline is set high in the stack (e.g., in the\nfrontend). The tree of RPCs emanating from an initial request will all have\nthe same absolute deadline. For example, if server A selects a 30-second\ndeadline, and processes the request for 7 seconds before sending an RPC to\nserver B, the RPC from A to B will have a 23-second deadline. If server B\ntakes 4 seconds to handle the request and sends an RPC to server C, the RPC\nfrom B to C will have a 19-second deadline, and so on. Ideally, each server in\nthe request tree implements deadline propagation.\n\nWithout deadline propagation, the following scenario may occur:\n\n  1. Server A sends an RPC to server B with a 10-second deadline.\n  2. Server B takes 8 seconds to start processing the request and then sends an RPC to server C.\n  3. If server B uses deadline propagation, it should set a 2-second deadline, but suppose it instead uses a hardcoded 20-second deadline for the RPC to server C.\n  4. Server C pulls the request off its queue after 5 seconds.\n\nHad server B used deadline propagation, server C could immediately give up on\nthe request because the 2-second deadline was exceeded. However, in this\nscenario, server C processes the request thinking it has 15 seconds to spare,\nbut is not doing useful work, since the request from server A to server B has\nalready exceeded its deadline.\n\nYou may want to reduce the outgoing deadline a bit (e.g., a few hundred\nmilliseconds) to account for network transit times and post-processing in the\nclient.\n\nAlso consider setting an upper bound for outgoing deadlines. You may want to\nlimit how long the server waits for outgoing RPCs to noncritical backends, or\nfor RPCs to backends that typically complete in a short duration. However, be\nsure to understand your traffic mix, because you might otherwise inadvertently\nmake particular types of requests fail all the time (e.g., requests with large\npayloads, or requests that require responding to a lot of computation).\n\nThere are some exceptions for which servers may wish to continue processing a\nrequest after the deadline has elapsed. For example, if a server receives a\nrequest that involves performing some expensive catchup operation and\nperiodically checkpoints the progress of the catchup, it would be a good idea\nto check the deadline only after writing the checkpoint, instead of after the\nexpensive operation.\n\n### Cancellation propagation\n\nPropagating cancellations reduces unneeded or doomed work by advising servers\nin an RPC call stack that their efforts are no longer necessary. To reduce\nlatency, some systems use \"hedged requests\" [Dea13] to send RPCs to a primary\nserver, then some time later, send the same request to other instances of the\nsame service in case the primary is slow in responding; once the client has\nreceived a response from any server, it sends messages to the other servers to\ncancel the now-superfluous requests. Those requests may themselves\ntransitively fan out to many other servers, so cancellations should be\npropagated throughout the entire stack.\n\nThis approach can also be used to avoid the potential leakage that occurs if\nan initial RPC has a long deadline, but subsequent critical RPCs between\ndeeper layers of the stack receive errors which can't succeed on retry, or\nhave short deadlines and time out. Using only simple deadline propagation, the\ninitial call continues to use server resources until it eventually times out,\ndespite being doomed to failure. Sending fatal errors or timeouts up the stack\nand cancelling other RPCs in the call tree prevents unneeded work if the\nrequest as a whole can't be fulfilled.\n\n### Bimodal latency\n\nSuppose that the frontend from the preceding example consists of 10 servers,\neach with 100 worker threads. This means that the frontend has a total of\n1,000 threads of capacity. During usual operation, the frontends perform 1,000\nQPS and requests complete in 100 ms. This means that the frontends usually\nhave 100 worker threads occupied out of the 1,000 configured worker threads\n(1,000 QPS * 0.1 seconds).\n\nSuppose an event causes 5% of the requests to never complete. This could be\nthe result of the unavailability of some Bigtable row ranges, which renders\nthe requests corresponding to that Bigtable keyspace unservable. As a result,\n5% of the requests hit the deadline, while the remaining 95% of the requests\ntake the usual 100 ms.\n\nWith a 100-second deadline, 5% of requests would consume 5,000 threads (50 QPS\n* 100 seconds), but the frontend doesn\u2019t have that many threads available.\nAssuming no other secondary effects, the frontend will only be able to handle\n19.6% of the requests (1,000 threads available / (5,000 + 95) threads\u2019 worth\nof work), resulting in an 80.4% error rate.\n\nTherefore, instead of only 5% of requests receiving an error (those that\ndidn\u2019t complete due to keyspace unavailability), most requests receive an\nerror.\n\nThe following guidelines can help address this class of problems:\n\n  * Detecting this problem can be very hard. In particular, it may not be clear that bimodal latency is the cause of an outage when you are looking at mean latency. When you see a latency increase, try to look at the distribution of latencies in addition to the averages.\n  * This problem can be avoided if the requests that don\u2019t complete return with an error early, rather than waiting the full deadline. For example, if a backend is unavailable, it\u2019s usually best to immediately return an error for that backend, rather than consuming resources until the backend is available. If your RPC layer supports a fail-fast option, use it.\n  * Having deadlines several orders of magnitude longer than the mean request latency is usually bad. In the preceding example, a small number of requests initially hit the deadline, but the deadline was three orders of magnitude larger than the normal mean latency, leading to thread exhaustion.\n  * When using shared resources that can be exhausted by some keyspace, consider either limiting in-flight requests by that keyspace or using other kinds of abuse tracking. Suppose your backend processes requests for different clients that have wildly different performance and request characteristics. You might consider only allowing 25% of your threads to be occupied by any one client in order to provide fairness in the face of heavy load by any single client misbehaving.\n\n# Slow Startup and Cold Caching\n\nProcesses are often slower at responding to requests immediately after\nstarting than they will be in steady state. This slowness can be caused by\neither or both of the following:\n\nRequired initialization\n\n    Setting up connections upon receiving the first request that needs a given backend\nRuntime performance improvements in some languages, particularly Java\n\n    Just-In-Time compilation, hotspot optimization, and deferred class loading\n\nSimilarly, some binaries are less efficient when caches aren\u2019t filled. For\nexample, in the case of some of Google\u2019s services, most requests are served\nout of caches, so requests that miss the cache are significantly more\nexpensive. In steady-state operation with a warm cache, only a few cache\nmisses occur, but when the cache is completely empty, 100% of requests are\ncostly. Other services might employ caches to keep a user\u2019s state in RAM. This\nmight be accomplished through hard or soft stickiness between reverse proxies\nand service frontends.\n\nIf the service is not provisioned to handle requests under a cold cache, it\u2019s\nat greater risk of outages and should take steps to avoid them.\n\nThe following scenarios can lead to a cold cache:\n\nTurning up a new cluster\n\n    A recently added cluster will have an empty cache.\nReturning a cluster to service after maintenance\n\n    The cache may be stale.\nRestarts\n\n    If a task with a cache has recently restarted, filling its cache will take some time. It may be worthwhile to move caching from a server to a separate binary like memcache, which also allows cache sharing between many servers, albeit at the cost of introducing another RPC and slight additional latency.\n\nIf caching has a significant effect on the service,^111 you may want to use\none or some of the following strategies:\n\n  * Overprovision the service. It\u2019s important to note the distinction between a latency cache versus a capacity cache: when a latency cache is employed, the service can sustain its expected load with an empty cache, but a service using a capacity cache cannot sustain its expected load under an empty cache. Service owners should be vigilant about adding caches to their service, and make sure that any new caches are either latency caches or are sufficiently well engineered to safely function as capacity caches. Sometimes caches are added to a service to improve performance, but actually wind up being hard dependencies.\n  * Employ general cascading failure prevention techniques. In particular, servers should reject requests when they\u2019re overloaded or enter degraded modes, and testing should be performed to see how the service behaves after events such as a large restart.\n  * When adding load to a cluster, slowly increase the load. The initially small request rate warms up the cache; once the cache is warm, more traffic can be added. It\u2019s a good idea to ensure that all clusters carry nominal load and that the caches are kept warm.\n\n## Always Go Downward in the Stack\n\nIn the example Shakespeare service, the frontend talks to a backend, which in\nturn talks to the storage layer. A problem that manifests in the storage layer\ncan cause problems for servers that talk to it, but fixing the storage layer\nwill usually repair both the backend and frontend layers.\n\nHowever, suppose the backends cross-communicate amongst each other. For\nexample, the backends might proxy requests to one another to change who owns a\nuser when the storage layer can\u2019t service a request. This intra-layer\ncommunication can be problematic for several reasons:\n\n  * The communication is susceptible to a distributed deadlock. Backends may use the same thread pool to wait on RPCs sent to remote backends that are simultaneously receiving requests from remote backends. Suppose backend A\u2019s thread pool is full. Backend B sends a request to backend A and uses a thread in backend B until backend A\u2019s thread pool clears. This behavior can cause the thread pool saturation to spread.\n\n  * If intra-layer communication increases in response to some kind of failure or heavy load condition (e.g., load rebalancing that is more active under high load), intra-layer communication can quickly switch from a low to high intra-layer request mode when the load increases enough.\n\nFor example, suppose a user has a primary backend and a predetermined hot\nstandby secondary backend in a different cluster that can take over the user.\nThe primary backend proxies requests to the secondary backend as a result of\nerrors from the lower layer or in response to heavy load on the master. If the\nentire system is overloaded, primary to secondary proxying will likely\nincrease and add even more load to the system, due to the additional cost of\nparsing and waiting on the request to the secondary in the primary.\n\n  * Depending on the criticality of the cross-layer communication, bootstrapping the system may become more complex.\n\nIt\u2019s usually better to avoid intra-layer communication\u2014i.e., possible cycles\nin the communication path\u2014in the user request path. Instead, have the client\ndo the communication. For example, if a frontend talks to a backend but\nguesses the wrong backend, the backend should not proxy to the correct\nbackend. Instead, the backend should tell the frontend to retry its request on\nthe correct backend.\n\n# Triggering Conditions for Cascading Failures\n\nWhen a service is susceptible to cascading failures, there are several\npossible disturbances that can initiate the domino effect. This section\nidentifies some of the factors that trigger cascading failures.\n\n## Process Death\n\nSome server tasks may die, reducing the amount of available capacity. Tasks\nmight die because of a Query of Death (an RPC whose contents trigger a failure\nin the process), cluster issues, assertion failures, or a number of other\nreasons. A very small event (e.g., a couple of crashes or tasks rescheduled to\nother machines) may cause a service on the brink of falling to break.\n\n## Process Updates\n\nPushing a new version of the binary or updating its configuration may initiate\na cascading failure if a large number of tasks are affected simultaneously. To\nprevent this scenario, either account for necessary capacity overhead when\nsetting up the service\u2019s update infrastructure, or push off-peak. Dynamically\nadjusting the number of in-flight task updates based on the volume of requests\nand available capacity may be a workable approach.\n\n## New Rollouts\n\nA new binary, configuration changes, or a change to the underlying\ninfrastructure stack can result in changes to request profiles, resource usage\nand limits, backends, or a number of other system components that can trigger\na cascading failure.\n\nDuring a cascading failure, it\u2019s usually wise to check for recent changes and\nconsider reverting them, particularly if those changes affected capacity or\naltered the request profile.\n\nYour service should implement some type of change logging, which can help\nquickly identify recent changes.\n\n## Organic Growth\n\nIn many cases, a cascading failure isn\u2019t triggered by a specific service\nchange, but because a growth in usage wasn\u2019t accompanied by an adjustment to\ncapacity.\n\n## Planned Changes, Drains, or Turndowns\n\nIf your service is multihomed, some of your capacity may be unavailable\nbecause of maintenance or outages in a cluster. Similarly, one of the\nservice\u2019s critical dependencies may be drained, resulting in a reduction in\ncapacity for the upstream service due to drain dependencies, or an increase in\nlatency due to having to send the requests to a more distant cluster.\n\n### Request profile changes\n\nA backend service may receive requests from different clusters because a\nfrontend service shifted its traffic due to load balancing configuration\nchanges, changes in the traffic mix, or cluster fullness. Also, the average\ncost to handle an individual payload may have changed due to frontend code or\nconfiguration changes. Similarly, the data handled by the service may have\nchanged organically due to increased or differing usage by existing users: for\ninstance, both the number and size of images, per user, for a photo storage\nservice tend to increase over time.\n\n### Resource limits\n\nSome cluster operating systems allow resource overcommitment. CPU is a\nfungible resource; often, some machines have some amount of slack CPU\navailable, which provides a bit of a safety net against CPU spikes. The\navailability of this slack CPU differs between cells, and also between\nmachines within the cell.\n\nDepending upon this slack CPU as your safety net is dangerous. Its\navailability is entirely dependent on the behavior of the other jobs in the\ncluster, so it might suddenly drop out at any time. For example, if a team\nstarts a MapReduce that consumes a lot of CPU and schedules on many machines,\nthe aggregate amount of slack CPU can suddenly decrease and trigger CPU\nstarvation conditions for unrelated jobs. When performing load tests, make\nsure that you remain within your committed resource limits.\n\n# Testing for Cascading Failures\n\nThe specific ways in which a service will fail can be very hard to predict\nfrom first principles. This section discusses testing strategies that can\ndetect if services are susceptible to cascading failures.\n\nYou should test your service to determine how it behaves under heavy load in\norder to gain confidence that it won\u2019t enter a cascading failure under various\ncircumstances.\n\n## Test Until Failure and Beyond\n\nUnderstanding the behavior of the service under heavy load is perhaps the most\nimportant first step in avoiding cascading failures. Knowing how your system\nbehaves when it is overloaded helps to identify what engineering tasks are the\nmost important for long-term fixes; at the very least, this knowledge may help\nbootstrap the debugging process for on-call engineers when an emergency\narises.\n\nLoad test components until they break. As load increases, a component\ntypically handles requests successfully until it reaches a point at which it\ncan\u2019t handle more requests. At this point, the component should ideally start\nserving errors or degraded results in response to additional load, but not\nsignificantly reduce the rate at which it successfully handles requests. A\ncomponent that is highly susceptible to a cascading failure will start\ncrashing or serving a very high rate of errors when it becomes overloaded; a\nbetter designed component will instead be able to reject a few requests and\nsurvive.\n\nLoad testing also reveals where the breaking point is, knowledge that\u2019s\nfundamental to the capacity planning process. It enables you to test for\nregressions, provision for worst-case thresholds, and to trade off utilization\nversus safety margins.\n\nBecause of caching effects, gradually ramping up load may yield different\nresults than immediately increasing to expected load levels. Therefore,\nconsider testing both gradual and impulse load patterns.\n\nYou should also test and understand how the component behaves as it returns to\nnominal load after having been pushed well beyond that load. Such testing may\nanswer questions such as:\n\n  * If a component enters a degraded mode on heavy load, is it capable of exiting the degraded mode without human intervention?\n  * If a couple of servers crash under heavy load, how much does the load need to drop in order for the system to stabilize?\n\nIf you\u2019re load testing a stateful service or a service that employs caching,\nyour load test should track state between multiple interactions and check\ncorrectness at high load, which is often where subtle concurrency bugs hit.\n\nKeep in mind that individual components may have different breaking points, so\nload test each component separately. You won\u2019t know in advance which component\nmay hit the wall first, and you want to know how your system behaves when it\ndoes.\n\nIf you believe your system has proper protections against being overloaded,\nconsider performing failure tests in a small slice of production to find the\npoint at which the components in your system fail under real traffic. These\nlimits may not be adequately reflected by synthetic load test traffic, so real\ntraffic tests may provide more realistic results than load tests, at the risk\nof causing user-visible pain. Be careful when testing on real traffic: make\nsure that you have extra capacity available in case your automatic protections\ndon\u2019t work and you need to manually fail over. You might consider some of the\nfollowing production tests:\n\n  * Reducing task counts quickly or slowly over time, beyond expected traffic patterns\n  * Rapidly losing a cluster\u2019s worth of capacity\n  * Blackholing various backends\n\n## Test Popular Clients\n\nUnderstand how large clients use your service. For example, you want to know\nif clients:\n\n  * Can queue work while the service is down\n  * Use randomized exponential backoff on errors\n  * Are vulnerable to external triggers that can create large amounts of load (e.g., an externally triggered software update might clear an offline client\u2019s cache)\n\nDepending on your service, you may or may not be in control of all the client\ncode that talks to your service. However, it\u2019s still a good idea to have an\nunderstanding of how large clients that interact with your service will\nbehave.\n\nThe same principles apply to large internal clients. Stage system failures\nwith the largest clients to see how they react. Ask internal clients how they\naccess your service and what mechanisms they use to handle backend failure.\n\n## Test Noncritical Backends\n\nTest your noncritical backends, and make sure their unavailability does not\ninterfere with the critical components of your service.\n\nFor example, suppose your frontend has critical and noncritical backends.\nOften, a given request includes both critical components (e.g., query results)\nand noncritical components (e.g., spelling suggestions). Your requests may\nsignificantly slow down and consume resources waiting for noncritical backends\nto finish.\n\nIn addition to testing behavior when the noncritical backend is unavailable,\ntest how the frontend behaves if the noncritical backend never responds (for\nexample, if it is blackholing requests). Backends advertised as noncritical\ncan still cause problems on frontends when requests have long deadlines. The\nfrontend should not start rejecting lots of requests, running out of\nresources, or serving with very high latency when a noncritical backend\nblackholes.\n\n# Immediate Steps to Address Cascading Failures\n\nOnce you have identified that your service is experiencing a cascading\nfailure, you can use a few different strategies to remedy the situation\u2014and of\ncourse, a cascading failure is a good opportunity to use your incident\nmanagement protocol (Managing Incidents).\n\n## Increase Resources\n\nIf your system is running at degraded capacity and you have idle resources,\nadding tasks can be the most expedient way to recover from the outage.\nHowever, if the service has entered a death spiral of some sort, adding more\nresources may not be sufficient to recover.\n\n## Stop Health Check Failures/Deaths\n\nSome cluster scheduling systems, such as Borg, check the health of tasks in a\njob and restart tasks that are unhealthy. This practice may create a failure\nmode in which health-checking itself makes the service unhealthy. For example,\nif half the tasks aren\u2019t able to accomplish any work because they\u2019re starting\nup and the other half will soon be killed because they\u2019re overloaded and\nfailing health checks, temporarily disabling health checks may permit the\nsystem to stabilize until all the tasks are running.\n\nProcess health checking (\u201cis this binary responding at all?\u201d) and service\nhealth checking (\u201cis this binary able to respond to this class of requests\nright now?\u201d) are two conceptually distinct operations. Process health checking\nis relevant to the cluster scheduler, whereas service health checking is\nrelevant to the load balancer. Clearly distinguishing between the two types of\nhealth checks can help avoid this scenario.\n\n## Restart Servers\n\nIf servers are somehow wedged and not making progress, restarting them may\nhelp. Try restarting servers when:\n\n  * Java servers are in a GC death spiral\n  * Some in-flight requests have no deadlines but are consuming resources, leading them to block threads, for example\n  * The servers are deadlocked\n\nMake sure that you identify the source of the cascading failure before you\nrestart your servers. Make sure that taking this action won\u2019t simply shift\naround load. Canary this change, and make it slowly. Your actions may amplify\nan existing cascading failure if the outage is actually due to an issue like a\ncold cache.\n\n## Drop Traffic\n\nDropping load is a big hammer, usually reserved for situations in which you\nhave a true cascading failure on your hands and you cannot fix it by other\nmeans. For example, if heavy load causes most servers to crash as soon as they\nbecome healthy, you can get the service up and running again by:\n\n  1. Addressing the initial triggering condition (by adding capacity, for example).\n  2. Reducing load enough so that the crashing stops. Consider being aggressive here\u2014if the entire service is crash-looping, only allow, say, 1% of the traffic through.\n  3. Allowing the majority of the servers to become healthy.\n  4. Gradually ramping up the load.\n\nThis strategy allows caches to warm up, connections to be established, etc.,\nbefore load returns to normal levels.\n\nObviously, this tactic will cause a lot of user-visible harm. Whether or not\nyou\u2019re able to (or if you even should) drop traffic indiscriminately depends\non how the service is configured. If you have some mechanism to drop less\nimportant traffic (e.g., prefetching), use that mechanism first.\n\nIt is important to keep in mind that this strategy enables you to recover from\na cascading outage once the underlying problem is fixed. If the issue that\nstarted the cascading failure is not fixed (e.g., insufficient global\ncapacity), then the cascading failure may trigger shortly after all traffic\nreturns. Therefore, before using this strategy, consider fixing (or at least\npapering over) the root cause or triggering condition. For example, if the\nservice ran out of memory and is now in a death spiral, adding more memory or\ntasks should be your first step.\n\n## Enter Degraded Modes\n\nServe degraded results by doing less work or dropping unimportant traffic.\nThis strategy must be engineered into your service, and can be implemented\nonly if you know which traffic can be degraded and you have the ability to\ndifferentiate between the various payloads.\n\n## Eliminate Batch Load\n\nSome services have load that is important, but not critical. Consider turning\noff those sources of load. For example, if index updates, data copies, or\nstatistics gathering consume resources of the serving path, consider turning\noff those sources of load during an outage.\n\n## Eliminate Bad Traffic\n\nIf some queries are creating heavy load or crashes (e.g., queries of death),\nconsider blocking them or eliminating them via other means.\n\n##### Cascading Failure and Shakespeare\n\nA documentary about Shakespeare\u2019s works airs in Japan, and explicitly points\nto our Shakespeare service as an excellent place to conduct further research.\nFollowing the broadcast, traffic to our Asian datacenter surges beyond the\nservice\u2019s capacity. This capacity problem is further compounded by a major\nupdate to the Shakespeare service that simultaneously occurs in that\ndatacenter.\n\nFortunately, a number of safeguards are in place that help mitigate the\npotential for failure. The Production Readiness Review process identified some\nissues that the team already addressed. For example, the developers built\ngraceful degradation into the service. As capacity becomes scarce, the service\nno longer returns pictures alongside text or small maps illustrating where a\nstory takes place. And depending on its purpose, an RPC that times out is\neither not retried (for example, in the case of the aforementioned pictures),\nor is retried with a randomized exponential backoff. Despite these safeguards,\nthe tasks fail one by one and are then restarted by Borg, which drives the\nnumber of working tasks down even more.\n\nAs a result, some graphs on the service dashboard turn an alarming shade of\nred and SRE is paged. In response, SREs temporarily add capacity to the Asian\ndatacenter by increasing the number of tasks available for the Shakespeare\njob. By doing so, they\u2019re able to restore the Shakespeare service in the Asian\ncluster.\n\nAfterward, the SRE team writes a postmortem detailing the chain of events,\nwhat went well, what could have gone better, and a number of action items to\nprevent this scenario from occurring again. For example, in the case of a\nservice overload, the GSLB load balancer will redirect some traffic to\nneighboring datacenters. Also, the SRE team turns on autoscaling, so that the\nnumber of tasks automatically increases with traffic, so they don\u2019t have to\nworry about this type of issue again.\n\n# Closing Remarks\n\nWhen systems are overloaded, something needs to give in order to remedy the\nsituation. Once a service passes its breaking point, it is better to allow\nsome user-visible errors or lower-quality results to slip through than try to\nfully serve every request. Understanding where those breaking points are and\nhow the system behaves beyond them is critical for service owners who want to\navoid cascading failures.\n\nWithout proper care, some system changes meant to reduce background errors or\notherwise improve the steady state can expose the service to greater risk of a\nfull outage. Retrying on failures, shifting load around from unhealthy\nservers, killing unhealthy servers, adding caches to improve performance or\nreduce latency: all of these might be implemented to improve the normal case,\nbut can improve the chance of causing a large-scale failure. Be careful when\nevaluating changes to ensure that one outage is not being traded for another.\n\n^107See Wikipedia, \u201cPositive feedback,\u201d\nhttps://en.wikipedia.org/wiki/Positive_feedback.\n\n^108A watchdog is often implemented as a thread that wakes up periodically to\nsee whether work has been done since the last time it checked. If not, it\nassumes that the server is stuck and kills it. For instance, requests of a\nknown type can be sent to the server at regular intervals; if one hasn\u2019t been\nreceived or processed when expected, this may indicate failure\u2014of the server,\nthe system sending requests, or the intermediate network.\n\n^109This is often not a good assumption due to geography; see also Job and\nData Organization.\n\n^110An instructive exercise, left for the reader: write a simple simulator and\nsee how the amount of useful work the backend can do varies with how much it\u2019s\noverloaded and how many retries are permitted.\n\n^111Sometimes you find that a meaningful proportion of your actual serving\ncapacity is as a function of serving from a cache, and if you lost access to\nthat cache, you wouldn\u2019t actually be able to serve that many queries. A\nsimilar observation holds for latency: a cache can help you achieve latency\ngoals (by lowering the average response time when the query is servable from\ncache) that you possibly couldn\u2019t meet without that cache.\n\nPrevious\n\nChapter 21 - Handling Overload\n\nNext\n\nChapter 23 - Managing Critical State: Distributed Consensus for Reliability\n\nCopyright \u00a9 2017 Google, Inc. Published by O'Reilly Media, Inc. Licensed under\nCC BY-NC-ND 4.0\n\n", "frontpage": false}
