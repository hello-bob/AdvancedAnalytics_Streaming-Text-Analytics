{"aid": "40248506", "title": "Hierarchical Navigable Small Worlds (HNSW)", "url": "https://www.pinecone.io/learn/series/faiss/hnsw/", "domain": "pinecone.io", "votes": 2, "user": "thunderbong", "posted_at": "2024-05-03 15:05:36", "comments": 0, "source_title": "Hierarchical Navigable Small Worlds (HNSW) | Pinecone", "source_text": "Hierarchical Navigable Small Worlds (HNSW) | Pinecone\n\nOpens in a new window Opens an external website Opens an external website in a\nnew window\n\nThis website utilizes technologies such as cookies to enable essential site\nfunctionality, as well as for analytics, personalization, and targeted\nadvertising purposes. You may change your settings at any time or accept the\ndefault settings. You may close this banner to continue with only essential\ncookies. Cookie Policy\n\nAnnouncementNew serverless free plan with 3x capacityLearn more\n\n# Hierarchical Navigable Small Worlds (HNSW)\n\n  * Foundations of HNSW\n  * Graph Construction\n  * Implementation of HNSW\n  * References\n\nHierarchical Navigable Small World (HNSW) graphs are among the top-performing\nindexes for vector similarity search[1]. HNSW is a hugely popular technology\nthat time and time again produces state-of-the-art performance with super fast\nsearch speeds and fantastic recall.\n\nYet despite being a popular and robust algorithm for approximate nearest\nneighbors (ANN) searches, understanding how it works is far from easy.\n\nNote: Pinecone lets you build scalable, performant vector search into\napplications without knowing anything about HNSW or vector indexing libraries.\nBut we know you like seeing how things work, so enjoy the guide!\n\nThis article helps demystify HNSW and explains this intelligent algorithm in\nan easy-to-understand way. Towards the end of the article, we\u2019ll look at how\nto implement HNSW using Faiss and which parameter settings give us the\nperformance we need.\n\n## Foundations of HNSW\n\nWe can split ANN algorithms into three distinct categories; trees, hashes, and\ngraphs. HNSW slots into the graph category. More specifically, it is a\nproximity graph, in which two vertices are linked based on their proximity\n(closer vertices are linked) \u2014 often defined in Euclidean distance.\n\nThere is a significant leap in complexity from a \u2018proximity\u2019 graph to\n\u2018hierarchical navigable small world\u2019 graph. We will describe two fundamental\ntechniques that contributed most heavily to HNSW: the probability skip list,\nand navigable small world graphs.\n\n### Probability Skip List\n\nThe probability skip list was introduced way back in 1990 by William Pugh [2].\nIt allows fast search like a sorted array, while using a linked list structure\nfor easy (and fast) insertion of new elements (something that is not possible\nwith sorted arrays).\n\nSkip lists work by building several layers of linked lists. On the first\nlayer, we find links that skip many intermediate nodes/vertices. As we move\ndown the layers, the number of \u2018skips\u2019 by each link is decreased.\n\nTo search a skip list, we start at the highest layer with the longest \u2018skips\u2019\nand move along the edges towards the right (below). If we find that the\ncurrent node \u2018key\u2019 is greater than the key we are searching for \u2014 we know we\nhave overshot our target, so we move down to previous node in the next level.\n\nA probability skip list structure, we start on the top layer. If our current\nkey is greater than the key we are searching for (or we reach end), we drop to\nthe next layer.\n\nHNSW inherits the same layered format with longer edges in the highest layers\n(for fast search) and shorter edges in the lower layers (for accurate search).\n\n### Navigable Small World Graphs\n\nVector search using Navigable Small World (NSW) graphs was introduced over the\ncourse of several papers from 2011-14 [4, 5, 6]. The idea is that if we take a\nproximity graph but build it so that we have both long-range and short-range\nlinks, then search times are reduced to (poly/)logarithmic complexity.\n\nEach vertex in the graph connects to several other vertices. We call these\nconnected vertices friends, and each vertex keeps a friend list, creating our\ngraph.\n\nWhen searching an NSW graph, we begin at a pre-defined entry-point. This entry\npoint connects to several nearby vertices. We identify which of these vertices\nis the closest to our query vector and move there.\n\nThe search process through a NSW graph. Starting at a pre-defined entry point,\nthe algorithm greedily traverses to connected vertices that are nearer to the\nquery vector.\n\nWe repeat the greedy-routing search process of moving from vertex to vertex by\nidentifying the nearest neighboring vertices in each friend list. Eventually,\nwe will find no nearer vertices than our current vertex \u2014 this is a local\nminimum and acts as our stopping condition.\n\nNavigable small world models are defined as any network with\n(poly/)logarithmic complexity using greedy routing. The efficiency of greedy\nrouting breaks down for larger networks (1-10K+ vertices) when a graph is not\nnavigable [7].\n\nThe routing (literally the route we take through the graph) consists of two\nphases. We start with the \u201czoom-out\u201d phase where we pass through low-degree\nvertices (degree is the number of links a vertex has) \u2014 and the later \u201czoom-\nin\u201d phase where we pass through higher-degree vertices [8].\n\nHigh-degree vertices have many links, whereas low-degree vertices have very\nfew links.\n\nOur stopping condition is finding no nearer vertices in our current vertex\u2019s\nfriend list. Because of this, we are more likely to hit a local minimum and\nstop too early when in the zoom-out phase (fewer links, less likely to find a\nnearer vertex).\n\nTo minimize the probability of stopping early (and increase recall), we can\nincrease the average degree of vertices, but this increases network complexity\n(and search time). So we need to balance the average degree of vertices\nbetween recall and search speed.\n\nAnother approach is to start the search on high-degree vertices (zoom-in\nfirst). For NSW, this does improve performance on low-dimensional data. We\nwill see that this is also a significant factor in the structure of HNSW.\n\n### Creating HNSW\n\nHNSW is a natural evolution of NSW, which borrows inspiration from\nhierarchical multi-layers from Pugh\u2019s probability skip list structure.\n\nAdding hierarchy to NSW produces a graph where links are separated across\ndifferent layers. At the top layer, we have the longest links, and at the\nbottom layer, we have the shortest.\n\nLayered graph of HNSW, the top layer is our entry point and contains only the\nlongest links, as we move down the layers, the link lengths become shorter and\nmore numerous.\n\nDuring the search, we enter the top layer, where we find the longest links.\nThese vertices will tend to be higher-degree vertices (with links separated\nacross multiple layers), meaning that we, by default, start in the zoom-in\nphase described for NSW.\n\nWe traverse edges in each layer just as we did for NSW, greedily moving to the\nnearest vertex until we find a local minimum. Unlike NSW, at this point, we\nshift to the current vertex in a lower layer and begin searching again. We\nrepeat this process until finding the local minimum of our bottom layer \u2014\nlayer 0.\n\nThe search process through the multi-layer structure of an HNSW graph.\n\n## Graph Construction\n\nDuring graph construction, vectors are iteratively inserted one-by-one. The\nnumber of layers is represented by parameter L. The probability of a vector\ninsertion at a given layer is given by a probability function normalized by\nthe \u2018level multiplier\u2019 m_L, where m_L = ~0 means vectors are inserted at layer\n0 only.\n\nThe probability function is repeated for each layer (other than layer 0). The\nvector is added to its insertion layer and every layer below it.\n\nThe creators of HNSW found that the best performance is achieved when we\nminimize the overlap of shared neighbors across layers. Decreasing m_L can\nhelp minimize overlap (pushing more vectors to layer 0), but this increases\nthe average number of traversals during search. So, we use an m_L value which\nbalances both. A rule of thumb for this optimal value is 1/ln(M) [1].\n\nGraph construction starts at the top layer. After entering the graph the\nalgorithm greedily traverse across edges, finding the ef nearest neighbors to\nour inserted vector q \u2014 at this point ef = 1.\n\nAfter finding the local minimum, it moves down to the next layer (just as is\ndone during search). This process is repeated until reaching our chosen\ninsertion layer. Here begins phase two of construction.\n\nThe ef value is increased to efConstruction (a parameter we set), meaning more\nnearest neighbors will be returned. In phase two, these nearest neighbors are\ncandidates for the links to the new inserted element q and as entry points to\nthe next layer.\n\nM neighbors are added as links from these candidates \u2014 the most\nstraightforward selection criteria are to choose the closest vectors.\n\nAfter working through multiple iterations, there are two more parameters that\nare considered when adding links. M_max, which defines the maximum number of\nlinks a vertex can have, and M_max0, which defines the same but for vertices\nin layer 0.\n\nExplanation of the number of links assigned to each vertex and the effect of\nM, M_max, and M_max0.\n\nThe stopping condition for insertion is reaching the local minimum in layer 0.\n\n## Implementation of HNSW\n\nWe will implement HNSW using the Facebook AI Similarity Search (Faiss)\nlibrary, and test different construction and search parameters and see how\nthese affect index performance.\n\nTo initialize the HNSW index we write:\n\nIn[2]:\n\n    \n    \n    # setup our HNSW parameters d = 128 # vector size M = 32 index = faiss.IndexHNSWFlat(d, M) print(index.hnsw)\n\nOut[2]:\n\n    \n    \n    <faiss.swigfaiss.HNSW; proxy of <Swig Object of type 'faiss::HNSW *' at 0x7f91183ef120> >\n\nWith this, we have set our M parameter \u2014 the number of neighbors we add to\neach vertex on insertion, but we\u2019re missing M_max and M_max0.\n\nIn Faiss, these two parameters are set automatically in the set_default_probas\nmethod, called at index initialization. The M_max value is set to M, and\nM_max0 set to M*2 (find further detail in the notebook).\n\nBefore building our index with index.add(xb), we will find that the number of\nlayers (or levels in Faiss) are not set:\n\nIn[3]:\n\n    \n    \n    # the HNSW index starts with no levels index.hnsw.max_level\n\nOut[3]:\n\n    \n    \n    -1\n\nIn[4]:\n\n    \n    \n    # and levels (or layers) are empty too levels = faiss.vector_to_array(index.hnsw.levels) np.bincount(levels)\n\nOut[4]:\n\n    \n    \n    array([], dtype=int64)\n\nIf we go ahead and build the index, we\u2019ll find that both of these parameters\nare now set.\n\nIn[5]:\n\n    \n    \n    index.add(xb)\n\nIn[6]:\n\n    \n    \n    # after adding our data we will find that the level # has been set automatically index.hnsw.max_level\n\nOut[6]:\n\n    \n    \n    4\n\nIn[7]:\n\n    \n    \n    # and levels (or layers) are now populated levels = faiss.vector_to_array(index.hnsw.levels) np.bincount(levels)\n\nOut[7]:\n\n    \n    \n    array([ 0, 968746, 30276, 951, 26, 1], dtype=int64)\n\nHere we have the number of levels in our graph, 0 -> 4 as described by\nmax_level. And we have levels, which shows the distribution of vertices on\neach level from 0 to 4 (ignoring the first 0 value). We can even find which\nvector is our entry point:\n\nIn[8]:\n\n    \n    \n    index.hnsw.entry_point\n\nOut[8]:\n\n    \n    \n    118295\n\nThat\u2019s a high-level view of our Faiss-flavored HNSW graph, but before we test\nthe index, let\u2019s dive a little deeper into how Faiss is building this\nstructure.\n\n#### Graph Structure\n\nWhen we initialize our index we pass our vector dimensionality d and number of\nneighbors for each vertex M. This calls the method \u2018set_default_probas\u2019,\npassing M and 1 / log(M) in the place of levelMult (equivalent to m_L above).\nA Python equivalent of this method looks like:\n\n    \n    \n    def set_default_probas(M: int, m_L: float): nn = 0 # set nearest neighbors count = 0 cum_nneighbor_per_level = [] level = 0 # we start at level 0 assign_probas = [] while True: # calculate probability for current level proba = np.exp(-level / m_L) * (1 - np.exp(-1 / m_L)) # once we reach low prob threshold, we've created enough levels if proba < 1e-9: break assign_probas.append(proba) # neighbors is == M on every level except level 0 where == M*2 nn += M*2 if level == 0 else M cum_nneighbor_per_level.append(nn) level += 1 return assign_probas, cum_nneighbor_per_level\n\nHere we are building two vectors \u2014 assign_probas, the probability of insertion\nat a given layer, and cum_nneighbor_per_level, the cumulative total of nearest\nneighbors assigned to a vertex at different insertion levels.\n\nIn[10]:\n\n    \n    \n    assign_probas, cum_nneighbor_per_level = set_default_probas( 32, 1/np.log(32) ) assign_probas, cum_nneighbor_per_level\n\nOut[10]:\n\n    \n    \n    ([0.96875, 0.030273437499999986, 0.0009460449218749991, 2.956390380859371e-05, 9.23871994018553e-07, 2.887099981307982e-08], [64, 96, 128, 160, 192, 224])\n\nFrom this, we can see the significantly higher probability of inserting a\nvector at level 0 than higher levels (although, as we will explain below, the\nprobability is not exactly as defined here). This function means higher levels\nare more sparse, reducing the likelihood of \u2018getting stuck\u2019, and ensuring we\nstart with longer range traversals.\n\nOur assign_probas vector is used by another method called random_level \u2014 it is\nin this function that each vertex is assigned an insertion level.\n\n    \n    \n    def random_level(assign_probas: list, rng): # get random float from 'r'andom 'n'umber 'g'enerator f = rng.uniform() for level in range(len(assign_probas)): # if the random float is less than level probability... if f < assign_probas[level]: # ... we assert at this level return level # otherwise subtract level probability and try again f -= assign_probas[level] # below happens with very low probability return len(assign_probas) - 1\n\nWe generate a random float using Numpy\u2019s random number generator rng\n(initialized below) in f. For each level, we check if f is less than the\nassigned probability for that level in assign_probas \u2014 if so, that is our\ninsertion layer.\n\nIf f is too high, we subtract the assign_probas value from f and try again for\nthe next level. The result of this logic is that vectors are most likely going\nto be inserted at level 0. Still, if not, there is a decreasing probability of\ninsertion at ease increment level.\n\nFinally, if no levels satisfy the probability condition, we insert the vector\nat the highest level with return len(assign_probas) - 1. If we compare the\ndistribution between our Python implementation and that of Faiss, we see very\nsimilar results:\n\nIn[12]:\n\n    \n    \n    chosen_levels = [] rng = np.random.default_rng(12345) for _ in range(1_000_000): chosen_levels.append(random_level(assign_probas, rng))\n\nIn[13]:\n\n    \n    \n    np.bincount(chosen_levels)\n\nOut[13]:\n\n    \n    \n    array([968821, 30170, 985, 23, 1], dtype=int64)\n\nDistribution of vertices across layers in both the Faiss implementation (left)\nand the Python implementation (right).\n\nThe Faiss implementation also ensures that we always have at least one vertex\nin the highest layer to act as the entry point to our graph.\n\n### HNSW Performance\n\nNow that we\u2019ve explored all there is to explore on the theory behind HNSW and\nhow this is implemented in Faiss \u2014 let\u2019s look at the effect of different\nparameters on our recall, search and build times, and the memory usage of\neach.\n\nWe will be modifying three parameters: M, efSearch, and efConstruction. And we\nwill be indexing the Sift1M dataset, which you can download and prepare using\nthis script.\n\nAs we did before, we initialize our index like so:\n\n    \n    \n    index = faiss.IndexHNSWFlat(d, M)\n\nThe two other parameters, efConstruction and efSearch can be modified after we\nhave initialized our index.\n\n    \n    \n    index.hnsw.efConstruction = efConstruction index.add(xb) # build the index index.hnsw.efSearch = efSearch # and now we can search index.search(xq[:1000], k=1)\n\nOur efConstruction value must be set before we construct the index via\nindex.add(xb), but efSearch can be set anytime before searching.\n\nLet\u2019s take a look at the recall performance first.\n\nRecall@1 performance for various M, efConstruction, and efSearch parameters.\n\nHigh M and efSearch values can make a big difference in recall performance \u2014\nand it\u2019s also evident that a reasonable efConstruction value is needed. We can\nalso increase efConstruction to achieve higher recall at lower M and efSearch\nvalues.\n\nHowever, this performance does not come for free. As always, we have a\nbalancing act between recall and search time \u2014 let\u2019s take a look.\n\nSearch time in \u03bcs for various M, efConstruction, and efSearch parameters when\nsearching for 1000 queries. Note that the y-axis is using a log scale.\n\nAlthough higher parameter values provide us with better recall, the effect on\nsearch times can be dramatic. Here we search for 1000 similar vectors\n(xq[:1000]), and our recall/search-time can vary from 80%-1ms to 100%-50ms. If\nwe\u2019re happy with a rather terrible recall, search times can even reach 0.1ms.\n\nIf you\u2019ve been following our articles on vector similarity search, you may\nrecall that efConstruction has a negligible effect on search-time \u2014 but that\nis not the case here...\n\nWhen we search using a few queries, it is true that efConstruction has little\neffect on search time. But with the 1000 queries used here, the small effect\nof efConstruction becomes much more significant.\n\nIf you believe your queries will mostly be low volume, efConstruction is a\ngreat parameter to increase. It can improve recall with little effect on\nsearch time, particularly when using lower M values.\n\nefConstruction and search time when searching for only one query. When using\nlower M values, the search time remains almost unchanged for different\nefConstruction values.\n\nThat all looks great, but what about the memory usage of the HNSW index? Here\nthings can get slightly less appealing.\n\nMemory usage with increasing values of M using our Sift1M dataset. efSearch\nand efConstruction have no effect on the memory usage.\n\nBoth efConstruction and efSearch do not affect index memory usage, leaving us\nonly with M. Even with M at a low value of 2, our index size is already above\n0.5GB, reaching almost 5GB with an M of 512.\n\nSo although HNSW produces incredible performance, we need to weigh that\nagainst high memory usage and the inevitable high infrastructure costs that\nthis can produce.\n\n#### Improving Memory Usage and Search Speeds\n\nHNSW is not the best index in terms of memory utilization. However, if this is\nimportant and using another index isn\u2019t an option, we can improve it by\ncompressing our vectors using product quantization (PQ). Using PQ will reduce\nrecall and increase search time \u2014 but as always, much of ANN is a case of\nbalancing these three factors.\n\nIf, instead, we\u2019d like to improve our search speeds \u2014 we can do that too! All\nwe do is add an IVF component to our index. There is plenty to discuss when\nadding IVF or PQ to our index, so we wrote an entire article on mixing-and-\nmatching of indexes.\n\nThat\u2019s it for this article covering the Hierarchical Navigable Small World\ngraph for vector similarity search! Now that you\u2019ve learned the intuition\nbehind HNSW and how to implement it in Faiss, you\u2019re ready to go ahead and\ntest HNSW indexes in your own vector search applications, or use a managed\nsolution like Pinecone or OpenSearch that has vector search ready-to-go!\n\nIf you\u2019d like to continue learning more about vector search and how you can\nuse it to supercharge your own applications, we have a whole set of learning\nmaterials aiming to bring you up to speed with the world of vector search.\n\n## References\n\n[1] E. Bernhardsson, ANN Benchmarks (2021), GitHub\n\n[2] W. Pugh, Skip lists: a probabilistic alternative to balanced trees (1990),\nCommunications of the ACM, vol. 33, no.6, pp. 668-676\n\n[3] Y. Malkov, D. Yashunin, Efficient and robust approximate nearest neighbor\nsearch using Hierarchical Navigable Small World graphs (2016), IEEE\nTransactions on Pattern Analysis and Machine Intelligence\n\n[4] Y. Malkov et al., Approximate Nearest Neighbor Search Small World Approach\n(2011), International Conference on Information and Communication Technologies\n& Applications\n\n[5] Y. Malkov et al., Scalable Distributed Algorithm for Approximate Nearest\nNeighbor Search Problem in High Dimensional General Metric Spaces (2012),\nSimilarity Search and Applications, pp. 132-147\n\n[6] Y. Malkov et al., Approximate nearest neighbor algorithm based on\nnavigable small world graphs (2014), Information Systems, vol. 45, pp. 61-68\n\n[7] M. Boguna et al., Navigability of complex networks (2009), Nature Physics,\nvol. 5, no. 1, pp. 74-80\n\n[8] Y. Malkov, A. Ponomarenko, Growing homophilic networks are natural\nnavigable small worlds (2015), PloS one\n\nFacebook Research, Faiss HNSW Implementation, GitHub\n\nShare via:\n\nPreviousProduct QuantizationNextComposite Indexes and the Faiss Index Factory\n\nFaiss: The Missing Manual\n\nChapters\n\n  1. Introduction to Facebook AI Similarity Search (Faiss)\n  2. Nearest Neighbor Indexes for Similarity Search\n  3. Locality Sensitive Hashing (LSH): The Illustrated Guide\n  4. Random Projection for Locality Sensitive Hashing\n  5. Product Quantization\n  6. Hierarchical Navigable Small Worlds (HNSW)\n\n     * Foundations of HNSW\n     * Graph Construction\n     * Implementation of HNSW\n     * References\n\n  7. Composite Indexes and the Faiss Index Factory\n\nProduct\n\nOverviewDocumentationIntegrationsTrust and Security\n\nSolutions\n\nCustomersRAGSemantic SearchMulti-Modal SearchCandidate\nGenerationClassification\n\nResources\n\nLearning CenterCommunityPinecone BlogSupport CenterSystem StatusWhat is a\nVector Database?What is Retrieval Augmented Generation (RAG)?\n\nCompany\n\nAboutPartnersCareersNewsroomContact\n\nLegal\n\nTermsPrivacyCookiesCookie Preferences\n\n\u00a9 Pinecone Systems, Inc. | San Francisco, CA\n\nPinecone is a registered trademark of Pinecone Systems, Inc.\n\n", "frontpage": false}
