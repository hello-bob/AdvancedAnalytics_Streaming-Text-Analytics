{"aid": "40130147", "title": "HTTP Fundamentals: Understanding Undici and Its Working Mechanism", "url": "https://blog.platformatic.dev/http-fundamentals-understanding-undici-and-its-working-mechanism", "domain": "platformatic.dev", "votes": 2, "user": "feross", "posted_at": "2024-04-23 09:32:26", "comments": 0, "source_title": "HTTP Fundamentals: Understanding Undici and its Working Mechanism", "source_text": "HTTP Fundamentals: Understanding Undici and its Working Mechanism\n\n# HTTP Fundamentals: Understanding Undici and its Working Mechanism\n\nMatteo Collina\n\n\u00b7Apr 23, 2024\u00b7\n\n11 min read\n\nNavigating through Node.js http/https can be a daunting task that often calls\nfor updates. This is particularly evident in how it uses the same API for both\nclient and server functionalities while intricately tying connection pooling\nwith the public API.\n\nA significant hurdle arises when considering modifications to http/https\nwithout disrupting frameworks like Express, which relies on the internal\nworkings of base classes, thereby altering the prototype.\n\nThis is where Undici, a modern HTTP client library for Node.js, steps in to\naddress these challenges. In this article, we delve into Undici's purpose and\nits operational mechanisms.\n\n### Why Undici.request?\n\nUndici.request is designed to optimize application performance without\ncompromising developer experience. It is highly configurable and harnesses the\npower of Node.js streams, while facilitating efficient HTTP/1.1 pipelining. By\nprioritizing speed and flexibility, Undici.request ensures applications run\nseamlessly.\n\n### Why Undici.fetch?\n\nUndici.fetch boasts an impressive 88% passing rate in the WPT tests, making it\nnearly compliant with the fetch standard. Leveraging WHATWG web streams, it\nenables isomorphic code usage, making it an ideal choice for cross-platform\ndevelopment. Moreover, by decoupling the protocol from the API and offering\nsupport for HTTP/2, Undici.fetch offers versatility and performance\nenhancements, including HTTP/1.1 pipelining.\n\n### Is overhead a discriminating factor?\n\nDespite being integrated into the Node core, Undici.fetch outperforms its\ncounterparts in terms of speed and efficiency. While Undici's fetch version\nmay not be the fastest, its overall performance exceeds other options\navailable in the Node.js ecosystem.\n\nAs seen above, undici-fetch can process up to 5043.80 requests per second!\n\n# Undici\u2019s Working Mechanisms\n\n### Normal HTTP\n\nIn traditional HTTP transactions, the process unfolds as follows: when a TCP\nrequest is initiated between a client and a server, and the client requests a\nresource, typically referred to as a \"blob,\" the server promptly responds to\nthis request within a timeframe of less than 50 milliseconds. This response\ntime encompasses the round trip duration plus the time allocated for server-\nside processing.\n\nFollowing this interaction, the server awaits further requests, processing\nthem sequentially in accordance with the typical flow observed in browsers and\nHTTP clients built on Node.js's node-core framework. However, this sequential\nprocessing approach presents a notable challenge: the sockets remain inactive\nduring idle periods, leading to potential inefficiencies in resource\nutilization.\n\n### HTTP/1.1 Pipelining in Action\n\nPipelining operates like a bulk request application, enabling applications to\nstack multiple HTTP requests upon each other. Subsequently, the server\nresponds to these requests in sequential order.\n\nIn the image above, we can see how an HTTP call between a client and server\ngoes. The client can request a blob of HTML or any API call and the server\nwill send a response in JSON, or other predefined data display format, within\n20 to 40 milliseconds.\n\nThe key advantage of pipelining is demonstrated here. The client immediately\nsends a second request instead of waiting for the server\u2019s response to the\nfirst request.\n\nThis is visualized in the image above as two GET requests are sent\nconsecutively before receiving any response.\n\nBy using pipelining, the process of receiving numerous responses swiftly via\nthe same socket is facilitated. This not only reduces the reliance on\nadditional sockets, but also optimizes their utilization, thereby expanding\nthe application's TCP window.\n\nConsequently, pipelining presents an efficient solution for those seeking to\nmanage and restrict the number of available sockets effectively.\n\n### Strict Serialization of Responses\n\nA potential challenge arises due to the strict serialization of responses,\nwherein the server adheres to the order of incoming requests.\n\nShould a preceding request encounter prolonged processing time, subsequent\nrequests may face premature termination. This aspect renders such mechanisms\nless favorable for rendering pipelines in browsers, primarily due to the\nunpredictable nature of request processing times.\n\nNevertheless, in Node.js systems, particularly for API calls, leveraging\nstrict response serialization can be beneficial by enhancing socket\nreusability and fostering improved response times compared to alternative\nmethods.\n\nConsequently, despite its limitations in certain contexts, strict response\nserialization remains a viable option for optimizing performance in Node.js-\nbased applications.\n\n### Undici Design Principles\n\nUndici serves as the cornerstone for Node.js' evolution towards its \"http\nnext\" iteration. It achieves this by meticulously segregating the APIs\nutilized by developers from the underlying internal systems that facilitate\nthese APIs.\n\nMoreover, Undici offers support for multiple APIs, each tailored to cater to\nvarying developer experiences and performance profiles. It accommodates\nHTTP/1.1 and HTTP/2 protocols through a unified API, ensuring compatibility\nacross different standards. In the future, it could be adapted to support\nHTTP/3.\n\nIts callback-based system is central to Undici's architecture, allowing users\nto avoid errorback or event-driven mechanisms. This deliberate choice\nstreamlines internal processes while focusing on efficiency and simplicity.\n\nUndici manually manages a connection pool to optimize performance, removes\nmost overhead and memory allocation, and minimizes the transition between\nnative code and JavaScript.\n\n### What is a dispatcher?\n\nIn Node.js, a dispatcher is created in a module to oversee or track server\nactions and emitted events within an application. The dispatcher empowers\ndevelopers to fine-tune essential parameters such as pipeline configurations,\nkeepAlive settings, retries, and the desired number of sockets to be opened\nfor a specific destination.\n\nThese configurations play a pivotal role in optimizing the performance and\nreliability of production systems, thereby circumventing potential issues.\nNotably, one variant of a dispatcher is known as an agent, which is the key\ncomponent that oversees the connection pooling.\n\nIn Undici, a global dispatcher is set and stored in the process. This global\ndispatcher can easily be reused. This approach underpins fetch() using the\nsame global dispatcher.\n\nBelow is a demo of how to configure a dispatcher in Undici\n\n    \n    \n    import { request, setGlobalDispatcher, Agent } from 'undici'; const agent = Agent ({ keepAliveTimeout: 10, keepAliveMaxTimeout: 10 }) setGlobalDispatcher(agent); const { statusCode, headers, trailers, body } = await request('http://localhost:3000/foo') console.log('response received', statusCode) console.log('headers', headers) console.log('data', await body.json()) console.log('trailers', trailers)\n\n### Dispatcher Hierarchy\n\nThe image above illustrates the dispatcher hierarchy. Let's take a deeper\nlook:\n\n  * The Dispatcher is the abstract concept at the top of the hierarchy, extending the DispatcherBase.\n\n  * The Client encompasses core elements and is wrapped into a single socket.\n\n  * Pools represent multiple sockets, with the BalancedPool allowing developers to do load balancing between multiple peers.\n\n  * The Agent uses several pools and creates a pool for each called origin, signifying the domain name plus the host and the port.\n\n### How to Mock a Request with Unidici\n\nMocking a request with Undici allows you to simulate a network response, which\nis mainly for testing purposes. This feature offers some advantages for\ntesting and development.\n\nThese advantages include isolated testing, mocking edge cases, and properly\nhandling errors. Undici mocking allows you to test your code\u2019s logic in\nisolation. This gives you the freedom to create custom responses that trigger\nspecific code paths for proper testing.\n\nMocking also allows you to simulate rare events that can crash your\napplication when they occur. You can also simulate common error scenarios in\nyour application by defining custom error codes or unexpected data. This\nallows you to better handle rare events, improving your code\u2019s overall\nrobustness and quality.\n\n    \n    \n    import { strict as assert } from \"assert\"; import { MockAgent, setGlobalDispatcher, fetch } from \"undici\"; // Create a new mock agent const mockAgent = new MockAgent(); // Set the mock agent as the global dispatcher setGlobalDispatcher(mockAgent); // Provide the base URL to the request const mockPool = mockAgent.get(\"http://localhost:3000\"); // Intercept the request mockPool .intercept({ path: \"/bank-transfer\", method: \"POST\", headers: { \"X-TOKEN-SECRET\": \"SuperSecretToken\", }, body: JSON.stringify({ recipient: \"1234567890\", amount: \"100\", }), }) .reply(200, { message: \"transaction processed\", }); // using fetch() with the mock agent async function performRequest() { const response = await fetch(\"http://localhost:3000/bank-transfer\", { method: \"POST\", headers: { \"Content-Type\": \"application/json\", \"X-TOKEN-SECRET\": \"SuperSecretToken\", }, body: JSON.stringify({ recipient: \"1234567890\", amount: \"100\", }), }); const data = await response.json(); assert.equal(data.message, \"transaction processed\"); console.log(\"Request successful:\", data.message); } performRequest().catch(console.error);\n\n\ud83d\udca1\n\nBelow is a quick explanation of the code we wrote above, where we:\n\n  * imported assert and Undici\n\n  * created and set a mockAgent\n\n  * used localhost as the base URL for the request\n\n  * intercepted the request with MockPool and sent a reply that the transaction is processed\n\n  * used fetch() with the mockAgent to fetch the bank-transfer in the local host\n\n  * handled any possible error from the fetch() request with catch and printed in the console\n\n### Handlers\n\nIn Node.js, a handler is a function\u2014or module in microservice\napplications\u2014that processes HTTP requests and responses. So, what is the\nconnection between handlers and dispatchers? Once a dispatcher gets any\nrequests, it directs it to a relevant handler.\n\nHere are some notable dispatchers and what they do:\n\n  * dispatcher.request() will create the RequestHandler\n\n  * dispatcher.stream() will create the StreamHandler\n\n  * dispatcher.fetch() will create the FetchHandler\n\n  * dispatcher.pipeline() will create the PipelineHandler\n\nBelow is what the dispatch handler looks like:\n\n    \n    \n    export interface DispatchHandlers { /** Invoked before request is dispatched on socket. May be invoked multiple times when a request is retried when the request at the head of the pipeline fails. */ onConnect?(abort: () => void): void; /** Invoked when an error has occurred. */ onError?(err: Error): void; /** Invoked when request is upgraded either due to a `Upgrade` header or `CONNECT` method. */ onUpgrade?(statusCode: number, headers: Buffer[] | string[] | null, socket: Duplex): void; /** Invoked when response is received, before headers have been read. **/ onResponseStarted?(): void; /** Invoked when statusCode and headers have been received. May be invoked multiple times due to 1xx informational headers. */ onHeaders?(statusCode: number, headers: Buffer[] | string[] | null, resume: () => void, statusText: string): boolean; /** Invoked when response payload data is received. */ onData?(chunk: Buffer): boolean; /** Invoked when response payload and trailers have been received and the request has completed. */ onComplete?(trailers: string[] | null): void; /** Invoked when a body chunk is sent to the server. May be invoked multiple times for chunked requests */ onBodySent?(chunkSize: number, totalBytesSent: number): void; }\n\nThere is the onConnect callback that implements the interface when it is\ncalled. It has an onError callback for errors. There is also an onUpgrade to\nhandle future upgrades to the package. The onHeaders is called when there is a\nheader. It can be called multiple times when there are multiple status codes.\n\nThis interface is responsible for the library\u2019s speed.\n\n### Interceptors\n\nAn interceptor is a middleware component or mechanism that intercepts and\nmodifies requests and responses as they travel through a system. Interceptors\nare commonly used in frameworks and libraries to add functionality or to\nmanipulate data before it reaches its destination or agent.\n\nTo create an interceptor, you'll need to create a function that takes a\ndispatch function and returns a new intercepted dispatch. Inside the\ninterceptor, you can modify the request or response as needed.\n\nHere is an example of an interceptor that adds a header to the request:\n\n    \n    \n    import { getGlobalDispatcher, request } from \"undici\" const insertHeaderInterceptor = dispatch => { return function InterceptedDispatch(opts, handler) { opts.headers = { ...opts.headers, authorization: \"Bearer [Some token]\" } return dispatch(opts, handler) } } const res = await request('http://localhost:3000/', { dispatcher: getGlobalDispatcher().compose(insertHeaderInterceptor) }) console.log(res.statusCode); console.log(await res.body.text());\n\nTo modify the response, you can use the DecoratorHandler to intercept the\nresponse when it is received. Here is an example of an interceptor that clears\nthe headers from the response:\n\n    \n    \n    import { DecoratorHandler, getGlobalDispatcher, request } from \"undici\" const clearHeadersInterceptor = dispatch => { class ResultInterceptor extends DecoratorHandler { onHeaders (statusCode, headers, resume) { return super.onHeaders(statusCode, [], resume) } } return function InterceptedDispatch(opts, handler){ return dispatch(opts, new ResultInterceptor(handler)) } } const res = await request('http://localhost:3000/', { dispatcher: getGlobalDispatcher().compose(clearHeadersInterceptor) }) console.log(res.statusCode); console.log(res.headers); console.log(await res.body.text());\n\nUndici has a few built-in interceptors that can be used to change the behavior\nof the client: retry and redirect.\n\nThe retry interceptor will retry the request if it fails:\n\n    \n    \n    import { getGlobalDispatcher, interceptors, request } from \"undici\" const res = await request('http://localhost:3000/', { dispatcher: getGlobalDispatcher() .compose( interceptors.retry({ maxRetries: 3, minTimeout: 1000, maxTimeout: 10000, timeoutFactor: 2, retryAfter: true, }) ) }); console.log(res.statusCode); console.log(res.headers); console.log(await res.body.text());\n\nThe redirect interceptor will follow the redirect response:\n\n    \n    \n    import { getGlobalDispatcher, interceptors, request } from \"undici\" const res = await request('http://localhost:3000/', { dispatcher: getGlobalDispatcher() .compose( interceptors.redirect({ maxRedirections: 3, throwOnMaxRedirects: true, }) ) }); console.log(res.statusCode); console.log(res.headers); console.log(await res.body.text());\n\nUndici allows you to compose multiple interceptors together, allowing you to\nchain multiple interceptors to create more complex behaviors. The order of the\ninterceptors is important, as they will be executed in the order they are\ncomposed.\n\nBelow is an example of composing retry and redirect interceptors:\n\n    \n    \n    import { getGlobalDispatcher, interceptors, request } from \"undici\" const res = await request('http://localhost:3000/', { dispatcher: getGlobalDispatcher() .compose( interceptors.redirect({ maxRedirections: 3, throwOnMaxRedirects: true, }) ) .compose( interceptors.retry({ maxRetries: 3, minTimeout: 1000, maxTimeout: 10000, timeoutFactor: 2, retryAfter: true, }) ) }); console.log(res.statusCode); console.log(res.headers); console.log(await res.body.text());\n\n## Why is Fetch() slow?\n\nUndici's request() function typically outperforms fetch() in terms of speed.\nThis is primarily because fetch() utilizes considerable resources to create\nweb streams. If optimizing speed is a priority in your application,\nundici.request() offers a superior alternative.\n\nIt's worth noting that the Node.js team is actively addressing performance\nissues in fetch(), as evidenced by ongoing improvements seen here.\n\n# Wrapping up\n\nWhen it comes to mastering HTTP functionality in Node.js, Undici is the go-to\nsolution. Built to tackle the evolving challenges of web communication, Undici\npresents a robust HTTP client library that not only simplifies tasks but also\nturbocharges performance.\n\nUndici's appeal lies in its streamlined approach and efficient features, which\nnot only improve HTTP operations, but also elevate overall application\nperformance.\n\nWith Undici.request, developers gain a hassle-free method to optimize\napplication speed effortlessly. And with Undici.fetch ensuring smooth cross-\nplatform compatibility, developers can seamlessly navigate diverse development\nenvironments.\n\nBy delving into Undici's advanced mechanisms like pipelining and response\nhandling, developers can unlock unparalleled scalability and responsiveness\nfor their Node.js applications. Whether managing complex network requests or\nhandling high traffic volumes, Undici's user-friendly design and potent\ncapabilities make it an indispensable tool for any Node.js developer looking\nto enhance their HTTP capabilities.\n\nIn the realm of Node.js development, Undici offers a user-friendly and\npowerful pathway to optimized performance.\n\nhttpundiciNode.jsfetchplatformatic\n\n### Written by\n\n# Matteo Collina\n\n### Published on\n\n# Platformatic Blog\n\nShare this\n\n### More articles\n\nMatteo Collina\n\n# A Deep Dive into Meraki 1.0\n\nImagine this: your development team kicks off a new project which includes\ninnovative new features. ...\n\nMatteo Collina\n\n# Introducing the Platformatic Control Module\n\nWhen dealing with microservices, simplicity, precision, and efficiency are\ncritical. As part of our ...\n\nAlexandra Manos\n\n# Introducing the Platformatic April 2024 Launch\n\nFor the 12 million backend developers across the globe, the work week can\noften feel like an endless...\n\n\u00a92024 Platformatic Blog\n\nArchive\u00b7Privacy policy\u00b7Terms\n\nWrite on Hashnode\n\nPowered by Hashnode - Home for tech writers and readers\n\n", "frontpage": false}
