{"aid": "40246803", "title": "The Time Linkerd Erased My Load Balancer", "url": "https://matduggan.com/the-time-linkerd-erased-my-load-balancer/", "domain": "matduggan.com", "votes": 1, "user": "nalgeon", "posted_at": "2024-05-03 12:16:08", "comments": 0, "source_title": "The Time Linkerd Erased My Load Balancer", "source_text": "The Time Linkerd Erased My Load Balancer\n\nMenu\n\nArticle kubernetes\n\n# The Time Linkerd Erased My Load Balancer\n\n#### Mathew Duggan\n\n03 May 2024 \u2022 8 min read\n\nA cautionary tale of K8s CRDs and Linkerd.\n\nA few months ago I had the genius idea of transitioning our production load\nbalancer stack from Ingress to Gateway API in k8s. For those unaware, Ingress\nis the classic way of writing a configuration to tell a load balancer what\nroutes should hit what services, effectively how do you expose services to the\nInternet. Gateway API is the re-imagined process for doing this where the\nproblem domain is scoped, allowing teams more granular control over their\nspecific services routes.\n\nIngress\n\n    \n    \n    apiVersion: networking.k8s.io/v1 kind: IngressClass metadata: name: external-lb spec: controller: example.com/ingress-controller parameters: apiGroup: k8s.example.com kind: IngressParameters name: external-lb\n    \n    12345678910\n\nThis is what setting up a load balancer in Ingress looks like\n\nSource\n\nAfter conversations with various folks at GCP it became clear to me that while\nIngress wasn't deprecated or slated to be removed, Gateway API is where all\nthe new development and features are moving to. I decided that we were a good\ncandidate for the migration since we are a microservice based backend with\nlower and higher priority hostnames, meaning we could safely test the feature\nwithout cutting over all of our traffic at the same time.\n\nI had this idea that we would turn on both Ingress and Gateway API and then\ncut between the two different IP addresses at the Cloudflare level. From my\nlow-traffic testing this approach seemed to work ok, with me being able to\nswitch between the two and then letting Gateway API bake for a week or two to\nshake out any problems. Then I decided to move to prod. Due to my lack of\nissues in the lower environments I decided that I wouldn't set up Cloudflare\nload balancing between the two and manage the cut-over in Terraform. This\nturned out to be a giant mistake.\n\nThe long and short of it is that the combination of Gateway API and Linkerd in\nGKE fell down under high volume of requests. Low request volume there were no\nproblems, but once we got to around 2k requests a second the Linkerd-proxy\nsidecar container memory usage started to grow unbounded. When I attempted to\ncut back from Gateway API to Ingress, I encountered a GKE bug I hadn't seen\nbefore in the lower environments.\n\n\"Translation failed: invalid ingress spec: service \"my_namespace/my_service\"\nis type \"ClusterIP\", expected \"NodePort\" or \"LoadBalancer\";\n\nWhat we were seeing was a mismatch between the annotations automatically added\nby GKE.\n\nIngress adds these annotations: cloud.google.com/neg: '{\"ingress\":true}'\ncloud.google.com/neg-status:\n'{\"network_endpoint_groups\":{\"80\":\"k8s1pokfef...\"},\"zones\":[\"us-\ncentral1-a\",\"us-central1-b\",\"us-central1-f\"]}'\n\nGateway adds these annotations: cloud.google.com/neg:\n'{\"exposed_ports\":{\"80\":{}}}' cloud.google.com/neg-status:\n'{\"network_endpoint_groups\":{\"80\":\"k8s1-oijfoijsdoifj-...\"},\"zones\":[\"us-\ncentral1-a\",\"us-central1-b\",\"us-central1-f\"]}'\n\nGateway doesn't understand the Ingress annotations and vice-versa. This\nobviously caused a massive problem and blew up in my face. I had thought I had\ntested this exact failure case, but clearly prod surfaced a different behavior\nthan I had seen in lower environments. Effectively no traffic was getting to\npods while I tried to figure out what had broken.\n\nI ended up making to manually modify the annotations to get things working and\nhad a pretty embarrassing blow-up in my face after what I had thought was\ncareful testing (but was clearly wrong).\n\n### Fast Forward Two Months\n\nI have learned from my mistake regarding the Gateway API and Ingress and was\nfunctioning totally fine on Gateway API when I decided to attempt to solve the\nLinkerd issue. The issue I was seeing with Linkerd was high-volume services\nwere seeing their proxies consume unlimited memory, steadily growing over time\nbut only while on Gateway API. I was installing Linkerd with their Helm\nlibraries, which have 2 components, the Linkerd CRD chart here:\nhttps://artifacthub.io/packages/helm/linkerd2/linkerd-crds and the Linkerd\ncontrol plane: https://artifacthub.io/packages/helm/linkerd2/linkerd-control-\nplane\n\nSince debug logs and upgrades hadn't gotten me any closer to a solution as to\nwhy the proxies were consuming unlimited memory until they eventually were\nOOMkilled, I decided to start fresh. I removed the Linkerd injection from all\ndeployments and removed the helm charts. Since this was a non-prod\nenvironment, I figured at least this way I could start fresh with debug logs\nand maybe come up with some justification for what was happening.\n\nExcept the second I uninstalled the charts, my graphs started to freak out. I\ncouldn't understand what was happening, how did removing Linkerd break\nsomething? Did I have some policy set to require Linkerd? Why was my traffic\nlevels quickly approaching zero in the non-prod environment?\n\nThen a coworker said \"oh it looks like all the routes are gone from the load\nbalancer\". I honestly hadn't even thought to look there, assuming the problem\nwas some misaligned Linkerd policy where our deployments required encryption\nto communicate or some mistake on my part in the removal of the helm charts.\nBut they were right, the load balancers didn't have any routes. kubectl\nconfirmed, no HTTProutes remained.\n\nSo of course I was left wondering \"what just happened\".\n\n### Gateway API\n\nSo a quick crash course in \"what is gateway API\". At a high level, as\ndiscussed before, it is a new way of defining Ingress which cleans up the\nannotation mess and allows for a clean separation of responsibility in an org.\n\nSo GCP defines the GatewayClass, I make the Gateway and developer provide the\nHTTPRoutes. This means developers can safely change the routes to their own\nservices without the risk that they will blow up the load balancer. It also\nprovides a ton of great customization for how to route traffic to a specific\nservice.\n\nSo first you make a Gateway like so in Helm or whatever:\n\n    \n    \n    --- kind: Gateway apiVersion: gateway.networking.k8s.io/v1beta1 metadata: name: {{ .Values.gateway_name }} namespace: {{ .Values.gateway_namespace }} spec: gatewayClassName: gke-l7-global-external-managed listeners: - name: http protocol: HTTP port: 80 allowedRoutes: kinds: - kind: HTTPRoute namespaces: from: Same - name: https protocol: HTTPS port: 443 allowedRoutes: kinds: - kind: HTTPRoute namespaces: from: All tls: mode: Terminate options: networking.gke.io/pre-shared-certs: \"{{ .Values.pre_shared_cert_name }},{{ .Values.internal_cert_name }}\"\n    \n    1234567891011121314151617181920212223242526272829\n\nThen you provide a different YAML of HTTPRoute for the redirect of http to\nhttps:\n\n    \n    \n    kind: HTTPRoute apiVersion: gateway.networking.k8s.io/v1beta1 metadata: name: redirect namespace: {{ .Values.gateway_namespace }} spec: parentRefs: - namespace: {{ .Values.gateway_namespace }} name: {{ .Values.gateway_name }} sectionName: http rules: - filters: - type: RequestRedirect requestRedirect: scheme: https\n    \n    123456789101112131415\n\nFinally you can set policies.\n\n    \n    \n    --- apiVersion: networking.gke.io/v1 kind: GCPGatewayPolicy metadata: name: tls-ssl-policy namespace: {{ .Values.gateway_namespace }} spec: default: sslPolicy: tls-ssl-policy targetRef: group: gateway.networking.k8s.io kind: Gateway name: {{ .Values.gateway_name }}\n    \n    12345678910111213\n\nThen your developers can configure traffic to their services like so:\n\n    \n    \n    kind: HTTPRoute apiVersion: gateway.networking.k8s.io/v1beta1 metadata: name: store spec: parentRefs: - kind: Gateway name: internal-http hostnames: - \"store.example.com\" rules: - backendRefs: - name: store-v1 port: 8080 - matches: - headers: - name: env value: canary backendRefs: - name: store-v2 port: 8080 - matches: - path: value: /de backendRefs: - name: store-german port: 8080\n    \n    123456789101112131415161718192021222324252627\n\n### Seems Straightforward\n\nRight? There isn't that much to the thing. So after I attempted to re-add the\nHTTPRoutes using Helm and Terraform (which of course didn't detect a diff even\nthough the routes were gone because Helm never seems to do what I want it to\ndo in a crisis) and then ended up bumping the chart version to finally force\nit do the right thing, I started looking around. What the hell had I done to\nbreak this?\n\nWhen I removed linkerd crds it somehow took out my httproutes. So then I went\nto the Helm chart trying to work backwards. Immediately I see this:\n\n    \n    \n    {{- if .Values.enableHttpRoutes }} --- apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: annotations: api-approved.kubernetes.io: https://github.com/kubernetes-sigs/gateway-api/pull/1923 gateway.networking.k8s.io/bundle-version: v0.7.1-dev gateway.networking.k8s.io/channel: experimental {{ include \"partials.annotations.created-by\" . }} labels: helm.sh/chart: {{ .Chart.Name }}-{{ .Chart.Version | replace \"+\" \"_\" }} linkerd.io/control-plane-ns: {{.Release.Namespace}} creationTimestamp: null name: httproutes.gateway.networking.k8s.io spec: group: gateway.networking.k8s.io names: categories: - gateway-api kind: HTTPRoute listKind: HTTPRouteList plural: httproutes singular: httproute scope: Namespaced versions:\n    \n    1234567891011121314151617181920212223242526\n\nSure enough, Linkerd CRD Helm chart has that set to default True:\n\nI also found this issue: https://github.com/linkerd/linkerd2/issues/12232\n\nSo yeah, Linkerd is, for some reason, pulling this CRD from a pull request\nfrom April 6th of last year that is marked as \"do not merge\".\nhttps://github.com/kubernetes-sigs/gateway-api/pull/1923\n\nLinkerd is aware of the possible problem but presumes you'll catch the\nconfiguration option on the Helm chart:\nhttps://github.com/linkerd/linkerd2/issues/11586\n\nTo be clear I'm not \"coming after Linkerd\" here. I just thought the whole\nthing was extremely weird and wanted to make sure, given the amount of usage\nLinkerd gets out there, that other people were made aware of it before running\nthe car into the wall at 100 MPH.\n\n### What are CRDs?\n\nKubernetes Custom Resource Definitions (CRDs) essentially extend the\nKubernetes API to manage custom resources specific to your application or\ndomain.\n\n  * CRD Object: You create a YAML manifest file defining the Custom Resource Definition (CRD). This file specifies the schema, validation rules, and names of your custom resource.\n  * API Endpoint: When you deploy the CRD, the Kubernetes API server creates a new RESTful API endpoint for your custom resource.\n\nEffectively when I enabled Gateway API in GKE with the following I hadn't\nconsidered that I could end up in a CRD conflict state with Linkerd:\n\n    \n    \n    gcloud container clusters create CLUSTER_NAME \\ --gateway-api=standard \\ --cluster-version=VERSION \\ --location=CLUSTER_LOCATION\n    \n    1234\n\nWhat I suspect happened is, since I had Linkerd installed before I had enabled\nthe gateway-api on GKE, when GCP attempted to install the CRD it failed\nsilently. Since I didn't know there was a CRD conflict, I didn't understand\nthat the CRD that the HTTPRoutes relied on was actually the Linkerd maintained\none, not the GCP one. Presumably had I attempted to do this the other way it\nwould have thrown an error when the Helm chart attempted to install a CRD that\nwas already present.\n\nTo be clear before you call me an idiot, I am painfully aware that the\ndeletion of CRDs is a dangerous operation. I understand I should have\ncarefully checked and I am admitting I didn't in large part because it just\nnever occurred to me that something like Linkerd would do this. Think of my\nfailure to check as a warning to you, not an indictment against Kubernetes or\nwhatever.\n\n### Conclusion\n\nIf you are using Linkerd and Helm and intend to use Gateway API, this is your\nwarning right now to go in there and flip that value in the Helm chart to\nfalse. Learn from my mistake.\n\nQuestions/comments/concerns: https://c.im/@matdevdug\n\nShare\n\nTopic kubernetes gcp Work\n\n## AI Is Speaking In Tongues\n\nIn my hometown in Ohio, church membership was a given for middle-class...\n\n23 Apr 2024\n\n", "frontpage": false}
