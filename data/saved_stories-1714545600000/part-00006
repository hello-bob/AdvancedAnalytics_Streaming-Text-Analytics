{"aid": "40214700", "title": "Direct Preference Optimization Explained In-Depth", "url": "https://www.tylerromero.com/posts/2024-04-dpo/", "domain": "tylerromero.com", "votes": 1, "user": "timshel1", "posted_at": "2024-04-30 18:49:18", "comments": 0, "source_title": "Direct Preference Optimization Explained In-depth", "source_text": "Direct Preference Optimization Explained In-depth\n\n# Direct Preference Optimization Explained In-depth\n\nSimpler preference-tuning without reinforcement learning\n\nApril 2024\n\nWith my first blog post, I want to cover an excellent paper that was published\nlast year: Direct Preference Optimization: Your Language Model is Secretly a\nReward Model by Rafailov et al.\n\nCommonly referred to as DPO, this method of preference tuning is an\nalternative to Reinforcement Learning from Human Feedback (RLHF) that avoids\nthe actual reinforcement learning. In this blog post, I will explain DPO from\nfirst principles; readers do not need an understanding of RLHF. However, fair\nwarning that there will be some math involved - mostly probability, algebra,\nand optimization - but I will do my best to explain everything clearly.\n\n## Training, tuning, and aligning LLMs\n\nTo contextualize DPO, and preference-tuning in general, let\u2019s review the\nmodern process for creating language models such as ChatGPT or Claude. The\nfollowing steps are sequential, with each one building upon the previous:\n\n  1. Pre-train a base model on internet-scale data. Given a snippet of text, this model is trained to predict the immediate next word. This conceptually simple task scales up extremely well and allows LLMs to encode a huge amount of knowledge from their training data. Examples of base models include GPT-3, Llama3, and Mistral.\n\n  2. Take a pre-trained base model and fine-tune it on a task-specific dataset of demonstrations. For example, if you are trying to create a helpful dialog model like ChatGPT, you would want to tune your model on a dataset of conversational dialog, so that your model\u2019s outputs sound more like parts of a conversation and less like a Wikipedia page. In this stage, we still use the next word prediction task, and the fine-tuning procedure updates our model to make predictions that more closely align with the high-quality task-specific examples we are feeding it. Examples of fine-tuned models in this stage are Alpaca, Vicuna and Mistral-Instruct.\n\n  3. Finally, we fine-tune the model based on human preferences. Human preferences are powerful because they are so easily and cheaply expressed. Think of how easy it is to compare two movies and pick a favorite. Yet how difficult it would be to make a film that embodies the qualities that drive you to visit a theater. Similarly, it is challenging to describe exactly how we want our model to behave (as we attempt to do in step 2), but given examples of model behavior it is straightforward to indicate a preference for a specific type of behavior. For a while, this sort of preference-tuning was done using RLHF. Recently, RLHF has been somewhat supplanted by DPO due to the relative simplicity of the latter. LLMs that have been tuned using human preferences include Llama 3 Instruct, ChatGPT-4, Claude 3 Opus, and Gemini Ultra.\n\nThe Gemini whitepaper provides a nice visual representation of these stages:\n\n## Tuning LLMs on preference data\n\nIt is hard and time-consuming work to create high-quality demonstrations of\nthe behavior we want our LLM to mimic. And it would be expensive to hire\nlabelers to help us create such data. However, once we have a model that is\n\u201cgood enough\u201d at demonstrating desired behavior, we can shift into high gear.\nGiven a prompt, we can sample two different responses from our LLM by\ninjecting a small amount of randomnessThis is typically done by generating\ntext with a temperature that is greater than zero. Here is a lovely little\ndemo that explains how temperature affects model outputs visually.. Now, it is\ncheap and easy to have a labeler express a preference for one of the two\ncompletions.\n\nWhile using ChatGPT or Gemini, you may have noticed that you will occasionally\nbe asked to choose between two similar answers from which to continue your\nconversation. This preference is recorded and used to improve the model in a\nfuture round of preference-tuning. Similarly, Chatbot Arena collects\npreference data for the purpose of rating LLMs based on human assessments:\n\nThere are many publicly available preference datasets, such as LMSys\u2019 Chatbot\nArena Conversations dataset, OpenAI\u2019s WebGPT Comparisons dataset, and\nAnthropic\u2019s Helpfulness-Harmlessness RLHF dataset (explicit/offensive content\nwarning).\n\nFormally, these datasets can be expressed as follows:\n\nD={x(i),yw(i),yl(i)}i=1N\n\nWhere x is the context/prompt, yw is the preferred completion, and yl is the\nless desirable completion.\n\n### The Bradley-Terry Model\n\nSo what do we do with all this preference data? We want to leverage it to\nmodify our LLM to output responses that better conform to the preferences. To\nbegin, let us explore a simple probability model:\n\np\u2217(i\u227bj)=si+sjsi\n\nThis is the Bradley-Terry model, which is a model for the outcome of pairwise\ncomparisons. In plain English, it says \"We model the trueThis is the reason\nfor the \u201cstar\u201d in p\u2217: to indicate that we are modeling the true underlying\ndistribution of human preferences. Likewise, shortly we will see r\u2217, which\nindicates the true underlying reward function that grades our completions, and\n\u03c0\u2217, which indicates the optimal policy we want our LLM to mimic. probability\nthat outcome i is preferred to outcome j as the score of i over the combined\nscores of i and j\".\n\nReaders may be familiar with the Bradley-Terry model from the context of Elo\nscores, which are popular in chess and other competitive games. The Bradley-\nTerry model is a generalization of the Elo rating system, where the\nprobability of player A beating player B is given by\np(A\u227bB)=1+10(RB\u2212RA)/4001=sA+sBsA. Here R indicates a player\u2019s ratingSo if\nplayer A\u2019s Elo rating is 2000 and player B\u2019s is 1600 then player A is expected\nto be 10 times more likely to win than player B, because\np(A\u227bB)=1+10(1600\u22122000)/4001=10/11. and s=10R/400.\n\nUnder the Bradley-Terry model, is common to choose to parameterize the score\nas s=er, where r stands for reward. The term \u201creward\u201d is borrowed from the\nworld of reinforcement learning, where greater rewards are received for a more\ndesirable series of actions - similar to achieving a higher score for\nperforming better in a video game.\n\nWith this parameterization, our model starts to look pretty nice - a simple\ndifference in reward values passed through the logistic functionThe logistic\nfunction is an S-shaped (or sigmoid) function commonly denoted using \u03c3(x). It\nfrequently appears when working with probabilities because it can \u201csquash\u201d\nvalues in R (the set of all real numbers) into (0,1) (the set of probabilities\nvalues, excluding exactly 0 or 1). .\n\np\u2217(i\u227bj)=si+sjsi=eri\u2217+erj\u2217eri\u2217=1+e\u2212(ri\u2217\u2212rj\u2217)1=\u03c3(ri\u2217\u2212rj\u2217)\n\n### Applying the Bradley-Terry Model to LLMs\n\nNow, we want to take the Bradley-Terry model and leverage it alongside a\ndataset of preferences in order to improve our LLM\u2019s generated outputs.\n\nIn our preference dataset (D), we have two comparisons and we want to model\nthe probability of one completion being preferred over the other. In a sense,\neach completion elicits some reward based on its quality, and our ultimate\ngoal will be to nudge our LLM to produce completions that are of higher\nquality. Therefore, we will parameterize the reward using our LLM. We will\ncall this reward r\u2217(x,y), which just means that the reward is a function of\nthe context/prompt (x) and the completion (y).\n\nSo after adapting our preference model to use our parameterized reward\nfunction, we have:\n\np\u2217(y1\u227by2\u2223x)=\u03c3(r\u2217(x,y1)\u2212r\u2217(x,y2))\n\nBut talking in terms of optimal solutions and rewards does us no good, since\nwe do not have access to the optimal reward function. In practice, it is\ncommon to learn a reward model r\u03c6(x,y) that mimics the optimal reward\nfunction. We can estimate the parameters \u03c6 of this reward model by framing\nthis as a binary classification problem where our objective is to minimize the\nfollowing negative log-likelihood loss function on our preference dataset\nD:E(x,y1,y2)\u223cD[f(x,yw,yl)] is just a formal way of saying \"the expected value\nof function f on data points sampled from our preference dataset\".\n\nLR(r\u03c6,D)=\u2212E(x,yw,yl)\u223cD[log(\u03c3(r\u03c6(x,yw)\u2212r\u03c6(x,yl)))]\n\nUnder the RLHF framework, we could leverage this learned reward model in a\nreinforcement learning setting to optimize an LLM to output completions that\nachieve high rewards. However, DPO takes a different tack - instead of the\ntwo-stage RLHF process, DPO reparameterizes the Bradley-Terry model so that we\ncan use a similar loss function to directly optimize the parameters of our LLM\nsuch that it produces outputs that are preferred by human observers.\n\n### The probability of a completion\n\nAt this point, the idea of optimizing LLMs based on preferences or rewards may\nfeel fairly abstract. So we\u2019re going to take a moment to introduce a new\nprobability function, \u03c0(y\u2223x), that represents the literal output of our LLM.\nIn reinforcement learning notation, \u03c0 indicates a policy (i.e. a strategy),\nand policies are optimized to maximize reward. Specifically, \u03c0\u03b8(y\u2223x) is the\nprobability of generating the completion y based on an LLM with parameters \u03b8\ngiven that we start with prompt x.\n\nWhat do we mean by \"the probability of generating the completion y\"? Our LLM\nis an auto-regressive text generator, and, upon each auto-regressive step, it\ncomputes a probability value for every wordIn practice, modern LLMs operate on\ntokens, not words. For our purposes, the difference doesn\u2019t really matter. You\ncan learn more by playing with an online tokenizer demo or digging through\nKarparthy\u2019s minbpe repo. in its vocabulary.\n\nSo - proceeding in order through every word in completion y - we compute the\nprobability of the next word in the completion given all of the proceeding\nwords. Now, we have a probability value for every word in the completion! So\nwe can compute the joint probability of generating the sequence of words as\nthe product of the individual probabilities of observing each word along the\nwayMultiplying probabilities can result in numerical underflow. It is common\nto instead work with logprobs: \u220fipi=e\u2211ilogpi. Since every term in the\nsummation of logprobs increases the magnitude of its output, underflow is\navoided. OpenAI has a nice guide to using token logprobs returned by an LLM.:\n\n\u03c0\u03b8(y\u2223x)=t=0\u220f\u2223y\u2223pLLM\u03b8(yt\u2223x,y0:t)\n\nAnother way to think about it is that there is a tree of possible completions\nand we are computing the probability of tracing one specific path from the\nroot (end of the prompt) to a leaf (stop-token).\n\nWhen training, we know the entire text completion ahead of time, so, by\napplying a causal attention mask, we can calculate all of the the individual\nnext-word probabilities (and thus \u03c0\u03b8(y\u2223x)) via a single forward pass through\nour LLM.\n\n## Optimizing our LLM based on preferences\n\nOk, so now that we\u2019ve got our framework in place. Let us remind ourselves of\nour goal: to improve the outputs of our LLM. Stated another way, we want the\ncompletion (y) our LLM provides for a prompt (x) to generate a large reward\nr(x,y). With this in mind, we can formulate an optimization problem where we\nwant to find the parameters of our LLM (\u03b8) that maximize our expected reward\nfor prompts similar to those we see in practice.Ex\u223cD,y\u223c\u03c0\u03b8(y\u2223x)[r(x,y)] is just\na formal way of saying \"the expected reward attained by completions\ngenerated/sampled from our model (y\u223c\u03c0\u03b8(y\u2223x)) based on prompts sampled from our\ndataset (x\u223cD)\".\n\n\u03b8maxEx\u223cD,y\u223c\u03c0\u03b8(y\u2223x)[r(x,y)]\n\nThis is a bit too simplistic, however. In practice, we start with the\nparameters of our fine-tuned base model, and we have some belief that the\noutputs generated by our fine-tuned base model are pretty good, so we don\u2019t\nwant the outputs of our model to change too much unless they improve the\nreward significantly. With that in mind, we amend our optimization problem to\ninclude a regularization constraint to help enforce this belief.\n\n\u03b8maxEx\u223cD,y\u223c\u03c0\u03b8(y\u2223x)[r(x,y)]\u2212\u03b2DKL[\u03c0\u03b8(y\u2223x) \u2225 \u03c0ref(y\u2223x)]\n\nDKL[P\u2225Q] is the Kullback-Leibler divergenceKL divergence is one of many\ntraditional methods for regularizing an RL agent\u2019s policy. In the cases of DPO\nand RLHF, it is a natural choice because we begin with a strong reference\npolicy at hand - the LLM output by our fine-tuning procedure., a statistical\ndistance measure. It quantifies how the probability distribution P differs\nfrom probability distribution Q. This constraint based on the KL divergence\njust encodes the idea that we want to penalize outputs from our model (\u03c0\u03b8)\nbased on how much they differ from outputs from the fine-tuned model (e.g. the\nreference model) we started with (\u03c0ref). \u03b2 is a scalar hyperparameter that\ncontrols the strength of the constraint.\n\nNow, we want to derive the optimal solution to this optimization problem. This\nwill rely on Gibb\u2019s Inequality - the fact that DKL[P\u2225Q]\u22650 and DKL[P\u2225Q]=0 if\nand only if P=Q.The intuition here is that the KL-divergence is a distance\nmeasure (kind of), and there is no distance between P and Q if they are equal,\nand there must be some distance if they are not equal.\n\n\u03c0\u03b8maxEx\u223cD,y\u223c\u03c0\u03b8(y\u2223x)[r(x,y)]\u2212\u03b2DKL[\u03c0\u03b8(y\u2223x) \u2225\n\u03c0ref(y\u2223x)]=\u03c0\u03b8maxEx\u223cD,y\u223c\u03c0\u03b8(y\u2223x)[r(x,y)]\u2212\u03b2Ey\u223c\u03c0\u03b8(y\u2223x)[log\u03c0ref(y\u2223x)\u03c0\u03b8(y\u2223x)]=\u03c0\u03b8maxEx\u223cDEy\u223c\u03c0\u03b8(y\u2223x)[r(x,y)\u2212\u03b2log\u03c0ref(y\u2223x)\u03c0\u03b8(y\u2223x)]=\u03c0\u03b8minEx\u223cDEy\u223c\u03c0\u03b8(y\u2223x)[log\u03c0ref(y\u2223x)\u03c0\u03b8(y\u2223x)\u2212\u03b21r(x,y)]=\u03c0\u03b8minEx\u223cDEy\u223c\u03c0\u03b8(y\u2223x)\u23a3\u23a1logZ(x)1\u03c0ref(y\u2223x)e\u03b21r(x,y)\u03c0\u03b8(y\u2223x)\u2212logZ(x)\u23a6\u23a4=...\n\nwhere Z(x)=\u2211y\u03c0ref(y\u2223x)e\u03b21r(x,y). Importantly, this Z(x) term depends only on x\nand \u03c0ref and not on y or \u03c0\u03b8. This lets us do a bit of reorganizing from where\nwe just left off.\n\n...=\u03c0\u03b8minEx\u223cD\u23a3\u23a1Ey\u223c\u03c0\u03b8(y\u2223x)\u23a3\u23a1logZ(x)1\u03c0ref(y\u2223x)e\u03b21r(x,y)\u03c0\u03b8(y\u2223x)\u23a6\u23a4\u2212logZ(x)\u23a6\u23a4=\u03c0\u03b8minEx\u223cD[DKL(\u03c0\u03b8(y\u2223x)\n\u2225 Z(x)1\u03c0ref(y\u2223x)e\u03b21r(x,y))\u2212logZ(x)]\n\nAnd we have nearly arrived! Since Z(x) does not depend on \u03c0\u03b8, we can just\nignore it when deriving the optimal solution. We can now use Gibb\u2019s inequality\nas mentioned above: DKL(\u03c0\u03b8(y\u2223x) \u2225 Z(x)1\u03c0ref(y\u2223x)e\u03b21r(x,y)) is minimized at\nzero if, and only if, the two distributions on either side of \u2225 are identical.\nSo, the optimal solution (denoted as \u03c0\u2217) to our optimization problem for all\nx\u2208D is:\n\n\u03c0\u2217(y\u2223x)=\u03c0\u03b8(y\u2223x)=Z(x)1\u03c0ref(y\u2223x)e\u03b21r(x,y)\n\n### Direct Preference Optimization\n\nSo we know the optimal solution to our optimization problem, but can we access\nit? No. The term Z(x)=\u2211y\u03c0ref(y\u2223x)e\u03b21r(x,y) is intractable - computing it\nrequires summing over every possible string of words.\n\nInstead, we can reorganize the optimal solution from above such that we\nexpress the reward function in terms of the optimal policy \u03c0\u03b8, the reference\npolicy \u03c0ref, and the intractable function Z:\n\nr(x,y)=\u03b2log\u03c0ref(y\u2223x)\u03c0\u03b8(y\u2223x)+\u03b2logZ(x)\n\nThis same reorganization can be applied using the underlying ground-truth\nreward r\u2217 and its corresponding optimal policy \u03c0\u2217.\n\nr\u2217(x,y)=\u03b2log\u03c0ref(y\u2223x)\u03c0\u2217(y\u2223x)+\u03b2logZ(x)\n\nNow here comes the clever trick noticed by the authors of DPO. We can use this\nreorganized expression of the optimal solution to our optimization problem to\nreparameterize the Bradley-Terry preference model from above so that it is\nexpressed in terms of an optimal policy \u03c0\u2217 and not in terms of an underlying\nreward function! And even better, once we plug everything in, we notice that\nthe intractable Z(x) function cancels out!\n\np\u2217(y1\u227by2\u2223x)=\u03c3(r\u2217(x,y1)\u2212r\u2217(x,y2))=\u03c3(\u03b2log\u03c0ref(y1\u2223x)\u03c0\u2217(y1\u2223x)+\u03b2logZ(x)\u2212(\u03b2log\u03c0ref(y2\u2223x)\u03c0\u2217(y2\u2223x)+\u03b2logZ(x)))=\u03c3(\u03b2log\u03c0ref(y1\u2223x)\u03c0\u2217(y1\u2223x)\u2212\u03b2log\u03c0ref(y2\u2223x)\u03c0\u2217(y2\u2223x))\n\nNow, with our reparameterized Bradley-Terry model, we can use supervised\nlearning to directly learn a policy that mimics the optimal policy. We can\nminimize a negative log-likelihood loss function over our preference dataset D\nto estimate the parameters of our policy \u03c0\u03b8:\n\nLDPO(\u03c0\u03b8;\u03c0ref)=\u2212E(yw,yl,x)\u223cD[log(\u03c3(\u03b2log\u03c0ref(yw\u2223x)\u03c0\u03b8(yw\u2223x)\u2212\u03b2log\u03c0ref(yl\u2223x)\u03c0\u03b8(yl\u2223x)))]=\u2212E(yw,yl,x)\u223cD[log(\u03c3(\u03b2(log\u03c0\u03b8(yl\u2223x)\u03c0\u03b8(yw\u2223x)\u2212log\u03c0ref(yl\u2223x)\u03c0ref(yw\u2223x))))]\n\nRecall that above we optimized a negative log-likelihood loss to estimate the\nparameters of a reward model that was then used downstream by RLHF to estimate\nthe parameters of a policy model. But now we are directly optimizing the\nparameters of our LLM policy model based on human preferences! Thus, Direct\nPreference Optimization.\n\nTo be explicit about the benefits of DPO over RLHF:\n\n  1. We avoid the need to train a reward model to estimate human preferences.\n  2. We avoid needing to perform any type of reinforcement learning, which is notoriously difficult and requires a lot of tribal knowledge to get right.\n  3. We can directly optimize our LLM on human preferences using supervised learning, which is a much more straightforward and well-understood process.\n\nThe avoidance of reinforcement learning is particularly important. DPO has\nmade preference-tuning a much more accessible process for practitioners who\nmay not have the time, resources, or expertise to navigate the complexities of\nreinforcement learning.\n\n### Properties and Caveats of DPO\n\nOne of the key properties of DPO is that when the Bradley-Terry model\nperfectly fits our preference data and RLHF learns the optimal reward\nfunction, then the global optimizer of RHLF and DPO is the same.\n\nThis is an important equivalence result; however, in practice:\n\n  1. The Bradley-Terry model often does not perfectly fit the preference data.For example, a preference cycle would cause the Bradley-Terry model to fail to perfectly fit the data. The Bradley-Terry model assumes transitive preferences. For example, if A\u227bB and B\u227bC then it expects that A\u227bC. But if instead C\u227bA, then there is a cycle and transitivity is broken.\n  2. The reward function learned by RLHF will not be the optimal reward function.\n  3. Gradient descent on a highly non-convex loss landscape - such as that of an LLM - does not find the global optimizer.\n\nAnother weakness of DPO is that it is prone to overfitting due to a lack of\nregularization. Azar et al. provide a compelling exampleThe original notation\nof the quote has been adjusted slightly to match the rest of this post.:\n\n> Consider the simple example where we have two actions y1 and y2 such that\n> p\u2217(y1\u227by2)=1, i.e., y1 is always preferred to y2. Then the Bradley-Terry\n> model would require that (r(y1)\u2212r(y2))\u2192+\u221e to [be satisfied]. If we plug this\n> into the optimal policy then we would get that \u03c0\u2217(y1)\u03c0\u2217(y2)=0 (i.e.\n> \u03c0\u2217(y2)=0) ... Thus the strength of the KL-regularization becomes weaker and\n> weaker the more deterministic the preferences.\n\nThey also point out that, in practice, we have a finite amount of preference\ndata. Therefore, we are likely to empirically estimate p^(y1\u227by2)=1 simply\nbecause we\u2019ve only seen a small number of comparisons between y and y\u2032.\nTherefore the empirical optimal policy would push \u03c0(y2)=0 regardless of the\nregularization term that is attempting to keep the policy similar to our\nreference policy.\n\nDespite these shortcomings, DPO is a highly effective tool; at the time of\nwriting, many of the most successful and performant open-source LLMs were\ninstruction-tuned using DPO.\n\n## Interested in learning more?\n\nI highly recommend reading the DPO paper. In this post, we\u2019ve done a deep dive\ninto the derivation of the DPO objective, but the paper covers other points of\ninterest, such as experimental results and additional theoretical properties.\n\nAnd if you\u2019re interested in learning more about preference-tuning in general,\nhere are additional resources that provide a deeper dive into the topic:\n\n  * OpenAI\u2019s post on aligning language models to follow human instructions (and the InstructGPT paper)\n  * HuggingFace\u2019s post on fine-tuning Llama2 with DPO\n  * Direct Nash Optimization, a recently proposed approach, avoids using the Bradley-Terry model altogether since the Bradley-Terry model fails to express complex intransitive or cyclic preference relations.\n\n## References\n\n[1] Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., & Finn,\nC. (2023). Direct Preference Optimization: Your Language Model is Secretly a\nReward Model. arXiv. https://arxiv.org/abs/2305.18290.\n\n[2] Bertrand, Q., Czarnecki, W. M., & Gidel, G. (2023). On the limitations of\nElo: Real-world games are transitive, not additive. arXiv.\nhttps://arxiv.org/abs/2206.12301.\n\n[3] Azar, M. G., Rowland, M., Piot, B., Guo, D., Calandriello, D., Valko, M.,\n& Munos, R. (2023). A General Theoretical Paradigm to Understand Learning from\nHuman Preferences. arXiv. https://arxiv.org/abs/2310.12036.\n\n[4] Jitkrittum, W. (2013). Log-Sum-Exp Trick to Prevent Numerical Underflow.\nhttp://wittawat.com/posts/log-sum_exp_underflow.html\n\n[5] Gemini Team (2024). Gemini: A Family of Highly Capable Multimodal Models.\narXiv. https://arxiv.org/abs/2312.11805.\n\n[6] Andrychowicz, M., Raichuk, A., Sta\u0144czyk, P., Orsini, M., Girgin, S.,\nMarinier, R., Hussenot, L., Geist, M., Pietquin, O., Michalski, M., Gelly, S.,\n& Bachem, O. (2020). What Matters In On-Policy Reinforcement Learning? A\nLarge-Scale Empirical Study. arXiv. https://arxiv.org/abs/2006.05990.\n\n\u00a9 2024 Tyler Romero\n\n", "frontpage": false}
