{"aid": "40167177", "title": "Computer scientist makes philosophical concepts such as meaning testable for AI", "url": "https://www.quantamagazine.org/does-ai-know-what-an-apple-is-she-aims-to-find-out-20240425/", "domain": "quantamagazine.org", "votes": 1, "user": "isaacfrond", "posted_at": "2024-04-26 08:46:07", "comments": 0, "source_title": "Does AI Know What an Apple Is? She Aims to Find Out. | Quanta Magazine", "source_text": "Does AI Know What an Apple Is? She Aims to Find Out. | Quanta Magazine\n\nWe care about your data, and we'd like to use cookies to give you a smooth\nbrowsing experience. Please agree and read more about our privacy policy.\n\nDoes AI Know What an Apple Is? She Aims to Find Out.\n\nRead Later\n\n###### Share\n\nCopied!\n\n  * Comments\n\n  * Read Later\n\nQ&A\n\n# Does AI Know What an Apple Is? She Aims to Find Out.\n\nBy John Pavlus\n\nApril 25, 2024\n\nThe computer scientist Ellie Pavlick is translating philosophical concepts\nsuch as \u201cmeaning\u201d into concrete, testable ideas.\n\nRead Later\n\nEllie Pavlick stands in Brown University\u2019s Computer History Museum. Her work\non how large language models understand concepts often merges philosophy with\nscience.\n\nAdam Wasilewski for Quanta Magazine\n\nBy John Pavlus\n\nContributing Writer\n\nApril 25, 2024\n\nView PDF/Print Mode\n\nartificial intelligencecomputer sciencemachine learningnatural language\nprocessingneural networksQ&AAll topics\n\n## Introduction\n\nStart talking to Ellie Pavlick about her work \u2014 looking for evidence of\nunderstanding within large language models (LLMs) \u2014 and she might sound as if\nshe\u2019s poking fun at it. The phrase \u201chand-wavy\u201d is a favorite, and if she\nmentions \u201cmeaning\u201d or \u201creasoning,\u201d it\u2019ll often come with conspicuous air\nquotes. This is just Pavlick\u2019s way of keeping herself honest. As a computer\nscientist studying language models at Brown University and Google DeepMind,\nshe knows that embracing natural language\u2019s inherent mushiness is the only way\nto take it seriously. \u201cThis is a scientific discipline \u2014 and it\u2019s a little\nsquishy,\u201d she said.\n\nPrecision and nuance have coexisted in Pavlick\u2019s world since adolescence, when\nshe enjoyed math and science \u201cbut always identified as more of a creative\ntype.\u201d As an undergraduate, she earned degrees in economics and saxophone\nperformance before pursuing a doctorate in computer science, a field where she\nstill feels like an outsider. \u201cThere are a lot of people who [think]\nintelligent systems will look a lot like computer code: neat and conveniently\nlike a lot of systems [we\u2019re] good at understanding,\u201d she said. \u201cI just\nbelieve the answers are complicated. If I have a solution that\u2019s simple, I\u2019m\npretty sure it\u2019s wrong. And I don\u2019t want to be wrong.\u201d\n\nA chance encounter with a computer scientist who happened to work in natural\nlanguage processing led Pavlick to embark on her doctoral work studying how\ncomputers could encode semantics, or meaning in language. \u201cI think it\nscratched a certain itch,\u201d she said. \u201cIt dips into philosophy, and that fits\nwith a lot of the things I\u2019m currently working on.\u201d Now, one of Pavlick\u2019s\nprimary areas of research focuses on \u201cgrounding\u201d \u2014 the question of whether the\nmeaning of words depends on things that exist independently of language\nitself, such as sensory perceptions, social interactions, or even other\nthoughts. Language models are trained entirely on text, so they provide a\nfruitful platform for exploring how grounding matters to meaning. But the\nquestion itself has preoccupied linguists and other thinkers for decades.\n\n\u201cThese are not only \u2018technical\u2019 problems,\u201d Pavlick said. \u201cLanguage is so huge\nthat, to me, it feels like it encompasses everything.\u201d\n\nQuanta spoke with Pavlick about making science out of philosophy, what\n\u201cmeaning\u201d means, and the importance of unsexy results. The interview has been\ncondensed and edited for clarity.\n\nShare this article\n\nCopied!\n\nNewsletter\n\nGet Quanta Magazine delivered to your inbox\n\nRecent newsletters\n\nWhile some researchers are dismissive of language models displaying\nintelligent behavior, Pavlick thinks there\u2019s value in figuring that out.\n\u201cWe\u2019ve hit on something quite exciting and quite new, and it\u2019s worth\nunderstanding it deeply.\u201d\n\nAdam Wasilewski for Quanta Magazine\n\n## Introduction\n\n### What does \u201cunderstanding\u201d or \u201cmeaning\u201d mean, empirically? What,\nspecifically, do you look for?\n\nWhen I was starting my research program at Brown, we decided that meaning\ninvolves concepts in some way. I realize this is a theoretical commitment that\nnot everyone makes, but it seems intuitive. If you use the word \u201capple\u201d to\nmean apple, you need the concept of an apple. That has to be a thing, whether\nor not you use the word to refer to it. That\u2019s what it means to \u201chave\nmeaning\u201d: there needs to be the concept, something you\u2019re verbalizing.\n\nI want to find concepts in the model. I want something that I can grab within\nthe neural network, evidence that there is a thing that represents \u201capple\u201d\ninternally, that allows it to be consistently referred to by the same word.\nBecause there does seem to be this internal structure that\u2019s not random and\narbitrary. You can find these little nuggets of well-defined function that\nreliably do something.\n\nI\u2019ve been focusing on characterizing this internal structure. What form does\nit have? It can be some subset of the weights within the neural network, or\nsome kind of linear algebraic operation over those weights, some kind of\ngeometric abstraction. But it has to play a causal role [in the model\u2019s\nbehavior]: It\u2019s connected to these inputs but not those, and these outputs and\nnot those.\n\nThat feels like something you could start to call \u201cmeaning.\u201d It\u2019s about\nfiguring out how to find this structure and establish relationships, so that\nonce we get it all in place, then we can apply it to questions like \u201cDoes it\nknow what \u2018apple\u2019 means?\u201d\n\n### Have you found any examples of this structure?\n\nYes, one result involves when a language model retrieves a piece of\ninformation. If you ask the model, \u201cWhat is the capital of France,\u201d it needs\nto say \u201cParis,\u201d and \u201cWhat is the capital of Poland\u201d should return \u201cWarsaw.\u201d It\nvery readily could just memorize all these answers, and they could be\nscattered all around [within the model] \u2014 there\u2019s no real reason it needs to\nhave a connection between those things.\n\nInstead, we found a small place in the model where it basically boils that\nconnection down into one little vector. If you add it to \u201cWhat is the capital\nof France,\u201d it will retrieve \u201cParis\u201d; and that same vector, if you ask \u201cWhat\nis the capital of Poland,\u201d will retrieve \u201cWarsaw.\u201d It\u2019s like this systematic\n\u201cretrieve-capital-city\u201d vector.\n\nThat\u2019s a really exciting finding because it seems like [the model is] boiling\ndown these little concepts and then applying general algorithms over them. And\neven though we\u2019re looking at these really [simple] questions, it\u2019s about\nfinding evidence of these raw ingredients that the model is using. In this\ncase, it would be easier to get away with memorizing \u2014 in many ways, that\u2019s\nwhat these networks are designed to do. Instead, it breaks [information] down\ninto pieces and \u201creasons\u201d about it. And we hope that as we come up with better\nexperimental designs, we might find something similar for more complicated\nkinds of concepts.\n\nBefore her doctorate in computer science, Pavlick double majored in economics\nand saxophone performance.\n\nAdam Wasilewski for Quanta Magazine\n\n## Introduction\n\n### How does grounding relate to these representations?\n\nThe way humans learn language is grounded in a ton of nonlinguistic input:\nyour bodily sensations, your emotions, whether you\u2019re hungry, whatever. That\u2019s\nconsidered to be really important to meaning.\n\nBut there are other notions of grounding which have more to do with internal\nrepresentations. There are words that aren\u2019t obviously connected to the\nphysical world, yet they still have meaning. A word like \u201cdemocracy\u201d is a\nfavorite example. It\u2019s a thing in your head: I can think about democracy\nwithout talking about it. So the grounding could be from language to that\nthing, that internal representation.\n\n### But you argue that even things that are more external, like color, might\nstill be anchored to internal \u201cconceptual\u201d representations, without relying on\nperceptions. How would that work?\n\nWell, a language model doesn\u2019t have eyes, right? It doesn\u2019t \u201cknow\u201d anything\nabout colors. So maybe [it captures] something more general, like\nunderstanding the relationships between them. I know that when I combine blue\nand red, I get purple; those kinds of relations could define this internal\n[grounding] structure.\n\nWe can give examples of color to an LLM using RGB codes [strings of numbers\nthat represent colors]. If you say \u201cOK, here\u2019s red,\u201d and give it the RGB code\nfor red, and \u201cHere\u2019s blue,\u201d with the RGB code for blue, and then say \u201cTell me\nwhat purple is,\u201d it should generate the RGB code for purple. This mapping\nshould be a good indication that the internal structure the model has is sound\n\u2014 it\u2019s missing the percepts [for color], but the conceptual structure\u2019s there.\n\nWhat\u2019s tricky is that [the model] could just memorize RGB codes, which are all\nover its training data. So we \u201crotated\u201d all the colors [away from their real\nRGB values]: We\u2019d tell the LLM that the word \u201cyellow\u201d was associated with the\nRGB code for green, and so on. The model performed well: When you asked for\ngreen, it would give you the rotated version of the RGB code. That suggests\nthat there is some kind of consistency to its internal representations for\ncolor. It\u2019s applying knowledge of their relations, not just memorizing.\n\nThat\u2019s the whole point of grounding. Mapping a name onto a color is arbitrary.\nIt\u2019s more about the relationships between them. So that was exciting.\n\nPavlick\u2019s work often deals with the question of \u201cgrounding,\u201d how a word\u2019s\nmeaning can depend on concepts from outside language.\n\nAdam Wasilewski for Quanta Magazine\n\n## Introduction\n\n### How can these philosophical-sounding questions be scientific?\n\nI recently learned of a thought experiment: What if the ocean swept up onto\nthe sand and [when it] pulled back, the patterns generated a poem? Does the\npoem have meaning? That seems super abstract, and you can have this long\nphilosophical debate.\n\nThe nice thing about language models is we don\u2019t need a thought experiment.\nIt\u2019s not like, \u201cIn theory, would such and such a thing be intelligent?\u201d It\u2019s\njust: Is this thing intelligent? It becomes scientific and empirical.\n\nSometimes people are dismissive; there\u2019s the \u201cstochastic parrots\u201d approach. I\nthink it [comes from] a fear that people are going to oversubscribe\nintelligence to these things \u2014 which we do see. And to correct for that,\npeople are like, \u201cNo, it\u2019s all a sham. This is smoke and mirrors.\u201d\n\nIt\u2019s a bit of a disservice. We\u2019ve hit on something quite exciting and quite\nnew, and it\u2019s worth understanding it deeply. That\u2019s a huge opportunity that\nshouldn\u2019t get skirted over because we\u2019re worried about over-interpreting the\nmodels.\n\n### Of course, you\u2019ve also produced research debunking exactly that kind of\nover-interpretation.\n\nThat work, where people were finding all the \u201cshallow heuristics\u201d that models\nwere exploiting [to mimic understanding] \u2014 those were very foundational to my\ncoming-of-age as a scientist. But it\u2019s complicated. It\u2019s like, don\u2019t declare\nvictory too soon. There\u2019s a bit of skepticism or paranoia [in me] that an\nevaluation was done right, even one that I know I designed very carefully!\n\nSo that\u2019s part of it: not over-claiming. Another part is that, if you deal\nwith these [language model] systems, you know that they\u2019re not human-level \u2014\nthe way that they\u2019re solving things is not as intelligent as it seems.\n\nPavlick meets with students at Brown University.\n\nAdam Wasilewski for Quanta Magazine\n\n## Introduction\n\n### When so many of the basic methods and terms are up for debate in this\nfield, how do you even measure success?\n\nWhat I think we\u2019re looking for, as scientists, is a precise, human-\nunderstandable description of what we care about \u2014 intelligence, in this case.\nAnd then we attach words to help us get there. We need some kind of working\nvocabulary.\n\nBut that\u2019s hard, because then you can get into this battle of semantics. When\npeople say \u201cDoes it have meaning: yes or no?\u201d I don\u2019t know. We\u2019re routing the\nconversation to the wrong thing.\n\nWhat I\u2019m trying to offer is a precise account of the behaviors we cared about\nexplaining. And it is kind of moot at that point whether you want to call it\n\u201cmeaning\u201d or \u201crepresentation\u201d or any of these loaded words. The point is,\nthere\u2019s a theory or a proposed model on the table \u2014 let\u2019s evaluate that.\n\n\u201cI want to find concepts in the model,\u201d Pavlick said. \u201cI want something that I\ncan grab within the neural network, evidence that there is a thing that\nrepresents \u2018apple\u2019 internally.\u201d\n\nAdam Wasilewski for Quanta Magazine\n\n## Introduction\n\n### So how can research on language models move toward that more direct\napproach?\n\nThe kinds of deep questions I would really like to be able to answer \u2014 What\nare the building blocks of intelligence? What does human intelligence look\nlike? What does model intelligence look like? \u2014 are really important. But I\nthink the stuff that needs to happen for the next 10 years is not very sexy.\n\n## Related:\n\n  1. ### New Theory Suggests Chatbots Can Understand Text\n\n  2. ### The Unpredictable Abilities Emerging From Large AI Models\n\n  3. ### The Computing Pioneer Helping AI See\n\n  4. ### What Does It Mean for AI to Understand?\n\nIf we want to deal with these [internal] representations, we need methods for\nfinding them \u2014 methods that are scientifically sound. If it\u2019s done in the\nright way, this low-level, super in-the-weeds methodological stuff won\u2019t\ndeliver headlines. But that\u2019s the really important stuff that will allow us to\nanswer these deep questions correctly.\n\nMeanwhile, the models are going to keep changing. So there\u2019s going to be a lot\nof stuff that people will keep publishing as though it\u2019s \u201cthe breakthrough,\u201d\nbut it\u2019s probably not. In my mind, it feels too soon to get big breakthroughs.\n\nPeople are studying these really simple tasks, like asking [a language model\nto complete] \u201cJohn gave a drink to _______,\u201d and trying to see whether it says\n\u201cJohn\u201d or \u201cMary.\u201d That doesn\u2019t have the feeling of a result that explains\nintelligence. But I do actually believe that the tools we\u2019re using to describe\nthis boring-ass problem are essential for answering the deep questions about\nintelligence.\n\nBy John Pavlus\n\nContributing Writer\n\nApril 25, 2024\n\nView PDF/Print Mode\n\nartificial intelligencecomputer sciencemachine learningnatural language\nprocessingneural networksQ&AAll topics\n\nShare this article\n\nCopied!\n\nNewsletter\n\nGet Quanta Magazine delivered to your inbox\n\nRecent newsletters\n\nThe Quanta Newsletter\n\nGet highlights of the most important news delivered to your email inbox\n\nRecent newsletters\n\n## Also in Computer Science\n\ncomputational complexity\n\n### Cryptography Tricks Make a Hard Problem a Little Easier\n\nBy Ben Brubaker\n\nApril 18, 2024\n\nRead Later\n\nartificial intelligence\n\n### How Do Machines \u2018Grok\u2019 Data?\n\nBy Anil Ananthaswamy\n\nApril 12, 2024\n\nRead Later\n\nTuring Award\n\n### Avi Wigderson, Complexity Theory Pioneer, Wins Turing Award\n\nBy Stephen Ornes\n\nApril 10, 2024\n\nRead Later\n\n## Comment on this article\n\nQuanta Magazine moderates comments to facilitate an informed, substantive,\ncivil conversation. Abusive, profane, self-promotional, misleading, incoherent\nor off-topic comments will be rejected. Moderators are staffed during regular\nbusiness hours (New York time) and can only accept comments written in\nEnglish.\n\n## Next article\n\nWhat Does Milk Do for Babies?\n\nAll Rights Reserved \u00a9 2024\n\nAn editorially independent publication supported by the Simons Foundation.\n\nLog in to Quanta\n\n## Use your social network\n\nor\n\nDon't have an account yet? Sign up\n\nSign Up\n\nCreating an account means you accept Quanta Magazine's Terms & Conditions and\nPrivacy Policy\n\nForgot your password?\n\nWe\u2019ll email you instructions to reset your password\n\nChange your password\n\nEnter your new password\n\n", "frontpage": false}
