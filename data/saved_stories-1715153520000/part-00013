{"aid": "40291318", "title": "Generalization in Diffusion Models", "url": "https://arxiv.org/abs/2310.02557", "domain": "arxiv.org", "votes": 1, "user": "blegr", "posted_at": "2024-05-07 20:51:35", "comments": 0, "source_title": "Generalization in diffusion models arises from geometry-adaptive harmonic representations", "source_text": "[2310.02557] Generalization in diffusion models arises from geometry-adaptive\nharmonic representations\n\nSkip to main content\n\nWe gratefully acknowledge support from the Simons Foundation, Massachusetts\nInstitute of Technology, and all contributors. Donate\n\n> cs > arXiv:2310.02557\n\n# Computer Science > Computer Vision and Pattern Recognition\n\narXiv:2310.02557 (cs)\n\n[Submitted on 4 Oct 2023 (v1), last revised 12 Apr 2024 (this version, v3)]\n\n# Title:Generalization in diffusion models arises from geometry-adaptive\nharmonic representations\n\nAuthors:Zahra Kadkhodaie, Florentin Guth, Eero P. Simoncelli, St\u00e9phane Mallat\n\nView a PDF of the paper titled Generalization in diffusion models arises from\ngeometry-adaptive harmonic representations, by Zahra Kadkhodaie and 3 other\nauthors\n\nView PDF HTML (experimental)\n\n> Abstract:Deep neural networks (DNNs) trained for image denoising are able to\n> generate high-quality samples with score-based reverse diffusion algorithms.\n> These impressive capabilities seem to imply an escape from the curse of\n> dimensionality, but recent reports of memorization of the training set raise\n> the question of whether these networks are learning the \"true\" continuous\n> density of the data. Here, we show that two DNNs trained on non-overlapping\n> subsets of a dataset learn nearly the same score function, and thus the same\n> density, when the number of training images is large enough. In this regime\n> of strong generalization, diffusion-generated images are distinct from the\n> training set, and are of high visual quality, suggesting that the inductive\n> biases of the DNNs are well-aligned with the data density. We analyze the\n> learned denoising functions and show that the inductive biases give rise to\n> a shrinkage operation in a basis adapted to the underlying image.\n> Examination of these bases reveals oscillating harmonic structures along\n> contours and in homogeneous regions. We demonstrate that trained denoisers\n> are inductively biased towards these geometry-adaptive harmonic bases since\n> they arise not only when the network is trained on photographic images, but\n> also when it is trained on image classes supported on low-dimensional\n> manifolds for which the harmonic basis is suboptimal. Finally, we show that\n> when trained on regular image classes for which the optimal basis is known\n> to be geometry-adaptive and harmonic, the denoising performance of the\n> networks is near-optimal.\n\nComments:| Accepted for oral presentation at ICLR, Vienna, May 2024  \n---|---  \nSubjects:| Computer Vision and Pattern Recognition (cs.CV); Machine Learning\n(cs.LG)  \nCite as:| arXiv:2310.02557 [cs.CV]  \n(or arXiv:2310.02557v3 [cs.CV] for this version)  \nhttps://doi.org/10.48550/arXiv.2310.02557arXiv-issued DOI via DataCite  \n  \n## Submission history\n\nFrom: Florentin Guth [view email] [v1] Wed, 4 Oct 2023 03:30:32 UTC (2,846 KB)\n[v2] Fri, 15 Mar 2024 18:21:48 UTC (6,790 KB) [v3] Fri, 12 Apr 2024 15:48:47\nUTC (6,797 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Generalization in diffusion models arises from\ngeometry-adaptive harmonic representations, by Zahra Kadkhodaie and 3 other\nauthors\n\n  * View PDF\n  * HTML (experimental)\n  * TeX Source\n  * Other Formats\n\nview license\n\nCurrent browse context:\n\ncs.CV\n\n< prev | next >\n\nnew | recent | 2310\n\nChange to browse by:\n\ncs cs.LG\n\n### References & Citations\n\n  * NASA ADS\n  * Google Scholar\n  * Semantic Scholar\n\na export BibTeX citation Loading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer (What is the Explorer?)\n\nLitmaps (What is Litmaps?)\n\nscite Smart Citations (What are Smart Citations?)\n\n# Code, Data and Media Associated with this Article\n\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\n\nDagsHub (What is DagsHub?)\n\nGotit.pub (What is GotitPub?)\n\nPapers with Code (What is Papers with Code?)\n\nScienceCast (What is ScienceCast?)\n\n# Demos\n\nReplicate (What is Replicate?)\n\nHugging Face Spaces (What is Spaces?)\n\nTXYZ.AI (What is TXYZ.AI?)\n\n# Recommenders and Search Tools\n\nInfluence Flower (What are Influence Flowers?)\n\nConnected Papers (What is Connected Papers?)\n\nCORE Recommender (What is CORE?)\n\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new\narXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and\naccepted our values of openness, community, excellence, and user data privacy.\narXiv is committed to these values and only works with partners that adhere to\nthem.\n\nHave an idea for a project that will add value for arXiv's community? Learn\nmore about arXivLabs.\n\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n\n  * About\n  * Help\n\n  * Contact\n  * Subscribe\n\n  * Copyright\n  * Privacy Policy\n\n  * Web Accessibility Assistance\n  * arXiv Operational Status Get status notifications via email or slack\n\n", "frontpage": false}
