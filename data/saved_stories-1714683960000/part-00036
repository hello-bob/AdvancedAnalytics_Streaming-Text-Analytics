{"aid": "40235106", "title": "Exploring exaptations in engineering practices within a RAG-Based application", "url": "https://blog.owulveryck.info/2024/04/29/exploring-exaptations-in-engineering-practices-within-a-rag-based-application.html", "domain": "owulveryck.info", "votes": 1, "user": "owulveryck", "posted_at": "2024-05-02 11:57:13", "comments": 0, "source_title": "Exploring exaptations in engineering practices within a RAG-Based application", "source_text": "Exploring exaptations in engineering practices within a RAG-Based application\n- Unladen swallow - Olivier Wulveryck\n\nowulveryck's blog\n\nowulveryck's blog\n\n# Exploring exaptations in engineering practices within a RAG-Based\napplication\n\nby Olivier Wulveryck\n\n2024-04-29\n\n## Context\n\nIn this article, I will explore the concept of RAG, but not in the typical\nway. My aim is to essentially create a RAG from the beginning to view it as a\npurely engineering problem.\n\nStarting from scratch will enable me to:\n\n  * potentially uncover a form of exaptation that can inform my decisions as an engineer and guide me in a specific direction.\n  * clarify any points of confusion I may have in comprehending the system.\n\nNote: The from scratch approach is difficult because the generation of the\nembedding is linked to the model and tokenization, but let\u2019s consider it as\nfrom scratch for the Engineering part, which will be sufficient for me.\n\nAs a bootstrap, I used the information from this article because it\u2019s clear\nand written in Go, and I am fluent in that language. I have no additional\ninsights to offer beyond the original article on the technical part (the\nauthor made a great job).\n\nTherefore, this isn\u2019t an article about Go, but really an article about IT\nengineering.\n\nIn this article, I will write the step by step method I used to write a simple\n(and non-efficient nor effective) RAG, but I will also note the discoveries\nthat may be useful for my work as a consultant and engineer.\n\n### Organisation of the article and of the code\n\n  * The initial section discusses data acquisition, emphasizing the significance of preparing the data to be readily utilized by a Language Model (LLM).\n  * The subsequent section involves transforming the data into a mathematical representation that facilitates easy searching. The outcomes are stored in a database that will be utilized by the application.\n  * The final section pertains to the application itself: it will interpret a question, identify the relevant data segment in the database, and query the LLM.\n  * The document concludes with a summary and suggestions on how to convert this Proof of Concept (POC) into a custom-made solution.\n\nThe sequence of the program is roughly this:\n\n## The use case\n\nIn the introductory section, I outlined the anticipated outcome I am aiming\nfor. This result revolves around discovering the partial answers to the\nquestion: \u201cWhat is the engineering role in the setup of an application powered\nby AI\u201d. To effectively steer my actions towards this goal, I require a use-\ncase. This use-case should have a clearly defined output that signifies the\nconclusion of this experiment.\n\nBelow is the detailed description of the use-case:\n\nI frequently dig into books that I regard as \u201creference\u201d materials, such as\n\u201cteam topologies,\u201d \u201cDDD,\u201d and others. One such reference that I\u2019m currently\nengrossed in is \u201cthe value flywheel effect\u201d.\n\nThis insightful book not only discusses strategy but also offers guidance on\nhow to apply Simon Wardley\u2019s theory. It describes a wide range of use case,\nsuch as how to utilize maps in a conversation with a CEO, or how to map out a\ntechnological solution.\n\nIn the realm of consulting assignments, mapping proves to be an invaluable\ntool. This book is a treasure trove of crucial information for maximizing the\neffectiveness of these tools.\n\nAs an illustration, I have compiled a list of questions that can function as\nan interview framework during a consulting mission.\n\nMy present goal is to interact in a \u201cconversation\u201d with my virtual assistant,\nposing particular inquiries and obtaining responses grounded on the book.\n\nTo achieve this, I will employ a RAG strategy: Retrieve the content\ncorresponding to my query, Augment the prompt with the information retrieved,\nand then allow the LLM to Generate the reply.\n\n## First Step: Data Acquisition\n\nThe initial stage in creating a RAG involves gathering the necessary data and\nconducting a thorough cleanup.\n\n### Data Collection\n\nTo experiment with the RAG, I need data, or in this case, a book. For The\nValue Flywheel Effect, I purchased the book.\n\nHowever, there\u2019s an initial hurdle to overcome: the need to secure the rights\nto use the data. Simply owning the book doesn\u2019t grant me the liberty to\nmanipulate its content. This is due to the book\u2019s licensing restrictions that\nprohibit such actions. For now, to verify the project\u2019s viability, I\u2019ll use a\ndifferent book.\n\nThis alternative book is under a creative commons license, already formatted,\nand is a work I\u2019m familiar with. Additionally, it\u2019s relevant to the subject\nmatter: it\u2019s Simon Wardley\u2019s book.\n\nFirst Lesson (obvious): Having access to the data is a significant advantage.\nI\u2019ve always been aware of this, but this experience truly emphasizes its\nsignificance.\n\n### Data Cleanup\n\nSimon Wardley\u2019s book has been converted into many formats. This repository\nprovides a version in asciidoc.\n\nThe text will be fed into the LLM, which is a Language model. Therefore, it\u2019s\ncrucial to aid the LLM in pinpointing the main component of the text - the\ncontent, and eliminate any distractions designed to help the human reader,\nsuch as centering or font size. However, we do not wish to remove the\nstructure and segmentation of the text, which serve as important indicators\nand dividers of the content.\n\nIn this scenario, Markdown proves to be exceptionally useful. The syntax is\nsimple enough and consumes few tokens and therefore avoid creating any noise\nfor the system.\n\nA little bit of \u201casciidoc and pandoc\u201d and there you go: a few markdown content\nfiles.\n\nSecond lesson: I was lucky because someone had already done the conversion\nwork into a \u201cdigitally exploitable\u201d format. This step can be long and is a\ndata engineering task.\n\n## Second step: creation of the embedding\n\nThis is a part that also falls under engineering. This part will aim to\nconvert pieces of text into numerical representation (an array of numbers, a\nvector). This process is called embedding (or word embedding).\n\nAn algorithm is used for converting a set of token (roughly pieces of words)\ninto vectors. As seen before, this algorithm is linked to the model that we\nwill use. Simply put, the program will call an OpenAI API for each piece that\nwill return the corresponding vector. This vector is then stored in the\ndatabase.\n\nBut how to slice the text ? Shall we slice it into fixed size parts? Shall we\nslice it by chapters? Paragraphs? It depends! There\u2019s no universal approach.\nTo clarify, let\u2019s take a step back and sketch out the basic concepts.\n\nThe workflow I\u2019m going to use is based on a question I\u2019ll pose to my engine.\nThe first step involves understanding the question and, depending on its\ncontext, identifying a section of the document that might contain the answer.\n\nThe process of embedding translates text into a \u201cvector\u201d. We then use\nmathematical tools to identify vectors that are similar. These similar vectors\nare likely to be dealing with the same context. Hence, it\u2019s essential to\nprecisely segment the text into sections to create relevant and meaningful\nvectors.\n\nConsider this sentence as an example:\n\n\u201cIn the summertime, our gardens overflow with fragrant mint, perfect for\nenhancing our homemade sauce\u201d.\n\nLet\u2019s say I have a vector representing \u201ccooking\u201d that is vertical, and another\nvector representing \u201cgardening\u201d. The entire sentence will lean more towards\ncooking than gardening. However, if I split the sentence into approximately\ntwo equal parts, I\u2019ll have one segment that is closely related to gardening,\nand a non-essential segment, closely related to cooking.\n\nThird lesson (obvious): A \u201cbusiness\u201d expertise may be necessary to analyze the\ndata and achieve maximum efficiency in the application.\n\nFor the purpose of this test, I will divide the data into equal segments of x\nnumber of words. This might be sufficient for the validation of my Proof of\nConcept.\n\nI execute the code exactly as outlined in the original blog post. This process\nwill segment the text, invoke the OpenAI embedding API for each segment, and\nsubsequently store the outcome in a relational SQLite database.\n\nPossible exaptation: I ultimately obtain a SQLite database that encapsulates\nthe Wardley book in a mathematical model compatible with OpenAI. If I possess\nmultiple books, I have the option to either expand this database or establish\nseparate databases for each book. The intriguing aspect is that the SQLite\ndatabase serves as a standalone knowledge base that can be utilized with the\nOpenAI API. This opens up the possibility of writing any additional code that\nleverages this database in whatever language seperating the \u201cbuilding process\u201d\nof the \u201crun process\u201d.\n\n## Last step: inference\n\nInference forms the core of my application. The process begins when I enter a\nquestion. The application then scours my database to find the piece that\naligns with the context of the question. This information is then forwarded to\nOpenAI, which generates a response.\n\nIn this scenario, there is no vector base, and the search process is\nstraightforward:\n\n  * First, we compute the embedding of the question. This is done through an API call, similar to how we calculate the embedding of the pieces.\n  * Next, we conduct a cosine similarity calculation for each element in the database.\n  * We then select the best result, which is the one that is most pertinent to the question.\n  * Finally, we send this result to the LLM engine via API in prompt mode, along with the original question.\n\n### Similarity computation: identifying the relevant segment\n\nIf the input dataset expands in size (for instance, if I use the same database\nfor multiple books), a more efficient approach for computing similarity will\nbecome necessary. This is where the power of a vector database shines.\n\nCurrently, the similarity calculation is manually executed in a large loop\nusing a basic similarity calculation algorithm. However, if the volume of data\nbecomes too large (for example, if I aim to index an entire library), this\nmethod will prove inefficient. At this point, we will transition to a vector-\nbased approach.\n\nThis vector-based system will identify the most suitable \u201cneighbor\u201d. It\nremains to be seen which algorithms they employ. Do all vector bases yield the\nsame result? This is a fascinating aspect that I believe warrants further\nexploration in my role as a consultant.\n\nLesson Four: Avoid over-engineering or complicating your tech stack, specially\nin the genesis/POC phase. Instead, concentrate on addressing your specific\nproblem. Seek the expertise of specialists when necessary for scaling (when\nentering stage II of evolution: crafting).\n\n### Let\u2019s prompt\n\nThe final step involves constructing a prompt using the extracted information,\nwhich will then be sent to the LLM. In my specific scenario, this involves\nmaking a call to the OpenAI API.\n\nBelow is the basic structure of the prompt that is hard-coded into the\nprogram. The %v placeholder will be substituted with the appropriate segment\nof text and the corresponding question:\n\n    \n    \n    1Use the below information to answer the subsequent question. 2Information: 3%v 4 5Question: %v\n\nFourth learning: We enter into prompt engineering, I can replace my hardcoded\nquestion with something like:\n\n    \n    \n    1Use the below information to answer the subsequent question and add the origin. 2Origin: 3 4chapter %v 5 6Information: 7%v 8 9Question: %v\n\nTo do this, I then have to complete my initial database by adding for each\npiece, its source (chapter). This requires a little thought about its use case\nupstream.\n\n#### Database and Prompt Coupling\n\nIn reality, the database comprises two tables:\n\n  * chunks\n  * embeddings\n\nThe chunks table currently has 4 columns:\n\n  * id\n  * path - the path of the source file (in my case chapter[1-9].md)\n  * nchunk - the chunk number in the segmentation (mostly for debugging)\n  * content - the content of the chunk\n\nThe embedding table contains:\n\n  * id\n  * embedding in \u201cblob\u201d format\n\nThe information of the prompt needs to be coherent with the information of the\ndatabase (specially in the \u201cchunks\u201d table). In the exploratory space in POC\nmode, it\u2019s not a problem, but entering the phase II will require a bit of\nproduct thinking and conception ahead of the code.\n\n## Results, findings, and fun part\n\nBy compiling the program, I can query my knowledge base:\n\n    \n    \n    1\u276f ./rag -db=../../data/db/wardley.db -answer \"give me examples of inertia\" 2>/dev/null 21. Resistance to change in business due to past success and uncertainty in co-evolving practices. 32. Consumer concerns about disruption to past norms, transition to the new, and the agency of the new when adopting cloud computing. 43. Suppliers' inertia to change due to past financial success, internal resistance, and external market expectations. 54. Financial markets' inertia towards stability and past results. 65. Cultural inertia caused by past success in fulfilling a business model. 76. Resistance to change caused by cutting costs in response to declining revenue in a changing industry. 87. Inertia in reacting to disruptive changes in the market, such as the shift from products to utility services in computing. 98. Inertia in transitioning from custom-built solutions to product offerings. 109. Resistance to change in response to disruptive changes in various industries, leading to companies' demise. 1110. Failure to adapt to predictable disruptions, such as the shift from products to utility services, leading to companies' downfall.\n\nAs the engine is the GPT-x language, I can even ask it in french, it will\nmanage:\n\n    \n    \n    1\u276f ./rag -db=../../data/db/wardley.db -answer \"donne moi tous les exemples d'inertie\" 2>/dev/null 2Les exemples d'inertie mentionn\u00e9s dans le texte sont : 3- \"Perte de capital social\" : r\u00e9sistance au changement due \u00e0 des relations commerciales existantes avec des fournisseurs. 4- \"Peur, incertitude et doute\" : tentative des fournisseurs de convaincre les \u00e9quipes internes de ne pas adopter les nouveaux changements. 5- \"Perte de capital politique\" : r\u00e9sistance au changement due \u00e0 un manque de confiance envers la direction. 6- \"Barri\u00e8res \u00e0 l'entr\u00e9e\" : peur que le changement permette l'entr\u00e9e de nouveaux concurrents. 7- \"Co\u00fbt de l'acquisition de nouvelles comp\u00e9tences\" : co\u00fbt croissant de l'acquisition de nouvelles comp\u00e9tences en raison d'une demande accrue. 8- \"Adaptabilit\u00e9\" : pr\u00e9occupations quant \u00e0 la pr\u00e9paration du march\u00e9 ou des clients au changement.\n\nFifth learning: it is observed here that the results are less complete. It is\na help, but not a search engine. Idempotence stops at the moment of retrieving\ninformation from the embedding base. Then it\u2019s YOLO :D\n\n## Conclusion and opening about coupling and software architecture\n\nI have successfully created two independent assets:\n\n  * A Go-based binary that doesn\u2019t require installation. It\u2019s designed to query any knowledge base in its specific format.\n  * The knowledge base itself: wardley.db\n\nIn the future, I can work on a different book, generate an embedding, and\nshare it. The more I break it down into parts, the more valuable the base will\nbecome, regardless of the inference engine used.\n\nKey takeaway: The versioning of the program is only loosely tied to my data.\nThis allows me to clean and feed data independently of IT engineering. I might\neven be able to automate this process through a pipeline.\n\nHowever, there\u2019s a risk to consider: altering the database could potentially\nbreak the SQL queries, and the same applies if I change the prompt.\n\nTo mitigate this, I have two options:\n\n  * I could version my database concurrently with the code. This means that version 1 of the code would only be compatible with version 1 of the database.\n  * Alternatively, I could extract the template to create an abstraction. This would result in a strong coupling between the template and the database, but a weaker coupling between the code and the database. (And of course, if I change the database, I\u2019ll have another issue to deal with, but we can manage that with adapters).\n\nA clever approach to managing this coupling is to treat the prompt as a\nseparate asset. This would create a sort of port-and-adapters architecture\nwhere communication is conducted by natural language. Fun!\n\nPowered by Hugo | Theme - Jane \u00a9 2015 - 2024 Olivier Wulveryck\n\n", "frontpage": false}
