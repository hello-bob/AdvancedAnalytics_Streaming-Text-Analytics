{"aid": "40051975", "title": "A visual guide to Vision Transformer \u2013 A scroll story", "url": "https://blog.mdturp.ch/posts/2024-04-05-visual_guide_to_vision_transformer.html", "domain": "mdturp.ch", "votes": 32, "user": "md2rp", "posted_at": "2024-04-16 14:00:58", "comments": 3, "source_title": "A Visual Guide to Vision Transformers | MDTURP", "source_text": "A Visual Guide to Vision Transformers | MDTURP\n\nSkip to content\n\nMDTURP\n\nAppearance\n\nOn this page\n\n# A Visual Guide to Vision Transformers\n\nThis is a visual guide to Vision Transformers (ViTs), a class of deep learning\nmodels that have achieved state-of-the-art performance on image classification\ntasks. Vision Transformers apply the transformer architecture, originally\ndesigned for natural language processing (NLP), to image data. This guide will\nwalk you through the key components of Vision Transformers in a scroll story\nformat, using visualizations and simple explanations to help you understand\nhow these models work and how the flow of the data through the model looks\nlike.\n\nPlease enjoy and start scrolling!\n\n### 0) Lets start with the data\n\nLike normal convolutional neural networks, vision transformers are trained in\na supervised manner. This means that the model is trained on a dataset of\nimages and their corresponding labels.\n\n### 1) Focus on one data point\n\nTo get a better understanding of what happens inside a vision transformer lets\nfocus on a single data point (batch size of 1). And lets ask the question: How\nis this data point prepared in order to be consumed by a transformer?\n\n### 2) Forget the label for the moment\n\nThe label will become more relevant later. For now the only thing that we are\nleft with is a single image.\n\n### 3) Create patches of the image\n\nTo prepare the image for the use inside the transformer we divide the image\ninto equally sized patches of size p x p.\n\n### 4) Flatting of the images patches\n\nThe patches are now flattened into vectors of dimension p'= p2*c where p is\nthe size of the patch and c is the number of channels.\n\n### 5) Creating patch embeddings\n\nThese image patch vectors are now encoded using a linear transformation. The\nresulting Patch Embedding Vector has a fixed size d.\n\n### 6) Embedding all patches\n\nNow that we have embedded our image patches into vectors of fixed size, we are\nleft with an array of size n x d where n is the the number of image patches\nand d is the size of the patch embedding\n\n### 7) Appending a classification token\n\nIn order for us to effectively train our model we extend the array of patch\nembeddings by an additional vector called classification token (cls token).\nThis vector is a learnable parameter of the network and is randomly\ninitialized. Note: We only have one cls token and we append the same vector\nfor all data points.\n\n### 8) Add positional embedding Vectors\n\nCurrently our patch embeddings have no positional information associated with\nthem. We remedy that by adding a learnable randomly initialized positional\nembedding vector to all our patch embeddings. We also add a such a positional\nembedding vector to our classification token.\n\n### 9) Transformer Input\n\nAfter the positional embedding vectors have been added we are left with an\narray of size (n+1) x d . This will be our input for the transformer which\nwill be explained in greater detail in the next steps\n\n### 10.1) Transformer: QKV Creation\n\nOur transformer input patch embedding vectors are linearly embedded into\nmultiple large vectors. These new vectors are than separated into three equal\nsized parts. The Q - Query Vector, the K - Key Vector and the V - Value Vector\n. We will have (n+1) of a all of those vectors.\n\n### 10.2) Transformer: Attention Score Calculation\n\nTo calculate our attention scores A we will now multiply all of our query\nvectors Q with all of our key vectors K.\n\n### 10.3)Transformer: Attention Score Matrix\n\nNow that we have the attention score matrix A we apply a `softmax` function to\nevery row such that every row sums up to 1.\n\n### 10.4)Transformer: Aggregated Contextual Information Calculation\n\nTo calculate the aggregated contextual information for the first patch\nembedding vector. We focus on the first row of the attention matrix. And use\nthe entires as weights for our Value Vectors V. The result is our aggregated\ncontextual information vector for the first image patch embedding.\n\n### 10.5)Transformer: Aggregated Contextual Information for every patch\n\nNow we repeat this process for every row of our attention score matrix and the\nresult will be N+1 aggregated contextual information vectors. One for every\npatch + one for the classification token. This steps concludes our first\nAttention Head.\n\n### 10.6)Transformer: Multi-Head Attention\n\nNow because we are dealing multi head attention we repeat the entire process\nfrom step 10.1 - 10-5 again with a different QKV mapping. For our explanatory\nsetup we assume 2 Heads but typically a VIT has many more. In the end this\nresults in multiple Aggregated contextual information vectors.\n\n### 10.7)Transformer: Last Attention Layer Step\n\nThese heads are stacked together and are mapped to vectors of size d which was\nthe same size as our patch embeddings had.\n\n### 10.8)Transformer: Attention Layer Result\n\nThe previous step concluded the attention layer and we are left with the same\namount of embeddings of exactly the same size as we used as input.\n\n### 10.9)Transformer: Residual connections\n\nTransformers make heavy use of residual connections which simply means adding\nthe input of the previous layer to the output the current layer. This is also\nsomething that we will do now.\n\n### 10.10)Transformer: Residual connection Result\n\nThe addition results in vectors of the same size.\n\n### 10.11)Transformer: Feed Forward Network\n\nNow these outputs are feed through a feed forward neural network with non\nlinear activation functions\n\n### 10.12)Transformer: Final Result\n\nAfter the transformer step there is another residual connections which we will\nskip here for brevity. And so the last step concluded the transformer layer.\nIn the end the transformer produced outputs of the same size as input.\n\n### 11) Repeat Transformers\n\nRepeat the entire transformer calculation Steps 10.1 - Steps 10.12 for the\nTransformer several times e.g. 6 times.\n\n### 12) Identify Classification token output\n\nLast step is to identify the classification token output. This vector will be\nused in the final step of our Vision Transformer journey.\n\n### 13) Final Step: Predicting classification probabilities\n\nIn the final and last step we use this classification output token and another\nfully connected neural network to predict the classification probabilities of\nour input image.\n\n### 14) Training of the Vision Transformer\n\nWe train the Vision Transformer using a standard cross-entropy loss function,\nwhich compares the predicted class probabilities with the true class labels.\nThe model is trained using backpropagation and gradient descent, updating the\nmodel parameters to minimize the loss function.\n\n## Conclusion\n\nIn this visual guide, we have walked through the key components of Vision\nTransformers, from the data preparation to the training of the model. We hope\nthis guide has helped you understand how Vision Transformers work and how they\ncan be used to classify images.\n\nI prepared this little Colab Notebook to help you understand the Vision\nTransformer even better. Please have look for the 'Blogpost' comment. The code\nwas taken from @lucidrains great VIT Pytorch implementation be sure to\ncheckout his work.\n\nIf you have any questions or feedback, please feel free to reach out to me.\nThank you for reading!\n\n## Acknowledgements\n\n  * VIT Pytorch implementation\n  * All images have been taken from Wikipedia and are licensed under the Creative Commons Attribution-Share Alike 4.0 International license.\n\nCreated by M. Dennis Turp\n\n", "frontpage": true}
