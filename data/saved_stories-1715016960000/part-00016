{"aid": "40273071", "title": "What Is in a Rust Allocator?", "url": "https://blog.sulami.xyz/posts/what-is-in-a-rust-allocator/", "domain": "sulami.xyz", "votes": 1, "user": "sulami", "posted_at": "2024-05-06 10:32:28", "comments": 0, "source_title": "What is in a Rust Allocator? - sulami's blog", "source_text": "What is in a Rust Allocator? - sulami's blog\n\nWeak Opinions, Strongly Held\n\nFeed \u2022 Uses \u2022 About\n\n# What is in a Rust Allocator?\n\nPublished on 2024-05-06\n\nTags: rust\n\nRecently I was reading the documentation for jemalloc and wondered what\nactually goes into a custom memory allocator for Rust. To add jemalloc, or any\nother custom allocator, to a Rust application, one uses the\n#[global_allocator] macro. Conveniently, the documentation for GlobalAlloc\nactually has a full example allocator for us to explore. It starts out as\nfollows:\n\n    \n    \n    const ARENA_SIZE: usize = 128 * 1024; const MAX_SUPPORTED_ALIGN: usize = 4096; #[repr(C, align(4096))] // 4096 == MAX_SUPPORTED_ALIGN struct SimpleAllocator { arena: UnsafeCell<[u8; ARENA_SIZE]>, remaining: AtomicUsize, // we allocate from the top, counting down }\n\nWe begin by defining a constant arena size of 128 kB, which will be statically\nallocated at startup. Arenas are often faster because they make fewer memory\nallocation requests to the operating system and also reduce memory\nfragmentation, at the cost of memory overhead due to the unused space in the\narena. In almost every real world application, a memory allocator would use\nmany smaller arenas as opposed to one large one, and also have the\ncapabilities to add additional arenas at runtime as the existing ones fill up.\n\nNext we define the representation via repr. The representation describes the\nlayout of this type in memory. The default representation is Rust, which means\nthe Rust compiler can do whatever it wants for optimization purposes. repr(C)\nlays out the memory like C/C++ do, which is useful for interoperability with\nother languages. It is also somewhat more predictable, as those layouts do not\nchange anymore. The Rust reference has a detailed breakdown of the different\nlayout options.\n\nIn this case we also define a memory alignment, which means we are saying that\nthe memory address of the allocator should be a multiple of 4096. Rust will\nalso make sure that each of the fields will be aligned in this way, and add\npadding in between the fields if necessary. We do this because clients will\nlater request memory with a certain memory alignment themselves, and aligning\nthe overall array means any aligned offset inside the array will also be\naligned automatically.\n\nThe struct itself has two fields. The arena is a fixed size byte array,\nwrapped in an UnsafeCell. UnsafeCell is the base for all the regular, safe\ncells like Cell or RefCell, all of which enable interior mutability. This is\nrequired because GlobalAlloc's alloc method takes a shared &self reference,\nnot a mutable &mut self one, and returns a *mut u8 pointer into the interior\narray. UnsafeCell gives us the required escape hatch to return a mutable\npointer into a struct we only have a shared reference to.\n\nThe second field remaining is an offset from the start of the arena,\neffectively acting as a pointer into the arena. Because we are counting down\n\"from the top,\" it starts out with the size of the arena, going down as memory\nis allocated.The benefit of this approach is that checking if we have enough\nmemory available to satisfy a request is as simple as checking if remaining is\nat least as large as the requested amount of memory.A potential downside is\nthat we cannot simply grow the most recent allocation without moving it, as\nwould be possible if counting up from zero.\n\nremaining is an AtomicUsize. Atomics are used to signal to both the compiler\nand the CPU that ordering of memory access is important to ensure correctness.\nIf we were using a plain usize, the compiler or the CPU could decide that it\nwould be faster to reorder reads or writes, compromising correctness. Rust\ninherits the C++ atomics memory model, which comes with a few ways of\ndescribing access restrictions, which we will dig into in a bit. The\nRustonomicon also has a whole section on atomics with more details.For those\ncurious, Memory Barriers: a Hardware View for Software Hackers has a low level\ndescription of why and how ordering is done inside CPUs.\n\n    \n    \n    #[global_allocator] static ALLOCATOR: SimpleAllocator = SimpleAllocator { arena: UnsafeCell::new([0x55; ARENA_SIZE]), remaining: AtomicUsize::new(ARENA_SIZE), };\n\nThis is the bit that actually selects the globally used allocator, using the\naforementioned #[global_allocator] macro. It is simply creating a static\ninstance of the allocator, initializing the arena with 0x55.To the best of my\nknowledge this value is selected because it results in alternating ones and\nzeroes, which might aid in debugging.\n\n    \n    \n    unsafe impl Sync for SimpleAllocator {}\n\nWe then implement the Sync marker trait for our allocator, which does not have\nany methods, but simply tells the compiler that it is safe to share references\nto the allocator across thread boundaries. This is an unsafe trait, and it\nrequires us to make sure that the resulting behaviour is actually sound. We\nwill see below how we actually accomplish this.\n\nNow let us look at the implementation for GlobalAlloc, again an unsafe trait.\n\n    \n    \n    unsafe impl GlobalAlloc for SimpleAllocator { unsafe fn alloc(&self, layout: Layout) -> *mut u8 { let size = layout.size(); let align = layout.align();\n\nThe first required method is alloc, which requests some memory from the\nallocator according to Layout, expecting a pointer to that memory. If there is\nno memory matching the request available, we return a null pointer. Layout has\ntwo methods that are interesting to us, size and align, which return the\ndesired size and alignment respectively. The size is just a number of bytes,\nand the alignment works just the same as we noted above when looking at repr.\n\n    \n    \n    // `Layout` contract forbids making a `Layout` with align=0, or align not power of 2. // So we can safely use a mask to ensure alignment without worrying about UB. let align_mask_to_round_down = !(align - 1);\n\nThis comment mentions a contract, a requirement that is not captured outside\nof documentation. We can assume that alignment is a non-zero power of two.\nThis is important for the next step. We set up a bitmask that we will use for\nensuring the correct alignment.\n\nAlignment is a power of two by contract, which means it's a single set bit. By\ndecrementing it by one, we transform e.g. 8 (00001000) to 7 (00000111), and\nthen invert that to set all high bits up to and including the original one,\n11111000. We will then & this onto our memory address later, which acts as a\nround down by setting all the lower bits to zero. Rounding down works because\nwe made sure to align the overall array, and are allocating from the top down,\nso that rounding down introduces a gap \"above\" the newly allocated memory. If\nwe were counting from the bottom up, we would be rounding up instead to\nintroduce a gap \"below.\"\n\n    \n    \n    if align > MAX_SUPPORTED_ALIGN { return null_mut(); }\n\nAt this point we just check if the client requested an alignment higher than\nwe have chosen to support, in which case we return a null pointer to signal\nthat we cannot fulfill the request. Again, this is dependent on the alignment\nof the arena array.\n\n    \n    \n    let mut allocated = 0; if self .remaining .fetch_update(SeqCst, SeqCst, |mut remaining| { if size > remaining { return None; } remaining -= size; remaining &= align_mask_to_round_down; allocated = remaining; Some(remaining) }) .is_err() { return null_mut(); };\n\nThis is where the heavy lifting of allocation is done, and there is a lot to\nunpack here. We recall that remaining is an AtomicUsize describing an offset\nfrom the start of the arena, counting down as we allocate memory. Atomics rely\non the underlying platform to maintain the correct order of operations, which\nis why they cannot be read or set like regular variables, but have a special\nset of methods to access them. In this case we are using the fetch_update\nmethod, which is a form of read-write-modify instruction. As mentioned above,\nif we were not using an atomic, either the compiler or the CPU itself could\ndecide to reorder the different reads and writes to the counter for\nperformance optimizations, compromising correctness.\n\nremaining starts out pointing at the start of the previous allocation, if any,\nso we just subtract the size of the new allocation. Note that this is also\nwhere we use our bitmask to round down the address to get the right alignment.\n\nThe function passed to the fetch_update method returns Some(new_value), in\nthis case Some(remaining), but fetch_update itself returns Result<old_value,\nErr>. Incidentally the passed function might also be called multiple times if\nthere is contention from other threads. The mutable variable allocated in the\nouter scope allows us to extract the new value of remaining without re-\nfetching remaining and risking a data race.\n\nfetch_update also takes two arguments which in this case are both SeqCst,\nshort for sequentially consistent. These are atomic memory orderings inherited\nfrom C++'s memory model, and specify the level of synchronization required\nwhen accessing this memory. There are a few different levels of which this is\nthe strictest, guaranteeing that no reads and writes are reordered such that\nthey switch order with the access to remaining. This is important to avoid\ndata races that could lead to overlapping allocations or over-committing\nmemory that is not actually available.\n\n    \n    \n    self.arena.get().cast::<u8>().add(allocated) }\n\nWe get a pointer to the start of the arena, add the offset of the newly\nallocated region from the start of the arena, and then return it as a pointer\nto that same region.\n\nThis is the reason for wrapping the arena in an UnsafeCell, it allows us to\nget a mutable pointer to its contents from a shared reference. We cannot use\nstd::ptr::addr_of_mut! or just coerce a reference to the right type because we\nonly have a shared reference to self, and thus cannot derive a mutable\nreference without interior mutability. We could use a RefCell and its as_ptr\nmethod though.I actually don't know why the author chose not to. The pull\nrequest adding this example has no discussion about this option.\n\n    \n    \n    unsafe fn dealloc(&self, _ptr: *mut u8, _layout: Layout) {}\n\nReleasing memory in this case is a no-op, which means that memory once\nallocated is gone forever, even after the contained values have been dropped.\nIn a real-world application, we would probably want to keep track of memory\nthat has become available again, so that we could use it again for new\nallocations.\n\nAnd that is all that is to it. Of course this is a very simple allocator, but\nit is actually functional. It does not actually dynamically request memory\nfrom the operating system, but that is actually a common pattern in embedded\ndevelopment where there is no operating system. Still, in those scenarios we\nwould probably want to reuse freed memory, which would require keeping track\nof which regions in the arena are currently in use and which are not, if we\ncan live with the memory overhead required to store that information.\n\nBuilt 2024-05-06 09:49\n\n", "frontpage": false}
