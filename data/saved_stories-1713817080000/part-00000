{"aid": "40113790", "title": "Implementation of vision language model in a single file of PyTorch", "url": "https://github.com/AviSoori1x/seemore", "domain": "github.com/avisoori1x", "votes": 4, "user": "avisoori1x", "posted_at": "2024-04-22 12:40:04", "comments": 0, "source_title": "GitHub - AviSoori1x/seemore: From scratch implementation of a vision language model in pure PyTorch", "source_text": "GitHub - AviSoori1x/seemore: From scratch implementation of a vision language\nmodel in pure PyTorch\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nAviSoori1x / seemore Public\n\n  * Notifications\n  * Fork 0\n  * Star 1\n\nFrom scratch implementation of a vision language model in pure PyTorch\n\n### License\n\nMIT license\n\n1 star 0 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# AviSoori1x/seemore\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nAviSoori1xUpdate README.mdApr 22, 2024a726b03 \u00b7 Apr 22, 2024Apr 22, 2024\n\n## History\n\n34 Commits  \n  \n### images\n\n|\n\n### images\n\n| Create blog.md| Apr 22, 2024  \n  \n### modules\n\n|\n\n### modules\n\n| Add files via upload| Apr 21, 2024  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Initial commit| Apr 17, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update README.md| Apr 22, 2024  \n  \n### seemore.py\n\n|\n\n### seemore.py\n\n| Add files via upload| Apr 21, 2024  \n  \n### seemore_Concise.ipynb\n\n|\n\n### seemore_Concise.ipynb\n\n| Add files via upload| Apr 21, 2024  \n  \n### seemore_from_Scratch.ipynb\n\n|\n\n### seemore_from_Scratch.ipynb\n\n| Add files via upload| Apr 22, 2024  \n  \n## Repository files navigation\n\n# seemore\n\nDeveloped using Databricks with \u2764\ufe0f\n\n#### Vision Language Model from scratch in Pytorch\n\nHuggingFace Community Blog that walks through this:\nhttps://huggingface.co/blog/AviSoori1x/seemore-vision-language-model\n\nIn this simple implementation of a vision language model (VLM), there are 3\nmain components.\n\n  1. Image Encoder to extract visual features from images. In this case I use a from scratch implementation of the original vision transformer used in CLIP. This is actually a popular choice in many modern VLMs. The one notable exception is Fuyu series of models from Adept, that passes the patchified images directly to the projection layer.\n\n  2. Vision-Language Projector - Image embeddings are not of the same shape as text embeddings used by the decoder. So we need to \u2018project\u2019 i.e. change dimensionality of image features extracted by the image encoder to match what\u2019s observed in the text embedding space. So image features become \u2018visual tokens\u2019 for the decoder. This could be a single layer or an MLP. I\u2019ve used an MLP because it\u2019s worth showing.\n\n  3. A decoder only language model. This is the component that ultimately generates text. In my implementation I\u2019ve deviated from what you see in LLaVA etc. a bit by incorporating the projection module to my decoder. Typically this is not observed, and you leave the architecture of the decoder (which is usually an already pretrained model) untouched.\n\nThe scaled dot product self attention implementation is borrowed from Andrej\nKapathy's makemore (https://github.com/karpathy/makemore). Also the decoder is\nan autoregressive character-level language model, just like in makemore. Now\nyou see where the name 'seemore' came from :)\n\nEverything is written from the ground up using pytorch. That includes the\nattention mechanism (both for the vision encoder and language decoder), patch\ncreation for the vision transformer and everything else. Hope this is useful\nfor anyone going through the repo and/ or the associated blog.\n\nPublications heavily referenced for this implementation:\n\n  * Large Multimodal Models: Notes on CVPR 2023 Tutorial: https://arxiv.org/pdf/2306.14895.pdf\n  * Visual Instruction Tuning: https://arxiv.org/pdf/2304.08485.pdf\n  * Language Is Not All You Need: Aligning Perception with Language Models: https://arxiv.org/pdf/2302.14045.pdf\n\nseemore.py is the entirety of the implementation in a single file of pytorch.\n\nseemore_from_Scratch.ipynb walks through the intuition for the entire model\narchitecture and how everything comes together. I recommend starting here.\n\nseemore_Concise.ipynb is the consolidated hackable implementation that I\nencourage you to hack, understand, improve and make your own\n\nThe input.txt with tinyshakespear and the base64 encoded string\nrepresentations + corresponding descriptions are in the inputs.csv file in the\nimages directory.\n\nThe modules subdirectory contains each of the components in their own .py file\nfor convenience (should you choose to hack on pieces individually/ reuse for\nyour own projects etc.)\n\nThe code was entirely developed on Databricks using a single A100 for compute.\nIf you're running this on Databricks, you can scale this on an arbitrarily\nlarge GPU cluster with no issues, on the cloud provider of your choice.\n\nI chose to use MLFlow (which comes pre-installed in Databricks. It's fully\nopen source and you can pip install easily elsewhere) as I find it helpful to\ntrack and log all the metrics necessary. This is entirely optional but\nencouraged.\n\nPlease note that the implementation emphasizes readability and hackability vs.\nperformance, so there are many ways in which you could improve this. Please\ntry and let me know!\n\nHope you find this useful. Happy hacking!!\n\n## About\n\nFrom scratch implementation of a vision language model in pure PyTorch\n\n### Topics\n\ndeep-learning pytorch neural-networks multimodal pytorch-implementation large-\nlanguage-models llm llms vision-language-model multimodal-large-language-\nmodels\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\nActivity\n\n### Stars\n\n1 star\n\n### Watchers\n\n1 watching\n\n### Forks\n\n0 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Languages\n\n  * Jupyter Notebook 78.7%\n  * Python 21.3%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
