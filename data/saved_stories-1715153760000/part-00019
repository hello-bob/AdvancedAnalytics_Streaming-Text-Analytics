{"aid": "40292380", "title": "Intel Arc A770: Arrays larger than 4GB crashes", "url": "https://github.com/intel/intel-extension-for-pytorch/issues/325", "domain": "github.com/intel", "votes": 7, "user": "aconz2", "posted_at": "2024-05-07 22:41:10", "comments": 0, "source_title": "Arrays larger than 4 GB crashes \u00b7 Issue #325 \u00b7 intel/intel-extension-for-pytorch", "source_text": "Arrays larger than 4 GB crashes \u00b7 Issue #325 \u00b7 intel/intel-extension-for-\npytorch \u00b7 GitHub\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nintel / intel-extension-for-pytorch Public\n\n  * Notifications\n  * Fork 204\n  * Star 1.4k\n\nJump to bottom\n\n# Arrays larger than 4 GB crashes #325\n\nOpen\n\nBA8F0D39 opened this issue Apr 8, 2023 \u00b7 48 comments\n\nOpen\n\n# Arrays larger than 4 GB crashes #325\n\nBA8F0D39 opened this issue Apr 8, 2023 \u00b7 48 comments\n\nLabels\n\nARC ARC GPU Crash Execution crashes\n\n## Comments\n\n###\n\nBA8F0D39 commented Apr 8, 2023 \u2022\n\n### Describe the bug\n\nIntel compute runtime doesn't allow allocating a buffer bigger than 4\nGB.intel/compute-runtime#627When you allocate an array in intel-extension-for-\npytorch bigger than 4 GB in A770 16GB, it crashes.\n\n    \n    \n    x = torch.rand(46000, 46000, dtype=torch.float32, device='xpu')\n\nIs it possible to allocate multiple buffers for an array instead of allocating\none buffer for one array?\n\n### Versions\n\n    \n    \n    Collecting environment information... PyTorch version: 1.13.0a0+gitb1dde16 PyTorch CXX11 ABI: Yes IPEX version: 1.13.10+xpu IPEX commit: 7d85b0e92 Build type: Release OS: Ubuntu 22.04.1 LTS (x86_64) GCC version: (Ubuntu 11.3.0-1ubuntu1~22.04) 11.3.0 Clang version: N/A IGC version: N/A CMake version: N/A Libc version: glibc-2.35 Python version: 3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0] (64-bit runtime) Python platform: Linux-6.3.0-1-x86_64-with-glibc2.35 Is XPU available: True DPCPP runtime version: N/A MKL version: N/A GPU models and configuration: [0] _DeviceProperties(name='Intel(R) Graphics [0x56a0]', platform_name='Intel(R) Level-Zero', dev_type='gpu, support_fp64=0, total_memory=15473MB, max_compute_units=512) Intel OpenCL ICD version: 22.43.24595.35+i538~22.04 Level Zero version: 1.3.24595.35+i538~22.04 CPU: Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Address sizes: 46 bits physical, 48 bits virtual Byte Order: Little Endian CPU(s): 20 On-line CPU(s) list: 0-19 Vendor ID: GenuineIntel BIOS Vendor ID: Intel(R) Corporation Model name: 13th Gen Intel(R) Core(TM) i5-13600K BIOS Model name: 13th Gen Intel(R) Core(TM) i5-13600K CPU family: 6 Model: 183 Thread(s) per core: 2 Core(s) per socket: 14 Socket(s): 1 Stepping: 1 CPU max MHz: 5100.0000 CPU min MHz: 800.0000 BogoMIPS: 6991.00 Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities Virtualization: VT-x L1d cache: 544 KiB (14 instances) L1i cache: 704 KiB (14 instances) L2 cache: 20 MiB (8 instances) L3 cache: 24 MiB (1 instance) NUMA node(s): 1 NUMA node0 CPU(s): 0-19 Vulnerability Itlb multihit: Not affected Vulnerability L1tf: Not affected Vulnerability Mds: Not affected Vulnerability Meltdown: Not affected Vulnerability Mmio stale data: Not affected Vulnerability Retbleed: Not affected Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl Vulnerability Spectre v1: Mitigation; usercopy/swapgs barriers and __user pointer sanitization Vulnerability Spectre v2: Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence Vulnerability Srbds: Not affected Vulnerability Tsx async abort: Not affected Versions of relevant libraries: [pip3] intel-extension-for-pytorch==1.13.10+xpu [pip3] numpy==1.24.1 [pip3] torch==1.13.0a0+gitb1dde16 [pip3] torchvision==0.14.1a0+0504df5 [conda] N/A  \n  \n---  \nThe text was updated successfully, but these errors were encountered:  \n  \nBA8F0D39 closed this as completed Apr 8, 2023\n\nBA8F0D39 reopened this Apr 14, 2023\n\njingxu10 added ARC ARC GPU Crash Execution crashes labels Apr 16, 2023\n\nContributor\n\n###\n\njingxu10 commented Apr 16, 2023\n\n@tye1  \n---  \n  \nAuthor\n\n###\n\nBA8F0D39 commented Apr 17, 2023\n\nI did some further tests and it seems like allocating more than 4GB returns\ngarbage or randomly crashes.Example of allocating less than 4GB in A770 16GB.\nThe mean is around 0.5 which is expected.\n\n    \n    \n    import torch import torchvision.models as models import numpy as np import intel_extension_for_pytorch as ipex torch.manual_seed(0) x = torch.rand(30000, 30000, dtype=torch.float32, device='xpu') print(\"Mean\") print(torch.mean(x).detach().cpu().numpy()) python3 ./test.py Failed to load image Python extension: warn(f\"Failed to load image Python extension: {e}\") Mean 0.50001085\n\nExample of allocating more than 4GB on CPU. The mean is around 0.5 which is\nexpected.\n\n    \n    \n    import torch import torchvision.models as models import numpy as np import intel_extension_for_pytorch as ipex torch.manual_seed(0) x = torch.rand(47000, 47000, dtype=torch.float32, device='cpu') print(\"Mean\") print(torch.mean(x).detach().cpu().numpy()) python3 ./test.py /usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: warn(f\"Failed to load image Python extension: {e}\") Mean 0.4999941\n\nExample of allocating more than 4GB on A770 16GB. The mean is around 0.014\nwhich is completely wrong.\n\n    \n    \n    import torch import torchvision.models as models import numpy as np import intel_extension_for_pytorch as ipex torch.manual_seed(0) x = torch.rand(47000, 47000, dtype=torch.float32, device='xpu') print(\"Mean\") print(torch.mean(x).detach().cpu().numpy()) python3 ./test.py /usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: warn(f\"Failed to load image Python extension: {e}\") Mean 0.014004011\n\nIn conclusion, allocating more than 4GB crashes or returns complete garbage.  \n---  \n  \nkwaa mentioned this issue Apr 24, 2023\n\nConsistently getting noise as output with Intel Arc comfyanonymous/ComfyUI#556\n\nOpen\n\nAuthor\n\n###\n\nBA8F0D39 commented Apr 24, 2023\n\n@jingxu10 Is memory allocation done by OpenCL, Level Zero, or OneDNN?  \n---  \n  \nContributor\n\n###\n\njingxu10 commented Apr 24, 2023\n\nIt should be allocated by Level-0. @gujinghui  \n---  \n  \nAuthor\n\n###\n\nBA8F0D39 commented Apr 25, 2023\n\n@jingxu10Will passing -ze-opt-greater-than-4GB-buffer-required into the build\noptions fix it?https://spec.oneapi.io/level-zero/latest/core/PROG.html#module-\nbuild-options  \n---  \n  \nvpirogov mentioned this issue Apr 25, 2023\n\nAllow arrays larger than 4GB on GPUs oneapi-src/oneDNN#1638\n\nClosed\n\n###\n\ncchheennhhaaoo commented Apr 27, 2023 \u2022\n\nHi, @BA8F0D39 What's the driver version? I cannot reproduce randomly crash\nwith agama-ci-devel-602. From what I've tried, the max workable input shape of\nyour ut is about 59500*59500, corresponds memory size of 13.2G. It is a\nreasonable result. For accuracy issue, we will check it.  \n---  \n  \nContributor\n\n###\n\nzejun-chen commented Apr 27, 2023\n\nHi @BA8F0D39Thank you for using intel product and IPEX. Now we can successfully create large memory(not larger than total physical memory size) and compute well. Can you provide the driver version you are using by the below? sudo dpkg -l | grep intelAnd is it possible to add the following flags and attach the log here when you find the error?\n    \n    \n    export SYCL_PI_TRACE=-1 export ZE_DEBUG=-1\n\nThank you.  \n---  \n  \nAuthor\n\n###\n\nBA8F0D39 commented Apr 27, 2023 \u2022\n\n@cchheennhhaaooOn windows 11 WSL\n\n    \n    \n    ii intel-level-zero-gpu 1.3.24595.35+i538~22.04 amd64 Intel(R) Graphics Compute Runtime for oneAPI Level Zero. ii intel-oneapi-runtime-ccl 2021.8.0-25371 amd64 Intel\u00ae oneAPI Collective Communications Library runtime ii intel-oneapi-runtime-compilers 2023.0.0-25370 amd64 Intel\u00ae oneAPI DPC++/C++ Compiler & Intel\u00ae C++ Compiler Classic runtime common files ii intel-oneapi-runtime-compilers-common 2023.0.0-25370 all Intel\u00ae oneAPI DPC++/C++ Compiler & Intel\u00ae C++ Compiler Classic runtime common files ii intel-oneapi-runtime-dpcpp-cpp 2023.0.0-25370 amd64 Intel\u00ae oneAPI DPC++/C++ Compiler & Intel\u00ae C++ Compiler Classic runtime ii intel-oneapi-runtime-dpcpp-cpp-common 2023.0.0-25370 all Intel\u00ae oneAPI DPC++/C++ Compiler & Intel\u00ae C++ Compiler Classic runtime ii intel-oneapi-runtime-mkl 2023.0.0-25398 amd64 Intel\u00ae oneAPI Math Kernel Library runtime ii intel-oneapi-runtime-mkl-common 2023.0.0-25398 all Intel\u00ae oneAPI Math Kernel Library runtime common ii intel-oneapi-runtime-mpi 2021.8.0-25329 amd64 Intel\u00ae MPI Library runtime ii intel-oneapi-runtime-opencl 2023.0.0-25370 amd64 Intel\u00ae CPU Runtime for OpenCL(TM) Applications runtime ii intel-oneapi-runtime-openmp 2023.0.0-25370 amd64 Intel\u00ae OpenMP* Runtime Library runtime ii intel-oneapi-runtime-openmp-common 2023.0.0-25370 all l_openmp.runtime.description> ii intel-oneapi-runtime-tbb 2021.8.0-25334 amd64 Intel\u00ae oneAPI Threading Building Blocks runtime ii intel-oneapi-runtime-tbb-common 2021.8.0-25334 all Intel\u00ae oneAPI Threading Building Blocks runtime common ii intel-opencl-icd 22.43.24595.35+i538~22.04 amd64 Intel graphics compute runtime for OpenCL\n\nCode\n\n    \n    \n    import torch import torchvision.models as models import numpy as np import intel_extension_for_pytorch as ipex torch.manual_seed(0) x = torch.rand(47000, 47000, dtype=torch.float32, device='xpu') print(\"Mean\") print(torch.mean(x).detach().cpu().numpy())\n    \n    \n    ZE ---> zeContextDestroy(DestoryZeContext) ZE ---> zeKernelDestroy(Kernel->ZeKernel) PI ---> piProgramRelease(KernelProgram) ZE ---> zeModuleBuildLogDestroy(ZeBuildLog) ZE ---> zeModuleDestroy(ZeModule) ZE_DEBUG=4: check balance of create/destroy calls ---------------------------------------------------------- zeContextCreate = 1 \\---> zeContextDestroy = 1 zeCommandQueueCreate = 1 \\---> zeCommandQueueDestroy = 1 zeModuleCreate = 1 \\---> zeModuleDestroy = 1 zeKernelCreate = 1 \\---> zeKernelDestroy = 1 zeEventPoolCreate = 1 \\---> zeEventPoolDestroy = 1 zeCommandListCreateImmediate = 1 | zeCommandListCreate = 2 \\---> zeCommandListDestroy = 3 zeEventCreate = 8 \\---> zeEventDestroy = 8 zeFenceCreate = 2 \\---> zeFenceDestroy = 2 zeImageCreate = 0 \\---> zeImageDestroy = 0 zeSamplerCreate = 0 \\---> zeSamplerDestroy = 0 zeMemAllocDevice = 1 | zeMemAllocHost = 0 | zeMemAllocShared = 0 \\---> zeMemFree = 0 ---> LEAK = 1 terminate called after throwing an instance of 'sycl::_V1::runtime_error' what(): Native API failed. Native API returns: -38 (PI_ERROR_INVALID_MEM_OBJECT) -38 (PI_ERROR_INVALID_MEM_OBJECT) Aborted\n\ncrashlog.txt  \n---  \n  \nAuthor\n\n###\n\nBA8F0D39 commented Apr 27, 2023 \u2022\n\n@cchheennhhaaoo @zejun-chenOn Ubuntu 22.04 Linux 6.3. It also crashes, but\nonly after I close python.\n\n    \n    \n    ii intel-level-zero-gpu 1.3.25593.18-601~22.04 amd64 Intel(R) Graphics Compute Runtime for oneAPI Level Zero. ii intel-oneapi-runtime-ccl 2021.9.0-43543 amd64 Intel\u00ae oneAPI Collective Communications Library runtime ii intel-oneapi-runtime-compilers 2023.1.0-46305 amd64 Intel\u00ae oneAPI DPC++/C++ Compiler & Intel\u00ae C++ Compiler Classic runtime common files ii intel-oneapi-runtime-compilers-common 2023.1.0-46305 all Intel\u00ae oneAPI DPC++/C++ Compiler & Intel\u00ae C++ Compiler Classic runtime common files ii intel-oneapi-runtime-dpcpp-cpp 2023.1.0-46305 amd64 Intel\u00ae oneAPI DPC++/C++ Compiler & Intel\u00ae C++ Compiler Classic runtime ii intel-oneapi-runtime-dpcpp-cpp-common 2023.1.0-46305 all Intel\u00ae oneAPI DPC++/C++ Compiler & Intel\u00ae C++ Compiler Classic runtime ii intel-oneapi-runtime-mkl 2023.1.0-46342 amd64 Intel\u00ae oneAPI Math Kernel Library runtime ii intel-oneapi-runtime-mkl-common 2023.1.0-46342 all Intel\u00ae oneAPI Math Kernel Library runtime common ii intel-oneapi-runtime-mpi 2021.9.0-43482 amd64 Intel\u00ae MPI Library runtime ii intel-oneapi-runtime-opencl 2023.1.0-46305 amd64 Intel\u00ae CPU Runtime for OpenCL(TM) Applications runtime ii intel-oneapi-runtime-openmp 2023.1.0-46305 amd64 Intel\u00ae OpenMP* Runtime Library runtime ii intel-oneapi-runtime-openmp-common 2023.1.0-46305 all l_openmp.runtime.description> ii intel-oneapi-runtime-tbb 2021.9.0-43484 amd64 Intel\u00ae oneAPI Threading Building Blocks runtime ii intel-oneapi-runtime-tbb-common 2021.9.0-43484 all Intel\u00ae oneAPI Threading Building Blocks runtime common ii intel-opencl-icd 23.05.25593.18-601~22.04 amd64 Intel graphics compute runtime for OpenCL ii libdrm-intel1:amd64 2.4.115+git2303241447.28d9a3c4~j~mesarc0 amd64 Userspace interface to intel-specific kernel DRM services -- runtime\n\nCode\n\n    \n    \n    import torch import torchvision.models as models import numpy as np import intel_extension_for_pytorch as ipex torch.manual_seed(0) x = torch.rand(47000, 47000, dtype=torch.float32, device='xpu') print(\"Mean\") print(torch.mean(x).detach().cpu().numpy())\n\nCrash\n\n    \n    \n    ZE ---> zeEventDestroy(Event->ZeEvent) ZE ---> zeEventDestroy(Event->ZeEvent) ZE ---> zeEventDestroy(Event->ZeEvent) ZE ---> zeEventDestroy(Event->ZeEvent) ZE ---> zeEventDestroy(Event->ZeEvent) ZE ---> zeEventPoolDestroy(ZePool) ZE ---> zeCommandListDestroy(ZeCommandListInit) ZE ---> zeCommandListDestroy(ZeCommandList) ZE ---> zeCommandListDestroy(ZeCommandList) ZE ---> zeCommandListDestroy(ZeCommandList) ZE ---> zeCommandListDestroy(ZeCommandList) ZE ---> zeCommandListDestroy(ZeCommandList) ZE ---> zeMemFree(Context->ZeContext, Ptr) ZE ---> zeContextDestroy(DestoryZeContext) ZE ---> zeKernelDestroy(Kernel->ZeKernel) PI ---> piProgramRelease(KernelProgram) ZE ---> zeModuleBuildLogDestroy(ZeBuildLog) ZE ---> zeModuleDestroy(ZeModule) ZE ---> zeKernelDestroy(Kernel->ZeKernel) PI ---> piProgramRelease(KernelProgram) ZE ---> zeKernelDestroy(Kernel->ZeKernel) PI ---> piProgramRelease(KernelProgram) ZE ---> zeModuleBuildLogDestroy(ZeBuildLog) ZE ---> zeModuleDestroy(ZeModule) ZE_DEBUG=4: check balance of create/destroy calls ---------------------------------------------------------- zeContextCreate = 1 \\---> zeContextDestroy = 1 zeCommandQueueCreate = 2 \\---> zeCommandQueueDestroy = 2 zeModuleCreate = 2 \\---> zeModuleDestroy = 2 zeKernelCreate = 3 \\---> zeKernelDestroy = 3 zeEventPoolCreate = 1 \\---> zeEventPoolDestroy = 1 zeCommandListCreateImmediate = 1 | zeCommandListCreate = 5 \\---> zeCommandListDestroy = 6 zeEventCreate = 18 \\---> zeEventDestroy = 18 zeFenceCreate = 5 \\---> zeFenceDestroy = 5 zeImageCreate = 0 \\---> zeImageDestroy = 0 zeSamplerCreate = 0 \\---> zeSamplerDestroy = 0 zeMemAllocDevice = 2 | zeMemAllocHost = 0 | zeMemAllocShared = 0 \\---> zeMemFree = 1 ---> LEAK = 1 terminate called after throwing an instance of 'sycl::_V1::runtime_error' what(): Native API failed. Native API returns: -38 (PI_ERROR_INVALID_MEM_OBJECT) -38 (PI_ERROR_INVALID_MEM_OBJECT) Aborted (core dumped)\n\ncrash2.txt  \n---  \n  \n###\n\ncchheennhhaaoo commented Apr 27, 2023\n\nI believe this issue is caused by incorrect env setting. You can follow this\nblog to setup IPEX environment on WSL2 with docker: https://medium.com/intel-\nanalytics-software/stable-diffusion-with-intel-arc-gpus-f2986bba8365  \n---  \n  \nAuthor\n\n###\n\nBA8F0D39 commented Apr 27, 2023 \u2022\n\n@cchheennhhaaoo @zejun-chen I have the same problem on Ubuntu Linux too (not\nusing windows)On Ubuntu 22.04 Linux 6.3. It also crashes, but only after I\nclose python.\n\n    \n    \n    ii intel-level-zero-gpu 1.3.25593.18-601~22.04 amd64 Intel(R) Graphics Compute Runtime for oneAPI Level Zero. ii intel-oneapi-runtime-ccl 2021.9.0-43543 amd64 Intel\u00ae oneAPI Collective Communications Library runtime ii intel-oneapi-runtime-compilers 2023.1.0-46305 amd64 Intel\u00ae oneAPI DPC++/C++ Compiler & Intel\u00ae C++ Compiler Classic runtime common files ii intel-oneapi-runtime-compilers-common 2023.1.0-46305 all Intel\u00ae oneAPI DPC++/C++ Compiler & Intel\u00ae C++ Compiler Classic runtime common files ii intel-oneapi-runtime-dpcpp-cpp 2023.1.0-46305 amd64 Intel\u00ae oneAPI DPC++/C++ Compiler & Intel\u00ae C++ Compiler Classic runtime ii intel-oneapi-runtime-dpcpp-cpp-common 2023.1.0-46305 all Intel\u00ae oneAPI DPC++/C++ Compiler & Intel\u00ae C++ Compiler Classic runtime ii intel-oneapi-runtime-mkl 2023.1.0-46342 amd64 Intel\u00ae oneAPI Math Kernel Library runtime ii intel-oneapi-runtime-mkl-common 2023.1.0-46342 all Intel\u00ae oneAPI Math Kernel Library runtime common ii intel-oneapi-runtime-mpi 2021.9.0-43482 amd64 Intel\u00ae MPI Library runtime ii intel-oneapi-runtime-opencl 2023.1.0-46305 amd64 Intel\u00ae CPU Runtime for OpenCL(TM) Applications runtime ii intel-oneapi-runtime-openmp 2023.1.0-46305 amd64 Intel\u00ae OpenMP* Runtime Library runtime ii intel-oneapi-runtime-openmp-common 2023.1.0-46305 all l_openmp.runtime.description> ii intel-oneapi-runtime-tbb 2021.9.0-43484 amd64 Intel\u00ae oneAPI Threading Building Blocks runtime ii intel-oneapi-runtime-tbb-common 2021.9.0-43484 all Intel\u00ae oneAPI Threading Building Blocks runtime common ii intel-opencl-icd 23.05.25593.18-601~22.04 amd64 Intel graphics compute runtime for OpenCL ii libdrm-intel1:amd64 2.4.115+git2303241447.28d9a3c4~j~mesarc0 amd64 Userspace interface to intel-specific kernel DRM services -- runtime\n\nCode\n\n    \n    \n    import torch import torchvision.models as models import numpy as np import intel_extension_for_pytorch as ipex torch.manual_seed(0) x = torch.rand(47000, 47000, dtype=torch.float32, device='xpu') print(\"Mean\") print(torch.mean(x).detach().cpu().numpy())\n\nCrash\n\n    \n    \n    ZE ---> zeEventDestroy(Event->ZeEvent) ZE ---> zeEventDestroy(Event->ZeEvent) ZE ---> zeEventDestroy(Event->ZeEvent) ZE ---> zeEventDestroy(Event->ZeEvent) ZE ---> zeEventDestroy(Event->ZeEvent) ZE ---> zeEventPoolDestroy(ZePool) ZE ---> zeCommandListDestroy(ZeCommandListInit) ZE ---> zeCommandListDestroy(ZeCommandList) ZE ---> zeCommandListDestroy(ZeCommandList) ZE ---> zeCommandListDestroy(ZeCommandList) ZE ---> zeCommandListDestroy(ZeCommandList) ZE ---> zeCommandListDestroy(ZeCommandList) ZE ---> zeMemFree(Context->ZeContext, Ptr) ZE ---> zeContextDestroy(DestoryZeContext) ZE ---> zeKernelDestroy(Kernel->ZeKernel) PI ---> piProgramRelease(KernelProgram) ZE ---> zeModuleBuildLogDestroy(ZeBuildLog) ZE ---> zeModuleDestroy(ZeModule) ZE ---> zeKernelDestroy(Kernel->ZeKernel) PI ---> piProgramRelease(KernelProgram) ZE ---> zeKernelDestroy(Kernel->ZeKernel) PI ---> piProgramRelease(KernelProgram) ZE ---> zeModuleBuildLogDestroy(ZeBuildLog) ZE ---> zeModuleDestroy(ZeModule) ZE_DEBUG=4: check balance of create/destroy calls ---------------------------------------------------------- zeContextCreate = 1 \\---> zeContextDestroy = 1 zeCommandQueueCreate = 2 \\---> zeCommandQueueDestroy = 2 zeModuleCreate = 2 \\---> zeModuleDestroy = 2 zeKernelCreate = 3 \\---> zeKernelDestroy = 3 zeEventPoolCreate = 1 \\---> zeEventPoolDestroy = 1 zeCommandListCreateImmediate = 1 | zeCommandListCreate = 5 \\---> zeCommandListDestroy = 6 zeEventCreate = 18 \\---> zeEventDestroy = 18 zeFenceCreate = 5 \\---> zeFenceDestroy = 5 zeImageCreate = 0 \\---> zeImageDestroy = 0 zeSamplerCreate = 0 \\---> zeSamplerDestroy = 0 zeMemAllocDevice = 2 | zeMemAllocHost = 0 | zeMemAllocShared = 0 \\---> zeMemFree = 1 ---> LEAK = 1 terminate called after throwing an instance of 'sycl::_V1::runtime_error' what(): Native API failed. Native API returns: -38 (PI_ERROR_INVALID_MEM_OBJECT) -38 (PI_ERROR_INVALID_MEM_OBJECT) Aborted (core dumped)\n\ncrash2.txt  \n---  \n  \n###\n\nfredlarochelle commented May 12, 2023\n\nI am able to replicate the same issue on Fedora 37 with 6.2 and Ubuntu 22.04\nwith 5.19. Both instances involve a build from the latest xpu-master branch.  \n---  \n  \nAuthor\n\n###\n\nBA8F0D39 commented May 13, 2023\n\nIt is weird the crash error is only reported when you enable DEBUG flags,\notherwise the code silently crashes.\n\n    \n    \n    export SYCL_PI_TRACE=-1 export ZE_DEBUG=-1  \n  \n---  \n  \nfredlarochelle mentioned this issue May 25, 2023\n\nLow Mixed Precision Performance #296\n\nOpen\n\n###\n\nfredlarochelle commented May 25, 2023\n\nHere is some quick findings I had, it's not exactly at 4GB, I don't think the\ngibberish is related...\n\n    \n    \n    # All good import torch import intel_extension_for_pytorch as ipex array = torch.rand(40000, 40000, dtype=torch.bfloat16, device='xpu') print(f\"The memory of the array is {(array.element_size() * array.nelement()) / 1e9}GB.\") #3.2GB print(\"Mean:\", torch.mean(array).item()) #0.5 print(\"Standard Deviation:\", torch.std(array).item()) #0.287109375\n    \n    \n    # All good import torch import intel_extension_for_pytorch as ipex array = torch.rand(46000, 46000, dtype=torch.bfloat16, device='xpu') print(f\"The memory of the array is {(array.element_size() * array.nelement()) / 1e9}GB.\") #4.232GB print(\"Mean:\", torch.mean(array).item()) #0.5 print(\"Standard Deviation:\", torch.std(array).item()) #0.2890625\n    \n    \n    # At 46001x46001 it goes gibberish import torch import intel_extension_for_pytorch as ipex array = torch.rand(46001, 46001, dtype=torch.bfloat16, device='xpu') print(f\"The memory of the array is {(array.element_size() * array.nelement()) / 1e9}GB.\") #4.423218400GB print(\"Mean:\", torch.mean(array).item()) #0.00372314453125 print(\"Standard Deviation:\", torch.std(array).item()) #0.049072265625\n\nFor FP16, I have some other weird bugs that sometimes it works, sometimes it\ndoesn't even for small array (less than 10000x10000). Even for multiple\nconsecutive run, it might work for 50 times in a row, than go bonkers for\n10.For FP32, the gibberish starts appearing at around 30800x30800 which is\n3.79456GB. Before that starting around 30400x30400, it is gibberish and then a\ngood output in alternance when doing multiple succesive runs.Which such\nnumerical instability, I might write a script and test every possible\ncombination at this point, might be worth to take a look at other random\nsampling methods too.  \n---  \n  \n###\n\nfredlarochelle commented May 25, 2023\n\nJust did another quick run for FP32 at 30800x30800 and this time, it works\njust fine (even 32000x32000 works this time around), there is some weird\ninstability going on...Quick thought, since I am not using a fixed seed in\nthose tests, might it be that some \"bad seeds\" are cause the instability?  \n---  \n  \nAuthor\n\n###\n\nBA8F0D39 commented May 25, 2023 \u2022\n\n@fredlarochelle I think some pointers in OneDNN GPU kernel use 32bit unsigned\nintegers and some use 64bit unsigned integers. Reading more than 4GB creates a\nbuffer over-read (reading adjacent memory locations and reading other\narrays).If the adjacent memory locations just so happens to have zeros, then\nthe mean is around 0.If the adjacent memory locations just so happens to have\nuniformly distributed values from 0 to 1, then the mean is 0.5 .It could allow\nyou to read other program's data in the GPU.  \n---  \n  \n###\n\nfredlarochelle commented May 25, 2023\n\n@BA8F0D39 That would make sense, but I still do get the instability for FP16\nand FP32 start acting weird before it before it would actually overfill a\n32bit buffer + instability, there is probably more than one problem going on\nat the same time.  \n---  \n  \n###\n\nfengyuan14 commented May 26, 2023\n\n>\n>     0.2890625\n\n@fredlarochelle @BA8F0D39 Thanks for feedbacks.The issue mentioned here (so-\ncalled numerical instability) looks like one we met recently in internal test.\nThe issue might be caused cache consistency after global memory fence. We are\nfollowing.BTW, as for crashes when allocating memory larger than 4GB, we\ncannot reproduce on recommended driver.  \n---  \n  \nAuthor\n\n###\n\nBA8F0D39 commented May 26, 2023\n\n@arthuryuan1987 On Windows 11 with WSL, it crashes 100% of the time.On Ubuntu\nLinux 22.04 with 5.19 out of tree driver (intel-i915-dkms intel-platform-vsec-\ndkms intel-platform-cse-dkms intel-fw-gpu), it randomly crashes and it is not\ndeterministic. https://dgpu-docs.intel.com/driver/client/overview.htmlOn\nUbuntu Linux 22.04 with 6.3 mainline kernel, it also randomly crashes.I can\nforce it to crash 100% of the time if you enable debug flags.\n\n    \n    \n    export SYCL_PI_TRACE=-1 export ZE_DEBUG=-1  \n  \n---  \n  \n###\n\nfredlarochelle commented May 27, 2023\n\n@arthuryuan1987 I am on Ubuntu 22.04.2 5.19.0.41-generic, on the lastest\ndriver, all following the installation instructions in the documentation with\na build from the lastest commit in the xpu-master branch.  \n---  \n  \nAuthor\n\n###\n\nBA8F0D39 commented Jun 9, 2023\n\n@arthuryuan1987I used a Vulkan GPU memory tester.\nhttps://github.com/GpuZelenograd/memtest_vulkanIt seems all memory regions\nabove 4GB are corrupt and the read transfer speed is 1.9 GB/s.\n\n    \n    \n    ./memtest_vulkan 1 9140000000 Error found. Mode NEXT_RE_READ, total errors 0x20000000 out of 0x2C000000 (72.72727273%) Errors address range: 0x30000000..=0xAFFFFFFF iteration:1 values range: 0x00000000..=0x00000000 FFFFFFFF-like count:0 bit-level stats table: 0x0 0x1 0x2 0x3| 0x4 0x5 0x6 0x7| 0x8 0x9 0xA 0xB| 0xC 0xD 0xE 0xF SinglIdx | 1 | | 1 TogglCnt 2 56 761|6673 42k 205k793k| 2m 6m 14m 27m| 45m 63m 76m 81m 0x1? 74m 58m 40m 24m| 12m 5m 1m589k|145k 28k 4277 457| 31 1 1sInValu536m | | | Error found. Mode INITIAL_READ, total errors 0x20000000 out of 0x2C000000 (72.72727273%) Errors address range: 0xE0000000..=0x15FFFFFFF iteration:1 values range: 0x00000000..=0x00000000 FFFFFFFF-like count:0 bit-level stats table: 0x0 0x1 0x2 0x3| 0x4 0x5 0x6 0x7| 0x8 0x9 0xA 0xB| 0xC 0xD 0xE 0xF SinglIdx | 1 | | 1 TogglCnt 2 56 761|6673 42k 205k793k| 2m 6m 14m 27m| 45m 63m 76m 81m 0x1? 74m 58m 40m 24m| 12m 5m 1m589k|145k 28k 4277 457| 31 1 1sInValu536m | | | Error found. Mode INITIAL_READ, total errors 0x20000000 out of 0x2C000000 (72.72727273%) Errors address range: 0x190000000..=0x20FFFFFFF iteration:1 values range: 0x00000000..=0x00000000 FFFFFFFF-like count:0 bit-level stats table: 0x0 0x1 0x2 0x3| 0x4 0x5 0x6 0x7| 0x8 0x9 0xA 0xB| 0xC 0xD 0xE 0xF SinglIdx | 1 | | 1 TogglCnt 2 56 761|6672 42k 205k793k| 2m 6m 14m 27m| 45m 63m 76m 81m 0x1? 74m 58m 40m 24m| 12m 5m 1m589k|145k 28k 4277 457| 31 1 1sInValu536m | | | Standard 5-minute test of 1: Bus=0x03:00 DevId=0x56A0 16GB Intel(R) Arc(tm) A770 Graphics (DG2) 1 iteration. Passed 5.6310 seconds written: 5.5GB 956.2GB/sec checked: 8.2GB 1.5GB/sec Error found. Mode NEXT_RE_READ, total errors 0x20000000 out of 0x2C000000 (72.72727273%) Errors address range: 0x30000000..=0xAFFFFFFF iteration:1 values range: 0x00000000..=0x00000000 FFFFFFFF-like count:0 bit-level stats table: 0x0 0x1 0x2 0x3| 0x4 0x5 0x6 0x7| 0x8 0x9 0xA 0xB| 0xC 0xD 0xE 0xF SinglIdx | 1 | | 1 TogglCnt 2 56 761|6673 42k 205k793k| 2m 6m 14m 27m| 45m 63m 76m 81m 0x1? 74m 58m 40m 24m| 12m 5m 1m589k|145k 28k 4277 457| 31 1 1sInValu536m | | | Error found. Mode INITIAL_READ, total errors 0x20000000 out of 0x2C000000 (72.72727273%) Errors address range: 0xE0000000..=0x15FFFFFFF iteration:2 values range: 0x00000000..=0x00000000 FFFFFFFF-like count:0 bit-level stats table: 0x0 0x1 0x2 0x3| 0x4 0x5 0x6 0x7| 0x8 0x9 0xA 0xB| 0xC 0xD 0xE 0xF SinglIdx | 1 | | 1 TogglCnt 2 56 760|6653 42k 204k789k| 2m 6m 14m 27m| 45m 63m 76m 81m 0x1? 74m 58m 40m 24m| 12m 5m 1m589k|145k 28k 4277 457| 31 1 1sInValu536m | | |  \n  \n---  \n  \n###\n\nfengyuan14 commented Jun 12, 2023\n\n@BA8F0D39 I checked the repo, https://github.com/GpuZelenograd/memtest_vulkan\nIt should be OpenCL based application (tool). As I know, A64 stateless\naddressing has a big performance penalty on ARC. Maybe, I guess OpenCL driver\ndisables >4GB allocation. Regarding stacks of IPEX, not all underlying stacks\nguarantee A64 stateless addressing. So after next code synchronization, IPEX\nwill raise an explicit error to users, as well.  \n---  \n  \n###\n\nfredlarochelle commented Jun 24, 2023\n\nCould you please provide an update on the status of this issue? On the lastest\nxpu_master branch, I have observed that it is currently exhibiting\nintermittent behavior. At times, when allocating a batch size larger than 4\nGB, it crashes with the -5 error, while other times it functions correctly\nwithout any issues. Or might the -5 error I am getting be related to another\nissue? Interestingly, from my observations, the error does not seem to occur\nwhen the batch size remains under 4 GB.  \n---  \n  \nAuthor\n\n###\n\nBA8F0D39 commented Jun 25, 2023\n\nI am using Xorg on Ubuntu 22.04 . On the xpu_master branch, allocating more\nthan 4GB on pytorch crashes Xorg for some reason. It seems to be overwriting\nXorg's memory regions in the GPU  \n---  \n  \n###\n\ncchheennhhaaoo commented Jun 25, 2023\n\n@fredlarochelle @BA8F0D39 After next code synchronization, memory allocation\ngreater than 4G will be disabled on Arc and an error message will be raised\nwhen user requests it.  \n---  \n  \n###\n\nfredlarochelle commented Jun 29, 2023\n\n@cchheennhhaaoo That is not a fix at all tho...  \n---  \n  \nAuthor\n\n###\n\nBA8F0D39 commented Aug 23, 2023 \u2022\n\n@cchheennhhaaoo I still can allocate more than 4GB on Intel Arc with IPEX\n2.0.110+xpu. However, inputting large images into resnet50 produces invalid\nresults even-though only 8GB of 16GB Intel Arc A770 is used.  \n---  \n  \nsimonlui mentioned this issue Aug 23, 2023\n\nAllow for stateless addressing flags for >4GB allocations for devices to be\npassed through SYCL intel/llvm#10946\n\nOpen\n\nBA8F0D39 mentioned this issue Sep 4, 2023\n\nMemory management #421\n\nOpen\n\n###\n\nSerizao commented Sep 5, 2023\n\nI have exactly the same bug : during torch finetuning, my script crash with\nPI_ERROR_INVALID_MEM_OBJECT full stack:\n\n    \n    \n    ---> piContextRelease( <unknown> : 0x5691520 ZE ---> zeEventDestroy(Event->ZeEvent) ZE ---> zeEventDestroy(Event->ZeEvent) ZE ---> zeEventDestroy(Event->ZeEvent) ZE ---> zeEventDestroy(Event->ZeEvent) ZE ---> zeEventDestroy(Event->ZeEvent) ZE ---> zeEventDestroy(Event->ZeEvent) ZE ---> zeEventDestroy(Event->ZeEvent) ZE ---> zeEventPoolDestroy(ZePool) ZE ---> zeCommandListDestroy(ZeCommandListInit) ZE ---> zeCommandListDestroy(ZeCommandList) ZE ---> zeCommandListDestroy(ZeCommandList) ZE ---> zeCommandListDestroy(ZeCommandList) ZE ---> zeMemFree(Context->ZeContext, Ptr) ZE ---> zeContextDestroy(DestoryZeContext) ) ---> pi_result : PI_SUCCESS ---> piKernelRelease( <unknown> : 0xf4d7d10 ZE ---> zeKernelDestroy(Kernel->ZeKernel) PI ---> piProgramRelease(KernelProgram) ) ---> pi_result : PI_SUCCESS ---> piProgramRelease( <unknown> : 0xf4c3e20 ZE ---> zeModuleBuildLogDestroy(ZeBuildLog) ZE ---> zeModuleDestroy(ZeModule) ) ---> pi_result : PI_SUCCESS ---> piKernelRelease( <unknown> : 0x1053fe50 ZE ---> zeKernelDestroy(Kernel->ZeKernel) PI ---> piProgramRelease(KernelProgram) ) ---> pi_result : PI_SUCCESS ---> piKernelRelease( <unknown> : 0x1053f3e0 ZE ---> zeKernelDestroy(Kernel->ZeKernel) PI ---> piProgramRelease(KernelProgram) ) ---> pi_result : PI_SUCCESS ---> piProgramRelease( <unknown> : 0xf4e63a0 ZE ---> zeModuleBuildLogDestroy(ZeBuildLog) ZE ---> zeModuleDestroy(ZeModule) ) ---> pi_result : PI_SUCCESS ---> piKernelRelease( <unknown> : 0xf4e73c0 ZE ---> zeKernelDestroy(Kernel->ZeKernel) PI ---> piProgramRelease(KernelProgram) ) ---> pi_result : PI_SUCCESS ---> piKernelRelease( <unknown> : 0xf4e56d0 ZE ---> zeKernelDestroy(Kernel->ZeKernel) PI ---> piProgramRelease(KernelProgram) ) ---> pi_result : PI_SUCCESS ---> piProgramRelease( <unknown> : 0xf4c3f50 ZE ---> zeModuleBuildLogDestroy(ZeBuildLog) ZE ---> zeModuleDestroy(ZeModule) ) ---> pi_result : PI_SUCCESS ---> piDeviceRelease( <unknown> : 0xf474a70 ) ---> pi_result : PI_SUCCESS ---> piDeviceRelease( <unknown> : 0xf475160 ) ---> pi_result : PI_SUCCESS ---> piTearDown( <unknown> : 0 ) ---> pi_result : PI_SUCCESS [out]void * : 0 ---> piTearDown( <unknown> : 0 ZE_DEBUG=4: check balance of create/destroy calls ---------------------------------------------------------- zeContextCreate = 1 \\---> zeContextDestroy = 1 zeCommandQueueCreate = 2 \\---> zeCommandQueueDestroy = 2 zeModuleCreate = 3 \\---> zeModuleDestroy = 3 zeKernelCreate = 5 \\---> zeKernelDestroy = 5 zeEventPoolCreate = 1 \\---> zeEventPoolDestroy = 1 zeCommandListCreateImmediate = 1 | zeCommandListCreate = 3 \\---> zeCommandListDestroy = 4 zeEventCreate = 7 \\---> zeEventDestroy = 7 zeFenceCreate = 3 \\---> zeFenceDestroy = 3 zeImageCreate = 0 \\---> zeImageDestroy = 0 zeSamplerCreate = 0 \\---> zeSamplerDestroy = 0 zeMemAllocDevice = 2 | zeMemAllocHost = 0 | zeMemAllocShared = 0 \\---> zeMemFree = 1 ---> LEAK = 1 ) ---> pi_result : -38 [out]void * : 0 terminate called after throwing an instance of 'sycl::_V1::runtime_error' what(): Native API failed. Native API returns: -38 (PI_ERROR_INVALID_MEM_OBJECT) -38 (PI_ERROR_INVALID_MEM_OBJECT) Abandon (core dumped)\n\ni have an Arc A 770 with 16Gb of memory. To do this i i last transformer\nversion wich integrate XPU to compute. Is a fix to use all available memory\nplanned?  \n---  \n  \n###\n\ncchheennhhaaoo commented Sep 7, 2023\n\n> @cchheennhhaaoo I still can allocate more than 4GB on Intel Arc with IPEX\n> 2.0.110+xpu. However, inputting large images into resnet50 produces invalid\n> results even-though only 8GB of 16GB Intel Arc A770 is used.\n\nPlease check this line in your repo. https://github.com/intel/intel-extension-\nfor-pytorch/blob/xpu-\nmaster/csrc/gpu/runtime/CachingDeviceAllocator.cpp#L190For invalid result\nissue, please refer to above arthuryuan1987's comment.  \n---  \n  \n###\n\ntye1 commented Sep 19, 2023\n\n@BA8F0D39 @fredlarochelle we don't plan to support this. You can still\nallocate > 4GB with 2.0.110+xpu because we disabled the allocation in master\nnot the previous released drop. Could you please provide the justification why\n>4GB allocation is required?  \n---  \n  \nAuthor\n\n###\n\nBA8F0D39 commented Sep 20, 2023\n\n@tye1Using an image larger than 768x512 in stable diffusion 1.5 results in a\nblank or garbled image when pytorch doesn't even use all of the 16 GB in A770\ncomfyanonymous/ComfyUI#556Every LLM is bigger than 4GB and they all fail to\nload on the A770 even though they can fit the VRAMOther huge models and\ndatasets bigger than 4GB runs out of memory. #421  \n---  \n  \n###\n\nfredlarochelle commented Sep 21, 2023 \u2022\n\n@tye1 Pretty much what @BA8F0D39 said and that you need to use work arounds\nthat you don't need to use with a Nvidia GPUs. For example, using a smaller\nbatch size and loading multiple separate batch on the GPU, ...The main problem\nI would say tho is a lot of Pytorch code you can find around the internet\nsimply assume that you can allocate more than 4GB since it's supported on\nNvidia GPUs.  \n---  \n  \nBA8F0D39 mentioned this issue Dec 24, 2023\n\nAllocation of greater than 4GB not possible #492\n\nClosed\n\nAuthor\n\n###\n\nBA8F0D39 commented Dec 24, 2023\n\n@tye1 All modern transformer/GAN models are larger than 4GB and they all fail\nwith IPEX #492  \n---  \n  \n###\n\nElliottDyson commented Dec 24, 2023\n\n@tye1 This is also an issue I have. For highly complex models and long\nsequence lengths, even a batch size of 1 has the possibility of being larger\nthan 4GB. Such limits should be determined by the VRAM capacity of the GPU,\nrather than in software I would have thought.  \n---  \n  \n###\n\nghost commented Dec 26, 2023\n\nAre there any updates on this or is the stance still \"we don't plan to support\nthis\"? Asking, since if it's the latter, I'd be looking to sell my gpu sooner\nrather than later.  \n---  \n  \n###\n\nElliottDyson commented Dec 27, 2023 \u2022\n\n> Are there any updates on this or is the stance still \"we don't plan to\n> support this\"? Asking, since if it's the latter, I'd be looking to sell my\n> gpu sooner rather than later.\n\nSame here. Machine learning is the only reason I paid extra for a 16gb card.  \n---  \n  \n###\n\ntye1 commented Jan 4, 2024 \u2022\n\nSorry for the late response. We disable >4GB memory allocation on ARC770 as\nthere are some hardware limitations on ARC, and there will be significant\nperformance drop as penalty to trade off. This is not acceptable in IPEX\u2019s\nusage scenarios, hence we have disabled it.  \n---  \n  \n###\n\nElliottDyson commented Jan 4, 2024 \u2022\n\n> Sorry for the late response. We disable >4GB memory allocation on ARC770 as\n> there are some hardware limitations on ARC, and there will be significant\n> performance drop as penalty to trade off. This is not acceptable in IPEX\u2019s\n> usage scenarios, hence we have disabled it.\n\nAgain, thank you for the great work on this project,But is there any\npossibility to have it enabled still but with a warning that comes up when\nexceeding 4GB of allocation and notes that performance would be significantly\nreduced? I imagine it's still better than CPU processing, which is the only\nalternative I (and I'm sure others too) have available.Again, the only reason\nI bought this 16GB card was the potential for machine learning, so only being\nable to use 4/16GB is really rather frustrating, I hope you can see where I'm\ncoming from.I also understand if there's absolutely nothing you can do, it\nwould just be really rather disappointing. If that is the case, perhaps this\nis maybe something the ARC/IPEX team could work on to make it a possibility?\nIf not directly possible in this extension that is.Thank you  \n---  \n  \nContributor\n\n###\n\njgong5 commented Jan 4, 2024\n\n> Again, the only reason I bought this 16GB card was the potential for machine\n> learning, so only being able to use 4/16GB is really rather frustrating, I\n> hope you can see where I'm coming from.\n\nI guess this would only impact the case where you have to allocate big memory\nchunks which are larger than 4GB. If your workload doesn't need such big\nchunk, you can still allocate large enough memory in total up to 16GB (maybe a\nlittle lower than that due to the need from runtime/driver)?  \n---  \n  \n###\n\nElliottDyson commented Jan 4, 2024 \u2022\n\n> > Again, the only reason I bought this 16GB card was the potential for\n> machine learning, so only being able to use 4/16GB is really rather\n> frustrating, I hope you can see where I'm coming from.\n>\n> I guess this would only impact the case where you have to allocate big\n> memory chunks which are larger than 4GB. If your workload doesn't need such\n> big chunk, you can still allocate large enough memory in total up to 16GB\n> (maybe a little lower than that due to the need from runtime/driver)?\n\nUnfortunately that's the issue, with long sequences and large model sizes when\nusing transformer encoders, my use case requires being able to move more than\n4GB in one go. Unless there is a way built into this extension that\nautomatically splits the model into chunks before loading it into memory (same\nwith samples and/or batches)?P.s. Or even a manual way to do this?  \n---  \n  \n###\n\ngo2tom42 commented Jan 16, 2024\n\n> > Are there any updates on this or is the stance still \"we don't plan to\n> support this\"? Asking, since if it's the latter, I'd be looking to sell my\n> gpu sooner rather than later.\n>\n> Same here. Machine learning is the only reason I paid extra for a 16gb card.\n\nSame here it's the ONLY reason I bought this card, first Intel product I've\nbought in 15 years, and it will be the last  \n---  \n  \n###\n\nNuullll commented Jan 16, 2024\n\nI implemented a W/A in stable-diffusion-webui for scaled_dot_product_attention\nwhich is memory intensive (so easily triggers the 4GB limitation on Arc):\nAUTOMATIC1111/stable-diffusion-webui#14353, by slicing large-batch SDPA into\nsmaller chunks.I'm wondering whether such mechanism could be implemented at\nIPEX framework level. Adding IPEX W/A in upper level applications is just not\nscalable.  \n---  \n  \n###\n\nElliottDyson commented Jan 16, 2024\n\n> I implemented a W/A in stable-diffusion-webui for\n> scaled_dot_product_attention which is memory intensive (so easily triggers\n> the 4GB limitation on Arc): AUTOMATIC1111/stable-diffusion-webui#14353, by\n> slicing large-batch SDPA into smaller chunks.I'm wondering whether such\n> mechanism could be implemented at IPEX framework level. Adding IPEX W/A in\n> upper level applications is just not scalable.\n\nWhilst a neat idea, batches can be easily sent in smaller chunks via just a\nsoftware implementation with PyTorch, so I can't see much of a need for this\nat the core level. Forgive me if I'm wrong. Something that can't be fixed in\nsoftware, only firmware/library-level is if you're already running stochastic\ngradient descent (batch size of 1) and are still exceeding the 4GB limit.  \n---  \n  \n###\n\ndbenedb commented Jan 21, 2024\n\nI'm ok with >4GB allocation to cause some slowdowns. But if this cannot be\nimplemented at all that means your GPUs are practically (not theoretically or\ntechnically) useless for Stable Diffusion and I'm going to sell my A770.  \n---  \n  \n###\n\nghchris2021 commented Jan 26, 2024\n\nI am still working my way through the Intel ARC documentation with respect to\nhow the global / process / surface / execution unit / physical / virtual etc.\netc. addressing works at the architecture level, and I have no full idea of\nhow the multiple Intel driver / compute software layers above the HW affect\nthe memory limitations but I'd like to better understand where these\nlimitations are between the HW / driver / compute SW stack.It is disappointing\nfor ARC A770-16 to have a 16GBy VRAM GPU and not be (as a programmer) able to\neasily just access as much data as desired at least anywhere in the card's\nVRAM (and also in my host side's application data RAM while programming,\nideally beyond even those limits as I exemplify below (q.v.)).It makes me\nconcerned for Battlemage, Celestial, Druid as well since apparently the\nprogrammer's model of memory access for the nvidia GPUs has been (IMO) so much\nbetter even on their consumer GPUs for several past GPU generations.I gladly\ngot the ARC A770-16 to use its 16GBy ram for GPGPU and I can see from several\nintel documents there are at least in parts of the supported architecture\ncapabilities to access 64 bit addresses, and 48 bit virtual addresses, so at\nfirst glance I don't see why there is such a limitation as this now, and I\ncertainly hope that as the Intel GPU line progresses (Battlemage, Celestial,\nDruid, ...) that the VRAM size per offered card model will increase into the\n24-64+ GBy range, that the Intel consumer motherboard platforms will evolve to\nsupport wider and 256-512GBy RAM, and in such cases it seems that it's only\nnatural to hope / expect that GPU / CPU virtual addressing can become seamless\nand extend to the system's VM size encompassing all system physical RAM and\nI/O if desired.From Intel documentation showing mostly hopeful capabilities\n(though maybe SW is turning some things into SW limitations?):\n\n    \n    \n    Graphics Virtual Memory ...Although the range of supported graphics virtual addresses varies, most GPU commands and GPU instructions use a common 64 bit definition for a graphics virtual address. Per-Process GTT with 48b VA The GPU typically operates on behalf of user-level processes (applications), each of which has it's own \"Per-Process\" virtual address space. The size of this space is 256TB (48b address width). ... Shared virtual global memory (SVM) Accessible and visible to all work items on any GPU and the host. ... NP STATE_BASE_ADDRESS Base addresses for the Instruction, General State, Surface State, and Bindless Surface State memory heaps. GPGPU kernels reference these memory areas using 32b offsets from the 64b base addresses. ... Address Models Data structures accessed by the data port are called \"surfaces\". There are four different Address Models used by the data port to access these surfaces: ... 64-bit Stateless model (A64). A64 Flat/Stateless Model This model is primarily intended for programmable shader programs.\n\nPlease see the below just for contrast in terms of what I'd consider (as a\ndeveloper) a most ideal programming model and therefore the implied\ncapabilities of the SW / HW architecture that goes below it to make it work so\nseamlessly.In contrast to the above Intel architecture, looking at this below\nexemplified case (working already on several generations of consumer NVIDIA\nGPUs), the developer is able to seamlessly access data anywhere in the VRAM of\nany of the GPUs in a system, but also CPU memory anywhere in their\napplication's CPU address space, and in fact also CPU virtual addresses that\ngreatly exceed the physical VRAM of any system GPU / CPU attached RAM, all\nseamlessly and with efficient / high performance reference to such memory from\neither CPU application software or GPGPU kernels executing on any of the\nsystem's GPUs.Here's the citations about the programmer's view of memory (as I\nunderstand it to be relevant) in competitive (i.e. both for consumer use\ngaming cards as well as enterprise ones) NVIDIA GPUs. The following citations\n/ sources are about CUDA/NVIDIA GPU's \"Unified Memory\" and \"Heterogeneous\nMemory\" models on GPUs including their consumer\nGPUs:https://developer.nvidia.com/blog/simplifying-gpu-application-\ndevelopment-with-heterogeneous-memory-\nmanagement/https://developer.nvidia.com/blog/unified-memory-cuda-\nbeginners/Here are small relevant excerpts\n\n    \n    \n    The Benefits of Unified Memory on Pascal and Later GPUs Starting with the Pascal GPU architecture, Unified Memory functionality is significantly improved with 49-bit virtual addressing and on-demand page migration. 49-bit virtual addresses are sufficient to enable GPUs to access the entire system memory plus the memory of all GPUs in the system. The Page Migration engine allows GPU threads to fault on non-resident memory accesses so the system can migrate pages on demand from anywhere in the system to the GPU\u2019s memory for efficient processing. In other words, Unified Memory transparently enables oversubscribing GPU memory, enabling out-of-core computations for any code that is using Unified Memory for allocations (e.g. cudaMallocManaged()). It \u201cjust works\u201d without any modifications to the application, whether running on one GPU or multiple GPUs. Also, Pascal and Volta GPUs support system-wide atomic memory operations. That means you can atomically operate on values anywhere in the system from multiple GPUs. This is useful in writing efficient multi-GPU cooperative algorithms. Demand paging can be particularly beneficial to applications that access data with a sparse pattern. In some applications, it\u2019s not known ahead of time which specific memory addresses a particular processor will access. Without hardware page faulting, applications can only pre-load whole arrays, or suffer the cost of high-latency off-device accesses (also known as \u201cZero Copy\u201d). But page faulting means that only the pages the kernel accesses need to be migrated.\n    \n    \n    Heterogeneous Memory Management (HMM) is a CUDA memory management feature that extends the simplicity and productivity of the [CUDA Unified Memory](https://developer.nvidia.com/blog/unified-memory-in-cuda-6/) programming model to include system allocated memory on systems with PCIe-connected NVIDIA GPUs. System allocated memory refers to memory that is ultimately allocated by the operating system; for example, through malloc, mmap, the C++ new operator (which of course uses the preceding mechanisms), or related system routines that set up CPU-accessible memory for the application. Previously, on PCIe-based machines, system allocated memory was not directly accessible by the GPU. The GPU could only access memory that came from special allocators such as cudaMalloc or cudaMallocManaged. With HMM enabled, all application threads (GPU or CPU) can directly access all of the application\u2019s system allocated memory. As with Unified Memory (which can be thought of as a subset of, or precursor to HMM), there is no need to manually copy system allocated memory between processors. This is because it is automatically placed on the CPU or GPU, based on processor usage. ... Atomic memory operations and synchronization primitives HMM supports all memory operations, which includes atomic memory operations. That is, programmers may use atomic memory operations to synchronize GPU and CPU threads with flags. ... Leverage memory-mapped I/O for fast development One of the interesting features that HMM provides is memory-mapped file I/O directly from the GPU. It enables developers to directly read files from supported storage or /disk without staging them in system memory and without copying the data to the high bandwidth GPU memory. ... The ERA5 dataset consists of hourly estimates of several atmospheric variables. In the dataset, total precipitation data for each month is stored in a separate file. We used 40 years of total precipitation data from 1981\u20132020, which sum to 480 input files aggregating to ~1.3 TB total input data size. See Figure 1 for example results. ... Using the Unix mmap API, input files can be mapped to a contiguous virtual address space. With HMM, this virtual address can be passed as input to a CUDA kernel which can then directly access the values to build a histogram of total precipitation for each hour for all the days in a year. ... Enabling and detecting HMM A GPU with one of the following supported architectures: NVIDIA Turing, NVIDIA Ampere, NVIDIA Ada Lovelace, NVIDIA Hopper, or newer. ...\n\nI'm not sure why we can't have such a capability of a programming model\nmapping to efficient HW operations for Arcanist, but I would have expected\nnaturally GPUs with NN GBy VRAM and CPUs with NNN GBy RAM and TBy scale VM\ncould simply access data in physical RAM/VRAM / virtual VM pretty flexibly as\nexemplified above by now.IMO it would be nice to see Battlemage, Celestial,\nDruid, Arcanist improve this aspect of the programming model, and also finally\nimplement SR-IOV so at least we can easily run graphics / compute in a few VMs\n(after all consumer desktops already virtualize / MMU / IOMMU everything else\nand have 128+ GBy RAM with 16+ core CPUs etc.).  \n---  \n  \n###\n\nElliottDyson commented Feb 17, 2024\n\n> > Sorry for the late response. We disable >4GB memory allocation on ARC770\n> as there are some hardware limitations on ARC, and there will be significant\n> performance drop as penalty to trade off. This is not acceptable in IPEX\u2019s\n> usage scenarios, hence we have disabled it.\n>\n> @tye1 This is not acceptable, is a solution that allows >4GB allocation\n> possible?The advertised VRAM on these cards is 16GB. It's unacceptable to\n> advertise that memory capacity, have it functional, then later disable it\n> leaving the customer with a GPU they no longer can use.I was very excited to\n> build a server with 7xA770s for machine learning work. In my previous work\n> with A770 I had no issues, which caused me to invest in $2500 worth of GPUs.\n> Think about how frustrating this is from the customer's perspective please.\n\nThe thing is, if all they're concerned about is slowdowns, then wouldn't it be\neasy enough to embed a warning that these slowdowns occur when transferring\ndata in chunks that are greater than 4GB in size.Significant slowdowns would\nmean at least it still works. Some functionality is better than no\nfunctionality. I'm sure a lot of people would agree with that. @tye1  \n---  \n  \n###\n\nghchris2021 commented Feb 20, 2024\n\n> The thing is, if all they're concerned about is slowdowns, then wouldn't it\n> be easy enough to embed a warning that these slowdowns occur when\n> transferring data in chunks that are greater than 4GB in size.Significant\n> slowdowns would mean at least it still works. Some functionality is better\n> than no functionality. I'm sure a lot of people would agree with that. @tye1\n\nExactly. I mean these 4GB limits (IIRC) have been variously mentioned here\n(wrt. pytorch programmers), for the OpenCL implementation (OCL programmers),\netc.Ok, so it (the limitation) is something that directly affects GPU/HPC/ML\nprogrammers.As a group that writes HPC / GPGPU code, I think we're especially\nused to benchmarking / analyzing / optimizing our code wrt. a myriad of trade-\noffs as to capability vs. speed vs. complexity etc.\"Oh look I'm going beyond\n{L1, L2, L3} cache size / cache line / page size -- significant performance\ndrop\" ok, expected, but often necessary / desired if one needs the added RAM\nsize.Same thing using RAM vs registers or accessing RAM non sequentially, or\nabout 50 other cases where real world code must / should deviate from the\nideal best case performance strategy and must have the flexibility to do it as\nthe programmer decides best at design time or even run-time.I'd rather the\nmost flexible / capable possibility \"just work\" easily, and if I have to\noptimize things somehow (if even possible) then I'll spend the time to\noptimize the DSA I used or choose new speed / capability trade-offs if that's\neven appropriate.I'm hoping our RAM / VRAM sizes will keep increasing\nsubstantially every generation (16G ARCs now, hopefully 32-48G \"ECCd\" B / C /\nD / NV / AMD / whatever cards in months / a year or so to come) so it seems\nkey to be able to actually use (\"it just works\" style) all that VRAM one has\npaid for (particularly since IIRC as aforementioned \"it just works\" on the CPU\nexecution device vs the GPU device having the unusual case limit).  \n---  \n  \n###\n\nProjectPhysX commented Feb 24, 2024 \u2022\n\nSingle >4GB VRAM allocations are possible on Arc, but currently they require 2\nsmall workarounds in the application. For OpenCL, these are:\n\n  1. In every cl::Buffer/clCreateBuffer allocation, you have to set the buffer flag bit (1<<23), which in the driver is called CL_MEM_ALLOW_UNRESTRICTED_SIZE_INTEL.\n  2. In cl::Program::build/clBuildProgram, you have to set the compiler option \"-cl-intel-greater-than-4GB-buffer-required\".\n\nI've added this to my OpenCL-Wrapper in this commit, so anything built on top\nof it works on Arc out-of-the-box.For Level Zero, the workarounds are similar:\nhttps://github.com/intel/compute-runtime/blob/master/programmers-\nguide/ALLOCATIONS_GREATER_THAN_4GB.mdI agree that >4GB allocations should be\nenabled by default. Such a limitation is not contemporary in a time where AI\nand simulation models commonly use much larger VRAM capacity. Using the full\n16GB VRAM capacity of a 16GB GPU has to work no matter what. ISVs should not\nhave to manually add patches only for Arc, to enable basic functionality.\nBetter eliminate this complication and just make it work, and provide the\noption to disable >4GB allocations for optimization.  \n---  \n  \nSign up for free to join this conversation on GitHub. Already have an account?\nSign in to comment\n\nLabels\n\nARC ARC GPU Crash Execution crashes\n\n15 participants\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": true}
