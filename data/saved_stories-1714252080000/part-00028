{"aid": "40178958", "title": "Deriving a Bit-Twiddling Hack: Signed Integer OVERFLOWpermalink", "url": "https://grack.com/blog/2022/12/20/deriving-a-bit-twiddling-hack/", "domain": "grack.com", "votes": 1, "user": "thunderbong", "posted_at": "2024-04-27 10:47:52", "comments": 0, "source_title": "Deriving a Bit-Twiddling Hack: Signed Integer Overflow", "source_text": "Deriving a Bit-Twiddling Hack: Signed Integer Overflow | grack\n\n# grack.com\n\n## Blog\n\n## Writing\n\n## Code\n\n# About\n\nTech stuff, updated infrequently.\n\n  * grack.com/blog (rss)\n  * matthew@mastracci.com\n  * gpg B9B0 44E1 44AC E235\n\nHome > Blog > 2022 December > 20 \u2190 post \u2192\n\n## Deriving a Bit-Twiddling Hack: Signed Integer Overflow\n\npermalink\n\nThanks to Raph Levien and Electronic Arts for inspiring this post!\n\nAs a thin layer on top of assembly language, C\u2019s integer arithmetic APIs have\nalways been minimal, effectively just mapping the underlying assembly opcodes\nto the C arithmetic operators. In addition, while unsigned arithmetic can\nsafely overflow in C, signed arithmetic overflow is considered undefined\nbehaviour, and UB can end up in heartache for C developers.\n\nMore modern languages like Rust have a much richer integer API, however. By\ndefault, using the standard addition and subtraction operators in debug mode,\ninteger APIs will panic! on overflow or underflow. The same operators will\nwrap in two\u2019s-complement mode in release mode, though the behaviour is\nconsidered defined. If the developer wants to specify carrying, wrapping,\nchecked, or saturating operations, APIs for each of these modes are available.\n\nWe don\u2019t have these convenient APIs available in C yet (see the epilogue for\nsome nuance), but it would be great to have them. Given that signed arithmetic\noverflow is undefined behaviour, can we build a function with the following C\nsignature that works?\n\n    \n    \n    bool will_add_overflow(int32_t a, int32_t b);\n\nAll modern machines use two\u2019s complement representation for negative integers,\nbut C was developed when computing was experimenting with other forms of\nrepresenting signed integers. In this post, we will safely assume that all\nprocessors we\u2019re targetting on are not using one of the alternative forms. If\nyou find yourself programming for esoteric machines, you may wish to consult\nyour local manual or guru.\n\nA valid, quick-and-dirty solution to this would be to use integer promotion\nand add 64-bit signed integers instead, checking to see if the result is\nwithin range of an int16_t:\n\n    \n    \n    bool will_add_overflow_64bit(int32_t a, int32_t b) { // a and b are promoted to 64-bit signed integers int64_t result = (int64_t)a + b; if (result < INT32_MIN || result > INT32_MAX) { return true; } return false; }\n\nNo undefined behaviour! It also has the advantage of being easily read and\nobviously correct. But this required sign-extending two 32-bit numbers and\nperforming two 64-bit additions:\n\n    \n    \n    will_add_overflow_64bit: movsxd rax, edi movsxd rcx, esi add rcx, rax movsxd rax, ecx cmp rax, rcx setne al ret\n\nWe can do better by taking advantage of the fact that on two\u2019s-complement\nmachines, addition is bitwise-identical between signed and unsigned numbers so\nlong as you ignore carry, overflow, underflow and any other flags. In\naddition, the C specification (C99 6.3.1.3 \u00b62) guarantees that the bit pattern\nwill be preserved on a two\u2019s-complement system.\n\nWe know that unsigned overflow is not UB, and we know that we can only\noverflow if a > 0 and b > 0, and we can only underflow if a < 0 and b < 0\\. If\neither a or b is zero, we\u2019re safe. We also know that adding two positive\nintegers must result in a positive result if no overflow occurred. For two\nnegative integers, the result must also be negative. If we find that the sign\nof the sum does not match the sign expected, we\u2019ve wrapped around!\n\n    \n    \n    bool will_add_overflow_if(int32_t a, int32_t b) { // Explicitly convert to uint32_t and then back int32_t c = (int32_t)((uint32_t)a + (uint32_t)b); if (a > 0 && b > 0 && c < 0) { return true; } if (a < 0 && b < 0 && c >= 0) { return true; } return false; }\n\nAnd we get a fairly hefty assembly representation:\n\n    \n    \n    will_add_overflow_if: lea ecx, [rsi + rdi] test edi, edi jle .LBB2_3 test esi, esi jle .LBB2_3 mov al, 1 test ecx, ecx jns .LBB2_3 ret .LBB2_3: test esi, edi sets dl test ecx, ecx setns al and al, dl ret\n\nThis is arguably a bit worse, as now we have a branch in the mix. But we can\nstart to see a pattern here:\n\na| b| c| result  \n---|---|---|---  \n> 0| > 0| < 0| true  \n< 0| < 0| >= 0| true  \n  \nIn two\u2019s-complement, the expression x < 0 is equivalent to the expression (x &\n0x80000000) == 0x80000000. Similarly, x >= 0 is equivalent to (x & 0x80000000)\n== 0.\n\nLet\u2019s create a NEG macro with the above expression and reproduce our pseudo-\ntruth table in code. Note that we\u2019ll also collapse the if statements into a\nsingle boolean expression so we can eliminate those branches:\n\n    \n    \n    bool will_add_overflow_expression(int32_t a_, int32_t b_) { // Explicitly work with uint32_t in this function uint32_t a = (uint32_t)a_, b = (uint32_t)b_; uint32_t c = (uint32_t)a + (uint32_t)b; #define NEG(x) (((uint32_t)(x) & 0x80000000) == 0x80000000) return ((!NEG(a) && !NEG(b) && NEG(c)) || (NEG(a) && NEG(b) && !NEG(c))); #undef NEG }\n\nThis is looking better, but because we\u2019re using short-circuiting logic, those\nbranches are still there: we still have a jump!\n\n    \n    \n    will_add_overflow_expression: mov eax, esi or eax, edi setns dl mov ecx, esi add ecx, edi sets al and al, dl test edi, edi jns .LBB3_3 test al, al jne .LBB3_3 test esi, esi sets dl test ecx, ecx setns al and al, dl .LBB3_3: ret\n\nWe can get rid of the branches by using non-short-circuiting bitwise\noperators:\n\n    \n    \n    bool will_add_overflow_bitwise(int32_t a_, int32_t b_) { uint32_t a = (uint32_t)a_, b = (uint32_t)b_; uint32_t c = (uint32_t)a + (uint32_t)b; #define NEG(x) (((uint32_t)(x) & 0x80000000) == 0x80000000) return ((!NEG(a) & !NEG(b) & NEG(c)) | (NEG(a) & NEG(b) & !NEG(c))); #undef NEG }\n\nAnd now it\u2019s starting to look pretty compact (though we can do better):\n\n    \n    \n    will_add_overflow_bitwise: lea ecx, [rsi + rdi] mov eax, esi or eax, edi and esi, edi xor eax, esi not eax and eax, ecx xor eax, esi shr eax, 31 ret\n\nNotice that the assembly gives us a bit of a hint here that repeated use of\nour macro isn\u2019t actually necessary. The sign bit we\u2019re interested in isn\u2019t\ntested until the end of the function! Because we\u2019re testing the same bit in\nevery part of the expression, and bits in a given position only interact with\nother bits in the same position, we can pull that bit test out of the whole\nexpression:\n\n    \n    \n    bool will_add_overflow_bitwise_2(int32_t a_, int32_t b_) { uint32_t a = (uint32_t)a_, b = (uint32_t)b_; uint32_t c = (uint32_t)a + (uint32_t)b; #define NEG(x) (((uint32_t)(x) & 0x80000000) == 0x80000000) return NEG((~a & ~b & c) | (a & b & ~c)); #undef NEG }\n\nWe can also make use of the knowledge that testing the sign bit is the same as\nan unsigned shift right:\n\n    \n    \n    bool will_add_overflow_bitwise_3(int32_t a_, int32_t b_) { uint32_t a = (uint32_t)a_, b = (uint32_t)b_; uint32_t c = (uint32_t)a + (uint32_t)b; return ((~a & ~b & c) | (a & b & ~c)) >> 31; }\n\nNot too bad! But let\u2019s revisit the truth table and instead use the value of\nthe sign bit directly. What we see is that a and b need to be the same value,\nand c needs to be the opposite value:\n\na| b| c  \n---|---|---  \n1| 1| 0  \n0| 0| 1  \n  \nThis truth table shows that what we ultimately want to test is this:\n\n    \n    \n    (a == 1 && b == 1 && c == 0) || (a == 0 && b == 0 && c == 1)\n\n... but with a bit of work, we can simplify this down to two shorter\nexpression candidates:\n\n    \n    \n    (a == b) && (a == !c) (c == !a) && (c == !b)\n\nFor bit twiddling like we\u2019re doing here, xor (^) can work like a \u201cnot-equals\u201d\noperator (outputs 1 iff the inputs are 0,1 or 1,0), which means we can re-\nwrite our two expressions like so:\n\n    \n    \n    ~(a ^ b) & (c ^ a) (c ^ a) & (c ^ b)\n\nBy looking at those two options, is there a hint that one might be cheaper to\nimplement? Let\u2019s plug both into the compiler and see what we get!\n\n    \n    \n    bool will_add_overflow_optimized_a(int32_t a_, int32_t b_) { uint32_t a = (uint32_t)a_, b = (uint32_t)b_; uint32_t c = (uint32_t)a + (uint32_t)b; return (~(a ^ b) & (c ^ a)) >> 31; } bool will_add_overflow_optimized_b(int32_t a_, int32_t b_) { uint32_t a = (uint32_t)a_, b = (uint32_t)b_; uint32_t c = (uint32_t)a + (uint32_t)b; return ((c ^ a) & (c ^ b)) >> 31; }\n\nAnd the resulting compiled versions:\n\n    \n    \n    will_add_overflow_optimized_a: lea eax, [rsi + rdi] xor eax, edi mov ecx, edi xor ecx, esi not ecx and eax, ecx shr eax, 31 ret will_add_overflow_optimized_b: lea eax, [rsi + rdi] xor edi, eax xor eax, esi and eax, edi shr eax, 31 ret\n\nWe have a clear winner here: the compiler can do a much better job with (c ^\na) & (c ^ b). This is most likely because of the common sub-expression and the\nremoval of the bitwise-not operator.\n\nWe can also confirm that there\u2019s no known undefined behaviour by compiling it\nwith clang\u2019s -fsanitize=undefined feature. No UB warnings are printed, which\nmeans no UB was detected!\n\n## Epilogue\n\nWhile this is the fastest we can get with bog-standard C99, this isn\u2019t\nnecessarily the best we can do.\n\nRust makes use of the compiler intrinsics to access the overflow flag of the\nprocessor directly:\n\n    \n    \n    pub fn add(a: i32, b: i32) -> bool { a.checked_add(b).is_none() } example::add: add edi, esi seto al ret\n\nIt turns out that both GCC and LLVM have C intrinsics that you can use. While\nthey are non-portable to some compilers, they drastically simplify the\nassembly output!\n\n    \n    \n    bool will_add_overflow_intrinsic(int32_t a, int32_t b) { int32_t result; return __builtin_add_overflow(a, b, &result); }\n\nAnd, just like with the Rust compiler above, this generates optimal assembly!\n\n    \n    \n    will_add_overflow_intrinsic: add edi, esi seto al ret\n\nNot to worry about this being so deeply compiler-specific for now, however.\nThis will be standardized in C23 with the addition of the functions in the\nstdckdint.h header.\n\nA full suite of tests to explore the solutions is available on Godbolt or as a\nGist.\n\nRead full post\n\n20 December 2022\n\n## Related Posts\n\n  * Hacking Bluetooth to Brew Coffee from GitHub Actions: Part 2 - Reve... 2 December 2022\n  * Hacking Bluetooth to Brew Coffee from GitHub Actions: Part 1 - Blue... 1 December 2022\n  * Hacking Bluetooth to Brew Coffee from GitHub Actions: Part 3 - GitH... 4 December 2022\n  * What's new in CSS Selectors 4 11 January 2015\n\n\u00a9 1998-2022 Matt Mastracci \u2014 contact\n\n\u00d7\n\nThis site is still being indexed. Please try your search again in a few\nminutes.\n\nsearch by swiftype\n\nsearch this website\n\n", "frontpage": false}
