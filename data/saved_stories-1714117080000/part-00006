{"aid": "40162582", "title": "Packaging CLI programs into Docker images to avoid dependency hell (2019)", "url": "https://andrewlock.net/packaging-cli-programs-into-docker-images-to-avoid-dependency-hell/", "domain": "andrewlock.net", "votes": 1, "user": "tosh", "posted_at": "2024-04-25 20:27:21", "comments": 0, "source_title": "Packaging CLI programs into Docker images to avoid dependency hell", "source_text": "Packaging CLI programs into Docker images to avoid dependency hell\n\n# Andrew Lock | .NET Escapades Andrew Lock\n\nSponsored by Nick Chapsas\u2014Want to learn how to build elegant REST APIs in\n.NET? Get 5% off Nick's latest course \"From Zero to Hero: REST APIs in .NET\"!\n\nOctober 08, 2019 ~8 min read\n\n  * AWS\n  * Docker\n  * Docker Hub\n\n# Packaging CLI programs into Docker images to avoid dependency hell\n\nShare on:\n\nIn this post, I'm not going to talk about ASP.NET Core for a change. Instead,\nI'm going to show one way to package CLI tools and their dependencies as\nDocker images. With a simple helper script, this allows you to run a CLI tool\nwithout having to install the dependencies on your host machine. I'll show how\nto create a Docker image containing your favourite CLI tool, and a helper\nscript for invoking it.\n\nAll the commands in this post describe using Linux containers. The same\nprincipal can be applied to Windows containers if you update the commands.\nHowever the benefits of isolating your environment come with the downside of\nlarge Docker image sizes.\n\n> If you're looking for a Dockerised version of the AWS CLI specifically, I\n> have an image on Docker hub which is generated from this GitHub repository.\n\n## The problem: dependency hell\n\nFor example, take the AWS CLI. The suggested way to install the CLI on Linux\nis to use Python and pip (Pip is the package installer for Python; the\nequivalent of NuGet for .NET). The recommended version to use is Python 3, but\nyou may have other apps that require Python 2, at which point you're in a\nworld of dependency hell.\n\nDocker containers can completely remove this problem. By packaging all the\ndependencies of an application into a container (even the operating system)\nyou isolate the apps from both your host machine, and other apps. Each\ncontainer runs in its own little world, and can have completely different\ndependencies to every other and the host system.\n\nThis is obviously one of the big selling points of containers, and is part of\nthe reason they're seeing such high adoption for production loads. But they\ncan also help with our AWS CLI problem. Instead of installing the CLI on our\nhost machine, we can install it in a Docker container instead, and execute our\nCLI commands there.\n\n## Creating a Docker image for the AWS CLI\n\nSo what does it actually take to package up a tool in a Docker container? That\ndepends on the tool in question. Hopefully, the installation instructions\ninclude a set of commands for you to run. In most cases, if you're at all\nfamiliar with Docker you can take these commands and convert them into a\nDockerfile.\n\nFor example, let's take the AWS CLI instructions. According to the\ninstallation instructions, you need to have Python and pip installed, after\nwhich you can run\n\n    \n    \n    pip3 install awscli --upgrade --user\n\nto install the CLI.\n\nOne of the main difficulties of packaging your app into a Docker container, is\nestablishing all of the dependencies. Python and pip are clearly required, but\ndepending on which operating system you use for your base image, you may find\nyou need to install additional dependencies.\n\nAlpine Linux is a common candidate for a base OS as it's tiny, which keeps\nyour final Docker images as small as possible. However Alpine is kept small by\nnot including much in the box. You may well find you need to add some extra\ndependencies for your target tool to work correctly.\n\nThe example Dockerfile below shows how to install the AWS CLI in an Alpine\nbase image. It's taken from the aws-cli image which is available on Docker\nHub:\n\n    \n    \n    FROM alpine:3.6 RUN apk -v --no-cache add \\ python \\ py-pip \\ groff \\ less \\ mailcap \\ && \\ pip install --upgrade awscli==1.16.206 s3cmd==2.0.2 python-magic && \\ apk -v --purge del py-pip VOLUME /root/.aws VOLUME /project WORKDIR /project ENTRYPOINT [\"aws\"]\n\nThis base image uses Alpine 3.6, and starts by installing a bunch of\nprerequisites:\n\n  * python: the Python (3) environment\n  * py-pip: the pip package installer we need to install the AWS CLI\n  * groff: used for formatting text\n  * less: used for controlling the amount of text displayed on a terminal\n  * mailcap: used for controlling how to display non-text\n\nNext, as part of the same RUN command (to keep the final Docker image as small\nas possible) we install the AWS CLI using pip. We also install the tool s3cmd\n(which makes it easier to work with S3 data), and python-magic (which helps\nwith mime-type detection).\n\nAs the last step of the RUN command, we uninstall the py-pip package. We only\nneeded it to install the AWS CLI and other tools, and now it's just taking up\nspace. Deleting (and purging) it helps keep the size of the final Docker image\ndown.\n\nThe next two VOLUME commands define locations known by the Docker container\nwhen it runs on your machine. The /root/.aws path is where the AWS CLI will\nlook for credential files. The /project path is where we set the working\ndirectory (using WORKDIR), so it's where the AWS CLI commands will be run.\nWe'll bind that at runtime to wherever we want to run the AWS CLI, as you'll\nsee shortly.\n\nFinally we set the ENTRYPOINT for the container. This sets the command that\nwill run when the container is executed. So running the Docker container will\nexecute aws, the AWS CLI.\n\nTo build the image, run docker build . in the same directory as Dockerfile,\nand give it a tag:\n\n    \n    \n    docker build -t example/aws-cli .\n\nYou will now have a Docker image containing the AWS CLI. The next step is to\nuse it!\n\n## Running your packaged tool image as a container\n\nYou can create a container from your tool image and run it in the most basic\nform using:\n\n    \n    \n    docker run --rm example/aws-cli\n\nIf you run this, Docker creates a container from your image, executes the aws\ncommand, and then exists. The --rm option means that the old container is\nremoved afterwards, so it doesn't clutter up your drive. In this example, we\ndidn't provide any command line arguments, so the AWS CLI shows the standard\nhelp text:\n\n    \n    \n    > docker run --rm example/aws-cli usage: aws [options] <command> <subcommand> [<subcommand> ...] [parameters] To see help text, you can run: aws help aws <command> help aws <command> <subcommand> help aws: error: too few arguments\n\nIf you want to do something useful, you'll need to provide some arguments to\nthe CLI. For example, lets try listing the available S3 buckets, by passing\nthe arguments s3 ls:\n\n    \n    \n    > docker run --rm example/aws-cli s3 ls Unable to locate credentials. You can configure credentials by running \"aws configure\".\n\nThis is where things start to get a bit more tricky. To call AWS, you need to\nprovide credentials. There are a variety of ways of doing this, including\nusing credentials files in your profile, or by setting environment variables.\nThe easiest approach is to use environment variables, by exporting them in\nyour host environment:\n\n    \n    \n    export AWS_ACCESS_KEY_ID=\"<id>\" export AWS_SECRET_ACCESS_KEY=\"<key>\" export AWS_SESSION_TOKEN=\"<token>\" #if using AWS SSO export AWS_DEFAULT_REGION=\"<region>\"\n\nAnd passing these to the docker run command:\n\n    \n    \n    docker run --rm \\ -e AWS_ACCESS_KEY_ID \\ -e AWS_SECRET_ACCESS_KEY \\ -e AWS_DEFAULT_REGION \\ -e AWS_SESSION_TOKEN \\ example/aws-cli \\ s3 ls\n\nI split the command over multiple lines as it's starting to get a bit\nunwieldy. If you have your AWS credentials stored in credentials files instead\nin $HOME/.aws instead of environment variables, you can pass those to the\ncontainer using:\n\n    \n    \n    docker run --rm \\ -v \"$HOME/.aws:/root/.aws\" \\ example/aws-cli \\ s3 ls\n\nIn these examples, we're just listing out our S3 buckets, so we're not\ninteracting with the file system directly. But what if you want to copy a file\nfrom a bucket to your local file system? To achieve this, you need to bind\nyour working directory to the /project volume inside the container. For\nexample:\n\n    \n    \n    docker run --rm \\ -v \"$HOME/.aws:/root/.aws\" \\ -v $PWD:/project \\ example/aws-cli \\ s3 cp s3://mybucket/test.txt test2.txt\n\nIn this snippet we bind the current directory ($PWD) to the working directory\nin the container /project. When we use s3 cp to download the test.txt file,\nit's written to /project/test2.txt in the container, which in turn writes it\nto your current directory on the host.\n\nBy now you might be getting a bit fatigued - having to run such a long command\nevery time you want to use the AWS CLI sucks. Luckily there's easy fixes by\nusing a small script\n\n## Using helper scripts to simplify running your containerised tool\n\nHaving to pass all those environment variables and volume mounts is a pain.\nThe simplest solution, is to create a basic script that includes all those\ndefaults for you:\n\n    \n    \n    #!/bin/bash docker run --rm \\ -v \"$HOME/.aws:/root/.aws\" \\ -v $PWD:/project \\ example/aws-cli \\ \"$@\"\n\nNote that this script is pretty much the same as the final example from the\nprevious section. The difference is that we're using the arguments catch-all\n\"$@\" at the end of the script, which means \"paste all of the arguments here as\nquoted string\".\n\nIf you save this script as aws.sh in your home directory (and give it execute\npermissions by running chmod +x ~/aws.sh), then copying a file becomes almost\nidentical to using the AWS CLI directly:\n\n    \n    \n    # Using the aws cli directly aws.sh s3 cp s3://mybucket/test.txt test2.txt # Using dockerised aws cli ~/aws.sh s3 cp s3://mybucket/test.txt test2.txt\n\nMuch nicer!\n\nYou could even go one step further and create an alias for aws to be the\ncontents of the script:\n\n    \n    \n    alias aws='docker run --rm -v \"$HOME/.aws:/root/.aws\" -v $PWD:/project example/aws-cli'\n\nor alternatively, copy the file into your path:\n\n    \n    \n    sudo cp ~/aws.sh /usr/local/bin/aws\n\nAs ever with Linux, there's a whole host of extra things you could do. You\ncould create different versions of the aws.sh script which is configured to\nuse alternative credentials or regions. But using a Dockerised tool rather\nthan installing the CLI directly on your host means you can also have scripts\nthat use different versions of the CLI. All the while, you've avoided\npolluting your host environment with dependencies!\n\n## Summary\n\nIn this post, I showed how you can Dockerise your CLI tools to avoid having to\ninstall dependencies in your host environment. I showed how to pass\nenvironment variables and arguments to the Dockerised tool, and how to bind to\nyour host's file system. Finally, I showed how you can use scripts to simplify\nexecuting your Docker images.\n\nIf you're looking for a Dockerised version of the AWS CLI specifically, I have\nan image on Docker hub which is generated from this GitHub repository (which\nis a fork of an original which fell out of maintenance).\n\nExample source code for this post\n\n  * \u201cknife-portable-swiss-blade-army\u201d by Clker-Free-Vector-Images licensed under Pixabay\n\nLoading...\n\nMy new book ASP.NET Core in Action, Third Edition is available now! It\nsupports .NET 7.0, and is available as an eBook or paperback.\n\nExample source code for this post\n\n\u00a9 2023 Andrew Lock | .NET Escapades. All Rights Reserved. | Image credits\n\nAndrew Lock | .Net Escapades\n\nWant an email when there's new posts?\n\nStay up to the date with the latest posts!\n\n", "frontpage": false}
