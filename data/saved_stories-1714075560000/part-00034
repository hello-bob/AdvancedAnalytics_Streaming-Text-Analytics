{"aid": "40155715", "title": "You won't train a better Gen AI model from your desk", "url": "https://www.marble.onl/posts/data_takers_and_makers.html", "domain": "marble.onl", "votes": 1, "user": "andy99", "posted_at": "2024-04-25 10:33:32", "comments": 0, "source_title": "You won't train a better Gen AI model from your desk", "source_text": "You won't train a better Gen AI model from your desk\n\n## You won't train a better Gen AI model from your desk\n\nAndrew Marble marble.onl andrew@willows.ai April 13, 2024\n\nThe progress we\u2019re seeing in Gen AI is skewed heavily towards what can be done\nwithout getting up from the computer. This means optimizing training\nalgorithms and, at best, automated data curation. Most are \u201ctakers\u201d \u2013 stuck\nwith what data is already available and trying to make the most of it.\nCompanies that are winning, particularly OpenAI, are making the data sets they\nneed. A focus on data, particularly data that you need to leave your desk to\nget, is what\u2019s going to continue to differentiate Gen AI offerings.\n\nAt my last company, we were improving computer vision AI for manufacturing\nproblems. The challenge was a lack of data. You\u2019d think manufacturers would\nhave lots, but it often ended up being millions of pictures of the same thing\nover and over again. This makes it challenging to build a robust model that\nhandles edge cases. We spent a lot of time and effort trying to\nalgorithmically engineer better models, and made incremental but limited\nprogress. We tried synthetic images, adding data from public data sets, better\npre-training, techniques to force models to generalize, and more. Then one day\nI got a camera and started taking pictures for myself. I was interested in\n\u201csurface defects\u201d like cracks, scratches, discoloration, chips, etc. So I\nstarted by going outside and taking pictures of anything that looked like a\ndefect.\n\nSome random surface defects captured in Montreal\n\nAlmost immediately, with a few dozen good images collected and labeled, I\ncould build a model that generalized better than any of the past things we\u2019d\nbeen trying. I spent a bit more money and bought some random metal parts from\nthe hardware store and some craft supplies and created, photographed and\nlabeled a whole set of images of the defects I was interested in. And built a\nset of models that outperformed anything I\u2019d seen and were able to identify\ndefects in different classes with no input data from the customer. (Without\ngoing on too much of a tangent, it\u2019s fun to note that the GPU cost of\ngenerating specialized training data was often as much or more than going out\nand building physical examples of what I wanted.)\n\nAI is still a very academic and theoretical discipling. It\u2019s a generalization,\nbut practitioners favor things they can do from their computer. Not that\nalgorithmic improvements aren\u2019t important. But they\u2019re not what makes the\n\u201cfront end\u201d of an AI product. That\u2019s still data, specifically good data. What\nwe call AI is really just an effective way of storing and querying a labeled\ndata set. It generalizes, so the query and the output can be very flexible,\nbut fundamentally the data needs to be there.\n\nIn the context of current Gen AI models, it was famously said that OpenAI has\nnot moat^1. But somehow GPT-4 (to stretch a bad analogy) has been under siege\nfor over a year and held out. We keep seeing models come close on some\ndimension or another, and I\u2019ve seen arguments that Anthropic performance has\ncaught up, but GPT-4 remains the industry leader. Almost every day, new closed\nand publicly available models advertise new training and data curation\ntechniques that result in incremental improvements in their performance\n(cynically, often against a cherry picked set of benchmarks and comparators).\nWe\u2019ve seen RLHF give way to DPO and now to DNO^2. With respect to the latter\ntechnique, to give a taste of the kind of qualifiers we see on these new\nresults, after controlling for response length they beat an older version of\nGPT-4 on AlpacaEval. Hard to get more definitive than that.\n\nWhat isn\u2019t discussed enough is that OpenAI has a huge moat in the form of\ndata. Details are scant, but it\u2019s clear they have invested more than anyone\nelse, especially the research and public models, in gathering and curating\ntheir own data sets. The evidence is in the quality but also the tone and\ndiction of the responses. There are now lots of examples showing how prevalent\nGPT-4 written article are, looking at the frequency over time of dead giveaway\nterms like \u201cdelve^3\u201d \u201ccommendable^4\u201d, etc. And of course there are all the\nclich\u00e9 adjective-verb constructions and \u201cit is crucial to remember that\u201d\ncaveats. All that didn\u2019t emerge, it was carefully trained in by an army of\ndata labelers. The same holds for image generation. All the Dall-E images now\nlook very similar across prompts.\n\nAll the Dall-E pictures have the same look and feel\n\nThe quality is great, but it\u2019s all the same people, the same computers, the\nsame style. We see this stuff all over the internet. It\u2019s clear they very\nintentionally curated a good data set to generate these styles \u2013 compare it to\nsome of the Stable Diffusion models trained on noisy web data and the\ndifference is obvious.\n\nAll this to say, OpenAI clearly gets what is important in building good\nmodels, and it\u2019s that focus that\u2019s kept them ahead over everyone else. All the\nincremental research helps, but my suspicion is that most model development\nhas skewed heavily towards the practitioners\u2019 preference of tackling problems\nthat can be solved from a desk. Look at all the models that are trained on\ndata sets generated by GPT-4 itself. My personal experience is that the way to\nmake big progress is to get your hands dirty and go out and collect the data\nyou need, and I\u2019m convinced this applies to advancing Gen AI as well. It\u2019s\nworked for OpenAI.\n\n  1. https://www.semianalysis.com/p/google-we-have-no-moat-and-neither\u21a9\ufe0e\n\n  2. https://arxiv.org/abs/2404.03715\u21a9\ufe0e\n\n  3. https://twitter.com/paulg/status/1777035484826349575\u21a9\ufe0e\n\n  4. https://arxiv.org/abs/2403.07183\u21a9\ufe0e\n\n", "frontpage": false}
