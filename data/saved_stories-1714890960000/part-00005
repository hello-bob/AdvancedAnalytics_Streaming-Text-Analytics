{"aid": "40260892", "title": "Speeding up Amazon ECS container deployments (2020)", "url": "https://nathanpeck.com/speeding-up-amazon-ecs-container-deployments/", "domain": "nathanpeck.com", "votes": 2, "user": "mooreds", "posted_at": "2024-05-04 22:41:21", "comments": 0, "source_title": "Speeding up Amazon ECS container deployments", "source_text": "Speeding up Amazon ECS container deployments | Nathan Peck\n\nNathan Peck Senior Developer Advocate for Generative AI at Amazon Web Services\nOct 19, 2020 10 min read\n\n# Speeding up Amazon ECS container deployments\n\nContainer orchestration is a complex problem. There are many different\ncomponents communicating with each other in support of your application\ncontainer deployment. First the orchestrator starts your application. Then the\norchestrator needs to make a decision about whether your application is ready\nfor web traffic. Later the application might be scaled down or a new version\nneeds to be rolled out, so the old version of the application needs to be\nstopped. The orchestrator must decide whether the application is safe to stop.\nThe orchestrator wants to maintain your application\u2019s availability while doing\na rolling deployment.\n\nAs a result of this you can sometimes end up in situations where your\ncontainer deployments seem to be taking longer than you expect. Have thought\nto yourself \u201cWhy is this new version of my container taking 15 minutes to roll\nout?\u201d If so this is usually because parts of your container orchestration are\nconfigured to be excessively \u201csafe\u201d. Here are some tips and tricks for\nconfiguring slightly less safe, but considerably faster container deployments\non Amazon ECS:\n\n## Load balancer healthchecks\n\nThere are two load balancer healthcheck settings that you will want to modify:\n\nDefault settings (Target group health check settings):\n\n  * HealthCheckIntervalSeconds: 30 seconds\n  * HealthyThresholdCount: 5\n\nBy default the load balancer requires 5 passing healthchecks, each of which is\nmade 30 seconds apart, before the target container is considered healthy. With\na little math (5 * 30 / 60 = 2.5) we can see that means 2 minutes and 30\nseconds. Because Amazon ECS uses the load balancer healthcheck as part of\ndetermining container health, this means that by default it takes a minimum of\n2 minutes and 30 seconds before ECS considers a freshly launched container to\nbe healthy.\n\nIf your service starts up and stabilizes fast (as most modern runtimes should\nbe able to) you can reduce this to the minimum:\n\nRecommended settings (Target group health check settings):\n\n  * HealthCheckIntervalSeconds: 5 seconds\n  * HealthyThresholdCount: 2\n\nThis configuration would mean it only takes 10 seconds before the the load\nbalancer and subsequently ECS can consider the container healthy.\n\n## Load balancer connection draining\n\nBrowsers and mobile clients usually maintain a keep alive connection to the\nservice running inside your container. This is an important optimization\nbecause it means that subsequent requests from that client can reuse a\npreexisting connection rather than needing to reestablish the connection from\nscratch each time they need to send a request.\n\nWhen you tell the load balancer that you want to stop sending traffic to a\ncontainer (probably because you want to shut it down) the load balancer will\nstop sending new connections to the downstream container, but it will wait for\nexisting connections to close on their own. It will only break the keep alive\nconnections forcefully after a period called the deregistration delay.\n\nDefault settings (Target group attributes):\n\n  * deregistration_delay.timeout_seconds: 300 seconds\n\nBy default the load balancer will wait up to 300 seconds (or 5 minutes) for\nthe existing keep alive connections to close on their own, before forcefully\nbreaking them. ECS waits for this deregistration to complete before sending a\nSIGTERM signal to your container, so that any inflight requests do not get\ndropped.\n\nIf your service is something like a fast REST API where average response times\nare under a second there is no harm in reducing this delay or removing it\nentirely. Do not do this if you have a service with long lived requests like\nslow file uploads, streaming connections, etc:\n\nRecommended settings (Target group attributes):\n\n  * deregistration_delay.timeout_seconds: 5 seconds\n\nThis will cause the load balancer to wait only 5 seconds before breaking any\nkeep alive connections between the client and your backend server, then it\nreports to ECS that draining is complete and ECS can stop the task.\n\n## SIGTERM responsiveness\n\nAfter load balancer draining is done, when ECS stops a running task, one of\nthe first things that happens is a SIGTERM signal is sent to the task. This is\nintended as a friendly warning that the containerized application needs to\nfinish what it is doing and shut down. However many application frameworks\nignore this signal. As a result there is a waiting period where ECS is giving\nthe application a chance to close in a clean manner, and when that expires a\nSIGKILL signal is sent to force terminate the process. By default this waiting\nperiod is configured to be 30 seconds long.\n\nDefault setting (ECS agent setting):\n\n  * ECS_CONTAINER_STOP_TIMEOUT: 30 seconds\n\nYou can speed up the waiting period by lowering the amount of time that the\ncontainer is given to gracefully shut down.\n\nRecommended setting (ECS agent setting):\n\n  * ECS_CONTAINER_STOP_TIMEOUT: 2 seconds\n\nNow ECS will only give the container 2 seconds to shut down, and if it does\nnot complete its work and exit within that time period then the container will\nbe force stopped with a SIGKILL signal. Ideally this waiting period should be\nset to at least twice as long as the slowest request or transaction response\ntime. In this case I assume that your application serves requests within 1\nsecond, so 2 seconds should be more than long enough for the process to finish\nall in-flight requests and exit.\n\nIf you have the ability to modify the application code you can also trap the\nSIGTERM signal and react to it in a responsive manner. For example in Node.js:\n\n    \n    \n    process.on('SIGTERM', function() { server.close(); })\n\nThis will cause the HTTP server to stop listening for any new requests, finish\nanswering any in-flight requests, and then the Node.js process will terminate\nas its event loop will have nothing left to do. This way if it takes the\nprocess only 500ms to finish its in-flight requests it will terminate early on\nits own without having to wait out the stop timeout and get sent a SIGKILL.\n\n## Container image pull behavior\n\nBy default the ECS agent always pulls the Docker image from its remote\nregistry when starting a task. If you are using a well designed image tagging\nscheme such as a new image tag for each release of your application this\nbehavior is unnecessary. The default behavior only benefits you if you are\ndoing something like overwriting your latest image tag with new content for\neach application release. Since using the latest tag is an anti-pattern\nanyway, you are likely using a unique image tag for each application release\nand can modify your image pull behavior.\n\nDefault setting (ECS agent setting):\n\nECS_IMAGE_PULL_BEHAVIOR: default\n\nRecommended setting (ECS agent setting):\n\nECS_IMAGE_PULL_BEHAVIOR: once or alternatively prefer-cached\n\nThis causes ECS to use a preexisting, downloaded image in disk cache on the\nEC2 host instead of downloading from the remote registry. This can save task\nstartup time, particularly for larger Docker images that may take 10-20\nseconds to pull over the network.\n\n## Deploy \u201csteps\u201d\n\nOne important optimization to consider is how many steps the container\norchestrator must go through in order to fully roll out a service update. One\nof the goals of a good container orchestrator is to maintain 100% uptime when\nrolling out a new version of your application. Imagine if the orchestrator did\na two step process:\n\n  1. Stop the old application containers\n  2. Start new application containers\n\nIn this case there would be a brief window between step 1 and step 2 where\nthere were no application containers and a user of the system would experience\nan error.\n\nInstead the orchestrator ideally does the following process:\n\n  1. Start new appliation containers alongside the old application containers\n  2. Ensure that the new application containers are healthy\n  3. Stop old containers\n\nDepending on your deployment configuration and the amount of free, unreserved\nspace in your cluster it may take multiple rounds of this to complete replace\nall old tasks with new tasks. For example:\n\nDefault settings (ECS Service Deployment setting):\n\n  * minimumHealthyPercent: 100%\n  * maximumPercent: 200%\n\nImagine a service that has 6 orange tasks, deployed in a cluster that only has\nroom for 8 tasks total. Additionally the deployment settings don\u2019t allow the\ndeployment to go below 100% of the 6 desired tasks:\n\n  1. The goal is to replace this orange deployment with a blue deployment instead.\n  2. Because of the settings the scheduler can\u2019t stop any existing tasks because that would go below 6 tasks so it starts two extra blue tasks.\n  3. Now the scheduler is able to stop two of the old orange tasks, but only two tasks because the service can\u2019t go below 6 tasks.\n  4. ECS still needs to replace 4 of those old orange tasks with blue tasks, but the only action it can take at this time is to launch two new blue tasks.\n  5. Once those new blue tasks are up and healthy it can stop two of the old orange tasks.\n  6. Once again the only available action is to launch two more blue tasks.\n  7. Finally the scheduler is able to stop the remaining two orange tasks, leaving 6 blue tasks.\n\nThe rolling deployment took 6 mutating steps to complete, and each time it\nlaunched a new task it had to wait for the new task to be come healthy\n(potentially 2.5 mins with the default settings). And each time it had to stop\nan old task it had to wait for the load balancer to drain connections\n(potentially up to 5 minutes with the default settings). And perhaps the\napplication was not behaving properly with SIGTERM so that was an extra 30\nseconds each time a container needed to be stopped. It\u2019s easy to see how this\nsimple rolling deploy could end up taking 30+ minutes to complete without\nfinetuning the settings.\n\nNow let\u2019s modify the minimumHealthPercent to 50%, which allows up to 50% of\nthe running tasks to be stopped before there are new replacement tasks\nrunning:\n\n  1. The goal is to replace this orange deployment with a blue deployment instead.\n  2. Because of the settings the scheduler can stop 50% of the old orange tasks, opening three empty slots.\n  3. Now the scheduler can launch 5 new blue tasks.\n  4. ECS still needs to replace the remaining three old orange tasks with blue tasks so it can stop all three at once.\n  5. Finally it can launch a blue task to get back to 6 blue tasks.\n\nThis time the deployment job was finished in 4 steps instead of 6.\n\nImagine adding a little bit more free space to the cluster, so that it can\ninstead run 10 tasks at once:\n\n  1. The goal is to replace this orange deployment with a blue deployment instead.\n  2. Because of the settings the scheduler can stop 50% of the old orange tasks, opening three empty slots.\n  3. Now the scheduler can launch all 6 new blue tasks.\n  4. Last but not least scheduler shuts down the old orange tasks.\n\nThe deployment job was finished in 3 steps.\n\nIf your tasks spend a fair amount of time idle, and you aren\u2019t hitting high\nutilization then you might be able to significantly speed up your deployments\nusing the following settings to allow ECS to stop running tasks before\nstarting replacements.\n\nRecommended setting (ECS Service Deployment setting):\n\n  * minimumHealthyPercent: 50%\n  * maximumPercent: 200%\n\nOf course if your services are heavily utilized you would want to keep the\ndefault setting of minimumHealthyPercent = 100 because otherwise you might\nimpact availability or latency in the period before the new application tasks\ncome online.\n\nIn this case if you are forced to keep the minimum healthy percentage at 100%\nyou should consider running your EC2 clusters with more empty room for\nstarting replacement tasks in parallel with old tasks, in order to reduce the\nnumber of steps that a rolling deployment takes.\n\n## Conclusion\n\nHopefully these tips and tricks help you to have faster rolling deployments in\nAmazon ECS. With a few easy configuration changes you can speed things up\ndramatically.\n\nYou can download the source files for the diagrams in this article here:\n\n  * speeding-up-ecs-deployments.pptx (4946 kb)\n\n  * Cloud Architecture\n  * Article\n\n\u00ab Amazon ECS Service Extensions for AWS Cloud Development Kit (CDK) Using AWS\nCopilot to Build CI/CD + Integration tests for your ECS app \u00bb\n\n\u00a9 Copyright 2009-2024 Nathan Peck\n\n", "frontpage": false}
