{"aid": "40096462", "title": "Poisson Designs and Minimum Detectable Effects", "url": "https://andrewpwheeler.com/2024/03/18/poisson-designs-and-minimum-detectable-effects/", "domain": "andrewpwheeler.com", "votes": 1, "user": "apwheele", "posted_at": "2024-04-20 11:17:45", "comments": 0, "source_title": "Poisson designs and Minimum Detectable Effects", "source_text": "Poisson designs and Minimum Detectable Effects | Andrew Wheeler\n\n  * About\n  * C.V.\n\n    * Datascience Portfolio\n  * Comment Policy\n  * Consulting\n  * Courses\n\n    * Advanced Criminology (Undergrad) Crim 3302\n    * Communities and Crime (Undergrad) Crim 4323\n    * Crim 7301 \u2013 UT Dallas \u2013 Seminar in Criminology Research and Analysis\n    * Crime Science (Graduate) Crim 7381\n    * GIS in Criminology/Criminal Justice (Graduate)\n    * Crime Analysis (Special Topics) \u2013 Undergrad\n  * Code Snippets TOC\n\n# Andrew Wheeler\n\nCrime Analysis and Crime Mapping\n\n# Poisson designs and Minimum Detectable Effects\n\nIan Adam\u2019s posted a working paper the other day on power analysis for\nanalyzing counts, Power Simulations of Rare Event Counts and Introduction to\nthe \u2018Power Lift\u2019 Metric (Adams, 2024). I have a few notes I wanted to make in\nregards to Ian\u2019s contribution. Nothing I say conflicts with what he writes,\nmoreso just the way I have thought about this problem. It is essentially the\nsame issue as I have written about monitoring crime trends (Wheeler, 2016), or\nexamining quasi-experimental designs with count data (Wheeler & Ratcliffe,\n2018; Wilson, 2022).\n\nI am going to make two broader points here: point 1, power is solely a\nproperty of the aggregate counts in treated vs control, you don\u2019t gain power\nby simply slicing your data into finer temporal time periods. Part 2 I show an\nalternative to power, called minimum detectable effect sizes. This focuses\nmore on how wide your confidence intervals are, as opposed to power (which as\nIan shows is not monotonic). I think it is easier to understand the\nimplications of certain designs when approached this way \u2013 both from \u201cI have\nthis data, what can I determine from it\u201d (a retrospective quasi-experimental\ndesign), as well as \u201chow long do I need to let this thing cook to determine if\nit is effective\u201d. Or more often \u201chow effective can I determine this thing is\nin a reasonable amount of time\u201d.\n\n## Part 1, Establishing it is all about the counts\n\nSo lets say you have a treated and control area, where the base rate is 10 per\nperiod (control), and 8 per period (treated):\n\n    \n    \n    ########## set.seed(10) n <- 20 # time periods reduction <- 0.2 # 20% reduced base <- 10 control <- rpois(n,base) treat <- rpois(n,base*(1-reduction)) print(cbind(control,treat)) ##########\n\nAnd this simulation produces 20 time periods with values below:\n\n    \n    \n    [1,] 10 6 [2,] 9 5 [3,] 5 3 [4,] 8 8 [5,] 9 5 [6,] 10 10 [7,] 10 7 [8,] 9 13 [9,] 8 6 [10,] 13 8 [11,] 10 6 [12,] 8 8 [13,] 11 8 [14,] 7 8 [15,] 10 7 [16,] 6 8 [17,] 12 3 [18,] 15 5 [19,] 10 8 [20,] 7 7\n\nNow we can fit a Poisson regression model, simply comparing treated to\ncontrol:\n\n    \n    \n    ########## outcome <- c(control,treat) dummy <- rep(0:1,each=n) m1 <- glm(outcome ~ dummy,family=poisson) summary(m1) ###########\n\nWhich produces:\n\n    \n    \n    Call: glm(formula = outcome ~ dummy, family = poisson) Deviance Residuals: Min 1Q Median 3Q Max -1.69092 -0.45282 0.01894 0.38884 2.04485 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) 2.23538 0.07313 30.568 < 2e-16 *** dummy -0.29663 0.11199 -2.649 0.00808 ** --- Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 32.604 on 39 degrees of freedom Residual deviance: 25.511 on 38 degrees of freedom AIC: 185.7 Number of Fisher Scoring iterations: 4\n\nIn this set of data, the total treated count is 139, and the total control\ncount is 187. Now watch what happens when we fit a glm model on the aggregated\ndata, where we just now have 2 rows of data?\n\n    \n    \n    ########## agg <- c(sum(treat),sum(control)) da <- c(1,0) m2 <- glm(agg ~ da,family=poisson) summary(m2) ##########\n\nAnd the results are:\n\n    \n    \n    Call: glm(formula = agg ~ da, family = poisson) Deviance Residuals: [1] 0 0 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) 5.23111 0.07313 71.534 < 2e-16 *** da -0.29663 0.11199 -2.649 0.00808 ** --- Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 (Dispersion parameter for poisson family taken to be 1) Null deviance: 7.0932e+00 on 1 degrees of freedom Residual deviance: 9.5479e-15 on 0 degrees of freedom AIC: 17.843 Number of Fisher Scoring iterations: 2\n\nNotice how the treatment effect coefficients and standard errors are the exact\nsame results as they are with the micro observations. This is something people\nwho do regression models often do not understand. Here you don\u2019t gain power by\nhaving more observations, power in the Poisson model is determined by the\ntotal counts of things you have observed.\n\nIf this were not the case, you could just slice observations into finer time\nperiods and gain power. Instead of counts per day, why not per hour? But that\nisn\u2019t how it works when using Poisson research designs. Counter-intuitive\nperhaps, you get smaller standard errors when you observe higher counts.\n\nIt ends up being the treatment effect estimate in this scenario is easy to\ncalculate in closed form. This is just riffing off of David Wilson\u2019s work\n(Wilson, 2022).\n\n    \n    \n    treat_eff <- log(sum(control)/sum(treat)) treat_se <- sqrt(1/sum(control) + 1/sum(treat)) print(c(treat_eff,treat_se))\n\nWhich produces [1] 0.2966347 0.1119903.\n\nFor scenarios in which are slightly more complicated, such as treated/control\nhave different number of periods, you can use weights to get the same\nestimates. Here for example we have 25 periods in treated and 19 periods in\nthe control using the regression approach.\n\n    \n    \n    # Micro observations, different number of periods treat2 <- rpois(25,base*(1 - reduction)) cont2 <- rpois(19,base) val2 <- c(treat2,cont2) dum2 <- c(rep(1,25),rep(0,19)) m3 <- glm(val2 ~ dum2,family=poisson) # Aggregate, estimate rates tot2 <- c(sum(treat2),sum(cont2)) weight <- c(25,19) rate2 <- tot2/weight tagg2 <- c(1,0) # errors for non-integer values is fine m4 <- glm(rate2 ~ tagg2,weights=weight,family=poisson) print(vcov(m3)/vcov(m4)) # can see these are the same estimates summary(m4)\n\nWhich results in:\n\n    \n    \n    >print(vcov(m3)/vcov(m4)) # can see these are the same estimates (Intercept) dum2 (Intercept) 0.9999999 0.9999999 dum2 0.9999999 0.9999992 >summary(m4) Call: glm(formula = rate2 ~ tagg2, family = poisson, weights = weight) Deviance Residuals: [1] 0 0 Coefficients: Estimate Std. Error z value Pr(>|z|) (Intercept) 2.36877 0.07019 33.750 < 2e-16 *** tagg2 -0.38364 0.10208 -3.758 0.000171 *** --- Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe treatment effect estimate is similar, where the variance is still dictated\nby the counts.\n\n    \n    \n    treat_rate <- log(rate2[1]/rate2[2]) treat_serate <- sqrt(sum(1/tot2)) print(c(treat_rate,treat_serate))\n\nWhich again is [1] -0.3836361 0.1020814, same as the regression results.\n\n## Part 2, MDEs\n\nSo Ian\u2019s paper has simulation code to determine power. You can do infinite\nsums with the Poisson distribution to get closer to closed form estimates,\nlike the e-test does in my ptools package. But the simulation approach is fine\noverall, so just use Ian\u2019s code if you want power estimates.\n\nThe way power analysis works, you pick an effect size, then determine the\nstudy parameters to be able to detect that effect size a certain percentage of\nthe time (the power, typically set to 0.8 for convenience). An alternative way\nto think about the problem is how variable will your estimates be? You can\nthen back out the minimum detectable effect size (MDE), given those particular\ncounts. (Another way people talk about this is plan for precision in your\nexperiment.)\n\nLets do a few examples to illustrate. So say you wanted to know if training\nreduced conducted energy device (CED) deployments. You are randomizing\ndifferent units of the city, so you have treated and control. Baseline rates\nare around 5% per arrest, and say you have 10 arrests per day in each\ntreated/control arm of the study. Around 30 days, you will have ~15 CED\nusages. Subsequently the standard error of the logged incident rate ratio will\nbe approximately sqrt(1/15 + 1/15) = 0.37. Thus, the smallest effect size you\ncould detect has to be a logged incident rate ratio pretty much double that\nvalue.\n\nPresumably we think the intervention will decrease CED uses, so we are looking\nat an IRR of exp(-0.37*2) = 0.48. So you pretty much need to cut CED usage in\nhalf to be able to detect if the intervention worked when only examining the\noutcomes for one month. (The 2 comes from using a 95% confidence interval.)\n\nIf we say we think best case the intervention had a 20% reduction in CED\nusage, we would then need exp(-se*2) = 0.8. log(0.8) ~ -0.22, so we need a\nstandard error of se = 0.11 to meet this minimum detectable effect. If we have\nequal counts in each arm, this is approximately sqrt(1/x + 1/x) = 0.11, with\nrearranging we get 0.11^2 = 2*(1/x), and then 2/(0.11^2) = x = 166. So we want\nover 160 events in each treated/control group, to be able to detect a 20%\nreduction.\n\nNow lets imagine a scenario in which one of the arms is fixed, such as\nretrospective analysis. (Say the control group is prior time periods before\ntraining, and 100% of the patrol officers gets the training.) So we have fixed\n100 events in the control group, in that scenario, we need to monitor our\ntreatment until we observe sqrt(1/x + 1/100) = 0.11, that 20% reduction\nstandard. We can rearrange this to be 0.11^2 - 1/100 = 1/x, which is x =\n1/0.0021 = 476.\n\nWhen you have a fixed background count, in either in a treated or control arm,\nthat pretty much puts a lower bound on the standard error. In this case with\nthe control arm that has a fixed 100 events, the standard error can never be\nsmaller than sqrt(1/100) = 0.1. So in that case, you can never detect an\neffect smaller than exp(-0.2).\n\nAnother way to think about this is that with smaller effect sizes, you can\napproximately translate the standard errors to percent point ranges. So if you\nwant to say plan for precision estimates of around +/- 5% \u2013 that is a standard\nerror of 0.05. We are going to need sqrt(z) ~ 0.05. At a minimum we need 400\nevents in one of the treated or control arms, since sqrt(1/400) = 0.05 (and\nthat is only taking into account one of the arms).\n\nFor those familiar with survey stats, these are close to the same sample size\nrecommendation for proportions \u2013 it is just instead of total sample size, it\nis the total counts we are interested in. E.g. if you want +/- 5% for sample\nproportions, you want around 1,000 observations.\n\nAnd most of the examples of more complicated research designs (e.g. fixed or\nrandom effects, overdispersion estimates) will likely make the power lower,\nnot higher, than the back of the envelope estimates here. But they should be a\nuseful starting to know whether a particular experimental design is dead in\nthe water to detect reasonable effect sizes of interest.\n\nIf you found this interesting, you will probably find my work on continuous\nmonitoring of crime trends over time also interesting:\n\nThis approach relies on very similar Poisson models to what Ian is showing\nhere, you just monitor the process over time and draw the error intervals as\nyou go. For low powered designs, the intervals will just seem hopelessly wide\nover time.\n\n# References\n\n  * Adams, I. (2024) Power Simulations of Rare Event Counts and Introduction to the \u2018Power Lift\u2019 Metric. CrimRxiv\n\n  * Blattman, C., Green, D., Ortega, D., & Tob\u00f3n, S. (2018). Place-based interventions at scale: The direct and spillover effects of policing and city services on crime (No. w23941). National Bureau of Economic Research.\n\n  * Wheeler, A. P. (2016). Tables and graphs for monitoring temporal crime trends: Translating theory into practical crime analysis advice. International Journal of Police Science & Management, 18(3), 159-172.\n\n  * Wheeler, A.P., & Ratcliffe, J.H. (2018). A simple weighted displacement difference test to evaluate place based crime interventions. Crime Science, 7(1), 11.\n\n  * Wilson, D. B. (2022). The relative incident rate ratio effect size for count-based impact evaluations: When an odds ratio is not an odds ratio. Journal of Quantitative Criminology, 38(2), 323\u201334.\n\n### Share this:\n\n  * Twitter\n  * Facebook\n  * LinkedIn\n  * Pinterest\n\nLike Loading...\n\n### Related\n\nSome additional plots to go with Crime Increase DispersionFebruary 21, 2020In\n\"Crime Analysis\"\n\nTesting changes in short run crime patterns: The Poisson e-testApril 29,\n2018In \"Crime Analysis\"\n\nStaggered Treatment Effect DiD count modelsMay 30, 2022In \"Crime Analysis\"\n\nLeave a comment\n\nby apwheele on March 18, 2024 \u2022 Permalink\n\nPosted in Crime Analysis, data science, R\n\nTagged Poisson\n\nPosted by apwheele on March 18, 2024\n\nhttps://andrewpwheeler.com/2024/03/18/poisson-designs-and-minimum-detectable-\neffects/\n\nPrevious Post Harmweighted hotspots, using ESRI python API, and Crime De-Coder\nUpdates\n\nNext Post Matching mentors to mentees using OR-tools\n\nLeave a comment\n\n### Leave a comment Cancel reply\n\n  * ## Recent Posts\n\n    * Grabbing the NHAMCS emergency room data in python\n    * Matching mentors to mentees using OR-tools\n    * Poisson designs and Minimum Detectable Effects\n    * Harmweighted hotspots, using ESRI python API, and Crime De-Coder Updates\n    * Getting started with github notes\n  * ## Categories\n\n  * ## Site RSS Feeds\n\n    * RSS - Posts\n    * RSS - Comments\n  * Join 372 other subscribers\n\n  * aoristic cartography census choropleth citeulike color cost-benefit courses crime-mapping crime-trends Crime Analysis Criminal Justice data-manipulation data visualization deep-learning excel flow-data geocoding ggplot2 github google-streetview-api grammar of graphics group-based-trajectory gun-violence healthcare homicide-rates hot spots hypothesis-testing kernel-density linear programming logistic-regression machine-learning MACRO mapping matplotlib meta multi-level negative-binomial network NetworkX officer-involved-shooting open-science paper Papers peer-review Poisson prediction Predictive-Policing presentation Python Python-programability pytorch quasi-experiment r recidivism regression resources scatterplot scholarly seaborn shootings simulation slopegraph small-multiples social-networking SPSS stackexchange Stata statistics survey time-series uncertainty wdd web-scraping writing\n\n  * ## Top Posts & Pages\n\n    * Git excluding specific files when merging branches\n    * Testing the equality of two regression coefficients\n    * Making nice margin plots in Stata\n  * ## Stack Exchange\n\nBlog at WordPress.com.\n\n  * Comment\n  * Reblog\n  * Subscribe Subscribed\n\n    * Andrew Wheeler\n    * Already have a WordPress.com account? Log in now.\n\n  * Privacy\n  *     * Andrew Wheeler\n    * Customize\n    * Subscribe Subscribed\n    * Sign up\n    * Log in\n    * Copy shortlink\n    * Report this content\n    * View post in Reader\n    * Manage subscriptions\n    * Collapse this bar\n\n%d\n\n", "frontpage": false}
