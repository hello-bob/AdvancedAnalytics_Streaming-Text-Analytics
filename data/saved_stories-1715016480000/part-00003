{"aid": "40271457", "title": "How LLMs Work, Explained Without Math \u2013 Miguel Grinberg", "url": "https://blog.miguelgrinberg.com/post/how-llms-work-explained-without-math", "domain": "miguelgrinberg.com", "votes": 13, "user": "kdamica", "posted_at": "2024-05-06 05:40:02", "comments": 0, "source_title": "How LLMs Work, Explained Without Math", "source_text": "How LLMs Work, Explained Without Math - miguelgrinberg.com\n\n# How LLMs Work, Explained Without Math\n\n## Posted by on under\n\nI'm sure you agree that it has become impossible to ignore Generative AI\n(GenAI), as we are constantly bombarded with mainstream news about Large\nLanguage Models (LLMs). Very likely you have tried ChatGPT, maybe even keep it\nopen all the time as an assistant.\n\nA basic question I think a lot of people have about the GenAI revolution is\nwhere does the apparent intelligence these models have come from. In this\narticle, I'm going to attempt to explain in simple terms and without using\nadvanced math how generative text models work, to help you think about them as\ncomputer algorithms and not as magic.\n\n## What Does An LLM Do?\n\nI'll begin by clearing a big misunderstanding people have regarding how Large\nLanguage Models work. The assumption that most people make is that these\nmodels can answer questions or chat with you, but in reality all they can do\nis take some text you provide as input and guess what the next word (or more\naccurately, the next token) is going to be. Let's start to unravel the mystery\nof LLMs from the tokens.\n\n### Tokens\n\nA token is the basic unit of text understood by the LLM. It is convenient to\nthink of tokens as words, but for the LLM the goal is to encode text as\nefficiently as possible, so in many cases tokens represent sequences of\ncharacters that are shorter or longer than whole words. Punctuation symbols\nand spaces are also represented as tokens, either individually or grouped with\nother characters.\n\nThe complete list of tokens used by an LLM are said to be the LLM's\nvocabulary, since it can be used to express any possible text. The byte pair\nencoding (BPE) algorithm is commonly used by LLMs to generate a token\nvocabulary given an input dataset. Just so that you have some rough idea of\nscale, the GPT-2 language model, which is open source and can be studied in\ndetail, uses a vocabulary of 50,257 tokens.\n\nEach token in an LLM's vocabulary is given a unique identifier, usually a\nnumber. The LLM uses a tokenizer to convert between regular text given as a\nstring and an equivalent sequence of tokens, given as a list of token numbers.\nIf you are familiar with Python and want to play with tokens, you can install\nthe tiktoken package from OpenAI:\n\n    \n    \n    $ pip install tiktoken\n\nThen try this in a Python prompt:\n\n    \n    \n    >>> import tiktoken >>> encoding = tiktoken.encoding_for_model(\"gpt-2\") >>> encoding.encode(\"The quick brown fox jumps over the lazy dog.\") [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13] >>> encoding.decode([464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13]) 'The quick brown fox jumps over the lazy dog.' >>> encoding.decode([464]) 'The' >>> encoding.decode([2068]) ' quick' >>> encoding.decode([13]) '.'\n\nYou can see in this experiment that for the GPT-2 language model token 464\nrepresents the word \"The\", and token 2068 represents the word \" quick\",\nincluding a leading space. This model uses token 13 for the period.\n\nBecause tokens are determined algorithmically, you may find strange things,\nsuch as these three variants of the word \"the\", all encoded as different\ntokens by GPT-2:\n\n    \n    \n    >>> encoding.encode('The') [464] >>> encoding.encode('the') [1169] >>> encoding.encode(' the') [262]\n\nThe BPE algorithm doesn't always map entire words to tokens. In fact, words\nthat are less frequently used do not get to be their own token and have to be\nencoded with multiple tokens. Here is an example of a word that this model\nencodes with two tokens:\n\n    \n    \n    >>> encoding.encode(\"Payment\") [19197, 434] >>> encoding.decode([19197]) 'Pay' >>> encoding.decode([434]) 'ment'\n\n### Next Token Predictions\n\nGiven some text, a language model maked predictions about what token will\nfollow right after. If it helps to see this with Python pseudo-code, here is\nhow you could run one of these models to get predictions for the next token:\n\n    \n    \n    predictions = get_token_predictions(['The', ' quick', ' brown', ' fox'])\n\nThe function gets a list of input tokens, which are encoded from the prompt\nprovided by the user. In this example I'm assuming words are all individual\ntokens. To keep things simple I'm using the textual representation of each\ntoken, but as you've seen before in reality each token will be passed to the\nmodel as a number.\n\nThe returned value of this function is a data structure that assigns each\ntoken in the vocabulary a probability to follow the input text. If this was\nbased on GPT-2, the return value of the function would be a list of 50,257\nfloating point numbers, each predicting a probability that the corresponding\ntoken will come next.\n\nIn the example above you could imagine that a well trained language model will\ngive the token \"jumps\" a high probability to follow the partial phrase \"The\nquick brown fox\" that I used as prompt. Once again assuming a model trained\nappropriately, you could also imagine that the probability of a random word\nsuch as \"potato\" continuing this phrase is going to be much lower and close to\n0.\n\nTo be able to produce reasonable predictions, the language model has to go\nthrough a training process. During training, it is presented with lots and\nlots of text to learn from. At the end of the training, the model is able to\ncalculate next token probabilities for a given token sequence using data\nstructures that it has built using all the text that it saw in training.\n\nIs this different from what you expected? I hope this is starting to look less\nmagical now.\n\n### Generating Long Text Sequences\n\nSince the model can only predict what the next token is going to be, the only\nway to make it generate complete sentences is to run the model multiple times\nin a loop. With each loop iteration a new token is generated, chosen from the\nreturned probabilities. This token is then added to the input that is given to\nthe model on the next iteration of the loop, and this continues until\nsufficient text has been generated.\n\nLet's look at a more complete Python pseudo-code showing how this would work:\n\n    \n    \n    def generate_text(prompt, num_tokens, hyperparameters): tokens = tokenize(prompt) for i in range(num_tokens): predictions = get_token_predictions(tokens) next_token = select_next_token(predictions, hyperparameters) tokens.append(next_token) return ''.join(tokens)\n\nThe generate_text() function takes a user prompt as an argument. This could\nbe, for example, a question.\n\nThe tokenize() helper function converts the prompt to an equivalent list of\ntokens, using tiktoken or a similar library. Inside the for-loop, the\nget_token_predictions() function is where the AI model is called to get the\nprobabilitles for the next token, as in the previous example.\n\nThe job of the select_next_token() function is to take the next token\nprobabilities (or predictions) and pick the best token to continue the input\nsequence. The function could just pick the token with the highest probability,\nwhich in machine learning is called a greedy selection. Better yet, it can\npick a token using a random number generator that honors the probabilities\nreturned by the model, and in that way add some variety to the generated text.\nThis will also make the model produce different responses if given the same\nprompt multiple times.\n\nTo make the token selection process even more flexible, the probabilities\nreturned by the LLM can be modified using hyperparameters, which are passed to\nthe text generation function as arguments. The hyperparameters allow you to\ncontrol the \"greediness\" of the token selection process. If you have used\nLLMs, you are likely familiar with the temperature hyperparameter. With a\nhigher temperature, the token probabilities are flattened out, and this\naugments the chances of less likely tokens to be selected, with the end result\nof making the generated text look more creative or unusual. You may have also\nused two other hyperparameters called top_p and top_k, which control how many\nof the highest probable tokens are considered for selection.\n\nOnce a token has been selected, the loop iterates and now the model is given\nan input that includes the new token at the end, and one more token is\ngenerated to follow it. The num_tokens argument controls how many iterations\nto run the loop for, or in other words, how much text to generate. The\ngenerated text can (and often does) end mid-sentence, because the LLM has no\nconcept of sentences or paragraphs, since it just works on one token at a\ntime. To prevent the generated text from ending in the middle of a sentence,\nwe could consider the num_tokens argument as a maximum instead of an exact\nnumber of tokens to generate, and in that case we could stop the loop when a\nperiod token is generated.\n\nIf you've reached this point and understood everything then congratulations,\nyou now know how LLMs work at a high level. Are you interested in more\ndetails? In the next section I'll get a bit more technical, while still doing\nmy best to avoid referencing the math that supports this technology, which is\nquite advanced.\n\n## Model Training\n\nUnfortunately, discussing how a model is trained is actually difficult without\nusing math. What I'm going to do is start by showing you a very simple\ntraining approach.\n\nGiven that the task is to predict tokens that follow other tokens, a simple\nway to train a model is to get all the pairs of consecutive tokens that appear\nin the training dataset and build a table of probabilities with them.\n\nLet's do this with a short vocabulary and dataset. Let's say the model's\nvocabulary has the following five tokens:\n\n    \n    \n    ['I', 'you', 'like', 'apples', 'bananas']\n\nTo keep this example short and simple, I'm not going to consider spaces or\npunctuation symbols as tokens.\n\nLet's use a training dataset that is composed of three sentences:\n\n  * I like apples\n  * I like bananas\n  * you like bananas\n\nWe can build a 5x5 table and in each cell write how many times the token\nrepresenting the row of the cell is followed by the token representing the\ncolumn. Here is the table built from the three sentences in the dataset:\n\n-| I| you| like| apples| bananas  \n---|---|---|---|---|---  \nI| 2  \nyou| 1  \nlike| 1| 2  \napples  \nbananas  \n  \nHopefully this is clear. The dataset has two instances of \"I like\", one\ninstance of \"you like\", one instance of \"like apples\" and two of \"like\nbananas\".\n\nNow that we know how many times each pair of tokens appeared in the training\ndataset, we can calculate the probabilities of each token following each\nother. To do this, we convert the numbers in each row to probabilities. For\nexample, token \"like\" in the middle row of the table was followed once by\n\"apples\" and twice by \"bananas\". That means that \"apples\" follows \"like\" 33.3%\nof the time, and \"bananas\" follows it the remaining 66.7%.\n\nHere is the complete table with all the probabilities calculated. Empty cells\nhave a probability of 0%.\n\n-| I| you| like| apples| bananas  \n---|---|---|---|---|---  \nI| 100%  \nyou| 100%  \nlike| 33.3%| 66.7%  \napples| 25%| 25%| 25%| 25%  \nbananas| 25%| 25%| 25%| 25%  \n  \nThe rows for \"I\", \"you\" and \"like\" are easy to calculate, but \"apples\" and\n\"bananas\" present a problem because they have no data at all, since the\ndataset does not have any examples with these tokens being followed by other\ntokens. Here we have a \"hole\" in our training, so to make sure that the model\nproduces a prediction even when lacking training, I have decided to split the\nprobabilities for a follow-up token for \"apples\" and \"bananas\" evenly across\nthe other four possible tokens, which could obviously generate strange\nresults, but at least the model will not get stuck when it reaches one of\nthese two tokens.\n\nThe problem of holes in training data is actually important. In real LLMs the\ntraining datasets are very large, so you would not find training holes that\nare so obvious as in my tiny example above. But smaller, more difficult to\ndetect holes due to low coverage in the training data do exist and are fairly\ncommon. The quality of the token predictions the LLM makes in these poorly\ntrained areas can be bad, but often in ways that are difficult to perceive.\nThis is one of the reasons LLMs can sometimes hallucinate, which happens when\nthe generated text reads well, but contains factual errors or inconsistencies.\n\nUsing the probabilities table above, you may now imagine how an implementation\nof the get_token_predictions() function would work. In Python pseudo-code it\nwould be something like this:\n\n    \n    \n    def get_token_predictions(input_tokens): last_token = input_tokens[-1] return probabilities_table[last_token]\n\nSimpler than expected, right? The function accepts a sequence of tokens, which\ncome from the user prompt. It takes the last token in the sequence, and\nreturns the row in the probabilities table that corresponds to that token.\n\nIf you were to call this function with ['you', 'like'] as input tokens, for\nexample, the function would return the row for \"like\", which gives the token\n\"apples\" a 33.3% chance of continuing the sentence, and the token \"bananas\"\nthe other 66.7%. With these probabilities, the select_next_token() function\nshown above should choose \"apples\" one out of three times.\n\nWhen the \"apples\" token is selected as a continuation of \"you like\", the\nsentence \"you like apples\" will be formed. This is an original sentence that\ndid not exist in the training dataset, yet it is perfectly reasonable.\nHopefully you are starting to get an idea of how these models can come up with\nwhat appears to be original ideas or concepts, just by reusing patterns and\nstitching together different bits of what they learned in training.\n\n### The Context Window\n\nThe approach I took in the previous section to train my mini-language model is\ncalled a Markov chain.\n\nAn issue with this technique is that only one token (the last of the input) is\nused to make a prediction. Any text that appears before that last token\ndoesn't have any influence when choosing how to continue, so we can say that\nthe context window of this solution is equal to one token, which is very\nsmall. With such a small context window the model constantly \"forgets\" its\nline of thought and jumps from one word to the next without much consistency.\n\nTo improve the model's predictions a larger probabilities table can be\nconstructed. To use a context window of two tokens, additional table rows\nwould have to be added with rows that represent all possible sequences of two\ntokens. With the five tokens I used in the example there would be 25 new rows\nin the probabilities table each for a pair of tokens, added to the 5 single-\ntoken rows that are already there. The model would have to be trained again,\nthis time looking at groups of three tokens in addition to the pairs. Then in\neach loop iteration of the get_token_predictions() function the last two\ntokens from the input would be used when available, to find the corresponding\nrow in the larger probabilities table.\n\nBut a context window of 2 tokens is still insufficient. For the generated text\nto be consistent with itself and make at least some basic sense, a much larger\ncontext window is needed. Without a large enough context it is impossible for\nnewly generated tokens to relate to concepts or ideas expressed in previous\ntokens. So what can we do? Increasing the context window to 3 tokens would add\n125 additional rows to the probabilities table, and the quality would still be\nvery poor. How large do we need to make the context window?\n\nThe open source GPT-2 model from OpenAI uses a context window of 1024 tokens.\nTo be able to implement a context window of this size using Markov chains,\neach row of the probabilities table would have to represent a sequence that is\nbetween 1 and 1024 tokens long. Using the above example vocabulary of 5\ntokens, there are 5^1024 possible sequences that are 1024 tokens long. How\nmany table rows are required to represent this? I did the calculation in a\nPython session (scroll to the right to see the complete number):\n\n    \n    \n    >>> pow(5, 1024) 55626846462680034577255817933310101605480399511558295763833185422180110870347954896357078975312775514101683493275895275128810854038836502721400309634442970528269449838300058261990253686064590901798039126173562593355209381270166265416453973718012279499214790991212515897719252957621869994522193843748736289511290126272884996414561770466127838448395124802899527144151299810833802858809753719892490239782222290074816037776586657834841586939662825734294051183140794537141608771803070715941051121170285190347786926570042246331102750604036185540464179153763503857127117918822547579033069472418242684328083352174724579376695971173152319349449321466491373527284227385153411689217559966957882267024615430273115634918212890625\n\nThat is a lot of rows! And this is only a portion of the table, since we would\nalso need sequences that are 1023 tokens long, 1022, etc., all the way to 1,\nsince we want to make sure shorter sequences can also be handled when not\nenough tokens are available in the input. Markov chains are fun to work with,\nbut they do have a big scalability problem.\n\nAnd a context window of 1024 tokens isn't even that great anymore. With GPT-3,\nthe context window was increased to 2048 tokens, then increased to 4096 in\nGPT-3.5. GPT-4 started with 8192 tokens, later got increased to 32K, and then\nagain to 128K (that's right, 128,000 tokens!). Models with 1M or larger\ncontext windows are starting to appear now, allowing models to have much\nbetter consistency and recall when they make token predictions.\n\nIn conclusion, Markov chains allow us to think about the problem of text\ngeneration in the right way, but they have big issues that prevent us from\nconsidering them as a viable solution.\n\n### From Markov Chains to Neural Networks\n\nObviously we have to forget the idea of having a table of probabilities, since\na table for a reasonable context window would require an impossibly large\namount of RAM. What we can do is replace the table with a function that\nreturns an approximation of what the token probabilities would be, generated\nalgorithmically instead of stored as a big table. This is actually something\nthat neural networks can do well.\n\nA neural network is a special type of function that takes some inputs,\nperforms some calculations on them, and returns an output. For a language\nmodel the inputs are the tokens that represent the prompt, and the output is\nthe list of predicted probabilities for the next token.\n\nI said neural networks are \"special\" functions. What makes them special is\nthat in addition to the function logic, the calculations they perform on the\ninputs are controlled by a number of externally defined parameters. Initially,\nthe parameters of the network are not known, and as a result, the function\nproduces and output that is completely useless. The training process for the\nneural network consists in finding the parameters that make the function\nperform the best when evaluated on the data from the training dataset, with\nthe assumption that if the function works well with the training data it will\nwork comparably well with other data.\n\nDuring the training process, the parameters are iteratively adjusted in small\nincrements using an algorithm called backpropagation which is heavy on math,\nso I won't discuss in this article. With each adjustment, the predictions of\nthe neural network are expected to become a tiny bit better. The updated\nnetwork is evaluated again against the training dataset, and the results\ninform the next round of adjustments. This process continues until the\nfunction performs good next token predictions on the training dataset.\n\nTo help you have an idea of the scale at which neural networks work, consider\nthat the GPT-2 model has about 1.5 billion parameters, and GPT-3 increased the\nparameter count to 175 billion. GPT-4 is said to have about 1.76 trillion\nparameters. Training neural networks at this scale with current generation\nhardware takes a very long time, usually weeks or months.\n\nWhat is interesting is that because there are so many parameters, all\ncalculated through a lengthy iterative process without human assistance, it is\ndifficult to understand how a model works. A trained LLM is like a black box\nthat is extremely difficult to debug, because most of the \"thinking\" of the\nmodel is hidden in the parameters. Even those who trained it have trouble\nexplaining its inner workings.\n\n### Layers, Transformers and Attention\n\nYou may be curious to know what mysterious calculations happen inside the\nneural network function that can, with the help of well tuned parameters, take\na list of input tokens and somehow output reasonable probabilities for the\ntoken that follows.\n\nA neural network is configured to perform a chain of operations, each called a\nlayer. The first layer receives the inputs, and performs some type of\ntransformation on them. The transformed inputs enter the next layer and are\ntransformed once again. This continues until the data reaches the final layer\nand is transformed one last time, generating the output, or prediction.\n\nMachine learning experts come up with different types of layers that perform\nmathematical transformations on the input data, and they also figure out ways\nto organize layers in ways that achieve a desired result. Some layers are of a\ngeneral purpose, while others are designed to work on a specific type of input\ndata, such as images or as in the case of LLMs, on tokenized text.\n\nThe neural network architecture that is the most popular today for text\ngeneration in large language models is called the Transformer. LLMs that use\nthis design are said to be GPTs, or Generative Pre-Trained Transformers.\n\nThe distinctive characteristic of transformer models is a layer calculation\nthey perform called Attention, that allows them to derive relationships and\npatterns between tokens that are in the context window, which are then\nreflected in the resulting probabilities for the next token.\n\nThe Attention mechanism was initially used in language translators, as a way\nto find which tokens in an input sequence are the most important to extract\nits meaning. This mechanism gives modern translators the ability to\n\"understand\" a sentence at a basic level, by focusing (or driving \"attention\")\nto the important words or tokens.\n\n## Do LLMs Have Intelligence?\n\nBy now you may be starting to form an opinion on wether LLMs show some form of\nintelligence in the way they generate text.\n\nI personally do not see LLMs as having an ability to reason or come up with\noriginal thoughts, but that does not mean to say they're useless. Thanks to\nthe clever calculations they perform on the tokens that are in the context\nwindow, LLMs are able to pick up on patterns that exist in the user prompt and\nmatch them to similar patterns learned during training. The text they generate\nis formed from bits and pieces of training data for the most part, but the way\nin which they stitch words (tokens, really) together is highly sophisticated,\nin many cases producing results that feel original and useful.\n\nOn the other side, given the propensity of LLMs to hallucinate, I wouldn't\ntrust any workflow in which the LLM produces output that goes straight to end\nusers without verification by a human.\n\nWill the larger LLMs that are going to appear in the following months or years\nachieve anything that resembles true intelligence? I feel this isn't going to\nhappen with the GPT architecture due to its many limitations, but who knows,\nmaybe with some future innovations we'll get there.\n\n## The End\n\nThank you for staying with me until the end! I hope I have picked your\ninterested enough for you to decide to continue learning, and eventually\nfacing all that scary math that you cannot avoid if you want to understand\nevery detail. In that case, I can't recommend Andrej Karpathy's Neural\nNetworks: Zero to Hero video series enough.\n\n##### Become a Patron!\n\nHello, and thank you for visiting my blog! If you enjoyed this article, please\nconsider supporting my work on this blog on Patreon!\n\n##### Share this post:\n\nHacker News\n\nReddit\n\nTwitter\n\nLinkedIn\n\nFacebook\n\nE-Mail\n\nNo comments yet\n\n### Leave a Comment\n\nMicroPython for the Raspberry Pi Pico W\n\nIf you like my MicroPython tutorial series on this blog, you may also like my\nMicroPython for the Raspberry Pi Pico W book.\n\nClick here to get this Book!\n\nAbout Miguel\n\nWelcome to my blog!\n\nI'm a software engineer and technical writer, currently living in Drogheda,\nIreland.\n\nYou can also find me on Twitter, Mastodon, Github, LinkedIn, YouTube, Facebook\nand Patreon.\n\nThank you for visiting!\n\nCategories\n\n\u00a9 2012-2024 by Miguel Grinberg. All rights reserved. Questions?\n\n", "frontpage": true}
