{"aid": "40108932", "title": "Versatile Systolic Array for Sparse and Dense Matrix Multiplications", "url": "https://www.mdpi.com/2079-9292/13/8/1500", "domain": "mdpi.com", "votes": 1, "user": "PaulHoule", "posted_at": "2024-04-21 20:27:36", "comments": 0, "source_title": "VerSA: Versatile Systolic Array Architecture for Sparse and Dense Matrix Multiplications", "source_text": "Electronics | Free Full-Text | VerSA: Versatile Systolic Array Architecture for Sparse and Dense Matrix Multiplications\n\nLoading [MathJax]/jax/output/HTML-CSS/fonts/Gyre-Pagella/Main/Regular/Main.js\n\n  * Consent\n  * Details\n  * [#IABV2SETTINGS#]\n  * About\n\n## This website uses cookies\n\nWe use cookies to personalise content and ads, to provide social media\nfeatures and to analyse our traffic. We also share information about your use\nof our site with our social media, advertising and analytics partners who may\ncombine it with other information that you\u2019ve provided to them or that they\u2019ve\ncollected from your use of their services.\n\nShow details\n\n  * Necessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies.\n\n    * Cookiebot\n\n1\n\nLearn more about this provider\n\n1.gifUsed to count the number of sessions to the website, necessary for\noptimizing CMP product delivery.\n\nExpiry: SessionType: Pixel\n\n    * Crazyegg\n\n2\n\nLearn more about this provider\n\n_ce.cchStores the user's cookie consent state for the current domain\n\nExpiry: SessionType: HTTP\n\nce_successful_csp_checkDetects whether user behaviour tracking should be\nactive on the website.\n\nExpiry: PersistentType: HTML\n\n    * Google\n\n1\n\nLearn more about this provider\n\ntest_cookieUsed to check if the user's browser supports cookies.\n\nExpiry: 1 dayType: HTTP\n\n    * LinkedIn\n\n2\n\nLearn more about this provider\n\nli_gcStores the user's cookie consent state for the current domain\n\nExpiry: 180 daysType: HTTP\n\nbscookieThis cookie is used to identify the visitor through an application.\nThis allows the visitor to login to a website through their LinkedIn\napplication for example.\n\nExpiry: 1 yearType: HTTP\n\n    * commenting.mdpi.com\n\n2\n\nSESS#Preserves users states across page requests.\n\nExpiry: SessionType: HTTP\n\nXSRF-TOKENEnsures visitor browsing-security by preventing cross-site request\nforgery. This cookie is essential for the security of the website and visitor.\n\nExpiry: SessionType: HTTP\n\n    * commenting.mdpi.com consent.cookiebot.com\n\n2\n\nCookieConsent [x2]Stores the user's cookie consent state for the current\ndomain\n\nExpiry: 1 yearType: HTTP\n\n    * matomo.mdpi.com\n\n1\n\n_pk_testcookie_domainThis cookie determines whether the browser accepts\ncookies.\n\nExpiry: 1 dayType: HTTP\n\n    * mdpi.com\n\n3\n\n__cfruidThis cookie is a part of the services provided by Cloudflare -\nIncluding load-balancing, deliverance of website content and serving DNS\nconnection for website operators.\n\nExpiry: SessionType: HTTP\n\ncf_clearanceThis cookie is used to distinguish between humans and bots.\n\nExpiry: 1 yearType: HTTP\n\nMDPIPHPSESSIDPending\n\nExpiry: SessionType: HTTP\n\n    * mdpi.com mdpi.org mdpi-res.com sciprofiles.com\n\n4\n\n__cf_bm [x4]This cookie is used to distinguish between humans and bots. This\nis beneficial for the website, in order to make valid reports on the use of\ntheir website.\n\nExpiry: 1 dayType: HTTP\n\n    * www.jisc.ac.uk\n\n2\n\nAWSALBRegisters which server-cluster is serving the visitor. This is used in\ncontext with load balancing, in order to optimize user experience.\n\nExpiry: 7 daysType: HTTP\n\nAWSALBCORSRegisters which server-cluster is serving the visitor. This is used\nin context with load balancing, in order to optimize user experience.\n\nExpiry: 7 daysType: HTTP\n\n    * www.mdpi.com\n\n7\n\ncf_chl_1This cookie is a part of the services provided by Cloudflare -\nIncluding load-balancing, deliverance of website content and serving DNS\nconnection for website operators.\n\nExpiry: 1 dayType: HTTP\n\niconify0Used by the website's content management system (CMS) to determine how\nthe website's menu-tabs should be displayed.\n\nExpiry: PersistentType: HTML\n\niconify1This cookie is set to ensure proper product displays on the website.\n\nExpiry: PersistentType: HTML\n\niconify2Used by the website's content management system (CMS) to determine how\nthe website's menu-tabs should be displayed.\n\nExpiry: PersistentType: HTML\n\niconify3Determines the device used to access the website. This allows the\nwebsite to be formatted accordingly.\n\nExpiry: PersistentType: HTML\n\niconify-countUsed by the website's content management system (CMS) to\ndetermine how the website's menu-tabs should be displayed.\n\nExpiry: PersistentType: HTML\n\niconify-versionUsed by the website's content management system (CMS) to\ndetermine how the website's menu-tabs should be displayed.\n\nExpiry: PersistentType: HTML\n\n  * Preference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in.\n\n    * LinkedIn\n\n1\n\nLearn more about this provider\n\nlidcRegisters which server-cluster is serving the visitor. This is used in\ncontext with load balancing, in order to optimize user experience.\n\nExpiry: 1 dayType: HTTP\n\n    * www.mdpi.com\n\n2\n\nmdpi_layout_typeThis cookie is used to store user setting of using fixed\ndesktop layout instead of the default responsive layout\n\nExpiry: 1 yearType: HTTP\n\nsettingsThis cookie is used to determine the preferred language of the visitor\nand sets the language accordingly on the website, if possible.\n\nExpiry: PersistentType: HTML\n\n  * Statistic cookies help website owners to understand how visitors interact with websites by collecting and reporting information anonymously.\n\n    * Crazyegg\n\n8\n\nLearn more about this provider\n\n_ce.clock_dataCollects data on the user\u2019s navigation and behavior on the\nwebsite. This is used to compile statistical reports and heatmaps for the\nwebsite owner.\n\nExpiry: 1 dayType: HTTP\n\n_ce.clock_eventCollects data on the user\u2019s navigation and behavior on the\nwebsite. This is used to compile statistical reports and heatmaps for the\nwebsite owner.\n\nExpiry: 1 dayType: HTTP\n\n_ce.gtldHolds which URL should be presented to the visitor when visiting the\nsite.\n\nExpiry: SessionType: HTTP\n\n_ce.sCollects data on the user\u2019s navigation and behavior on the website. This\nis used to compile statistical reports and heatmaps for the website owner.\n\nExpiry: 1 yearType: HTTP\n\ncebsTracks the individual sessions on the website, allowing the website to\ncompile statistical data from multiple visits. This data can also be used to\ncreate leads for marketing purposes.\n\nExpiry: SessionType: HTTP\n\ncebsp_Collects data on the user\u2019s navigation and behavior on the website. This\nis used to compile statistical reports and heatmaps for the website owner.\n\nExpiry: SessionType: HTTP\n\nce_fvdCollects data on the user\u2019s navigation and behavior on the website. This\nis used to compile statistical reports and heatmaps for the website owner.\n\nExpiry: PersistentType: HTML\n\ncetabidSets a unique ID for the session. This allows the website to obtain\ndata on visitor behaviour for statistical purposes.\n\nExpiry: SessionType: HTML\n\n    * Google\n\n5\n\nLearn more about this provider\n\ncollectUsed to send data to Google Analytics about the visitor's device and\nbehavior. Tracks the visitor across devices and marketing channels.\n\nExpiry: SessionType: Pixel\n\n_gaRegisters a unique ID that is used to generate statistical data on how the\nvisitor uses the website.\n\nExpiry: 2 yearsType: HTTP\n\n_ga_#Used by Google Analytics to collect data on the number of times a user\nhas visited the website as well as dates for the first and most recent visit.\n\nExpiry: 2 yearsType: HTTP\n\n_gatUsed by Google Analytics to throttle request rate\n\nExpiry: 1 dayType: HTTP\n\n_gidRegisters a unique ID that is used to generate statistical data on how the\nvisitor uses the website.\n\nExpiry: 1 dayType: HTTP\n\n    * Hotjar\n\n5\n\nLearn more about this provider\n\nhjActiveViewportIdsThis cookie contains an ID string on the current session.\nThis contains non-personal information on what subpages the visitor enters \u2013\nthis information is used to optimize the visitor's experience.\n\nExpiry: PersistentType: HTML\n\nhjViewportIdSaves the user's screen size in order to adjust the size of images\non the website.\n\nExpiry: SessionType: HTML\n\n_hjSession_#Collects statistics on the visitor's visits to the website, such\nas the number of visits, average time spent on the website and what pages have\nbeen read.\n\nExpiry: 1 dayType: HTTP\n\n_hjSessionUser_#Collects statistics on the visitor's visits to the website,\nsuch as the number of visits, average time spent on the website and what pages\nhave been read.\n\nExpiry: 1 yearType: HTTP\n\n_hjTLDTestRegisters statistical data on users' behaviour on the website. Used\nfor internal analytics by the website operator.\n\nExpiry: SessionType: HTTP\n\n    * LinkedIn\n\n1\n\nLearn more about this provider\n\nAnalyticsSyncHistoryUsed in connection with data-synchronization with third-\nparty analysis service.\n\nExpiry: 30 daysType: HTTP\n\n    * Twitter Inc.\n\n1\n\nLearn more about this provider\n\npersonalization_idThis cookie is set by Twitter - The cookie allows the\nvisitor to share content from the website onto their Twitter profile.\n\nExpiry: 400 daysType: HTTP\n\n    * matomo.mdpi.com\n\n2\n\n_pk_id#Collects statistics on the user's visits to the website, such as the\nnumber of visits, average time spent on the website and what pages have been\nread.\n\nExpiry: 1 yearType: HTTP\n\n_pk_ses#Used by Piwik Analytics Platform to track page requests from the\nvisitor during the session.\n\nExpiry: 1 dayType: HTTP\n\n    * www.mdpi.com\n\n1\n\nsentryReplaySessionRegisters data on visitors' website-behaviour. This is used\nfor internal analysis and website optimization.\n\nExpiry: SessionType: HTML\n\n  * Marketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers.\n\n    * Meta Platforms, Inc.\n\n3\n\nLearn more about this provider\n\nlastExternalReferrerDetects how the user reached the website by registering\ntheir last URL-address.\n\nExpiry: PersistentType: HTML\n\nlastExternalReferrerTimeDetects how the user reached the website by\nregistering their last URL-address.\n\nExpiry: PersistentType: HTML\n\n_fbpUsed by Facebook to deliver a series of advertisement products such as\nreal time bidding from third party advertisers.\n\nExpiry: 3 monthsType: HTTP\n\n    * Google\n\n2\n\nLearn more about this provider\n\npagead/1p-user-list/#Tracks if the user has shown interest in specific\nproducts or events across multiple websites and detects how the user navigates\nbetween sites. This is used for measurement of advertisement efforts and\nfacilitates payment of referral-fees between websites.\n\nExpiry: SessionType: Pixel\n\ntdRegisters statistical data on users' behaviour on the website. Used for\ninternal analytics by the website operator.\n\nExpiry: SessionType: Pixel\n\n    * LinkedIn\n\n4\n\nLearn more about this provider\n\nbcookieUsed by the social networking service, LinkedIn, for tracking the use\nof embedded services.\n\nExpiry: 1 yearType: HTTP\n\nli_sugrCollects data on user behaviour and interaction in order to optimize\nthe website and make advertisement on the website more relevant.\n\nExpiry: 3 monthsType: HTTP\n\nUserMatchHistoryEnsures visitor browsing-security by preventing cross-site\nrequest forgery. This cookie is essential for the security of the website and\nvisitor.\n\nExpiry: 30 daysType: HTTP\n\nli_adsIdCollects data on user behaviour and interaction in order to optimize\nthe website and make advertisement on the website more relevant.\n\nExpiry: PersistentType: HTML\n\n    * Twitter Inc.\n\n3\n\nLearn more about this provider\n\ni/adsct [x2]The cookie is used by Twitter.com in order to determine the number\nof visitors accessing the website through Twitter advertisement content.\n\nExpiry: SessionType: Pixel\n\nmuc_adsCollects data on user behaviour and interaction in order to optimize\nthe website and make advertisement on the website more relevant.\n\nExpiry: 400 daysType: HTTP\n\n    * YouTube\n\n22\n\nLearn more about this provider\n\n#-#Pending\n\nExpiry: SessionType: HTML\n\niU5q-!O9@$Registers a unique ID to keep statistics of what videos from YouTube\nthe user has seen.\n\nExpiry: SessionType: HTML\n\nLAST_RESULT_ENTRY_KEYUsed to track user\u2019s interaction with embedded content.\n\nExpiry: SessionType: HTTP\n\nLogsDatabaseV2:V#||LogsRequestsStorePending\n\nExpiry: PersistentType: IDB\n\nnextIdUsed to track user\u2019s interaction with embedded content.\n\nExpiry: SessionType: HTTP\n\nremote_sidNecessary for the implementation and functionality of YouTube video-\ncontent on the website.\n\nExpiry: SessionType: HTTP\n\nrequestsUsed to track user\u2019s interaction with embedded content.\n\nExpiry: SessionType: HTTP\n\nServiceWorkerLogsDatabase#SWHealthLogNecessary for the implementation and\nfunctionality of YouTube video-content on the website.\n\nExpiry: PersistentType: IDB\n\nTESTCOOKIESENABLEDUsed to track user\u2019s interaction with embedded content.\n\nExpiry: 1 dayType: HTTP\n\nVISITOR_INFO1_LIVETries to estimate the users' bandwidth on pages with\nintegrated YouTube videos.\n\nExpiry: 180 daysType: HTTP\n\nVISITOR_PRIVACY_METADATAStores the user's cookie consent state for the current\ndomain\n\nExpiry: 180 daysType: HTTP\n\nYSCRegisters a unique ID to keep statistics of what videos from YouTube the\nuser has seen.\n\nExpiry: SessionType: HTTP\n\nyt.innertube::nextIdRegisters a unique ID to keep statistics of what videos\nfrom YouTube the user has seen.\n\nExpiry: PersistentType: HTML\n\nytidb::LAST_RESULT_ENTRY_KEYStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: PersistentType: HTML\n\nYtIdbMeta#databasesUsed to track user\u2019s interaction with embedded content.\n\nExpiry: PersistentType: IDB\n\nyt-remote-cast-availableStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-cast-installedStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-connected-devicesStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: PersistentType: HTML\n\nyt-remote-device-idStores the user's video player preferences using embedded\nYouTube video\n\nExpiry: PersistentType: HTML\n\nyt-remote-fast-check-periodStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-session-appStores the user's video player preferences using embedded\nYouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-session-nameStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\n    * cdn.pbgrd.com\n\n2\n\npagead/gen_204Collects data on visitor behaviour from multiple websites, in\norder to present more relevant advertisement - This also allows the website to\nlimit the number of times that they are shown the same advertisement.\n\nExpiry: SessionType: Pixel\n\ncsiCollects data on visitors' preferences and behaviour on the website - This\ninformation is used make content and advertisement more relevant to the\nspecific visitor.\n\nExpiry: SessionType: Pixel\n\n    * pub.mdpi-res.com\n\n1\n\nOAIDRegisters a unique ID that identifies a returning user's device. The ID is\nused for targeted ads.\n\nExpiry: 1 yearType: HTTP\n\n  * Unclassified cookies are cookies that we are in the process of classifying, together with the providers of individual cookies.\n\n    * Crazyegg\n\n1\n\nLearn more about this provider\n\n_ce.irvPending\n\nExpiry: SessionType: HTTP\n\n    * matomo.mdpi.com\n\n1\n\n_pk_hsr.0.01efPending\n\nExpiry: 1 dayType: HTTP\n\n    * www.mdpi.com\n\n3\n\nhypothesis.testKeyPending\n\nExpiry: PersistentType: HTML\n\nmdpi_layout_type_v2Pending\n\nExpiry: 1 yearType: HTTP\n\nsettings_cachedPending\n\nExpiry: SessionType: HTTP\n\nCross-domain consent[#BULK_CONSENT_DOMAINS_COUNT#] [#BULK_CONSENT_TITLE#]\n\nList of domains your consent applies to: [#BULK_CONSENT_DOMAINS#]\n\nCookie declaration last updated on 3/25/24 by Cookiebot\n\n## [#IABV2_TITLE#]\n\n[#IABV2_BODY_INTRO#]\n\n[#IABV2_BODY_LEGITIMATE_INTEREST_INTRO#]\n\n[#IABV2_BODY_PREFERENCE_INTRO#]\n\n[#IABV2_BODY_PURPOSES_INTRO#]\n\n[#IABV2_BODY_PURPOSES#]\n\n[#IABV2_BODY_FEATURES_INTRO#]\n\n[#IABV2_BODY_FEATURES#]\n\n[#IABV2_BODY_PARTNERS_INTRO#]\n\n[#IABV2_BODY_PARTNERS#]\n\nCookies are small text files that can be used by websites to make a user's\nexperience more efficient.\n\nThe law states that we can store cookies on your device if they are strictly\nnecessary for the operation of this site. For all other types of cookies we\nneed your permission.\n\nThis site uses different types of cookies. Some cookies are placed by third\nparty services that appear on our pages.\n\nYou can at any time change or withdraw your consent from the Cookie\nDeclaration on our website.\n\nLearn more about who we are, how you can contact us and how we process\npersonal data in our Privacy Policy.\n\nPlease state your consent ID and date when you contact us regarding your\nconsent.\n\nPowered by Cookiebot by Usercentrics\n\nNext Article in Journal\n\nError Analysis of Common Power Meter Installation Faults on Three-Phase\nNetworks\n\nPrevious Article in Journal\n\nA 2.25 ppm/\u00b0C High-Order Temperature-Segmented Compensation Bandgap Reference\n\n## Journals\n\nActive Journals Find a Journal Proceedings Series\n\n## Topics\n\n## Information\n\nFor Authors For Reviewers For Editors For Librarians For Publishers For\nSocieties For Conference Organizers\n\nOpen Access Policy Institutional Open Access Program Special Issues Guidelines\nEditorial Process Research and Publication Ethics Article Processing Charges\nAwards Testimonials\n\n## Author Services\n\n## Initiatives\n\nSciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS\nProceedings Series\n\n## About\n\nOverview Contact Careers News Press Blog\n\nSign In / Sign Up\n\n## Notice\n\nclear\n\n## Notice\n\nYou are accessing a machine-readable page. In order to be human-readable,\nplease install an RSS reader.\n\nContinue Cancel\n\nclear\n\nAll articles published by MDPI are made immediately available worldwide under\nan open access license. No special permission is required to reuse all or part\nof the article published by MDPI, including figures and tables. For articles\npublished under an open access Creative Common CC BY license, any part of the\narticle may be reused without permission provided that the original article is\nclearly cited. For more information, please refer to\nhttps://www.mdpi.com/openaccess.\n\nFeature papers represent the most advanced research with significant potential\nfor high impact in the field. A Feature Paper should be a substantial original\nArticle that involves several techniques or approaches, provides an outlook\nfor future research directions and describes possible research applications.\n\nFeature papers are submitted upon individual invitation or recommendation by\nthe scientific editors and must receive positive feedback from the reviewers.\n\nEditor\u2019s Choice articles are based on recommendations by the scientific\neditors of MDPI journals from around the world. Editors select a small number\nof articles recently published in the journal that they believe will be\nparticularly interesting to readers, or important in the respective research\narea. The aim is to provide a snapshot of some of the most exciting work\npublished in the various research areas of the journal.\n\nOriginal Submission Date Received: .\n\n  * Journals\n\n    *       * Active Journals\n      * Find a Journal\n      * Proceedings Series\n\n  * Topics\n  * Information\n\n    *       * For Authors\n      * For Reviewers\n      * For Editors\n      * For Librarians\n      * For Publishers\n      * For Societies\n      * For Conference Organizers\n\n      * Open Access Policy\n      * Institutional Open Access Program\n      * Special Issues Guidelines\n      * Editorial Process\n      * Research and Publication Ethics\n      * Article Processing Charges\n      * Awards\n      * Testimonials\n\n  * Author Services\n  * Initiatives\n\n    *       * Sciforum\n      * MDPI Books\n      * Preprints.org\n      * Scilit\n      * SciProfiles\n      * Encyclopedia\n      * JAMS\n      * Proceedings Series\n\n  * About\n\n    *       * Overview\n      * Contact\n      * Careers\n      * News\n      * Press\n      * Blog\n\nSign In / Sign Up Submit\n\nJournals\n\nElectronics\n\nVolume 13\n\nIssue 8\n\n10.3390/electronics13081500\n\nSubmit to this Journal Review for this Journal Propose a Special Issue\n\n\u25ba \u25bc Article Menu\n\n## Article Menu\n\n  * Academic Editor\n\nJieyang Chen\n\n  * Subscribe SciFeed\n  * Recommended Articles\n  * Related Info Link\n\n    * Google Scholar\n\n  * More by Authors Links\n\n    * on DOAJ\n\n      * Seo, J.\n      * Kong, J.\n\n    * on Google Scholar\n\n      * Seo, J.\n      * Kong, J.\n\n    * on PubMed\n\n      * Seo, J.\n      * Kong, J.\n\n/ajax/scifeed/subscribe\n\nArticle Views 435\n\n  * Table of Contents\n\n    * Abstract\n    * Introduction\n    * Related Works\n    * Background and Motivation\n    * VerSA Architecture\n    * Evaluation\n    * Discussion\n    * Conclusions\n    * Author Contributions\n    * Funding\n    * Data Availability Statement\n    * Conflicts of Interest\n    * References\n\nAltmetric share Share announcement Help format_quote Cite question_answer\nDiscuss in SciProfiles thumb_up\n\n...\n\nEndorse textsms\n\n...\n\nComment\n\n## Need Help?\n\n### Support\n\nFind support for a specific problem in the support section of our website.\n\nGet Support\n\n### Feedback\n\nPlease let us know what you think of our products and services.\n\nGive Feedback\n\n### Information\n\nVisit our dedicated information section to learn more about MDPI.\n\nGet Information\n\nclear\n\n## JSmol Viewer\n\nclear\n\nfirst_page\n\nDownload PDF\n\nsettings\n\nOrder Article Reprints\n\nFont Type:\n\nArial Georgia Verdana\n\nFont Size:\n\nAa Aa Aa\n\nLine Spacing:\n\nColumn Width:\n\nBackground:\n\nOpen AccessArticle\n\n# VerSA: Versatile Systolic Array Architecture for Sparse and Dense Matrix\nMultiplications\n\nby\n\nJuwon Seo\n\nJuwon Seo\n\nSciProfiles Scilit Preprints.org Google Scholar\n\n^^ and\n\nJoonho Kong\n\nJoonho Kong\n\nSciProfiles Scilit Preprints.org Google Scholar\n\n^ *^\n\nSchool of Electronic and Electrical Engineering, Kyungpook National\nUniversity, Daegu 41566, Republic of Korea\n\n^*\n\nAuthor to whom correspondence should be addressed.\n\nElectronics 2024, 13(8), 1500; https://doi.org/10.3390/electronics13081500\n\nSubmission received: 27 February 2024 / Revised: 9 April 2024 / Accepted: 11\nApril 2024 / Published: 15 April 2024\n\n(This article belongs to the Special Issue Heterogeneous and Energy-Efficient\nComputing Systems)\n\nDownload keyboard_arrow_down\n\nDownload PDF Download PDF with Cover Download XML Download Epub\n\nBrowse Figures\n\nVersions Notes\n\nArticle Views\n\nCitations -\n\n## Abstract\n\nA key part of modern deep neural network (DNN) applications is matrix\nmultiplication. As DNN applications are becoming more diverse, there is a need\nfor both dense and sparse matrix multiplications to be accelerated by\nhardware. However, most hardware accelerators are designed to accelerate\neither dense or sparse matrix multiplication. In this paper, we propose VerSA,\na versatile systolic array architecture for both dense and sparse matrix\nmultiplications. VerSA employs intermediate paths and SRAM buffers between the\nrows of the systolic array (SA), thereby enabling an early termination in\nsparse matrix multiplication with a negligible performance overhead when\nrunning dense matrix multiplication. When running sparse matrix\nmultiplication, 256 \u00d7 256 VerSA brings performance (i.e., an inverse of\nexecution time) improvement and energy saving by 1.21\u00d7\u20131.60\u00d7 and 7.5\u201330.2%,\nrespectively, when compared to the conventional SA. When running dense matrix\nmultiplication, VerSA results in only a 0.52% performance overhead compared to\nthe conventional SA.\n\nKeywords:\n\nmatrix multiplication; systolic array; sparse matrix; dense matrix; hardware\nacceleration\n\n## 1\\. Introduction\n\nThe rise of artificial intelligence (AI)-based applications has brought about\na huge change in human life. One of the most important key enablers of this\nchange is the improvement in computing power. A core operation in AI is matrix\nmultiplication (MM). A dataflow-based architecture enables an efficient\nprocessing of matrix multiplication. The operations in dataflow architecture\nare mainly performed by moving the data and results through computing logic\nsuch as an arithmetic logical unit (ALU). One of the most widely used dataflow\narchitectures to accelerate MM is a systolic array-based architecture.\n\nSystolic arrays (SAs) are typically composed of two-dimensional processing\nelement (PE) arrays. A PE performs a multiply-and-accumulation (MAC) operation\nwith temporarily latching and forwarding of the inputs and/or outputs. There\nare three different dataflows, depending on which elements are stationary in\nthe PE, in general SAs: input stationary, weight stationary, and output\nstationary. Depending on the dataflows, different elements are pinned to the\nPEs and then transferred throughout the PEs. The systolic arrays can\nefficiently execute the matrix multiplication due to the massive parallelism\namong the PEs, thereby enabling an abundant number of parallel MAC operations.\nDue to their low design complexity and satisfactory performance, SAs have been\nwidely adopted in many industrial products such as tensor processing units\n(TPUs) [1,2,3].\n\nNonetheless, SAs often suffer from inefficiency when running sparse matrix\nmultiplication (SpMM). Since there is no intermediate path that is directly\nconnected to the output buffer, the ineffectual operations have no choice but\nto be executed throughout the datapath anyway. This causes huge latency and\nenergy overheads when performing SpMM. To overcome this problem, many works\nhave focused on sparsity-aware processing engines\n[4,5,6,7,8,9,10,11,12,13,14,15]. Though they show a significant speedup when\nexecuting SpMM, they are not appropriate for dense matrix multiplication. For\nexample, a specialized format (e.g., a compressed sparse row (CSR)) must be\nused to execute SpMM [12], and it could be desirable for sparse matrices in\nterms of the data size and computation efficiency. However, they are still\nproblematic for dense matrix multiplication. Due to the large metadata size of\nspecialized formats, the data size would be rather increased when compared to\na dense format (i.e., storing the matrix in a row-wise or column-wise manner\nwithout the coordinate metadata), which incurs large storage or memory\noverhead, as well as computation inefficiency. Even worse in typical embedded\nor mobile systems where accommodated hardware resources are constrained, it\ncould be very hard to employ separate hardware accelerators for dense and\nsparse MMs.\n\nIn this paper, we propose VerSA, Versatile Systolic Array architecture, to\naccelerate both dense and sparse matrix multiplications. By providing two\noperation modes, i.e., sparse and dense modes, VerSA adaptively executes dense\nMM and SpMM in a single-hardware architecture. In the sparse mode, the model\nutilizes intermediate paths and buffers (IPBs) between the rows of PEs, which\nis designed for the early termination of SpMM by skipping ineffectual\noperations. In the dense mode, VerSA operates similarly as the conventional\nsystolic array (SA) with negligible performance overhead. Our evaluation\nresults show that our 256 \u00d7 256 VerSA architecture when running in the sparse\nmode shows a better performance by 1.21\u00d7\u20131.60\u00d7 when compared to the\nconventional SA. Also, VerSA when running in the dense mode shows a comparable\nperformance when compared to the conventional SA, incurring only a 0.52%\nperformance overhead. When compared to the state-of-the-art SpMM accelerator\n[12], VerSA shows a better performance by 20.1\u00d7, on average, when executing\nvarious SpMM benchmark applications. The main contributions of this work can\nbe summarized as follows:\n\n  * We propose the VerSA architecture, which can be used for both dense and sparse matrix multiplications in a versatile manner;\n\n  * When executing SpMM, 256 \u00d7 256 (128 \u00d7 128) VerSA results in performance improvement and energy saving by 1.21\u00d7\u20131.60\u00d7 (1.16\u00d7\u20131.45\u00d7) and 7.5\u201330.2% (1.6\u201321.3%), respectively, on average, when compared to the conventional SA;\n\n  * When compared to the state-of-the-art SpMM accelerator, our 256 \u00d7 256 (128 \u00d7 128) VerSA shows a better performance by 20.1\u00d7 (5.7\u00d7), on average, meaning that VerSA can be used for a broader range of MM applications;\n\n  * In terms of logic synthesis results, 256 \u00d7 256 (128 \u00d7 128) VerSA architecture can be implemented with only small hardware and power overheads when compared to the conventional SA by 12.6% (14.9%) and 11.7% (14.4%), respectively.\n\nThe remainder of this paper is organized as follows. Section 2 reviews the\nrecent literature that are closely related to our work. Section 3 explains the\nbackground for general systolic arrays and our motivation. Section 4 explains\nour VerSA architecture in detail. Section 5 shows our evaluation results in\nterms of performance and energy. Section 6 discusses the hardware and software\noverheads and limitations of this work. Section 7 then concludes this paper.\n\n## 2\\. Related Works\n\nFor dense matrix multiplication, systolic arrays are widely used due to their\nsimple yet efficient logic architecture (e.g., [1]) and the easy employment of\nvarious dataflows such as weight stationary, row stationary, input stationary,\nand output stationary [16]. However, conventional systolic arrays are not\nadequate for sparse matrix multiplication as they cannot skip ineffectual\noperations.\n\nFor sparse matrix multiplication supports, many works have been focused on\nremoving the ineffectual operations from matrix multiplication\n[4,5,6,7,8,9,10,11,12,13,14,15]. The most widely used method is to employ a\ncompressed format (e.g., compressed sparse row [12] or channel cyclic sparse\nrow format [9]) for performing sparse matrix multiplication. Since the\ncompressed format already removes many of the zero values in the operand\nmatrices, MM operations with compressed formats also remove a large portion of\nthe ineffectual operations. However, the compressed format often leads to a\nlarger data size in the case of dense matrices due to the metadata size being\neven larger than the non-zero data. Furthermore, the compressed format is\noften required to be uncompressed for MAC operations. Several works have also\ncompared the indices (e.g., the column index of the input matrix and row index\nof the weight matrix) of the non-zero values, and they only performed\nmultiplications with those values [5], which are often referred to as the\ninner product. However, non-zero index matching often incurs a huge overhead\nin logic and time complexities as the density in the operand matrices\nincreases. In [15], a flexible architecture that supports three different\ndataflows for sparse matrix multiplications was proposed. Though the\naforementioned approach enables a flexible dataflow change within a single\nhardware architecture, only dataflows for sparse MM are supported.\n\nAs explained above, accelerators that are used only for sparse matrix\nmultiplication are very hard to be employed for dense matrix multiplication\ndue to the inefficiency of their compressed formats and dataflows, which are\noptimized only for sparse matrix multiplication. When compared to the related\nworks introduced in this section, our VerSA architecture can be employed to\nperform both dense and sparse MMs with a unified hardware architecture. Due to\nits versatility, VerSA is more suitable for resource-constrained embedded\nsystems where separate hardware accelerators for dense and sparse MMs in the\nsystem are not desirable.\n\n## 3\\. Background and Motivation\n\nOne of the most widely used architectures for accelerating MM is a systolic\narray (SA), which is shown in Figure 1. The main advantages of the SA are its\ndesign simplicity and high efficiency for matrix multiplication. The\nconventional systolic arrays for matrix multiplication can provide three\ndifferent dataflows: input stationary, weight stationary, and output\nstationary [17]. Assuming we perform A \u00d7 B = C where A, B, and C are matrices,\ninput, weight, and output stationary dataflows fix (i.e., preloaded and are\nnot moved throughout the PEs) the elements in the matrix A, B, and C,\nrespectively, in the PEs of the SA. For typical DNN applications, the weight\nstationary dataflow is widely used for DNN inference accelerators because the\nweights can be heavily reused across the batches when performing DNN\ninferences, which minimizes the data transfer overhead in the SA.\n\nFigure 1. The conventional systolic array architecture with weight stationary\ndataflow. When performing , A, B, and C correspond to the input (A in the\nfigure), weight (W in the figure), and output (O in the figure), respectively.\n\nWhen performing sparse matrix multiplication (SpMM), the main disadvantage of\nthe conventional SA is that we cannot skip ineffectual operations such as\nmultiply-with-zero or add-with-zero. Thus, for SpMM, the conventional SA takes\nthe same clock cycles (and the same execution time with a fixed-clock\nfrequency) for both SpMM and dense MM. Since there is a huge opportunity in\nremoving the ineffectual operations present in SpMM, employing the SA for SpMM\nmay cause a huge energy waste and performance loss. On the contrary, employing\nthe specialized hardware accelerator for SpMM could be beneficial for\naccelerating the matrix multiplication. For example, in [12], where a\nspecialized hardware accelerator for SpMM was used, up to 47\u00d7 (on average)\nspeedup can be obtained when compared to the conventional SA. However, the\nspecialized SpMM hardware can be useful only when performing SpMM as it\nresults in even worse performance for dense matrix multiplication. As general\nembedded systems are resource-constrained, it would be hard to deploy both\nSpMM and dense MM hardware accelerators. Consequently, employing a versatile\nhardware accelerator that can adapt to both SpMM and dense MM would be\ndesirable.\n\nBased on the motivations described above, we will focus on the following\ndesign principles and considerations:\n\n  * We will devise a novel, unified hardware architecture for efficiently executing both sparse and dense MMs;\n\n  * For the versatility of our hardware, we will also devise appropriate hardware and software supports. We will also focus on minimizing the overhead caused from those supports.\n\n## 4\\. VerSA Architecture\n\n#### 4.1. Overview\n\nIn this subsection, we briefly explain the overview of VerSA architecture.\nVerSA architecture consists of two parts: the hardware accelerator and\nsoftware supports. VerSA hardware is built upon a general systolic array that\ncan perform matrix multiplication. We newly introduce the intermediate paths\nand SRAM output buffers (IPBs) between the group of the rows (which we call\nthe \u2018subarray\u2019 in this paper) in the SA. For software supports, we need to\nmake the preloaded matrix (weight) in the condensed format similar to that in\n[7] so that we can reduce the clock cycles required for sparse MM execution.\nThe column indices of the partial sum output matrices should also be adjusted\nwhen using a column-wise-condensed weight matrix, and it should be added by\nsoftware supports when performing a blocked MM (i.e., when the size of the\ninput or weight matrix is too large to be executed in a single systolic array\nor a subarray).\n\nFigure 2 depicts the overall execution flow of the VerSA architecture (A \u00d7 B =\nC), which is similar to the conventional SA execution flow where several steps\nare added for the sparse mode supports. For the dense mode, which performs\ndense matrix multiplication, our VerSA operates almost same as the\nconventional SA-based MM execution. However, the following additional steps\nare required for sparse mode operations: (1) pre-processing when conducting a\ncolumn-wise condensing of the weights (i.e., B matrix) and (2) post-processing\nfor adjusting the column indices of the generated partial sum matrix (i.e.,\nthe partial sum of the C matrix). Column-wise condensing has the effect of\nremoving many of the zero-valued weights in advance. Thus, by generating and\npreloading the column-wise-condensed weight matrix, our hardware can skip many\nineffectual operations. Figure 3 shows an example of the column-wise-condensed\nmatrix generation. We first divide the weight matrix into multiple groups of\nrows so that the number of rows inside of a single group is the same as the\n(see Table 1 for notation). For each group of rows, the matrix is condensed\nwhile maintaining the shape of each column within the group (our matrix\ncondensing method is similar to that introduced in [7], but our method also\nmaintains the shape of each column, which makes our hardware design less\ncomplicated.). The original column indices are also maintained for column\nrestoration during the post-processing stage. As explained above, the partial\nsum matrices should also be added together when performing a blocked MM;\nhowever, it is also required for the conventional SA, meaning that our VerSA\nhas a negligible overhead when compared to the conventional SA.\n\nFigure 2. The overall execution flow of the VerSA architecture (A \u00d7 B = C).\nThe steps from \u2780 to \u2786 correspond to sparse mode operations while those from a\nto f correspond to dense mode operations.\n\nFigure 3. An example of a column-wise matrix condensing with a 4 \u00d7 8 weight\nmatrix and = 2. The gray and white cells represent the non-zero and zero\nweight elements, respectively.\n\nTable 1. Summarization the design parameter notation in VerSA.\n\n#### 4.2. Hardware Architecture\n\nVerSA hardware architecture is similar to conventional systolic arrays;\nhowever, the key difference is an intermediate path inside the systolic array.\nWhen performing matrix multiplication, passing the generated partial sum from\nthe first row to the last row takes N clock cycles in the case of systolic\narrays. In the case of dense matrix multiplication, using the full datapath\nwould be meaningful because most of the operations are effectual during the\ncontinuous partial sum generation (i.e., the partial sum transfer and\ngeneration from the first row to the last row). However, in the case of sparse\nmatrix multiplication, the rows will not be fully utilized with a very high\nprobability. In this case, we could perform an early termination of partial\nsum generation in the case where certain rows do not need to be used.\n\nTo reduce the clock cycles needed for passing the partial sums to the lowest\nrow in the SA, VerSA introduces intermediate paths and SRAM output buffers\nbetween the rows in the SA. Figure 4 shows the architecture of our VerSA with\nintermediate paths and buffers (IPBs). Firstly, we group the adjacent rows of\nthe SA, which is referred to as \u2018subarray\u2019 (the number of rows in a single\nsubarray is denoted as ). Between the subarrays, there is an IPB, which is\ncomposed of the intermediate output path and SRAM buffer. If we can obtain the\npartial sum results from the IPBs (i.e., the ones earlier than those that pass\nthrough all the PEs in the same column from the first row to the last row),\nthen we can reduce the required clock cycles for MM operation. Table 1\nsummarizes the design parameters for VerSA.\n\nFigure 4. The hardware architecture of VerSA. The internal architecture of a\nsingle processing element in VerSA is the same as that in the conventional SA.\nIn the case of the dense mode, the hardware performs the operations of Steps\nb, d, and e, as shown in Figure 2. In the case of the sparse mode, the\nhardware performs the operations of Steps \u2781, \u2783, and \u2784, as shown in Figure 2.\n\nThe IPB consists of the output buffers and multiplexors (MUXes). To enable\nboth dense and sparse MMs in VerSA hardware, the mode selection bit is\nconnected to the MUXes in the IPBs. In the case of the dense mode, the partial\nsums from the upper (i.e., previous) subarray are selected in the MUXes and\ndelivered to the next subarray. In this case, the subarrays are connected via\nthe IPBs. On the contrary, in the case of the sparse mode, the zero values are\nselected in the MUXes of the IPBs, meaning that each subarray independently\noperates in the sparse mode.\n\nPassing the IPB requires one clock cycle because the partial sums should pass\nthrough the flip-flops (FFs) inside of the IPB. Thus, for dense mode\noperations, we additionally require clock cycles in comparison to the\nconventional SA, where is the total number of the IPBs in the VerSA. Since the\nmain goal of VerSA is to enable both dense and sparse MM executions within a\nsingle SA, there can be a negligible performance overhead from the additional\nclock cycles when considering the performance gain from the sparse mode\noperations. In the case of the sparse mode, the subarrays operate separately\n(i.e., operate in parallel). Thus, the number of the required clock cycles for\npassing the partial sums to the output buffer in the IPBs or the last output\nbuffer can be reduced to (one additional clock cycle is for the IPB). Early\ntermination in the sparse mode could be performed by using a special purpose\ncontrol signal that notifies the timing of the termination in the hardware\naccelerator. With the given input matrix (i.e., the A matrix) dimension,\ncondensed weight matrix (i.e., the condensed B matrix) dimension, and the\nhardware design parameters shown in Table 1, the required clock cycles can be\ncalculated, which enables an early termination that is achieved by counting\nthe executed clock cycles and comparing it with the required clock cycles.\n\n#### 4.3. An Example of the Sparse and Dense Mode Operations\n\nIn the following subsections, we explain how the sparse and dense mode\noperations are performed in detail.\n\n#### 4.3.1. Sparse Mode Operations\n\nIn the sparse mode, each subarray can operate independently. To accomplish it,\nthe IPBs between the subarrays select zero values in the MUXes. In the case of\nthe conventional systolic array, the N-th row in the SA starts the input\nstreaming after N clock cycles after the first row input streaming begins. On\nthe contrary, the subarrays in VerSA accept the input streaming independently,\nas shown in Figure 5. Assuming that there are rows in a single subarray, where\nsubarrays exist in the SA, the input streaming of the K-th row within each\nsubarray starts at K-th clock cycles. In other words, rows in the SA will\nbegin the input streaming at the same clock cycle.\n\nFigure 5. An example of sparse mode operations in VerSA.\n\nFigure 5 demonstrates an example of the sparse mode operation of VerSA. In\nthis example, we use the design parameters with = 4, = 2, = 2, and = 1. For\nthe sparse mode operation, we first perform column-wise matrix condensing\n(Figure 5 \u2780). The weight matrix is then preloaded (i.e., with a weight\nstationary dataflow) to the SA (Figure 5 \u2781). When performing the matrix\nmultiplication, the input matrix is streamed to the subarrays (Figure 5 \u2782). As\nshown in Figure 5, the subarrays operate independently as the MUXes in the IPB\nforcibly make the partial sum input as zero (i.e., where zero is selected in\nthe MUXes). After the MAC operations are performed in the PEs, the outputs or\npartial sums are generated from the IPB and the last output buffer (Figure 5\n\u2783). Since the subarrays operate independently, the outputs are also generated\nsimultaneously from each subarray.\n\nSince we convert the weight matrix into a condensed format, the column indices\nof the partial sum matrices must also be restored to the original indices\nbefore the summation among the partial sums. Figure 6 demonstrates the\nrestoration of the partial sum matrices and summation, thereby generating the\nfinal output matrix. By referring to the original column indices, which are\nstored during the matrix condensing, the column indices of the partial sums\ncan also be restored to generate the final output matrix.\n\nFigure 6. The restoration of the partial sum matrices and summation in VerSA,\nwhich correspond to the operations of Steps \u2785 and \u2786 in Figure 2.\n\nSince the restoration of the partial sum indices and summation of the partial\nsum matrices are performed in software (i.e., not in our VerSA hardware), it\nincurs additional delays; however, it could be negligible when compared to the\ndelay of the summation of the partial sums when performing a blocked MM.\nMoreover, the conventional SA will also have a delay overhead for the\naccumulation of the partial sums when performing a blocked MM. Thus, the\nadditional delay overhead of the post-processing in VerSA would be marginal.\n\n#### 4.3.2. Dense Mode Operations\n\nIn the case of the dense mode operation, our VerSA operates similarly to\nconventional systolic arrays while the main difference is the consideration of\nthe additional delay from the IPBs. Since the IPB takes one clock cycle, the\ninput streaming should be performed with consideration of the fact that an\nadditional one clock cycle will be taken from the IPB. Thus, for the input\nstreaming to a certain row within a subarray, N clock cycle delays should be\nadded when there are N IPBs above the current row. For example, as shown in\nFigure 7, the input streaming of the rows in the second subarray is delayed by\none clock cycle because there is one IPB above the rows in the second\nsubarray. Though the remaining operations can be performed similarly to the\nconventional SA, there should be a little performance overhead when compared\nto the conventional SA due to the additional delay from the IPBs.\n\nFigure 7. An example of dense mode operations in VerSA.\n\n#### 4.4. Implementation and Logic Synthesis\n\nWe have implemented our VerSA hardware architecture with a Verilog hardware\ndescription language (HDL) and synthesized it with 32 nm process technology\nusing Design Compiler. The power, performance, and area (PPA) of the input and\noutput buffers with the SRAM cells are estimated by CACTI7 [18] and\nincorporated into our PPA evaluation results. For each PE, we use an integer\n8-bit MAC unit (multiplier and adder) and registers for preloaded weights and\ntemporarily latched partial sums and inputs. One should note that 8-bit\ninteger formats are widely used in DNN inference engines due to the prevalence\nof quantization methods [19]. For systolic arrays, we used 128 \u00d7 128 and 256 \u00d7\n256 PE array dimensions, which are widely used in commercial systolic arrays\n[2]. The number of subarrays () in VerSA is set to eight for both dimensions.\nAs summarized in Table 2, both the conventional systolic array and our VerSA\nwere synthesized with a 250 MHz clock frequency. The size of the SRAM buffers\nfor a 128 \u00d7 128 (256 \u00d7 256) architecture is 64 KB (256 KB) for each input and\noutput buffer. In the case of VerSA, the output buffers are distributed across\nthe IPBs, and the output buffer is also placed below the last subarray. Though\nthere could be various design choices, we evenly distribute the output buffers\nto the IPBs and the last output buffer, thereby resulting in 8 KB (32 KB) for\neach IPB and last output buffer in a 128 \u00d7 128 (256 \u00d7 256) configuration. As\nshown in Table 2, adding the IPB in VerSA increases the area and power\nconsumption when compared to the conventional SA. As a result, the 128 \u00d7 128\n(256 \u00d7 256) VerSA increases the area and power by 14.8% (12.6%) and 14.4%\n(11.7%), respectively, when compared to the conventional SA with the same\narray dimension.\n\nTable 2. A logic synthesis comparison of the conventional SA (Conv_SA) and\nVerSA.\n\n## 5\\. Evaluation\n\n#### 5.1. Methodology\n\nFor cycle-level performance evaluations, we use SCALE-sim [17], which is an\narchitectural simulator for systolic arrays. For the evaluations of VerSA, we\nincorporated the cycle-level impact of the IPBs and exact cycle-level\nbehaviors together in the simulator. We used the synthesis results from Table\n2 for the clock cycle time and power, which are then used in the performance\nand energy evaluations. For the benchmarks, we used the following various\nmatrix multiplications from real-world DNN workloads: GPT2 [20], GNMT [21],\nNCF [22], Transformer [23], ResNet-50 [24], and VGG-19 [25]. For the purpose\nof comparison with the state-of-the-art MM accelerator, we compared our VerSA\nwith a row-wise sparse matrix multiplication hardware accelerator [12] with\nSuiteSparse benchmarks [26].\n\n#### 5.2. Performance\n\nFigure 8 summarizes the speedup results of our VerSA in comparison to the\nconventional SA in both the sparse mode and dense mode across different\nsparsity levels. For the comparison of our VerSA with the conventional SA, the\nsame clock frequency (250 MHz) was used for both designs. We show the relative\nperformance of VerSA in comparison to the conventional SA with the metric of\nspeedup. The speedup of VerSA against the conventional SA (ConvSA) can be\nformulated as follows:\n\nwhere and are the performance of X and the execution time of X, respectively.\n\nFigure 8. The speedup of VerSA architecture when compared to the conventional\nSA. SM_X% represents the sparse mode with an X% sparsity level. DM is the\ndense mode. The is set to eight in both 128 \u00d7 128 and 256 \u00d7 256 SA.\n\nIn the case of the sparse mode, our 128 \u00d7 128 VerSA (256 \u00d7 256) leads to\nbetter performance when compared to the conventional SA by 1.16\u00d7\u20131.45\u00d7\n(1.21\u00d7\u20131.60\u00d7) across various sparsity levels. The early termination that\noccurs due to the IPBs in our VerSA leads to the clock cycle (i.e., execution\ntime) reduction that is required for the MM workload execution. As the array\nsize increases, the performance gain of the sparse mode is likely to increase\nbecause it is likely to skip more rows in the case of bigger arrays;\nmeanwhile, the conventional SA must pass through the whole rows to generate\nthe outputs. In the case of the dense mode, our 128 \u00d7 128 (256 \u00d7 256) VerSA\nshows only a little performance overhead by 0.85% (0.52%) when compared to the\nconventional SA. As our model has a bigger array size with fixed , the\nperformance overhead caused by the additional IPBs is reduced.\n\nTable 3 shows the speedup of our VerSA against the state-of-the-art SpMM\naccelerator [12]. In this paper, we used the same clock frequency for both the\nSpMM accelerator and VerSA to compare the performance. Please note that this\nis a very conservative assumption when considering the logic complexity\nbetween the two designs; our VerSA has a much lower logic complexity than the\nSpMM accelerator in [12].\n\nTable 3. Speedup of our VerSA against the state-of-the-art sparse MM\naccelerator with a 4PE configuration [12].\n\nIn the case of the large workloads with a relatively high sparsity (web-\nGoogle, mario002, amazon0312, and m133-b2), our 128 \u00d7 128 (256 \u00d7 256) VerSA\nresults in 82.8\u201398.0% (38.6\u201392.9%) performance losses, on average, when\ncompared to the accelerator in [12]. Since the SpMM accelerator in [12]\nprimarily focuses on sparse matrix multiplication, it has an advantage when\ndealing with large and sparse MM workloads. However, for the rest of the\nworkloads, our VerSA shows a speedup of more than three times when compared to\nthe accelerator used in [12]. It means that our VerSA hardware accelerator\nperforms matrix multiplication with a much higher versatility when compared to\nthe SpMM accelerator.\n\n#### 5.3. Energy\n\nFigure 9 depicts the energy results of our VerSA relative to the conventional\nSA. We present the energy consumption of VerSA relative to that of the\nconventional SA (ConvSA), which can be formulated as follows:\n\nwhere is the energy consumption in the case of X.\n\nFigure 9. The energy consumption of our VerSA normalized to the conventional\nSA (=1.0). SM_X% is the sparse mode with an X% sparsity level. DM means the\ndense mode.\n\nIn the case of the sparse mode, our 128 \u00d7 128 (256 \u00d7 256) VerSA shows lower\nenergy consumption when compared to the conventional SA by 1.6\u201321.3%\n(7.5\u201330.2%) across various sparsity levels. Despite the increased power\nconsumption, thanks to the reduced execution time, the total energy\nconsumption of our VerSA is less than that of the conventional SA. In the case\nof the dense mode, our 128 \u00d7 128 (256 \u00d7 256) VerSA consumes more energy, by\n15.4% (12.3%), when compared to the conventional SA due to the additional\nclock cycles and power consumption of the IPBs.\n\n## 6\\. Discussion\n\n#### 6.1. Hardware Overhead\n\nAs mentioned in Section 4.4, our VerSA hardware architecture shows power and\narea overheads when compared to the conventional SA. Our VerSA hardware\nobviously employs IPBs, which correspond to additional logic gates when\ncompared to the conventional SA. However, VerSA has several advantages over\nthe conventional SA or SpMM hardware when implementing a system. When\nintegrating the intellectual properties (IPs) in a single chip or system to\nexecute both dense and sparse MMs, VerSA enables system implementation with a\nVerSA hardware block only while also not requiring heterogeneous integration\n(i.e., integrating different hardware blocks for dense MM and sparse MM). It\nalso implies that integration with VerSA leads to less hardware complexity and\nbetter IP reusability, which eventually results in a cost reduction. Moreover,\nby enabling both dense and sparse MMs within a single hardware, the VerSA-\nintegrated system will have better hardware utilization when compared to the\nsystem with heterogeneous integration. In the case of a system with\nheterogeneous integration, when performing dense MM, the sparse MM hardware\nblock will be in the idle state, and also vice versa.\n\n#### 6.2. Software Overhead\n\nVerSA requires several software supports such as weight matrix condensing and\ncolumn restoration. Weight matrix condensing requires the search of zero\nvalues in the weight matrix, and it condenses the matrix in a column-wise\nmanner. Performing the matrix condensing can incur non-negligible execution\ntime overhead. However, once the weight matrix is preloaded into VerSA, the\nweights can be reused across a large number of DNN inferences because the\nweights are not changed during the DNN inference. This means that the weight\nmatrix condensing overhead can be amortized across multiple DNN inferences,\nthus resulting in negligible execution time overhead. Though the column\nrestoration delay overhead seems to be inevitable, the relative delay overhead\nof the column restoration would also be negligible when compared to that of\nthe partial sum matrix accumulation for the blocked MM. As modern DNN\nworkloads need to execute the multiplication between operand matrices with a\nvery large dimension (where a single MM should be executed by multiple MM\noperations with the blocked operand matrices and accumulation among the\npartial sum matrices), the blocked MM will be frequently executed for DNN\ninferences. This means the delay overhead from the column restoration would be\nnegligible when running real-world DNN workloads.\n\n#### 6.3. Limitations of This Work\n\nThe limitations of this work can be summarized as follows:\n\n  * In the evaluation results, we only considered the hardware execution time. Though the software execution time overhead could be marginal, as mentioned in Section 6.2, it would also be desirable for evaluating end-to-end performance;\n\n  * Since the main contribution of this paper is to design VerSA architecture, our evaluation is based on cycle-level simulation and logic synthesis results. A verification and evaluation with full system implementation and software supports (e.g., an implementation in field programmable gate arrays) would also be interesting;\n\n  * As presented in Section 5.2 and Section 5.3, our hardware architecture has inevitable performance and energy overheads when performing dense MM when compared to the conventional SA. This is an inherent limitation that arises from the VerSA architecture design. However, considering that the contemporary DNN models have non-negligible sparsity, our VerSA can sufficiently compensate for the performance and energy overheads of the dense MM.\n\nOvercoming the limitations listed above can be an interesting future research\ndirection, and we have a plan to further delve into these topics as our future\nwork.\n\n## 7\\. Conclusions\n\nThough conventional systolic arrays have been widely used to accelerate matrix\nmultiplication, they are not efficient for sparse matrix multiplication (SpMM)\ndue to their inability to skip the ineffectual operations. To resolve this\nproblem, many hardware accelerators have been proposed, which show much better\nperformance when compared to the conventional systolic array when running\nsparse matrix multiplication workloads. However, most SpMM hardware\narchitectures are not suitable for dense matrix multiplication. In this paper,\nwe propose VerSA architecture, a versatile systolic array architecture to\naccelerate both dense and sparse matrix multiplications. By adding\nintermediate paths and SRAM buffers (IPB), SpMM can be terminated earlier than\nthe conventional SA, thus accelerating SpMM. Since our architecture is built\nupon a systolic array, dense MM can also be executed with negligible\nperformance overhead. In comparison to the conventional SA, our 256 \u00d7 256\nVerSA architecture improves the performance of SpMM by 1.21\u00d7\u20131.60\u00d7 across\nvarious sparsity levels while there is only a 0.52% performance overhead in\nthe case of dense MM. In terms of energy consumption, our 256 \u00d7 256 VerSA\narchitecture reduces the energy consumption of SpMM by 7.5\u201330.2%, while there\nis only a 12.3% energy overhead in the case of dense MM. DNN hardware\naccelerators are widely deployed in mobile edge or embedded devices. In\naddition, DNN workloads are also becoming increasingly diverse, thus\nnecessitating the acceleration of both dense and sparse matrix\nmultiplications. We believe that our VerSA can be a promising alternative that\ncan be employed for both dense and sparse matrix multiplications with a\nunified hardware architecture.\n\n## Author Contributions\n\nConceptualization, J.S. and J.K.; Methodology, J.S. and J.K.; Software, J.S.;\nValidation, J.S. and J.K.; Investigation, J.S.; Data curation, J.S.;\nWriting\u2014original draft, J.S. and J.K.; Writing\u2014review & editing, J.S. and\nJ.K.; Visualization, J.S.; Supervision, J.K.; Project administration, J.K.;\nFunding acquisition, J.K. All authors have read and agreed to the published\nversion of the manuscript.\n\n## Funding\n\nThis research was supported by the Basic Science Research Program through the\nNational Research Foundation of Korea (NRF), which was funded by the Ministry\nof Education (NRF-2021R1I1A3A04037455) and Samsung Electronics Co., Ltd.\n(IO221005-02702-01). The EDA tool was supported by the IC Design Education\nCenter (IDEC).\n\n## Data Availability Statement\n\nData are contained within the article.\n\n## Conflicts of Interest\n\nThe authors declare no conflict of interest.\n\n## References\n\n  1. Jouppi, N.P.; Young, C.; Patil, N.; Patterson, D.; Agrawal, G.; Bajwa, R.; Bates, S.; Bhatia, S.; Boden, N.; Borchers, A.; et al. In-Datacenter Performance Analysis of a Tensor Processing Unit. In Proceedings of the 44th Annual International Symposium on Computer Architecture, Toronto, ON, Canada, 24\u201328 June 2017; pp. 1\u201312. [Google Scholar] [CrossRef]\n  2. Jouppi, N.P.; Hyun Yoon, D.; Ashcraft, M.; Gottscho, M.; Jablin, T.B.; Kurian, G.; Laudon, J.; Li, S.; Ma, P.; Ma, X.; et al. Ten Lessons From Three Generations Shaped Google\u2019s TPUv4i: Industrial Product. In Proceedings of the 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA), Valencia, Spain, 14\u201318 June 2021; pp. 1\u201314. [Google Scholar] [CrossRef]\n  3. Jouppi, N.; Kurian, G.; Li, S.; Ma, P.; Nagarajan, R.; Nai, L.; Patil, N.; Subramanian, S.; Swing, A.; Towles, B.; et al. TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings. In Proceedings of the 50th Annual International Symposium on Computer Architecture, Orlando, FL, USA, 17\u201321 June 2023. [Google Scholar] [CrossRef]\n  4. Pal, S.; Beaumont, J.; Park, D.H.; Amarnath, A.; Feng, S.; Chakrabarti, C.; Kim, H.S.; Blaauw, D.; Mudge, T.; Dreslinski, R. OuterSPACE: An Outer Product Based Sparse Matrix Multiplication Accelerator. In Proceedings of the 2018 IEEE International Symposium on High Performance Computer Architecture (HPCA), Vienna, Austria, 24\u201328 February 2018; pp. 724\u2013736. [Google Scholar] [CrossRef]\n  5. Gondimalla, A.; Chesnut, N.; Thottethodi, M.; Vijaykumar, T.N. SparTen: A Sparse Tensor Accelerator for Convolutional Neural Networks. In Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture, Columbus, OH, USA, 12\u201316 October 2019; pp. 151\u2013165. [Google Scholar] [CrossRef]\n  6. Qin, E.; Samajdar, A.; Kwon, H.; Nadella, V.; Srinivasan, S.; Das, D.; Kaul, B.; Krishna, T. SIGMA: A Sparse and Irregular GEMM Accelerator with Flexible Interconnects for DNN Training. In Proceedings of the 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA), San Diego, CA, USA, 22\u201326 February 2020; pp. 58\u201370. [Google Scholar] [CrossRef]\n  7. Zhang, Z.; Wang, H.; Han, S.; Dally, W.J. SpArch: Efficient Architecture for Sparse Matrix Multiplication. In Proceedings of the 2020 IEEE International Symposium on High Performance Computer Architecture (HPCA), San Diego, CA, USA, 22\u201326 February 2020; pp. 261\u2013274. [Google Scholar] [CrossRef]\n  8. Hojabr, R.; Sedaghati, A.; Sharifian, A.; Khonsari, A.; Shriraman, A. SPAGHETTI: Streaming Accelerators for Highly Sparse GEMM on FPGAs. In Proceedings of the 2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA), Seoul, Republic of Korea, 27 February\u20133 March 2021; pp. 84\u201396. [Google Scholar] [CrossRef]\n  9. Srivastava, N.; Jin, H.; Liu, J.; Albonesi, D.; Zhang, Z. MatRaptor: A Sparse-Sparse Matrix Multiplication Accelerator Based on Row-Wise Product. In Proceedings of the 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), Athens, Greece, 17\u201321 October 2020; pp. 766\u2013780. [Google Scholar] [CrossRef]\n  10. Zhang, G.; Attaluri, N.; Emer, J.S.; Sanchez, D. Gamma: Leveraging Gustavson\u2019s Algorithm to Accelerate Sparse Matrix Multiplication. In Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Virtual, 19\u201323 April 2021; pp. 687\u2013701. [Google Scholar] [CrossRef]\n  11. Kwon, J.; Kong, J.; Munir, A. Sparse convolutional neural network acceleration with lossless input feature map compression for resource-constrained systems. IET Comput. Digit. Technol. 2022, 16, 29\u201343. [Google Scholar] [CrossRef]\n  12. Lee, J.H.; Park, B.; Kong, J.; Munir, A. Row-Wise Product-Based Sparse Matrix Multiplication Hardware Accelerator With Optimal Load Balancing. IEEE Access 2022, 10, 64547\u201364559. [Google Scholar] [CrossRef]\n  13. Li, S.; Huai, S.; Liu, W. An Efficient Gustavson-Based Sparse Matrix\u2013Matrix Multiplication Accelerator on Embedded FPGAs. IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst. 2023, 42, 4671\u20134680. [Google Scholar] [CrossRef]\n  14. Li, Z.; Li, J.; Chen, T.; Niu, D.; Zheng, H.; Xie, Y.; Gao, M. Spada: Accelerating Sparse Matrix Multiplication with Adaptive Dataflow. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Vancouver, BC, Canada, 25\u201329 March 2023; pp. 747\u2013761. [Google Scholar] [CrossRef]\n  15. Mu\u00f1oz Mart\u00ednez, F.; Garg, R.; Pellauer, M.; Abell\u00e1n, J.L.; Acacio, M.E.; Krishna, T. Flexagon: A Multi-dataflow Sparse-Sparse Matrix Multiplication Accelerator for Efficient DNN Processing. In Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Vancouver, BC, Canada, 25\u201329 March 2023; pp. 252\u2013265. [Google Scholar] [CrossRef]\n  16. Sze, V.; Chen, Y.H.; Yang, T.J.; Emer, J.S. Efficient Processing of Deep Neural Networks: A Tutorial and Survey. Proc. IEEE 2017, 105, 2295\u20132329. [Google Scholar] [CrossRef]\n  17. Samajdar, A.; Joseph, J.M.; Zhu, Y.; Whatmough, P.; Mattina, M.; Krishna, T. A systematic methodology for characterizing scalability of DNN accelerators using SCALE-sim. In Proceedings of the 2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS), Boston, MA, USA, 23\u201325 August 2020; pp. 58\u201368. [Google Scholar] [CrossRef]\n  18. Balasubramonian, R.; Kahng, A.B.; Muralimanohar, N.; Shafiee, A.; Srinivas, V. CACTI 7: New Tools for Interconnect Exploration in Innovative Off-Chip Memories. ACM Trans. Archit. Code Optim. 2017, 14, 1\u201325. [Google Scholar] [CrossRef]\n  19. Han, S.; Mao, H.; Dally, W.J. Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding. In Proceedings of the 4th International Conference on Learning Representations (ICLR), San Juan, Puerto Rico, 2\u20134 May 2016. [Google Scholar]\n  20. Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I. Language Models are Unsupervised Multitask Learners. OpenAI 2019, 1, 9. [Google Scholar]\n  21. Wu, Y.; Schuster, M.; Chen, Z.; Le, Q.V.; Norouzi, M.; Macherey, W.; Krikun, M.; Cao, Y.; Gao, Q.; Macherey, K.; et al. Google\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation. arXiv 2016, arXiv:1609.08144. [Google Scholar] [CrossRef]\n  22. He, X.; Liao, L.; Zhang, H.; Nie, L.; Hu, X.; Chua, T.S. Neural Collaborative Filtering. In Proceedings of the 26th International Conference on World Wide Web, Perth, Australia, 3\u20137 April 2017; pp. 173\u2013182. [Google Scholar] [CrossRef]\n  23. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A.N.; Kaiser, L.u.; Polosukhin, I. Attention is All you Need. arXiv 2017, arXiv:1706.03762. [Google Scholar] [CrossRef]\n  24. He, K.; Zhang, X.; Ren, S.; Sun, J. Deep Residual Learning for Image Recognition. In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 27\u201330 June 2016; pp. 770\u2013778. [Google Scholar] [CrossRef]\n  25. Simonyan, K.; Zisserman, A. Very Deep Convolutional Networks for Large-Scale Image Recognition. In Proceedings of the 3rd International Conference on Learning Representations (ICLR), San Diego, CA, USA, 7\u20139 May 2015. [Google Scholar]\n  26. Davis, T.A.; Hu, Y. The university of Florida sparse matrix collection. ACM Trans. Math. Softw. 2011, 38, 1\u201325. [Google Scholar] [CrossRef]\n\nFigure 1. The conventional systolic array architecture with weight stationary\ndataflow. When performing , A, B, and C correspond to the input (A in the\nfigure), weight (W in the figure), and output (O in the figure), respectively.\n\nFigure 2. The overall execution flow of the VerSA architecture (A \u00d7 B = C).\nThe steps from \u2780 to \u2786 correspond to sparse mode operations while those from a\nto f correspond to dense mode operations.\n\nFigure 3. An example of a column-wise matrix condensing with a 4 \u00d7 8 weight\nmatrix and = 2. The gray and white cells represent the non-zero and zero\nweight elements, respectively.\n\nFigure 4. The hardware architecture of VerSA. The internal architecture of a\nsingle processing element in VerSA is the same as that in the conventional SA.\nIn the case of the dense mode, the hardware performs the operations of Steps\nb, d, and e, as shown in Figure 2. In the case of the sparse mode, the\nhardware performs the operations of Steps \u2781, \u2783, and \u2784, as shown in Figure 2.\n\nFigure 5. An example of sparse mode operations in VerSA.\n\nFigure 6. The restoration of the partial sum matrices and summation in VerSA,\nwhich correspond to the operations of Steps \u2785 and \u2786 in Figure 2.\n\nFigure 7. An example of dense mode operations in VerSA.\n\nFigure 8. The speedup of VerSA architecture when compared to the conventional\nSA. SM_X% represents the sparse mode with an X% sparsity level. DM is the\ndense mode. The is set to eight in both 128 \u00d7 128 and 256 \u00d7 256 SA.\n\nFigure 9. The energy consumption of our VerSA normalized to the conventional\nSA (=1.0). SM_X% is the sparse mode with an X% sparsity level. DM means the\ndense mode.\n\nTable 1. Summarization the design parameter notation in VerSA.\n\nDescription| Notation Used in the Paper  \n---|---  \nTotal number of rows in the SA  \nThe number of rows in a single subarray  \nThe number of subarrays in the SA  \nThe number of IPBs| = \u2212 1  \n  \nTable 2. A logic synthesis comparison of the conventional SA (Conv_SA) and\nVerSA.\n\nArray Size| Clock Frequency| Design Area (mm^2)| Power (W)  \n---|---|---|---  \nConv_SA| 128 \u00d7 128| N/A| 250 MHz| 19.2346| 1.4145  \nVerSA| 8| 22.0997| 1.6184  \nConv_SA| 256 \u00d7 256| N/A| 76.9042| 5.6125  \nVerSA| 8| 86.6091| 6.2699  \n  \nTable 3. Speedup of our VerSA against the state-of-the-art sparse MM\naccelerator with a 4PE configuration [12].\n\nMatrix| Dimension| Sparsity| 128 \u00d7 128 Speedup| 256 \u00d7 256 Speedup  \n---|---|---|---|---  \nweb-Google| 916 k \u00d7 916 k| 99.9994%| 0.020| 0.071  \nmario002| 390 k \u00d7 390 k| 99.9986%| 0.126| 0.449  \namazon0312| 401 k \u00d7 401 k| 99.9981%| 0.172| 0.614  \nm133-b2| 200 k \u00d7 200 k| 99.9980%| 0.154| 0.552  \ncage12| 130 k \u00d7 130 k| 99.9883%| 3.486| 12.446  \n2cubes-sphere| 101 k \u00d7 101 k| 99.9843%| 5.858| 20.908  \nfilter3D| 106 k \u00d7 106 k| 99.9766%| 9.935| 35.460  \nca-CondMat| 23 k \u00d7 23 k| 99.9656%| 23.553| 83.665  \nwikiVote| 8.3 k \u00d7 8.3 k| 99.8529%| 206.464| 694.082  \npoisson3Da| 14 k \u00d7 14 k| 99.8179%| 284.415| 1,005.949  \nFacebook| 4 k \u00d7 4 k| 98.9331%| 10,692.458| 36,502.207  \n  \nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in\nall publications are solely those of the individual author(s) and\ncontributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s)\ndisclaim responsibility for any injury to people or property resulting from\nany ideas, methods, instructions or products referred to in the content.  \n---  \n\u00a9 2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an\nopen access article distributed under the terms and conditions of the Creative\nCommons Attribution (CC BY) license\n(https://creativecommons.org/licenses/by/4.0/).\n\n## Share and Cite\n\nMDPI and ACS Style\n\nSeo, J.; Kong, J. VerSA: Versatile Systolic Array Architecture for Sparse and\nDense Matrix Multiplications. Electronics 2024, 13, 1500.\nhttps://doi.org/10.3390/electronics13081500\n\nAMA Style\n\nSeo J, Kong J. VerSA: Versatile Systolic Array Architecture for Sparse and\nDense Matrix Multiplications. Electronics. 2024; 13(8):1500.\nhttps://doi.org/10.3390/electronics13081500\n\nChicago/Turabian Style\n\nSeo, Juwon, and Joonho Kong. 2024. \"VerSA: Versatile Systolic Array\nArchitecture for Sparse and Dense Matrix Multiplications\" Electronics 13, no.\n8: 1500. https://doi.org/10.3390/electronics13081500\n\nNote that from the first issue of 2016, this journal uses article numbers\ninstead of page numbers. See further details here.\n\n## Article Metrics\n\nYes\n\n### Citations\n\nNo citations were found for this article, but you may check on Google Scholar\n\nNo\n\n### Article Access Statistics\n\nFor more information on the journal statistics, click here.\n\nMultiple requests from the same IP address are counted as one view.\n\nZoom | Orient | As Lines | As Sticks | As Cartoon | As Surface | Previous Scene | Next Scene\n\n## Cite\n\nExport citation file: BibTeX | EndNote | RIS\n\nMDPI and ACS Style\n\nSeo, J.; Kong, J. VerSA: Versatile Systolic Array Architecture for Sparse and\nDense Matrix Multiplications. Electronics 2024, 13, 1500.\nhttps://doi.org/10.3390/electronics13081500\n\nAMA Style\n\nSeo J, Kong J. VerSA: Versatile Systolic Array Architecture for Sparse and\nDense Matrix Multiplications. Electronics. 2024; 13(8):1500.\nhttps://doi.org/10.3390/electronics13081500\n\nChicago/Turabian Style\n\nSeo, Juwon, and Joonho Kong. 2024. \"VerSA: Versatile Systolic Array\nArchitecture for Sparse and Dense Matrix Multiplications\" Electronics 13, no.\n8: 1500. https://doi.org/10.3390/electronics13081500\n\nNote that from the first issue of 2016, this journal uses article numbers\ninstead of page numbers. See further details here.\n\nclear\n\nElectronics, EISSN 2079-9292, Published by MDPI\n\nRSS Content Alert\n\n### Further Information\n\nArticle Processing Charges Pay an Invoice Open Access Policy Contact MDPI Jobs\nat MDPI\n\n### Guidelines\n\nFor Authors For Reviewers For Editors For Librarians For Publishers For\nSocieties For Conference Organizers\n\n### MDPI Initiatives\n\nSciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS\nProceedings Series\n\n### Follow MDPI\n\nLinkedIn Facebook Twitter\n\n\u00a9 1996-2024 MDPI (Basel, Switzerland) unless otherwise stated\n\nDisclaimer\n\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in\nall publications are solely those of the individual author(s) and\ncontributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s)\ndisclaim responsibility for any injury to people or property resulting from\nany ideas, methods, instructions or products referred to in the content.\n\nTerms and Conditions Privacy Policy\n\nWe use cookies on our website to ensure you get the best experience. Read more\nabout our cookies here.\n\nAccept\n\n## Share Link\n\nCopy\n\nclear\n\n## Share\n\nclear\n\nBack to TopTop\n\n", "frontpage": false}
