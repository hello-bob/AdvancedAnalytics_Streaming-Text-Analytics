{"aid": "40252368", "title": "Always Measure One Level Deeper (2018)", "url": "https://cacm.acm.org/research/always-measure-one-level-deeper/", "domain": "acm.org", "votes": 1, "user": "skilled", "posted_at": "2024-05-03 21:03:45", "comments": 0, "source_title": "Always Measure One Level Deeper \u2013 Communications of the ACM", "source_text": "Always Measure One Level Deeper \u2013 Communications of the ACM\n\n## This website uses cookies\n\nWe use cookies to personalise content and ads, to provide social media\nfeatures and to analyse our traffic. We also share information about your use\nof our site with our social media, advertising and analytics partners who may\ncombine it with other information that you\u2019ve provided to them or that they\u2019ve\ncollected from your use of their services.\n\nDeny Allow all Show details\n\nOK\n\nDeny Allow selection Allow all\n\nShow details\n\nCookie declaration [#IABV2SETTINGS#] About\n\nNecessary (19) Preferences (6) Statistics (11) Marketing (26) Unclassified (0)\n\nNecessary cookies help make a website usable by enabling basic functions like\npage navigation and access to secure areas of the website. The website cannot\nfunction properly without these cookies.\n\nName| Provider| Purpose| Expiry| Type  \n---|---|---|---|---  \n__cf_bm [x3]| ACM Vimeo| This cookie is used to distinguish between humans and\nbots. This is beneficial for the website, in order to make valid reports on\nthe use of their website.| 1 day| HTTP  \nobject(#-#-##:#:#.#)| api.kaltura.nordu.net| Holds the users timezone.|\nPersistent| HTML  \nCookieConsent| Cookiebot| Stores the user's cookie consent state for the\ncurrent domain| 1 year| HTTP  \nwpEmojiSettingsSupports| ACM| This cookie is part of a bundle of cookies which\nserve the purpose of content delivery and presentation. The cookies keep the\ncorrect state of font, blog/picture sliders, color themes and other website\nsettings.| Session| HTML  \nmopDeploy| Mopinion| This is the mechanism by which CACM collects feedback\nfrom website users.| Session| HTML  \n__jid| c.disquscdn.com| Used to add comments to the website and remember the\nuser's Disqus login credentials across websites that use said service.|\nSession| HTTP  \ndisqusauth| c.disquscdn.com| Registers whether the user is logged in. This\nallows the website owner to make parts of the website inaccessible, based on\nthe user's log-in status.| Session| HTTP  \n_cfuvid [x2]| ACM Vimeo| This cookie is a part of the services provided by\nCloudflare - Including load-balancing, deliverance of website content and\nserving DNS connection for website operators.| Session| HTTP  \nhex (32)| CERN| Used to manage server calls to the website's backend systems.|\nSession| HTTP  \n1.gif| Cookiebot| Used to count the number of sessions to the website,\nnecessary for optimizing CMP product delivery.| Session| Pixel  \nbcookie| LinkedIn| Used in order to detect spam and improve the website's\nsecurity.| 1 year| HTTP  \nJSESSIONID [x3]| New Relic LinkedIn Lynda.com| Preserves users states across\npage requests.| Session| HTTP  \nt.gif| WordPress.com| Ensures that product pictures are presented correctly on\nwebsite.| Session| Pixel  \nbscookie| LinkedIn| This cookie is used to identify the visitor through an\napplication. This allows the visitor to login to a website through their\nLinkedIn application for example.| 1 year| HTTP  \n  \nPreference cookies enable a website to remember information that changes the\nway the website behaves or looks, like your preferred language or the region\nthat you are in.\n\nName| Provider| Purpose| Expiry| Type  \n---|---|---|---|---  \naet-dismiss| c.disquscdn.com| Necessary for the functionality of the website's\ncomment-system.| Persistent| HTML  \ndrafts.queue| c.disquscdn.com| Necessary for the functionality of the\nwebsite's comment-system.| Persistent| HTML  \nsubmitted_posts_cache| c.disquscdn.com| Necessary for the functionality of the\nwebsite's comment-system.| Persistent| HTML  \nlang [x2]| LinkedIn Lynda.com| Necessary for maintaining language-settings\nacross subpages on the website.| Session| HTTP  \nlidc| LinkedIn| Registers which server-cluster is serving the visitor. This is\nused in context with load balancing, in order to optimize user experience.| 1\nday| HTTP  \n  \nStatistic cookies help website owners to understand how visitors interact with\nwebsites by collecting and reporting information anonymously.\n\nName| Provider| Purpose| Expiry| Type  \n---|---|---|---|---  \n_hjSession_#| Hotjar| Collects statistics on the visitor's visits to the\nwebsite, such as the number of visits, average time spent on the website and\nwhat pages have been read.| 1 day| HTTP  \n_hjSessionUser_#| Hotjar| Collects statistics on the visitor's visits to the\nwebsite, such as the number of visits, average time spent on the website and\nwhat pages have been read.| 1 year| HTTP  \n_hjTLDTest| Hotjar| Registers statistical data on users' behaviour on the\nwebsite. Used for internal analytics by the website operator.| Session| HTTP  \natuserid| mybbc-analytics.files.bbci.co.uk| Registers statistical data on\nusers' behaviour on the website. Used for internal analytics by the website\noperator.| 13 months| HTTP  \nINVENIOSESSION| CERN| Supports playback tracking for videos embedded from\nCERN.| Session| HTTP  \nINVENIOSESSIONstub| CERN| Supports playback tracking for videos embedded from\nCERN.| Session| HTTP  \njwplayer.captionLabel| CERN| Supports playback tracking for videos embedded\nfrom CERN.| Session| HTTP  \ndisqus_unique| c.disquscdn.com| Collects statistics related to the user's\nvisits to the website, such as number of visits, average time spent on the\nwebsite and loaded pages.| Session| HTTP  \ng.gif| WordPress.com| Registers statistical data on users' behaviour on the\nwebsite. Used for internal analytics by the website operator.| Session| Pixel  \nvuid| Vimeo| Collects data on the user's visits to the website, such as which\npages have been read.| 2 years| HTTP  \ntd| Google| Registers statistical data on users' behaviour on the website.\nUsed for internal analytics by the website operator.| Session| Pixel  \n  \nMarketing cookies are used to track visitors across websites. The intention is\nto display ads that are relevant and engaging for the individual user and\nthereby more valuable for publishers and third party advertisers.\n\nName| Provider| Purpose| Expiry| Type  \n---|---|---|---|---  \n__Host-GAPS| Google| Collects data on the user's visits to the website, such\nas the number of visits, average time spent on the website and what pages have\nbeen loaded with the purpose of generating reports for optimising the website\ncontent.| 2 years| HTTP  \noptin| acm.nui.media| Supports display advertising from CACM's ad partner| 1\nday| HTTP  \n_ga| Google| Used to send data to Google Analytics about the visitor's device\nand behavior. Tracks the visitor across devices and marketing channels.| 2\nyears| HTTP  \n_ga_#| Google| Used to send data to Google Analytics about the visitor's\ndevice and behavior. Tracks the visitor across devices and marketing\nchannels.| 2 years| HTTP  \nbadges-message| c.disquscdn.com| Collects data on the visitor\u2019s use of the\ncomment system on the website, and what blogs/articles the visitor has read.\nThis can be used for marketing purposes.| Persistent| HTML  \nNID| Google| Registers a unique ID that identifies a returning user's device.\nThe ID is used for targeted ads.| 6 months| HTTP  \ncsi| Google| Collects data on visitors' preferences and behaviour on the\nwebsite - This information is used make content and advertisement more\nrelevant to the specific visitor.| Session| Pixel  \n#-#| YouTube| Pending| Session| HTML  \niU5q-!O9@$| YouTube| Registers a unique ID to keep statistics of what videos\nfrom YouTube the user has seen.| Session| HTML  \nLAST_RESULT_ENTRY_KEY| YouTube| Used to track user\u2019s interaction with embedded\ncontent.| Session| HTTP  \nLogsDatabaseV2:V#||LogsRequestsStore| YouTube| Pending| Persistent| IDB  \nremote_sid| YouTube| Necessary for the implementation and functionality of\nYouTube video-content on the website.| Session| HTTP  \nServiceWorkerLogsDatabase#SWHealthLog| YouTube| Necessary for the\nimplementation and functionality of YouTube video-content on the website.|\nPersistent| IDB  \nTESTCOOKIESENABLED| YouTube| Used to track user\u2019s interaction with embedded\ncontent.| 1 day| HTTP  \nVISITOR_INFO1_LIVE| YouTube| Tries to estimate the users' bandwidth on pages\nwith integrated YouTube videos.| 180 days| HTTP  \nVISITOR_PRIVACY_METADATA| YouTube| Stores the user's cookie consent state for\nthe current domain| 180 days| HTTP  \nYSC| YouTube| Registers a unique ID to keep statistics of what videos from\nYouTube the user has seen.| Session| HTTP  \nytidb::LAST_RESULT_ENTRY_KEY| YouTube| Used to track user\u2019s interaction with\nembedded content.| Persistent| HTML  \nYtIdbMeta#databases| YouTube| Used to track user\u2019s interaction with embedded\ncontent.| Persistent| IDB  \nyt-remote-cast-available| YouTube| Stores the user's video player preferences\nusing embedded YouTube video| Session| HTML  \nyt-remote-cast-installed| YouTube| Stores the user's video player preferences\nusing embedded YouTube video| Session| HTML  \nyt-remote-connected-devices| YouTube| Stores the user's video player\npreferences using embedded YouTube video| Persistent| HTML  \nyt-remote-device-id| YouTube| Stores the user's video player preferences using\nembedded YouTube video| Persistent| HTML  \nyt-remote-fast-check-period| YouTube| Stores the user's video player\npreferences using embedded YouTube video| Session| HTML  \nyt-remote-session-app| YouTube| Stores the user's video player preferences\nusing embedded YouTube video| Session| HTML  \nyt-remote-session-name| YouTube| Stores the user's video player preferences\nusing embedded YouTube video| Session| HTML  \n  \nUnclassified cookies are cookies that we are in the process of classifying,\ntogether with the providers of individual cookies.\n\nWe do not use cookies of this type.  \n---  \n  \n[#IABV2_LABEL_PURPOSES#] [#IABV2_LABEL_FEATURES#] [#IABV2_LABEL_PARTNERS#]\n\n[#IABV2_BODY_PURPOSES#]\n\n[#IABV2_BODY_FEATURES#]\n\n[#IABV2_BODY_PARTNERS#]\n\nCookies are small text files that can be used by websites to make a user's\nexperience more efficient.\n\nThe law states that we can store cookies on your device if they are strictly\nnecessary for the operation of this site. For all other types of cookies we\nneed your permission.\n\nThis site uses different types of cookies. Some cookies are placed by third\nparty services that appear on our pages.\n\nYou can at any time change or withdraw your consent from the Cookie\nDeclaration on our website.\n\nLearn more about who we are, how you can contact us and how we process\npersonal data in our Privacy Policy.\n\nPlease state your consent ID and date when you contact us regarding your\nconsent.\n\nCookie declaration last updated on 4/26/24 by Cookiebot\n\nSkip to content\n\nSearch Sign In\n\nJoin ACM\n\nResearch and Advances\n\nComputing Applications Contributed articles\n\n# Always Measure One Level Deeper\n\nPerformance measurements often go wrong, reporting surface-level results that\nare more marketing than science.\n\nBy John Ousterhout\n\nPosted Jul 1 2018\n\n  * Share\n\n    * Twitter\n    * Reddit\n    * Hacker News\n  * Download PDF\n  * Print\n  * Join the Discussion\n  * View in the ACM Digital Library\n\n  1. Introduction\n  2. Key Insights\n  3. Most Common Mistakes\n  4. Keys to High-Quality Performance Analysis\n  5. Measurement Infrastructure\n  6. Conclusion\n  7. Acknowledgments\n  8. References\n  9. Author\n\nPerformance measurement is one of the most important parts of software\ndevelopment. In academic research a thorough performance evaluation is\nconsidered essential for many publications to prove the value of a new idea.\nIn industry, performance evaluation is necessary to maintain a high level of\nperformance across the lifetime of a product. For example, cloud services\npromise to maintain particular performance levels; service providers must thus\nbe able to detect when performance drops below acceptable levels and quickly\nidentify and fix the problem.\n\nBack to Top\n\n### Key Insights\n\n  * Performance measurement is less straightforward than it might seem; it is easy to believe results that are incorrect or misleading and overlook important system behaviors.\n  * The key to good performance measurement is to make many more measurements besides the ones you think will be important; it is crucial to understand not just the system\u2019s performance but also why it performs that way.\n  * Performance measurement done well results in new discoveries about the system being measured and new intuition about system behavior for the person doing the measuring.\n\nA good performance evaluation provides a deep understanding of a system\u2019s\nbehavior, quantifying not only the overall behavior but also its internal\nmechanisms and policies. It explains why a system behaves the way it does,\nwhat limits that behavior, and what problems must be addressed in order to\nimprove the system. Done well, performance evaluation exposes interesting\nsystem properties that were not obvious previously. It not only improves the\nquality of the system being measured but the developer\u2019s intuition, resulting\nin better systems in the future.\n\nUnfortunately, there is no widespread understanding or agreement as to how to\nmeasure performance. Performance evaluation is rarely taught in computer\nscience classes. And new faculty lack well-developed performance-measurement\nskills, making it difficult for them to train their students. The only way to\nbecome expert is through trial and error.\n\nAs a result, performance measurement is often done poorly, even by experienced\ndevelopers. For example, if you have written a conference paper on a software\nsystem, it probably unfolded like this: The system implementation took longer\nthan expected, so performance evaluation could not begin until a week or two\nbefore the paper submission deadline. The first attempts to run benchmarks\nresulted in system crashes, so you spent the next week fixing bugs. At this\npoint the benchmarks ran, but the system\u2019s performance was not much better\nthan the comparison systems. You tried different experiments, hoping to find\none where the system looked good; this exposed yet more bugs that had to be\nfixed. Time was running out, so you stopped measuring as soon as you found an\nexperiment that produced positive results. The paper focused on this\nexperiment, omitting the results that were less favorable. There were a few\nthings about these results that did not make complete sense, but you did your\nbest to come up with plausible explanations for them. There was not enough\ntime to validate or double-check the numbers, and you could only hope there\nwere not too many errors.\n\nMeasurements gathered this way are likely incomplete, misleading, or even\nerroneous. This article describes how to conduct performance measurement well.\nI first discuss five mistakes that account for most of the problems with\nperformance measurements, all of which occurred in the scenario I just\noutlined. I then spell out four rules to follow when evaluating performance.\nThese rules will help you avoid the mistakes and produce high-quality\nperformance evaluations. Finally, I offer four suggestions about\ninfrastructure to assist in performance evaluation.\n\nThe most important idea overall, as reflected in this article\u2019s headline, is\nto dig beneath the surface, measuring the system in depth and detail from\nmultiple angles to create a complete and accurate understanding of\nperformance.\n\nBack to Top\n\n### Most Common Mistakes\n\nWhen performance measurements go wrong, it is usually due to five common\nmistakes:\n\nMistake 1: Trusting the numbers. Engineers are easily fooled during\nperformance measurements because measurement bugs are not obvious. Engineers\nare used to dealing with functional bugs, which tend to be noticeable because\nthey cause the system to crash or misbehave. If the system produces the\ndesired behavior, it is probably working. Engineers tend to apply the same\nphilosophy to performance measurements; if performance numbers are being\ngenerated and the system is not crashing, they assume the numbers are correct.\n\nPerformance-measurement code is just as likely to have bugs as any other code,\nbut the bugs are less obvious. Most bugs in performance-measurement code do\nnot cause crashes or prevent numbers from appearing; they simply produce\nincorrect numbers. There is no easy way to tell from a number whether it is\nright or wrong, so engineers tend to assume the numbers are indeed correct.\nThis is a mistake. There are many ways for errors to creep into performance\nmeasurements. There may be bugs in the benchmarks or test applications, so the\nmeasured behavior is not the desired behavior. There may be bugs in the code\nthat gathers metrics and processes them, as when, say, a clock is read at the\nwrong time or the 99^th percentile is miscomputed. The system being measured\nmay have functional bugs. And, finally, the system may have performance bugs,\nso the measurements do not reflect the system\u2019s true potential.\n\n> Performance measurements should be considered guilty until proven innocent.\n\nI have been involved in dozens of performance-measurement projects and cannot\nrecall a single one in which the first results were correct. In each case\nthere were multiple problems from the list just outlined. Only after working\nthrough them all did my colleagues and I obtain measurements that were\nmeaningful.\n\nMistake 2: Guessing instead of measuring. The second common mistake is to draw\nconclusions about a system\u2019s performance based on educated guesses, without\nmeasurements to back them up. For example, I found the following explanation\nin a paper I reviewed recently: \u201c... throughput does not increase with the\nnumber of threads ... This is because the time taken to traverse the\nrelatively long linked list bounds server performance.\u201d There was no\nindication that the authors had measured the actual length of the list or the\ntime taken to traverse it, yet they stated their conclusion as fact. I\nfrequently encounter unsubstantiated conclusions in papers; there were at\nleast five other occurrences in the paper with the quote.\n\nEducated guesses are often correct and play an important role in guiding\nperformance measurement; see Rule 3 (Use your intuition to ask questions, not\nanswer them). However, engineers\u2019 intuition about performance is not reliable.\nWhen my students and I designed our first log-structured file system,^4 we\nwere fairly certain that reference patterns exhibiting locality would result\nin better performance than those without locality. Fortunately, we decided to\nmeasure, to be sure. To our surprise, the workloads with locality behaved\nworse than those without. It took considerable analysis to understand this\nbehavior. The reasons were subtle, but they exposed important properties of\nthe system and led us to a new policy for garbage collection that improved the\nsystem\u2019s performance significantly. If we had trusted our initial guess, we\nwould have missed an important opportunity for performance improvement.\n\nIt is unsafe to base conclusions on intuition alone, yet engineers do it all\nthe time. A common mistake is for an engineer to hypothesize that a particular\ndata structure is too slow and then replace it with a new data structure the\nengineer believes will be faster. If the problem is not verified by measuring\nperformance, there is a good chance the optimization will not improve\nperformance. The code change will simply waste a lot of time and probably\nintroduce unnecessary complexity.\n\nWhen I find a guess presented as fact and ask for justification, I sometimes\nget this response: \u201cWhat else could it possibly be?\u201d But this is a cop-out,\nsuggesting it is up to others to prove the theory wrong and OK to make\nunsubstantiated claims until someone else proves them false. In some cases the\nperson making the comment feels a process of elimination had been used, ruling\nout all possible alternatives. Unfortunately, a process of elimination is not\nreliable in performance evaluation, because it is not possible to know with\ncertainty that every possible cause has been considered. Many factors can\ninfluence performance, and the ultimate cause of behavior is often something\nnon-obvious, meaning a process of elimination will not consider it. It is\nunsafe to present an explanation as fact unless measurements confirm the\nspecific behavior(s).\n\nMistake 3: Superficial measurements. Most performance measurements I see are\nsuperficial, measuring only the outermost visible behavior of a system (such\nas the overall running time of an application or the average latency of\nrequests made to a server). These measurements are essential, as they\nrepresent the bottom line by which a system is likely to be judged, but they\nare not sufficient. They leave many questions unanswered (such as \u201cWhat are\nthe limits that keep the system from performing better?\u201d and \u201cWhich of the\nimprovements had the greatest impact on performance?\u201d). In order to get a deep\nunderstanding of system performance, the internal behavior of a system must be\nmeasured, in addition to its top-level performance.\n\nSuperficial measurements are often combined with Mistake 1 (Trusting the\nnumbers) and Mistake 2 (Guessing instead of measuring); the engineers measure\nonly top-level performance, assume the numbers are correct, and then invent\nunderlying behaviors to explain the numbers. For example, I found the\nfollowing claim in a paper I reviewed recently (system names obscured to\npreserve author anonymity): \u201cUnlike YYY, XXX observes close-to-linear-\nthroughput scaling with more publishers due to its lock-free resolution of\nwrite-write contentions.\u201d The paper measured scaling, but there were no\nmeasurements of write-write contention, and systems XXX and YYY differed in\nmany ways, so other explanations were possible for the performance difference.\n\nMistake 4: Confirmation bias. Performance measurement is rarely indifferent;\nwhen you measure performance, you are probably hoping for a particular\noutcome. If you have just built a new system or improved an existing one, you\nprobably hope the performance measurements will show your ideas were good\nones. If the measurements turn out well, it increases the likelihood your\npaper will be accepted or your boss will be impressed.\n\nUnfortunately, such hope results in a phenomenon called \u201cconfirmation bias.\u201d^1\nConfirmation bias causes people to select and interpret data in a way that\nsupports their hypotheses. For example, confirmation bias affects your level\nof trust. When you see a result that supports your hypothesis, you are more\nlikely to accept the result without question. In contrast, if a measurement\nsuggests your new approach is not performing well, you are more likely to dig\ndeeper to understand exactly what is happening and perhaps find a way to fix\nthe problem. This means that an error in a positive result is less likely to\nbe detected than is an error in a negative result.\n\nWhen choosing benchmarks, you are more likely to choose ones that produce the\ndesired results and less likely to choose ones that show the weaknesses of\nyour approach. For example, a recent paper described a new network protocol\nand compared it to previous proposals. The previous proposals had all measured\nlatency using the 99^th-percentile worst case, but this particular paper\nmeasured at the median. The results appeared favorable for the new proposal.\nMy students reran the measurements for the new protocol and discovered its\n99^th-percentile latency was significantly worse than the comparison\nprotocols. We wondered if the paper\u2019s authors had intentionally switched\nmetrics to exaggerate the performance of their protocol.\n\nConfirmation bias also affects how you present information. You are more\nlikely to include results that support your hypothesis and downplay or omit\nresults that are negative. For example, I frequently see claims in papers of\nthe form: \u201cXXX is up to 3.5x faster than YYY.\u201d Such claims cherry-pick the\nbest result to report and are misleading because they do not indicate what\nperformance can be expected in the common case. Statements like this belong in\nlate-night TV commercials, not scientific papers.\n\nIf applied consciously, bias is intellectually dishonest. Even if not applied\nconsciously, it can cause results to be reported in a way that is more\nmarketing than science; it sounds like you are trying to sell a product rather\nthan uncover the truth about a system\u2019s behavior. Confirmation bias makes it\nmore likely that results will be inaccurate (because you did not find bugs) or\nmisleading (because you did not present all relevant data).\n\nMistake 5: Haste. The last mistake in performance evaluation is not allowing\nenough time. Engineers usually underestimate how long it takes to measure\nperformance accurately, so they often carry out evaluations in a rush. When\nthis happens, they will make mistakes and take shortcuts, leading to all the\nother mistakes.\n\nThe time issue is particularly problematic when working under a deadline (such\nas for a conference publication). There is almost always a rush to meet the\nsubmission deadline. The system implementation always takes longer than\nexpected, delaying the start of performance measurement; there is often only a\nweek or two for evaluation before the submission deadline, resulting in a\nsloppy evaluation. In principle, authors could keep working on the\nmeasurements while waiting for the paper to be reviewed, but in practice this\nrarely happens. Instead, they tell themselves: \u201cLet\u2019s not spend more time on\nthe paper until we see whether it is accepted.\u201d Once the paper is accepted,\nthere are only a few weeks before the deadline for final papers, so there is\nyet another rush.\n\nBack to Top\n\n### Keys to High-Quality Performance Analysis\n\nConsider four rules that are likely to prevent the mistakes from the preceding\nsection and lead to trustworthy and informative evaluations:\n\nRule 1: Allow lots of time. The first step toward high-quality performance\nmeasurements is to allow enough time. If you are measuring a non-trivial\nsystem, you should plan on at least two to three months. I tell my graduate\nstudents to aim for a complete set of preliminary measurements at least one\nmonth before the submission deadline; even this is barely enough time to find\nand fix problems with both the measurements and the system.\n\nPerformance analysis is not an instantaneous process like taking a picture of\na finished artwork. It is a long and drawn-out process of confusion,\ndiscovery, and improvement. Performance analysis goes through several phases,\neach of which can take anywhere from a few days to a few weeks. First, you\nmust add instrumentation code to the system to record the desired metrics. You\nmust then get benchmark applications running, either by writing them or by\ndownloading and installing existing programs. Running benchmarks will probably\nstress the system enough to expose bugs, and you will need to then track down\nand fix them. Eventually, the system will run well enough to start producing\nperformance numbers. However, these numbers will almost certainly be wrong.\nThe next step is to find and fix bugs in the measurements. Once you have\nverified the accuracy of the measurements, you will start to uncover problems\nwith the system itself. As you look over the performance measurements, you\nwill probably uncover additional functional bugs. Once they have been fixed,\nyou can start analyzing the performance in depth. You will almost certainly\ndiscover opportunities to improve performance, and it is important to have\nenough time to make these improvements. You will encounter many things that do\nnot make sense; in order to resolve them, you will need to add new metrics and\nvalidate them. To get the best results, you must iterate several times\nimproving the metrics, measuring performance, and improving the system.\n\nRule 2: Never trust a number generated by a computer. Under Mistake 2\n(Guessing instead of measuring), I discussed how it is tempting to believe\nperformance numbers, even though they are often wrong. The only way to\neliminate this mistake is to distrust every measurement until it has been\ncarefully validated. Performance measurements should be considered guilty\nuntil proven innocent. When students come to me with measurements, I often\nchallenge them by asking: \u201cSuppose I said I don\u2019t believe these measurements.\nWhat can you say to convince me that they are correct?\u201d\n\nThe way to validate a measurement is to find different ways to measure the\nsame thing:\n\nTake different measurements at the same level. For example, if you are\nmeasuring file-system throughput, do not measure just the throughput seen by a\nuser application; also measure the throughput observed inside the operating\nsystem (such as at the file block cache). These measurements should match;\n\nMeasure the system\u2019s behavior at a lower level to break down the factors that\ndetermine performance, as I discuss later under Rule 4 (Always measure one\nlevel deeper);\n\nMake back-of-the-envelope calculations to see if the measurements are in the\nballpark expected; and\n\nRun simulations and compare their results to measurements of the real\nimplementation.\n\nAs you begin collecting measurements, compare them and be alert for\ninconsistencies. There will almost always be things that do not make sense.\nWhen something does not make complete sense, stop and gather more data. For\nexample, in a recent measurement of a new network transport protocol, a\nbenchmark indicated that a server could handle no more than 600,000 packets\nper second. However, my colleagues and I had seen servers process more than\n900,000 packets per second with other protocols and believed the new protocol\nwas at least as efficient as the old ones. We decided to gather additional\ndata. As a result, we discovered a bug in the flow-control mechanism on the\nclient side: clients were not transmitting data fast enough to keep the server\nfully loaded. Fixing the bug improved performance to the level we expected.\n\nFurther analysis will sometimes show the unexpected behavior is correct, as in\nthe log-structured file system example discussed under Mistake 2 (Guessing\ninstead of measuring); such situations are usually interesting, and you will\nlearn something important as you resolve the contradiction. In my experience,\ninitial performance measurements are always riddled with contradictions and\nthings we don\u2019t understand, and resolving them is always useful; either we fix\na problem or we deepen our understanding of the system.\n\nAbove all, do not tolerate anything you do not understand. Assume there are\nbugs and problems with every measurement, and your job is to find and fix\nthem. If you do not find problems, you should feel uneasy, because there are\nprobably bugs you missed. Curmudgeons make good performance evaluators because\nthey trust nothing and enjoy finding problems.\n\nRule 3: Use your intuition to ask questions, not to answer them. Intuition is\na wonderful thing. As you accumulate knowledge and experience in an area, you\nwill start having gut-level feelings about a system\u2019s behavior and how to\nhandle certain problems. If used properly, such intuition can save significant\ntime and effort. However, it is easy to become over-confident and assume your\nintuition is infallible. This leads to Mistake 2 (Guessing instead of\nmeasuring).\n\nThe best way to use intuition is to identify promising areas for further\nexploration. For example, when looking over performance measurements, ask\nyourself if they make sense. How does the performance compare to what you\nexpected? Does it seem too good to be true? Does the system scale more poorly\nthan you had hoped? Does a curve jump unexpectedly when you expected it to be\nsmooth? Do some benchmarks exhibit behavior that is dramatically different\nfrom others? Consider anything that does not match your intuition a red flag\nand investigate it, as described in Rule 2 (Never trust a number generated by\na computer). Intuition can be very helpful in identifying problems.\n\nIntuition is great for directing attention but not reliable enough to make\ndecisions on it alone. Intuition should always be validated with data before\nmaking decisions or claims. If your intuition suggests why a particular result\nis occurring, follow it up with measurements that prove or disprove the\nintuition. Draw conclusions based on the measurements, not the intuition, and\ninclude some of the measured data in the conclusion, so others know it is not\njust a guess.\n\nIf you continually form intuitions and then test them you will gain knowledge\nthat helps you form better intuition in the future. Every false intuition\nmeans there was something you did not fully understand; in the process of\ntesting it and discovering why it is false, you will learn something useful.\n\nRule 4: Always measure one level deeper. If you want to understand the\nperformance of a system at a particular level, you must measure not just that\nlevel but also the next level deeper. That is, measure the underlying factors\nthat contribute to the performance at the higher level. If you are measuring\noverall latency for remote procedure calls, you could measure deeper by\nbreaking down that latency, determining how much time is spent in the client\nmachine, how much time is spent in the network, and how much time is spent on\nthe server. You could also measure where time is spent on the client and\nserver. If you are measuring the overall throughput of a system, the system\nprobably consists of a pipeline containing several components. Measure the\nutilization of each component (the fraction of time that component is busy).\nAt least one component should be 100% utilized; if not, it should be possible\nto achieve a higher throughput.\n\nMeasuring deeper is the best way to validate top-level measurements and\nuncover bugs. Once you have collected some deeper measurements, ask yourself\nwhether they seem consistent with the top-level measurements and with each\nother. You will almost certainly discover things that do not make sense; make\nadditional measurements to resolve the contradictions. For example, in a\nrecent analysis of a distributed transaction processing system, deeper\nmeasurements by my students included network throughput and disk throughput.\nWe were surprised to see that the network throughput was greater than the disk\nthroughput; this did not make sense, since every byte had to pass through both\nthe network and the disk. It turned out that the disk subsystem had been\nconfigured with no limit on queue length; the disk was not keeping up, and its\noutput queue was growing without bound. Once the students set a limit on queue\nlength, the rest of the system throttled itself to match the disk throughput.\nUnfortunately, this meant our initial measurements of overall throughput were\ntoo optimistic.\n\n> Curmudgeons make good performance evaluators because they trust nothing and\n> enjoy finding problems.\n\nMeasuring deeper will also indicate whether you are getting the best possible\nperformance and, if not, how to improve it. Use deeper measurements to find\nout what is limiting performance. Try to identify the smallest elements that\nhave the greatest impact on overall performance. For example, if the overall\nmetric is latency, measure the individual latencies of components along the\ncritical path; typically, there will be a few components that account for most\nof the overall latency. You can then focus on optimizing those components.\n\nIn recent measurements of a new network transport, one of my students found\nthat round-trip tail latency was higher than our simulations had predicted.\nThe student measured software latency in detail on both the sending and the\nreceiving machines but found nothing that could account for the high tail\nlatency. At this point we were about to conclude that the delays must be\ncaused by the network switch. What else could it be? This would have been\nMistake 2 (Guessing instead of measuring). Before giving up, we decided to dig\ndeeper and measure precise timings for each individual packet. The\nmeasurements surprised us, showing that outlier delays were not isolated\nevents. Delay tended to build up over a series of packets, affecting all of\nthe packets from a single sender over a relatively long time interval,\nincluding packets for different destinations. This was a crucial clue. After\nseveral additional measurements, the student discovered that long queues were\nbuilding up in the sender\u2019s network interface due to a software bug. The\ntransport included code to estimate the queue length and prevent queue\nbuildup, but there was a bug in the estimator caused by underflow of an\nunsigned integer. The underflow was easy to fix, at which point tail latency\ndropped dramatically. Not only did this process improve the system\u2019s\nperformance, it taught us an important lesson about the risks of unsigned\nintegers.\n\nAnother way to measure deeper is to consider more detail. Instead of just\nlooking at average values, graph the entire distribution and noodle over the\nshape to see if it provides useful information. Then look at some of the raw\ndata samples to see if there are patterns. In one measurement of RPC latency,\na student found that the average latency was higher than we expected. The\nlatency was not intolerably high, and it would have been easy to simply accept\nthis level of performance. Fortunately, the student decided to graph the times\nfor individual RPCs. It turned out the data was bimodal, whereby every other\nRPC completed quickly, but the intervening ones were all significantly slower.\nWith this information, the student tracked down and fixed a configuration\nerror that eliminated all of the slow times. In this case, the average value\nwas not a good indicator of system behavior.\n\nThe examples in this article may seem so esoteric that they must be outliers,\nbut they are not. Every performance measurement project I have seen has had\nmultiple such examples, which are extremely subtle, difficult to track down,\nand defy all intuition, until suddenly a simple explanation appears (such as\nunsigned integer underflow). Deeper measurements almost always produce\nsubstantial performance improvement, important discoveries about system\nbehavior, or both.\n\n> It can be as fancy as an interactive webpage or as simple as a text file,\n> but a dashboard is essential for any nontrivial measurement effort.\n\nDo not spend a lot of time agonizing over which deeper measurements to make.\nIf the top-level measurements contain contradictions or things that are\nsurprising, start with measurements that could help resolve them. Or pick\nmeasurements that will identify performance bottlenecks. If nothing else,\nchoose a few metrics that are most obvious and easiest to collect, even if you\nare not sure they will be particularly illuminating. Once you look at the\nresults, you will almost certainly find things that do not make sense; from\nthis point on, track down and resolve everything that does not make perfect\nsense. Along the way you will discover other surprises; track them down as\nwell. Over time, you will develop intuition about what kinds of deeper\nmeasurements are most likely to be fruitful.\n\nMeasuring deeper is the single most important ingredient for high-quality\nperformance measurement. Focusing on this one rule will prevent most of the\nmistakes anyone could potentially make. For example, in order to make deeper\nmeasurements you will have to allocate extra time. Measuring deeper will\nexpose bugs and inconsistencies, so you will not accidentally trust bogus\ndata. Most of the suggestions under Rule 2 (Never trust a number generated by\na computer) are actually examples of measuring deeper. You will never need to\nguess the reasons for performance, since you will have actual data. Your\nmeasurements will not be superficial. Finally, you are less likely to be\nderailed by subconscious bias, since the deeper measurements will expose\nweaknesses, as well as strengths.\n\nBack to Top\n\n### Measurement Infrastructure\n\nMaking good performance measurements takes time, so it is worth creating\ninfrastructure to help you work more efficiently. The infrastructure will\neasily pay for itself by the time the measurement project is finished.\nFurthermore, performance measurements tend to be run repeatedly, making\ninfrastructure even more valuable. In a cloud service provider, for example,\nmeasurements must be made continuously in order to maintain contractual\nservice levels. In a research project, the full suite of performance\nmeasurements will be run several times (such as before submission, after the\npaper is accepted, and again during the writing of a Ph.D. dissertation). It\nis important to have infrastructure that makes it easy to rerun tests.\n\nAutomate measurements. It should be possible to type a single command line\nthat invokes the full suite of measurements, including not just top-level\nmeasurements but also the deeper measurements. Each run should produce a large\namount of performance data in an easy-to-read form. It should also be easy to\ninvoke a single benchmark by itself or vary the parameters for a benchmark.\nAlso useful is a tool that can compare two sets of output to identify\nnontrivial changes in performance.\n\nCreate a dashboard. A dashboard is a display that shows all performance\nmeasurements from a particular run of a particular benchmark or from a\ndeployed system. If you have been measuring deeply, the dashboard can easily\nshow hundreds of measurements. A good dashboard brings together a lot of data\nin one place and makes it easy to examine performance from many angles. It can\nbe as fancy as an interactive webpage or as simple as a text file, but a\ndashboard is essential for any nontrivial measurement effort.\n\nFigure 1 shows approximately one-third of a dashboard my students created to\nanalyze the performance of crash recovery in a distributed storage system.^3\nIn this benchmark, one of the system\u2019s storage servers has crashed, and\nseveral other servers (\u201crecovery masters\u201d) reconstruct the lost data by\nreading copies stored on a collection of backup servers. This sample\nillustrates several important features of dashboards. Any dashboard should\nstart with a summary section, giving the most important metric(s)\u2014total\nrecovery time in this case\u2014and the parameters that controlled the benchmark.\nEach of the remaining sections digs deeper into one specific aspect of the\nperformance. For example, \u201cRecovery Master Time\u201d analyzes where the recovery\nmasters spent their time during recovery, showing CPU time for each component\nas both an absolute time and a percentage of total recovery time; the\npercentages help identify bottlenecks. It was important for the storage system\nbeing analyzed to make efficient use of the network during recovery, so we\nadded a separate section to analyze network throughput for each of the\nservers, as well as for the cluster as a whole. Most measurements in the\ndashboard show averages across a group of servers, but in several cases the\nworst-case server is also shown. The dashboard also has a special section\nshowing the worst-case performance in several categories, making it possible\nto see whether outliers are affecting overall performance.\n\nFigure 1. Excerpt from the dashboard used to evaluate crash recovery in a\nlarge-scale main memory storage system.^3\n\nYou should create a simple dashboard as soon as you start making measurements;\ninitially, it will include just the benchmark parameters and a few overall\nmetrics. Every time you think of a new question to answer or a deeper\nmeasurement to take, add more data to the dashboard. Never remove metrics from\nthe dashboard, even if you think you will never need them again. You probably\nwill.\n\nIf you make a change and performance suddenly degrades, you can scan the\ndashboard for metrics that have changed significantly. The dashboard might\nindicate that, for example, the network is now overloaded or the fraction of\ntime waiting for incoming segments suddenly increased. You can maintain a\n\u201cgood\u201d dashboard for comparing with later dashboards and record dashboards at\nregular time intervals to track performance over long periods of time. A\ndashboard serves a purpose for performance measurement similar to that of unit\ntests for functional testing\u2014providing a detailed analysis and making it easy\nto detect regressions.\n\nDo not remove the instrumentation. Leave as much instrumentation as possible\nin the system at all times, so performance information is constantly\navailable. For online services that run continuously, the dashboard should\ntake the form of a webpage that can be displayed at any time. For applications\nthat are run manually, it is convenient to have a command-line switch that\nwill cause performance metrics to be recorded during execution and dumped when\nthe application finishes.\n\nOne simple-yet-effective technique is to define a collection of counters that\naccumulate statistics (such as number of invocations of each externally\nvisible request type and number of network bytes transmitted and received).\nIncrementing a counter is computationally inexpensive enough that a system can\ninclude a large number of them without hurting its performance. Make it easy\nto define new counters and read out all existing counters. For long-running\nservices, it should be possible to sample the counters at regular intervals,\nand the dashboard should display historical trends for the counters.\n\nPresentation matters. If you want to analyze performance in depth,\nmeasurements must be displayed in a way that exposes a lot of detail and\nallows it to be understood easily. In addition, the presentation must clarify\nthe things that are most important. Think of this as feeding your intuition.\nThe way to discover interesting things is to absorb a lot of information and\nlet your intuition go to work, identifying patterns, contradictions, and\nthings that seem like they might be significant. You can then explore them in\nmore detail.\n\nWhen students bring their first measurements to me, the measurements are often\nin a barely comprehensible form (such as unaligned comma-separated values),\ntelling me they did not want to spend time on a nice graph until they knew\nwhat data is important. However, the early phase of analysis, where you are\ntrying to figure out what is happening and why, is when it is most important\nfor information to be presented clearly. It is worth getting in the habit of\nalways presenting data clearly from the start (such as with graphs rather than\ntables). Do not waste time with displays that are difficult to understand.\nMaking graphs takes little time once you have learned how to use the tools,\nand you can reuse old scripts for new graphs. Consider clarity even when\nprinting raw data, because you will occasionally want to look at it. Arrange\nthe data in neat columns with labels, and use appropriate units (such as\nmicroseconds), rather than, say, \u201c1.04e-07.\u201d\n\nFigure 2 and Figure 3 show how the organization of a graph can have a big\neffect on how easy (or difficult) it is to visualize performance data. In\nFigure 2 my students and I aimed to understand tail latency (99.9^th or\n99.99^th percentile worst-case performance) for write requests in the RAMCloud\nstorage system. A traditional cumulative distribution function (CDF) like the\none in Figure 2a emphasizes the mean value but makes it difficult to see tail\nlatency. When we switched to a reverse cumulative distribution function with\nlog-scale axes (see Figure 2b) the complete tail became visible, all the way\nout to the slowest of 100 million total samples. Figure 2b made it easy to see\nfeatures worthy of additional study (such as the \u201cshoulders\u201d at approximately\n70 us and 1 ms); additional measurements showed the shoulder at 1 ms was\ncaused by interference from concurrent garbage collection. If we had only\nconsidered a few discreet measurements of tail latency we might not have\nnoticed these features.\n\nFigure 2. Two different ways to display tail latency: (a) a traditional CDF\nwith linear axes; and (b) a complementary CDF (each y-value is the fraction of\nsamples greater than the corresponding x-value) with log-scale axes.\n\nFigure 3. Each figure displays 99^th-percentile slowdown (delivery time for\nmessages of a given size, divided by the best possible time for messages of\nthat size) as a function of message size in a given workload: (a) x-axis is\nlinear; (b) x-axis is logarithmic; and (c) x-axis is scaled to match the CDF\nof message lengths. Different curves correspond to different settings of the\n\u201cunscheduled bytes\u201d parameter.\n\nFigure 3 arose during development of a new network transport protocol. My\nstudents and I wanted to understand the effect of a particular parameter\nsetting on the delivery time for messages of different size in a given\nworkload. The first question we had to address in graphing the data was what\nmetric to display. Displaying the absolute delivery times for messages would\nnot be very useful, since it would not be obvious whether a particular time is\ngood. Furthermore, comparisons between messages of different lengths would not\nbe meaningful, as longer messages inherently take more time to deliver.\nInstead, we displayed slowdown, the actual delivery time for a message divided\nby the best possible time for messages of that size. This choice made it easy\nto see whether a particular time is indeed good; 1.0 is perfect, 2.0 means the\nmessage took twice as long as necessary, and so on. Slowdown also made it\npossible to compare measurements for messages of different length, since\nslowdown takes into account the inherent cost for each length.\n\nThe second question was the choice of the x-axis. An obvious choice would have\nbeen a linear x-axis, as in Figure 3a. However, the vast majority of messages\nis very small, so almost all the messages are bunched together at the left\nedge of that graph. A log-scale x-axis (see Figure 3b) makes it easier to see\nthe small messages but still does not indicate how many messages were affected\nby each value of the parameter. To address this problem, we rescaled the\nx-axis to match the distribution of message lengths (see Figure 3c); the\nx-axis is labeled with message size but is linear in number of messages, with\neach of the 10 tickmarks corresponding to 10% of all messages. With this view\nof the data it became easy to see that the parameter matters, as it affected\napproximately 70% of all messages in the experiment (those smaller than\napproximately 5 Kbytes).\n\nFigure 3c includes more information than the other graphs; in addition to\ndisplaying slowdown, it also displays the CDF of message sizes via the x-axis\nlabels. As a result it is easy to see that messages in this workload are\nmostly short; 60% of all messages require no more than 960 bytes. Figure 3c\nmakes it clear that Figure 3a and Figure 3b are misleading.\n\nBack to Top\n\n### Conclusion\n\nThe keys to good performance evaluation are a keen eye for things that do not\nmake sense and a willingness to measure from many different angles. This takes\nmore time than the quick and shallow measurements that are common today but\nprovides a deeper and more accurate understanding of the system being\nmeasured. In addition, if you apply the scientific method, making and testing\nhypotheses, you will improve your intuition about systems. This will result in\nboth better designs and better performance measurements in the future.\n\nBack to Top\n\n### Acknowledgments\n\nThis article benefited from comments and suggestions from Jonathan Ellithorpe,\nCollin Lee, Yilong Li, Behnam Montazeri, Seojin Park, Henry Qin, Stephen Yang,\nand the anonymous Communications reviewers.\n\nBack to Top\n\nBack to Top\n\n1\\. Nickerson, R.S. Confirmation bias: A ubiquitous phenomenon in many guises.\nReview of General Psychology 2, 2 (June 1998), 175\u2013220.\n\n2\\. Ousterhout, J. A Philosophy of Software Design. Yaknyam Press, Palo Alto,\nCA, 2018.\n\n3\\. Ousterhout, J., Gopalan, A., Gupta, A., Kejriwal, A., Lee, C., Montazeri,\nB., Ongaro, D., Park, S.J., Qin, H., Rosenblum, M. et al. The RAMCloud storage\nsystem. ACM Transactions on Computer Systems 33, 3 (Aug. 2015), 7.\n\n4\\. Rosenblum, M. and Ousterhout, J.K. The design and implementation of a log-\nstructured file system. ACM Transactions on Computer Systems 10, 1 (Feb.\n1992), 26\u201352.\n\n#### Author\n\nJohn Ousterhout (ouster@cs.stanford.edu) is the Bosack Lerner Professor of\nComputer Science at Stanford University, Stanford, CA, USA.\n\n  * Share\n\n    * Twitter\n    * Reddit\n    * Hacker News\n  * Download PDF\n  * Print\n  * Join the Discussion\n\nSubmit an Article to CACM\n\nCACM welcomes unsolicited submissions on topics of relevance and value to the\ncomputing community.\n\nYou Just Read\n\n#### Always Measure One Level Deeper\n\nView in the ACM Digital Library\n\n\u00a92018 ACM 0001-0782/18/7\n\nPermission to make digital or hard copies of part or all of this work for\npersonal or classroom use is granted without fee provided that copies are not\nmade or distributed for profit or commercial advantage and that copies bear\nthis notice and full citation on the first page. Copyright for components of\nthis work owned by others than ACM must be honored. Abstracting with credit is\npermitted. To copy otherwise, to republish, to post on servers, or to\nredistribute to lists, requires prior specific permission and/or fee. Request\npermission to publish from permissions@acm.org or fax (212) 869-0481.\n\nThe Digital Library is published by the Association for Computing Machinery.\nCopyright \u00a9 2018 ACM, Inc.\n\n### DOI\n\n10.1145/3213770\n\n### July 2018 Issue\n\nPublished: July 1, 2018\n\nVol. 61 No. 7\n\nPages: 74-83\n\nTable of Contents\n\n### Related Reading\n\n  * Research and Advances\n\nA Framework For Health Care Information Assurance Policy and Compliance\n\nComputing Applications\n\n  * BLOG@CACM\n\nTransitioning to Distance Learning and Virtual Conferencing\n\nComputing Applications\n\n  * Research and Advances\n\n-Understanding the Seductive Experience\n\nComputing Applications\n\n  * Research and Advances\n\nA Roadmap for Using Continuous Integration Environments\n\nData and Information\n\nAdvertisement\n\nAdvertisement\n\n### Join the Discussion (0)\n\n#### Become a Member or Sign In to Post a Comment\n\nSign In Sign Up\n\n### The Latest from CACM\n\nExplore More\n\nBLOG@CACM May 3 2024\n\nPioneering Sustainable IT with Green Computing\n\nAlex Williams\n\nArchitecture and Hardware\n\nNews May 2 2024\n\n3D Printing Finds a Home\n\nSamuel Greengard\n\nArchitecture and Hardware\n\nBLOG@CACM May 1 2024\n\nHiPEAC\u2019s Vision for the Future\n\nTullio Vardanega and Marc Duranton\n\nComputing Profession\n\n### Shape the Future of Computing\n\nACM encourages its members to take a direct hand in shaping the future of the\nassociation. There are more ways than ever to get involved.\n\nGet Involved\n\n### Communications of the ACM (CACM) is now a fully Open Access publication.\n\nBy opening CACM to the world, we hope to increase engagement among the broader\ncomputer science community and encourage non-members to discover the rich\nresources ACM has to offer.\n\nLearn More\n\nTopics\n\n  * Architecture and Hardware\n  * Artificial Intelligence and Machine Learning\n  * Computer History\n  * Computing Applications\n  * Computing Profession\n  * Data and Information\n  * Education\n  * HCI\n  * Philosophy of Computing\n  * Security and Privacy\n  * Society\n  * Software Engineering and Programming Languages\n  * Systems and Networking\n  * Theory\n\nMagazine\n\n  * Latest Issue\n  * Magazine Archive\n  * Editorial Staff and Board\n  * Submit an Article\n  * Alerts & Feeds\n  * Author Guidelines\n\nCommunications of the ACM\n\n  * About Us\n  * Frequently Asked Questions\n  * Contact Us\n  * For Advertisers\n  * Join ACM\n\n\u00a9 2024 Communications of the ACM. All Rights Reserved.\n\n  * Cookie Notice\n  * Privacy Policy\n\n", "frontpage": false}
