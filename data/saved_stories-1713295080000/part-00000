{"aid": "40051486", "title": "Higher-order Granger reservoir computing", "url": "https://www.nature.com/articles/s41467-024-46852-1", "domain": "nature.com", "votes": 1, "user": "Anon84", "posted_at": "2024-04-16 13:15:30", "comments": 0, "source_title": "Higher-order Granger reservoir computing: simultaneously achieving scalable complex structures inference and accurate dynamics prediction", "source_text": "Higher-order Granger reservoir computing: simultaneously achieving scalable complex structures inference and accurate dynamics prediction | Nature Communications\n\nLoading [MathJax]/jax/output/HTML-CSS/config.js\n\nSkip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n\n## Search\n\nAdvanced search\n\n### Quick links\n\n    * Explore articles by subject\n    * Find a job\n    * Guide to authors\n    * Editorial policies\n\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n\n  * Sign up for alerts\n  * RSS feed\n\nHigher-order Granger reservoir computing: simultaneously achieving scalable\ncomplex structures inference and accurate dynamics prediction\n\nDownload PDF\n\nDownload PDF\n\n  * Article\n  * Open access\n  * Published: 20 March 2024\n\n# Higher-order Granger reservoir computing: simultaneously achieving scalable\ncomplex structures inference and accurate dynamics prediction\n\n  * Xin Li^1,2,\n  * Qunxi Zhu ORCID: orcid.org/0000-0001-7281-5274^2,3,\n  * Chengli Zhao ORCID: orcid.org/0009-0005-1993-0229^1,\n  * Xiaojun Duan^1,\n  * Bolin Zhao^2,3,\n  * Xue Zhang^1,\n  * Huanfei Ma ORCID: orcid.org/0000-0002-4262-0123^4,\n  * Jie Sun^2,5 &\n  * ...\n  * Wei Lin ORCID: orcid.org/0000-0002-1863-4306^2,3,6\n\nNature Communications volume 15, Article number: 2506 (2024) Cite this article\n\n  * 5405 Accesses\n\n  * 1 Citations\n\n  * 45 Altmetric\n\n  * Metrics details\n\n## Abstract\n\nRecently, machine learning methods, including reservoir computing (RC), have\nbeen tremendously successful in predicting complex dynamics in many fields.\nHowever, a present challenge lies in pushing for the limit of prediction\naccuracy while maintaining the low complexity of the model. Here, we design a\ndata-driven, model-free framework named higher-order Granger reservoir\ncomputing (HoGRC), which owns two major missions: The first is to infer the\nhigher-order structures incorporating the idea of Granger causality with the\nRC, and, simultaneously, the second is to realize multi-step prediction by\nfeeding the time series and the inferred higher-order information into HoGRC.\nWe demonstrate the efficacy and robustness of the HoGRC using several\nrepresentative systems, including the classical chaotic systems, the network\ndynamical systems, and the UK power grid system. In the era of machine\nlearning and complex systems, we anticipate a broad application of the HoGRC\nframework in structure inference and dynamics prediction.\n\n### Similar content being viewed by others\n\n### Collective intelligence: A unifying concept for integrating biology across\nscales and substrates\n\nArticle Open access 28 March 2024\n\nPatrick McMillen & Michael Levin\n\n### Reconstructing the evolution history of networked complex systems\n\nArticle Open access 02 April 2024\n\nJunya Wang, Yi-Jiao Zhang, ... Yanqing Hu\n\n### High-throughput prediction of protein conformational distributions with\nsubsampled AlphaFold2\n\nArticle Open access 27 March 2024\n\nGabriel Monteiro da Silva, Jennifer Y. Cui, ... Brenda M. Rubenstein\n\n## Introduction\n\nMachine learning has been recently recognized as a vital engine in efficiently\naddressing numerous scientific and real-world problems that are not easily\nsolvable using traditional methods^1,2,3,4,5,6. To this end, a significant\neffort has been devoted to applying model-free, machine learning methods to\nthose observational data of time series for analyzing and predicting complex\ndynamics, attracting tremendous attention^7,8,9,10,11,12,13. Despite initial\nor/and partial successes, those machine learning methods still meet\ndifficulties in typical scenarios where the investigated complex systems are\nof higher dimensions, replete with different types of interactions, and even\nexhibiting highly complex dynamical behaviors^14,15,16,17,18. Thus, it is\ncrucial to develop and implement delicate machine learning methods for not\nonly uncovering internal interactions in such complex systems but also\npredicting their future evolution by leveraging the discovered interactions.\n\nCompared to classical methods such as auto-regressive models (ARMA)^19 and\nmulti-layer perceptron (MLP)^20, machine learning techniques such as the\nrecurrent neural networks (RNNs)^21, neural ordinary differential equations\n(NODEs)^22, and deep residual learning^23 offer several advantages for\nanalyzing time series data generated by nonlinear and complex systems.\nSpecifically, RNNs and their variants, including long short-term memory\n(LSTM)^24 networks and gated recurrent units (GRU)^25, exhibit excellent\nperformance in predicting dynamics but require estimation of many parameters.\nIn addition to these networks with a huge number of parameters for updating,\nreservoir computing (RC), a lightweight RNN, was recently proposed for\npredicting temporal-spatial behaviors of chaotic dynamics and aroused great\ninterest^26,27,28,29,30,31. Actually, in an RC, the hidden states are of high\ndimension and only the weights of the output layer require training. As a\nresult, it possesses a strong modeling ability but needs less computational\ncost.\n\nAlthough the advantages of the RC framework have been validated in many\nscenarios^32,33,34, there is still room for improvement so that outstanding\nendeavors have been paid for recently and persistently. Examples abound: Lu\nand Luko\u0161evi\u010dius et al. added nonlinear terms of hidden states and raw data,\nrespectively, in the output layer to enhance the modeling ability of the\nRC^35,36; Gauthier et al. introduced some nonlinear combinations of the\noriginal data into the input layer to greatly improve the computational\nefficiency^37, and Gallicchio et al. extended the RC to its deep network\nforms^38. While these approaches improve the performance of RC, they encounter\ndifficulties when the dynamics dimension is higher, the nonlinearity is\nstronger and the structure is more complex. To exceed the ceiling, the latest\nworks in refs. ^39,40 proposed a parallel forecasting method, parallel RC\n(PRC), for complex dynamical networks, using the local structure of systems.\nThese pairwise structures used in the PRC method can be obtained through\ntraditional causal inference methods and their improved\nvariants^41,42,43,44,45,46; however, they cannot uncover directly the higher-\norder structures, a kind of more complex interactions that are ubiquitous in\ncomplex dynamical systems. In fact, recent studies show that the higher-order\nstructures are vital to the emergence of complex dynamics^47, viz.\ndiffusion^48, synchronization^49, and evolutionary processes^50. It thus is\nbelieved that an appropriate introduction of not only the traditional\nstructural information but also the higher-order structures into the RC is\nbeneficial to achieving more accurate and long-term predictions. In addition,\nconventional system identification algorithms including SINDy (sparse\nidentification of the nonlinear dynamics)^16,51,52 or entropic regression^53\naim to fit equations using a predefined set of basis functions in dynamical\nsystems. However, these methods have certain limitations. They are restricted\nto a particular set of bases and necessitate high-quality observational data.\nWhen there are more complex interactions within the system, the risk of\nproducing an erroneous sparse model increases. Such incorrect identification\nof interactions may inevitably lead to catastrophic predictive performance,\nwhile a simple RC even without any structure information can often yield\nsatisfactory results. Naturally and consequently, two missions are at hand: 1)\nthe inference of higher-order structures solely based on observational data,\nand 2) the utilization of the inferred optimal structures to make more\naccurate and long-term predictions.\n\nTo address the aforementioned issue, we propose a novel computing paradigm\ncalled higher-order RC, which aims to embed structural information, especially\nthe higher-order structures, into the reservoir. However, the higher-order\nstructures of the underlying complex dynamical systems are commonly unknown a\npriori. To this end, we incorporate the concept of Granger causality (GC) into\nthe higher-order RC to identify the system\u2019s underlying higher-order\ninteractions in an iterative manner, thereby enabling more accurate dynamical\npredictions with the inferred optimal higher-order structures. During this\nprocess, GC inference and RC prediction are performed simultaneously and\ncomplement each other, hence named as Higher-Order Granger RC (HoGRC)\nframework. This framework is highly scalable, in that, at the node level,\nsimultaneously achieved are complex structure inference and accurate dynamics\nprediction. This therefore makes the developed framework applicable widely to\nhigher-dimensional and more intricate dynamical systems.\n\n## Results\n\n### Classical reservoir computing\n\nWe start with a nonlinear dynamical network of N variables of the following\ngeneral form,\n\n$$\\dot{{{{{{{{\\bf{x}}}}}}}}}(t)={{{{{{{\\boldsymbol{f}}}}}}}}[{{{{{{{\\bf{x}}}}}}}}(t)],$$\n\n(1)\n\nwhere \\\\({{{{{{{\\bf{x}}}}}}}}(t)={[{x}_{1}(t),\\ldots,{x}_{N}(t)]}^{\\top }\\\\)\ndenotes the N-dimensional (N-D) state of the system at time t, and\n\\\\({{{{{{{\\boldsymbol{f}}}}}}}}[{{{{{{{\\bf{x}}}}}}}}(t)]={\\left({f}_{1}[{{{{{{{\\bf{x}}}}}}}}(t)],{f}_{2}[{{{{{{{\\bf{x}}}}}}}}(t)],\\ldots,{f}_{N}[{{{{{{{\\bf{x}}}}}}}}(t)]\\right)}^{\\top\n}\\\\) is the N-D nonlinear vector field. In this article, we assume that\nneither the vector field f (equivalently, each element f_i) nor the underlying\ncomplex interaction mechanism among these N variables is partially or\ncompletely unknown a prior. The only available information about the\nunderlying system is the observational time series x(t) at the discrete time\nsteps. Here, we choose a regularly sampled time increment \u0394t.\n\nThe traditional RC, a powerful tool for modeling time series data, embeds the\nobservational data x(t) into an n-dimensional hidden state r(t) using an input\nmatrix W_in of dimension n \u00d7 N. Then the hidden state r(t) evolves within the\nreservoir with a weighted adjacency matrix A of dimension n \u00d7 n, given by\n\n$${{{{{{{\\bf{r}}}}}}}}(t+{{\\Delta }}t)=(1-l)\\cdot\n{{{{{{{\\bf{r}}}}}}}}(t)+l\\cdot \\tanh\n\\left[{{{{{{{{\\bf{W}}}}}}}}}_{{{{{{{{\\rm{in}}}}}}}}}{{{{{{{\\bf{x}}}}}}}}(t)+{{{{{{{\\bf{A}}}}}}}}{{{{{{{\\bf{r}}}}}}}}(t)+{{{{{{{{\\bf{b}}}}}}}}}_{{{{{{{{\\rm{r}}}}}}}}}\\right],$$\n\n(2)\n\nwhere l is the leaky rate and b_r is the bias term. Subsequently, an\nadditional output layer is employed, typically implemented as a simple linear\ntransformation using the matrix W_out, mapping the reservoir state space to\nthe desired output space. Here, the output space is the original data space,\n\n$$\\hat{{{{{{{{\\bf{x}}}}}}}}}(t+{{\\Delta\n}}t)={{{{{{{\\bf{x}}}}}}}}(t)+{{{{{{{{\\bf{W}}}}}}}}}_{{{{{{{{\\rm{out}}}}}}}}}{{{{{{{\\bf{r}}}}}}}}(t+{{\\Delta\n}}t),$$\n\n(3)\n\nwhere W_outr(t + \u0394t) can be explained as the predicted residue between x(t +\n\u0394t) and x(t), or equivalently the approximated integral operator\n\\\\(\\int\\nolimits_{t}^{t+{{\\Delta\n}}t}{{{{{{{\\boldsymbol{f}}}}}}}}[{{{{{{{\\bf{x}}}}}}}}(\\tau\n)]{{{{{{{\\rm{d}}}}}}}}\\tau\\\\). It is important to note that the only trained\nmodule is the output layer, i.e., W_out, which can be solved explicitly via\nthe Tikhonov regularized regression^54 with the loss unction:\n\n$${{{{{{{{\\mathcal{L}}}}}}}}}_{{{\\Delta\n}}t}=\\mathop{\\sum}\\limits_{t}{\\left\\\\{{{{{{{{{\\bf{W}}}}}}}}}_{{{{{{{{\\rm{out}}}}}}}}}{{{{{{{\\bf{r}}}}}}}}(t+{{\\Delta\n}}t)-[{{{{{{{\\bf{x}}}}}}}}(t+{{\\Delta\n}}t)-{{{{{{{\\bf{x}}}}}}}}(t)]\\right\\\\}}^{2}+{\\lambda }_{W}\\cdot \\parallel\n{{{{{{{{\\bf{W}}}}}}}}}_{{{{{{{{\\rm{out}}}}}}}}}\\parallel,$$\n\n(4)\n\nwhere \u03bb_W is the regularization coefficient. By leveraging the trained RC, one\ncan accurately achieve dynamics prediction.\n\n### Higher-order structure in dynamical systems\n\nTo establish our framework, we first introduce a few important definitions\nabout the higher-order structure for any given function of vector field based\non the simplicial complexes summarized in^55.\n\n### Definition 1\n\nSeparable and inseparable functions. Assume that g(s) is an arbitrarily given\nscalar function with respect to s = {v_1, v_2, . . . , v_k}, a non-empty set\ncontaining k variables. If there are two variable sets s_1, s_2 \u2208 {s_1,\ns_2\u2223s_1 \u2284 s_2, s_2 \u2284 s_1, s_1 \u222a s_2 = s}, and two scalar functions g_1 and g_2\nsuch that\n\n$$g({{{{{{{\\bf{s}}}}}}}})={g}_{1}({{{{{{{{\\bf{s}}}}}}}}}_{1})+{g}_{2}({{{{{{{{\\bf{s}}}}}}}}}_{2}),$$\n\n(5)\n\nthen g(s) is a separable function with respect to s, i.e., g(s) can be\ndecomposed into the sum of two functions whose variable sets have no inclusion\nrelationship; otherwise g(s) is an inseparable function.\n\n### Definition 2\n\nHigher-order neighbors. Consider the nonlinear scalar differential equation\n\\\\(\\dot{u}=g({{{{{{{{\\bf{s}}}}}}}}}_{u})\\\\), where g(s_u) is a scalar function\nwith respect to a set of variables s_u. We decompose the function g(s_u) into\na sum of several inseparable functions g_i(s_u,i) as\n\n$$g({{{{{{{{\\bf{s}}}}}}}}}_{u})={g}_{1}({{{{{{{{\\bf{s}}}}}}}}}_{u,1})+{g}_{2}({{{{{{{{\\bf{s}}}}}}}}}_{u,2})+...+{g}_{{D}_{u}}({{{{{{{{\\bf{s}}}}}}}}}_{u,{D}_{u}}),\\\\\\\n{{{{{{{{\\bf{s}}}}}}}}}_{u}={{{{{{{{\\bf{s}}}}}}}}}_{u,1}\\cup\n{{{{{{{{\\bf{s}}}}}}}}}_{u,2}\\cup \\cdots \\cup\n{{{{{{{{\\bf{s}}}}}}}}}_{u,{D}_{u}},\\,\\,{{{{{{{{\\bf{s}}}}}}}}}_{u,i}\\not\\subset\n{{{{{{{{\\bf{s}}}}}}}}}_{u,j},$$\n\n(6)\n\nfor all i, j \u2208 {1, 2, . . . , D_u} with i \u2260 j, where D_u is the number of\nterms. Then, we name the set\n\\\\({{{{{{{{\\bf{s}}}}}}}}}_{u,i}=\\\\{{v}_{{i}_{1}},{v}_{{i}_{2}},...,{v}_{{i}_{{k}_{i}}}\\\\}\\\\)\nas the (k_i-1)-D simplicial complex, and the i-th higher-order neighbor of\nnode u. Denote by\n\\\\({{{{{{{{\\mathscr{S}}}}}}}}}_{u}=\\\\{{{{{{{{{\\bf{s}}}}}}}}}_{u,1},{{{{{{{{\\bf{s}}}}}}}}}_{u,2},...,{{{{{{{{\\bf{s}}}}}}}}}_{u,{D}_{u}}\\\\}\\\\)\nthe set of the higher-order neighbors of node u.\n\nWe construct a hypergraph or a hypernetwork, denoted by\n\\\\({{{{{{{\\mathcal{G}}}}}}}}=(V,S)\\\\), of system (1) under consideration.\nHere, V = {x_1, x_2, . . . , x_N} denotes the set of nodes, corresponding to\nthe state variables of the system. According to Definitions 1 & 2, we\nintroduce the concept of the higher-order neighbors\n\\\\({{{{{{{{\\mathscr{S}}}}}}}}}_{u}\\\\) of an arbitrary node u \u2208 V, yielding the\nset of higher-order neighbors for all nodes\n\\\\(S=\\\\{{{{{{{{{\\mathscr{S}}}}}}}}}_{{x}_{1}},{{{{{{{{\\mathscr{S}}}}}}}}}_{{x}_{2}},...,{{{{{{{{\\mathscr{S}}}}}}}}}_{{x}_{N}}\\\\}\\\\).\nHereafter, for simplicity of notation\u2019s usage, node u is used as a placeholder\nof any element in the set V.\n\nTo better elucidate these concepts, we directly utilize the Lorenz63 system as\nan illustrative example. As shown in \u201cExplanation (1)\" of Fig. 1, for the\nthird node u = z in system (12), we write out\n\n$$\\dot{z}={f}_{3}(x,\\, y,\\, z)=-\\beta z+xy=g(x,\\, y,\\,\nz)={g}_{1}(z)+{g}_{2}(x,\\, y),$$\n\n(7)\n\nwhere g_1(z) \u225c \u2212 \u03b2z, g_2(x, y) \u225c xy, and D_z \u225c 2. Consequently, according to\nDefinitions 1 & 2, the set of the higher-order neighbors of node u = z is\n\\\\({{{{{{{{\\mathscr{S}}}}}}}}}_{z}=\\\\{{{{{{{{{\\bf{s}}}}}}}}}_{z,1},\\,\n{{{{{{{{\\bf{s}}}}}}}}}_{z,2}\\\\}=\\\\{\\\\{z\\\\},\\\\{x,y\\\\}\\\\}\\\\). Similarly, we have\n\\\\({{{{{{{{\\mathscr{S}}}}}}}}}_{x}=\\\\{{{{{{{{{\\bf{s}}}}}}}}}_{x,1},\\,\n{{{{{{{{\\bf{s}}}}}}}}}_{x,2}\\\\}=\\\\{\\\\{x\\\\},\\\\{y\\\\}\\\\}\\\\) for node u = x and\n\\\\({{{{{{{{\\mathscr{S}}}}}}}}}_{y}=\\\\{{{{{{{{{\\bf{s}}}}}}}}}_{y,1},\\,\n{{{{{{{{\\bf{s}}}}}}}}}_{y,2}\\\\}=\\\\{\\\\{y\\\\},\\\\{x,z\\\\}\\\\}\\\\) for node u = y.\nConsequently, we obtain the higher-order structure of the Lorenz63 system as\n\\\\({{{{{{{\\mathcal{G}}}}}}}}=(V,\\, S)=((x,\\, y,\\,\nz),({{{{{{{{\\mathscr{S}}}}}}}}}_{x},\\, {{{{{{{{\\mathscr{S}}}}}}}}}_{y},\\,\n{{{{{{{{\\mathscr{S}}}}}}}}}_{z}))\\\\).\n\nFig. 1: Schematic diagrams for illustrating the proposed HoGRC framework.\n\na The input data consists of time series data and higher-order structure\ninformation. b The new paradigm \\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{u}\\\\) using\nthe higher-order structural information. c The HoGRC framework enables the\ninference of higher-order structures. d The HoGRC framework achieves multi-\nstep dynamics prediction using the inferred optimal structure. The markers\n\u201cS1\u201d\u2013\u201cS8\u201d correspond to the steps in Table 2. We offer \u201cExplanation 1\u201d to\nelucidate the concept of higher-order structures, and use \u201cExplanation 2\u201d to\nclarify the notion of the higher-order structure embedding. Due to the same\nprocess and the independently trained \\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{u}\\\\)\nfor all nodes, it makes the HoGRC own the scalability or parallel merit^40.\n\nFull size image\n\n### A paradigm of reservoir computing with structure input\n\nDespite the tremendous success achieved by the traditional RC in dynamics\npredictions in many fields, a difficulty still lies in pushing for the limit\nof prediction accuracy while maintaining the low complexity of the model. We\nattribute this difficulty to a lack of direct utilization of the structural\ninformation from the underlying dynamical system, since the structure is an\nimportant component of the system. Actually, the PRC, the recent framework^40\nintegrated pairwise structures to predict dynamics in complex systems.\nHowever, they cannot reveal the higher-order structures, a more precise\nrepresentation of the complex interactions in complex dynamical systems.\n\nThus, we introduce a new computing paradigm into the RC, termed higher-order\nRC, to incorporate the time-series data with the higher-order structure to\nmake accurate dynamics predictions. Specifically, as shown in Fig. 1b, we\nmodel each state variable (i.e., node u, as defined above) of the original\nsystem independently with a block of n neurons in a reservoir network. Then we\nincorporate the higher-order neighbors of node u into the corresponding RC,\ndefined as \\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{u}\\\\). Subsequently, inspired by\nbut different from the classical RC method (2), the hidden dynamics in the\nhigher-order \\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{u}\\\\) is given by\n\n$${{{{{{{{\\bf{r}}}}}}}}}_{u}(t+{{\\Delta }}t)=(1-l)\\cdot\n{{{{{{{{\\bf{r}}}}}}}}}_{u}(t)+l\\cdot \\tanh\n\\left[{\\tilde{{{{{{{{\\bf{W}}}}}}}}}}_{{{{{{{{\\rm{in}}}}}}}},u}{{{{{{{\\bf{x}}}}}}}}(t)+{\\tilde{{{{{{{{\\boldsymbol{A}}}}}}}}}}_{u}{{{{{{{{\\bf{r}}}}}}}}}_{u}(t)+{{{{{{{{\\bf{b}}}}}}}}}_{{{{{{{{\\rm{r}}}}}}}}}\\right],$$\n\n(8)\n\nfor different u \u2208 V. Thus, we establish a total of \u2223V\u2223 sub-RC networks, where\n\u2223V\u2223 denotes the number of the elements in the set V. In contrast to the\ntraditional RC method (2) that solely relies on a single random matrix W_in\nand a single random matrix A without including any higher-order structural\ninformation, the framework (8) operates at node level, notably incorporating\nthe corresponding higher-order structural information. Specifically for each\nnode u \u2208 V, this framework embeds the higher-order structural information\ndirectly into the matrices\n\\\\({\\tilde{{{{{{{{\\bf{W}}}}}}}}}}_{{{{{{{{\\rm{in}}}}}}}},u}\\\\) and\n\\\\({\\tilde{{{{{{{{\\bf{A}}}}}}}}}}_{u}\\\\) in the following forms:\n\n$${\\tilde{{{{{{{{\\bf{W}}}}}}}}}}_{{{{{{{{\\rm{in}}}}}}}},u}={\\left[{\\psi\n}^{\\top }({{{{{{{{\\bf{s}}}}}}}}}_{u,1}),{\\psi }^{\\top\n}({{{{{{{{\\bf{s}}}}}}}}}_{u,2}),...,{\\psi }^{\\top\n}({{{{{{{{\\bf{s}}}}}}}}}_{u,{D}_{u}})\\right]}^{\\top }\\in\n{{\\mathbb{R}}}^{n\\times N},\\quad \\psi ({{{{{{{{\\bf{s}}}}}}}}}_{u,i})\\in\n{{\\mathbb{R}}}^{\\lfloor n/{D}_{u}\\rfloor \\times N},\\\\\\\n{\\tilde{{{{{{{{\\bf{A}}}}}}}}}}_{u}={{{{{{{\\rm{diag}}}}}}}}\\\\{\\varphi\n({{{{{{{{\\bf{s}}}}}}}}}_{u,1}),\\varphi\n({{{{{{{{\\bf{s}}}}}}}}}_{u,2}),...,\\varphi\n({{{{{{{{\\bf{s}}}}}}}}}_{u,{D}_{u}})\\\\}\\in {{\\mathbb{R}}}^{n\\times n},\\quad\n\\varphi ({{{{{{{{\\bf{s}}}}}}}}}_{u,i})\\in {{\\mathbb{R}}}^{\\lfloor\nn/{D}_{u}\\rfloor \\times \\lfloor n/{D}_{u}\\rfloor },$$\n\n(9)\n\nwhere \u230a \u22c5 \u230b is the floor function, and the integer n is selected as a multiple\nof D_u. Different from W_in, a randomly initialized matrix in its entirety, in\nthe traditional RC framework (2), each \u03c8(s_u,i) in\n\\\\({\\tilde{{{{{{{{\\bf{W}}}}}}}}}}_{{{{{{{{\\rm{in}}}}}}}},u}\\\\) is a random\n(resp., zero) block submatrix of dimension \u230an/D_u\u230b \u00d7 N such that, if x_j \u2208\n(resp., \u2209 )s_u,i for j \u2208 {1, 2, . . . , N}, all elements of the j-th column of\n\u03c8(s_u,i) are set as random values (resp., zeros), and \u03c6(s_u,i) represents a\nrandom sparse submatrix of dimension \u230an/D_u\u230b \u00d7 \u230an/D_u\u230b. Actually, these block\nconfigurations in the reservoir facilitate a more precise utilization of the\nhigher-order structural information.\n\nTo enhance the transparency of the above configurations, we provide a visual\nrepresentation in \u201cExplanation (2)\" of Fig. 1, where depicted is the true\nhigher-order RC structure (i.e., the optimal network finally obtained in the\nfollowing inference task, see the next subsection) under consideration of the\nLorenz63 system. Specifically, as mentioned above, for node u = z, the set of\nthe higher-order neighbors becomes {{z}, {x, y}} with D_z = 2. Thus, we obtain\n\\\\({\\tilde{{{{{{{{\\bf{W}}}}}}}}}}_{{{{{{{{\\rm{in}}}}}}}},z}={[{\\psi }^{\\top\n}(z),{\\psi }^{\\top }[(x,y)]]}^{\\top }\\\\) according to the notations set in\n(9), where the third column of \u03c8^\u22a4(z) and the first and the second columns of\n\u03c8^\u22a4[(x, y)] are the random sparse submatrices, and the remaining parts are\nzero submatrices. Moreover, we obtain\n\\\\({\\tilde{{{{{{{{\\bf{A}}}}}}}}}}_{z}={{{{{{{\\rm{diag}}}}}}}}\\\\{\\varphi (z),\\,\n\\varphi [(x,y)]\\\\}\\\\), which is a block diagonal matrix comprising two random\nsparse submatrices. Additionally, we provide a simple illustrative example\nabout the difference between the traditional RC method (2) and the newly\nproposed higher-order RC framework (8) in Supplementary Note 1.3.\n\nNow, by embedding the higher-order structural information into the dynamics of\nthe reservoir in the above manner, we obtain the n-D hidden state\n\\\\({{{{{{{{\\bf{r}}}}}}}}}_{u}(t)={[{r}_{u,1},{r}_{u,2},...,{r}_{u,n}]}^{\\top\n}(t)\\\\) for each u \u2208 V. This allows us to predict the system\u2019s state u in the\nnext time step as\n\n$$\\hat{u}(t+{{\\Delta\n}}t)=u(t)+{\\tilde{{{{{{{{\\bf{W}}}}}}}}}}_{{{{{{{{\\rm{out}}}}}}}},u}{{{{{{{{\\bf{r}}}}}}}}}_{u}(t+{{\\Delta\n}}t),$$\n\n(10)\n\nwhere \\\\({\\tilde{{{{{{{{\\bf{W}}}}}}}}}}_{{{{{{{{\\rm{out}}}}}}}},u}\\\\)\nrepresents an output matrix of dimension 1 \u00d7 n, employed for the prediction of\nu.\n\nSignificantly, our framework fully inherits the parallel merit of the existing\nwork^40. In particular, the above process operates at the node level, focusing\nexclusively on every node u, and such a process can be applied across all\nnodes in V. Different from the classical RC, we use a specific higher-order\n\\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{u}\\\\) to model each node u, thereby requiring\na smaller reservoir size n or resulting in a lightweight model. Moreover,\nsince all lightweight reservoirs \\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{u}\\,(u\\in\nV)\\\\) are independently trained, our framework can be efficiently processed in\na parallel manner, which in turn makes our framework scalable to higher-\ndimensional systems.\n\n### Integration of structure inference and dynamics prediction\n\nIn the preceding section, the setup of the higher-order\n\\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{u}\\\\) requires the exact information of the\nstructures. However, in real-world scenarios, the specific form as well as the\nhigher-order structures of a system are always unknown before the setup of\n\\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{u}\\\\). So, we design an iterative algorithm to\nseek the optimal structure for \\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{u}\\\\) which is\ninitially endowed with a structure containing all possible candidates or only\npartially known information. To carry out this design, we novelly integrate\nthe concept of the Granger causality (GC) into the higher-oder\n\\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{u}\\\\) (see Table 1). Subsequently, the\ninferred structures are utilized to update\n\\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{u}\\\\), thereby further enhancing its\nprediction performance. This iterative procedure is repeated until the model\nachieves optimal prediction accuracy. Consequently, we refer to this\nintegrated model as the HoGRC framework, as depicted in the composite of Fig.\n1a-d.\n\nTable 1 The process of inferring higher-order neighbors using Algorithm 1\n\nFull size table\n\nParticularly, we develop an efficient greedy strategy, as outlined in Table 1\nof the Methods section, to infer the true higher-order structure of system (1)\nsolely from the time series data. As shown in Fig. 1c, for any node u, we\nemploy the one-step prediction error of \\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{u}\\\\)\nbased on the concept of the GC (see Definition 3) to iteratively refine the\ninitial and coarse-grained candidate neighbors into the optimal and fine-\ngrained higher-order neighbors, until an optimal structure is obtained,\ntending to align with the true higher-order structure defined in Definition 2.\nIn the iterative procedure, the GC inference and the dynamics prediction using\n\\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{u}\\\\) are complementarily and mutually\nreinforcing. As depicted by the blue loop in Fig. 1, the structure discovered\nby the GC significantly enhances the predictability of\n\\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{u}\\\\), and conversely, the updated\n\\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{u}\\\\) in the iterative procedure makes the GC\ndiscover the structure in a more effective manner.\n\nFurthermore, as indicated by the orange arrows in Fig. 1d, we obtain the\noptimal \\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{u}\\\\) for all nodes u based on the\ninput of the optimal higher-order structure. Then, these optimal models can\nperform multi-step prediction by continually adding the most recent forecasted\nvalues to the input data, which significantly outperforms the traditional\nprediction methods. Therefore, the HoGRC framework, integrating the node-level\nRC and the GC inference, simultaneously achieve two functions: (I) structures\ninference (Fig. 1c) and (II) dynamics prediction (Fig. 1d). To enhance\ncomprehension of the HoGRC workflow, we provide a summary of the key execution\nsteps in Table 2, where the steps correspond to the markers \u201cS1\"\u2013\"S8\" in Fig.\n1. For more detailed information about the HoGRC framework, please refer to\nMethods section.\n\nTable 2 Main steps of the HoGRC framework\n\nFull size table\n\n### Evaluation metrics\n\nTo demonstrate the efficacy of the two tasks achieved by the proposed\nframework, we conduct experiments using several representative systems from\ndifferent fields. For Task (I), we utilize the one-step extrapolation\nprediction error produced by the HoGRC framework to search the higher-order\nneighbors of all dimensions in order to identify the higher-order structure\nwith higher accuracy. For Task (II), we test the classical RC, the PRC^40, and\nthe HoGRC, respectively, on several representative dynamical systems and\ncompare their prediction performances (see Methods section for the differences\namong these three methods). For a clearer illustration, we define the valid\npredictive steps (VPS) as the predictive time steps when the prediction\naccuracy exceeds a certain threshold. Additionally, we adopt the root mean\nsquare error (RMSE) as a metric to quantitatively evaluate the prediction\nerror,\n\n$${{{{{{{\\rm{RMSE}}}}}}}}(t)=\\sqrt{\\frac{1}{N}\\mathop{\\sum\n}\\limits_{i=1}^{N}{\\left[\\frac{{\\hat{x}}_{i}(t)-{x}_{i}(t)}{{\\sigma\n}_{i}}\\right]}^{2}},$$\n\n(11)\n\nwhere \u03c3_i is the standard deviation of x_i(t). In our work, we use the VPS to\nevaluate the prediction performance of the HoGRC, i.e.,\n\\\\({{{{{{{\\rm{VPS}}}}}}}}=\\inf \\\\{s:{{{{{{{\\rm{RMSE}}}}}}}}(s{{\\Delta }}t) \\,\n> \\, {\\epsilon }_{{{{{{{{\\rm{r}}}}}}}}}\\\\}\\\\), where \u03b5_r is the positive\nthreshold and \u0394t is the time step size. In the following numerical\nsimulations, without a specific statement, we always set \u03b5_r = 0.01.\n\n### Performances in representative dynamical systems\n\nHere, we aim to demonstrate the effectiveness of the HoGRC framework using\nseveral representative dynamical systems. We take a 3-D Lorenz63 system and a\n15-D coupled Lorenz63 system as examples. Additional experiments for more\nsystems are included in Supplementary Note 2.\n\nFirst, we consider the Lorenz63 system^56 which is a typical chaotic model\ndescribed by the following equations:\n\n$$\\dot{x}= {f}_{1}(x,y,z)=\\sigma (y-x),\\\\\\ \\dot{y}= {f}_{2}(x,y,z)=\\rho x-y-\nxz,\\\\\\ \\dot{z}= {f}_{3}(x,y,z)=-\\\\!\\beta z+xy,$$\n\n(12)\n\nwhere \u03c3, \u03b2, \u03c1 are system parameters. In the simulations, we take the first 60%\nof the data generated by the system as the training set, and reserve the\nremaining data for testing purposes.\n\nWe begin our analysis by using the proposed method to identify the higher-\norder neighbors of the considered system. All the other hyperparameters of the\nRC, the PRC, and the HoGRC are specified, respectively, in Supplementary Note\n3. Subsequently, we employ Algorithm 1 of Table 1 to infer the higher-order\nneighbors of all nodes in the Lorenz63 system. Specifically, Fig. 2a presents\nan inference process for node z using Algorithm 1 of Table 1, a greedy\nstrategy. At the beginning, when no information regarding the network\nstructure is available, the set of the higher-order neighbors for node z is\ninitially assigned as\n\\\\({{{{{{{{\\mathscr{C}}}}}}}}}_{z}={{{{{{{{\\mathscr{C}}}}}}}}}_{z}^{0}=\\\\{\\\\{x,\\,\ny,\\, z\\\\}\\\\}\\\\). Thus,\n\\\\({\\tilde{{{{{{{{\\bf{W}}}}}}}}}}_{{{{{{{{\\rm{in}}}}}}}},z}\\\\) and\n\\\\({\\tilde{{{{{{{{\\bf{A}}}}}}}}}}_{z}\\\\), the input and the adjacency\nmatrices, are constructed with \\\\({{{{{{{{\\mathscr{C}}}}}}}}}_{z}^{0}\\\\), and\n\\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{z}^{0}\\\\), the corresponding higher-order RC,\nis utilized to calculate the one-step prediction error e(z), designated as\ne_1. Next, one needs to decide whether to rectify\n\\\\({{{{{{{{\\mathscr{C}}}}}}}}}_{z}\\\\) by reducing the dimensionality based on\nAlgorithm 1 of Table 1. To do so, set\n\\\\({{{{{{{{\\mathscr{C}}}}}}}}}_{z}^{1}=\\\\{\\\\{x,\\, y\\\\},\\\\{y,\\, z\\\\},\\\\{x,\\,\nz\\\\}\\\\}\\\\), and then the prediction error e_2 is obtained using\n\\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{z}^{1}\\\\) with\n\\\\({{{{{{{{\\mathscr{C}}}}}}}}}_{z}^{1}\\\\). Here, by setting a small threshold\n\u03b5_e (e.g., 10^\u22127), it is found that e_1 + \u03b5_e \u2265 e_2, which implies a\nprediction promotion and thus, results in a resetting\n\\\\({{{{{{{{\\mathscr{C}}}}}}}}}_{z}={{{{{{{{\\mathscr{C}}}}}}}}}_{z}^{1}\\\\)\nbased on Definition 3. Then, one needs to decide whether to delete any\nelement, e.g. {y, z}, in the current set\n\\\\({{{{{{{{\\mathscr{C}}}}}}}}}_{z}\\\\). To do so, set\n\\\\({{{{{{{{\\mathscr{C}}}}}}}}}_{z}^{2}=\\\\{\\\\{x,y\\\\},\\\\{x,z\\\\}\\\\}\\\\). Thus, the\nprediction error e_3 is obtained using\n\\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{z}^{2}\\\\) with\n\\\\({{{{{{{{\\mathscr{C}}}}}}}}}_{z}^{2}\\\\), which further yields e_2 + \u03b5_e \u2265\ne_3. This prediction promotion leads us to reset\n\\\\({{{{{{{{\\mathscr{C}}}}}}}}}_{z}={{{{{{{{\\mathscr{C}}}}}}}}}_{z}^{2}\\\\).\nHowever, as the sets \\\\({{{{{{{{\\mathscr{C}}}}}}}}}_{z}^{3}=\\\\{\\\\{x,z\\\\}\\\\}\\\\)\nand \\\\({{{{{{{{\\mathscr{C}}}}}}}}}_{z}^{4}=\\\\{\\\\{x,\\, z\\\\}\\\\}\\\\) are,\nrespectively, taken into account, e_3 + \u03b5_e < e_4 and e_3 + \u03b5_e < e_5 are\nobtained using \\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{z}^{3}\\\\) with\n\\\\({{{{{{{{\\mathscr{C}}}}}}}}}_{z}^{3}\\\\) and\n\\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{z}^{4}\\\\) with\n\\\\({{{{{{{{\\mathscr{C}}}}}}}}}_{z}^{4}\\\\), respectively. These inequalities\nindicate that there is no improvement in prediction and, consequently, no\nrectification needed for the set \\\\({{{{{{{{\\mathscr{C}}}}}}}}}_{z}\\\\) at this\nstage. Therefore, the set should remain unaltered as\n\\\\({{{{{{{{\\mathscr{C}}}}}}}}}_{z}={{{{{{{{\\mathscr{C}}}}}}}}}_{z}^{2}\\\\). In\nwhat follows, one still needs to decide whether to further rectify\n\\\\({{{{{{{{\\mathscr{C}}}}}}}}}_{z}\\\\) by reducing the dimensionality based on\nAlgorithm 1 of Table 1. To do so, set\n\\\\({{{{{{{{\\mathscr{C}}}}}}}}}_{z}^{5}=\\\\{\\\\{x,y\\\\},\\\\{z\\\\}\\\\}\\\\). Thus, e_6\nand e_3 + \u03b5_e \u2265 e_6 are obtained, which leads us to further reset\n\\\\({{{{{{{{\\mathscr{C}}}}}}}}}_{z}={{{{{{{{\\mathscr{C}}}}}}}}}_{z}^{5}\\\\). As\nsuggested in Fig. 2, prediction is not improved by further reducing the\ndimensionality of \\\\({{{{{{{{\\mathscr{C}}}}}}}}}_{z}\\\\) as\n\\\\({{{{{{{{\\mathscr{C}}}}}}}}}_{z}^{6}=\\\\{\\\\{x\\\\},\\\\{y\\\\},\\\\{z\\\\}\\\\}\\\\). This,\nwith the greedy strategy we use, indicates an iteration terminal for inferring\nthe higher-order neighbors with an output\n\\\\({{{{{{{{\\mathscr{S}}}}}}}}}_{z}={{{{{{{\\mathscr{C}}}}}}}}={{{{{{{{\\mathscr{C}}}}}}}}}_{z}^{5}\\\\).\nHere, actually \\\\({{{{{{{{\\mathcal{R}}}}}}}}}_{z}^{5}\\\\) with\n\\\\({{{{{{{{\\mathscr{S}}}}}}}}}_{z}={{{{{{{{\\mathscr{C}}}}}}}}}_{z}^{5}\\\\)\nafter training is the optimal higher-order RC of dynamics reconstruction and\nprediction for the state of node z. In addition, the inferred results of nodes\nx and y can be found in Supplementary Note 4.1.\n\nFig. 2: Higher-order structure inference and dynamics prediction for the\nLorenz63 system and the CL63 system.\n\na The successively iterative results on higher-order neighbors inference for\nnode z using Algorithm 1 in Table 1, where the red pentagram indicates the\ninferred higher-order neighbors \\\\({{{{{{{{\\mathscr{S}}}}}}}}}_{z}\\\\). b The\nunderlying coupling network of the CL63 system. c The inferred coupling\nneighbors for each subsystem of the CL63 system. d, e The average number of\npredictable steps and average prediction error of different methods for the\nnonlinear coupling cases, respectively. The orange line in the middle of the\nbox represents the median, the upper and lower boundaries of the box represent\nthe upper and lower quartiles, respectively. The boundaries of the upper and\nlower whiskers represent the maxima and minima, respectively. We set\nparameters as \u03c3 = 10, \u03c1 = 28 and \u03b2 = 8/3. Here, we use a time-step size \u0394t =\n0.02 and a time-step number T = 5000.\n\nFull size image\n\nIn Task (II), we perform multi-step prediction using different methods, we\nfind that the HoGRC framework yields the best prediction despite utilizing\ninformation solely from higher-order neighbors (see Supplementary Note 4.1).\nAdditionally, in Supplementary Note 2, we also conduct similar experiments\nusing other classic chaotic systems. Our findings indicate that systems with\nstronger nonlinearity and more complex structures tend to exhibit better\nprediction performance using the HoGRC framework.\n\nNext, we investigate the coupled Lorenz63 (CL63) system^34 with a more complex\nstructure and stronger nonlinear interactions, in which the dynamical\nbehaviors of each subsystem is described by:\n\n$${\\dot{x}}_{i} =-\\sigma \\left[{x}_{i}-{y}_{i}+\\gamma \\mathop{\\sum }\\limits_{j\n=1}^{m}{w}_{ij}{g}_{ij}({y}_{i},{y}_{j})\\right],\\\\\\ {\\dot{y}}_{i} =\\rho\n(1+{h}_{i}){x}_{i}-{y}_{i}-{x}_{i}{z}_{i},\\,\\,{\\dot{z}}_{i}={x}_{i}{y}_{i}-\\beta\n{z}_{i},$$\n\n(13)\n\nwhere m denotes the number of the subsystems, h_i is the scale of the i-th\nsubsystem, \u03b3 represents the coupling strength, w_ij is the coupling weight,\nand g_ij denotes the coupling function. We consider a 15-dimensional CL63\nsystem with 5 subsystems, and the structure and the coupling weights are\ndepicted in Fig. 2b. We generate data with the coupling strength \u03b3 = 0.5 and\nthe coupling function g_ij = (y_j \u2212 y_i). Based on this data, we calculate the\nLyapunov Exponents (LE\u2019s) of the system (see Supplementary Note 4.2), which\nsuggests a higher-degree complexity emerging in the system, as more than half\nof the LEs are positive.\n\nOur HoGRC framework considers complexes {y_i} and {y_j} as the higher-order\nneighbors of x_i if subsystem j has a coupling effect on i. Thus, by virtue of\nDefinition 3, we are able to infer such a coupling relationship between any\ntwo subsystems. As depicted in Fig. 2c, we initially present the one-step\nprediction error for any subsystem i, considering all four other subsystems\nare treated as neighbors. Subsequently, we proceed to present the prediction\nerrors when each neighboring subsystem is individually removed. The\nexperimental results demonstrate that with the removal of subsystem j, the\nstronger the coupling effect of subsystem j on i, the worse the prediction\nperformance of subsystem i is. This enables us to directly infer the true\ninteraction network among subsystems (marked by the red pentagrams).\n\nFor our second task, we perform multi-step predictions on the CL63 system\nusing different methods. We randomly select 50 points from the testing data as\nstarting points and use the predictable steps to quantify the prediction\nperformances for the various methods. Figure 2d displays a boxplot of the\npredictable steps for various methods on 50 testing sets. The results clearly\nindicate that the HoGRC framework outperforms the other two methods,\nhighlighting its superior ability in the extrapolation prediction.\nFurthermore, we extend our analysis by generalizing the linear coupling term\ng_ij = (y_j \u2212 y_i) to two more nonlinear forms, namely \\\\(\\sin\n({y}_{j}-{y}_{i})\\\\) and \u2223y_j \u2212 y_i\u2223. Correspondingly, we include the complex\n{y_i, y_j} in the higher-order neighbors of x_i. The heatmap of the prediction\nerrors along with the time steps for various methods is illustrated in Fig.\n2e. Combining with Fig. 2d, it becomes apparent that the HoGRC framework\nmaintains its superiority in terms of prediction performance.\n\n### Investigations on network dynamics\n\nIn recent years, network dynamical systems (NDS) have gained significant\nattention for their broad range of applications. As a special form of system\n(1), NDS often exhibits a higher number of dimensions and more complex\nstructural information. Therefore, our framework has become an efficient tool\nfor NDS\u2019s structural inference and dynamic prediction. Generally, NDS\u2019s\ndynamics are modeled as:\n\n$${\\dot{{{{{{{{\\bf{x}}}}}}}}}}_{i}=F({{{{{{{{\\bf{x}}}}}}}}}_{i})+\\gamma\n\\mathop{\\sum }\\limits_{j=1}^{m}{\\omega }_{ij}G({{{{{{{{\\bf{x}}}}}}}}}_{i},\\,\n{{{{{{{{\\bf{x}}}}}}}}}_{j}),$$\n\n(14)\n\nwhere \\\\({{{{{{{{\\bf{x}}}}}}}}}_{i}={({x}_{i}^{1},\\ldots,{x}_{i}^{N})}^{\\top\n}\\\\) denotes the N-D state of the i-th subsystem, F represents the self-\ndynamics, G represents the interaction dynamics, \u03b3 is the coupling strength,\nw_ij is the interaction weight of subsystem j to i. Before presenting the\nresults of our numerical investigations, we first make three remarks. (i)\nSince the HoGRC framework is a node-level based method, here we set the\ncoupling network structure between any two subsystems as depicted in Fig. 2d.\n(ii) A very small coupling strength implies a weak coupling effect on the\ndynamics, while sufficiently strong coupling tends to increase predictability\ndue to a high probability of synchronization occurrence (see Supplementary\nNote 4.6 for details). Therefore, in our investigations, we selected a\nmoderate level of coupling strength to increase prediction difficulty. (iii)\nIn addition to the RC and the PRC methods, we use two recently proposed\npowerful methods, namely the Neural Dynamics on Complex Network (NDCN)^15 and\nthe Two-Phase Inference (TPI)^16, as the baseline methods for NDS predictions.\nThe NDCN combines the graph neural networks with differential equations to\nlearn and predict complex network dynamics, while the TPI automatically learns\nsome basis functions to infer dynamic equations of complex system behavior for\nnetwork dynamics prediction. Refer to Supplementary Note 5 for further\ndetails.\n\nWe first consider the coupled FitzHugh\u2013Nagumo system (FHNS)^57 that describes\nthe dynamical activities of a group of interacted neurons with\n\n$$F({{{{{{{{\\bf{x}}}}}}}}}_{i})= F({x}_{i}^{1},\\,\n{x}_{i}^{2})={\\left({x}_{i}^{1}-{({x}_{i}^{1})}^{3}-{x}_{i}^{2},\\,\na+b{x}_{i}^{1}+c{x}_{i}^{2}\\right)}^{\\top },\\\\\\\nG({{{{{{{{\\bf{x}}}}}}}}}_{i},\\, {{{{{{{{\\bf{x}}}}}}}}}_{j})= G({x}_{i}^{1},\\,\n{x}_{j}^{1})=\\frac{1}{{k}_{i}^{{{{{{{{\\rm{in}}}}}}}}}}({x}_{i}^{1}-{x}_{j}^{1}),$$\n\n(15)\n\nin network dynamics (14). Here, we set \u03b3 = 0.5, a = 0.28, b = 0.5, c = \u22120.04,\nand m = 5 to generate experimental data. As shown in Fig. 3a, the trajectory\npredicted by our HoGRC framework closely matches the true trajectory of the\nFHNS system. In task (I), we begin by examining the inference of the coupling\nnetwork among subsystems. Figure 3b displays the prediction errors for each\nsubsystem under different coupling structures. The bar chart above includes\nmultiple letters indicating the candidate neighbors of the corresponding\nsubsystem. It is evident that the inferred coupling structures, illustrated\nwith red pentagrams, align with our initial setting. Furthermore, in\nSupplementary Note 4.3, we provide the inference of higher-order neighbors for\nindividual nodes within the subsystem as well, which further validates the\neffectiveness of our method. For task (II), we conduct the multi-step\nprediction experiments and compared our results to the baseline methods on 50\ntesting sets. The results, depicted in Fig. 3c, demonstrate that our method\noutperforms the other methods in terms of the extrapolation prediction\nperformance.\n\nFig. 3: Coupling network inference, system reconstruction, and dynamics\nprediction for network systems.\n\nThe corresponding results using the HoGRC framework and two other methods for\nthe FHNS (a\u2013c), CRoS (d\u2013f), and CsH^2S (g\u2013i) network systems. The orange line\nin the middle of the box represents the median, the upper and lower boundaries\nof the box represent the upper and lower quartiles, respectively. The\nboundaries of the upper and lower whiskers represent the maxima and minima,\nrespectively. The experimental data for these systems are generated by setting\nT = 5000 and using \u0394t = 0.25, 0.1, and 0.04, respectively.\n\nFull size image\n\nWe also investigate two other network dynamics, namely the coupled Rossler\nsystem (CRoS)^58 and the coupled simplified Hodgkin-Huxley system (CsH^2S)^59.\nThe CRoS has the form\n\n$$F({{{{{{{{\\bf{x}}}}}}}}}_{i})= F({x}_{i}^{1},\\, {x}_{i}^{2},\\,\n{x}_{i}^{3})={\\left(-{h}_{i}{x}_{i}^{2}-{x}_{i}^{3},\\,\n{h}_{i}{x}_{i}^{1}+a{x}_{i}^{2},\\, b+{x}_{i}^{3}({x}_{i}^{1}+c)\\right)}^{\\top\n},\\\\\\ G({{{{{{{{\\bf{x}}}}}}}}}_{i},\\, {{{{{{{{\\bf{x}}}}}}}}}_{j})=\nG({x}_{i}^{1},\\, {x}_{j}^{1})={x}_{j}^{1}-{x}_{i}^{1},$$\n\n(16)\n\nin network dynamics (14), with h_i representing the scale of the i-th\nsubsystem, and with a = 0.2, b = 0.2, c = \u2212 6, \u03b3 = 1 and m = 5. The CsH^2S has\nthe form\n\n$$F({{{{{{{{\\bf{x}}}}}}}}}_{i})= F({x}_{i}^{1},{x}_{i}^{2},{x}_{i}^{3}) \\\\\\ =\n{\\left({x}_{i}^{2}-a{({x}_{i}^{1})}^{3}+b{({x}_{i}^{1})}^{2}-{x}_{i}^{3}+{I}_{{{{{{{{\\rm{ext}}}}}}}}},c-u{({x}_{i}^{1})}^{2}-{x}_{i}^{2},\\,\nr[s({x}_{i}^{1}-{x}_{0})-{x}_{i}^{3}]\\right)}^{\\top },\\\\\\\nG({{{{{{{{\\bf{x}}}}}}}}}_{i},{{{{{{{{\\bf{x}}}}}}}}}_{j})= G({x}_{i}^{1},\\,\n{x}_{j}^{1})= ({V}_{{{{{{{{\\rm{syn}}}}}}}}}-{x}_{i}^{1})\\cdot \\mu\n({x}_{j}^{1}),\\,\\mu (x)=\\frac{1}{1+{{{{{{{{\\rm{e}}}}}}}}}^{-\\lambda\n(x-{{{\\Omega }}}_{syn})}},$$\n\n(17)\n\nin network dynamics (14), with a = 1, b = 3, c = 1, u = 5, s = 4, r = 0.005,\nx_0 = \u2212 1.6, \u03b3 = 0.1, V_syn = 2, \u03bb = 10, \u03a9 = 1, I_ext = 3.24, and m = 5. The\ninvestigation results, respectively, presented in Fig. 3d\u2013f, g\u2013i, suggest that\nour HoGRC framework possesses extraordinary capability in dynamics\nreconstructions and predictions using the inferred information of higher-order\nstructures. It is noted that, in the examples above, the performances of the\nNDCN and the TPI are not satisfactory. This is because the NDCN is a network-\nlevel method that may not achieve good performance in complex nonlinear\nsystems, and because the interaction function weights w_ij in front of G(x_i,\nx_j) are different, so the TPI method cannot learn the accurate basis function\n(refer to Supplementary Note 5 for the detailed illustration).\n\n### Application to the UK power grid system\n\nFinally, we apply the HoGRC framework to a real power system. We choose the UK\npower grid^60 as the network structure, which includes 120 units (10\ngenerators and 110 consumers) and 165 undirected edges, as shown in Fig. 4a.\nTo better describe the power grid dynamics, we consider a more general\nKuramoto model with higher-order interactions^61, which can be represented as:\n\n$${\\dot{\\theta }}_{i}={\\omega }_{i}+{\\gamma }_{1}\\mathop{\\sum\n}\\limits_{j=1}^{N}{A}_{ij}\\sin ({\\theta }_{j}-{\\theta }_{i})+{\\gamma\n}_{2}\\mathop{\\sum }\\limits_{j=1}^{N}\\mathop{\\sum\n}\\limits_{k=1}^{N}{B}_{ijk}\\sin ({\\theta }_{j}+{\\theta }_{k}-2{\\theta\n}_{i}),$$\n\n(18)\n\nwhere \u03b8_i and \u03c9_i denote the phase and natural frequency of the ith oscillator\nrespectively, \u03b3_1 and \u03b3_2 are the coupling strengths, while pairwise and\nhigher-order interactions are encoded in the adjacency matrix A and adjacency\ntensor B. Under specific coupling settings, this kind of system exhibits\nextremely complex chaotic dynamics rather than synchronization.\n\nFig. 4: Higher-order neighbors inference and dynamics prediction for the UK\npower grid system using the higher-order Kuramoto model.\n\na The UK power grid. b Local coupling structure of node 33. c Higher-order\nneighbors inference of node 33. d The average predictable steps of the entire\nsystem in the test set. The orange line in the middle of the box represents\nthe median, the upper and lower boundaries of the box represent the upper and\nlower quartiles, respectively. The boundaries of the upper and lower whiskers\nrepresent the maxima and minima, respectively. e Extrapolation prediction of\nnode 33 under different methods, with the true value shown in blue and the\npredicted value in red. We set T = 10,000, \u0394t = 0.08, \u03b3_1 = 0.4, and \u03b3_2 =\n0.4.\n\nFull size image\n\nDue to the special form of this model and the prediction challenges posed by\nhigher-order terms, we need to apply a special treatment when using the HoGRC\nframework. We take the 2-D data \\\\((\\sin (\\theta (t)),\\cos (\\theta (t)))\\\\) as\nthe input of the HoGRC framework at time t and \u0394\u03b8 = (\u03b8(t + 1) \u2212 \u03b8(t))/\u0394t as\nthe output. Therefore, the predicted value in the next step is \\\\(\\hat{\\theta\n}(t+1)={{\\Delta }}\\theta {{\\Delta }}t+\\theta (t)\\\\). Thus, in multi-step\nprediction tasks, we can use the predicted value \\\\((\\sin (\\hat{\\theta\n}(t+1)),\\cos (\\hat{\\theta }(t+1)))\\\\) as the input for iterative prediction.\nFor fairness, the RC and PRC methods also adopt the same treatment in the\nsubsequent comparative tests.\n\nTo verify the advantages of our method, we consider the higher-order\ninteractions which are constructed by identifying each distinct triangle from\nthe UK power grid and generated data for the experiment, and Fig. 4b shows the\nlocal coupling network of node 33 (see Supplementary Note 4.4 for details of\nall higher-order interactions). Figure 4c shows the one-step prediction error\nfor cases with different neighbors. We observe that the real higher-order\nneighbors correspond to the lowest prediction error. In the prediction task,\nour method outperforms the RC and PRC methods (see Fig. 4d, e), thanks to the\nstructural complexity and high nonlinearity of the model, which make\ntraditional methods prone to overfitting. Our method can learn the real\ndynamics of the system, leading to accurate predictions over a longer range.\n\n### Different role of noise perturbation\n\nNoise perturbation is a major factor that can affect the efficacy of any\nmethod in dealing with data. Hence, to demonstrate the robustness of our\nmethod against noise perturbations, we introduce noises of different\nintensities into the generated data.\n\nIn particular, we use Gaussian noise with zero mean and standard deviation \u03c3_n\nto introduce noise into the data. Empirically, due to the presence of noise,\nwe increase the threshold \u03b5_r to 0.03. Figure 5a shows the prediction\nperformances for cases without and with added noise. With a certain level of\nnoise intensity, such as \u03c3_n = 0.2, our method is able to infer higher-order\nneighbors for both the Lorenz63 system and the CL63 system (refer to\nSupplementary Note 4.5 for specific details). Figure 5b\u2013e shows the prediction\nperformances when increasing noise intensity for the Lorenz63 and CL63\nsystems, while Fig. 5f shows the results for the hyperchaotic system (see\nSupplementary Note 2.2). Clearly, our method works robustly on data with noise\nintensity in a certain range.\n\nFig. 5: Impact of noise on dynamics reconstruction and prediction using\ndifferent methods.\n\na Dynamics reconstruction and prediction with and without noise for the\nLorenz63 system. Prediction performances using different methods change with\nthe noise intensity for the Lorenz63 system (b), for the CL63 system (c\u2013e),\nwhere the corresponding coupling terms are selected, respectively, as (y_j \u2212\ny_i), \\\\(\\sin ({y}_{j}-{y}_{i})\\\\), and \u2223y_j \u2212 y_i\u2223), and for the hyperchaotic\nsystem (f).\n\nFull size image\n\nTo be candid, the excessive noise can adversely affect the accuracy of\npredictions across various examples. However, we interestingly find that in\nsome cases, a moderate amount of noise can promote predictions, as shown in\nFig. 5c\u2013f. This type of noise can enhance the generalization ability of our\nmethod, especially when the HoGRC framework experiences overfitting issues\neven after sufficient training. If the structures or dynamics of the learned\ndynamical system are not too complex, the HoGRC framework after training can\napproximate the original dynamics with high fidelity. Nevertheless, noise\ngenerally has a negative effect.\n\n### Influence of training set sizes and coupling network\n\nTraining set sizes and network structures are factors that significantly\ninfluence dynamic predictions. Typically, machine learning methods learn and\npredict unknown dynamics better with larger training set sizes or simpler\nnetwork structures. Although all methods follow this general rule, our HoGRC\nmethod still has several advantages. To demonstrate this, we conduct the\nfollowing numerical experiments.\n\nOn one hand, we use CRoS as an example to generate experimental data with\ndifferent time lengths (other settings are the same as above). As shown in\nFig. 6a, increasing the training data size initially improves prediction\naccuracy, which then levels off. Our method outperforms baseline methods even\nwith a sufficient amount of training data, suggesting that our method can\nlearn dynamics with fewer data points and more accurately capture real\ndynamical mechanisms. On the other hand, we investigate the impact of\ndifferent network structures. We begin by considering regular networks with\nvarying numbers of subsystems and generate experimental data using CRoS with a\nlength of 5000 and \u0394t = 0.1. As shown in Fig. 6b, the network scale does\naffect the prediction accuracy in that, for a long-term prediction task, the\nprediction failure of one subsystem in the network can impact the prediction\nof the other subsystems via its neighbors. Compared to baseline methods, our\nmethod is less affected by network size and presents better predictability for\nlarge-scale systems. These advantages persist when considering the Erd\u00f6s\u2013R\u00e9nyi\n(ER) networks^62 and the Barabasi-Albert (BA) networks^63 containing 30\nsubsystems, as demonstrated in Fig. 6c, e, f. Here, the average degrees of the\nregular, the ER, and the BA networks, respectively, are 2, 2.2, and 1.87. We\nrandomly generate the coupling weights connecting every two subsystems in\nthese networks.\n\nFig. 6: Impact of training set sizes and system structures on dynamics\nprediction.\n\na The prediction performances with different training set sizes. b The\nprediction performances in the regular networks with varying numbers of\nsubsystems. c The prediction performances in ER and BA networks with 30\nsubsystems. The orange line in the middle of the box represents the median,\nthe upper and lower boundaries of the box represent the upper and lower\nquartiles, respectively. The boundaries of the upper and lower whiskers\nrepresent the maxima and minima, respectively. d The predictable steps change\nwith the degree of the subsystems, respectively, for ER and BA networks. e, f\nThe prediction errors of the subsystems change with the time evolution using\nthe HoGRC framework for the ER and the BA networks, respectively.\n\nFull size image\n\nAdditionally, from Fig. 6e, f, we interestingly find that, under the same\naverage degrees, predicting the system using the BA network seems to be more\ndifficult, while using the regular network makes prediction much easier. This\nfinding is understandable since the degree distribution of the BA network\nfollows a power law distribution, which creates more complex structures and\nmore fruitful dynamics in the system. To further verify this finding, we use\nthe degree of subsystems as an indicator to reveal the complexity of\nsubsystems and depict different negative correlations between the number of\npredictable steps and the degree of each subsystem for different network\nsettings, as shown in Fig. 6d.\n\n### Direct and indirect causality\n\nIn our framework, the GC inference and the RC prediction are performed\nsimultaneously and complement each other. Notably, the HoGRC framework does\nnot require precise learning of the system structure through GC. Instead, our\nframework focuses on optimizing the coupling structures to further maximize\nthe prediction accuracy. As a result, both direct and indirect causality can\nbe inferred in the inference task. Despite this, our framework consistently\nand accurately infers the high-order structures in multiple experiments\nconducted in this study (see Supplementary Note 1.4 for specific reasons).\n\nTo further identify the direct and indirect causality, we can extend our HoGRC\nframework by combining it with the existing methods. In particular, we propose\ntwo strategies: (1) conditional Granger causality and (2) further causal\nidentification. We provide the details of the above two strategies and\nexperimental validation in Supplementary Note 1.4. The experimental results\ndemonstrate the high flexibility and generality of our framework, enabling it\nto identify direct and indirect causality in conjunction with some existing\ntechniques.\n\n## Discussion\n\nIn this article, we have introduced a scalable HoGRC framework that is\ninspired by the classic idea of Granger causality and advances achieved in\ndynamics predictions using RC framework. Our proposed method facilitates\naccurate system reconstructions and long-term dynamics predictions by\ninferring higher-order structures at the node level. The method comprises of\ntwo inseparable tasks: high-order structure inference and multi-step dynamics\nprediction. To close this article, we provide the concluding remarks as\nfollows.\n\nFirst, in many complex chaotic systems, the system variables often lack mutual\ncorrelation. As a result, traditional methods may lead to false causality and\nnegatively impact prediction accuracy. However, numerical experiments suggest\nthat stronger coupling weights between dynamic causes make them more easily\ninferred. Nonetheless, weak coupling weights still have a non-negligible\neffect on prediction accuracy and require delicate methods such as the HoGRC\nframework. In addition, our framework possesses high flexibility and\ngenerality, allowing for further identification of direct and indirect\ncausality by incorporating existing techniques.\n\nSecond, higher-order neighbors provide richer information than pairwise\nstructures. This is because they not only eliminate non-causal signals but\nalso significantly reduce the spurious interaction between causal signals.\nCompared to traditional methods, the HoGRC framework is better suited to\naccurately learning true dynamic mechanisms, thus avoiding overfitting during\nlong-term predictions of dynamics. Additionally, the HoGRC\u2019s node-level\nprediction method allows for parallel implementation of inference and\nprediction tasks, making it ideal for large-scale system data. Particularly\nfor complex coupling connections, where cause signals of nodes are intricate,\nthe HoGRC framework shines, whereas traditional methods are prone to\noverfitting.\n\nIn terms of the future research topics, there are several areas of focus that\nwarrant exploration. Firstly, it would be highly valuable to apply the newly\nproposed framework to a wider range of general dynamical systems with much\nmore complex higher-order interaction structures. Additionally, there is a\nneed to develop an efficient algorithm that can effectively eliminate the\nissue of indirect causality. Indeed, theoretical interpretations regarding\nthis new framework would be much more meaningful, promoting us to further\nenhance the framework. Future extensions would combine our framework with the\nother advanced neural programming frameworks^64 and extend its application to\nmore real-world complex systems. Overall, these future research directions\nwill contribute to advancing our understanding of complex dynamical systems\nand improving the practicality, scalability, and robustness of the proposed\nframework.\n\n## Methods\n\nHere, we formulate the HoGRC framework by incorporating the higher-order\nstructures that are possibly present in complex systems into the conventional\nRC method. To utilize higher-order neighbors precisely, we develop an\nalgorithm inspired by the Granger causality. This renders the HoGRC framework\napplicable to both structure inference and dynamics prediction.\n\n### From RC to higher-order RC\n\nThe traditional RC method comprises three parts, namely the input layer,\nhidden layer, and output layer. The N-D data x is embedded into a high-\ndimensional reservoir network at the input layer. Then, the n-D state sequence\n{r(t)} is obtained by specific rules within the reservoir as Eq. (2). Here,\nW_in and A are randomly generated and fixed, so we only need to train the\nparameter matrix W_out in the output layer. To better present our framework,\nwe introduce an equivalent transformation here where we predict the difference\ninstead of the next step value, given by Eq. (3). The ridge regression\ntechnique is generally used to obtain optimal W_out with the loss function as\nEq. (4). However, the single RC method discussed above disregards the\nintrinsic correlation of the N-D input data and instead predicts the entire\ndynamics through training as a black box. This approach makes it challenging\nto unveil underlying dynamical structures in high-dimensional complex systems.\n\nTo address this limitation, a parallel local strategy PRC based on entropy\ncausality was later proposed^39,40. In the PRC approach, a directed edge from\nnode v to u is connected and deemed a dynamic causal link if the dynamic\nequation of node u contains v. However, this approach only incorporates the\npairwise structures at the most elementary level for characterizing complex\nsystems. Instead, we integrate u and all its different order of neighbors as\ninputs into the input layer, thereby enhancing the prediction of node u.\n\nIn order to enhance the accuracy of reconstructing and predicting complex\ndynamics from the observational data, it is crucial to integrate the higher-\norder structures into our model. In light of this, we propose the HoGRC\nframework that integrates these structures. Specifically, for any node u\nwithin the system, analogous to Eq. (2), the hidden dynamic at the node-level\nin the HoGRC framework is given by (8), where the key of the structure input\nlies in encoding the higher-order neighbors into the input and the adjacency\nmatrices of the hidden dynamics, denoted as\n\\\\({\\tilde{{{{{{{{\\bf{W}}}}}}}}}}_{{{{{{{{\\rm{in}}}}}}}},u}\\\\) and\n\\\\({\\tilde{{{{{{{{\\bf{A}}}}}}}}}}_{u}\\\\) (see settings in (9)). In addition,\nthe higher sparsity in\n\\\\({\\tilde{{{{{{{{\\bf{W}}}}}}}}}}_{{{{{{{{\\rm{in}}}}}}}},u}\\\\) and\n\\\\({\\tilde{{{{{{{{\\bf{A}}}}}}}}}}_{u}\\\\) in the HoGRC framework eases the\nlearning task and minimizes overfitting. We provide theoretical explanations\nthrough the following proposition, assuming that different RC methods share\nthe same hyperparameters (see Supplementary Note 1.1 for its proof).\n\n### Proposition 1\n\nAssuming that the input matrix and the adjacency matrix in different RC models\nare generated by the same random method. Then,\n\n$${{{{{{{{\\mathscr{H}}}}}}}}}_{{{{{{{{\\rm{HoGRC}}}}}}}}}\\subseteq\n{{{{{{{{\\mathscr{H}}}}}}}}}_{{{{{{{{\\rm{RC}}}}}}}}},$$\n\n(19)\n\nwhere \\\\({{{{{{{{\\mathscr{H}}}}}}}}}_{{{{{{{{\\rm{RC}}}}}}}}}\\\\) and\n\\\\({{{{{{{{\\mathscr{H}}}}}}}}}_{{{{{{{{\\rm{HoGRC}}}}}}}}}\\\\) denote the sets\nof the hidden dynamical systems modeled by Eq. (2) in RC and by Eq. (8) in\nHoGRC, respectively. Furthermore, if the dataset has an upper bound, denoted\nby B, on its potential distribution \\\\({{{{{{{\\mathscr{D}}}}}}}}\\\\), i.e.,\n\n$$\\mathop{\\max}\\limits_{{{\\bf{x}}} \\sim {{\\mathscr{D}}}}\n\\parallel{{{\\bf{x}}}}\\parallel_{\\infty }\\le B,$$\n\n(20)\n\nwhere x is the N-D data. Then, the HoGRC framework has a smaller upper bound\nof the generalization error, that is,\n\n$$G{E}_{{{{{{{{\\rm{u}}}}}}}}}({h}_{{{{{{{{\\rm{HoGRC}}}}}}}}})\\le\nG{E}_{{{{{{{{\\rm{u}}}}}}}}}({h}_{{{{{{{{\\rm{RC}}}}}}}}}),$$\n\n(21)\n\nwhere \\\\({h}_{{{{{{{{\\rm{HoGRC}}}}}}}}}\\in\n{{{{{{{{\\mathscr{H}}}}}}}}}_{{{{{{{{\\rm{HoGRC}}}}}}}}}\\\\), \\\\({h}_{RC}\\in\n{{{{{{{{\\mathscr{H}}}}}}}}}_{RC}\\\\), and GE_u(h) denotes the upper bound on\nthe generalization error when reconstructing the original dimension u using\nthe hidden dynamical system h.\n\n### Structures inference and dynamics prediction\n\nAs mentioned earlier, our framework aims to leverage information from higher-\norder neighbors for prediction. However, in practice, the structure\ninformation is often unknown a priori, necessitating the inference of higher-\norder causal links connecting nodes before making predictions. Consequently,\nthe HoGRC possesses a two-folded mission: Higher-order neighbors inference and\ndynamics prediction using the inferred higher-order structures.\n\nTask (I): Inferring higher-order neighbors. Since higher-order interactions\nare inherently complex and nonlinear, the classic Granger causality method\ncannot be directly applied but brings us some inspiration. To this end, we\nconsider the case where node u \u2208 V awaits prediction, so we have\n\n$$u(t)=q\\left(\\\\{{{{{{{{{\\bf{c}}}}}}}}}_{1},{{{{{{{{\\bf{c}}}}}}}}}_{2},...,{{{{{{{{\\bf{c}}}}}}}}}_{K}\\\\}(\\\\!\n\\le \\\\! t)\\right)+{{{{{{{{\\bf{e}}}}}}}}}_{t},$$\n\n(22)\n\nwhere q is the prediction function represented by the HoGRC method,\n\\\\({{{{{{{\\mathscr{C}}}}}}}}=\\\\{{{{{{{{{\\bf{c}}}}}}}}}_{1},...,{{{{{{{{\\bf{c}}}}}}}}}_{K}\\\\}\\\\)\nis the candidate complex set containing higher-order neighbors of node u, and\n\\\\(q({{{{{{{\\mathscr{C}}}}}}}}(\\le t))\\\\) represents the one-step prediction\nresult obtained by inputting higher-order structure\n\\\\({{{{{{{\\mathscr{C}}}}}}}}\\\\) and the observed data x before time t. Then we\ncan define the mean prediction error as\n\n$${e}_{\\\\{{{{{{{{{\\bf{c}}}}}}}}}_{1},...,{{{{{{{{\\bf{c}}}}}}}}}_{K}\\\\}}(u)=\\frac{1}{T}\\mathop{\\sum}\\limits_{t}|\nq\\left(\\\\{{{{{{{{{\\bf{c}}}}}}}}}_{1},{{{{{{{{\\bf{c}}}}}}}}}_{2},...,{{{{{{{{\\bf{c}}}}}}}}}_{K}\\\\}(\\\\!\n\\le \\\\! t)\\right)-u(t+{{\\Delta }}t)|,$$\n\n(23)\n\nwhere T denotes the length of the data. In this context, excluding the Granger\ncausality from c_k to u implies that the function q does not depend on c_k. We\nformally define this concept as follows.\n\n### Definition 3\n\nAssume that all the higher-order causal links for node u are included in the\ncandidate set {c_1, . . . , c_K}. Also, assume that c_k is not a subcomplex of\nany other candidate simplicial complex and further that the inequality\n\n$${e}_{\\\\{{{{{{{{{\\bf{c}}}}}}}}}_{1},...,{{{{{{{{\\bf{c}}}}}}}}}_{K}\\\\}}(u)+{\\epsilon\n}_{{{{{{{{\\rm{e}}}}}}}}}\\ge\n{e}_{\\\\{{{{{{{{{\\bf{c}}}}}}}}}_{1},...,{{{{{{{{\\bf{c}}}}}}}}}_{k-1},{{{{{{{{\\bf{c}}}}}}}}}_{k+1},...,{{{{{{{{\\bf{c}}}}}}}}}_{K}\\\\}}(u)$$\n\n(24)\n\nis satisfied. Then, the simplicial complex c_k is not the causal factor in\nGranger\u2019s sense for node u, where \u03b5_e is a threshold taking positive value.\nThat is, the complex c_k is not a higher-order neighbor of node u.\n\nIn truth, other metrics may also be used to evaluate prediction performance.\nWe propose a greedy strategy that searches for the exact higher-order\nneighbors and filters candidate complexes in order of decreasing dimension and\nimportance. The algorithmic process is briefly outlined in Algorithm 1 of\nTable 1, with additional details about the algorithm and the selection of the\nthreshold \u03b5_e provided in Supplementary Note 1.2.\n\nTask (II): Predicting dynamics using the HoGRC framework. Using the inferred\nhigher-order interactions, we provide data for each node and its higher-order\nneighbors to the HoGRC, which then predicts subsequent values over time. By\ncontinually adding the most recent forecasted values to the input data, we can\nmake multistep-ahead predictions.\n\n## Data availability\n\nAll the datasets generated in this study have been deposited in the Github\ndatabase under the accession code in \u201cdataset\u201d folder in GitHub repository:\nhttps://github.com/CsnowyLstar/HoGRC[https://doi.org/10.5281/zenodo.10685733]^65.\n\n## Code availability\n\nThe code used in this study is freely available in the public GitHub\nrepository:\nhttps://github.com/CsnowyLstar/HoGRC[https://doi.org/10.5281/zenodo.10685733]^65.\n\n## References\n\n  1. LeCun, Y. & Bengio, Y. Deep learning. Nature 521, 436\u2013444 (2015).\n\nArticle CAS PubMed ADS Google Scholar\n\n  2. Devlin, J., Chang, M.W., Lee, K. & Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, Vol. 1, p 2 (Association for Computational Linguistics, 2019).\n\n  3. Davies, A. Advancing mathematics by guiding human intuition with ai. Nature 600, 70\u201374 (2021).\n\nArticle CAS PubMed PubMed Central ADS Google Scholar\n\n  4. Jumper, J. Highly accurate protein structure prediction with alphafold. Nature 596, 583\u2013589 (2021).\n\nArticle CAS PubMed PubMed Central ADS Google Scholar\n\n  5. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C. & Chen, M. Hierarchical text-conditional image generation with clip latents. Preprint at https://arxiv.org/abs/2204.06125 (2022).\n\n  6. Zhang, J., Zhu, Q. & Lin, W. Learning hamiltonian neural koopman operator and simultaneously sustaining and discovering conservation laws. Phys. Rev. Res. 6, L012031 (2024).\n\nArticle Google Scholar\n\n  7. Smale, S. Differentiable dynamical systems. Bull. Am. Math. Soc. 73, 747\u2013817 (1967).\n\nArticle MathSciNet Google Scholar\n\n  8. Zeger, S. L., Irizarry, R. & Peng, R. D. On time series analysis of public health and biomedical data. Annu. Rev. Public Health 27, 57\u201379 (2006).\n\nArticle PubMed Google Scholar\n\n  9. Ma, H., Lin, W. & Lai, Ying-Cheng Detecting unstable periodic orbits in high-dimensional chaotic systems from time series: Reconstruction meeting with adaptation. Phys. Rev. E 87, 050901 (2013).\n\nArticle ADS Google Scholar\n\n  10. Vlahogianni, E. I., Karlaftis, M. G. & Golias, J. C. Short-term traffic forecasting: Where we are and where we\u2019re going. Transp. Res. Part C Emerg. Technol. 43, 3\u201319 (2014).\n\nArticle Google Scholar\n\n  11. Reuter, J. A., Spacek, D. V. & Snyder, M. P. High-throughput sequencing technologies. Mol. Cell 58, 586\u2013597 (2015).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  12. Ma, H., Leng, S., Aihara, K., Lin, W. & Chen, L. Randomly distributed embedding making short-term high-dimensional data predictable. Proc. Natl Acad. Sci. USA 115, E9994\u2013E10002 (2018).\n\nArticle MathSciNet CAS PubMed PubMed Central ADS Google Scholar\n\n  13. Beregi, S., Barton, DavidA. W., Rezgui, D. & Neild, S. Using scientific machine learning for experimental bifurcation analysis of dynamic systems. Mech. Syst. Signal Process. 184, 109649 (2023).\n\nArticle Google Scholar\n\n  14. Casdagli, M. Nonlinear prediction of chaotic time series. Phys. D Nonlinear Phenom. 35, 335\u2013356 (1989).\n\nArticle MathSciNet ADS Google Scholar\n\n  15. Zang, C. & Wang, F. Neural dynamics on complex networks. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 892\u2013902, (ACM, 2020).\n\n  16. Gao, Ting-Ting & Yan, G. Autonomous inference of complex network dynamics from incomplete and noisy data. Nat. Comput. Sci. 2, 160\u2013168 (2022).\n\nArticle PubMed Google Scholar\n\n  17. Harush, U. & Barzel, B. Dynamic patterns of information flow in complex networks. Nat. Commun. 8, 1\u201311 (2017).\n\nArticle CAS Google Scholar\n\n  18. Sanhedrai, H. et al. Reviving a failed network through microscopic interventions. Nat. Phys. 18, 338\u2013349 (2022).\n\nArticle CAS Google Scholar\n\n  19. Navarro-Moreno, Jes\u00fas Arma prediction of widely linear systems by using the innovations algorithm. IEEE Trans. Signal Process. 56, 3061\u20133068 (2008).\n\nArticle MathSciNet ADS Google Scholar\n\n  20. Rico-Martinez, R., Krischer, K., Kevrekidis, I. G., Kube, M. C. & Hudson, J. L. Discrete-vs. continuous-time nonlinear signal processing of cu electrodissolution data. Chem. Eng. Commun. 118, 25\u201348 (1992).\n\nArticle CAS Google Scholar\n\n  21. Medsker, L. R. & Jain, L. C. Recurrent neural networks. Des. Appl. 5, 2 (2001).\n\nGoogle Scholar\n\n  22. Chen, R.T., Rubanova, Y., Bettencourt, J. & Duvenaud, D.K. Neural ordinary differential equations. In Advances in Neural Information Processing Systems, Vol. 31. (eds. Bengio, S. et al.) (Curran Associates, Inc., 2018).\n\n  23. Mukhopadhyay, S. & Banerjee, S. Learning dynamical systems in noise using convolutional neural networks. Chaos Interdiscip. J. Nonlinear Sci. 30, 103125 (2020).\n\nArticle MathSciNet Google Scholar\n\n  24. Hochreiter, S. & Schmidhuber, J. \u00fcrgen Long short-term memory. Neural Comput. 9, 1735\u20131780 (1997).\n\nArticle CAS PubMed Google Scholar\n\n  25. Cho, K. Learning phrase representations using RNN encoder\u2013decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), (eds. Moschitti, A., Pang, B., Daelemans, W.) 1724\u20131734 (Association for Computational Linguistics, 2014).\n\n  26. Tang, Y., Kurths, J. \u00fcrgen, Lin, W., Ott, E. & Kocarev, L. Introduction to focus issue: When machine learning meets complex systems: Networks, chaos, and nonlinear dynamics. Chaos Interdiscip. J. Nonlinear Sci. 30, 063151 (2020).\n\nArticle MathSciNet Google Scholar\n\n  27. Luko\u0161evi\u010dius, M. & Jaeger, H. Reservoir computing approaches to recurrent neural network training. Comput. Sci. Rev. 3, 127\u2013149 (2009).\n\nArticle Google Scholar\n\n  28. Jaeger, H. The \u201cecho state\u201d approach to analysing and training recurrent neural networks-with an erratum note. Bonn. Ger. Ger. Natl Res. Cent. Inf. Technol. GMD Tech. Rep. 148, 13 (2001).\n\nGoogle Scholar\n\n  29. Li, X. et al. Tipping point detection using reservoir computing. Research 6, 0174 (2023).\n\nArticle PubMed PubMed Central Google Scholar\n\n  30. Su\u00e1rez, L. E. et al. Connectome-based reservoir computing with the conn2res toolbox. Nat. Commun. 15, 656 (2024).\n\nArticle PubMed PubMed Central ADS Google Scholar\n\n  31. Duan, Xing-Yue et al. Embedding theory of reservoir computing and reducing reservoir network using time delays. Phys. Rev. Res. 5, L022041 (2023).\n\nArticle CAS Google Scholar\n\n  32. Zhu, Q., Ma, H. & Lin, W. Detecting unstable periodic orbits based only on time series: When adaptive delayed feedback control meets reservoir computing. Chaos Interdiscip. J. Nonlinear Sci. 29, 093125 (2019).\n\nArticle MathSciNet Google Scholar\n\n  33. Griffith, A., Pomerance, A. & Gauthier, D. J. Forecasting chaotic systems with very low connectivity reservoir computers. Chaos Interdiscip. J. Nonlinear Sci. 29, 123108 (2019).\n\nArticle MathSciNet Google Scholar\n\n  34. Banerjee, A., Pathak, J., Roy, R., Restrepo, J. G. & Ott, E. Using machine learning to assess short term causal dependence and infer network links. Chaos Interdiscip. J. Nonlinear Sci. 29, 121104 (2019).\n\nArticle MathSciNet Google Scholar\n\n  35. Lu, Z., Hunt, B. R. & Ott, E. Attractor reconstruction by machine learning. Chaos Interdiscip. J. Nonlinear Sci. 28, 061104 (2018).\n\nArticle MathSciNet Google Scholar\n\n  36. Luko\u0161evi\u010dius, M. A practical guide to applying echo state networks. In Neural networks: Tricks of the trade, 659\u2013686 (Springer, 2012).\n\n  37. Gauthier, D. J., Bollt, E., Griffith, A. & Barbosa, WendsonA. S. Next generation reservoir computing. Nat. Commun. 12, 1\u20138 (2021).\n\nArticle Google Scholar\n\n  38. Gallicchio, C., Micheli, A. & Pedrelli, L. Deep reservoir computing: A critical experimental analysis. Neurocomputing 268, 87\u201399 (2017).\n\nArticle Google Scholar\n\n  39. Pathak, J., Hunt, B., Girvan, M., Lu, Z. & Ott, E. Model-free prediction of large spatiotemporally chaotic systems from data: A reservoir computing approach. Phys. Rev. Lett. 120, 024102 (2018).\n\nArticle CAS PubMed ADS Google Scholar\n\n  40. Srinivasan, K. et al. Parallel machine learning for forecasting the dynamics of complex networks. Phys. Rev. Lett. 128, 164101 (2022).\n\nArticle MathSciNet CAS PubMed ADS Google Scholar\n\n  41. Granger, C.W. Investigating causal relations by econometric models and cross-spectral methods. Econometrica, 37, 424\u2013438, (1969).\n\n  42. Tank, A., Covert, I., Foti, N., Shojaie, A. & Fox, E. B. Neural granger causality. IEEE Trans. Pattern Anal. Mach. Intell. 44, 4267\u20134279 (2021).\n\nGoogle Scholar\n\n  43. Duggento, A., Stankovski, T., McClintock, PeterV. E. & Stefanovska, A. Dynamical bayesian inference of time-evolving interactions: From a pair of coupled oscillators to networks of oscillators. Phys. Rev. E 86, 061126 (2012).\n\nArticle ADS Google Scholar\n\n  44. Sugihara, G. et al. Detecting causality in complex ecosystems. Science 338, 496\u2013500 (2012).\n\nArticle CAS PubMed ADS Google Scholar\n\n  45. Leng, S. et al. Partial cross mapping eliminates indirect causal influences. Nat. Commun. 11, 2632 (2020).\n\nArticle CAS PubMed PubMed Central ADS Google Scholar\n\n  46. Ying, X. et al. Continuity scaling: A rigorous framework for detecting and quantifying causality accurately. Research 2022, 9870149 (2022).\n\nArticle PubMed PubMed Central ADS Google Scholar\n\n  47. Battiston, F. The physics of higher-order interactions in complex systems. Nat. Phys. 17, 1093\u20131098 (2021).\n\nArticle CAS Google Scholar\n\n  48. Schaub, M. T., Benson, A. R., Horn, P., Lippner, G. & Jadbabaie, A. Random walks on simplicial complexes and the normalized hodge 1-laplacian. SIAM Rev. 62, 353\u2013391 (2020).\n\nArticle MathSciNet Google Scholar\n\n  49. Skardal, PerSebastian & Arenas, A. Abrupt desynchronization and extensive multistability in globally coupled oscillator simplexes. Phys. Rev. Lett. 122, 248301 (2019).\n\nArticle CAS PubMed ADS Google Scholar\n\n  50. Alvarez-Rodriguez, U. et al. Evolutionary dynamics of higher-order interactions in social networks. Nat. Hum. Behav. 5, 586\u2013595 (2021).\n\nArticle PubMed Google Scholar\n\n  51. Brunton, S. L., Proctor, J. L. & Kutz, J. N. Discovering governing equations from data by sparse identification of nonlinear dynamical systems. Proc. Natl Acad. Sci. 113, 3932\u20133937 (2016).\n\nArticle MathSciNet CAS PubMed PubMed Central ADS Google Scholar\n\n  52. Kaiser, E., Kutz, J. N. & Brunton, S. L. Sparse identification of nonlinear dynamics for model predictive control in the low-data limit. Proc. R. Soc. A 474, 20180335 (2018).\n\nArticle MathSciNet CAS PubMed PubMed Central ADS Google Scholar\n\n  53. AlRahman R AlMomani, A., Sun, J. & Bollt, E. M. How entropic regression beats the outliers problem in nonlinear system identification. Chaos 30, 013107 (2019).\n\nArticle MathSciNet ADS Google Scholar\n\n  54. Tikhonov, A. N. On the solution of ill-posed problems and the method of regularization. In Doklady akademii nauk, Vol 151, 501\u2013504 (Russian Academy of Sciences, 1963).\n\n  55. Battiston, F. et al. Networks beyond pairwise interactions: structure and dynamics. Phys. Rep. 874, 1\u201392 (2020).\n\nArticle MathSciNet ADS Google Scholar\n\n  56. Lorenz, E. Chaos in meteorological forecast. J. Atmos. Sci. 20, 130\u2013141 (1963).\n\nArticle ADS Google Scholar\n\n  57. FitzHugh, R. Impulses and physiological states in theoretical models of nerve membrane. Biophys. J. 1, 445\u2013466 (1961).\n\nArticle CAS PubMed PubMed Central ADS Google Scholar\n\n  58. Li, Xiao-Wen & Zheng, Zhi-Gang Phase synchronization of coupled rossler oscillators: amplitude effect. Commun. Theor. Phys. 47, 265 (2007).\n\nArticle ADS Google Scholar\n\n  59. Rabinovich, M. I., Varona, P., Selverston, A. I. & Abarbanel, HenryD. I. Dynamical principles in neuroscience. Rev. Mod. Phys. 78, 1213 (2006).\n\nArticle ADS Google Scholar\n\n  60. Rohden, M., Sorge, A., Timme, M. & Witthaut, D. Self-organized synchronization in decentralized power grids. Phys. Rev. Lett. 109, 064101 (2012).\n\nArticle PubMed ADS Google Scholar\n\n  61. Skardal, PerSebastian & Arenas, A. Higher order interactions in complex networks of phase oscillators promote abrupt synchronization switching. Commun. Phys. 3, 1\u20136 (2019).\n\nGoogle Scholar\n\n  62. RENYI, E. On random graph. Publ. Math. 6, 290\u2013297 (1959).\n\nMathSciNet Google Scholar\n\n  63. Barab\u00e1si, Albert-L. \u00e1szl\u00f3 & Albert, R. \u00e9ka Emergence of scaling in random networks. Science 286, 509\u2013512 (1999).\n\nArticle MathSciNet PubMed ADS Google Scholar\n\n  64. Kim, J. Z. & Bassett, D. S. A neural machine code and programming framework for the reservoir computer. Nat. Mach. Intell. 5, 622\u2013630 (2023).\n\nArticle Google Scholar\n\n  65. Li, X. Higher-order Granger reservoir computing: analysis code. zenodo https://doi.org/10.5281/zenodo.10685734 (2024).\n\nDownload references\n\n## Acknowledgements\n\nQ.Z. is supported by the China Postdoctoral Science Foundation (No.\n2022M720817), by the Shanghai Postdoctoral Excellence Program (No. 2021091),\nand by the STCSM (Nos. 21511100200, 22ZR1407300, and 22dz1200502). W.L. is\nsupported by the National Natural Science Foundation of China (No. 11925103)\nand by the STCSM (Nos. 22JC1402500, 22JC1401402, and 2021SHZDZX0103). H.M. is\nsupported by the National Natural Science Foundation of China (No. 12171350).\nThe computational work presented in this article is supported by the CFFF\nplatform of Fudan University.\n\n## Author information\n\n### Authors and Affiliations\n\n  1. Center for Applied Mathematics (NUDT), Changsha, 410073, Hunan, China\n\nXin Li, Chengli Zhao, Xiaojun Duan & Xue Zhang\n\n  2. Research Institute of Intelligent Complex Systems and MOE Frontiers Center for Brain Science, Fudan University, Shanghai, 200433, China\n\nXin Li, Qunxi Zhu, Bolin Zhao, Jie Sun & Wei Lin\n\n  3. School of Mathematical Sciences, SCMS, SCAM, and CCSB, Fudan University, Shanghai, 200433, China\n\nQunxi Zhu, Bolin Zhao & Wei Lin\n\n  4. School of Mathematical Sciences, Soochow University, Suzhou, 215006, China\n\nHuanfei Ma\n\n  5. HUAWEI Technologies Co., Ltd., Hong Kong, China\n\nJie Sun\n\n  6. Shanghai Artificial Intelligence Laboratory, Shanghai, 200232, China\n\nWei Lin\n\nAuthors\n\n  1. Xin Li\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  2. Qunxi Zhu\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  3. Chengli Zhao\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  4. Xiaojun Duan\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  5. Bolin Zhao\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  6. Xue Zhang\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  7. Huanfei Ma\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  8. Jie Sun\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  9. Wei Lin\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n### Contributions\n\nW. Lin conceived the idea. C. L. Zhao and Q. X. Zhu designed the research and\nhelped perform the analysis with constructive discussions. X. Li performed the\nexperiments and wrote the initial draft of the manuscript. X. Zhang and B. L.\nZhao collected the data and carried out additional analyses. X. J. Duan, H. F.\nMa, and J. Sun contributed to refining the ideas. All authors contributed to\nwriting the manuscript.\n\n### Corresponding authors\n\nCorrespondence to Qunxi Zhu, Chengli Zhao or Wei Lin.\n\n## Ethics declarations\n\n### Competing interests\n\nThe authors declare no competing interests.\n\n## Peer review\n\n### Peer review information\n\nNature Communications thanks Raphael Couturier, J\u00fcrgen Kurths and the other\nanonymous reviewer(s) for their contribution to the peer review of this work.\nA peer review file is available.\n\n## Additional information\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional\nclaims in published maps and institutional affiliations.\n\n## Supplementary information\n\n### Supplementary Information\n\n### Peer Review File\n\n## Rights and permissions\n\nOpen Access This article is licensed under a Creative Commons Attribution 4.0\nInternational License, which permits use, sharing, adaptation, distribution\nand reproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the\nCreative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article\u2019s\nCreative Commons licence, unless indicated otherwise in a credit line to the\nmaterial. If material is not included in the article\u2019s Creative Commons\nlicence and your intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this licence, visit\nhttp://creativecommons.org/licenses/by/4.0/.\n\nReprints and permissions\n\n## About this article\n\n### Cite this article\n\nLi, X., Zhu, Q., Zhao, C. et al. Higher-order Granger reservoir computing:\nsimultaneously achieving scalable complex structures inference and accurate\ndynamics prediction. Nat Commun 15, 2506 (2024).\nhttps://doi.org/10.1038/s41467-024-46852-1\n\nDownload citation\n\n  * Received: 05 July 2023\n\n  * Accepted: 12 March 2024\n\n  * Published: 20 March 2024\n\n  * DOI: https://doi.org/10.1038/s41467-024-46852-1\n\n### Share this article\n\nAnyone you share the following link with will be able to read this content:\n\nSorry, a shareable link is not currently available for this article.\n\nProvided by the Springer Nature SharedIt content-sharing initiative\n\n### Subjects\n\n  * Complex networks\n  * Computational science\n  * Information theory and computation\n  * Nonlinear phenomena\n\n## Comments\n\nBy submitting a comment you agree to abide by our Terms and Community\nGuidelines. If you find something abusive or that does not comply with our\nterms or guidelines please flag it as inappropriate.\n\nDownload PDF\n\n## Associated content\n\nFocus\n\n### Applied physics and mathematics\n\nAdvertisement\n\nNature Communications (Nat Commun) ISSN 2041-1723 (online)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n  * Cancel contracts here\n\n\u00a9 2024 Springer Nature Limited\n\nSign up for the Nature Briefing: AI and Robotics newsletter \u2014 what matters in\nAI and robotics research, free to your inbox weekly.\n\nGet the most important science stories of the day, free in your inbox. Sign up\nfor Nature Briefing: AI and Robotics\n\n", "frontpage": false}
