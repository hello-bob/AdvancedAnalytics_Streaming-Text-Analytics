{"aid": "40157664", "title": "Options for accessing Llama 3 from the terminal using LLM", "url": "https://simonwillison.net/2024/Apr/22/llama-3/#fast-api-access-via-groq", "domain": "simonwillison.net", "votes": 1, "user": "gmays", "posted_at": "2024-04-25 13:57:24", "comments": 0, "source_title": "Options for accessing Llama 3 from the terminal using LLM", "source_text": "Options for accessing Llama 3 from the terminal using LLM\n\n# Simon Willison\u2019s Weblog\n\nSubscribe\n\n## Options for accessing Llama 3 from the terminal using LLM\n\n22nd April 2024\n\nLlama 3 was released on Thursday. Early indications are that it\u2019s now the best\navailable openly licensed model\u2014Llama 3 70b Instruct has taken joint 5th place\non the LMSYS arena leaderboard, behind only Claude 3 Opus and some GPT-4s and\nsharing 5th place with Gemini Pro and Claude 3 Sonnet. But unlike those other\nmodels Llama 3 70b is weights available and can even be run on a (high end)\nlaptop!\n\nMy LLM command-line tool and Python library provides access to dozens of\nmodels via plugins. Here are several ways you can use it to access Llama 3,\nboth hosted versions and running locally on your own hardware.\n\n  * Llama-3-8B-Instruct locally with llm-gpt4all\n  * Fast API access via Groq\n  * Local Llama 3 70b Instruct with llamafile\n  * Paid access via other API providers\n\n#### Llama-3-8B-Instruct locally with llm-gpt4all #\n\nIf you want to run Llama 3 locally, the easiest way to do that with LLM is\nusing the llm-gpt4all plugin. This plugin builds on the excellent gpt4all\nproject by Nomic AI, providing a quantized (q4) Llama 3 8B instruct model that\nneeds 8GB of RAM and requires a 4.34GB download.\n\nIf you don\u2019t have LLM installed yet, install it like this:\n\n    \n    \n    brew install llm\n\nOr like this:\n\n    \n    \n    pipx install llm\n\nInstall the plugin:\n\n    \n    \n    llm install llm-gpt4all\n\nThe first time you run a prompt it will download the model:\n\n    \n    \n    llm -m Meta-Llama-3-8B-Instruct \"Three great names for a pet emu\"\n\n> What an interesting question! Here are three name suggestions for your pet\n> emu:\n>\n>   1. Emmet: A playful take on the word \u201cemu\u201d that suits a friendly and\n> curious bird.\n>   2. Gizmo: Emus are known for their quirky behavior, so Gizmo is a fitting\n> name to capture their unique personality.\n>   3. Aussie: As an Australian native species, naming your emu Aussie would\n> be a nod to its heritage.\n>\n\n>\n> Remember to choose a name that suits your pet\u2019s temperament and\n> characteristics!\n\nEach time you run a command like this the model will be loaded into memory\nfrom scratch. To keep it resident in memory, use the llm chat command:\n\n    \n    \n    llm chat -m Meta-Llama-3-8B-Instruct\n\nHere\u2019s an example session:\n\n    \n    \n    Chatting with Meta-Llama-3-8B-Instruct Type 'exit' or 'quit' to exit Type '!multi' to enter multiple lines, then '!end' to finish > javascript function to reverse a string % Here is the JavaScript code that reverses a given string: ``` function reverseString(str) { return str.split(\"\").reverse().join(\"\"); } ``` You can use this function by passing your desired input string as an argument, like so: ```javascript let reversedStr = reverseString(\"Hello World\"); console.log(reversedStr); // Output: \"dlroW olleH\" ``` In the code above: - `str.split(\"\")` splits the original string into individual characters. - `.reverse()` reverses the order of those characters. - `.join(\"\")` combines them back together as a single string. This function works for any input string, not just \"Hello World\". Try it out with different strings to see how it handles punctuation and spaces! > capitalize it too % Here's an updated JavaScript function that not only reverses the input string but also capitalizes each character: ```javascript function reverseAndCapitalize(str) { return str.split(\"\").reverse().map(function(char) { if (char.match(/[a-z]/i)) { return char.toUpperCase(); } else { return char; } }).join(\"\"); } ```\n\n(I\u2019m not sure I\u2019d write reverseAndCapitalize() quite like that, but still a\nfun example!)\n\nConsult the LLM documentation for more details on how to use the command-line\ntool.\n\n#### Fast API access via Groq #\n\nGroq serve openly licensed LLMs at ludicrous speeds using their own custom LPU\n(Language Processing Unit) Inference Engine. They currently offer a free\npreview of their API: you can sign up and obtain an API key to start using it.\n\nYou can run prompts against Groq using their OpenAI compatible API endpoint.\n\nEdit the file ~/Library/Application Support/io.datasette.llm/extra-openai-\nmodels.yaml\u2014creating it if it doesn\u2019t exist\u2014and add the following lines to it:\n\n    \n    \n    - model_id: groq-openai-llama3 model_name: llama3-70b-8192 api_base: https://api.groq.com/openai/v1 api_key_name: groq - model_id: groq-openai-llama3-8b model_name: llama3-8b-8192 api_base: https://api.groq.com/openai/v1 api_key_name: groq\n\nThis tells LLM about those models, and makes them accessible via those\nconfigured model_id values.\n\nRun this command to confirm that the models were registered correctly:\n\n    \n    \n    llm models | grep groq\n\nYou should see this:\n\n    \n    \n    OpenAI Chat: groq-openai-llama3 OpenAI Chat: groq-openai-llama3-8b\n\nSet your Groq API key like this:\n\n    \n    \n    llm keys set groq # <Paste your API key here>\n\nNow you should be able to run prompts through the models like this:\n\n    \n    \n    llm -m groq-openai-llama3 \"A righteous sonnet about a brave owl\"\n\nGroq is fast.\n\nThere\u2019s also a llm-groq plugin but it hasn\u2019t shipped support for the new\nmodels just yet\u2014though there\u2019s a PR for that by Lex Herbert here and you can\ninstall the plugin directly from that PR like this:\n\n    \n    \n    llm install https://github.com/lexh/llm-groq/archive/ba9d7de74b3057b074a85fe99fe873b75519bd78.zip llm keys set groq # paste API key here llm -m groq-llama3-70b 'say hi in spanish five ways'\n\n#### Local Llama 3 70b Instruct with llamafile #\n\nThe Llama 3 8b model is easy to run on a laptop, but it\u2019s pretty limited in\ncapability. The 70b model is the one that\u2019s starting to get competitive with\nGPT-4. Can we run that on a laptop?\n\nI managed to run the 70b model on my 64GB MacBook Pro M2 using llamafile\n(previously on this blog)\u2014after quitting most other applications to make sure\nthe 37GB of RAM it needed was available.\n\nI used the Meta-Llama-3-70B-Instruct.Q4_0.llamafile Q4 version from\njartine/Meta-Llama-3-70B-Instruct-llamafile\u2014a 37GB download. I have a\ndedicated external hard disk (a Samsung T7 Shield) for this kind of thing.\n\nHere\u2019s how I got it working:\n\n    \n    \n    curl -L -o Meta-Llama-3-70B-Instruct.Q4_0.llamafile 'https://huggingface.co/jartine/Meta-Llama-3-70B-Instruct-llamafile/resolve/main/Meta-Llama-3-70B-Instruct.Q4_0.llamafile?download=true' # That downloads 37GB - now make it executable chmod 755 Meta-Llama-3-70B-Instruct.Q4_0.llamafile # And start it running: ./Meta-Llama-3-70B-Instruct.Q4_0.llamafile\n\nA llamafile is an executable that runs on virtually any platform\u2014see my\nprevious notes on Cosmopolitan and Actually Portable Executable for more on\nhow that works.\n\nThis will take quite a while to start, because it needs to load that full 37GB\nof binary content into memory. Once it\u2019s finished loading a local web server\nbecomes available at http://127.0.0.1:8080/\u2014this serves a web UI you can use\nto interact with the model, and also serves another OpenAI-compatible API\nendpoint.\n\nThe easiest way to access this from LLM is to install the llm-llamafile\nplugin:\n\n    \n    \n    llm install llm-llamafile\n\nAll this plugin does is configure a model called llamafile that attempts to\naccess the model hosted on port 8080. You can run prompts like this:\n\n    \n    \n    llm -m llamafile \"3 neat characteristics of a pelican\"\n\n> Here are three neat characteristics of a pelican:\n>\n>   1. Unique Beak: Pelicans have a distinctive beak that is shaped like a\n> scoop or a basket. This beak is specially designed to catch fish, and it can\n> hold up to 3 gallons of water! The beak is also very sensitive, which helps\n> pelicans detect the presence of fish in the water.\n>\n>   2. Waterproof Feathers: Pelicans have a special coating on their feathers\n> that makes them waterproof. This is essential for their fishing lifestyle,\n> as they need to be able to dive into the water without getting weighed down\n> by wet feathers. The coating is made up of a waxy substance that helps to\n> repel water.\n>\n>   3. Pouch-Like Throat: Pelicans have a unique throat pouch that allows them\n> to catch and store fish. When they dive into the water, they use their beak\n> to scoop up fish, and then they store them in their throat pouch. The pouch\n> can expand to hold multiple fish, and the pelican can then swallow the fish\n> whole or regurgitate them to feed their young. This pouch is a key\n> adaptation that helps pelicans thrive in their aquatic environment.\n>\n>\n\nIf you don\u2019t want to install another plugin, you can instead configure the\nmodel by adding this to your openai-extra-models.yaml file:\n\n    \n    \n    - model_id: llamafile model_name: llamafile api_base: http://localhost:8080/v1 api_key: x\n\nOne warning about this approach: if you use LLM like this then every prompt\nyou run through llamafile will be stored under the same model name in your\nSQLite logs, even if you try out different llamafile models at different\ntimes. You could work around this by registering them with different model_id\nvalues in the YAML file.\n\n#### Paid access via other API providers #\n\nA neat thing about open weight models is that multiple API providers can offer\nthem, encouraging them to aggressively compete on price.\n\nGroq is currently free, but that\u2019s with a limited number of free requests.\n\nA number of other providers are now hosting Llama 3, and many of them have\nplugins available for LLM. Here are a few examples:\n\n  * Perplexity Labs are offering llama-3-8b-instruct and llama-3-70b-instruct. The llm-perplexity plugin provides access\u2014llm install llm-perplexity to install, llm keys set perplexity to set an API key and then run prompts against those two model IDs. Current price for 8b is $0.20 per million tokens, for 80b is $1.00.\n  * Anyscale Endpoints have meta-llama/Llama-3-8b-chat-hf ($0.15/million tokens) and meta-llama/Llama-3-70b-chat-hf ($1.0/million tokens) (pricing). llm install anyscale-endpoints, then llm keys set anyscale-endpoints to set the API key.\n  * Fireworks AI have fireworks/models/llama-v3-8b-instruct for $0.20/million and fireworks/models/llama-v3-70b-instruct for $0.90/million (pricing). llm install fireworks, then llm keys set fireworks to set the API key.\n  * OpenRouter provide proxied accessed to Llama 3 from a number of different providers at different prices, documented on their meta-llama/llama-3-70b-instruct and meta-llama/llama-3-8b-instruct pages (and more). Use the llm-openrouter plugin for those.\n  * Together AI has both models as well. The llm-together plugin provides access to meta-llama/Llama-3-8b-chat-hf and meta-llama/Llama-3-70b-chat-hf.\n\nI\u2019m sure there are more\u2014these are just the ones I\u2019ve tried out myself. Check\nthe LLM plugin directory for other providers, or if a provider emulates the\nOpenAI API you can configure with the YAML file as shown above or described in\nthe LLM documentation.\n\n#### That\u2019s a lot of options #\n\nOne key idea behind LLM is to use plugins to provide access to as many\ndifferent models as possible. Above I\u2019ve listed two ways to run Llama 3\nlocally and six different API vendors that LLM can access as well.\n\nIf you\u2019re inspired to write your own plugin it\u2019s pretty simple: each of the\nabove plugins is open source, and there\u2019s a detailed tutorial on Writing a\nplugin to support a new model on the LLM website.\n\nPosted 22nd April 2024 at 1:38 pm \u00b7 Follow me on Mastodon or Twitter or\nsubscribe to my newsletter\n\n## More recent articles\n\n  * Weeknotes: Llama 3, AI for Data Journalism, llm-evals and datasette-secrets - 23rd April 2024\n  * AI for Data Journalism: demonstrating what we can do with this stuff right now - 17th April 2024\n  * Three major LLM releases in 24 hours (plus weeknotes) - 10th April 2024\n  * Building files-to-prompt entirely using Claude 3 Opus - 8th April 2024\n  * Running OCR against PDFs and images directly in your browser - 30th March 2024\n  * llm cmd undo last git commit - a new plugin for LLM - 26th March 2024\n  * Building and testing C extensions for SQLite with ChatGPT Code Interpreter - 23rd March 2024\n  * Claude and ChatGPT for ad-hoc sidequests - 22nd March 2024\n  * Weeknotes: the aftermath of NICAR - 16th March 2024\n\nThis is Options for accessing Llama 3 from the terminal using LLM by Simon\nWillison, posted on 22nd April 2024.\n\nprojects 372 ai 533 generativeai 467 llama 48 homebrewllms 49 llms 442 llm 52\n\nNext: Weeknotes: Llama 3, AI for Data Journalism, llm-evals and datasette-\nsecrets\n\nPrevious: AI for Data Journalism: demonstrating what we can do with this stuff\nright now\n\n  * Source code\n  * \u00a9\n  * 2002\n  * 2003\n  * 2004\n  * 2005\n  * 2006\n  * 2007\n  * 2008\n  * 2009\n  * 2010\n  * 2011\n  * 2012\n  * 2013\n  * 2014\n  * 2015\n  * 2016\n  * 2017\n  * 2018\n  * 2019\n  * 2020\n  * 2021\n  * 2022\n  * 2023\n  * 2024\n\n", "frontpage": false}
