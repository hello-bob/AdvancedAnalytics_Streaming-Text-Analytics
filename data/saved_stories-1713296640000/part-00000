{"aid": "40051737", "title": "Stupid tricks with io_uring: a server that does zero syscalls per reques (2021)", "url": "https://wjwh.eu/posts/2021-10-01-no-syscall-server-iouring.html", "domain": "wjwh.eu", "votes": 1, "user": "fanf2", "posted_at": "2024-04-16 13:42:03", "comments": 0, "source_title": "The blog of wjwh - Stupid tricks with io_uring: a server that does zero syscalls per request", "source_text": "The blog of wjwh - Stupid tricks with io_uring: a server that does zero\nsyscalls per request\n\nMy blog\n\nHome About Archive\n\n# Stupid tricks with io_uring: a server that does zero syscalls per request\n\nPosted on October 1, 2021 by wjwh\n\nThe new io_uring subsystem for Linux is pretty cool and has a lot of momentum\nbehind it. In a nutshell, it allows for asynchronous system calls in an \u201cevent\nloop\u201d-like fashion. One of the design goals is to reduce the amount of context\nswitches between user- and kernel-space for I/O intensive programs and this\ngot me thinking about exactly how much we can reduce these context switches.\nIt turns out that you can get all the way to zero syscalls per connection (as\nmeasured by strace), provided you don\u2019t care about CPU use and don\u2019t allocate\nany memory. The result is very much a toy, but I enjoyed building it so that\u2019s\nenough for me.\n\n## Recap of io_uring\n\nThis post will probably not make a lot of sense to you if you don\u2019t know what\nio_uring is or how it works, but a complete explanation is outside the scope\nof this blog post. For a very complete overview of the io_uring subsystem and\nits capabilities, check out the excellent Lord of the io_uring website. If you\nare just interested in the TL;DR version: io_uring is a new(-ish) subsystem in\nLinux that allows for asynchronous system calls to be submitted to the kernel,\npotentially in big batches. Submitting many syscalls in a batch saves on\ncontext switching overhead, since only one synchronous system call has to be\nmade to submit the requests for many asynchronous system calls. After each\nsyscall is completed, the kernel will place the result(s) in a special bit of\nmemory (known as the \u201cCompletion Queue\u201d) that is shared between the userspace\nprogram and the kernel. This means that the program can check for any\ncompleted syscalls without calling into the kernel and this allows for further\nsavings on context switching overhead.\n\n## Basic server\n\nKeeping in mind Postel\u2019s law, this will be the simplest possible server in\nregards to parsing user requests. To be precise, we will accept any and all\nrequests, no matter if they are well-formed or not (even empty requests!).\nEvery single request will be answered with a standardized response wishing the\nuser a nice day, since that is a nice thing to want for our users. While\nmalloc() and free() are not technically system calls, they do have the\npotential to cause the brk() and/or sbrk() system calls as part of their\noperation. Therefore, to make sure we don\u2019t accidentally do any syscalls,\nwe\u2019ll preallocate everything and do no memory management whatsoever. Oh, the\nsacrifices we make for zero syscalls! Anyway, the basic loop for such a server\nis fairly straightforward:\n\n  * Wait for new connections to come in with accept(), then for each connection:\n\n    1. Write the message to the new connection.\n    2. Close the connection.\n\nA \u201cnormal\u201d server anno 2021 would typically either run the accept operation in\na \u201cmain\u201d thread and handle each connection with a separate thread (possibly\nfrom a thread pool for efficiency), or it would run an event loop-based system\nin a single thread and monitor all the sockets involved with epoll() or\nsomething similar. The io_uring based version is also an event loop system,\nbut instead of waiting for socket readiness we\u2019ll submit all the operations\nasynchronously and wait until the kernel is done with them to read the result\nof the syscall.\n\nTo make the program work, we need to do three basic operations that would\nnormally be handled by syscalls: accepting requests, sending the standard\nresponse to clients and closing the connections. Accepting connections can be\ndone in liburing with the io_uring_prep_accept() function. The io_uring system\nuses a special 64-bit field called userdata that you can pass into a SQE and\nit will be copied unaltered into the corresponding CQE. It\u2019s intended function\nis to provide a way for applications to track data associated with the SQE and\nusually it is a pointer to a struct of some sort. However in this case we\nsimply need a way to distinguish accept() related CQEs from send()/close()\nrelated CQEs. We can do this by passing in a magic number that will only be\npresent on accept() CQEs. After an extremely rigorous selection process I\nsettled on the number 123.\n\n    \n    \n    void add_accept_request(int server_socket, struct sockaddr_in *client_addr, socklen_t *client_addr_len) { struct io_uring_sqe *sqe = io_uring_get_sqe(&ring); io_uring_prep_accept(sqe, server_socket, (struct sockaddr *) client_addr, client_addr_len, 0); // magic number in the userdata to differentiate between accept CQEs // and other types of CQE io_uring_sqe_set_data(sqe, (void*) 123); io_uring_submit(&ring); }\n\nThe write() and close() SQEs could be handled separately, but that is not\nnecessary in this case: io_uring allows for linking SQEs to force them to be\nperformed in a certain order. By setting IOSQE_IO_LINK in the SQE flags, we\ncan be sure that the SQE will be performed before the next SQE submitted. This\nis required because submitting them both at the same time could result in the\nconnection being closed before all the data is sent. The resulting function\nlooks like this:\n\n    \n    \n    void add_write_and_close_requests(int fd) { struct io_uring_sqe *sqe; sqe = io_uring_get_sqe(&ring); io_uring_prep_write(sqe, fd, standard_response, strlen(standard_response), 0); // make sure the write is complete before doing the close(): sqe->flags |= IOSQE_IO_LINK; sqe = io_uring_get_sqe(&ring); io_uring_prep_close(sqe, fd); io_uring_submit(&ring); }\n\nYou can see the general pattern for creating a SQE in liburing: first you\nrequest a new (empty) SQE with io_uring_get_sqe(), then you call one of the\nmany io_uring_prep_XYZ() functions on it to set the fields correctly for the\ntype of operation you want and optionally you can set some flags on the SQE to\nalter how io_uring will handle it. Note that for the write() and close() SQEs\nno userdata is set, since we don\u2019t do anything with the results of the\nactions.\n\n## Removing syscalls for submission events\n\nOne of the parts of io_uring that I find the most interesting is the\npossibility to have the kernel poll for new SQEs instead of the user having to\ninform the kernel via io_uring_enter() (wrapped by io_uring_submit() in\nliburing). By using io_uring_queue_init_params to pass in flags to io_uring\ninitialisation and setting the IORING_SETUP_SQPOLL flag, the kernel will keep\npolling for up to params.sq_thread_idle milliseconds after the last SQE was\nsubmitted. Any SQEs you put into the SQE ring will automatically be picked up\nwithout any system calls required. After sq_thread_idle milliseconds have\npassed, the polling kernel thread will stop and you will need to call\nio_uring_enter() again to start it back up. When using liburing,\nio_uring_submit() will automagically keep track of whether the kernel thread\nis still alive and skip the syscall if it is not required.\n\nClearly, this is a very powerful tool for any program that wishes to submit as\nfew syscalls as possible. One downside is that you need elevated privileges to\ncreate an io_uring with this flag set. While technically having CAP_SYS_NICE\nis sufficient, let\u2019s just mandate root privileges for now. Also, the\ndocumentation states:\n\n    \n    \n    The kernel\u2019s poller thread can take up a lot of CPU.\n\nFor this particular case, I don\u2019t think I\u2019ll worry a lot about that. I have\nplenty of cores available and don\u2019t plan on running the server for more than 5\nminutes at a time anyway. So, we can add the following to the setup of the\nio_uring to enable submission queue polling by the kernel:\n\n    \n    \n    struct io_uring_params params; if (geteuid()) { fprintf(stderr, \"You need root privileges to run this program.\\n\"); return 1; } // malloc(123); memset(&params, 0, sizeof(params)); params.flags |= IORING_SETUP_SQPOLL; params.sq_thread_idle = 120000; // 2 minutes in ms int ret = io_uring_queue_init_params(ENTRIES, &ring, &params);\n\nAs long as we submit at least one SQE every two minutes, the kernel thread\nwill never die and so we\u2019ll never incur a syscall when we call\nio_uring_submit(). Awesome!\n\n## Removing syscalls for completion events\n\nNow that we can submit SQEs without any syscalls, surely we are already done?\nBut no! It turns out that the normal way of polling for a CQE with liburing\n(with io_uring_wait_cqe()) will block if there are no CQEs ready to be\nconsumed. So unless we have a super busy application that will always have a\nCQE ready to be consumed, eventually the application will issue a blocking\nsyscall to wait until a CQE is ready. Luckily, there is also an\nio_uring_peek_cqe() function available that will never wait. If no CQE is\navailable, it will return a non-zero integer, and conversely it will return\nzero if a CQE was waiting. We can use this fact to write the main loop of the\nserver to loop while checking if there are any CQEs available.\n\n    \n    \n    while(1){ peek_result = io_uring_peek_cqe(&ring,&cqe); // peek_result is 0 if a cqe was available and -errno otherwise if(!peek_result){ if (cqe->user_data == 123) { // magic number for an accept CQE add_write_and_close_requests(cqe->res); add_accept_request(server_socket, &client_addr, &client_addr_len); } else { // no action required } io_uring_cqe_seen(&ring, cqe); } }\n\nLooping like this and continuously checking the state of the CQE ring will\ntake up even more CPU of course, but that is pretty much a sailed ship after\nwhat we did with the kernel-side SQE polling.\n\n## Conclusion\n\nWhen running this application under strace to monitor the syscalls being\ngenerated, it shows no syscalls being generated after initial startup, no\nmatter how often you connect to it with nc or curl. So the answer is yes, you\ncan write a server that does no syscalls for handling requests, if you are\nwilling to forego memory allocations, don\u2019t count asynchronous syscalls as\nbeing \u201creal\u201d syscalls and are also willing to use 100% of one CPU for the\nprocess and some unspecified amount of CPU for the kernel thread. Clearly this\nis not really a viable approach for production-quality applications, but it\nwas fun to see how low we could go. Still though, it is interesting that it is\npossible at all and in general I think io_uring has a lot of potential. One of\nthe things I would love to use it in would be an nchan-like websocket pubsub\nfanout server, because that kind of application would lend itself well to\nbatching all the send() calls. I also think that many of the current event-\nloop-based language runtimes will eventually evolve to incorporate\nasynchronous syscalls \u201cnatively\u201d instead of the current epoll-based systems.\nInteresting times!\n\nThe complete program can be found as a gist here.\n\nSite proudly generated by Hakyll\n\n", "frontpage": false}
