{"aid": "40155153", "title": "Some Shortcomings of CSRankings", "url": "https://data-mining.philippe-fournier-viger.com/some-shortcomings-of-csrankings/", "domain": "philippe-fournier-viger.com", "votes": 1, "user": "greghn", "posted_at": "2024-04-25 09:05:20", "comments": 0, "source_title": "Some shortcomings of CSRankings", "source_text": "Some shortcomings of CSRankings | The Data Blog\n\nThe Data Blog\n\nA blog about data mining, data science, machine learning and big data, by\nPhilippe Fournier-Viger\n\nSkip to content\n\n  * Home\n  * About the author\n\n\u2190 An Online Demo of the Eclat Algorithm\n\nCall for tutorials at BESC 2023 \u2192\n\n# Some shortcomings of CSRankings\n\nPosted on 2023-06-05 by Philippe Fournier-Viger\n\nCSRankings is a popular website that provides a ranking of computer science\ndepartments around the world. The website can be found at:\nhttps://csrankings.org/ In this blog post, I will talk about this ranking and\nsome of its shortcomings. Of course, no ranking is perfect, and what I write\nbelow is just my personal opinion.\n\n## What is CSRankings?\n\nFirst, it needs to be said that there exist many rankings to evaluate computer\nscience departments, and they use various criteria based on teaching, research\nor a combination of both. CSRankings is purely focused on research. It\nevaluates a department based on its research output in terms of articles in\nthe very top level conferences. A good thing about that ranking is that the\nranking algorithm is completely transparent and well explained: (1) it uses\npublic data, and (2) the code to assign a score to each department is\nexplained and is also open-source.\n\nThe ranking looks like this:\n\n## Shortcomings of CSRankings\n\nNow, let\u2019s talk about what I see are the main shortcomings of CSRankings:\n\n1) The ranking ignores journal papers to focus only on conference papers but\nin several countries, journals are deemed more important than conference\npublications. Thus, there is a bias there.\n\n2) It is a US-centric ranking. As explained in the FAQ of CSRankings, a\nconference is only included in this ranking if at least 50 R1 US universities\nhave published in it during the last 10 years.\n\n3) Some sub-fields of computer science are not well-represented and some\nconferences appear to be easier to publish than others. For example, from my\nperspective, I am a data mining researcher and KDD is arguably the top\nconference in my field. KDD is highly competitive with thousands of\nsubmissions, and generally an acceptance rate around 10-20%, but it is\ndeactivated by default from CSRankings:\n\n. But I also notice that most top data mining conferences are not included\neither like ICDM, CIKM etc. ICDE is another data mining related conference\nwith an acceptance rate of about 19%. It is there but it is also deactivated\nby default:\n\nI find this quite surprising because for other fields, some conferences that\nare arguably easier to publish than ICDE and KDD are included in the ranking.\nFor example, ICDE and KDD typically have acceptance rate in the range of\n10-20%, while for robotics the IROS and ICRA conferences are included in the\nCSRankings, but they have a much higher acceptance rate. IROS has an\nacceptance rate of around 49 % and ICRA an acceptance rate of around 43 % as\ncan be seen below (source https://staff.aist.go.jp/k.koide/acceptance-\nrate.html ):\n\nThus, it seems to me that the ranking is unequal for different fields. It\nseems that some fields have some conferences that are much easier to publish\nthat are included in the ranking than some other fields. I think that this\nproblem emerges due to the design decision of CSRankings to only include a\nabout 3 conferences for each research areas and to define the research areas\nbased on ACM Special Interest Groups.\n\n4) It is a conservative ranking that focus on big conferences for popular\nresearch areas. It does not encourage researchers to publish in new\nconferences but rather to focus on well-established big conferences, as\neverything else does not count. It also does not encourage publishing in\nsmaller conferences that might be more relevant. For example, while doing my\nPhD, I was working on intelligent tutoring systems, and the two top\nconferences in that field are Intelligent Tutoring Systems (ITS) and\nArtificial Intelligence In Education (AIED). These conferences are rather\nsmall and specific, so they are totally ignored from CSRanking. But those are\nthe conferences that matters in that field.\n\n5) By design, the ranking focuses only on research. But some other important\naspects like teaching may be relevant to some people. For example, an\nundergraduate student may be interested in other aspects such as how likely he\nwill find a job after graduating. In that case, other rankings should be used.\n\n## Conclusion\n\nThat was just a quick blog post to point out what I see as some shortcomings\nof the CSRankings. Of course, no ranking is perfect, and CSRankings still\nprovide some useful information and can be useful. But in my opinion, I think\nit has some limitations. It seems to me that not all fields are equal in this\nranking.\n\nWhat do you think? Post your opinion in the comment section below.\n\n\u2014 Philippe Fournier-Viger is a full professor working in China and founder of\nthe SPMF open source data mining software.\n\nThis entry was posted in Academia and tagged computer science, csrankings,\nranking. Bookmark the permalink.\n\n\u2190 An Online Demo of the Eclat Algorithm\n\nCall for tutorials at BESC 2023 \u2192\n\n### Leave a Reply Cancel reply\n\n  * ### Archives\n\n  * ### Categories\n\n    * Academia (82)\n    * artificial intelligence (34)\n    * Big data (80)\n    * Bioinformatics (3)\n    * cfp (10)\n    * Chinese posts (1)\n    * Conference (70)\n    * Data Mining (182)\n    * Data science (102)\n    * Database (1)\n    * General (42)\n    * Industry (2)\n    * Interview (1)\n    * Java (11)\n    * Latex (10)\n    * Machine Learning (20)\n    * Mathematics (2)\n    * open-source (37)\n    * Other (3)\n    * Pattern Mining (84)\n    * Plagiarism (1)\n    * Programming (17)\n    * Research (109)\n    * spmf (53)\n    * Time series (3)\n    * Uncategorized (22)\n    * Utility Mining (22)\n    * Video (19)\n    * Website (3)\n  * ### Recent Posts\n\n    * SPMF: bug fix about screen resolution\n    * SPMF 2.60 is released!\n    * How to download an offline copy of the SPMF documentation?\n    * Some interesting statistics about SPMF\n    * Sneak peak at the new user interface of SPMF (part 3)\n    * ChatGPT, LLMs and homework\n    * When ChatGPT is used to write papers...\n    * Sneak peak at the new user interface of SPMF (part 2)\n    * Sneak peak at the new user interface of SPMF (part 1)\n    * UDML 2024 Accepted papers\n  * ### Recent Comments\n\n    * K. P. Birla on About the author\n    * An Overview of Pattern Mining Techniques | The Data Blog on An Introduction to Data Mining\n    * Key Papers about Episode Mining | The Data Blog on An introduction to periodic pattern mining\n    * Dr J Gangadhar Naik on How to improve the quality of your research papers?\n    * Philippe Fournier-Viger on About the author\n  * ### Tag cloud\n\n    * academia\n    * ai\n    * algorithm\n    * apriori\n    * article\n    * articles\n    * artificial intelligence\n    * association rule\n    * big data\n    * cfp\n    * china\n    * conference\n    * data\n    * data mining\n    * data science\n    * episode\n    * graph\n    * high utility itemset mining\n    * icdm\n    * itemset\n    * itemset mining\n    * java\n    * journal\n    * latex\n    * machine learning\n    * open-source\n    * open source\n    * pakdd\n    * paper\n    * papers\n    * pattern mining\n    * periodic pattern\n    * phd\n    * Research\n    * researcher\n    * reviewer\n    * sequence\n    * sequential pattern\n    * software\n    * spmf\n    * udml\n    * utility mining\n    * video\n    * workshop\n    * writing\n\n  * ### Number of visitors:\n\n2,280,235\n\nThe Data Blog\n\nProudly powered by WordPress.\n\n", "frontpage": false}
