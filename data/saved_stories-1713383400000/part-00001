{"aid": "40064567", "title": "Diffusion Models for Video Generation", "url": "https://lilianweng.github.io/posts/2024-04-12-diffusion-video/", "domain": "lilianweng.github.io", "votes": 1, "user": "alexmolas", "posted_at": "2024-04-17 13:49:36", "comments": 0, "source_title": "Diffusion Models for Video Generation", "source_text": "Diffusion Models for Video Generation | Lil'Log\n\n# Diffusion Models for Video Generation\n\nDate: April 12, 2024 | Estimated Reading Time: 20 min | Author: Lilian Weng\n\nDiffusion models have demonstrated strong results on image synthesis in past\nyears. Now the research community has started working on a harder task\u2014using\nit for video generation. The task itself is a superset of the image case,\nsince an image is a video of 1 frame, and it is much more challenging because:\n\n  1. It has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\n  2. In comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\n\n> \ud83e\udd51 Required Pre-read: Please make sure you have read the previous blog on\n> \u201cWhat are Diffusion Models?\u201d for image generation before continue here.\n\n# Video Generation Modeling from Scratch#\n\nFirst let\u2019s review approaches for designing and training diffusion video\nmodels from scratch, meaning that we do not rely on pre-trained image\ngenerators.\n\n## Parameterization & Sampling Basics#\n\nHere we use a slightly different variable definition from the previous post,\nbut the math stays the same. Let be a data point sampled from the real data\ndistribution. Now we are adding Gaussian noise in small amount in time,\ncreating a sequence of noisy variations of , denoted as , with increasing\namount of noise as increases and the last . The noise-adding forward process\nis a Gaussian process. Let define a differentiable noise schedule of the\nGaussian process:\n\nTo represent for , we have:\n\nLet the log signal-to-noise-ratio be , we can represent the DDIM (Song et al.\n2020) update as:\n\nThere is a special -prediction () parameterization, proposed by Salimans & Ho\n(2022). It has been shown to be helpful for avoiding color shift in video\ngeneration compared to -parameterization.\n\nThe -parameterization is derived with a trick in the angular coordinate.\nFirst, we define and thus we have . The velocity of can be written as:\n\nThen we can infer,\n\nThe DDIM update rule is updated accordingly,\n\nFig. 1. Visualizing how the diffusion update step works in the angular\ncoordinate, where DDIM evolves by moving it along the direction. (Image\nsource: Salimans & Ho, 2022)\n\nThe -parameterization for the model is to predict .\n\nIn the case of video generation, we need the diffusion model to run multiple\nsteps of upsampling for extending video length or increasing the frame rate.\nThis requires the capability of sampling a second video conditioned on the\nfirst , , where might be an autoregressive extension of or be the missing\nframes in-between for a video at a low frame rate.\n\nThe sampling of needs to condition on besides its own corresponding noisy\nvariable. Video Diffusion Models (VDM; Ho & Salimans, et al. 2022) proposed\nthe reconstruction guidance method using an adjusted denoising model such that\nthe sampling of can be properly conditioned on :\n\nwhere are reconstructions of provided by the denoising model. And is a\nweighting factor and a large one is found to improve sample quality. Note that\nit is also possible to simultaneously condition on low resolution videos to\nextend samples to be at the high resolution using the same reconstruction\nguidance method.\n\n## Model Architecture: 3D U-Net & DiT#\n\nSimilar to text-to-image diffusion models, U-net and Transformer are still two\ncommon architecture choices. There are a series of diffusion video modeling\npapers from Google based on the U-net architecture and a recent Sora model\nfrom OpenAI leveraged the Transformer architecture.\n\nVDM (Ho & Salimans, et al. 2022) adopts the standard diffusion model setup but\nwith an altered architecture suitable for video modeling. It extends the 2D\nU-net to work for 3D data (Cicek et al. 2016), where each feature map\nrepresents a 4D tensor of frames x height x width x channels. This 3D U-net is\nfactorized over space and time, meaning that each layer only operates on the\nspace or time dimension, but not both:\n\n  * Processing Space:\n\n    * Each old 2D convolution layer as in the 2D U-net is extended to be space-only 3D convolution; precisely, 3x3 convolutions become 1x3x3 convolutions.\n    * Each spatial attention block remains as attention over space, where the first axis (frames) is treated as batch dimension.\n  * Processing Time:\n\n    * A temporal attention block is added after each spatial attention block. It performs attention over the first axis (frames) and treats spatial axes as the batch dimension. The relative position embedding is used for tracking the order of frames. The temporal attention block is important for the model to capture good temporal coherence.\n\nFig. 2. The 3D U-net architecture. The noisy video , conditioning information\nand the log signal-to-noise ratio (log-SNR) are inputs to the network. The\nchannel multipliers represent the channel counts across layers. (Image source:\nSalimans & Ho, 2022)\n\nImagen Video (Ho, et al. 2022) is constructed on a cascade of diffusion models\nto enhance the video generation quality and upgrades to output 1280x768 videos\nat 24 fps. The Imagen Video architecture consists of the following components,\ncounting 7 diffusion models in total.\n\n  * A frozen T5 text encoder to provide text embedding as the conditioning input.\n  * A base video diffusion model.\n  * A cascade of interleaved spatial and temporal super-resolution diffusion models, including 3 TSR (Temporal Super-Resolution) and 3 SSR (Spatial Super-Resolution) components.\n\nFig. 3. The cascaded sampling pipeline in Imagen Video. In practice, the text\nembeddings are injected into all components, not just the base model. (Image\nsource: Ho et al. 2022)\n\nThe base denoising models performs spatial operations over all the frames with\nshared parameters simultaneously and then the temporal layer mixes activations\nacross frames to better capture temporal coherence, which is found to work\nbetter than frame-autoregressive approaches.\n\nFig. 4. The architecture of one space-time separable block in the Imagen Video\ndiffusion model. (Image source: Ho et al. 2022)\n\nBoth SSR and TSR models condition on the upsampled inputs concatenated with\nnoisy data channel-wise. SSR upsamples by bilinear resizing, while TSR\nupsamples by repeating the frames or filling in blank frames.\n\nImagen Video also applies progressive distillation to speed up sampling and\neach distillation iteration can reduce the required sampling steps by half.\nTheir experiments were able to distill all 7 video diffusion models down to\njust 8 sampling steps per model without any noticeable loss in perceptual\nquality.\n\nTo achieve better scaling efforts, Sora (Brooks et al. 2024) leverages DiT\n(Diffusion Transformer) architecture that operates on spacetime patches of\nvideo and image latent codes. Visual input is represented as a sequence of\nspacetime patches which act as Transformer input tokens.\n\nFig. 5. Sora is a diffusion transformer model. (Image source: Brooks et al.\n2024)\n\n# Adapting Image Models to Generate Videos#\n\nAnother prominent approach for diffusion video modeling is to \u201cinflate\u201d a pre-\ntrained image-to-text diffusion model by inserting temporal layers and then we\ncan choose to only fine-tune new layers on video data, or avoid extra training\nat all. The prior knowledge of text-image pairs is inherited by the new model\nand thus it can help alleviate the requirement on text-video pair data.\n\n## Fine-tuning on Video Data#\n\nMake-A-Video (Singer et al. 2022) extends a pre-trained diffusion image model\nwith a temporal dimension, consisting of three key components:\n\n  1. A base text-to-image model trained on text-image pair data.\n  2. Spatiotemporal convolution and attention layers to extend the network to cover temporal dimension.\n  3. A frame interpolation network for high frame rate generation\n\nFig. 6. The illustration of Make-A-Video pipeline. (Image source: Singer et\nal. 2022)\n\nThe final video inference scheme can be formulated as:\n\nwhere:\n\n  * is the input text.\n  * is the BPE-encoded text.\n  * is the CLIP text encoder, .\n  * is the prior, generating image embedding given text embedding and BPE encoded text : . This part is trained on text-image pair data and not fine-tuned on video data.\n  * is the spatiotemporal decoder that generates a series of 16 frames, where each frame is a low-resolution 64x64 RGB image .\n  * is the frame interpolation network, increasing the effective frame rate by interpolating between generated frames. This is a fine-tuned model for the task of predicting masked frames for video upsampling.\n  * are the spatial and spatiotemporal super-resolution models, increasing the image resolution to 256x256 and 768x768, respectively.\n  * is the final generated video.\n\nSpatiotemporal SR layers contain pseudo-3D convo layers and pseudo-3D\nattention layers:\n\n  * Pseudo-3D convo layer : Each spatial 2D convo layer (initialized from the pre-training image model) is followed by a temporal 1D layer (initialized as the identity function). Conceptually, the convo 2D layer first generates multiple frames and then frames are reshaped to be a video clip.\n  * Pseudo-3D attention layer: Following each (pre-trained) spatial attention layer, a temporal attention layer is stacked and used to approximate a full spatiotemporal attention layer.\n\nFig. 7. How pseudo-3D convolution (left) and attention (right) layers work.\n(Image source: Singer et al. 2022)\n\nThey can be represented as:\n\nwhere an input tensor (corresponding to batch size, channels, frames, height\nand weight); and swaps between temporal and spatial dimensions; is a matrix\noperator to convert to be and reverses that process.\n\nDuring training, different components of Make-A-Video pipeline are trained\nindependently.\n\n  1. Decoder , prior and two super-resolution components are first trained on images alone, without paired text.\n  2. Next the new temporal layers are added, initialized as identity function, and then fine-tuned on unlabeled video data.\n\nTune-A-Video (Wu et al. 2023) inflates a pre-trained image diffusion model to\nenable one-shot video tuning: Given a video containing frames, , paired with a\ndescriptive prompt , the task is to generate a new video based on a slightly\nedited & related text prompt . For example, = \"A man is skiing\" can be\nextended to =\"Spiderman is skiing on the beach\". Tune-A-Video is meant to be\nused for object editing, background change, and style transfer.\n\nBesides inflating the 2D convo layer, the U-Net architecture of Tune-A-Video\nincorporates the ST-Attention (spatiotemporal attention) block to capture\ntemporal consistency by querying relevant positions in previous frames. Given\nlatent features of frame , previous frames and the first frame are projected\nto query , key and value , the ST-attention is defined as:\n\nFig. 8. The Tune-A-Video architecture overview. It first runs a light-weighted\nfine-tuning stage on a single video before the sampling stage. Note that the\nentire temporal self-attention (T-Attn) layers get fine-tuned because they are\nnewly added, but only query projections in ST-Attn and Cross-Attn are updated\nduring fine-tuning to preserve prior text-to-image knowledge. ST-Attn improves\nspatial-temporal consistency, Cross-Attn refines text-video alignment. (Image\nsource: Wu et al. 2023)\n\nGen-1 model (Esser et al. 2023) by Runway targets the task of editing a given\nvideo according to text inputs. It decomposes the consideration of structure\nand content of a video for generation conditioning. However, to do a clear\ndecomposition of these two aspects is not easy.\n\n  * Content refers to appearance and semantics of the video, that is sampled from the text for conditional editing. CLIP embedding of the frame is a good representation of content, and stays largely orthogonal to structure traits.\n  * Structure depicts greometry and dynamics, including shapes, locations, temporal changes of objects, and is sampled from the input video. Depth estimation or other task-specific side information (e.g. human body pose or face landmarks for human video synthesis) can be used.\n\nThe architecture changes in Gen-1 are quite standard, i.e. adding 1D temporal\nconvo layer after each 2D spatial convo layer in its residual blocks and\nadding 1D temporal attention block after each 2D spatial attention block in\nits attention blocks. During training, the structure variable is concatenated\nwith the diffusion latent variable , where the content variable is provided in\nthe cross-attention layer. At inference time, the clip embedding is converted\nvia a prior to convert CLIP text embedding to be CLIP image embedding.\n\nFig. 9. The overview of the Gen-1 model training pipeline. (Image source:\nEsser et al. 2023)\n\nVideo LDM (Blattmann et al. 2023) trains a LDM (Latent diffusion models) image\ngenerator first. Then the model is fine-tuned to produce videos with a\ntemporal dimension added. The fine-tuning only applies to these newly added\ntemporal layers on encoded image sequences. The temporal layers in the Video\nLDM (See Fig. 10) are interleaved with existing spatial layers which stays\nfrozen during fine-tuning. That\u2019s being said, we only fine-tune the new\nparameters but not the pre-trained image backbone model parameters . The\npipeline of Video LDM first generates key frames at low fps and then processes\nthrough 2 steps of latent frame interpolations to increase fps.\n\nThe input sequence of length is interpreted as a batch of images (i.e. ) for\nthe base image model and then gets reshaped into video format for temporal\nlayers. There is a skip connection leads to a combination of temporal layer\noutput and the spatial output via a learned merging parameter . There are two\ntypes of temporal mixing layers implemented in practice: (1) temporal\nattention and (2) residual blocks based on 3D convolutions.\n\nFig. 10. A pre-training LDM for image synthesis is extended to be a video\ngenerator. are batch size, sequence length, channels, height and width,\nrespectively. is an optional conditioning/context frame. (Image source:\nBlattmann et al. 2023)\n\nHowever, there is a remaining issue with LDM\u2019s pretrainined autoencoder which\nonly sees images never videos. Naively using that for video generation can\ncause flickering artifacts without good temporal coherence. So Video LDM adds\nadditional temporal layers into the decoder and fine-tuned on video data with\na patch-wise temporal discriminator built from 3D convolutions, while the\nencoder remains unchanged so that we still can reuse the pretrained LDM.\nDuring temporal decoder fine-tuning, the frozen encoder processes each frame\nin the video independently, and enforce temporally coherent reconstructions\nacross frames with a video-aware discriminator.\n\nFig. 11. The training pipeline of autoencoder in video latent diffusion\nmodels. The decoder is fine-tuned to have temporal coherency with a new\nacross-frame discriminator while the encoder stays frozen. (Image source:\nBlattmann et al. 2023)\n\nSimilar to Video LDM, the architecture design of Stable Video Diffusion (SVD;\nBlattmann et al. 2023) is also based on LDM with temporal layers inserted\nafter every spatial convolution and attention layer, but SVD fine-tunes the\nentire model. There are three stages for training video LDMs:\n\n  1. Text-to-image pretraining is important and helps improve both quality and prompt following.\n  2. Video pretraining is beneficial to be separated and should ideally occur on a larger scale, curated dataset.\n  3. High-quality video finetuning works with a smaller, pre-captioned video of high visual fidelity.\n\nSVD specially emphasizes the critical role of dataset curation in model\nperformance. They applied a cut detection pipeline to get more cuts per video\nand then applied three different captioner models: (1) CoCa for mid-frame, (2)\nV-BLIP for a video caption, and (3) LLM based captioning based on previous two\ncaptions. Then they were able to continue to improve video datasets, by\nremoving clips with less motion (filtered by low optical flow scores\ncalculated at 2 fps), excessive text presence (apply optical character\nrecognition to identify videos with lots of text), or generally low aesthetic\nvalue (annotate the first, middle, and last frames of each clip with CLIP\nembeddings and calculate aesthetics scores & text-image similarities). The\nexperiments showed that a filtered, higher quality dataset leads to better\nmodel quality, even when this dataset is much smaller.\n\nThe key challenge of generating distant key frames first and then adding\ninterpolation with temporal super-resolution is how to maintain high-quality\ntemporal consistency. Lumiere (Bar-Tal et al. 2024) instead adopts a space-\ntime U-Net (STUNet) architecture that generates the entire temporal duration\nof the video at once through a single pass, removing the dependency on TSR\n(temporal super-resolution) components. STUNet downsamples the video in both\ntime and space dimensions and thus expensive computation happens in a compact\ntime-space latent space.\n\nFig. 12. Lumiere removes TSR (temporal super-resolution) models. The inflated\nSSR network can operate only on short segments of the video due to memory\nconstraints and thus SSR models operate on a set of shorter but overlapped\nvideo snippets. (Image source: Bar-Tal et al. 2024)\n\nSTUNet inflates a pretrained text-to-image U-net to be able to downsample and\nupsample videos at both time and space dimensions. Convo-based blocks consist\nof pre-trained text-to-image layers, followed by a factorized space-time\nconvolution. And attention-based blocks at the coarsest U-Net level contains\nthe pre-trained text-to-image, followed by temporal attention. Further\ntraining only happens with the newly added layers.\n\nFig. 13. The architecture of (a) Space-Time U-Net (STUNet), (b) the\nconvolution-based block, and (c) the attention-based block. (Image source:\nBar-Tal et al. 2024)\n\n## Training-Free Adaptation#\n\nSomehow surprisingly, it is possible to adapt a pre-trained text-to-image\nmodel to output videos without any training \ud83e\udd2f.\n\nIf we naively sample a sequence of latent codes at random and then construct a\nvideo of decoded corresponding images, there is no guarantee in the\nconsistency in objects and semantics in time. Text2Video-Zero (Khachatryan et\nal. 2023) enables zero-shot, training-free video generation by enhancing a\npre-trained image diffusion model with two key mechanisms for temporal\nconsistency:\n\n  1. Sampling the sequence of latent codes with motion dynamics to keep the global scene and the background time consistent;\n  2. Reprogramming frame-level self-attention using a new cross-frame attention of each frame on the first frame, to preserve the context, appearance, and identity of the foreground object.\n\nFig. 14. An overview of the Text2Video-Zero pipeline. (Image source:\nKhachatryan et al. 2023)\n\nThe process of sampling a sequence of latent variables, , with motion\ninformation is described as follows:\n\n  1. Define a direction for controlling the global scene and camera motion; by default, we set . Also define a hyperparameter controlling the amount of global motion.\n  2. First sample the latent code of the first frame at random, ;\n  3. Perform DDIM backward update steps using the pre-trained image diffusion model, e.g. Stable Diffusion (SD) model in the paper, and obtain the corresponding latent code where .\n  4. For each frame in the latent code sequence, we apply corresponding motion translation with a warping operation defined by to obtain .\n  5. Finally apply DDIM forward steps to all to obtain .\n\nBesides, Text2Video-Zero replaces the self-attention layer in a pre-trained SD\nmodel with a new cross-frame attention mechanism with reference to the first\nframe. The motivation is to preserve the information about the foreground\nobject\u2019s appearance, shape, and identity throughout the generated video.\n\nOptionally, the background mask can be used to further smoothen and improve\nbackground consistency. Let\u2019s say, we obtain a corresponding foreground mask\nfor the -th frame using some existing method, and background smoothing merges\nthe actual and the warped latent code at the diffusion step , w.r.t. the\nbackground matrix:\n\nwhere is the actual latent code and is the warped latent code on the\nbackground; is a hyperparameter and the papers set in the experiments.\n\nText2video-zero can be combined with ControlNet where the ControlNet\npretrained copy branch is applied per frame on each for in each diffusion\ntime-step and add the ControlNet branch outputs to the skip-connections of the\nmain U-net.\n\nControlVideo (Zhang et al. 2023) aims to generate videos conditioned on text\nprompt and a motion sequence (e.g., depth or edge maps), . It is adapted from\nControlNet with three new mechanisms added:\n\n  1. Cross-frame attention: Adds fully cross-frame interaction in self-attention modules. It introduces interactions between all the frames, by mapping the latent frames at all the time steps into matrices, different from Text2Video-zero which only configures all the frames to attend to the first frame.\n  2. Interleaved-frame smoother is a mechanism to employ frame interpolation on alternated frames to reduce the flickering effect. At each time step , the smoother interpolates the even or odd frames to smooth their corresponding three-frame clips. Note that the number of frames decreases in time after smoothing steps.\n  3. Hierarchical sampler utilizes a hierarchical sampler to enable long videos with time consistency under memory constraints. A long video is split into multiple short clips and each has a key frame selected. The model pre-generates these keyframes with full cross-frame attention for long-term coherency and each corresponding short clip is synthesized sequentially conditioned on the keyframes.\n\nFig. 15. The overview of ControlVideo. (Image source: Zhang et al. 2023)\n\n# Citation#\n\nCited as:\n\n> Weng, Lilian. (Apr 2024). Diffusion Models Video Generation. Lil\u2019Log.\n> https://lilianweng.github.io/posts/2024-04-12-diffusion-video/.\n\nOr\n\n    \n    \n    @article{weng2024video, title = \"Diffusion Models Video Generation.\", author = \"Weng, Lilian\", journal = \"lilianweng.github.io\", year = \"2024\", month = \"Apr\", url = \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\" }\n\n# References#\n\n[1] Cicek et al. 2016. \u201c3D U-Net: Learning Dense Volumetric Segmentation from\nSparse Annotation.\u201d\n\n[2] Ho & Salimans, et al. \u201cVideo Diffusion Models.\u201d 2022 | webpage\n\n[3] Bar-Tal et al. 2024 \u201cLumiere: A Space-Time Diffusion Model for Video\nGeneration.\u201d\n\n[4] Brooks et al. \u201cVideo generation models as world simulators.\u201d OpenAI Blog,\n2024.\n\n[5] Zhang et al. 2023 \u201cControlVideo: Training-free Controllable Text-to-Video\nGeneration.\u201d\n\n[6] Khachatryan et al. 2023 \u201cText2Video-Zero: Text-to-image diffusion models\nare zero-shot video generators.\u201d\n\n[7] Ho, et al. 2022 \u201cImagen Video: High Definition Video Generation with\nDiffusion Models.\u201d\n\n[8] Singer et al. \u201cMake-A-Video: Text-to-Video Generation without Text-Video\nData.\u201d 2022.\n\n[9] Wu et al. \u201cTune-A-Video: One-Shot Tuning of Image Diffusion Models for\nText-to-Video Generation.\u201d ICCV 2023.\n\n[10] Blattmann et al. 2023 \u201cAlign your Latents: High-Resolution Video\nSynthesis with Latent Diffusion Models.\u201d\n\n[11] Blattmann et al. 2023 \u201cStable Video Diffusion: Scaling Latent Video\nDiffusion Models to Large Datasets.\u201d\n\n[12] Esser et al. 2023 \u201cStructure and Content-Guided Video Synthesis with\nDiffusion Models.\u201d\n\n[13] Bar-Tal et al. 2024 \u201cLumiere: A Space-Time Diffusion Model for Video\nGeneration.\u201d\n\n  * generative-model\n  * video-generation\n\n\u00a9 2024 Lil'Log Powered by Hugo & PaperMod\n\n", "frontpage": false}
