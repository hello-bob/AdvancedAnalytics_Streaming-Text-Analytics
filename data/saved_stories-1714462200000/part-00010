{"aid": "40205778", "title": "Safe and Secure AI Innovation Act \u2013 Highlights and FAQ", "url": "https://safesecureai.org/learn", "domain": "safesecureai.org", "votes": 1, "user": "hendrycks", "posted_at": "2024-04-30 00:08:07", "comments": 0, "source_title": "Learn more \u2014 Safe & Secure AI Innovation Act", "source_text": "Learn more \u2014 Safe & Secure AI Innovation Act\n\n0\n\nSkip to Content\n\nSafe & Secure AI Innovation Act\n\nTake Action\n\nSafe & Secure AI Innovation Act\n\nTake Action\n\n# What does SB 1047 do and why is it needed?\n\nHighlights from SB 1047\n\n  * ## Covered models\n\nThis bill only applies to AI models larger than any in existence today that\ncost over $100M to train: \u2022 Models with computing power over 10^26 FLOP\n(floating-point operations per second), or other models with similar\ncapabilities \u2022 The vast majority of startups are not covered by the bill\n\nThe bill only addresses extreme risks from these models: \u2022 Cyberattacks\ncausing over $500 million in damage \u2022 Autonomous crime causing $500M in damage\n\u2022 Creation of a chemical, biological, radiological, or nuclear weapon using AI\n\n  * ## Requirements for developers\n\nDevelopers conduct self-assessments for covered risks and adopt precautions\nfor models assessed to be risky: \u2022 Before training: developers adopt\ncybersecurity precautions, implement shutdown ability, and report safety\nprotocols. \u2022 Before deploying: developers implement reasonable safeguards to\nprevent societal-scale catastrophes. \u2022 After deploying: developers monitor\nsafety incidents and monitor continued compliance. \u2022 Developers of derivative\nor low-risk models only have simple reporting requirements.\n\n  * ## Enforcement\n\nThese provisions are enforced in the following ways: \u2022 Whistleblower\nprotections are provided to employees at frontier labs to ensure that\ninformation on compliance is readily available. \u2022 Civil suits can be brought\nby the Attorney General against developers who cause catastrophic harm or\nthreaten public safety by neglecting the requirements.\n\n  * ## CalCompute\n\nCreates a new CalCompute research cluster to support academic research on AI\nand the startup ecosystem, inspired by federal work on the National Artificial\nIntelligence Research Resource Pilot (NAIRR).\n\n  * ## Open-source advisory council\n\nEstablishes a new advisory council to advocate for and support open-source AI\ndevelopment.\n\n  * ## Transparent pricing\n\nRequires cloud computing providers and frontier model developers to provide\nfair and transparent pricing, to avoid price discrimination impeding\ncompetition.\n\nFull bill text\n\n## FAQs\n\n  * California has become a vibrant hub for artificial intelligence. Universities, startups, and technology companies are using Al to accelerate drug discovery, coordinate wildfire responses, optimize energy consumption, uncover rare minerals that produce clean energy, and enhance creativity. Artificial intelligence has enormous potential to benefit our state and the world. California must act now to ensure that it remains at the forefront of dynamic innovation in AI development.\n\nAt the same time, scientists, engineers, and business leaders at the cutting\nedge of this technology - including the three most cited machine learning\nresearchers of all time - have repeatedly warned policymakers that failure to\ntake appropriate precautions to prevent irresponsible AI development could\nhave severe consequences for public safety and national security. California\nmust ensure that the small handful of companies developing extremely powerful\nAI models \u2014 including companies explicitly aiming to develop \u201cartificial\ngeneral intelligence\u201d \u2014 take reasonable care to prevent their models from\ncausing very serious harms as they continue to produce models of greater and\ngreater power.\n\n  * The bill has two main components:\n\nPromoting responsible AI development: The bill defines a set of hazardous\nimpacts the largest AI models could have, from cyberattacks to the development\nof biological weapons. It requires developers of these AI models to conduct\nself-assessments to ensure these outcomes will be prevented and empowers the\nAttorney General to take action against developers whose technology causes\ncatastrophic harm or threatens public safety.\n\nSupports AI competition and innovation: The bill also promotes ongoing\nacademic research on AI, creating CalCompute, a new state research cluster to\nsupport the AI startup ecosystem. The legislation creates a new open-source\nadvisory council that will be tasked with advocating for and supporting safe\nand secure open-source AI development. The bill also promotes competition by\nrequiring large-scale AI developers to provide fair and transparent pricing.\n\n  * SB 1047 sets out clear standards for developers of AI models trained using a quantity of computing power greater than 10^26 floating-point operations (and other models with similar capabilities). These models, which would cost over $100,000,000 to train, would be substantially more powerful than any model that exists today.\n\nSpecifically, SB 1047 clarifies that developers of these models must invest in\nbasic precautions such as pre-deployment safety testing, red-teaming,\ncybersecurity, safeguards to prevent the misuse of dangerous capabilities, and\npost-deployment monitoring. Furthermore, developers of covered models must\ndisclose the precautionary measures they have taken to the California\nDepartment of Technology. If the developer of an extremely powerful model\ncauses severe harm to Californian citizens by behaving irresponsibly, or if\nthe developer\u2019s negligence poses an imminent threat to public safety, SB 1047\nempowers the Attorney General of California to take appropriate enforcement\naction.\n\nSB 1047 also creates whistleblower protections for employees of frontier\nlaboratories, and requires companies that provide cloud compute for frontier\nmodel training to institute \u201cknow your customer\u201d policies to help prevent the\ndangerous misuse of AI systems.\n\n  * SB 1047 is focused on models capable of causing extraordinary harms that involve the creation of weapons of mass destruction, or AI systems causing $500 million of damage through cyberattacks or autonomously executed criminal activity.\n\nThese are extreme capabilities that models currently do not possess. It\u2019s\npossible that models trained in the next couple of years will have these\ncapabilities, and so developers need to start taking reasonable, narrowly\ntargeted precautions when training the most advanced models.\n\n  * Yes. First, SB 1047\u2019s requirements only apply to a very small set of AI developers, making the largest, most capital-intensive models, today costing in excess of $100 million dollars. The vast majority of AI startups, and all AI application and use-case developers, would have little or no new duties under SB 1047.\n\nSecond, similar safety testing and disclosure is already being done by several\nleading developers under voluntary commitments made to the White House. The\nsame threshold was used in President Biden\u2019s Executive Order for similar\nreasons.\n\n  * No. Developers self-assess whether their models qualify for a \u201climited duty exemption,\u201d and need not wait for approval from any government agency.\n\n  * SB 1047 helps ensure California remains the world leader in AI innovation, by establishing a process to create a public cloud-computing cluster that will conduct research into the safe and secure deployment of large-scale artificial intelligence (AI) models. The model will allow smaller startups, researchers, and community groups to participate in the development of large-scale AI systems, helping to align them with the needs of California communities.\n\nAdditionally, to support the flourishing open-source ecosystem, SB 1047\ncreates a new advisory council to advocate for and support safe and secure\nopen-source AI development.\n\nFinally, in order to ensure that smaller startup developers have equal\nopportunities to larger players, SB 1047 requires cloud-computing companies\nand frontier model developers to provide transparent pricing and avoid price\ndiscrimination.\n\n  * The bill requires developers of the largest AI models, which cost well over $100 million to train today, to conduct self-assessments to protect against potential risks and adopt a set of defined precautions. These steps include:\n\nBefore training a model self-assessed to be risky: developers must adopt\ncybersecurity precautions, implement shutdown ability, follow guidance from\nthe National Institute of Standards and Technology and standard-setting\norganizations, and report safety protocols.\n\nBefore deploying a model self-assessed to be risky: developers must implement\nreasonable safeguards to prevent societal-scale catastrophes.\n\nAfter deploying a model self-assessed to be risky: developers must\nmonitor/report safety incidents and monitor/report continued compliance.\n\n  * A developer can open-source any AI model covered by this bill so long as they conduct safety tests and reasonably determine that it doesn't have specific, highly hazardous capabilities. The author is actively working with developers to ensure these tests are minimally burdensome and continues to welcome input on how innovation can be fostered safely.\n\n## California needs your help\n\nThere are lots of ways to help the bill, whether you\u2019re an citizen,\norganization, or reporter.\n\nAbout\n\nHome Learn More Support for SB 1047 Take Action Contact us\n\nTake action\n\nSB 1047\n\nSafe and secure AI innovation\n\nFull bill text\n\nLearn more\n\nThis website is supported and maintained by the sponsors of SB 1047: Economic\nSecurity Project California Action, Center for AI Safety Action Fund, and\nEncode Justice\n\n", "frontpage": false}
