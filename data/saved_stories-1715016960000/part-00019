{"aid": "40273113", "title": "Need for Speed: LLMs with C#, .NET 8 SSE and Channels, Llama3, and Fireworks.ai", "url": "https://chrlschn.dev/blog/2024/05/need-for-speed-llms-beyond-openai-w-dotnet-sse-channels-llama3-fireworks-ai/", "domain": "chrlschn.dev", "votes": 1, "user": "CharlieDigital", "posted_at": "2024-05-06 10:41:03", "comments": 0, "source_title": "Need for Speed: LLMs Beyond OpenAI with C#, .NET 8 SSE + Channels, Llama3, and Fireworks.ai", "source_text": "@chrlschn - Need for Speed: LLMs Beyond OpenAI with C#, .NET 8 SSE + Channels,\nLlama3, and Fireworks.ai\n\nCharles Chen (@chrlschn)\n\n# Need for Speed: LLMs Beyond OpenAI with C#, .NET 8 SSE + Channels, Llama3,\nand Fireworks.ai\n\nMay 5, 2024\n\n## Summary\n\n  * While OpenAI\u2019s GTP-4 is undoubtedly the champ when it comes to general purpose workloads, its overall throughput (or rather lack thereof) leaves a lot to be desired. This makes it great for \u201coffline\u201d workloads, but can leave it feeling less than suitable for applications where users are expecting more responsiveness and may even preclude some use cases due to the inferior UX.\n  * A recent post on Hackernews by the team at TheFastest.ai highlights just how big this disparity can be in terms of both the model and the platform. In particular, Groq.com (not to be confused with Musk\u2019s Grok) and Fireworks.ai paired with Meta\u2019s Llama 3 70B provides blazing throughput with minimal sacrifice in output compared to GPT-4 for some workloads.\n  * Combining this with C#/.NET 8 System.Threading.Channels and server-sent events (SSE), novel use cases can be built that simply would not work with OpenAI due to the low throughput and high latency.\n\n## Intro\n\nWhile we await GPT-5, few would argue that in May, 2024, OpenAI\u2019s GPT-4 is\nstill the champ when it comes to overall performance as an LLM. Where it comes\nup short is its relatively low throughput and high latency which can make it\nsub-optimal if the UX requires a more interactive experience.\n\nWhat may not be obvious is just how low throughput it is if the use case has a\nneed for speed.\n\nA recent Hackernews thread led me to TheFastest.ai and I was quite intrigued\nby both the high throughput of Meta\u2019s Llama 3 as well as two platforms:\nGroq.com and Fireworks.ai.\n\n(The former is unfortunate in that it is often confused with Musk\u2019s Grok AI).\n\nIn this article, we\u2019ll explore building an app with Fireworks.ai, Meta Llama 3\n8B/70B, .NET 8, System.Threading.Channels, and Server Sent Events (SSE).\n\n> \ud83d\udca1 Full repo is available here: https://github.com/CharlieDigital/dn8-sk-\n> llama3-fireworks. Follow the instructions in the README.md to get it up and\n> running. Start by signing up for a free account and credits at Fireworks.ai\n\n## Measuring the Difference\n\nThe top of the stack is dominated by Llama-3 and Groq with Fireworks.ai\nrounding out the top 5 (we\u2019ll discuss in a bit why teams should probably\nchoose Fireworks)\n\nTTFT = Time to First Token, TPS = Tokens Per Second, Total = Total Time to\nLast Token\n\nIn contrast, OpenAI\u2019s GPT-4 sits near the bottom:\n\nOpenAI GPT-4 nearly 10x slower to the last token\n\n> Note that these benchmarks are subject to change on any given day based on\n> various factors; these screenshots should only be considered relevant at the\n> time they were captured.\n\nAnyone that has worked with OpenAI\u2019s GPT-4 is already aware of just how poor\nthe throughput is. But seeing it measured out like this just highlights how\nbig this gap is. Groq\u2019s Llama-3 70B is nearly 10x higher throughput to final\ntoken than GPT-4!\n\nGiven this, I have found that GPT-4 is really good for cases where: there is\nno need for interactivity (pre-processed), the workload requires the large\ncontext window, or where there is a need for \u201cbenchmark quality\u201d output with\ncomplex prompts and contexts.\n\nBut what if the use case has a different need? What if there is a need for\nspeed?\n\n## Kicking the Tires with Groq and Fireworks\n\nOne of the problems with OpenAI\u2019s poor throughput is that it can subjectively\nmake the user experience worse, even if the content ultimately adds value.\n\nUsing OpenAI\u2019s ChatGPT, it may not be obvious as the SSE masks the fact that a\nchat response could take several seconds to complete. This lack of throughput\nwith GPT-4 is not readily apparent until trying the alternatives.\n\n### Groq.com\n\nGroq is an interesting platform as their claim to fame is custom hardware\nspecifically designed for LLMs dubbed an \u201cLPU\u201d or \u201cLanguage Processing Unit\u201d:\n\n> The LPU is designed to overcome the two LLM bottlenecks: compute density and\n> memory bandwidth. An LPU has greater compute capacity than a GPU and CPU in\n> regards to LLMs. This reduces the amount of time per word calculated,\n> allowing sequences of text to be generated much faster. Additionally,\n> eliminating external memory bottlenecks enables the LPU Inference Engine to\n> deliver orders of magnitude better performance on LLMs compared to GPUs.\n\nAt least on paper, it does seem to be more than just marketing hype and the\nplatform is objectively high throughput.\n\nBut the main problem comes down to their current SaaS offering:\n\nGroq currently lacks an intermediate paid tier with only free or enterprise\nbilling.\n\nThe free tier is only usable for experiments and even then, just barely:\n\n7000 tokens per minute is really easy to consume and it\u2019s using a bucket-type\ntoken refresh algorithm; throttling happens easily during the development\ncycle.\n\nSo even though Groq is quite fast, it\u2019s unusable except for sandboxing and\nthen possibly via their enterprise billing.\n\n### Fireworks.ai\n\nAs of this writing, Fireworks\u2019 Llama-3 70B comes in at 9th overall and is the\nsecond fastest Llama-3 70B:\n\nAt 250ms to the last token, it only trails groq.com\u2019s Llama 3 8B by 139ms\n\nAt 260ms to last token, it is still plenty fast while offering really good LLM\nperformance comparable to something between GPT-3.5 and GPT-4 based on my use\ncases.\n\nWhile Fireworks.ai also lacks an intermediate paid tier, the 600 RPM is usable\nfor small apps and there is no hard token limit:\n\nSign up and you\u2019ll get some free credits; enough for 1 million tokens\ngenerated\n\nFor teams trying to build something fast today, Fireworks.ai is probably the\nbest bet. (No, I\u2019m not getting paid by them)\n\n## A Practical Example with .NET 8, System.Threading.Channels, and Server Sent\nEvents (SSE)\n\nTo take the greatest advantage of this incredible throughput, what is needed\nis a concurrent processing strategy that will allow generation through\nmultiple streams at once and then coalescing into one final output stream to\nthe front-end.\n\nThis is the perfect use case for combining .NET\u2019s System.Threading.Channels\nalong with Server Sent Events (SSE) to fully exploit this throughput and build\na highly responsive generative AI experience.\n\nI\u2019ve previously written about both topics separately:\n\n  * .NET Task Parallel Library vs System.Threading.Channels\n  * Concurrent Processing in .NET 6 with System.Threading.Channels (Bonus: Interval Trees)\n  * Server Sent Events with .NET 7\n\nToday, we\u2019ll put them together and see what kind of interactive experience we\ncan create with .NET 8 channels, Semantic Kernel, and gen AI!\n\nOur sample application will take a list of ingredients on hand plus a target\nprep time and then:\n\n  1. Generate a list of recipes that can be made using the ingredients\n  2. Pick one of the recipes at random\n  3. Generate a list of all of the ingredients required for the recipe\n  4. Generate an intro paragraph for the recipe\n  5. Generate a short writeup about the nutritional information for each ingredient on hand\n  6. Generate a short list of suggested side dishes\n  7. Generate a list of steps\n\nSteps 3-6 can run in parallel, but because we need to pick a recipe first,\nstep 1-2 run first. Then we also need to wait for the full list of ingredients\nbefore starting to generate the steps.\n\n### Concurrent Execution with .NET Channels\n\nThe entrypoint of the API call is a single POST endpoint that will receive the\nrequest:\n\n    \n    \n    // \ud83d\udc47 The main entry point. app.MapPost(\"/generate\", async ( HttpContext context, // From dependency injection RecipeGenerator generator, // From dependency injection RecipeRequest request, // From the body CancellationToken cancellation = default ) => { context.Response.Headers.ContentType = \"text/event-stream\"; await generator.GenerateAsync( request, // Handler that writes the streaming response for each fragment async (Fragment f) => { await context.Response.WriteAsync( $\"data: {f.Part}|{f.Content}{Environment.NewLine}{Environment.NewLine}\", cancellation ); await context.Response.Body.FlushAsync(cancellation); } ); });\n\nThe RecipeGenerator.GenerateAsync method contains the main flow:\n\n    \n    \n    /// <summary> /// The main entry point /// </summary> public async Task GenerateAsync( RecipeRequest request, Func<Fragment, Task> handler, // \ud83d\udc48 This is the hook to the HTTP response stream CancellationToken cancellation = default ) { var (ingredientsOnHand, prepTime) = request; // \ud83d\udc47 (1) Generate the list of 3 recipes and pick one at random var recipes = await GenerateRecipesAsync(ingredientsOnHand, prepTime, cancellation); Console.WriteLine($\"Generated {recipes.Length} recipes.\"); var recipe = recipes[Random.Shared.Next(0, 2)]; // \ud83d\udc47 (2) Since we have all of the recipes, we aggregate as an HTML string var alternates = recipes .Where(r => r.Name != recipe.Name) .Aggregate(new StringBuilder(), (html, r) => { html.Append($\"<li><b>{r.Name}</b> &nbsp;\"); html.Append($\"<i>{r.Intro}</i></li>\"); return html; }).ToString(); // \ud83d\udc47 (3) This is our loop on the reader side of the channel; we start it first var fragmentHandler = async () => { while (await _channel.Reader.WaitToReadAsync()) { if (_channel.Reader.TryRead(out var fragment)) { await handler(fragment); } } }; var completion = fragmentHandler(); // \ud83d\udc47 (4) Now we run the generation prompts concurrently Task.WaitAll([ handler(new (\"alt\", alternates)), GenerateIngredientsAsync(recipe, ingredientsOnHand, request.PrepTime, cancellation), GenerateIntroAsync(recipe, cancellation), GenerateIngredientIntroAsync(ingredientsOnHand, cancellation), GenerateSidesAsync(recipe, cancellation) ]); // \ud83d\udc47 (5) And wait for everything to complete. _channel.Writer.Complete(); await completion; }\n\nEach of the generation steps follows a similar pattern:\n\n    \n    \n    private async Task GenerateIntroAsync( RecipeSummary recipe, CancellationToken cancellation = default ) { var prompt = \"...\"; await ExecutePromptAsync( \"int\", // \ud83d\udc48 This matches the ID of a front-end target where the output goes prompt, new () { MaxTokens = 250, Temperature = 0.55, TopP = 0 }, cancellation: cancellation ); }\n\nAnd the method for executing the prompts:\n\n    \n    \n    /// <summary> /// Executes the prompt and writes the result to the channel. /// </summary> private async Task ExecutePromptAsync( string part, string prompt, OpenAIPromptExecutionSettings settings, Action<string>? resultHandler = null, CancellationToken cancellation = default ) { // \ud83d\udc47 Build our Semantic Kernel instance var kernelBuilder = Kernel.CreateBuilder(); var kernel = kernelBuilder .AddOpenAIChatCompletion( // \ud83d\udc48 Fireworks.ai is API compatible with OpenAI modelId: _model, apiKey: _key, endpoint: _endpoint ) .Build(); // \ud83d\udc47 Initialize our chat var chat = kernel.GetRequiredService<IChatCompletionService>(); var history = new ChatHistory(); var buffer = new StringBuilder(); history.AddUserMessage(prompt); // \ud83d\udc47 Stream the response and write each part to our channel await foreach (var message in chat.GetStreamingChatMessageContentsAsync( history, settings, kernel, cancellation ) ) { await _channel.Writer.WriteAsync( // \ud83d\udc48 The writer end of our channel new(part, message.Content ?? \"\"), cancellation ); buffer.Append(message.Content); // \ud83d\udc48 A buffer to hold the full output } var output = buffer.ToString(); // \ud83d\udc47 If the caller wants the full result, we have it here resultHandler?.Invoke(output); }\n\n### Simultaneous Streams with SSE\n\nTo help visualize this flow, check out the diagram below:\n\nThe Task.WaitAll block of code is handed the shared, thread-safe writer end of\na Channel while the reader end is connected to the HTTP response stream via a\ncallback.\n\nThat callback simply formats the Fragment per the required format spec for\nEventSource.\n\nIn this case:\n\n    \n    \n    data: ing|tomatoes data: ing|basil data: ste|3. Chop the\n\nThe front-end receives a stream of these messages which gets accumulated into\ndifferent sections of the UI.\n\n  1. The first part, ing identifies the front-end part that this content belongs to (\u201cingredients\u201d in this case)\n  2. The text after the | represents a set of output tokens written by the LLM\n\nOn the front-end, @microsoft/fetch-event-source is polyfilled for the native\nEventSource to allow the usage of POST.\n\n> Note: it is non-trivial to render the output as HTML either on the client\n> side or on the server side. Part of the problem is that the output is\n> generated partially so writing out < results in the HTML entity &lt; if we\n> don\u2019t have the rest of the element (assuming we output HTML from the\n> backend). Probably the best solution is to \u201ccache\u201d the raw HTML text in a\n> hidden element along with a rendered version.\n\nThe receiver takes each message and decodes it:\n\n    \n    \n    onmessage: (msg) => { var payload = msg.data var [part, content] = payload.split('|') if (!part || !$el(`#${part}`)) { return // Discard this message } // \ud83d\udc47 This is a hack to encode newlines and replace them here. content = content.replace(/\u2b91/gi, \"\\n\") $el(`#${part}`).innerHTML += content },\n\nA special note with text/event-stream is that double newlines indicate the end\nof a message block. So newlines need to be encoded somehow (there are many\nways). In this case, using the single character \u2b91 makes it easy to find and\nreplace it with \\n when we append the content.\n\nThe CSS simply needs to account for this:\n\n    \n    \n    #add, #ing, #ste { white-space: pre-line; }\n\nThe HTML itself is simple:\n\n    \n    \n    <!-- This block holds the additional ingredients --> <div class=\"additional\"> <h2>Ingredients Needed</h2> <!-- \ud83d\udc47 This ID matches the Fragment.Part --> <div id=\"add\"></div> </div> <!-- This block holds the steps --> <div class=\"recipe\"> <h2>Recipe Steps</h2> <!-- \ud83d\udc47 This ID matches the Fragment.Part --> <div id=\"ste\"></div> </div>\n\n### Putting it Together\n\nNow that all the parts are in place, running the app should yield the\nfollowing experience:\n\nThere is a slight initial delay as the call to generate the list of recipes is\nblocking.\n\nHowever, once the list is generated and one is selected at random, the\nadditional generation occurs concurrently with only the steps being blocked by\nthe full list of ingredients (since we need that to generate accurate steps\nthat reflect our full ingredient list).\n\n## Conclusion\n\nFor applications where the UX benefits from having higher throughput and you\ncan make it work with smaller context windows, Fireworks.ai with Llama-3\n8B/70B is absolutely a game changer; it lets teams build for use cases that\nwould otherwise compromise the overall UX with the high latency of OpenAI\u2019s\nGPT models.\n\nPlugging that into a .NET 8 web API using System.Threading.Channels combined\nwith SSE means that it is possible to concurrently generate multiple chunks of\ncontent and open up a new set of possibilities for leveraging generative AI\nwhether the objective is to build more interactive gen AI experiences or\nsimply speeding up your generative workflows.\n\nThe same techniques (minus SSE) can increase the throughput of your server\ngeneration workloads by processing multiple prompts in parallel using lower\nlatency + higher throughput models and platforms.\n\n> \ud83d\udca1 Full repo is available here: https://github.com/CharlieDigital/dn8-sk-\n> llama3-fireworks\n\n\u00a9 2024 Charles Chen\n\nWebsite Template developed by Manuel Ernesto \u26a1\ufe0f\n\n  * Home\n  * Blog\n  * Profile\n  * KDramas\n  * Contact\n\n", "frontpage": false}
