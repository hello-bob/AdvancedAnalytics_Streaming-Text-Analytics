{"aid": "40082409", "title": "Setting up PostgreSQL for running integration tests", "url": "https://gajus.com/blog/setting-up-postgre-sql-for-running-integration-tests", "domain": "gajus.com", "votes": 1, "user": "mooreds", "posted_at": "2024-04-19 01:00:49", "comments": 0, "source_title": "Setting up PostgreSQL for running integration tests", "source_text": "Setting up PostgreSQL for running integration tests\n\nFri Apr 12 2024\n\n# Setting up PostgreSQL for running integration tests\n\n### On this page\n\n  * Isolation as the primary goal\n  * What didn't work\n  * Using transactions\n  * Using SQLite\n  * Using `pg_tmp`\n  * What worked\n  * Template Databases\n  * Mounting a memory disk\n  * Using Docker container with a memory disk\n  * Managing test databases\n  * Conclusion\n\nWhen it comes to testing, achieving performance and reliability is crucial. In\nthis article, I'll explain how to set up PostgreSQL for tests and discuss some\ncommon pitfalls to avoid.\n\n## Isolation as the primary goal\n\nBefore we dive into the details, let's define our goals:\n\n  * Isolation \u2013 We want to ensure that each test runs in isolation. At the very least, this means that each test should have its own database. This ensures that tests don't interfere with each other and that you can run tests in parallel without any issues.\n  * performance \u2013 We want to ensure that setting up PostgreSQL for tests is fast. A slow solution is going to be cost prohibitive for running tests in CI/CD pipelines. The solution that we come up with must allow us to execute tests without introducing too much overhead.\n\nThe rest of this article will focus on what we have tried, what worked, and\nwhat didn't work.\n\n## What didn't work\n\n### Using transactions\n\nThe first approach we tried was to use transactions. We would start a\ntransaction at the beginning of each test and roll it back at the end.\n\nThe basic idea is illustrated in the following example:\n\n    \n    \n    test('calculates total basket value', async () => { await pool.transaction(async (tx) => { await tx.query(sql.unsafe` INSERT INTO basket (product_id, quantity) VALUES (1, 2) `); const total = await getBasketTotal(tx); expect(total).toBe(20); }); });\n\nThe transaction approach works well for simple cases (e.g., testing a single\nfunction), but it quickly becomes a problem when dealing with tests that test\nintegration between multiple components. Due to connection pooling, nested\ntransactions, and other factroes, the necessary work to make the transaction\napproach work would have meant that we are not replicating the real-world\nbehavior of our application, i.e. it would not provide the confidence we need.\n\nFor consistency, we also want to avoid mixing testing approaches. Even though\nusing transactions would suffice for some tests, we want to have a consistent\napproach across all tests.\n\n### Using SQLite\n\nAnother approach we tried was to use SQLite. SQLite is an in-memory database\nthat is fast and easy to set up.\n\nSimilar to the transaction approach, SQLite works well for simple cases.\nHowever, it quickly becomes a problem when dealing with code paths that use\nPostgreSQL-specific features. In our case, due to the use of various\nPostgreSQL extensions, PL/pgSQL functions, and other PostgreSQL-specific\nfeatures, we couldn't use SQLite for our tests.\n\npglite provides PostgreSQL packaged as a WASM module that can be used in\nNode.js. This might be a good alternative, though we haven't tried it yet.\nRegardless, the current lack of support for extensions would have been a\nblocker for us.\n\n### Using pg_tmp\n\nAnother approach we tried was to use pg_tmp. pg_tmp is a tool that creates a\ntemporary PostgreSQL instance for each test.\n\nIn theory, pg_tmp is a good solution. It allows a complete isolation of tests.\nIn practice, is a lot slower than we could tolerate. With pg_tmp, it takes a\nfew seconds to start and populate the database, and this overhead quickly adds\nup when running thousands of tests.\n\nLet's say you have 1000 tests, and each test takes 1 second to run. If you add\n2 seconds of overhead for creating a new database, you are looking at an\nadditional 2000 seconds (33 minutes) of overhead.\n\nIf you like this approach, you could also probably get away with using Docker\ncontainers. Depending on many factors, Docker containers might be even faster\nthan pg_tmp.\n\nintegresql is a project that I came across in a HN thread. It seems like a\ngood alternative that reduces the overhead of creating a new database to about\n500ms. It has a pooling mechanism that allows you to reduce the overhead even\nfurther. We decided against continuing on this path because we were happy with\nthe level of isolation that we got from using template databases.\n\n## What worked\n\nAfter trying various approaches, we settled on combining two approaches:\ntemplate databases and mounting a memory disk. This approach allowed us to\nisolate each test at a database level without introducing too much overhead or\ncomplexity.\n\n### Template Databases\n\nA template databases is a database that serves as a template for creating new\ndatabases. When you create a new database from a template database, the new\ndatabase has the same schema as the template database. The steps to create a\nnew database from a template database are as follows:\n\n  1. Create a template database (ALTER DATABASE <database_name> is_template=true;)\n  2. Create a new database from the template database (CREATE DATABASE <new_database_name> TEMPLATE <template_database_name>;)\n\nThe key advantage of using template databases is that you do not need to mess\nwith managing multiple PostgreSQL instances. You can create copy databases and\nhave each test run in isolation.\n\nHowever, on its own, template databases are not fast enough for our use case.\nThe time it takes to create a new database from a template database is still\ntoo high for running thousands of tests:\n\n    \n    \n    postgres=# CREATE DATABASE foo TEMPLATE contra; CREATE DATABASE Time: 1999.758 ms (00:02.000)\n\nThis is where the memory mounting comes in.\n\nThe other limitation of template databases to be aware of is that no other\nsessions can be connected to the source database while it is being copied.\nCREATE DATABASE will fail if any other connection exists when it starts;\nduring the copy operation, new connections to the source database are\nprevented. It is an easy enough limitation to work around using a mutex\npattern, but it is something to be aware of.\n\n### Mounting a memory disk\n\nThe final piece of the puzzle is mounting a memory disk. By mounting a memory\ndisk, and creating the template database on the memory disk, we can\nsignificantly reduce the overhead of creating a new database.\n\nI will talk about how to mount a memory disk in the next section, but first,\nlet's see how much of a difference it makes.\n\n    \n    \n    postgres=# CREATE DATABASE bar TEMPLATE contra; CREATE DATABASE Time: 87.168 ms\n\nThis is a significant improvement and makes the approach viable for our use\ncase.\n\nNeedless to say, this approach is not without its drawbacks. The data is\nstored in memory, which means that it is not persistent. If the database\ncrashes or the server restarts, the data is lost. However, for running tests,\nthis is not a problem. The data is recreated from the template database each\ntime a new database is created.\n\n### Using Docker container with a memory disk\n\nThe approach we settled on was to use a Docker container with a memory disk.\nHere is how you can set it up:\n\n    \n    \n    $ docker run \\ -p 5435:5432 \\ --tmpfs /var/lib/pg/data \\ -e PGDATA=/var/lib/pg/data \\ -e POSTGRES_PASSWORD=postgres \\ --name contra-database \\ --rm \\ postgres:14\n\nIn the above command, we are creating a Docker container with a memory disk\nmounted at /var/lib/pg/data. We are also setting the PGDATA environment\nvariable to /var/lib/pg/data to ensure that PostgreSQL uses the memory disk\nfor data storage. The end result is that the underlying data is stored in\nmemory, which significantly reduces the overhead of creating a new database.\n\n### Managing test databases\n\nThe basic idea is to create a template database before running the tests and\nthen create a new database from the template database for each test. Here is a\nsimplified version of how you can manage test databases:\n\n    \n    \n    import { createPool, sql, stringifyDsn, } from 'slonik'; type TestDatabase = { destroy: () => Promise<void>; getConnectionUri: () => string; name: () => string; }; const createTestDatabasePooler = async (connectionUrl: string) => { const pool = await createPool(connectionUrl, { connectionTimeout: 5_000, // This ensures that we don't attempt to create multiple databases in parallel. maximumPoolSize: 1, }); const createTestDatabase = async ( templateName: string, ): Promise<TestDatabase> => { const database = 'test_' + uid(); await pool.query(sql.typeAlias('void')` CREATE DATABASE ${sql.identifier([database])} TEMPLATE ${sql.identifier([templateName])} `); return { destroy: async () => { await pool.query(sql.typeAlias('void')` DROP DATABASE ${sql.identifier([database])} `); }, getConnectionUri: () => { return stringifyDsn({ ...parseDsn(connectionUrl), databaseName: database, password: 'unsafe_password', username: 'contra_api', }); }, name: () => { return database; }, }; }; return () => { return createTestDatabase('contra_template'); }; }; const getTestDatabase = await createTestDatabasePooler();\n\nAt this point, you can use getTestDatabase to create a new database for each\ntest. The destroy method can be used to clean up the database after the test\nhas run.\n\n## Conclusion\n\nThis setup allows us to run thousands of tests in parallel across multiple\nshards without any issues. The overhead of creating a new database is minimal,\nand the isolation is at a database level. We are happy with the performance\nand reliability that this setup provides.\n\n  * \u2b05\ufe0e Back to Blog\n  * Newsletter\n  * Report an issue\n\n", "frontpage": false}
