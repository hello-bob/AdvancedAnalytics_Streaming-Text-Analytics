{"aid": "40211377", "title": "The second law of infodynamics and its implications for the simulated universe", "url": "https://pubs.aip.org/aip/adv/article/13/10/105308/2915332/The-second-law-of-infodynamics-and-its", "domain": "aip.org", "votes": 2, "user": "gkew", "posted_at": "2024-04-30 14:37:29", "comments": 0, "source_title": "The second law of infodynamics and its implications for the simulated universe hypothesis", "source_text": "The second law of infodynamics and its implications for the simulated universe hypothesis | AIP Advances | AIP Publishing\n\nOpens in a new window Opens an external website Opens an external website in a\nnew window\n\nWe and our 0 partners store and access information on your device for\npersonalized ads and content. Personal data may be processed, such as cookie\nidentifiers, unique device identifiers, and browser information. Third parties\nmay store and access information on your device and process this personal\ndata. You may change or withdraw your preferences by clicking on the cookie\nicon or link; however, as a consequence, you may not see relevant ads or\npersonalized content. You may change your settings at any time or accept the\ndefault settings. You may close this banner to continue with only essential\ncookies. Privacy Policy\n\nStorage Preferences Third Parties\n\nSkip to Main Content\n\nSearch Dropdown Menu\n\nAdvanced Search |Citation Search\n\nUser Tools Dropdown\n\nSign In\n\nToggle MenuMenu\n\nSkip Nav Destination\n\nArticle navigation\n\nVolume 13, Issue 10\n\nOctober 2023\n\n  * I. INTRODUCTION\n\n  * II. SECOND LAW OF INFODYNAMICS AND DIGITAL INFORMATION\n\n  * III. SECOND LAW OF INFODYNAMICS AND GENETIC INFORMATION\n\n  * IV. SECOND LAW OF INFODYNAMICS AND HUND\u2019S RULE\n\n  * A. Numerical calculations\n\n  * B. s-orbital\n\n  * C. p-orbital\n\n  * D. d-orbital\n\n  * E. f-orbital\n\n  * V. SECOND LAW OF INFODYNAMICS IN COSMOLOGY\n\n  * VI. SECOND LAW OF INFODYNAMICS AND SYMMETRIES\n\n  * VII. CONCLUSIONS\n\n  * ACKNOWLEDGMENTS\n\n  * AUTHOR DECLARATIONS\n\n  * Conflict of Interest\n\n  * Author Contributions\n\n  * DATA AVAILABILITY\n\n  * REFERENCES\n\nResearch Article| October 06 2023\n\n# The second law of infodynamics and its implications for the simulated\nuniverse hypothesis\n\nMelvin M. Vopson\n\n0000-0002-8073-5538\n\nMelvin M. Vopson ^a)\n\n(Conceptualization, Formal analysis, Funding acquisition, Investigation,\nMethodology, Writing \u2013 original draft, Writing \u2013 review & editing)\n\nSchool of Mathematics and Physics, University of Portsmouth\n\n, Portsmouth PO1 3QL,\n\nUnited Kingdom\n\na)Author to whom correspondence should be addressed: melvin.vopson@port.ac.uk\n\nSearch for other works by this author on:\n\nThis Site\n\nPubMed\n\nGoogle Scholar\n\nAuthor & Article Information\n\na)Author to whom correspondence should be addressed: melvin.vopson@port.ac.uk\n\nAIP Advances 13, 105308 (2023)\n\nhttps://doi.org/10.1063/5.0173278\n\nArticle history\n\nReceived:\n\nAugust 21 2023\n\nAccepted:\n\nSeptember 19 2023\n\n  * Split-Screen\n  * Views Icon Views\n\n    * Article contents\n    * Figures & tables\n  * Open the PDF for in another window\n  * Share Icon Share\n\n    * Twitter\n    * Facebook\n    * Reddit\n    * LinkedIn\n  * Tools Icon Tools\n\n    * Reprints and Permissions\n\n    * Cite Icon Cite\n\n  * Search Site\n\nCitation\n\nMelvin M. Vopson; The second law of infodynamics and its implications for the\nsimulated universe hypothesis. AIP Advances 1 October 2023; 13 (10): 105308.\nhttps://doi.org/10.1063/5.0173278\n\nDownload citation file:\n\n  * Ris (Zotero)\n  * Reference Manager\n  * EasyBib\n  * Bookends\n  * Mendeley\n  * Papers\n  * EndNote\n  * RefWorks\n  * BibTex\n\nThe simulation hypothesis is a philosophical theory, in which the entire\nuniverse and our objective reality are just simulated constructs. Despite the\nlack of evidence, this idea is gaining traction in scientific circles as well\nas in the entertainment industry. Recent scientific developments in the field\nof information physics, such as the publication of the mass-energy-information\nequivalence principle, appear to support this possibility. In particular, the\n2022 discovery of the second law of information dynamics (infodynamics)\nfacilitates new and interesting research tools at the intersection between\nphysics and information. In this article, we re-examine the second law of\ninfodynamics and its applicability to digital information, genetic\ninformation, atomic physics, mathematical symmetries, and cosmology, and we\nprovide scientific evidence that appears to underpin the simulated universe\nhypothesis.\n\nTopics\n\nLaws of thermodynamics, General relativity, Information theory entropy,\nElectronic configuration, Nucleotides\n\n## I. INTRODUCTION\n\nIn 2022, a new fundamental law of physics has been proposed and demonstrated,\ncalled the second law of information dynamics, or simply the second law of\ninfodynamics.^1 Its name is an analogy to the second law of thermodynamics,\nwhich describes the time evolution of the physical entropy of an isolated\nsystem, which requires the entropy to remain constant or to increase over\ntime. In contrast to the second law of thermodynamics, the second law of\ninfodynamics states that the information entropy of systems containing\ninformation states must remain constant or decrease over time, reaching a\ncertain minimum value at equilibrium. This surprising observation has massive\nimplications for all branches of science and technology. With the ever-\nincreasing importance of information systems such as digital information\nstorage or biological information stored in DNA/RNA genetic sequences, this\nnew powerful physics law offers an additional tool for examining these systems\nand their time evolution.^2\n\nIt is important to clearly distinguish between physical entropy and\ninformation entropy. The physical entropy of a given system is a measure of\nall its possible physical microstates compatible with the macrostate, S_Phys.\nThis is a characteristic of the non-information bearing microstates within the\nsystem. Assuming the same system, and assuming that one is able to create N\ninformation states within the same physical system (for example, by writing\ndigital bits in it), the effect of creating a number of N information states\nis to form N additional information microstates superimposed onto the existing\nphysical microstates. These additional microstates are information bearing\nstates, and the additional entropy associated with them is called the entropy\nof information, S_Info. We can now define the total entropy of the system as\nthe sum of the initial physical entropy and the newly created entropy of\ninformation, S_tot = S_Phys + S_Info, showing that the information creation\nincreases the entropy of a given system. It is also important to clarify that\ninformation state is defined as any physical state, process, or event that can\ncontain information in Shannon\u2019s information theory framework.^3 When a set of\nn independent and distinctive information states are created, X = {x_1, x_2,\n..., x_n}, having a discrete probability distribution P = {p_1, p_2, ...,\np_n}, the average information content per state is given by the Shannon\ninformation entropy formula^3\n\n(1)\n\nThe base of the logarithm, b, gives the units of information. When b = 2, the\nfunction H(X) returns an information value in bits. The function H(X) is\nmaximum when the events x_j have equal probabilities of occurring, p_j = 1/n.\n\nThe reader should not confuse the Shannon information entropy H(X), with the\nentropy of the information bearing states, S_Info. Although the two parameters\nare closely linked, they are rather different quantities. If N information\nstates are created within a given system containing n independent and\ndistinctive information states, N \u2265 n, then the additional possible states,\nalso known as distinct messages in Shannon\u2019s original formalism, are\nequivalent to the number of information bearing microstates, \u03a9 compatible with\nthe macrostate^4^,\n\n(2)\n\nThe general entropy of the information bearing states is now derived as\nfollows:\n\n(3)\n\nor S_Info = N \u00b7 k_b \u00b7 ln n \u00b7 H(X), where k_b = 1.380 64 \u00d7 10^\u221223 J/K is the\nBoltzmann constant. The second law of infodynamics states that^1\n\n(4)\n\nSince k_b is a constant and n is the number of distinct events (information\nstates), which is also a constant of the system, the decrease in the entropy\nof the information states can only come from the reduction over time in the\ntotal number of states, N, or a reduction over time in the Shannon entropy due\nto changes to the probabilities p_j.\n\nIn what follows, we will examine a few diverse applications of the second law\nof infodynamics and demonstrate the universal nature of this new physics law,\nincluding the fact that it points to the characteristics of a computational\nsystem, underpinning to some degree the simulated universe hypothesis.\nSections II and III have been covered in greater detail in the 2022 article,^1\nbut they are discussed briefly here to reinforce our point and introduce the\ncontext of the second law of infodynamics to the reader.\n\n## II. SECOND LAW OF INFODYNAMICS AND DIGITAL INFORMATION\n\nA digital data storage system contains digital information states, having two\ndistinct states, , so n = 2 and probabilities . The base in relation (1) is\ntaken as b = 2 for units of digital bits. Assuming the system contains N bits,\naccording to (2), it will have a total number of possible microstates,\n\n(5)\n\nThe entropy of the information bearing states for a digital information system\nis\n\n(6)\n\nThe maximum Shannon entropy of this system is H(X) = 1, and it can deviate\nslightly from this upper limit, but this value is stable over time. Hence, in\nthe case of digital information, the only parameter that can drive the time\nevolution of the entropy of the information bearing states is the total number\nof states, N. If N increases, then the information entropy increases. However,\nthere is no mechanism that would result in spontaneous information being\ncreated without external intervention (i.e., energy input). In a previous\nstudy, we demonstrated that the only possible evolution of N over time is down\nor constant,^1 in accordance with the second law of infodynamics. This is a\nvery straightforward process and a direct consequence of the second law of\nthermodynamics because, over time, the digital states are eroded by thermal\nfluctuations, leading to the self-erasure of data. The higher the temperature\nof the environment, the more probable the data self-erasure processes are.\nHence, in the case of digital information, the second law of infodynamics is\nrather trivial and fully expected. In our previous study, we demonstrated this\nusing room temperature (300 K) micromagnetics modeling^5 of a granular\nmagnetic thin film structure with perpendicular uniaxial anisotropy of K_a =\n8.75 \u00d7 10^6 J/m^3 and M_s = 1710 kA/m. Figure 1 shows the schematic of the\nword INFORMATION written digitally onto a 400 \u00d7 550 \u00d7 2 nm^3 magnetic thin\nfilm structure, resulting in a bit size of 50 \u00d7 50 nm^2, which was allowed to\nevolve over time at room temperature.\n\nFIG. 1.\n\nView largeDownload slide\n\n(a) Schematics of the word INFORMATION is written on a material in binary code\nusing magnetic recording. Red denotes magnetization pointing out of the plane\nand blue is magnetization pointing into the plane. (b)\u2013(d) Time evolution of\nthe digital magnetic recording information states simulated using\nmicromagnetic Monte Carlo. (b) Initial random state. (c) INFORMATION is\nwritten (t = 0 s). (d) Iteration 930 (t = 1395 s) showing the degradation of\ninformation states. Reproduced with permission from M. M. Vopson and S.\nLepadatu, AIP Adv. 12, 075310 (2022). Copyright 2022 AIP Publishing.\n\nFIG. 1.\n\nView largeDownload slide\n\n(a) Schematics of the word INFORMATION is written on a material in binary code\nusing magnetic recording. Red denotes magnetization pointing out of the plane\nand blue is magnetization pointing into the plane. (b)\u2013(d) Time evolution of\nthe digital magnetic recording information states simulated using\nmicromagnetic Monte Carlo. (b) Initial random state. (c) INFORMATION is\nwritten (t = 0 s). (d) Iteration 930 (t = 1395 s) showing the degradation of\ninformation states. Reproduced with permission from M. M. Vopson and S.\nLepadatu, AIP Adv. 12, 075310 (2022). Copyright 2022 AIP Publishing.\n\nClose modal\n\nThe average unit cell size (cubic) was V = 10^\u221227 m^3, which is intentionally\n\u223c1.9 times lower than the required size for a thermally stable medium, in\norder to speed up the computation time. This resulted in a relaxation time of\n1.5 s, which corresponds to a single iteration in the Monte Carlo algorithm.\nThe simulations show that the entropy of the information bearing states will\nremain constant or decrease over time, and after a sufficiently long time, all\ninformation states will become self-erased, leading to zero entropy of\ninformation states. Figure 1(b) shows the simulated specimen before data were\nrecorded on it. Figure 1(c) shows the same sample with the data written on it\nat time zero. Figure 1(d) shows the time evolution of the data after 930 Monte\nCarlo cycles, showing the degradation of the data. After 1990 cycles, the\nentire data got self-erased, and the information entropy became zero.\n\n## III. SECOND LAW OF INFODYNAMICS AND GENETIC INFORMATION\n\nA very interesting information storage system is a DNA/RNA sequence encoding\nbiological information. This can be represented as a long string of the\nletters A, C, G, and T, where the characters represent adenine (A), cytosine\n(C), guanine (G), and thymine (T) [replaced with uracil (U) in RNA sequences].\nTherefore, within Shannon\u2019s information theory framework, a typical genome\nsequence can be represented as a probabilistic system of four distinctive\nstates, n = 4, and probabilities . Using digital information units and Eq.\n(1), we determine that the maximum Shannon information entropy is H(X) = 2,\nand each nucleotide can encode a maximum of 2 bits: A = 00, C = 01, G = 10, T\n= 11. For a given genomic sequence containing N nucleotides, the total number\nof possible microstates is\n\n(7)\n\nThe entropy of the information bearing states of a genomic sequence is\n\n(8)\n\nThe time evolution of the entropy of genetic DNA/RNA information systems is\ngiven by the time evolution of the changes in their nucleotide sequence,\ncalled genetic mutations. Genetic mutations can take place via three\nmechanisms: (i) Single nucleotide polymorphisms (SNPs), where changes occur so\nthat the number of nucleotides N remains constant; (ii) deletions, where N\ndecreases; and (iii) insertions, where N is increasing.\n\nSimilar to the case of digital information, a reduction of N would most likely\nresult in a reduction of the overall entropy of the information bearing\nstates, so \u201cdeletion\u201d mutations would automatically fulfill the second law of\ninfodynamics. In our previous study, we examined real data from RNA sequences\nthat underwent only SNP mutations, which maintained the value of the N\nconstant, and the reduction in the information entropy came only from\nShannon\u2019s information entropy function.^1,2 Our test RNA sequences were\nvariants of the novel SARS-CoV-2 virus, which emerged in December 2019\nresulting in the COVID-19 pandemic. The reference RNA sequence of the SARS-\nCoV-2, collected in Wuhan, China in December 2019 (MN908947),^6 has 29 903\nnucleotides, so N = 29 903. All analyzed variants had 29 903 nucleotides and\nhave been collected and sequenced at a later time, after undergoing an\nincremental number of SNP mutations. Shannon information entropies of the\nreference sequence and of the variants were computed using relation (1) and\npreviously developed software, GENIES.^7,8\n\nRemarkably, the results indicate a unique correlation between the information\nand the dynamics of the genetic mutations by showing that the Shannon\ninformation entropy, H(X), and the overall information entropy of the SARS-\nCoV-2 variants (S_Info) computed using Eq. (8) decrease linearly with the\nnumber of mutations and over time, i.e., because number of mutations increase\nover time (see Fig. 2). The corresponding code names of the genome variants\nextracted from the NCBI database^9\u201314 and analyzed in this work are shown next\nto each data point in Fig. 2. This result not only confirms the universal\nvalidity of the second law of infodynamics but also points to a possible\ngoverning mechanism of genetic mutations,^2 currently believed to be just\nrandom events. The observation of the information entropic force that governs\ngenetic mutations is very powerful because it challenges the Darwinian view\nthat genetic mutations are complete random events and could be used to develop\npredictive algorithms for genetic mutations before they occur.^2 We should\nacknowledge that, while all analyzed SARS-Cov-2 variants showed a decrease in\ntheir information entropy as they underwent genetic mutations, the data points\npresented in Fig. 2 have been carefully selected to emphasize the linear\ntrend.\n\nFIG. 2.\n\nView largeDownload slide\n\nShannon information entropy values of variants of the SARS-CoV-2 virus as a\nfunction of the number of SNP mutations per variant.\n\nFIG. 2.\n\nView largeDownload slide\n\nShannon information entropy values of variants of the SARS-CoV-2 virus as a\nfunction of the number of SNP mutations per variant.\n\nClose modal\n\nNaturally, we asked whether the same RNA system would display behavior\nconsistent with the second law of infodynamics, when the SARS-CoV-2 variants\nsuffered \u201caddition\u201d mutations, so the number of nucleotides N is no longer\nconstant but becomes larger than 29 903, increasing the information entropy.\nUsing the NCBI database, we searched all the sequenced SARS-CoV-2 variants\nfrom January 1 2020 to January 1 2022. We searched only complete sequences\nwith no missing/undetermined nucleotides, and the result was a total of 4.48 \u00d7\n10^6 sequences. When we restricted the results to only the sequences that had\nat least 29 903 nucleotides or more, then 48 450 sequences were identified.\nUnfortunately, only one suffered a mutation where the resultant number of\nnucleotides increased by 1\u201329 904. Hence, 98.92% of all mutations took place\nvia \u201cdeletion,\u201d reducing the total number of nucleotides. Since only one\ngenome out of 4.48 \u00d7 10^6 appeared to increase the number of nucleotides, this\nis statistically irrelevant. Hence, we concluded that, for this test case,\ngenetic mutations appear to take place in a way that reduces their information\nentropy, mostly via a deletion mechanism or a SNP. This is fully consistent\nwith the second law of infodynamics, as a deletion would automatically\ndecrease the total information entropy, and the SNPs have been shown to take\nplace in a way that the information entropy is again reduced due to a\nreduction in Shannon\u2019s information entropy.\n\nWe would also like to quote the famous Spiegelman\u2019s experiment that took place\nin 1972.^15 In this experiment, Spiegelman studied the evolution of a virus\nover 74 generations. The virus was kept isolated in ideal conditions to\nsurvive, and with each generation, the virus was sequenced. The initial virus\nhad 4500 base points, and with each generation, the genome decreased\nconsistently in size. After 74 generations, the virus evolved to only 218 base\npoints, showing an interesting and unexplained reduction of its genome of over\n95%. Just as the 2022 study on SARS-CoV-2,^1 Spiegelman\u2019s experiment is fully\nconsistent with the second law of infodynamics, which requires the information\nentropy to remain constant or to decrease over time, reaching a minimum value\nat equilibrium.\n\n## IV. SECOND LAW OF INFODYNAMICS AND HUND\u2019S RULE\n\nElectronic states in atoms are fully described by four principal quantum\nnumbers: (a) the principal quantum number, n. This number determines the\nenergy of a particular shell or orbit, and it takes non-zero positive integral\nvalues n = 1, 2, 3, 4, ... (b) the orbital angular momentum quantum number, l.\nThis quantum number describes the subshell, and gives the total angular\nmomentum of an electron due to its orbital motion. This quantum number takes\nintegral values restricted to l = 0, 1, 2, ..., n \u2212 1. (c) The magnetic\nquantum number, m_l. This quantum number determines the component (projection)\nof the orbital angular momentum along a specific direction, usually the\ndirection of an applied magnetic field. It takes integral values, and for a\ngiven value of l, it may have (2l + 1) possible values: m_l = l, l \u2212 1, l \u2212 2,\n..., 0, \u2212l, ..., \u2212(l \u2212 1), \u2212l. (d) The spin quantum number s, and the\nsecondary spin quantum number, m_s. The spin quantum number s gives the\neigenvalues of the spin angular momentum operator, and it is related to the\nfact that the electron has an intrinsic angular momentum called \u201cspin\u201d or spin\nangular momentum, which results from the rotation of the electron around an\ninternal axis. The spin quantum number takes the values s = n/2, where n is a\npositive integer, so that s = 0, 1/2, 1, 3/2, 2, .... The secondary quantum\nspin number m_s determines the direction (i.e., projection) of the spin\nangular momentum along the direction of an applied magnetic field. The allowed\nvalues of m_s are 2s + 1 values from \u2212s to +s in steps of 1. For example, an\nelectron has s = 1/2, so the allowed values of m_s are \u22121/2 and +1/2.\n\nThe electrons occupy atomic shells according to Pauli\u2019s exclusion\nprinciple,^16 which states that two or more identical fermions cannot\nsimultaneously occupy the same quantum state within a quantum system. In the\ncase of electrons in atoms, this means that it is impossible for two electrons\nin a multi-electron atom to have the same values of the four quantum numbers\ndescribed above. For example, if two electrons reside in the same orbital,\nthen their n, l, and m_l values are the same, so their m_s must be different,\nimposing that the electrons must have opposite half-integer spin projections\nof 1/2 and \u22121/2.\n\nHowever, in the case of multi-electron atoms, multiple electron arrangements\nare possible while fulfilling Pauli\u2019s exclusion principle.\n\nIn order to determine the electron population of an atomic orbital\ncorresponding to the ground state of a multi-electron atom, German physicist\nFriedrich Hund formulated in 1927 a set of rules^17 derived from\nphenomenological observations. These are called Hund\u2019s rules, and when used in\nconjunction with Pauli\u2019s exclusion principle, they are useful in atomic\nphysics to determine the electron population of atoms corresponding to the\nground state.\n\nTo explain this, let us assume that an atom has three electrons on its p\norbital. Figure 3 shows the three allowed ground state distinctive\nconfigurations that fulfill Pauli\u2019s exclusion principle, resulting in total\nspin quantum values of 1/2, 3/2, and 1/2, respectively.\n\nFIG. 3.\n\nView largeDownload slide\n\nThree electrons residing on a p orbital and their allowed arrangements\naccording to Pauli\u2019s exclusion principle.\n\nFIG. 3.\n\nView largeDownload slide\n\nThree electrons residing on a p orbital and their allowed arrangements\naccording to Pauli\u2019s exclusion principle.\n\nClose modal\n\nThe correct electronic arrangement is given by Hund\u2019s first rule, which is the\nmost important, and it is simply called Hund\u2019s rule. This states that the\nlowest energy atomic state is the one that maximizes the total spin quantum\nnumber, meaning simply that the orbitals of the subshell are each occupied\nsingly with electrons of parallel spin before double occupation occurs.\nTherefore, the term with the lowest energy is also the term with the maximum\nnumber of unpaired electrons, so for our example shown in Fig. 3, Hund\u2019s rule\ndictates that the correct configuration is the middle one, resulting in a\ntotal spin quantum value of 3/2.\n\nHund\u2019s rule is derived from empirical observations, and there is no clear\nunderstating of why the electrons populate atomic orbitals in this way. So\nfar, two different physical explanations have been given in Ref. 18. Both\nexplanations revolve around the energetic balance of the electrons and their\ninteractions in the atom. The first mechanism implies that electrons in\ndifferent orbitals are further apart, so that electron\u2013electron repulsion\nenergy is reduced. The second mechanism claims that the electrons in singly\noccupied orbitals are less effectively screened from the nucleus, resulting in\na contraction of the orbitals, which increases the electron\u2013nucleus attraction\nenergy.^19\n\nIn this article, we examine the electronic population in atoms within the\nframework of information theory^3 and we demonstrate that Hund\u2019s rule (Hund\u2019s\nfirst rule) is a direct consequence of the second law of information\ndynamics.^1 This requires that, at equilibrium in the ground state, electrons\noccupy the orbitals in such a way that their information entropy is minimum,\nor equivalently, the bit information content per electron is minimum.\n\n### A. Numerical calculations\n\nWe treat the two possible values of the secondary quantum spin number m_s of\nthe electrons in atoms, m_s = \u22121/2, +1/2, as two possible events, or as a two-\nletter message within Shannon\u2019s information theory framework. The secondary\nquantum spin number m_s is a very important parameter because it is the only\nquantity that distinguishes two electrons residing in the same orbital. Since\ntheir n, l, and m_l values are the same, their m_s must be different to\nfulfill Pauli\u2019s exclusion principle.\n\nWe will allocate to the two possible projections of the m_s the spin up \u2191 and\nspin down \u2193 states. In this context, the set of n = 2 independent and\ndistinctive information states is X = {\u2191, \u2193}, with a discrete probability\ndistribution P = {p_\u2191, p_\u2193}.\n\nHence, for any N electrons, we have N_\u2191 and N_\u2193 electrons, so that N = N_\u2191 +\nN_\u2193, and relation (1) gives the Shannon information entropy per electron spin,\nor the bit information content stored per electron spin, while relation (3)\ngives the total information entropy per N electrons. Hence, relation (1)\nbecomes\n\n(9)\n\nwhere p_\u2191 = N_\u2191/N and p_\u2193 = N_\u2193/N, which allows re-writing Eq. (9) as\n\n(10)\n\nSince the electronic populations are stable, then N is constant, and the\nminimum in the entropy of the information bearing states, S_Info corresponds\nto a minimum in Shannon\u2019s information entropy. We now consider the s, p, d,\nand f orbitals, and we analyze in detail the Shannon\u2019s information entropy of\neach possible distinctive electronic configuration, for any possible occupancy\nnumber of these orbitals. The maximum allowed value for the information\nentropy H(X) = IE is 1 bit, and the minimum possible value is 0 bits. We will\ndemonstrate that for each orbital, the configuration that has the lowest\nShannon information entropy, i.e., the lowest bit information content,\ncorresponds to the highest total spin quantum value. Hence, Hund\u2019s rule is, in\nfact, a consequence of the second law of infodynamics.\n\n### B. s-orbital\n\nThe s orbital can accommodate a maximum of N = 2 electrons. Figure 4 shows the\npossible electronic configurations of an s orbital. When N = 1, or N = 2, the\nIE = 1 bit in both cases, while the total spin quantum value is 0.5 and 0,\nrespectively. Since there are no other possible configurations, the case for\ns-orbital is rather trivial. Figure 5(a) shows a plot of the IE values vs the\ntotal spin quantum value, S for the s orbital.\n\nFIG. 4.\n\nView largeDownload slide\n\nRepresentation of all possible distinctive electronic populations of an s\norbital.\n\nFIG. 4.\n\nView largeDownload slide\n\nRepresentation of all possible distinctive electronic populations of an s\norbital.\n\nClose modal\n\nFIG. 5.\n\nView largeDownload slide\n\nCalculated IE values for (a) s orbital, (b) p orbital, (c) d orbital, and (d)\nf orbital. Data represent each possible distinct electronic configuration vs\nthe total spin quantum value, S. The data show categorically that IE is\nminimum when S is maximum is each case.\n\nFIG. 5.\n\nView largeDownload slide\n\nCalculated IE values for (a) s orbital, (b) p orbital, (c) d orbital, and (d)\nf orbital. Data represent each possible distinct electronic configuration vs\nthe total spin quantum value, S. The data show categorically that IE is\nminimum when S is maximum is each case.\n\nClose modal\n\n### C. p-orbital\n\nThe p orbital can accommodate a maximum of N = 6 electrons. Figure 6 shows the\nelectronic populations on the p orbital for all possible N values. We should\nmention that only distinct configurations have been represented in the\ndiagram. Any electronic arrangement that results in the same ratio of spin-up\nand spin-down electrons is not represented, as it would duplicate the results.\n\nFIG. 6.\n\nView largeDownload slide\n\nRepresentation of all possible distinctive electronic populations of a p\norbital. Configurations highlighted in green are the correct arrangements when\nmultiple states are possible for N = 2, 3, and 4, respectively.\n\nFIG. 6.\n\nView largeDownload slide\n\nRepresentation of all possible distinctive electronic populations of a p\norbital. Configurations highlighted in green are the correct arrangements when\nmultiple states are possible for N = 2, 3, and 4, respectively.\n\nClose modal\n\nSimilarly, configurations obtained by inverting all spins, i.e., mirror\nimages, result in the same IE values and are not considered to avoid\nduplications.\n\nFigure 5(b) shows the graph of the IE values vs the total spin quantum value\nfor all possible distinct occupancy cases of the p orbital. As shown in Fig.\n6, each time multiple arrangements are possible, as is the case for N = 2, 3,\nand 4, respectively, the maximum spin quantum value corresponds to the minimum\nIE value estimated using Eq. (10). For N = 2 and 3, the minimum IE is 0 in\neach case, while for N = 4, the minimum IE value is 0.811. To emphasize this,\nwe highlighted, in Fig. 6, the correct configurations that are required by\nHund\u2019s rule.\n\n### D. d-orbital\n\nWe now examine the case of the d orbital, which can accommodate a maximum of N\n= 10 electrons. Figure 7 shows the distinct electronic populations allowed on\nthe d orbital for all possible N values.\n\nFIG. 7.\n\nView largeDownload slide\n\nRepresentation of all possible distinctive electronic populations of a d\norbital. Configurations highlighted in green are the correct arrangements when\nmultiple states are possible for N = 2\u20138.\n\nFIG. 7.\n\nView largeDownload slide\n\nRepresentation of all possible distinctive electronic populations of a d\norbital. Configurations highlighted in green are the correct arrangements when\nmultiple states are possible for N = 2\u20138.\n\nClose modal\n\nFor N = 1, 9, and 10, only a single distinct arrangement is possible, while\nfor all the other N values, multiple electronic arrangements are allowed\nwithin Pauli\u2019s exclusion principle.\n\nFigure 5(c) shows the IE values vs the total spin quantum value for all\npossible distinct occupancy cases of the d orbital. The data indicates that\nthe maximum spin quantum value corresponds to the minimum IE value estimated\nusing Eq. (10). For each N value, we highlighted, in Fig. 7, the correct\nconfigurations that are required by Hund\u2019s rule. These all correspond exactly\nto the lowest IE value, reinforcing the validity of the second law of\ninfodynamics. The minimum IE value of 0 is achieved for N = 2, 3, 4, and 5.\nThe minimum IE values are IE = 0.65 for N = 6, IE = 0.863 for N = 7, and IE =\n0.954 for N = 8, respectively.\n\n### E. f-orbital\n\nFinally, we examine the f-orbital, which can accommodate a maximum of N = 14\nelectrons. Therefore, we have 14 possible groups, with N = 1, 13, and 14\nhaving only a single distinct electronic arrangement possible, while for all\nthe other N values, multiple electronic arrangements are allowed by Pauli\u2019s\nexclusion principle. Figure 8 shows the distinct electronic populations\nallowed on the f orbital for all possible N values.\n\nFIG. 8.\n\nView largeDownload slide\n\nRepresentation of all possible distinctive electronic populations of an f\norbital. Configurations highlighted in green are the correct arrangements\naccording to Hund\u2019s rule when multiple states are possible for N = 2\u201312.\n\nFIG. 8.\n\nView largeDownload slide\n\nRepresentation of all possible distinctive electronic populations of an f\norbital. Configurations highlighted in green are the correct arrangements\naccording to Hund\u2019s rule when multiple states are possible for N = 2\u201312.\n\nClose modal\n\nAgain, we highlighted the correct arrangements as dictated by Hund\u2019s rule, and\nwe calculated the IE values for all possible configurations. The minimum IE\nvalue of 0 is achieved for N = 2, 3, 4, 5, 6, and 7. For the remaining groups\nwith multiple electronic configurations, the minimum IE values are IE = 0.544\nfor N = 8, IE = 0.764 for N = 9, IE = 0.881 for N = 10, IE = 0.946 for N = 11,\nand IE = 0.98 for N = 12, respectively. The data show categorically that, in\nall cases, the minimum IE value corresponds to the maximum spin quantum value,\nS, so the second law of infodynamics appears to be the real driving force\nbehind Hund\u2019s rule.\n\n## V. SECOND LAW OF INFODYNAMICS IN COSMOLOGY\n\nThe universe can only be either finite/close or infinite/open. The current\nconsensus is that we live in an infinite universe that is in continuous\nexpansion. Regardless of whether the universe is finite or infinite, the\nthermodynamic laws are equally applicable. The first law of thermodynamics\nstates that energy can neither be created nor destroyed; it is conserved. The\nenergy in the universe can only be converted from one form to another, but\noverall, it remains constant. Using Clausius\u2019 sign convention, the\nmathematical differential form of the first law of thermodynamics is\n\n(11)\n\nwhere Q is the net heat energy supplied to the universe, W captures the work\ndone by the universe in all possible forms, and U represents the total\ninternal energy of matter and radiation in the universe.\n\nHowever, the universe does not exchange heat with anything, so if the universe\nis expanding adiabatically, then the first law becomes\n\n(12)\n\nWe now recall the relation that links heat to entropy, dQ = T \u00b7 dS, where S is\nthe total entropy of the universe and T is the temperature. Since T has a non-\nzero value as dictated by the third law of thermodynamics, and the average\ntemperature of the observable universe could, in fact, be considered to be 2.7\nK, we deduce that dS = 0. This implies that the total entropy of the universe\nmust be constant. This constant entropy does not violate the second law of\nthermodynamics, which allows the entropy to be constant over time or to\nincrease. However, in an expanding universe, the entropy will always increase\nbecause more possible microstates are being created via the expansion of the\nspace itself. Figure 9 shows a diagram of a physical system containing matter\nwhen the size of the system is in continuous expansion, while its physical\ncontent remains unchanged.\n\nFIG. 9.\n\nView largeDownload slide\n\nDiagram of a physical system under continuous expansion in time, resulting in\nentropy increasing.\n\nFIG. 9.\n\nView largeDownload slide\n\nDiagram of a physical system under continuous expansion in time, resulting in\nentropy increasing.\n\nClose modal\n\nJust as in our expanding universe, the space expansion in the schematic\nphysical system shown in Fig. 9 facilitates the emergence of more microstates,\nand the total entropy increases rapidly. If the universe does not expand, at\nsome point, the entropy will reach its maximum and the universe will achieve\nequilibrium.\n\nThis process allows for the physical entropy of the universe in the past to\nhave been at its maximum value when the universe would have been much smaller\nthan today and at near equilibrium. The evidence for this is the cosmic\nmicrowave background (CMB) radiation,^20 which is almost isotropic, having a\ntemperature of \u223c2.7 K in all directions,^21 and a very low-level temperature\nanisotropy (\u0394T/T \u223c 10^\u22125).^22,23 The time origin of this low level of\ntemperature anisotropy can be traced back to \u223c370 000 years after the big\nbang^24 when the universe was close to chemical and thermal equilibrium and\nthe density inhomogeneities were comparable to the temperature anisotropies\n(\u0394\u03c1/\u03c1 \u223c \u0394T/T \u223c 10^\u22125).\n\nHowever, in order to comply with the first law of thermodynamics and the\nadiabatic expansion, we just showed that the total entropy of the universe\nmust be constant. If this is the case, how can the physical entropy of our\nexpanding universe increase continuously? This is called the \u201cEntropic\nParadox\u201d and to solve it, there are only three possibilities:\n\n  * The laws of thermodynamics are not valid;\n\n  * The universe is not expanding;\n\n  * The entropy budget of the universe contains an unaccounted entropy term.\n\nThe readers would agree that possibilities (a) and (b) are out of the question\nas these are supported by undisputed empirical evidence. Therefore, we are\nleft with the search for another entropy term responsible for the initial high\nentropy of the universe. This entropy term must also balance the total entropy\nbudget of the universe in order to ensure that the overall entropy remains\nconstant over time, despite the evident increase in physical entropy that we\ncan observe in the expanding universe.\n\nIn this paper, we propose that the missing entropy term is the entropy\nassociated with the information content of the universe.\n\nLet us write the total entropy of the universe, S, as the sum of the physical\nentropy and the information entropy,\n\n(13)\n\nBy differentiating (13), we get\n\n(14)\n\nImposing the dS = 0 condition, and taking a time derivative, we obtain\n\n(15)\n\nSince dS_Phys/dt \u2265 0, i.e., physical entropy always increases over time\naccording to the second law of thermodynamics and according to empirical\nobservations, then the increase in the physical entropy must be balanced by\nthe decrease in the information entropy over the same time interval, so ,\nwhich means\n\n(16)\n\nThe relation (16) is identical to relation (4), and it is exactly the second\nlaw of infodynamics, requiring that the entropy of the information states must\ndecrease over time. Hence, the second law of infodynamics appears to be\nuniversally applicable and is, in fact, a cosmological necessity. It is\nimportant to realize that in order for the overall entropy of the universe to\nremain constant, the absolute values of physical entropy and information\nentropy do not have to be equal. Only their absolute change over time must be\nequal, in order to ensure a constant overall entropy of the universe.\n\n## VI. SECOND LAW OF INFODYNAMICS AND SYMMETRIES\n\nSymmetry is a mathematical concept in which a certain property, for instance,\nthe geometrical shape of an object, is preserved under certain transformations\napplied to the object. Such transformations include translations, rotations,\nreflections, and more complex operations combining these. In each case, the\nobject remains invariant upon transformation. In the context of Euclidian\ngeometry, these transformations are called symmetry operations. A symmetry\noperation is the movement of an object into an equivalent and\nindistinguishable orientation that is carried around a symmetry element. A\nsymmetry element is a point, line, or plane about which a symmetry operation\nis carried out. The classical group theory is the mathematical tool for the\nstudy of symmetry, describing the structure of transformations that map\nobjects to themselves exactly.\n\nHowever, symmetry is not merely a mathematical concept. It transcends\ndisciplines, connecting mathematics, chemistry, biology, and physics, and\nappears to be a fundamental property of the universe.\n\nThis is evidenced by everything around us, from the elegant symmetrical\npatterns of snowflakes to the fundamental symmetries governing subatomic\nparticles. Symmetry occurs at all scales, playing a pivotal role in the\nstructure and behavior of matter in the universe. Figure 10 shows a few\nexamples of amazing symmetries manifesting in nature.\n\nFIG. 10.\n\nView largeDownload slide\n\nA few examples of the abundance of symmetry in the universe.\n\nFIG. 10.\n\nView largeDownload slide\n\nA few examples of the abundance of symmetry in the universe.\n\nClose modal\n\nThis abundance of symmetry in the natural world begs the question: Why does\nsymmetry dominate all systems in the universe instead of asymmetry? After all,\nthe entropic evolution of the universe tends to a higher entropy state, yet\neverything in nature appears to prefer high symmetry and a high degree of\norder.\n\nHere, we explore the mathematical underpinnings of symmetry and its crucial\nsignificance in the context of the second law of infodynamics. We demonstrate\na unique observation that a high symmetry corresponds to a low information\nentropy state, which is exactly what the second law of infodynamics requires.\nHence, this remarkable observation appears to explain why symmetry dominates\nin the universe: it is due to the second law of information dynamics.\n\nBefore we proceed to our proof, it is useful to establish a way of measuring\nthe symmetry of an object quantitatively. In other words, how much symmetry\ndoes a shape have? One accepted method of measuring the symmetry of an object\nis by counting the number of symmetry operations that one can carry out on the\nobject. The more symmetry operations a shape has, the more symmetric it is.\n\nSince the symmetry operations are carried out around the symmetry elements, we\npropose to quantify the symmetry of a shape by counting its number of symmetry\nelements instead of counting the number of symmetry operations. For example, a\nperfect square has eight symmetry operations (four rotations and four\nreflections) and five symmetry elements (one axis of rotation and four axes of\nreflection).\n\nOur main objective is to describe the relationship between the symmetry of an\nobject, determined by the number of its symmetry elements (SE) and its\ninformation entropy (IE).\n\nIn order to do this, let us consider a range of simple Euclidian 2D geometric\nshapes. We start with an ordinary triangle, defined by three sides of length\na, b, and c and three corresponding angles \u03b1, \u03b2, and \u03b3, respectively (see Fig.\n11). These parameters are a unique representation of the shape, as there is no\nother possible way of forming a triangle that looks different using this set\nof parameters.\n\nFIG. 11.\n\nView largeDownload slide\n\nRegular triangle with no symmetry elements.\n\nFIG. 11.\n\nView largeDownload slide\n\nRegular triangle with no symmetry elements.\n\nClose modal\n\nWithin Shannon\u2019s information theory framework, we define the set of six\ndistinct characters, n = N = 6, , and a probability distribution on X. The\nprobabilities of the set are . The average information per character, or the\nnumber of bits of information per character for this set, is given by Eq. (1),\n.\n\nThis ordinary triangle has no symmetry, and accordingly, it has zero symmetry\nelements, so SE = 0.\n\nWe now examine an isosceles triangle, as shown in Fig. 12. This shape has one\nsymmetry element (a reflection axis), so SE = 1 and it is fully defined by the\nset of four distinct characters n = 4, , and a probability distribution . In\nthis case, the IE is\n\nFinally, we are examining the triangle shape that has the highest symmetry,\nthe equilateral triangle (see Fig. 13). The equilateral triangle has four\nsymmetry elements, SE = 4 (three reflection axes and one rotation axis), and\nit is fully defined by the set of two distinct characters n = 2, , and a\nprobability distribution . In this case, the IE is\n\nExamining the relationship between the information entropy (IE) and the\nsymmetry elements (SE) of these triangles, we observe that the symmetry scales\ninversely proportionally with the information entropy.\n\nFIG. 12.\n\nView largeDownload slide\n\nSymmetry elements of an isosceles triangle.\n\nFIG. 12.\n\nView largeDownload slide\n\nSymmetry elements of an isosceles triangle.\n\nClose modal\n\nFIG. 13.\n\nView largeDownload slide\n\nSymmetry elements of an equilateral triangle.\n\nFIG. 13.\n\nView largeDownload slide\n\nSymmetry elements of an equilateral triangle.\n\nClose modal\n\nHigh symmetry = low information entropy This behavior is clearly emphasized in\nFig. 14, showing the IE vs SE for all possible triangle shapes.\n\nFIG. 14.\n\nView largeDownload slide\n\nInformation entropy vs number of symmetry elements of triangular shapes.\n\nFIG. 14.\n\nView largeDownload slide\n\nInformation entropy vs number of symmetry elements of triangular shapes.\n\nClose modal\n\nAlthough this was observed in the case of a single 2D geometric shape, we\npostulate that this is a universal behavior of symmetries. In order to\nconvince the reader, let us examine the case of quadrilaterals. There are\nseven possible geometries of a quadrilateral figure in terms of its possible\nsymmetries. Table I gives all seven possible geometries and their SE values.\nFor each geometry, we computed the IE value. The data are also summarized in\nFig. 15.\n\nTABLE I.\n\nSummarized results of the analysis performed on quadrilaterals.\n\n.| .| .| .| IE .  \n---|---|---|---|---  \nShape .| SE .| Set X of distinct characters .| Probabilities .| .  \n0| 3  \n1| 2.25  \n1| 2.25  \n1| 1.905  \n3| 1.5  \n3| 1.5  \n5| 1  \n  \n.| .| .| .| IE .  \n---|---|---|---|---  \nShape .| SE .| Set X of distinct characters .| Probabilities .| .  \n0| 3  \n1| 2.25  \n1| 2.25  \n1| 1.905  \n3| 1.5  \n3| 1.5  \n5| 1  \n  \nView Large\n\nFIG. 15.\n\nView largeDownload slide\n\nInformation entropy vs number of SE of quadrilaterals.\n\nFIG. 15.\n\nView largeDownload slide\n\nInformation entropy vs number of SE of quadrilaterals.\n\nClose modal\n\nAgain, the shape with the highest symmetry has the lowest information content.\n\nThe same analysis can be applied to any geometric figure, including 3D\ngeometries, producing the same results. In each case, the symmetry scales\ninversely with the information content.\n\nThis remarkable result demonstrates that the symmetries manifesting everywhere\nin nature, and in the entire universe, are a consequence of the second law of\ninformation dynamics, which requires the minimization of the information\nentropy in any system or process in the universe.\n\n## VII. CONCLUSIONS\n\nIn this study, we revisited the second law of infodynamics, first introduced\nin 2022.^1 The second law of infodynamics states that the information entropy\nof systems containing information states must remain constant or decrease over\ntime, reaching a certain minimum value at equilibrium. This is very\ninteresting because it is in total opposition to the second law of\nthermodynamics, which describes the time evolution of the physical entropy\nthat must increase up to a maximum value at equilibrium.\n\nWe showed that the second law of infodynamics is universally applicable to any\nsystem containing information states, including biological systems and digital\ndata. Remarkably, this indicates that the evolution of biological life tends\nin such a way that genetic mutations are not just random events as per the\ncurrent Darwinian consensus, but instead undergo genetic mutations according\nto the second law of infodynamics, minimizing their information entropy. This\ndiscovery has massive implications for genetic research, evolutionary biology,\ngenetic therapies, pharmacology, virology, and pandemic monitoring, to name a\nfew.\n\nHere, we also expanded the applicability of the second law of infodynamics to\nexplain phenomenological observations in atomic physics. In particular, we\ndemonstrated that the second law of infodynamics explains the rule followed by\nthe electrons to populate the atomic orbitals in multi-electron atoms, known\nas the Hund\u2019s rule. Electrons arrange themselves on orbitals, at equilibrium\nin the ground state, in such a way that their information entropy is always\nminimal.\n\nMost interesting is the fact that the second law of infodynamics appears to be\na cosmological necessity. Here, we re-derived this new physics law using\nthermodynamic considerations applied to an adiabatically expanding universe.\n\nFinally, one of the great mysteries of nature is: Why does symmetry dominate\nin the universe? has also been explained using the second law of infodynamics.\nUsing simple geometric shapes, we demonstrated that high symmetry always\ncorresponds to the lowest information entropy state, or lowest information\ncontent, explaining why everything in nature tends to symmetry instead of\nasymmetry.\n\nThe key question is now: \u201cWhat can we learn from the second law of\ninfodynamics and what is its meaning?\u201d\n\nThe second law of infodynamics essentially minimizes the information content\nassociated with any event or process in the universe. The minimization of the\ninformation really means an optimisation of the information content, or the\nmost effective data compression, as described in Shannon\u2019s information theory.\nThis behavior is fully reminiscent of the rules deployed in programming\nlanguages and computer coding. Since the second law of infodynamics appears to\nbe manifesting universally and is, in fact, a cosmological necessity, we could\nconclude that this points to the fact that the entire universe appears to be a\nsimulated construct. A super complex universe like ours, if it were a\nsimulation, would require a built-in data optimization and compression\nmechanism in order to reduce the computational power and the data storage\nrequirements. This is exactly what we are observing via empirical evidence all\naround us, including digital data, biological systems, atomistic systems,\nsymmetries, and the entire universe.\n\nAnother important aspect of the second law of infodynamics is the fact that it\nappears to validate the mass-energy-information equivalence principle\nformulated in 2019.^4 According to this principle, the information itself is\nnot just a mathematical construct or just physical, as postulated by\nLandauer^25 and experimentally demonstrated recently,^26\u201329 but it has a small\nmass and can be regarded as the fifth form of matter.^4 This principle has not\nbeen confirmed experimentally yet, and it has attracted a fair share of\nskepticism. Whether information is physical or not is irrelevant to this study\nbecause the second law of infodynamics is applicable regardless of whether\ninformation has mass or not. However, if information is physical (equivalent\nto mass and energy), then the second law of thermodynamics requires systems to\nevolve in such a way that the energy is minimized at equilibrium. Hence, a\nreduction in the information content, would translate into a reduction of\nmass-energy according to the mass-energy-information equivalence principle.\nTherefore, the second law of infodynamics is not just a cosmological\nnecessity, but since it is required to fulfill the second law of\nthermodynamics, we can conclude that this new physics law proves that\ninformation is indeed physical. The scientific evidence supporting the\nsimulated universe theory is also discussed in greater detail in a recently\npublished book. ^30\n\n## ACKNOWLEDGMENTS\n\nM.M.V. acknowledges financial support received for this research from the\nUniversity of Portsmouth and the Information Physics Institute. The author is\nalso deeply grateful to all his supporters and would like to acknowledge the\ngenerous contributions received to his research in the field of information\nphysics, from the following donors and crowd funding backers, listed in\nalphabetical order: Alban Frachisse, Alexandra Lifshin, Allyssa Sampson, Ana\nLeao-Mouquet, Andre Brannvoll, Andrews83, Angela Pacelli, Aric R. Bandy, Ariel\nSchwartz, Arne Michael Nielsen, Arvin Nealy, Ash Anderson, Barry Anderson,\nBenjamin Jakubowicz, Beth Steiner, Bruce McAllister, Caleb M. Fletcher, Chris\nBallard, Cincero Rischer, Colin Williams, Colyer Dupont, Cruciferous1, Daniel\nDawdy, Darya Trapeznikova, David Catuhe, Dirk Peeters, Dominik Cech, Kenneth\nPower, Eric Rippingale, Ethel Casey, Ezgame Workplace, Frederick H.\nSullenberger III, Fuyi Zhou, George Fletcher, Gianluca Carminati, Gordo Tek,\nGraeme Hewson, Graeme Kirk, Graham Wilf Taylor, Heath McStay, Heyang Han, Ian\nWickramasekera, Ichiro Tai, Inspired Designs LLC, Ivaylo Aleksiev, Jamie C.\nLiscombe, Jan Stehlak, Jason Huddleston, Jason Olmsted, Jennifer Newsom,\nJerome Taurines, John Jones, John Vivenzio, John Wyrzykowski, Josh Hansen,\nJoshua Deaton, Josiah Kuha, Justin Alderman, Kamil Koper, Keith Baton, Keith\nTrack, Kristopher Bagocius, Land Kingdom, Lawrence Zehnder, Lee Fletcher, Lev\nX, Linchuan Wang, Liviu Zurita, Loraine Haley, Manfred Weltenberg, Mark Matt\nHarvey-Nawaz, Matthew Champion, Mengjie Ji, Michael Barnstijn, Michael Legary,\nMichael Stattmann, Michelle A. Neeshan, Michiel van der Bruggen, Molly R.\nMcLaren, Mubarrat Mursalin, Nick Cherbanich, Niki Robinson, Norberto Guerra\nPallares, Olivier Climen, Pedro Decock, Piotr Martyka, Ray Rozeman, Raymond\nO\u2019Neill, Rebecca Marie Fraijo, Robert Montani, Shenghan Chen, Sova Novak,\nSteve Owen Troxel, Sylvain Laporte, Tam\u00e1s Tak\u00e1cs, Tilo Bohnert, Tomasz Sikora,\nTony Koscinski, Turker Turken, Walter Gabrielsen III, Will Strinz, William\nBeecham, William Corbeil, Xinyi Wang, Yanzhao Wu, Yves Permentier, Zahra\nMurad, and Ziyan Hu.\n\n## AUTHOR DECLARATIONS\n\n### Conflict of Interest\n\nThe author has no conflicts to disclose.\n\n### Author Contributions\n\nMelvin M. Vopson: Conceptualization (lead); Formal analysis (lead); Funding\nacquisition (lead); Investigation (lead); Methodology (lead); Writing \u2013\noriginal draft (lead); Writing \u2013 review & editing (lead).\n\n## DATA AVAILABILITY\n\nThe numerical data associated with this work is available within this\nmanuscript. The RNA sequences used in this study are freely available from\nRefs. 6 and 9\u201314.\n\n## REFERENCES\n\n1.\n\nM. M.\n\nVopson\n\nand\n\nS.\n\nLepadatu\n\n, \u201c\n\nThe second law of information dynamics\n\n,\u201d\n\nAIP Adv.\n\n12\n\n,\n\n075310\n\n(\n\n2022\n\n).\n\nhttps://doi.org/10.1063/5.0100358\n\nGoogle Scholar\n\nCrossref\n\nSearch ADS\n\n2.\n\nM. M.\n\nVopson\n\n, \u201c\n\nA possible information entropic law of genetic mutations\n\n,\u201d\n\nAppl. Sci.\n\n12\n\n,\n\n6912\n\n(\n\n2022\n\n).\n\nhttps://doi.org/10.3390/app12146912\n\nGoogle Scholar\n\nCrossref\n\nSearch ADS\n\n3.\n\nC. E.\n\nShannon\n\n, \u201c\n\nA mathematical theory of communication\n\n,\u201d\n\nBell Syst. Tech. J.\n\n27\n\n,\n\n379\n\n\u2013\n\n423\n\n(\n\n1948\n\n).\n\nhttps://doi.org/10.1002/j.1538-7305.1948.tb01338.x\n\nGoogle Scholar\n\nCrossref\n\nSearch ADS\n\n4.\n\nM. M.\n\nVopson\n\n, \u201c\n\nThe mass-energy-information equivalence principle\n\n,\u201d\n\nAIP Adv.\n\n9\n\n(\n\n9\n\n),\n\n095206\n\n(\n\n2019\n\n).\n\nhttps://doi.org/10.1063/1.5123794\n\nGoogle Scholar\n\nCrossref\n\nSearch ADS\n\n5.\n\nS.\n\nLepadatu\n\n, \u201c\n\nMicromagnetic Monte Carlo method with variable magnetization length based on\nthe Landau\u2013Lifshitz\u2013Bloch equation for computation of large-scale\nthermodynamic equilibrium states\n\n,\u201d\n\nJ. Appl. Phys.\n\n130\n\n,\n\n163902\n\n(\n\n2021\n\n).\n\nhttps://doi.org/10.1063/5.0059745\n\nGoogle Scholar\n\nCrossref\n\nSearch ADS\n\n6.\n\nSee\n\nhttps://www.ncbi.nlm.nih.gov/nuccore/MN908947\n\nfor the RNA sequence of MN908947\n\n.\n\n7.\n\nSee https://sourceforge.net/projects/information-entropy-spectrum/ for GENIES\nsoftware free download.\n\n8.\n\nM. M.\n\nVopson\n\nand\n\nS. C.\n\nRobson\n\n, \u201c\n\nA new method to study genome mutations using the information entropy\n\n,\u201d\n\nPhysica A\n\n584\n\n,\n\n126383\n\n(\n\n2021\n\n).\n\nhttps://doi.org/10.1016/j.physa.2021.126383\n\nGoogle Scholar\n\nCrossref\n\nSearch ADS\n\n9.\n\nSee\n\nhttps://www.ncbi.nlm.nih.gov/nuccore/MT956915\n\nfor the RNA sequence of MT956915\n\n.\n\n10.\n\nSee\n\nhttps://www.ncbi.nlm.nih.gov/nuccore/OM098426\n\nfor the RNA sequence of OM098426\n\n.\n\n11.\n\nSee\n\nhttps://www.ncbi.nlm.nih.gov/nuccore/MW679505\n\nfor the RNA sequence of MW679505\n\n.\n\n12.\n\nSee\n\nhttps://www.ncbi.nlm.nih.gov/nuccore/OK546282\n\nfor the RNA sequence of OK546282\n\n.\n\n13.\n\nSee\n\nhttps://www.ncbi.nlm.nih.gov/nuccore/OK104651\n\nfor the RNA sequence of OK104651\n\n.\n\n14.\n\nSee\n\nhttps://www.ncbi.nlm.nih.gov/nuccore/OL351371\n\nfor the RNA sequence of OL351371\n\n.\n\n15.\n\nD. L.\n\nKacian\n\n,\n\nD. R.\n\nMills\n\n,\n\nF. R.\n\nKramer\n\n, and\n\nS.\n\nSpiegelman\n\n, \u201c\n\nA replicating RNA molecule suitable for a detailed analysis of extracellular\nevolution and replication\n\n,\u201d\n\nProc. Natl. Acad. Sci. U. S. A.\n\n69\n\n(\n\n10\n\n),\n\n3038\n\n\u2013\n\n3042\n\n(\n\n1972\n\n).\n\nhttps://doi.org/10.1073/pnas.69.10.3038\n\nGoogle Scholar\n\nCrossref\n\nSearch ADS\n\nPubMed\n\n16.\n\nW.\n\nPauli\n\n, \u201c\n\n\u00dcber den zusammenhang des abschlusses der elektronengruppen im atom mit der\nkomplexstruktur der spektren\n\n,\u201d\n\nZ. Phys.\n\n31\n\n(\n\n1\n\n),\n\n765\n\n\u2013\n\n783\n\n(\n\n1925\n\n).\n\nhttps://doi.org/10.1007/BF02980631\n\nGoogle Scholar\n\nCrossref\n\nSearch ADS\n\n17.\n\nG. L.\n\nMiessler\n\nand\n\nD. A.\n\nTarr\n\n,\n\nInorganic Chemistry\n\n,\n\n2nd ed.\n\n(\n\nPrentice-Hall\n\n,\n\n1999\n\n), pp.\n\n358\n\n\u2013\n\n360\n\n.\n\nGoogle Scholar\n\n18.\n\nI. N.\n\nLevine\n\n,\n\nQuantum Chemistry\n\n,\n\n4th ed.\n\n(\n\nPrentice-Hall\n\n,\n\n1991\n\n), pp.\n\n303\n\n\u2013\n\n330\n\n.\n\nGoogle Scholar\n\n19.\n\nR. J.\n\nBoyd\n\n, \u201c\n\nA quantum mechanical explanation for Hund\u2019s multiplicity rule\n\n,\u201d\n\nNature\n\n310\n\n,\n\n480\n\n\u2013\n\n481\n\n(\n\n1984\n\n).\n\nhttps://doi.org/10.1038/310480a0\n\nGoogle Scholar\n\nCrossref\n\nSearch ADS\n\n20.\n\nA. A.\n\nPenzias\n\nand\n\nR. W.\n\nWilson\n\n, \u201c\n\nA measurement of excess antenna temperature at 4080 Mc/s\n\n,\u201d\n\nAstrophys. J.\n\n142\n\n(\n\n1\n\n),\n\n419\n\n\u2013\n\n421\n\n(\n\n1965\n\n).\n\nhttps://doi.org/10.1086/148307\n\nGoogle Scholar\n\nCrossref\n\nSearch ADS\n\n21.\n\nD. J.\n\nFixsen\n\n, \u201c\n\nThe temperature of the cosmic microwave background\n\n,\u201d\n\nAstrophys. J.\n\n707\n\n(\n\n2\n\n),\n\n916\n\n\u2013\n\n920\n\n(\n\n2009\n\n).\n\nhttps://doi.org/10.1088/0004-637X/707/2/916\n\nGoogle Scholar\n\nCrossref\n\nSearch ADS\n\n22.\n\nE. R.\n\nHarrison\n\n, \u201c\n\nFluctuations at the threshold of classical cosmology\n\n,\u201d\n\nPhys. Rev. D\n\n1\n\n(\n\n10\n\n),\n\n2726\n\n\u2013\n\n2730\n\n(\n\n1970\n\n).\n\nhttps://doi.org/10.1103/PhysRevD.1.2726\n\nGoogle Scholar\n\nCrossref\n\nSearch ADS\n\n23.\n\nP. J. E.\n\nPeebles\n\nand\n\nJ. T.\n\nYu\n\n, \u201c\n\nPrimeval adiabatic perturbation in an expanding universe\n\n,\u201d\n\nAstrophys. J.\n\n162\n\n,\n\n815\n\n\u2013\n\n836\n\n(\n\n1970\n\n).\n\nhttps://doi.org/10.1086/150713\n\nGoogle Scholar\n\nCrossref\n\nSearch ADS\n\n24.\n\nD. N.\n\nSpergel\n\n,\n\nL.\n\nVerde\n\n,\n\nH. V.\n\nPeiris\n\n,\n\nE.\n\nKomatsu\n\n,\n\nM. R.\n\nNolta\n\n,\n\nC. L.\n\nBennett\n\n,\n\nM.\n\nHalpern\n\n,\n\nG.\n\nHinshaw\n\net al, \u201c\n\nFirst-year Wilkinson microwave anisotropy probe (WMAP) observations:\nDetermination of cosmological parameters\n\n,\u201d\n\nAstrophys. J., Suppl. Ser.\n\n148\n\n(\n\n1\n\n),\n\n175\n\n\u2013\n\n194\n\n(\n\n2003\n\n).\n\nhttps://doi.org/10.1086/377226\n\nGoogle Scholar\n\nCrossref\n\nSearch ADS\n\n25.\n\nR.\n\nLandauer\n\n, \u201c\n\nIrreversibility and heat generation in the computing process\n\n,\u201d\n\nIBM J. Res. Dev.\n\n5\n\n(\n\n3\n\n),\n\n183\n\n\u2013\n\n191\n\n(\n\n1961\n\n).\n\nhttps://doi.org/10.1147/rd.53.0183\n\nGoogle Scholar\n\nCrossref\n\nSearch ADS\n\n26.\n\nJ.\n\nHong\n\n,\n\nB.\n\nLambson\n\n,\n\nS.\n\nDhuey\n\n, and\n\nJ.\n\nBokor\n\n, \u201c\n\nExperimental test of Landauer\u2019s principle in single-bit operations on\nnanomagnetic memory bits\n\n,\u201d\n\nSci. Adv.\n\n2\n\n(\n\n3\n\n),\n\ne1501492\n\n(\n\n2016\n\n).\n\nhttps://doi.org/10.1126/sciadv.1501492\n\nGoogle Scholar\n\nCrossref\n\nSearch ADS\n\nPubMed\n\n27.\n\nR.\n\nGaudenzi\n\n,\n\nE.\n\nBurzur\u00ed\n\n,\n\nS.\n\nMaegawa\n\n,\n\nH. S. J.\n\nvan der Zant\n\n, and\n\nF.\n\nLuis\n\n, \u201c\n\nQuantum Landauer erasure with a molecular nanomagnet\n\n,\u201d\n\nNat. Phys.\n\n14\n\n,\n\n565\n\n\u2013\n\n568\n\n(\n\n2018\n\n).\n\nhttps://doi.org/10.1038/s41567-018-0070-7\n\nGoogle Scholar\n\nCrossref\n\nSearch ADS\n\n28.\n\nA.\n\nB\u00e9rut\n\n,\n\nA.\n\nArakelyan\n\n,\n\nA.\n\nPetrosyan\n\n,\n\nS.\n\nCiliberto\n\n,\n\nR.\n\nDillenschneider\n\n, and\n\nE.\n\nLutz\n\n, \u201c\n\nExperimental verification of Landauer\u2019s principle linking information and\nthermodynamics\n\n,\u201d\n\nNature\n\n483\n\n,\n\n187\n\n\u2013\n\n189\n\n(\n\n2012\n\n).\n\nhttps://doi.org/10.1038/nature10872\n\nGoogle Scholar\n\nCrossref\n\nSearch ADS\n\nPubMed\n\n29.\n\nY.\n\nJun\n\n,\n\nM.\n\nGavrilov\n\n, and\n\nJ.\n\nBechhoefer\n\n, \u201c\n\nHigh-precision test of Landauer\u2019s principle in a feedback trap\n\n,\u201d\n\nPhys. Rev. Lett.\n\n113\n\n(\n\n19\n\n),\n\n190601\n\n(\n\n2014\n\n).\n\nhttps://doi.org/10.1103/physrevlett.113.190601\n\nGoogle Scholar\n\nCrossref\n\nSearch ADS\n\nPubMed\n\n30.\n\nM.M.\n\nVopson\n\nPhD,\n\nReality Reloaded: The Scientific Case for a Simulated Universe\n\n(\n\nIPI Publishing\n\n,\n\nHampshire, UK\n\n,\n\n2023\n\n), pp.\n\n1\n\n\u2013\n\n140\n\nhttps://doi.org/10.59973/rrtscfasu.\n\nGoogle Scholar\n\nCrossref\n\nSearch ADS\n\n\u00a9 2023 Author(s). All article content, except where otherwise noted, is\nlicensed under a Creative Commons Attribution (CC BY) license\n(http://creativecommons.org/licenses/by/4.0/).\n\n101,869 Views\n\n4 Crossref\n\nView Metrics\n\n\u00d7\n\n### Citing articles via\n\nGoogle Scholar\n\nCrossRef (4)\n\n### Submit your article\n\n### Sign up for alerts\n\n  * Most Read\n  * Most Cited\n\nThe second law of infodynamics and its implications for the simulated universe\nhypothesis\n\nMelvin M. Vopson\n\nSecond law of information dynamics\n\nMelvin M. Vopson, S. Lepadatu\n\nExperimental protocol for testing the mass\u2013energy\u2013information equivalence\nprinciple\n\nMelvin M. Vopson\n\n  * Online ISSN 2158-3226\n\n### Resources\n\n  * For Researchers\n  * For Librarians\n  * For Advertisers\n  * Our Publishing Partners\n\n### Explore\n\n  * Journals\n  * Physics Today\n  * Conference Proceedings\n  * Books\n  * Special Topics\n  * Publishers\n\n### pubs.aip.org\n\n  * About\n  * User Guide\n  * Contact Us\n  * Register\n  * Help\n  * Privacy Policy\n  * Terms of Use\n\n### Connect with AIP Publishing\n\n  * Facebook\n  * LinkedIn\n  * Twitter\n  * YouTube\n\n  * \u00a9 Copyright 2024 AIP Publishing LLC\n\nClose Modal\n\nClose Modal\n\n##### This Feature Is Available To Subscribers Only\n\nSign In or Create an Account\n\nClose Modal\n\nClose Modal\n\n", "frontpage": false}
