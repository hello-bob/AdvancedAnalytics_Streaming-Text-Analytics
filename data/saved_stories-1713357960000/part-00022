{"aid": "40060441", "title": "Advanced Signal Processing: A Concise Guide", "url": "https://www.accessengineeringlibrary.com/content/book/9781260458930", "domain": "accessengineeringlibrary.com", "votes": 1, "user": "teleforce", "posted_at": "2024-04-17 04:33:35", "comments": 0, "source_title": "Advanced Signal Processing: A Concise Guide", "source_text": "Advanced Signal Processing: A Concise Guide | McGraw-Hill Education - Access Engineering\n\nSkip to main content\n\nMenu Menu\n\nRequest subscription information\n\nMy account\n\nBrowse AccessEngineering content by...\n\nShow moreShow less\n\n# Advanced Signal Processing: A Concise Guide, 1st Edition\n\n  * Amir-Homayoon Najmi\n  * Todd K. Moon\n\nISBN:\n\n9781260458930\n\nPublication Date & Copyright:\n\n2020\n\nMcGraw Hill\n\nA comprehensive introduction to the mathematical principles and algorithms in\nstatistical signal processing and modern neural networks.\n\nThis text is an expanded version of a graduate course on advanced signal\nprocessing at the Johns Hopkins University Whiting School program for\nprofessionals, with students from electrical engineering,...\n\nA comprehensive introduction to the mathematical principles and algorithms in\nstatistical signal processing and modern neural networks.\n\nThis text is an expanded version of a graduate course on advanced signal\nprocessing at the Johns Hopkins University Whiting School program for\nprofessionals, with students from electrical engineering, physics, computer\nand data science, and mathematics backgrounds. It covers the theory underlying\napplications in statistical signal processing, including spectral estimation,\nlinear prediction, adaptive filters, and optimal processing of uniform spatial\narrays. Unique among books on the subject, it also includes a comprehensive\nintroduction to modern neural networks with examples in time series\nforecasting and image classification.\n\nCoverage includes:\n\n\u2022 Mathematical structures of signal spaces and matrix factorizations\n\n\u2022 Linear time-invariant systems and transforms\n\n\u2022 Least squares filters\n\n\u2022 Random variables, estimation theory, and random processes\n\n\u2022 Spectral estimation and autoregressive signal models\n\n\u2022 Linear prediction and adaptive filters\n\n\u2022 Optimal processing of linear arrays\n\n\u2022 Neural networks\n\nShow more Show less\n\n  * Summary PDF\n\n  * Annotate\n\n  * Table of Contents\n  * Resources (1)\n\n  * Overview\n  * A About the Authors\n  * B Dedication\n  * C List of Figures\n  * D List of Tables\n  * E Acronyms\n  * F Preface\n  * G Acknowledgments\n  * View sub-sections\n\n1 Mathematical Structures of Signal Spaces\n\n    * 1.1 Introduction\n    * 1.2 Vector Spaces, Norms, and Inner Products\n    * 1.3 Orthonormal Vectors and the Gram-Schmidt Method\n    * 1.4 Complete and Orthonormal Bases\n    * 1.5 Linear Operators in Function Spaces\n    * 1.6 Matrix Determinant, Eigenvectors, and Eigenvalues\n    * 1.7 Matrix Norms\n    * 1.8 Solutions to Ax = b\n    * 1.9 Projections in a Hilbert Space\n    * 1.10 The Prolate Spheroidal Functions\n    * 1.11 The Approximation Problem and the Orthogonality Principle\n    * 1.12 Orthogonal Projections and the Haar Scaling and Wavelet Functions\n    * 1.13 Multi-Resolution Analysis Subspaces and Discrete Orthogonal Wavelet Bases\n    * 1.14 Compressive Sensing\n\n  * View sub-sections\n\n2 Matrix Factorizations and the Least Squares Problem\n\n    * 2.1 Introduction\n    * 2.2 QR Factorization\n    * 2.3 QR Factorization Using Givens Rotations\n    * 2.4 QR Using Householder Reflections\n    * 2.5 QR Factorization and Full Rank Least Squares\n    * 2.6 Cholesky Factorization and Full Rank Least Squares\n    * 2.7 Singular Value Decomposition (SVD)\n    * 2.8 SVD and Reduced Rank Approximation\n    * 2.9 SVD and Matrix Subspaces\n    * 2.10 SVD: Full Rank Least Squares and Minimum Norm Solutions\n    * 2.11 Total Least Squares\n    * 2.12 SVD and the Orthogonal Procrustes Problem\n\n  * View sub-sections\n\n3 Linear Time-Invariant Systems and Transforms\n\n    * 3.1 Introduction\n    * 3.2 The Laplace Transform\n    * 3.3 Phase and Group Delay Response: Continuous Time\n    * 3.4 The Z Transform\n    * 3.5 Phase and Group Delay Response: Discrete Time\n    * 3.6 Minimum Phase and Front Loading Property\n    * 3.7 The Fourier Transform\n    * 3.8 The Short-Time Fourier Transform and the Spectrogram\n    * 3.9 The Discrete Time Fourier Transform\n    * 3.10 The Chirp Z Transform\n    * 3.11 Finite Convolutions\n    * 3.12 The Cepstrum\n    * 3.13 The Orthogonal Discrete Wavelet Transform\n    * 3.14 The Hilbert Transform Relations\n    * 3.15 The Analytic Signal and Instantaneous Frequency\n    * 3.16 Time-Frequency Distribution Functions\n\n  * View sub-sections\n\n4 Least Squares Filters\n\n    * 4.1 Introduction\n    * 4.2 Quadratic Minimization Problems\n    * 4.3 Frequency Domain Least Squares Filters\n    * 4.4 Time Domain Least Squares Shaping Filters\n    * 4.5 Gradient Descent Iterative Solution to Least Squares Filtering\n    * 4.6 Time Delay Estimation\n\n  * View sub-sections\n\n5 Random Variables and Estimation Theory\n\n    * 5.1 Real Random Variables and Random Vectors\n    * 5.2 Complex Random Variables and Random Vectors\n    * 5.3 Random Processes\n    * 5.4 Gaussian Random Variables and Random Vectors\n    * 5.5 Gram-Schmidt Decorrelation\n    * 5.6 Principal Components Analysis\n    * 5.7 The Karhunen-Lo\u00e9ve Transformation\n    * 5.8 Statistical Properties of the Least Squares Filter\n    * 5.9 Estimation of Random Variables\n    * 5.10 Jointly Gaussian Random Vectors, the Conditional Mean and Covariance\n    * 5.11 The Conditional Mean and the Linear Model\n    * 5.12 The Kalman Filter\n    * 5.13 Parameter Estimation and the Cramer-Rao Lower Bound\n    * 5.14 Linear MVU and Maximum Likelihood Estimators\n    * 5.15 Maximum Likelihood Estimate of the Parameter Vector of a Linear Model\n    * 5.16 Maximum Likelihood Estimate of Complex Amplitude of a Complex Sinusoid in Gaussian Noise\n    * 5.17 Maximum Likelihood Estimate of a First Order Gaussian Markov Process\n    * 5.18 Information Theory: Entropy and Mutual Information\n    * 5.19 Independent Components Analysis\n    * 5.20 Maximum Likelihood ICA\n\n  * View sub-sections\n\n6 WSS Random Processes\n\n    * 6.1 Auto-Correlation and the Power Spectral Density\n    * 6.2 Complex Sinusoids in Zero-Mean White Noise\n    * 6.3 The MUSIC Algorithm\n    * 6.4 Pisarenko Harmonic Decomposition (PHD)\n    * 6.5 The ESPRIT Algorithm\n    * 6.6 The Auto-Correlation Matrix for Time Reversed Signal Vectors\n\n  * View sub-sections\n\n7 Linear Systems and Stochastic Inputs\n\n    * 7.1 Filtered Random Processes\n    * 7.2 Detection of a Known Non-Random Signal in WSS Noise\n    * 7.3 Detection of a WSS Random Signal in WSS Random Noise\n    * 7.4 Canonical Factorization\n    * 7.5 The Continuous-Time Causal Wiener Filter\n    * 7.6 The Discrete-Time Causal Wiener Filter\n    * 7.7 The Causal Wiener Filter and the Kalman Filter\n    * 7.8 The Non-Causal Wiener Filter and the Coherence Function\n    * 7.9 Generalized Cross-Correlation and Time-Delay Estimation\n    * 7.10 Random Fields\n\n  * View sub-sections\n\n8 Power Spectral Density Estimation and Signal Models\n\n    * 8.1 Introduction\n    * 8.2 Ergodicity\n    * 8.3 Sample Estimates of Mean and Correlation Functions\n    * 8.4 The Periodogram\n    * 8.5 Statistical Properties of the Periodogram\n    * 8.6 Reducing the Periodogram Variance\n    * 8.7 The Multitaper Method\n    * 8.8 Example Applications of Classical Spectral Estimation\n    * 8.9 Minimum Variance Distortionless Spectral Estimator\n    * 8.10 Autoregressive Moving Average (ARMA) Signal Models\n    * 8.11 Autoregressive Signal Models\n    * 8.12 Maximum Entropy and the AR(P) Process\n    * 8.13 Spectral Flatness and the AR(P) Process\n    * 8.14 AR(P) Process Examples\n    * 8.15 The Levinson-Durbin Algorithm\n    * 8.16 The Relationship Between MVD and AR Spectra\n    * 8.17 Autoregressive Model of a Zero-Mean WSS Random Signal\n    * 8.18 Autoregressive Model of a Complex Sinusoid in White Noise\n    * 8.19 Autoregressive Model of Multiple Complex Sinusoids in White Noise\n    * 8.20 Resolution of AR Models\n    * 8.21 AR Model Parameter Estimation\n    * 8.22 Maximum Likelihood AR Parameter Estimation: the Auto-Correlation Method\n    * 8.23 Maximum Likelihood AR Parameter Estimation: the Covariance Method\n    * 8.24 Model Order Selection\n    * 8.25 Akaike Information Criterion\n    * 8.26 Bayesian Model Order Selection\n    * 8.27 Minimum Description Length\n\n  * View sub-sections\n\n9 Discrete Time Wiener Filter and Linear Prediction\n\n    * 9.1 Introduction\n    * 9.2 The Discrete Time FIR Wiener Filter\n    * 9.3 The Forward Prediction Problem\n    * 9.4 The Backward Prediction Problem\n    * 9.5 Prediction Error Sequences and Partial Correlations\n    * 9.6 Lattice Filters\n    * 9.7 The Minimum Phase Property of the Forward PEF\n    * 9.8 AR Parameter Estimation: the Burg Method\n    * 9.9 Linear Prediction and Speech Recognition\n\n  * View sub-sections\n\n10 Adaptive Filters\n\n    * 10.1 Introduction\n    * 10.2 The LMS Algorithm\n    * 10.3 Complex LMS\n    * 10.4 Sign Adaptive LMS Algorithms\n    * 10.5 Normalized LMS Algorithm\n    * 10.6 Equalizing LMS Convergence Rates\n    * 10.7 Recursive Least Squares (RLS)\n    * 10.8 RLS Implementation\n\n  * View sub-sections\n\n11 Optimal Processing of Linear Arrays\n\n    * 11.1 Uniform Linear Array (ULA)\n    * 11.2 The Signal Model on a ULA\n    * 11.3 Beamforming\n    * 11.4 Optimal Beamforming\n    * 11.5 Performance of the Optimal Beamformer\n    * 11.6 Optimal Beamforming in Practice\n    * 11.7 Recursive Methods in SMI Beamforming\n    * 11.8 PCA and Dominant Mode Rejection (DMR) Beamforming\n    * 11.9 Direction of Arrival (DOA) Estimation\n\n  * View sub-sections\n\n12 Neural Networks\n\n    * 12.1 Introduction\n    * 12.2 The Perceptron\n    * 12.3 Fully Connected Feed Forward Neural Networks\n    * 12.4 The Backpropagation Algorithm\n    * 12.5 Loss Functions in Neural Network Training\n    * 12.6 Gradient Descent Variants\n    * 12.7 Single Hidden Layer and Multiple Hidden Layers Neural Networks\n    * 12.8 Mini-Batch Training and Normalization\n    * 12.9 Network Initialization\n    * 12.10 Regularization\n    * 12.11 Convolutional Neural Networks (CNNs)\n    * 12.12 Time Series Classification with a CNN\n    * 12.13 Image Classification with a CNN\n    * 12.14 Recurrent Neural Networks (RNNs)\n    * 12.15 Unsupervised Learning\n    * 12.16 Generative Adversarial Networks\n    * 12.17 Perspective\n\n  * A References\n\nLoading ...\n\n## Related searches\n\nSearch AccessEngineering for other content tagged with these...\n\n### Subjects\n\n  * Artificial intelligence\n  * Data science\n  * Signal processing\n\n\u00a9 McGraw-Hill Education. All rights reserved. Any use is subject to the Terms\nof Use, Privacy Notice and copyright information.\n\nYour IP address is 128.140.102.183 Troubleshooter page\n\nScroll to the top of the page.\n\n\u2713\n\nThanks for sharing!\n\nAddToAny\n\nMore...\n\n#### Cite this book\n\nNajmi, Amir-Homayoon, and Todd K. Moon. 2020. Advanced Signal Processing: A\nConcise Guide. 1st ed. New York: McGraw Hill.\nhttps://www.accessengineeringlibrary.com/content/book/9781260458930\n\n##### Download as...\n\n  * RIS\n\n#### Share this book\n\nClick <Link> button to copy the content permanent URL.\n\nThis link can be shared with users that are connected to the\ninstitution\u2019s/school\u2019s network and they will automatically have access to the\ncontent.\n\nTo share a link in a Learning Management System (LMS) course (such as\nBlackboard, Canvas, Moodle, etc.) that will work for remote users as well as\nthose connected to the school\u2019s network:\n\n  1. 1\\. Contact customersuccess@mheducation.com to confirm that your LMS has been set up correctly in our system.\n\n     * \\- Provide your school name and the link for your course in the email.\n  2. 2\\. Click <Link> button to copy the content permanent URL and paste into your course.\n\nLink Share on social media FacebookLinkedIn\n\n#### Annotate with Hypothesis\n\nWe've teamed up with Hypothesis to provide a tool that integrates your\nAccessEngineering annotations with annotations you make elsewhere on the web.\n\nUse Hypothesis to hold discussions, read socially, organize your research, and\ntake personal notes on any web page or PDF.\n\n  1. Click \"Open Hypothesis\" to open the Hypothesis sidebar.\n  2. Sign up for or log into your free Hypothesis account.\n  3. Select some text and start annotating.\n  4. Select whether to keep your annotations private, share with a specific group, or share publicly.\n  5. Note: Viewing highlights and annotations within AccessEngineering is \u201coff\u201d by default. Click the \u201ceye\u201d icon at the left side of the hypothesis sidebar to show highlights inside AccessEngineering.\n\nFind out how to share comments within a class or a project team or take a look\nat the Quickstart Guides for Students and Faculty.\n\nAnnotations require a separate Hypothesis account and are not part of your\nAccessEngineering user account.\n\nWe use cookies on this site to enhance your user experience. By clicking any\nlink on this page you are giving your consent for us to set cookies.\n\n", "frontpage": false}
