{"aid": "40235247", "title": "One Pytest Marker to Track the Performance of Your Tests", "url": "https://codspeed.io/blog/one-pytest-marker-to-track-the-performance-of-your-tests", "domain": "codspeed.io", "votes": 1, "user": "adriencaccia", "posted_at": "2024-05-02 12:13:36", "comments": 0, "source_title": "One pytest Marker to Track the Performance of Your Tests - CodSpeed", "source_text": "One pytest Marker to Track the Performance of Your Tests - CodSpeed\n\nFeaturesBlogDocsPricingExplore\n\nLoginGet Started\n\nBack to blog\n\n# One pytest Marker to Track the Performance of Your Tests\n\nPosted on May 2nd, 2024 by\n\nAdrien Cacciaguerra\n\n@adriencaccia\n\nArthur Pastel\n\n@art049\n\nWhile unit tests have become a standard for ensuring that the code performs as\nexpected, they focus on checking the logic, not the performance. Yet,\nperformance regressions can be just as dangerous as functional bugs, putting\nyour whole software at risk. This is why performance should be checked early\nwhile testing the code, the icing on the cake being to check it in CI\nenvironments.\n\n## Effortless performance testing with pytest\n\nBenchmarking is a way of testing a block of code's performance. It is like a\ntest case but for performance. It will execute the code and measure how long\nit takes to run.\n\nLet's see how to implement that with pytest. First, install the pytest-\ncodspeed library to enable the benchmark marker and fixture:\n\n    \n    \n    pip install pytest-codspeed\n\nThen, you're ready to use the @pytest.mark.benchmark marker for measuring\nperformance. You can use it directly on a single test:\n\n    \n    \n    + import pytest + @pytest.mark.benchmark def test_my_fn(): inputs = gen_inputs() results = my_fn(inputs) assert results == \"expected_result\"\n\nBut you can also apply it at the module level by using the global pytestmark\nvariable, effectively enabling performance testing on all the tests contained\nwithin it:\n\n    \n    \n    import pytest pytestmark = pytest.mark.benchmark # The rest of the test cases are now performance tests as well\n\nYou can then run the performance tests locally to ensure that everything\nworks:\n\n    \n    \n    pytest tests/ --codspeed\n\n## Rely on your CI to avoid Spray and Pray\n\nWhile being a good starting point, isolated runs do not work for long-term\nperformance tracking and the risk of missing important performance changes is\njust too high. Regressions will surface when you less expect them and it's\nessential to automate these checks into your CI pipeline. This ensures that\nany performance degradation is caught automatically during the development\ncycle and builds a history of your codebase performance.\n\nUsing the CodSpeed test runner helps a lot to make the measurement extremely\nsteady. Our runner relies on CPU simulation, enabling us to separate the noisy\nneighbours (other VMs, workloads, users) from the precious workload you want\nto measure.\n\nA typical setup with the runner in GitHub Actions would be as simple as:\n\n    \n    \n    - uses: CodSpeedHQ/action@v2 with: run: pytest tests/ --codspeed\n\nThis setup not only runs your tests but also uploads the results to CodSpeed,\nwhere you can track performance over time.\n\nSample pull request report on CodSpeed\n\n## Measuring only what matters\n\nSometimes, you want more granularity in what is measured. For example, you do\nnot want to measure the time it took to generate the inputs for calling our\nfunction or the assertions after getting the result; and instead focus only on\nthe actual function call.\n\nWe can modify the unit test:\n\n    \n    \n    def test_my_fn(benchmark): inputs = gen_inputs() results = benchmark(my_fn, inputs) assert results == \"expected_result\"\n\nThis test uses the benchmark fixture to only measure the execution time of\nmy_fn. The fixture makes it easy to focus on what matters\u2014how long it takes\nyour function to execute under test conditions.\n\nUsing the benchmark fixture will automatically mark the test as a benchmark,\nwithout having to use the pytest.mark.benchmark marker.\n\n## Optimizing hot paths with Differential Profiling\n\nWhen you encounter a performance regression, your next mission(if you accept\nit\ud83e\udd35) is often to investigate and find what/who/when/why was this issue\nintroduced in the first place.\n\nThis is where differential profiling comes in handy. This allows to compare\ntwo execution profiles to find exactly what changed between two separate\nmeasurements.\n\nThe good news is CodSpeed automatically profiles your benchmark's code while\nmeasuring performance. So if you spot a regression, you'll have all the data\nto investigate:\n\nExecution profile\n\ntest_parse_pr\n\nparse_pr (92.89%)\n\nprepare_parsing_body (63.21%)\n\nparse_body (57.32%)\n\nparse_issue_fixed (25.49%)\n\nlog_metrics (19.64%)\n\nsend_event (8.01%)\n\nparse_title (28.67%)\n\nmodify_title (6.49%) (new)\n\nlog_metrics (21.33%)\n\nsend_event (8.70%)\n\n__create_fn__.<locals>.__init__ (1.32%)\n\nSlower\n\nFaster\n\nNew\n\nSample flame graph with regressions, improvements, and added code\n\n## Takeaways\n\nIntegrating performance testing into your development process with tools like\npytest and CodSpeed fosters a culture of continuous improvement. It ensures\nthat performance considerations are never an afterthought but a key component\nof your software development lifecycle from the ground up.\n\nTo see CodSpeed in action, you can check out open-source repositories using\nthe tool in the explore page. A lot of them are actually using the pytest\nintegration we just talked about, like pydantic and polars\n\nLast but certainly not least, shout out to patrick91, who pioneered this use\ncase and whose contributions have made it significantly easier for developers\nto incorporate benchmarking into their existing unit tests.\n\n## Resources\n\n  * pytest-codspeed: the plugin for pytest\n  * @codspeed/action: the GitHub Action to run benchmarks and generate flame graphs\n  * @codspeed/runner: the generic CodSpeed runner working with various CI providers\n  * CodSpeed Docs: Python Integration\n  * Pinpoint performance regressions with CI-Integrated differential profiling\n\nShare this:\n\n## Ready to bench?\n\nUnlock the full potential of your code today. Don't guess, just measure.\n\nGet started\n\nRequest a Demo\n\nResourcesHomePricingDocsBlogGitHub\n\nGetting StartedSample repositoryExplore repositoriesSupport\n\nAboutTwitterDiscordContact UsTerms of ServicePrivacy Policy\n\nCopyright \u00a9 2024 CodSpeed Technology SAS. All rights reserved.\n\n", "frontpage": false}
