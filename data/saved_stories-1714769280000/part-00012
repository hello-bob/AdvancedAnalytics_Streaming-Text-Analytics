{"aid": "40248266", "title": "Show HN: A comics-filled guide to AI Safety by Nicky Case and teenagers", "url": "https://aisafety.dance/", "domain": "aisafety.dance", "votes": 2, "user": "ncasenmare", "posted_at": "2024-05-03 14:43:50", "comments": 0, "source_title": "AI Safety for Fleshy Humans: a whirlwind tour", "source_text": "AI Safety for Fleshy Humans: a whirlwind tour\n\n  * AI Safety for Fleshy Humans: a whirlwind tour\n  * \ud83d\udca1 The Core Ideas of AI & AI Safety\n  * Part 1: The past, present, and possible futures\n  * Part 2: The problems\n  * Part 3: The proposed solutions\n  * \ud83e\udd14 (Optional flashcard review!)\n  * \ud83e\udd37\ud83c\udffb\u2640\ufe0f Five common misconceptions about AI Safety\n  * 1) No, AI Safety isn't a fringe concern by sci-fi weebs.\n  * 2) No, AI Risk is NOT about AI becoming \"sentient\" or \"conscious\" or gaining a \"will to power\".\n  * 3) No, AI Risk isn't necessarily extinction, SkyNet, or nanobots\n  * 4) Yes, folks worried about AI's downsides do recognize its upsides.\n  * 5) No, experts don't think current AIs are high-risk/reward.\n  * \ud83e\udd14 (Optional flashcard review #2!)\n  * \ud83e\udd18 Introduction, in Summary:\n\nDark Mode\n\nFont size: 19px\n\nFont type:\n\nshare on... w/e\n\n~17m\n\n#\n\nAI Safety for Fleshy Humans: a whirlwind tour\n\nAI Safety for Fleshy Humans\n\nby Nicky Case & Hack Club\n\nintroduction intro\n\npart 1 past & future part 1\n\npart 2 problems part 2\n\npart 3 solutions? part 3\n\nconclusion outro\n\ntable of contents\n\nchange style \ud83d\ude0e\n\nThe AI debate is actually 100 debates in a trenchcoat.\n\nWill artificial intelligence (AI) help us cure all disease, and build a post-\nscarcity world full of flourishing lives? Or will AI help tyrants surveil and\nmanipulate us further? Are the main risks of AI from accidents, abuse by bad\nactors, or a rogue AI itself becoming a bad actor? Is this all just hype? Why\ncan AI imitate any artist's style in a minute, yet gets confused drawing more\nthan 3 objects? Why is it hard to make AI robustly serve humane values, or\nrobustly serve any goal? What if an AI learns to be more humane than us? What\nif an AI learns humanity's inhumanity, our prejudices and cruelty? Are we\nheaded for utopia, dystopia, extinction, a fate worse than extinction, or \u2014\nthe most shocking outcome of all \u2014 nothing changes? Also: will an AI take my\njob?\n\n...and many more questions.\n\nAlas, to understand AI with nuance, we must understand lots of technical\ndetail... but that detail is scattered across hundreds of articles, buried\nsix-feet-deep in jargon.\n\nSo, I present to you:\n\nThis 3-part series is your one-stop-shop to understand the core ideas of AI &\nAI Safety* \u2014 explained in a friendly, accessible, and slightly opinionated\nway!\n\n(* Related phrases: AI Risk, AI X-Risk, AI Alignment, AI Ethics, AI Not-Kill-\nEveryone-ism. There is no consensus on what these phrases do & don't mean, so\nI'm just using \"AI Safety\" as a catch-all.)\n\nThis series will also have comics starring a Robot Catboy Maid. Like so:\n\n[tour guide voice] And to your right \ud83d\udc49, you'll see buttons for the Table of\nContents, changing this webpage's style, and a reading-time-remaining clock.\n\nThis series was first published on May 2024. Intro & Part One are out, Part\nTwo & Three will be in a few months. OPTIONAL: If you'd like to be notified on\ntheir release, signup below!\ud83d\udc47 You will not be spammed with other stuff, just\nthe two notification emails. (Buuuuut, [podcast sponsor voice] if you're in\nhigh school or earlier, and interested in AI/code/engineering, consider\nchecking the box to learn more about Hack Club! P.S: There's free stickers~~~\n\u2728)\n\nAnyway, [tour guide voice again] before we hike through the rocky terrain of\nAI & AI Safety, let's take a 10,000-foot look of the land:\n\n##\n\n\ud83d\udca1 The Core Ideas of AI & AI Safety\n\nIn my opinion, the main problems in AI and AI Safety come down to two core\nconflicts:\n\nNote: What \"Logic\" and \"Intuition\" are will be explained more rigorously in\nPart One. For now: Logic is step-by-step cognition, like solving math\nproblems. Intuition is all-at-once recognition, like seeing if a picture is of\na cat. \"Intuition and Logic\" roughly map onto \"System 1 and 2\" from cognitive\nscience.^[1]^[2] (\ud83d\udc48 hover over these footnotes! they expand!)\n\nAs you can tell by the \"scare\" \"quotes\" on \"versus\", these divisions ain't\nreally so divided after all...\n\nHere's how these conflicts repeat over this 3-part series:\n\n###\n\nPart 1: The past, present, and possible futures\n\nSkipping over a lot of detail, the history of AI is a tale of Logic vs\nIntuition:\n\nBefore 2000: AI was all logic, no intuition.\n\nThis was why, in 1997, AI could beat the world champion at chess... yet no AIs\ncould reliably recognize cats in pictures.^[3]\n\n(Safety concern: Without intuition, AI can't understand common sense or humane\nvalues. Thus, AI might achieve goals in logically-correct but undesirable\nways.)\n\nAfter 2000: AI could do \"intuition\", but had very poor logic.\n\nThis is why generative AIs (as of current writing, May 2024) can dream up\nwhole landscapes in any artist's style... yet gets confused drawing more than\n3 objects. (\ud83d\udc48 click this text! it also expands!)\n\n(Safety concern: Without logic, we can't verify what's happening in an AI's\n\"intuition\". That intuition could be biased, subtly-but-dangerously wrong, or\nfail bizarrely in new scenarios.)\n\nCurrent Day: We still don't know how to unify logic & intuition in AI.\n\nBut if/when we do, that would give us the biggest risks & rewards of AI:\nsomething that can logically out-plan us, and learn general intuition. That'd\nbe an \"AI Einstein\"... or an \"AI Oppenheimer\".\n\nSummed in a picture:\n\nSo that's \"Logic vs Intuition\". As for the other core conflict, \"Problems in\nthe AI vs The Humans\", that's one of the big controversies in the field of AI\nSafety: are our main risks from advanced AI itself, or from humans misusing\nadvanced AI?\n\n(Why not both?)\n\n###\n\nPart 2: The problems\n\nThe problem of AI Safety is this:^[4]\n\n> The Value Alignment Problem: \u201cHow can we make AI robustly serve humane\n> values?\u201d\n\nNOTE: I wrote humane, with an \"e\", not just \"human\". A human may or may not be\nhumane. I'm going to harp on this because both advocates & critics of AI\nSafety keep mixing up the two.^[5]^[6]\n\nWe can break this problem down by \"Problems in Humans vs AI\":\n\n> Humane Values: \u201cWhat are humane values, anyway?\u201d (a problem for philosophy &\n> ethics)\n\n> The Technical Alignment Problem: \u201cHow can we make AI robustly serve any\n> intended goal at all?\u201d (a problem for computer scientists - surprisingly,\n> still unsolved!)\n\nThe technical alignment problem, in turn, can be broken down by \"Logic vs\nIntuition\":\n\n> Problems with AI Logic:^[7] (\"game theory\" problems)\n>\n>   * AIs may accomplish goals in logical but undesirable ways.\n>   * Most goals logically lead to the same unsafe sub-goals: \"don't let\n> anyone stop me from accomplishing my goal\", \"maximize my ability & resources\n> to optimize for that goal\", etc.\n>\n\n> Problems with AI Intuition:^[8] (\"deep learning\" problems)\n>\n>   * An AI trained on human data could learn our prejudices.\n>   * AI \"intuition\" isn't understandable or verifiable.\n>   * AI \"intuition\" is fragile, and fails in new scenarios.\n>   * AI \"intuition\" could partly fail, which may be worse: an AI with intact\n> skills, but broken goals, would be an AI that skillfully acts towards\n> corrupted goals.\n>\n\n(Again, what \"logic\" and \"intuition\" are will be more precisely explained\nlater!)\n\nSummed in a picture:\n\nAs intuition for how hard these problems are, note that we haven't even solved\nthem for us humans \u2014 People follow the letter of the law, not the spirit.\nPeople's intuition can be biased, and fail in new circumstances. And none of\nus are 100% the humane humans we wished we were.\n\nSo, if I may be a bit sappy, maybe understanding AI will help us understand\nourselves. And just maybe, we can solve the human alignment problem: How do we\nget humans to robustly serve humane values?\n\n###\n\nPart 3: The proposed solutions\n\nFinally, we can understand some (possible) ways to solve the problems in\nlogic, intuition, AIs, and humans! These include:\n\n  * Technical solutions\n  * Policy/governance solutions\n  * \"How 'bout you just shut it down & don't build the torture nexus\"\n\n\u2014 and more! Experts disagree on which proposals will work, if any... but it's\na good start.\n\n(Unfortunately, I can't give a layperson-friendly summary in this Intro,\nbecause these solutions won't make sense until you understand the problems,\nwhich is what Part 1 & 2 are for. That said, if you want spoilers, click here\nto see what Part 3 will cover!)\n\n##\n\n\ud83e\udd14 (Optional flashcard review!)\n\nHey, d'ya ever get this feeling?\n\n  1. \"Wow that was a wonderful, insightful thing I just read\"\n  2. [forgets everything 2 weeks later]\n  3. \"Oh no\"\n\nTo avoid that for this guide, I've included some OPTIONAL interactive\nflashcards! They use \"Spaced Repetition\", an easy-ish, evidence-backed way to\n\"make long-term memory a choice\". (click here to learn more about Spaced\nRepetition!)\n\nHere: try the below flashcards, to retain what you just learnt!\n\n(There's an optional sign-up at the end, if you want to save these cards for\nlong-term study. Note: I do not own or control this app, it's third-party.)\n\n(Also, you don't need to memorize the answers exactly, just the gist. You be\nthe judge if you got it \"close enough\".)\n\n##\n\n\ud83e\udd37\ud83c\udffb\u2640\ufe0f Five common misconceptions about AI Safety\n\n> \u201cIt ain\u2019t what you don\u2019t know that gets you into trouble. It\u2019s what you know\n> for sure that just ain\u2019t so.\u201d\n>\n> ~ often attributed to Mark Twain, but it just ain't so^[9]\n\nFor better and worse, you've already heard too much about AI. So before we\nconnect new puzzle pieces in your mind, we gotta take out the old pieces that\njust ain't so.\n\nThus, if you'll indulge me in a \"Top 5\" listicle...\n\n###\n\n1) No, AI Safety isn't a fringe concern by sci-fi weebs.\n\nAI Safety / AI Risk used to be less mainstream, but now in 2024, the US & UK\ngovernments now have AI Safety-specific departments!^[10] This resulted from\nmany of the top AI researchers raising alarm bells about it. These folks\ninclude:\n\n  * Geoffrey Hinton^[11] and Yoshua Bengio^[12], co-winners of the 2018 Turing Prize (the \"Nobel Prize of Computing\") for their work on deep neural networks, the thing that all the new famous AIs use.^[13]\n  * Stuart Russell and Peter Norvig, the authors of the most-used textbook on Artificial Intelligence.^[14]\n  * Paul Christiano, pioneer of the AI training/safety technique that made ChatGPT possible.^[15]\n\n(To be clear: there are also top AI researchers against fears of AI Risk, such\nYann LeCun,^[16] co-winner of the 2018 Turing Prize, and chief AI researcher\nat Facebook Meta. Another notable name is Melanie Mitchell^[17], a researcher\nin AI & complexity science.)\n\nI'm aware \"look at these experts\" is an appeal to authority, but this is only\nto counter the idea of, \"eh, only sci-fi weebs fear AI Risk\". But in the end,\nappeal to authority/weebs isn't enough; you have to actually understand the\ndang thing. (Which you are doing, by reading this! So thank you.)\n\nBut speaking of sci-fi weebs...\n\n###\n\n2) No, AI Risk is NOT about AI becoming \"sentient\" or \"conscious\" or gaining a\n\"will to power\".\n\nSci-fi authors write sentient AIs because they're writing stories, not\ntechnical papers. The philosophical debate on artificial consciousness is\nfascinating, and irrelevant to AI Safety. Analogy: a nuclear bomb isn't\nconscious, but it can still be unsafe, no?\n\nAs mentioned earlier, the real problems in AI Safety are \"boring\": an AI\nlearns the wrong things from its biased training data, it breaks in slightly-\nnew scenarios, it logically accomplishes goals in undesired ways, etc.\n\nBut, \"boring\" doesn't mean not important. The technical details of how to\ndesign a safe elevator/airplane/bridge are boring to most laypeople... and\nalso a matter of life-and-death.\n\n(Catastrophic AI Risk doesn't even require \"super-human general intelligence\"!\nFor example, an AI that's \"only\" good at designing viruses could help a bio-\nterrorist organization (like Aum Shinrikyo^[18]) kill millions of people.)\n\nBut speaking of killing people...\n\n###\n\n3) No, AI Risk isn't necessarily extinction, SkyNet, or nanobots\n\nWhile most AI researchers do believe advanced AI poses a 5+% risk of\n\"literally everybody dies\"^[19], it's very hard to convince folks (especially\npolicymakers) of stuff that's never happened before.\n\nSo instead, I'd like to highlight the ways that advanced AI \u2013 (especially when\nit's available to anyone with a high-end computer) \u2013 could lead to\ncatastrophes, \"merely\" by scaling up already-existing bad stuff.\n\nFor example:\n\n  * Bio-engineered pandemics: A bio-terrorist cult (like Aum Shinrikyo^[18:1]) uses AI (like AlphaFold^[20]) and DNA-printing (which is getting cheaper fast^[21]) to design multiple new super-viruses, and release them simultaneously in major airports around the globe.\n\n    * (Proof of concept: Scientists have already re-built polio from mail-order DNA... two decades ago.^[22])\n  * Digital authoritarianism: A tyrant uses AI-enhanced surveillance to hunt down protestors (already happening), generate individually-targeted propaganda (kind of happening), and autonomous military robots (soon-to-be happening)... all to rule with a silicon fist.\n  * Cybersecurity Ransom Hell: Cyber-criminals make a computer virus that does its own hacking & re-programming, so it's always one step ahead of human defenses. The result: an unstoppable worldwide bot-net, which holds critical infrastructure ransom, and manipulates top CEOs and politicians to do its bidding.\n\n    * (For context: without AI, hackers have already damaged nuclear power plants,^[23] held hospitals ransom^[24] which maybe killed someone,^[25] and almost poisoned a town's water supply twice.^[26] With AI, deepfakes have been used to swing an election,^[27] steal $25 million in a single heist,^[28] and target parents for ransom, using the faked voices of their children being kidnapped & crying for help.^[29])\n    * (This is why it's not easy to \"just shut down an AI when we notice it going haywire\"; as the history of computer security shows, we just suck at noticing problems in general. I cannot over-emphasize how much the modern world is built on an upside-down house of cards.)\n\nThe above examples are all \"humans misuse AI to cause havoc\", but remember\nadvanced AI could do the above by itself, due to \"boring\" reasons: it's\naccomplishing a goal in a logical-but-undesirable way, its goals glitch out\nbut its skills remain intact, etc.\n\n(Bonus, Some concrete, plausible ways a rogue AI could \"escape containment\",\nor affect the physical world.)\n\nPoint is: even if one doesn't think AI is a literal 100% human extinction\nrisk... I'd say \"homebrew bio-terrorism\" & \"1984 with robots\" are still worth\ntaking seriously.\n\nOn the flipside...\n\n###\n\n4) Yes, folks worried about AI's downsides do recognize its upsides.\n\nAI Risk folks aren't Luddites. In fact, they warn about AI's downsides\nprecisely because they care about AI's upsides.^[30] As humorist Gil Stern\nonce said:^[31]\n\n> \u201cBoth the optimist and the pessimist contribute to society: the optimist\n> invents the airplane, and the pessimist invents the parachute.\u201d\n\nSo: even as this series goes into detail on how AI is already going wrong,\nit's worth remembering the few ways AI is already going right:\n\n  * AI can analyze medical scans as well or better than human specialists! ^[32] That's concretely life-saving!\n  * AlphaFold basically solved a 50-year-old, major problem in biology: how to predict the shape of proteins.^[20:1] (AlphaFold can predict a protein's shape to within the width of an atom!) This has huge applications to medicine & understanding disease.\n\nToo often, we take technology \u2014 even life-saving technology \u2014 for granted. So,\nlet me zoom out for context. Here's the last 2000+ years of child mortality,\nthe percentage of kids who die before puberty:\n\n(from Dattani, Spooner, Ritchie and Roser (2023))\n\nFor thousands of years, in nations both rich and poor, a full half of kids\njust died. This was a constant. Then, starting in the 1800s \u2014 thanks to\nscience/tech like germ theory, sanitation, medicine, clean water, vaccines,\netc \u2014 child mortality fell off like a cliff. We still have far more to go \u2014 I\nrefuse to accept^[33] a worldwide 4.3% (1 in 23) child death rate \u2014 but let's\njust appreciate how humanity so swiftly cut down an eons-old scourge.\n\nHow did we achieve this? Policy's a big part of the story, but policy is \"the\nart of the possible\"^[34], and the above wouldn't have been possible without\ngood science & tech. If safe, humane AI can help us progress further by even\njust a few percent \u2014 towards slaying the remaining dragons of cancer,\nAlzheimer's, HIV/AIDS, etc \u2014 that'd be tens of millions more of our loved\nones, who get to beat the Reaper for another day.\n\nF#@\u2606 going to Mars, that's why advanced AI matters.\n\n. . .\n\nWait, really? Toys like ChatGPT and DALL-E are life-and-death stakes? That\nleads us to the final misconception I'd like to address:\n\n###\n\n5) No, experts don't think current AIs are high-risk/reward.\n\nOh come on, one might reasonably retort, AI can't consistently draw more than\n3 objects. How's it going to take over the world? Heck, how's it even going to\ntake my job?\n\nI present to you, a relevant xkcd:\n\nThis is how I feel about \"don't worry about AI, it can't even do [X]\".\n\nIs our postmodern memory-span that bad? One decade ago, just one, the world's\nstate-of-the-art AIs couldn't even recognize pictures of cats. Now, not only\ncan AI do that at human-performance level, AIs can pump out a picture of a\ncat-ninja slicing a watermelon in the style of Vincent Van Gogh in under a\nminute.\n\nIs current AI a huge threat to our jobs, or safety? No. (Well, besides the\naforementioned deepfake scams.)\n\nBut: if AI keeps improving at the same rate as it has been for the last\ndecade... it seems plausible to me we could get \"Einstein/Oppenheimer-level\"\nAI in 30 years.^[35] That's well within many folks' lifetimes!\n\nAs \"they\" say:^[36]\n\n> The best time to plant a tree was 30 years ago. The second best time is\n> today.\n\nLet's plant that tree today!\n\n##\n\n\ud83e\udd14 (Optional flashcard review #2!)\n\n##\n\n\ud83e\udd18 Introduction, in Summary:\n\n  * The 2 core conflicts in AI & AI Safety are:\n\n    * Logic \"versus\" Intuition\n    * Problems in the AI \"versus\" in the Humans\n  * Correcting misconceptions about AI Risk:\n\n    * It's not a fringe concern by sci-fi weebs.\n    * It doesn't require AI consciousness or super-intelligence.\n    * There's many risks besides \"literal 100% human extinction\".\n    * We are aware of AI's upsides.\n    * It's not about current AI, but about how fast AI is advancing.\n\n(To review the flashcards, click the Table of Contents icon in the right\nsidebar, then click the \"\ud83e\udd14 Review\" links.)\n\nFinally! Now that we've taken the 10,000-foot view, let's get hiking on our\nwhirlwind tour of AI Safety... for us warm, normal, fleshy humans!\n\nClick to continue \u2935\n\nPART ONE \u2192\n\nAI Safety for Fleshy Humans was made by Nicky Case in collaboration with Hack\nClub.\n\n\ud83e\udd95 Hack Club is a nonprofit where teenagers code awesome projects together -\nlike cpu.land, SineRider, and this! Join in-person hackathons, run a club at\nyour school, and connect with other friendly teenagers.\n\n\ud83d\ude3b Nicky Case is fifteen cats in a trenchcoat. She makes internet playthings,\nlike The Evolution of Trust, Adventures with Anxiety, Explorable Explanations,\nand more.\n\n\ud83d\udcb8 If you're not a teen, and are an AI moneybags person, click these links to\nlearn more about Hack Club and how to support them! Also, Nicky has a Patreon\n& Ko-Fi. (p.s: thank-you page for my supporters!)\n\n. . .\n\nSpecial thanks to these teens at Hack Club for being free child labor beta-\nreading & giving feedback on this piece:\n\n> Arthur Beck, Atharv Gupta, Brendan Lee, Celeste, Charalampos Fanoulis,\n> Charlie, Cheru Berhanu, Claire Wang, Elijah, Fred Han, Gia B\u00e1ch Nguy\u1ec5n,\n> Hajrah Siddiqui, Jakob, Joseph Ross, Kieran Klukas, Lexi Mattick, Mason\n> Meirs, Michael Panait, Nick Zandbergen, Nila Palmo Ram, Pixelglide, py660,\n> River Lewis, Samuel Cottrell, Samuel Fernandez, Saran Wagner, Skyler Grey, S\n> P U N G E, Vihaan Sondhi\n\nAlso thank you to these non-teenagers for giving feedback: (Though I assume\nthey were teenagers at some point)\n\n> B Cavello, Paul Dancstep, Tobias Rose-Stockwell\n\nAny errors remaining are solely the fault of Suzie the Scapegoat.\n\n. . .\n\nAI Safety For Fleshy Humans is free for anyone to share & remix, as long as\nit's for non-commercial use (e.g. education): CC BY-NC 4.0\n\nIf you'd like to cite this work and you're a Serious PersonTM, here's your\ncitation:\n\n> Nicky Case, \u201cAI Safety for Fleshy Humans\u201d, https://AIsafety.dance, Hack Club\n> (2024).\n\nFinally, here's the open-source code for this website!\n\nThank you for being the kind of person to read the credits~ \ud83d\ude4f\n\nOh dang, it's a post-credits scene:\n\nSee all feetnotes \ud83d\udc63\n\nAlso, expandable \"Nutshells\" that make good standalone bits:\n\nWhat is Spaced Repetition? Concrete ways an AI could 'escape containment' &\naffect the world\n\nAlso, the dancing robot catboy animation is based off this JerryTerry video.\n\nclose all nutshells\n\n\u00d7\n\nStep 1) Copy this code into the <head> of your site:\n\nStep 2) In your article, create a link to and make sure the link text starts\nwith a :colon, :like this, so Nutshell knows to make it expandable.\n\nStep 3) That's all, folks! \ud83c\udf89\n\nlearn more about Nutshell\n\n", "frontpage": false}
