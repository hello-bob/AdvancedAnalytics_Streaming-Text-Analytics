{"aid": "40167999", "title": "NeRF-XL: NeRF at Any Scale with Multi-GPU", "url": "https://research.nvidia.com/labs/toronto-ai/nerfxl/", "domain": "nvidia.com", "votes": 1, "user": "smusamashah", "posted_at": "2024-04-26 11:19:51", "comments": 0, "source_title": "NeRF-XL: Scaling NeRFs with Multiple GPUs", "source_text": "Toronto AI Lab NVIDIA Research Berkeley AI Research\n\nNeRF-XL: Scaling NeRFs with Multiple GPUs\n\n# NeRF-XL: NeRF at Any Scale with Multi-GPU\n\nRuilong Li^1,2\n\nSanja Fidler^1,3\n\nAngjoo Kanazawa^2\n\nFrancis Williams^1\n\n^1 NVIDIA\n\n^2 UC Berkeley\n\n^3 University of Toronto\n\ndescription arXiv description Paper description Code (Soon)\n\nTL;DR Our principled multi-GPU distributed training algorithm enables scaling\nup NeRFs to arbitrarily-large scale.\n\n## Abstract\n\nWe present NeRF-XL,a principled method for distributing Neural Radiance Fields\n(NeRFs) across multiple GPUs thus enabling training and rendering NeRFs of\narbitrarily large capacity. We begin by revisiting existing multi-GPU\napproaches which decompose large scenes into multiple independently trained\nNeRFs and identify several fundamental issues with these methods that hinder\nimprovements in reconstruction quality as additional computational resources\n(GPUs) are used in training. NeRF-XL remedies these issues and enables\ntraining and rendering NeRFs with an arbitrary number of parameters by simply\nusing more hardware. At the core of our method lies a novel distributed\ntraining and rendering formulation which is mathematically equivalent to the\nclassic single-GPU case and minimizes communication between GPUs. By unlocking\nNeRFs with arbitrarily-large parameter counts, our approach is the first to\nreveal multi-GPU scaling laws for NeRFs, showing reconstruction quality\nimprovements with larger parameter counts and speed improvements with more\nGPUs. We show the effectiveness of NeRF-XL on a wide variety of datasets,\nincluding the largest open-source dataset to date, MatrixCity, containing 258K\nimages covering a 25km^2 city area.\n\n## Method\n\nOur method jointly trains multiple NeRFs across all GPUs, each of which covers\na non-overlapped spatial region. The communication across GPUs only happens in\nthe forward pass but not the backward pass (shown in gray arrows), which\nsignificantly reducing the communication overhead. (a) We can train this\nsystem by evaluating each NeRF to get the sample color and density, then\nbroadcast these values to all other GPUs for a global volume rendering. (b) By\nrewriting volume rendering equation (and other losses) into a distributed form\nwe can dramatically reduce the data transfer to one value per-ray, thus\nimproving efficiency. Mathematically, our approach is identical to training\nand rendering a NeRF represented by multiple small NeRFs (e.g. Kilo-NeRF) on a\nsingle large GPU.\n\nDistributed Volume Rendering\n\nDistributed Distortion Loss\n\n## Independent Training v.s. Joint Training\n\nZero Model Capacity Redundancy. Prior works (e.g., Block-NeRF, Mega-NeRF)\ntypically involve training multiple NeRFs independently, with each NeRF being\ntrained on a separate GPU. This setup necessitates each NeRF to handle both\nthe focal region and its surroundings, resulting in redundant modeling within\nthe capacity of the model. In contrast, our joint training methodology employs\nnon-overlapping NeRFs, eliminating redundancy altogether.\n\nNo Blending Any More. Prior works with independent Training relies on blending\nfor novel-view synthesis at inference time. Either blending in 2D (e.g.,\nBlock-NeRF) or 3D (e.g., Mega-NeRF) could be harmful for the rendering\nquality. In contrast, our joint training approach does not rely on any\nblending for rendering. thereby eliminating the train-test discrepancy\nintroduced by independent training in prior works.\n\nShared Per-camera Embedding. Camera optimization in NeRF can be achieved by\neither transform the inaccurate camera itself or all other cameras along with\nthe underlying 3D scene. Thus, training multiple NeRFs independently with\ncamera optimization may lead to inconsistencies in camera corrections and\nscene geometry, causing more difficulties for blended rendering. Conversely,\nour joint training approach allows optimizing a single set of per-camera\nembedding (through multi-GPU synchronization) thus ensuring consistent camera\noptimization across the entire scene.\n\n## More GPUs, Better Quality and Faster Rendering\n\nPrior works based on independent training fails to realize performance\nimprovement with additional GPUs, which contradicts the fundamental objective\nof leveraging multi-GPU setups to enhance large-scale NeRF performance.\nWithout any heuristics, our approach gracefully reveals scaling laws for NeRFs\nin the multi-GPU setting across various types of data and scales.\n\n## More Novel-view Rendering Results\n\nLaguna Seca Raceway (In-house Capture) on 8 GPUs.\n\nMexico Beach after Hurricane Michael 2018 (Source) on 32 GPUs.\n\nMatrixCity (Source) on 64 GPUs.\n\n## Citation\n\n    \n    \n    @misc{li2024nerfxl, title={NeRF-XL: Scaling NeRFs with Multiple GPUs}, author={Ruilong Li and Sanja Fidler and Angjoo Kanazawa and Francis Williams}, year={2024}, eprint={2404.16221}, archivePrefix={arXiv}, primaryClass={cs.CV} }\n\n## Acknowledgement\n\nThis project is supported in part by IARPA DOI/IBC 140D0423C0035. We would\nlike to thank Brent Bartlett and Tim Woodard for providing and helping with\nprocessing the Mexico Beach data. We would also like to thank Yixuan Li for\nthe help and discussion around the MatrixCity data.\n\n", "frontpage": false}
