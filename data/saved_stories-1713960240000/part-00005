{"aid": "40138933", "title": "vLLM", "url": "https://docs.vllm.ai/en/latest/", "domain": "vllm.ai", "votes": 1, "user": "jonbaer", "posted_at": "2024-04-24 00:15:34", "comments": 0, "source_title": "Welcome to vLLM! \u2014 vLLM", "source_text": "Welcome to vLLM! \u2014 vLLM\n\nSkip to main content\n\n  * .rst\n\n# Welcome to vLLM!\n\n## Contents\n\n# Welcome to vLLM!#\n\nEasy, fast, and cheap LLM serving for everyone\n\nvLLM is a fast and easy-to-use library for LLM inference and serving.\n\nvLLM is fast with:\n\n  * State-of-the-art serving throughput\n\n  * Efficient management of attention key and value memory with PagedAttention\n\n  * Continuous batching of incoming requests\n\n  * Fast model execution with CUDA/HIP graph\n\n  * Quantization: GPTQ, AWQ, SqueezeLLM, FP8 KV Cache\n\n  * Optimized CUDA kernels\n\nvLLM is flexible and easy to use with:\n\n  * Seamless integration with popular HuggingFace models\n\n  * High-throughput serving with various decoding algorithms, including parallel sampling, beam search, and more\n\n  * Tensor parallelism support for distributed inference\n\n  * Streaming outputs\n\n  * OpenAI-compatible API server\n\n  * Support NVIDIA GPUs and AMD GPUs\n\n  * (Experimental) Prefix caching support\n\n  * (Experimental) Multi-lora support\n\nFor more information, check out the following:\n\n  * vLLM announcing blog post (intro to PagedAttention)\n\n  * vLLM paper (SOSP 2023)\n\n  * How continuous batching enables 23x throughput in LLM inference while reducing p50 latency by Cade Daniel et al.\n\n## Documentation#\n\nGetting Started\n\n  * Installation\n  * Installation with ROCm\n  * Installation with Neuron\n  * Installation with CPU\n  * Quickstart\n  * Examples\n\nServing\n\n  * OpenAI Compatible Server\n  * Deploying with Docker\n  * Distributed Inference and Serving\n  * Production Metrics\n  * Usage Stats Collection\n  * Integrations\n\nModels\n\n  * Supported Models\n  * Adding a New Model\n  * Engine Arguments\n  * Using LoRA adapters\n\nQuantization\n\n  * AutoAWQ\n  * FP8 E5M2 KV Cache\n  * FP8 E4M3 KV Cache\n\nDeveloper Documentation\n\n  * Sampling Params\n\n    * SamplingParams\n  * vLLM Engine\n\n    * LLMEngine\n    * AsyncLLMEngine\n  * vLLM Paged Attention\n\n    * Inputs\n    * Concepts\n    * Query\n    * Key\n    * QK\n    * Softmax\n    * Value\n    * LV\n    * Output\n\n# Indices and tables#\n\n  * Index\n\n  * Module Index\n\nnext\n\nInstallation\n\nContents\n\nBy the vLLM Team\n\n\u00a9 Copyright 2024, vLLM Team.\n\n", "frontpage": false}
