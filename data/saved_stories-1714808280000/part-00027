{"aid": "40251747", "title": "StructLM: Towards Building Generalist Models for Structured Knowledge Grounding", "url": "https://arxiv.org/abs/2402.16671", "domain": "arxiv.org", "votes": 3, "user": "PaulHoule", "posted_at": "2024-05-03 20:01:41", "comments": 0, "source_title": "StructLM: Towards Building Generalist Models for Structured Knowledge Grounding", "source_text": "[2402.16671] StructLM: Towards Building Generalist Models for Structured\nKnowledge Grounding\n\nSkip to main content\n\nWe gratefully acknowledge support from the Simons Foundation, Yale University,\nand all contributors. Donate\n\n> cs > arXiv:2402.16671\n\n# Computer Science > Computation and Language\n\narXiv:2402.16671 (cs)\n\n[Submitted on 26 Feb 2024 (v1), last revised 24 Apr 2024 (this version, v6)]\n\n# Title:StructLM: Towards Building Generalist Models for Structured Knowledge\nGrounding\n\nAuthors:Alex Zhuang, Ge Zhang, Tianyu Zheng, Xinrun Du, Junjie Wang, Weiming\nRen, Stephen W. Huang, Jie Fu, Xiang Yue, Wenhu Chen\n\nView a PDF of the paper titled StructLM: Towards Building Generalist Models\nfor Structured Knowledge Grounding, by Alex Zhuang and 9 other authors\n\nView PDF HTML (experimental)\n\n> Abstract:Structured data sources, such as tables, graphs, and databases, are\n> ubiquitous knowledge sources. Despite the demonstrated capabilities of large\n> language models (LLMs) on plain text, their proficiency in interpreting and\n> utilizing structured data remains limited. Our investigation reveals a\n> notable deficiency in LLMs' ability to process structured data, e.g.,\n> ChatGPT lags behind state-of-the-art (SoTA) model by an average of 35%. To\n> augment the Structured Knowledge Grounding (SKG) capabilities in LLMs, we\n> have developed a comprehensive instruction tuning dataset comprising 1.1\n> million examples. Utilizing this dataset, we train a series of models,\n> referred to as StructLM, based on the Mistral and the CodeLlama model\n> family, ranging from 7B to 34B parameters. Our StructLM series surpasses\n> task-specific models on 16 out of 18 evaluated datasets and establishes new\n> SoTA performance on 8 SKG tasks. Furthermore, StructLM demonstrates strong\n> generalization across 6 novel held-out SKG tasks, outperforming TableLlama\n> by an average of 35\\% and Flan-UL2 20B by an average of 10\\%. Contrary to\n> expectations, we observe that scaling model size offers marginal benefits,\n> with StructLM-34B showing only slight improvements over StructLM-7B. This\n> suggests that structured knowledge grounding is still a challenging task and\n> requires more innovative design to push to a new level.\n\nComments:| Technical Report  \n---|---  \nSubjects:| Computation and Language (cs.CL)  \nCite as:| arXiv:2402.16671 [cs.CL]  \n(or arXiv:2402.16671v6 [cs.CL] for this version)  \nhttps://doi.org/10.48550/arXiv.2402.16671arXiv-issued DOI via DataCite  \n  \n## Submission history\n\nFrom: Wenhu Chen [view email] [v1] Mon, 26 Feb 2024 15:47:01 UTC (554 KB) [v2]\nWed, 28 Feb 2024 14:49:03 UTC (555 KB) [v3] Sun, 31 Mar 2024 20:14:20 UTC\n(1,141 KB) [v4] Sun, 21 Apr 2024 01:06:24 UTC (1,043 KB) [v5] Tue, 23 Apr 2024\n17:29:25 UTC (1,042 KB) [v6] Wed, 24 Apr 2024 21:13:24 UTC (1,059 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled StructLM: Towards Building Generalist Models\nfor Structured Knowledge Grounding, by Alex Zhuang and 9 other authors\n\n  * View PDF\n  * HTML (experimental)\n  * TeX Source\n  * Other Formats\n\nview license\n\nCurrent browse context:\n\ncs.CL\n\n< prev | next >\n\nnew | recent | 2402\n\nChange to browse by:\n\ncs\n\n### References & Citations\n\n  * NASA ADS\n  * Google Scholar\n  * Semantic Scholar\n\na export BibTeX citation Loading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer (What is the Explorer?)\n\nLitmaps (What is Litmaps?)\n\nscite Smart Citations (What are Smart Citations?)\n\n# Code, Data and Media Associated with this Article\n\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\n\nDagsHub (What is DagsHub?)\n\nGotit.pub (What is GotitPub?)\n\nPapers with Code (What is Papers with Code?)\n\nScienceCast (What is ScienceCast?)\n\n# Demos\n\nReplicate (What is Replicate?)\n\nHugging Face Spaces (What is Spaces?)\n\nTXYZ.AI (What is TXYZ.AI?)\n\n# Recommenders and Search Tools\n\nInfluence Flower (What are Influence Flowers?)\n\nConnected Papers (What is Connected Papers?)\n\nCORE Recommender (What is CORE?)\n\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new\narXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and\naccepted our values of openness, community, excellence, and user data privacy.\narXiv is committed to these values and only works with partners that adhere to\nthem.\n\nHave an idea for a project that will add value for arXiv's community? Learn\nmore about arXivLabs.\n\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n\n  * About\n  * Help\n\n  * Contact\n  * Subscribe\n\n  * Copyright\n  * Privacy Policy\n\n  * Web Accessibility Assistance\n  * arXiv Operational Status Get status notifications via email or slack\n\n", "frontpage": true}
