{"aid": "40141175", "title": "Golang PGO builds using GitHub Actions", "url": "https://www.dolthub.com/blog/2024-04-19-golang-pgo-builds-using-github-actions/", "domain": "dolthub.com", "votes": 4, "user": "kiyanwang", "posted_at": "2024-04-24 06:08:25", "comments": 0, "source_title": "Golang PGO builds using GitHub Actions", "source_text": "Golang PGO builds using GitHub Actions | DoltHub Blog\n\nBlog\n\n# Golang PGO builds using GitHub Actions\n\nDustin Brown\n\nApril 19, 2024\n\nGOLANG\n\n22 min read\n\nIn February of this year, I announced that Dolt releases are now built as\nprofile-guided optimized (pgo) binaries, levering this powerful feature of\nGolang 1.20 to increase Dolt's read performance by 5.3%.\n\nPrior to my announcement, Zach, one of our resident Golang experts,\nexperimented and tested Golang's pgo feature and wrote about the performance\ngains he observed in Dolt after building it with profiles created while\nrunning our Sysbench benchmarking tests against it first. From there, we knew\nwe had to get those performance gains into our released binaries, so we\nretooled Dolt's release process to build pgo releases.\n\nToday, I'll cover Dolt's general release process, which uses GitHub Actions,\nand I'll breakdown all phases of our release process. I'll also go over what\nwe changed in our process to start releasing pgo builds. Hopefully this will\nallow you to glean some insights you can use for working pgo builds into your\nown Golang releases!\n\nLet's dive right in.\n\n# Dolt releases with GitHub Actions\n\nDolt leverages GitHub Actions to perform a number of automated tasks, one of\nwhich is creating and publishing releases.\n\nGitHub Actions uses files called workflows to define jobs that it will do the\nwork defined in the workflow file. These jobs are deployed to runners, or host\nmachines, that you can either host yourself or allow GitHub to host for you.\n\nSelf-hosted runners are provisioned and maintained by you, external to GitHub\nActions. GitHub-hosted runners, which are free for public repositories, are\nall hosted and maintained by GitHub, but they have specific storage, memory,\nand cpu limits depending on your tier of subscription. For Dolt, we use the\nfree-tier GitHub-hosted runners.\n\nAt a high-level, the Dolt release process needs to accomplish a few\nobjectives.\n\nFirst, and most importantly, the process needs to create a tag and release for\nthe new version of Dolt and upload precompiled binaries of Dolt to the release\nassets.\n\nSecond, the release process needs to run our Sysbench benchmarking tests\nagainst this new version of Dolt and email the results to our DoltHub team.\n\nThird, and not super relevant to this blog, the process needs to kick off any\nother auxiliary tasks we need to perform during a release, like creating\nDolt's release notes that depend on pull request descriptions from multiple\nrepositories, publishing the release to various package managers so that it\ncan be easily installed from them, pushing new Docker images to DockerHub, or\nupgrading the Dolt dependency in various repositories we own.\n\nSo, with these objectives in mind, we came up with a suite of GitHub Actions\nworkflows that leverage the repository_dispatch event so that we can\naccomplish each of these objectives. Let's look at a diagram that shows what\nthis design looks like in principle, then we'll dive into the specifics of the\nworkflows.\n\nIn the above diagram you'll see two contexts, the GitHub Actions context and\nthe Kubernetes (K8s) context. Let's discuss the GitHub Actions context first.\n\nFor Dolt's original release process, we used three workflows: the \"Release\nDolt\" workflow, the \"Deploy K8s Sysbench benchmarking Job\" workflow, and the\n\"Email team\" workflow.\n\nThe \"Release Dolt\" workflow kicks off the entire Dolt release process, and is\nrun manually by our engineering team when they're ready to release a new\nversion of Dolt. Here is a pared-down version of the workflow that references\nthe steps shown in the diagram above.\n\n    \n    \n    name: Release Dolt on: workflow_dispatch: inputs: version: description: 'SemVer format release tag, i.e. 0.24.5' required: true jobs: format-version: runs-on: ubuntu-22.04 outputs: version: ${{ steps.format_version.outputs.version }} steps: - name: Format Input id: format_version run: | version=\"${{ github.event.inputs.version }}\" if [[ $version == v* ]]; then version=\"${version:1}\" fi echo \"version=$version\" >> $GITHUB_OUTPUT create-release: needs: format-version name: Create release runs-on: ubuntu-22.04 outputs: release_id: ${{ steps.create_release.outputs.id }} steps: - name: Checkout code uses: actions/checkout@v3 - name: Set up Go 1.x uses: actions/setup-go@v3 with: go-version: ^1.21 - name: Update dolt version command run: sed -i -e 's/ Version = \".*\"/ Version = \"'\"$NEW_VERSION\"'\"/' \"$FILE\" env: FILE: ${{ format('{0}/go/cmd/dolt/dolt.go', github.workspace) }} NEW_VERSION: ${{ needs.format-version.outputs.version }} - name: Set minver TBD to version run: sed -i -e 's/minver:\"TBD\"/minver:\"'\"$NEW_VERSION\"'\"/' \"$FILE\" env: FILE: ${{ format('{0}/go/cmd/dolt/commands/sqlserver/yaml_config.go', github.workspace) }} NEW_VERSION: ${{ needs.format-version.outputs.version }} - name: update minver_validation.txt working-directory: ./go run: go run -mod=readonly ./utils/genminver_validation/ $FILE env: FILE: ${{ format('{0}/go/cmd/dolt/commands/sqlserver/testdata/minver_validation.txt', github.workspace) }} - uses: EndBug/add-and-commit@v9.1.1 with: message: ${{ format('[ga-bump-release] Update Dolt version to {0} and release v{0}', needs.format-version.outputs.version) }} add: ${{ format('[\"{0}/go/cmd/dolt/dolt.go\", \"{0}/go/cmd/dolt/commands/sqlserver/yaml_config.go\", \"{0}/go/cmd/dolt/commands/sqlserver/testdata/minver_validation.txt\"]', github.workspace) }} cwd: \".\" pull: \"--ff\" - name: Build Binaries id: build_binaries run: | latest=$(git rev-parse HEAD) echo \"commitish=$latest\" >> $GITHUB_OUTPUT GO_BUILD_VERSION=1.21 go/utils/publishrelease/buildbinaries.sh - name: Create Release id: create_release uses: dolthub/create-release@v1 env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} with: tag_name: v${{ needs.format-version.outputs.version }} release_name: ${{ needs.format-version.outputs.version }} draft: false prerelease: false commitish: ${{ steps.build_binaries.outputs.commitish }} - name: Upload Linux AMD64 Distro id: upload-linux-amd64-distro uses: dolthub/upload-release-asset@v1 env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} with: upload_url: ${{ steps.create_release.outputs.upload_url }} asset_path: go/out/dolt-linux-amd64.tar.gz asset_name: dolt-linux-amd64.tar.gz asset_content_type: application/zip ... - name: Upload Install Script id: upload-install-script uses: dolthub/upload-release-asset@v1 env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} with: upload_url: ${{ steps.create_release.outputs.upload_url }} asset_path: go/out/install.sh asset_name: install.sh asset_content_type: text/plain trigger-performance-benchmark-email: needs: [format-version, create-release] runs-on: ubuntu-22.04 steps: - name: Trigger Performance Benchmarks uses: peter-evans/repository-dispatch@v2.0.0 with: token: ${{ secrets.REPO_ACCESS_TOKEN }} event-type: release-dolt client-payload: '{\"version\": \"${{ needs.format-version.outputs.version }}\", \"actor\": \"${{ github.actor }}\"}'\n\nThis workflow is triggered manually, using the workflow_dispatch event, and\nrequires a new version number as input. From there, it does some quick\nformatting of the version input, then writes and commits this new version to\nDolt's main branch so that the released binaries will output this new number\nfrom the dolt version command.\n\nIn the \"Build Binaries\" step, the create-release job runs the buildbinaries.sh\nscript, which builds Dolt from source using a Golang Docker container that\nruns the go build command.\n\nWe use Docker containers to build Dolt so that the path's output by stack\ntraces are generic Linux go paths and not paths that reference a go\ninstallation on the runner or on one of our personal computers (which has\nhappened in early, early versions of Dolt \ud83e\udd20).\n\nNext, the \"Create Release\" step creates the tag and publishes the release on\nGitHub. It also provides an upload_url which is used in all subsequent steps\nof the create-release job to upload the compiled binaries to the new GitHub\nrelease.\n\nThe final portion of this workflow is another job that runs after all the\nprevious jobs have completed. This job is called trigger-performance-\nbenchmark-email. It uses a GitHub Action we found on their marketplace to emit\na repository_dispatch event which kicks off a separate Dolt workflow. One that\nwe can see if we look back at our diagram.\n\nOur diagram shows the final step of the \"Release Dolt\" workflow pointing to\nanother workflow called \"Deploy K8s Sysbench benchmarking Job\". This is the\nworkflow that is started by the trigger-performance-benchmark-email workflow\njob.\n\nThis workflow, and others like it, were designed to be dispatched\nasynchronously in part so that it wouldn't be tightly coupled with only the\n\"Release Dolt\" workflow.\n\nIn fact, various workflows will trigger this workflow using a\nrepository_dispatch event, since we need to run performance benchmarks at\ndifferent times, not just during a release. Interestingly, this workflow,\nitself, kicks off another asynchronous process, which we can see in the\ndiagram indicated by the arrow\u2014it deploys a K8s Job that runs our Sysbench\nbenchmarks.\n\nAs it happens, we've written quite a bit about Dolt's use of Sysbench for\nbenchmarking Dolt and MySQL to compare their performance, but I don't think we\ndiscussed the implementation details for how we do that specifically. This\nblog is a good one for me to go over that, so I will momentarily. Before I do\nthough, let's look briefly at the \"Deploy K8s Sysbench benchmarking Job\"\nworkflow.\n\n    \n    \n    name: Benchmark Latency on: repository_dispatch: types: [ benchmark-latency ] jobs: performance: runs-on: ubuntu-22.04 name: Benchmark Performance strategy: matrix: dolt_fmt: [ \"__DOLT__\" ] steps: - name: Checkout uses: actions/checkout@v3 - uses: azure/setup-kubectl@v3.0 with: version: 'v1.23.6' - name: Install aws-iam-authenticator run: | curl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.18.8/2020-09-18/bin/linux/amd64/aws-iam-authenticator && \\ chmod +x ./aws-iam-authenticator && \\ sudo cp ./aws-iam-authenticator /usr/local/bin/aws-iam-authenticator aws-iam-authenticator version - name: Configure AWS Credentials uses: aws-actions/configure-aws-credentials@v2.2.0 with: aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }} aws-region: us-west-2 - name: Create and Auth kubeconfig run: | echo \"$CONFIG\" > kubeconfig KUBECONFIG=kubeconfig kubectl config set-credentials github-actions-dolt --exec-api-version=client.authentication.k8s.io/v1alpha1 --exec-command=aws-iam-authenticator --exec-arg=token --exec-arg=-i --exec-arg=eks-cluster-1 KUBECONFIG=kubeconfig kubectl config set-context github-actions-dolt-context --cluster=eks-cluster-1 --user=github-actions-dolt --namespace=performance-benchmarking KUBECONFIG=kubeconfig kubectl config use-context github-actions-dolt-context env: CONFIG: ${{ secrets.CORP_KUBECONFIG }} - name: Create Sysbench Performance Benchmarking K8s Job run: ./.github/scripts/performance-benchmarking/run-benchmarks.sh env: FROM_SERVER: ${{ github.event.client_payload.from_server }} FROM_VERSION: ${{ github.event.client_payload.from_version }} TO_SERVER: ${{ github.event.client_payload.to_server }} TO_VERSION: ${{ github.event.client_payload.to_version }} MODE: ${{ github.event.client_payload.mode }} ISSUE_NUMBER: ${{ github.event.client_payload.issue_number }} ACTOR: ${{ github.event.client_payload.actor }} ACTOR_EMAIL: ${{ github.event.client_payload.actor_email }} REPO_ACCESS_TOKEN: ${{ secrets.REPO_ACCESS_TOKEN }} KUBECONFIG: \"./kubeconfig\" INIT_BIG_REPO: ${{ github.event.client_payload.init_big_repo }} NOMS_BIN_FORMAT: ${{ matrix.dolt_fmt }} TEMPLATE_SCRIPT: ${{ github.event.client_payload.template_script }} - name: Create TPCC Performance Benchmarking K8s Job run: ./.github/scripts/performance-benchmarking/run-benchmarks.sh env: FROM_SERVER: ${{ github.event.client_payload.from_server }} FROM_VERSION: ${{ github.event.client_payload.from_version }} TO_SERVER: ${{ github.event.client_payload.to_server }} TO_VERSION: ${{ github.event.client_payload.to_version }} MODE: ${{ github.event.client_payload.mode }} ISSUE_NUMBER: ${{ github.event.client_payload.issue_number }} ACTOR: ${{ github.event.client_payload.actor }} ACTOR_EMAIL: ${{ github.event.client_payload.actor_email }} REPO_ACCESS_TOKEN: ${{ secrets.REPO_ACCESS_TOKEN }} KUBECONFIG: \"./kubeconfig\" INIT_BIG_REPO: ${{ github.event.client_payload.init_big_repo }} NOMS_BIN_FORMAT: ${{ matrix.dolt_fmt }} WITH_TPCC: \"true\" TEMPLATE_SCRIPT: ${{ github.event.client_payload.template_script }}\n\nShort, but kinda busy, this workflow authenticates a kubectl client against a\nK8s cluster where we run our Sysbench benchmarks and supplies the required\nenvironment variables to run a script called run-benchmarks.sh. This script\nuses the values from these variables to write a K8s Job configuration file and\nthen apply it, which deploys the benchmarking Job in our K8s cluster.\n\nAt this point you might be wondering why we choose to run benchmarks for Dolt\nin a K8s cluster and not just use GitHub Actions and its runners to benchmark\nDolt. Well there are a couple reasons for this.\n\nOne, GitHub-hosted runners have very specific limits, at least for the free-\ntier, and for benchmarking our database we do not necessarily want to be\nconstrained to these.\n\nAdditionally, there's no way for us to know, or control, what other processes\nor software is running on the GitHub-hosted runners while we'd be doing a\nbenchmarking run, and that could negatively impact the results of the run in\nunpredictable ways.\n\nAnd while it's certainly possible to use a self-hosted runner in GitHub\nActions to circumvent these two problems, in which case we could benchmark\nDolt using only GitHub Actions, we already have easily provision-able hosts\navailable in our K8s cluster, so we opted to simply use those instead.\n\nIn fact, simply applying our K8s benchmarking Job will provision a new\nbenchmarking host using the K8s cluster autoscaler, which is pretty cool.\n\nAnyway, returning to our diagram for a brief moment, we see that after\nauthenticating the kubectl client the \"Deploy Sysbench benchmarking Job\"\nworkflow deploys the K8s Job, and the process moves to the K8s context and the\n\"K8s Sysbench benchmarking job\" is run.\n\nNow technically, this part of the original Dolt release process was more of a\npost-release step. Running the benchmarking job was not required to create a\nnew Dolt release on GitHub, it just provided our team with a report on the\nrelease's latency. But it's important to see this part of our original release\nprocess so that our pgo updates to the Dolt release process will make more\nsense, but more on that later.\n\nIn the K8s context of the diagram we can see that the benchmarking Job\nperforms a few steps. It builds a Dolt binary from a supplied commit SHA. In\nthis case it's the SHA from the HEAD of Dolt main.\n\nNext, it runs the Sysbench tests against that compiled Dolt version, then\nuploads the results of the Sysbench run to an AWS S3 bucket. Finally, it\ntriggers a different GitHub Actions workflow that lives in the Dolt repository\ncalled the \"Email team\" workflow.\n\nTo perform all of this benchmarking and uploading and triggering, we've\nwritten an internal tool that can be used to benchmark a version of Dolt\nagainst a version of MySQL.\n\nThis tool uses some library code we maintain in the Dolt repository, but I'll\nprovide some relevant snippets from the internal tool and the library code so\nyou get a sense of how we've implemented these to run our benchmarks.\n\nOur internal benchmarking tool code is essentially the following go function:\n\ncompare is used to compare the Sysbench results of one version of a database\nto another. You can see from the function's parameters that this tool is not\nonly used for Dolt and MySQL, but is also used for benchmarking our latest\nproduct DoltgreSQL, and it's competitor PostgreSQL.\n\nThe compare function refers to a fromServerConfig, which is the configuration\nfor the \"from\" database server, and refers to a toServerConfig, which is the\nconfiguration of the \"to\" database server. Semantically, here, this tool will\ncompare the \"from\" database to the \"to\" database side-by-side for easy\nanalysis. During the Dolt release process, MySQL will be the \"from\" server and\nDolt will be the \"to\" server.\n\nYou may also notice that we use sqlite3 in this tool, as referenced by\nbenchmark.NewSqlite3ResultsDb, which is a legacy artifact from days before\nDolt v1.0.0, but it still has some unique value here.\n\nUnder-the-hood, after the benchmarks run with sr.Run(), we load the results\ninto a sqlite3 database and run some queries against it to get the comparative\nresults of each database server. A benefit of using sqlite3 over Dolt for\ndoing this, which works just as well, is that sqlite3 returns query output in\nmany formats with the use of simple flags, like --html and --markdown, which\nsaves us from having to code up query result transformation logic.\n\nThe uploadDir returned from db.QueryResults() contains the results of the\ncomparison queries and a copy of the sqlite3 database to be uploaded to S3.\nThese results will soon be downloaded by the \"Email team\" workflow, as we'll\nsee shortly.\n\nWhen it comes to actually running the Sysbench benchmarks, the\nbenchmark.NewSysbenchComparer(config) is simply a wrapper struct around the\nRun function from some benchmarking library code we maintain in the Dolt\nrepository.\n\nThis function creates a Benchmarker based on the type of server it sees, then\ncalls Benchmark(), which runs the Sysbench tests against that server. Here's\nan example of what Dolt Benchmarker's Benchmark() implementation looks like:\n\nDuring the Benchmark() call, this implementation will check the Dolt\ninstallation, update some global Dolt configuration, get the arguments used to\nstart the Dolt SQL server, start the server, acquire the Sysbench tests it's\ngoing to run, then run those tests by calling tester.Test().\n\nWhen it's done, it returns the results and cleans up what it wrote to disk.\n\nAnd, as we've seen in the internal tool's compare function, these results are\nloaded into sqlite3 and uploaded to S3 so they can be emailed to the DoltHub\nteam. But, we're still missing one step\u2014that is, triggering the \"Email team\"\nworkflow with a repository_dispatch event after the internal benchmarking tool\nfinishes uploading the results.\n\nSo the final piece of our internal tool includes:\n\n    \n    \n    err := d.DispatchEmailReportEvent(ctx, *toVersion, *nomsBinFormat, *bucket, key) if err != nil { log.Fatal(err) }\n\nThe DispatchEmailReportEvent() method is on a Dispatcher interface we wrote.\nIt simply makes an HTTP request to the GitHub Actions Workflow REST API which\nemits the repository_dispatch event, triggering the \"Email team\" workflow to\nrun. So let's look at that next.\n\nLike the \"Deploy K8s Sysbench benchmarking Job\" workflow the \"Email team\"\nworkflow is used by multiple processes besides the just Dolt release process,\nso that's why we trigger it with repository_dispatch events. The workflow file\nis as follows:\n\n    \n    \n    name: Email Team Members on: repository_dispatch: types: [ email-report ] jobs: email-team: runs-on: ubuntu-22.04 name: Email Team Members steps: - uses: actions/checkout@v3 - name: Configure AWS Credentials uses: aws-actions/configure-aws-credentials@v2.2.0 with: aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }} aws-region: us-west-2 - name: Get Results id: get-results run: aws s3api get-object --bucket=\"$BUCKET\" --key=\"$KEY\" results.log env: KEY: ${{ github.event.client_payload.key }} BUCKET: ${{ github.event.client_payload.bucket }} - name: Get Addresses id: get-addresses run: | addresses=\"$TEAM\" if [ ! -z \"$RECIPIENT\" ]; then addresses=\"[\\\"$RECIPIENT\\\"]\" fi echo \"addresses=$addresses\" >> $GITHUB_OUTPUT env: RECIPIENT: ${{ github.event.client_payload.email_recipient }} TEAM: '[\"${{ secrets.PERF_REPORTS_EMAIL_ADDRESS }}\"]' - name: Send Email uses: ./.github/actions/ses-email-action with: template: ${{ github.event.client_payload.template }} region: us-west-2 version: ${{ github.event.client_payload.version }} format: ${{ github.event.client_payload.noms_bin_format }} toAddresses: ${{ steps.get-addresses.outputs.addresses }} dataFile: ${{ format('{0}/results.log', github.workspace) }}\n\nAs shown in the diagram, the summary of this workflow is that it downloads the\nSysbench results for the Dolt release and then sends them in an email to our\nteam; nothing crazy.\n\nAnd that's the Dolt release process. Or that was the Dolt release process. Now\nI'll go over how we updated this process to start building pgo binaries of\nDolt on release.\n\n# PGO releases with GitHub Actions\n\nFor those unfamiliar with pgo builds, they require the -pgo flag with the path\nto a Golang profile supplied during the go build command. That part is\nactually very simple. But prior to that, you need to create the profile you\nwant to use for your optimized build, and this required us to update some of\nour benchmark library code, and our internal tool code, so that they could\nboth generate a profile and accept a profile as input. Let me explain in more\ndetail.\n\nIn our benchmarking library code, we use another Dolt utility called\ndolt_builder to actually build Dolt binaries from source. To use this tool,\nyou simply provide the commit SHA or tag you want to build Dolt from, and it\nwill build it for you. So we use this tool in a number of places for easily\nbuilding multiple versions of Dolt simultaneously.\n\nSo the first thing we did was update this tool to accept a Golang profile it\ncan use to build Dolt:\n\nThe next thing we did was update the benchmark library code to run in a\n\"profiling\" mode. In the default mode, as described above, this code calls\nBenchmark() and returns the results. In the new \"profiling\" mode, the code\ncalls Profile() on a Profiler interface:\n\n    \n    \n    ... case Dolt: // handle a profiling run sc, ok := serverConfig.(ProfilingServerConfig) if ok { if string(sc.GetServerProfile()) != \"\" { fmt.Println(\"Profiling dolt while running sysbench tests\") p := NewDoltProfiler(cwd, config, sc) return p.Profile(ctx) } } ...\n\nProfile() works similarly to Benchmark(), but creates a golang profile taken\nwhile running the Sysbench benchmarks. This allows us to easily generate\nprofiles for Dolt we can use in our new release process.\n\nWe also update this library code to accept a profile as an input. That way we\ncan supply it a profile, which it in turn will supply to dolt_builder to\ncreate a pgo binary, and then run Sysbench and output those results.\n\nTo clarify, we basically updated this library code so that we can run it in\none mode to generate a Golang profile, and then run it in the default mode to\nget our normal benchmarking results, but it will also accept a Golang profile\nas input, and use that to build Dolt with the go build -pgo . Hopefully that\nmakes sense to you, since it's a bit tricky for me to describe \ud83e\udd20.\n\nMoving on, we then needed to update our internal tool that uses all this\nlibrary code to also have a \"profiling\" mode and accept Golang profiles as\ninput. Our plan for the new release process was to run the internal tool once,\nin profiling mode, to create a Golang profile. Then, run the internal tool\nagain in default mode, but supply the Golang profile back to it, which would\nproduce the benchmarking results against a pgo built Dolt.\n\nSo, like the compare function, we were able to add a profile function to the\ninternal tool that generates a Golang cpu profile of a Dolt version.\n\nThis function returns its toUpload directory like compare does, but this time\nit contains the profile to be uploaded to S3.\n\nAfter these changes to the code, we were ready to update our GitHub Actions\nworkflows to start creating pgo releases of Dolt. Here's a diagram showing the\nnew Dolt release process with GitHub Actions.\n\nAs you can see from the new release workflow diagram we've added some new\nGitHub Actions workflows, but they're similar to the original ones. Let's look\nmore closely at them.\n\nFor the new Dolt release process, the first workflow we run, called \"Release\nDolt (Profile)\", does not actually create a GitHub release or build any Dolt\nbinaries.\n\nInstead, its only function is to trigger a second workflow called \"Deploy K8s\nSysbench Profiling Job\".\n\n    \n    \n    name: Release Dolt (Profile) on: workflow_dispatch: inputs: version: description: 'SemVer format release tag, i.e. 0.24.5' required: true jobs: format-version: runs-on: ubuntu-22.04 outputs: version: ${{ steps.format_version.outputs.version }} steps: - name: Format Input id: format_version run: | version=\"${{ github.event.inputs.version }}\" if [[ $version == v* ]]; then version=\"${version:1}\" fi echo \"version=$version\" >> $GITHUB_OUTPUT profile-benchmark-dolt: runs-on: ubuntu-22.04 needs: format-version name: Trigger Benchmark Profile K8s Workflows steps: - uses: actions/checkout@v4 with: ref: main - name: Get sha id: get_sha run: | sha=$(git rev-parse --short HEAD) echo \"sha=$sha\" >> $GITHUB_OUTPUT - uses: peter-evans/repository-dispatch@v3 with: token: ${{ secrets.REPO_ACCESS_TOKEN }} event-type: profile-dolt client-payload: '{\"from_version\": \"${{ steps.get_sha.outputs.sha }}\", \"future_version\": \"${{ needs.format-version.outputs.version }}\", \"mode\": \"release\", \"actor\": \"${{ github.actor }}\", \"actor_email\": \"dustin@dolthub.com\", \"template_script\": \"./.github/scripts/performance-benchmarking/get-dolt-profile-job-json.sh\"}'\n\nThe \"Deploy K8s Sysbench Profiling Job\" works almost identically to the\n\"Deploy K8s Sysbench Benchmarking Job\", except it deploys a benchmarking Job\nrunning in \"profiling\" mode to the K8s cluster, so that we create a Golang\nprofile using the HEAD of Dolt main.\n\n    \n    \n    name: Profile Dolt while Benchmarking on: repository_dispatch: types: [ profile-dolt ] jobs: performance: runs-on: ubuntu-22.04 name: Profile Dolt while Benchmarking steps: - name: Checkout uses: actions/checkout@v4 - uses: azure/setup-kubectl@v4 with: version: 'v1.23.6' - name: Install aws-iam-authenticator run: | curl -o aws-iam-authenticator https://amazon-eks.s3.us-west-2.amazonaws.com/1.18.8/2020-09-18/bin/linux/amd64/aws-iam-authenticator && \\ chmod +x ./aws-iam-authenticator && \\ sudo cp ./aws-iam-authenticator /usr/local/bin/aws-iam-authenticator aws-iam-authenticator version - name: Configure AWS Credentials uses: aws-actions/configure-aws-credentials@v4 with: aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }} aws-region: us-west-2 - name: Create and Auth kubeconfig run: | echo \"$CONFIG\" > kubeconfig KUBECONFIG=kubeconfig kubectl config set-credentials github-actions-dolt --exec-api-version=client.authentication.k8s.io/v1alpha1 --exec-command=aws-iam-authenticator --exec-arg=token --exec-arg=-i --exec-arg=eks-cluster-1 KUBECONFIG=kubeconfig kubectl config set-context github-actions-dolt-context --cluster=eks-cluster-1 --user=github-actions-dolt --namespace=performance-benchmarking KUBECONFIG=kubeconfig kubectl config use-context github-actions-dolt-context env: CONFIG: ${{ secrets.CORP_KUBECONFIG }} - name: Create Profile Benchmarking K8s Job run: ./.github/scripts/performance-benchmarking/run-benchmarks.sh env: PROFILE: \"true\" FUTURE_VERSION: ${{ github.event.client_payload.future_version }} FROM_VERSION: ${{ github.event.client_payload.from_version }} MODE: ${{ github.event.client_payload.mode }} ACTOR: ${{ github.event.client_payload.actor }} ACTOR_EMAIL: ${{ github.event.client_payload.actor_email }} REPO_ACCESS_TOKEN: ${{ secrets.REPO_ACCESS_TOKEN }} KUBECONFIG: \"./kubeconfig\" INIT_BIG_REPO: ${{ github.event.client_payload.init_big_repo }} NOMS_BIN_FORMAT: \"__DOLT__\" TEMPLATE_SCRIPT: ${{ github.event.client_payload.template_script }}\n\nOnce the benchmarking K8s Job is running in \"profiling\" mode, we can see the\nsteps it performs in our updated diagram. We also see that the output of this\nJob is a fresh Golang profile, uploaded to S3, that's ready to be used by the\nremaining steps of our process to create pgo builds.\n\nAt the end of the profiling K8s Job, after uploading the profile, it triggers\nthe \"Release Dolt\" workflow. This workflow works basically the same as the\noriginal \"Release Dolt\" workflow, except that its first step is downloading\nthe Golang Profile that the profiling Job uploaded.\n\n    \n    \n    ... create-pgo-release: needs: format-version runs-on: ubuntu-22.04 name: Release PGO Dolt outputs: release_id: ${{ steps.create_release.outputs.id }} steps: - uses: actions/checkout@v4 with: ref: main - name: Set up Go 1.x uses: actions/setup-go@v5 with: go-version-file: go/go.mod - name: Configure AWS Credentials uses: aws-actions/configure-aws-credentials@v4 with: aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }} aws-region: us-west-2 - name: Get Results id: get-results run: aws s3api get-object --bucket=\"$BUCKET\" --key=\"$KEY\" dolt-cpu-profile.pprof env: KEY: ${{ github.event.inputs.profile_key || github.event.client_payload.profile_key }} BUCKET: ${{ github.event.inputs.profile_bucket || github.event.client_payload.bucket }} ...\n\nIt then supplies the downloaded profile, here called dolt-cpu-profile.pprof to\nthe buildbinaries.sh script, which runs go build -pgo=./dolt-cpu-\nprofile.pprof, which compiles the new Dolt binaries. Then, like the original\nversion of the workflow, it creates a GitHub release and uploads these\nbinaries as release assets.\n\nBefore completing, one of the final jobs in this workflow kicks off another\nbenchmarking K8s Job, only this time supplying the job with the S3 key to the\nGolang profile it used to build the Dolt binaries.\n\n    \n    \n    ... trigger-performance-benchmark-email: needs: [format-version, create-pgo-release] runs-on: ubuntu-22.04 steps: - name: Trigger Performance Benchmarks uses: peter-evans/repository-dispatch@v3 with: token: ${{ secrets.REPO_ACCESS_TOKEN }} event-type: release-dolt client-payload: '{\"version\": \"${{ needs.format-version.outputs.version }}\", \"actor\": \"${{ github.actor }}\", \"profile_key\": \"${{ github.event.inputs.profile_key || github.event.client_payload.profile_key }}\"}'\n\nThis deploys a benchmarking Job to our K8s cluster once again, but now the Job\nwill download the Golang profile from S3 and use it to construct pgo binaries\nof Dolt to use for benchmarking and producing results.\n\nAnd we can see from the diagram in the K8s context, the final step of this\nsecond benchmarking Job kicks off the \"Email team\" workflow, so that our team\ngets the benchmarking results for the now pgo'd Dolt.\n\nAnd so we've done it! We are now releasing pgo builds of Dolt.\n\n# Conclusion\n\nAs you can see there's a bit of complexity involved in updating a release\nprocess to produce pgo binaries, at least that was the case for us. But the\neffort was definitely worth the performance gains we've seen.\n\nI hope you found this helpful for your own endeavors, and we encourage you to\ntry updating your releases as well. If you do, we'd love to hear about it.\nCome by and share your experience on our Discord.\n\nAlso, don't forget to check out each of our different product offerings below:\n\n  * Dolt\u2014it's Git for data.\n  * DoltHub\u2014it's GitHub for data.\n  * DoltLab\u2014it's GitLab for data.\n  * Hosted Dolt\u2014it's RDS for Dolt databases.\n\nSHARE\n\nBlog\n\nDolt is open source\n\nJOIN THE DATA EVOLUTION\n\n# Get started with Dolt\n\nOr join our mailing list to get product updates.\n\n", "frontpage": true}
