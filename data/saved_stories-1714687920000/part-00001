{"aid": "40237883", "title": "Real-time AI using scalable non-expert crowdsourcing in colorectal surgery", "url": "https://www.nature.com/articles/s41746-024-01095-8", "domain": "nature.com", "votes": 10, "user": "zachwdc", "posted_at": "2024-05-02 16:10:49", "comments": 0, "source_title": "Real-time near infrared artificial intelligence using scalable non-expert crowdsourcing in colorectal surgery", "source_text": "Real-time near infrared artificial intelligence using scalable non-expert crowdsourcing in colorectal surgery | npj Digital Medicine\n\nSkip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n\n## Search\n\nAdvanced search\n\n### Quick links\n\n    * Explore articles by subject\n    * Find a job\n    * Guide to authors\n    * Editorial policies\n\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n\n  * Sign up for alerts\n  * RSS feed\n\nReal-time near infrared artificial intelligence using scalable non-expert\ncrowdsourcing in colorectal surgery\n\nDownload PDF\n\nDownload PDF\n\n  * Brief Communication\n  * Open access\n  * Published: 22 April 2024\n\n# Real-time near infrared artificial intelligence using scalable non-expert\ncrowdsourcing in colorectal surgery\n\n  * Garrett Skinner ORCID: orcid.org/0000-0003-3627-7864^1,2,\n  * Tina Chen^2,\n  * Gabriel Jentis^2,\n  * Yao Liu^2,3,\n  * Christopher McCulloh^2,\n  * Alan Harzman^4,\n  * Emily Huang^4,\n  * Matthew Kalady^4 &\n  * ...\n  * Peter Kim ORCID: orcid.org/0000-0002-8932-4962^1,2\n\nnpj Digital Medicine volume 7, Article number: 99 (2024) Cite this article\n\n  * 545 Accesses\n\n  * 4 Altmetric\n\n  * Metrics details\n\n## Abstract\n\nSurgical artificial intelligence (AI) has the potential to improve patient\nsafety and clinical outcomes. To date, training such AI models to identify\ntissue anatomy requires annotations by expensive and rate-limiting surgical\ndomain experts. Herein, we demonstrate and validate a methodology to obtain\nhigh quality surgical tissue annotations through crowdsourcing of non-experts,\nand real-time deployment of multimodal surgical anatomy AI model in colorectal\nsurgery.\n\n### Similar content being viewed by others\n\n### Intelligent surgical workflow recognition for endoscopic submucosal\ndissection with real-time animal study\n\nArticle Open access 21 October 2023\n\n### The Dresden Surgical Anatomy Dataset for Abdominal Organ Segmentation in\nSurgical Data Science\n\nArticle Open access 12 January 2023\n\n### Automated segmentation by deep learning of loose connective tissue fibers\nto define safe dissection planes in robot-assisted gastrectomy\n\nArticle Open access 27 October 2021\n\nSurgical artificial intelligence (AI) is a nascent field with potential to\nimprove patient safety and clinical outcomes. Current surgical AI models can\nidentify surgical phases, critical events, and surgical anatomy^1,2,3. Most of\nthese models utilize supervised machine learning and require large amounts of\nannotated video data, typically by domain experts. Crowdsourcing, using\nlayperson annotations to form consensus annotations, can scale and accelerate\nacquisition of high-quality training data^4.\n\nCrowdsourced annotations of surgical video, however, have historically relied\non unsophisticated crowdsourcing methodologies and have been limited to\nannotations of simple rigid surgical instruments and other non-tissue\nstructures. Models trained to segment laparoscopic surgical instruments\nperformed equally well when trained on non-expert crowdsourced annotations as\nwhen trained on expert annotations^5. However, annotations of deformable and\nmobile surgical tissues are believed to require domain expertise due to\ncomplexity and need for accurate contextual knowledge of surgical anatomy^4.\nThe acquisition of expert-annotated training data is cost-prohibitive, time\nconsuming, and slows the development and deployment of surgical AI models for\nclinical benefit.\n\nHere we describe an application of gamified, continuous-performance-monitored\ncrowdsourcing to obtain annotated training data of surgical tissues used to\ntrain a soft tissue segmentation AI model. We validate this by training and\ndeploying a highly accurate, real-time AI-assisted multimodal imaging platform\nto increase precision when assessing tissue perfusion which may help reduce\ncomplications such as anastomotic leak in bowel surgery^6,7.\n\nAll video data, composed of 95 de-identified colorectal procedures for benign\nand malignant indications (IRB #OSU2021H0218), were included for model\ntraining (train dataset) and testing (test dataset) (Supplementary Table 1,\nMethods). Crowdsourced annotations of the train and test dataset were obtained\nusing a gamified crowdsourcing platform utilizing continuous performance\nmonitoring and performance-based incentivization (Fig. 1a, Methods)^8. Five\ncrowdsourcing parameters were controlled: testing score (TS), running score\n(RS), minimum crowdsource annotations (n), majority vote (MV), and review\nthreshold (RT) (Fig. 1b, Methods).\n\nFig. 1: Gamified crowdsourcing methodology and expert time savings.\n\na Screenshot images of annotation instructions (Centaur Labs, Boston MA) for\nbowel and abdominal wall. b Crowdsource annotation parameters values used for\nbowel and abdominal wall tasks. For test and train datasets: c Number of\nvideos and frames. d Estimated expert hours saved by utilizing crowdsourcing.\ne Crowdsource worker demographics indicating percentage of non-MD/unknown\n(black), MD (green), and surgical MD (red). f Difficulty level (difficulty\nindex) of bowel and abdominal wall (wall) annotations with median values\n(green dashed line).\n\nFull size image\n\nDue to the impracticality of time constraints by experts to annotate the large\ntrain dataset (27,000 frames), a smaller test dataset (510 frames) was\ncreated. This dataset was annotated by crowdsourced workers, the models\ntrained on crowdsourced worker annotations, and one of four surgical experts\nwith surgical domain expertise (Methods). The test dataset was then used to\ncompare the annotations from crowdsourced workers and the models trained from\ncrowdsourced workers to expert annotations. These comparisons were done using\nstandardized metrics of Intersection over Union (IoU) (Supplementary Fig. 1)\nand the harmonic mean of precision and recall (F1) (Methods, Supplementary Eq.\n(1)).\n\nBowel.CSS (bowel crowdsourced segmentation), was trained to segment bowel and\nabdominal wall using crowdsourced annotations of the train dataset.\nAdditionally, a streamlined model was optimized for real-time segmentation of\nbowel and deployed as a part of an AI-assisted multimodal imaging platform\n(Methods).\n\nWe validate the use of non-expert crowdsourcing with the following primary\nendpoints:\n\n  1. 1.\n\nExpertise level of crowdsource workers.\n\n  2. 2.\n\nExpert hours saved.\n\n  3. 3.\n\nAccuracy of the crowdsource annotations to expert annotations.\n\n  4. 4.\n\nAccuracy of the Bowel.CSS model predictions to expert annotations.\n\nSecondary endpoints were:\n\n  1. 1.\n\nDifficulty level of the crowdsourced annotations in the train and test\ndatasets.\n\n  2. 2.\n\nAccuracy of real-time predictions of the deployed Bowel.CSS model to expert\nannotations.\n\nTrain dataset was annotated by 206 crowdsourced workers (CSW) giving 250,000\nindividual annotations and 54,000 consensus annotations of bowel and abdominal\nwall. 3% (7/206) of CSW identified as MDs, and 1% (2/206) identified as\nsurgical MDs. Test dataset was annotated by 48 CSW giving 5100 individual\nannotations and 1020 consensus annotations. 4% (2/48) of CSW identified as\nMDs, and 0% as surgical MDs (Fig. 1c, e, Supplementary Table 1, Methods).\n\nThese demographics indicate non-domain expertise of the CSW. Although\ndemographic data is self-reported and not available for every CSW, the\nplatform reports that the majority of the active CSW are health science\nstudents (59.7%) looking to improve their clinical skills (57.3%)\n(Supplementary Table 2).\n\nOn average, an expert spent 120.3 s annotating a frame for bowel and abdominal\nwall in the test dataset. This extrapolates to an estimated 902 expert hours\nsaved during the annotation of the train dataset by utilizing crowdsourcing\nmethodology, and an estimated 17 expert hours saved in the test dataset (if\nexpert annotations of the test dataset weren\u2019t required for this study).\nAssuming each of the four expert annotators annotated one hour per day, this\nestimates to 120 frames annotated per day. In contrast, CSW annotated an\naverage of 774 frames per day in the train dataset (Fig. 1d, Methods).\n\nThe difficulty of crowdsourced annotations was measured by Difficulty Index\n(DI) (Methods). The median difficulty of the crowdsourced annotations was 0.09\nDI for bowel and 0.12 DI for abdominal wall in the train dataset, and 0.18 DI\nfor bowel and 0.26 DI for abdominal wall in the test dataset, indicating a\nrobust spectrum of task difficulty across the frame populations. (Fig. 1f,\nMethods).\n\nCompared to expert annotations of bowel and abdominal wall within the test\ndataset, crowdsource workers and Bowel.CSS were highly accurate; F1 values of\n0.86 \u00b1 0.20 for bowel and 0.79 \u00b1 0.26 for abdominal wall for crowdsource\nworkers and 0.89 \u00b1 0.16 and 0.78 \u00b1 0.28 for bowel and abdominal wall for\nBowel.CSS (Fig. 2a, b).\n\nFig. 2: Evaluation of crowdsource and model anatomy segmentations and\ndeployment of near-infrared artificial intelligence system.\n\na Crowdsourced annotations and Bowel.CSS predictions of bowel and abdominal\nwall compared to expert annotations in the test dataset. ^aIoU intersection\nover union, ^bF1 dice similarity coefficient. b Representative frames\ncomparing crowdsourced annotations and Bowel.CSS predictions to expert\nannotations with corresponding difficulty index. c Schematic representing\nintraoperative deployment of real-time artificial intelligence. d Example of\ndeployed version of Bowel.CSS incorporated into real-time artificial\nintelligence assisted multimodal imaging utilizing laser speckle contrast\nimaging to allow visualization of physiologic information beyond human vision.\n\nFull size image\n\nA streamlined version of Bowel.CSS optimized for real-time bowel segmentation\nwas deployed in real-time to provide AI-assisted display of multimodal imaging\nand provided highly accurate segmentation of bowel tissue compared to expert\nannotation. This allowed surgeons to visualize physiologic perfusion the colon\nand rectum that is normally invisible to human eye (Fig. 2c, d, Supplementary\nTable 3, Methods).\n\nHerein, we report the first complete and adaptable methodology to obtain\nhighly accurate segmentations of surgical tissues using non-expert\ncrowdsourcing. We outline five crowdsourcing parameters; TS, RS, n, MV, and RT\nwhich could be adjusted to fit a variety of segmentations depending on task\ndifficulty and applications. We validated this methodology by showing the\ncrowdsourced annotations can be used to train a highly accurate surgical\ntissue segmentation model, while greatly accelerating the speed of development\nby eliminating over 900 expert annotation hours. This study is limited by lack\nof source video diversity as all videos came from colorectal procedures at a\nsingle institution, and thus performance may suffer when applied to other\nvideo datasets. Another limitation is the inability to train segmentation\nmodels using both crowdsourced and expert annotations due to the inability to\nsource expert annotations for 27,000 video frames in the train dataset due to\nexpert time constraints. However, the crowdsource annotations and the\ncrowdsource trained model predictions were shown highly accurate to expert\nannotations, and the inability to secure high volume of expert annotations\ndemonstrates the need for crowdsourcing.\n\nWhile we demonstrated that crowdsourcing is viable when scaling these surgical\ntissue annotations, further work should be done to determine the limitations\nof this methodology when applied to increasingly complex anatomical\nstructures. While we showed that the deployed AI model accurately segmented\nbowel as a part of an AI-assisted multimodal imaging platform, future work\nshould be done to investigate clinical outcomes with the use this technology.\nThis accelerated model development using crowdsource annotations will further\nenable additional applications of AI-assisted multimodal imaging data for\nenhanced real-time clinical decision support for safer surgery and improved\noutcomes.\n\n## Methods\n\nThis study was approved by The Ohio State University Institutional Review\nBoard (IRB #OSU2021H0218). All patients provided written informed consent.\n\n### Video source and frame sampling\n\nSurgical videos were obtained from a prospective clinical trial evaluating the\nutility of real-time laser speckle contrast imaging for perfusion assessment\nin colorectal surgery (IRB #OSU2021H0218). In the source material for the\ntrain dataset, video clips were not prefiltered, and frames were extracted at\na regular interval (1 frame per second and 1 frame per 30 seconds) to create a\ndiverse set of training data and eliminate frame selection bias. For the test\ndataset, clips were extracted when the surgeon was assessing perfusion of the\ncolon. Frames were extracted at 1 frame per second to minimize frame selection\nbias. The final video and frame counts are represented in Supplementary Table\n1.\n\n### Crowdsourced annotations\n\nCrowdsourced annotations of bowel and abdominal wall were obtained using a\ngamified crowdsourcing platform (Centaur Labs, Boston MA) utilizing continuous\nperformance monitoring and performance-based incentivization^8. This\nmethodology differs from standard crowdsourcing platforms such as Amazon\u2019s\nMechanical Turk, which don\u2019t allow for such continuous performance monitoring\nand incentivization^9. Previous implementations of crowdsourcing annotations\nin surgical computer vision have typically only utilized the majority vote\ncrowdsourcing parameter^5.\n\nAnnotation instructions were developed utilizing as little specialized\nsurgical knowledge as possible while following surgical data science best\npractices^10. Crowdsourced annotation instructions given to the crowdsourced\nworkers (CSW) included 13 training steps for each task with 11 and 14 example\nannotations of abdominal wall and bowel, respectively (Fig. 1a). Four experts\n(two senior surgical trainees and two trained surgeons) provided expert\nannotations used to calculate training (TS) and running (RS) scores. In our\nstudy, CSW were required to achieve a minimum training score (TS) as measured\nby intersection-over-union (IoU) with 10 expert annotations prior to\nperforming any annotations. A running score (RS) was calculated by\nintermittently testing the CSW in the same fashion. Annotations from CSW with\na sufficient TS and RS were used in consensus generation. A minimum of 5\nannotations (n) were required to generate the consensus crowdsourced\nannotation using the majority vote parameter (MV) to only include pixels\nannotated by 4 or more, and 2 or more annotations for bowel and abdominal wall\nrespectively. Difficulty index (DI) was calculated for each frame using IoU\nwith values between 0 and 1, higher indicating increasing difficulty\n(Supplementary Eq. (2), Methods). Quality assurance (QA) was performed by\nexperts (two surgical trainees) on randomly selected frames above the\ndifficulty review threshold (RT) of 0.4 difficulty index (Fig. 1b).\n\n### SegFormer B3 framework and model training\n\nSegFormer is a semantic segmentation framework developed in partnership with\nNVIDIA and Caltech. It was selected for the real-time implementation for\npowerful and yet efficient semantic segmentation capabilities accomplished by\nunifying transformers with lightweight multilayer perception decoders^11.\n\nUsing the SegFormer B3 framework, we trained two versions of Bowel.CSS.\nBowel.CSS was trained on the entire crowdsource-annotated 27,000 frame dataset\n(78 surgical videos). A second model, Bowel.CSS-deployed, was trained on a\nsubset of the train dataset (3500 frames from 11 surgical videos) and\noptimized for real-time segmentation of bowel. This model was deployed in\nreal-time as a part of an AI-assisted multimodal imaging platform (Methods).\n\n### Train and test dataset crowdsourced annotations and demographics\n\nTrain dataset frames (n = 27,000) were annotated by 206 CSW giving 250,000\nindividual annotations and 54,000 consensus annotations of bowel and abdominal\nwall. 3% (7/206) of CSW identified as MDs, and 1% (2/206) identified as\nsurgical MDs. Test dataset frames (n = 510) were annotated by 48 CSW giving\n5100 individual annotations and 1020 consensus annotations. 4% (2/48) of CSW\nidentified as MDs, and 0% as surgical MDs (Fig. 1c, e, Supplementary Table 1,\nMethods).\n\nTo further characterize \u201cunknown\u201d CSW demographics in the crowdsource user\npopulation in this study, Supplementary Table 3 presents CSW demographics for\nthe entire annotation platform in the year 2022. It shows the majority (59.7%)\nwere health science students, and the majority listed the reason for\nparticipating in crowdsource annotations as \u201cto improve my skills\u201d (57.3%).\nThis supports the conclusion that most users on this platform are non-\nphysicians and are not full-time annotators.\n\n### Crowdsource vs expert hours saved\n\nA primary goal of the use of crowdsourced annotations is to mitigate the rate-\nlimiting and expensive time of experts. The average time for the three domain\nexperts to complete a frame annotation for bowel and abdominal wall was 120.3\ns in test dataset. Using the average time to annotate, and the frame totals of\n27,000 and 510, crowdsourcing saved an estimated 902 expert hours in the train\ndataset, and 17 in the test dataset (if experts would have not been required\nto annotate the test dataset for this study).\n\n### Annotation comparison statistics\n\nThe pixel-level agreement of both crowdsourced and Bowel.CSS annotations were\ncompared to expert annotation using accuracy, sensitivity, specificity, IoU\nand F1 scores (Supplementary Fig. 1, Supplementary Eq. (1)). These metrics are\naccepted measurements of accuracy of segmentation annotations in computer\nvision and surgical data science^12.\n\n### Difficulty index\n\nDifficulty of the annotation task was measured per frame using a difficulty\nindex (DI) defined in Supplementary Equation 2 which utilizes the average\ninter-annotator agreement of the individual CSW annotations to the\ncrowdsourced consensus annotation as measured by IoU. This index is supported\nby evidence that lower inter-annotator agreement has shown to be an indicator\nof higher annotation difficulty when other factors such domain expertise,\nannotation expertise, instructions, platform and source material are\nconstant^13,14. DI values range from 0 (100% inter-annotator agreement) to 1\n(0% inter-annotator agreement). Values closer to 0 indicate easier frames,\nespecially when the annotation target is not visible and the annotation of \u201cno\nfinding\u201d is used since annotations of \u201cno finding\u201d are in 100% agreement.\nValues closer to 1 indicate harder frames where there is less agreement\namongst the CSWs.\n\nThe DI of bowel was 0.09 and 0.12 for abdominal wall in the train dataset and\nwas lower than the DI of 0.18 for bowel and 0.12 for abdominal wall in the\ntest dataset. The train dataset included full surgical videos versus the test\ndataset, which included only clips of surgeons assessing perfusion of the\nbowel, leading to an increased proportion of \u201cno finding\u201d annotation of bowel\n(22%) and abdominal wall (32%) in train dataset versus 2.4% and 11% for bowel\nand abdominal wall in the test dataset. The \u201cno finding\u201d annotations have low\ndifficulty indices leading to the lower median difficulty of the train\ndataset.\n\n### Real-time deployment of near infrared artificial intelligence\n\nAdvanced near infrared physiologic imaging like indocyanine green fluorescence\nangiography and laser speckle contrast imaging show levels of tissue perfusion\nbeyond what is visible in standard white light imaging. These technologies are\nused in colorectal resections to ensure adequate perfusion of the colon and\nrectum during reconstruction to reduce complications and improve patient\noutcomes. Subjectively interpreting physiologic imaging can be challenging and\nis dependent on user experience.\n\nBowel.CSS was developed to mask the physiologic imaging data to only those\ntissues relevant to the surgeon during colorectal resection and reconstruction\nto assist with interpretation of the visual signal. The output of this model\nwas the bowel label only and it was deployed in real-time on a modified\nresearch unit of a commercially available advanced physiologic imaging\nplatform for laparoscopic, robotic, and open surgery.\n\nBowel.CSS-deployed successfully segmented the bowel in real-time during 2\ncolorectal procedures at 10 frames per second. The intraoperative labels were\nnot saved from the procedures, so to evaluate the intraoperative performance\nof the model, 10 s clips from each procedure were sampled at 1 FPS (20 frames\ntotal) from when the surgeon activated the intraoperative AI model. To assess\nfor accuracy, the model outputs of Bowel.CSS and Bowel.CSS-deployed were\ncompared to annotations by one of three surgical experts (1 trainee and 2\nboard-certified surgeons). Model outputs were compared to the expert\nannotations in these 20 frames using standard computer vision metrics.\n(Supplementary Table 3).\n\n### Reporting summary\n\nFurther information on research design is available in the Nature Research\nReporting Summary linked to this article.\n\n## Data availability\n\nRequests for additional study data will be evaluated by the corresponding\nauthor upon request.\n\n## Code availability\n\nThe trained Bowel.CSS models are available free and open source\n(https://github.com/ACTIV-Sugical/Bowel.CSS).\n\n## References\n\n  1. Madani, A. et al. Artificial Intelligence for Intraoperative Guidance: Using Semantic Segmentation to Identify Surgical Anatomy During Laparoscopic Cholecystectomy. Ann. Surg. 276, 363\u2013369 (2022).\n\nArticle PubMed Google Scholar\n\n  2. Mascagni, P. et al. A Computer Vision Platform to Automatically Locate Critical Events in Surgical Videos: Documenting Safety in Laparoscopic Cholecystectomy. Ann. Surg. 274, e93\u2013e95 (2021).\n\nArticle PubMed Google Scholar\n\n  3. Hashimoto, D. A. et al. Computer Vision Analysis of Intraoperative Video: Automated Recognition of Operative Steps in Laparoscopic Sleeve Gastrectomy. Ann. Surg. 270, 414 (2019).\n\nArticle PubMed Google Scholar\n\n  4. Ward, T. M. et al. Challenges in surgical video annotation. Comput. Assist. Surg. 26, 58\u201368 (2021).\n\nArticle Google Scholar\n\n  5. Maier-Hein, L. et al. Can Masses of Non-Experts Train Highly Accurate Image Classifiers?: A Crowdsourcing Approach to Instrument Segmentation in Laparoscopic Images. In Medical Image Computing and Computer-Assisted Intervention \u2013 MICCAI 2014 (eds. Golland, P., Hata, N., Barillot, C., Hornegger, J. & Howe, R.) 8674 438\u2013445 (Springer International Publishing, Cham, 2014).\n\n  6. Vignali, A. et al. Altered microperfusion at the rectal stump is predictive for rectal anastomotic leak. Dis. Colon Rectum. 43, 76\u201382 (2000).\n\nArticle CAS PubMed Google Scholar\n\n  7. Skinner, G. et al. Clinical Utility of Laser Speckle Contrast Imaging (LSCI) Compared to Indocyanine Green (ICG) and Quantification of Bowel Perfusion in Minimally Invasive, Left-Sided Colorectal Resections. Dis. Colon. Rectum (In press).\n\n  8. Van Gaalen, A. E. J. et al. Gamification of health professions education: a systematic review. Adv. Health Sci. Educ. 26, 683\u2013711 (2021).\n\nArticle Google Scholar\n\n  9. Bhattacherjee, A. & Fitzgerald, B. Shaping the Future of ICT Research: Methods and Approaches. In IFIP WG 8.2 Working Conference, Tampa, FL, USA, Proceedings. (Springer, Heidelberg New York, 2012).\n\n  10. R\u00e4dsch, T. et al. Labelling instructions matter in biomedical image analysis. Nat. Mach. Intell. 5, 273\u2013283 (2023).\n\nArticle Google Scholar\n\n  11. Xie, E. et al. SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers. Preprint at http://arxiv.org/abs/2105.15203 (2021).\n\n  12. Hicks, S. A. et al. On evaluation metrics for medical applications of artificial intelligence. Sci. Rep. 12, 5979 (2022).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  13. Kentley, J. et al. Agreement Between Experts and an Untrained Crowd for Identifying Dermoscopic Features Using a Gamified App: Reader Feasibility Study. JMIR Med. Inf. 11, e38412 (2023).\n\nArticle Google Scholar\n\n  14. Ribeiro, V., Avila, S. & Valle, E. Handling Inter-Annotator Agreement for Automated Skin Lesion Segmentation. Preprint at http://arxiv.org/abs/1906.02415 (2019).\n\nDownload references\n\n## Acknowledgements\n\nThis study was funded by Activ Surgical, Inc. (Boston, MA). We extend our\ngratitude to the hardworking research team at The Ohio State University Wexner\nMedical Center for collection and generation of the de-identified video\ndatabase used in this study.\n\n## Author information\n\n### Authors and Affiliations\n\n  1. Jacobs School of Medicine and Biomedical Sciences, University at Buffalo, Buffalo, NY, USA\n\nGarrett Skinner & Peter Kim\n\n  2. Activ Surgical, University at Buffalo, Buffalo, NY, USA\n\nGarrett Skinner, Tina Chen, Gabriel Jentis, Yao Liu, Christopher McCulloh &\nPeter Kim\n\n  3. Warren Alpert Medical School Alpert Medical School of Brown University, Providence, RI, USA\n\nYao Liu\n\n  4. The Ohio State University Wexner Medical Center, Columbus, OH, USA\n\nAlan Harzman, Emily Huang & Matthew Kalady\n\nAuthors\n\n  1. Garrett Skinner\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  2. Tina Chen\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  3. Gabriel Jentis\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  4. Yao Liu\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  5. Christopher McCulloh\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  6. Alan Harzman\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  7. Emily Huang\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  8. Matthew Kalady\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  9. Peter Kim\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n### Contributions\n\nT.C., G.S., and P.K. were responsible for designing and training the initial\nartificial intelligence models. T.C. and G.S. were responsible for obtaining\ncrowdsourced annotations. G.J, T.C., C.M. and G.S. performed data analysis on\ncrowdsourcing demographics and comparisons to expert annotators. G.S., C.M.,\nY.L, and P.K. provided expert annotations. A.H., M.K., and E.H., provided\ndesign considerations and clinical feedback during training and deployment of\nartificial intelligence models. P.K. supervised this work. All authors\ncontributed to manuscript preparation, critical revisions, and have read and\napproved the manuscript.\n\n### Corresponding author\n\nCorrespondence to Peter Kim.\n\n## Ethics declarations\n\n### Competing interests\n\nThis study was funded by Activ Surgical Inc., Boston, MA. Current or previous\nconsultants for Activ Surgical Inc.: G.S., A.H., M.K. Current or previous\nemployment by Activ Surgical Inc.: T.C., G.J., C.M., Y.L. Founder/Ownership of\nActiv Surgical Inc.: P.K. No competing interests: E.H.\n\n## Additional information\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional\nclaims in published maps and institutional affiliations.\n\n## Supplementary information\n\n### Supplemental materials\n\n### Reporting Summary\n\n## Rights and permissions\n\nOpen Access This article is licensed under a Creative Commons Attribution 4.0\nInternational License, which permits use, sharing, adaptation, distribution\nand reproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the\nCreative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article\u2019s\nCreative Commons licence, unless indicated otherwise in a credit line to the\nmaterial. If material is not included in the article\u2019s Creative Commons\nlicence and your intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this licence, visit\nhttp://creativecommons.org/licenses/by/4.0/.\n\nReprints and permissions\n\n## About this article\n\n### Cite this article\n\nSkinner, G., Chen, T., Jentis, G. et al. Real-time near infrared artificial\nintelligence using scalable non-expert crowdsourcing in colorectal surgery.\nnpj Digit. Med. 7, 99 (2024). https://doi.org/10.1038/s41746-024-01095-8\n\nDownload citation\n\n  * Received: 31 August 2023\n\n  * Accepted: 29 March 2024\n\n  * Published: 22 April 2024\n\n  * DOI: https://doi.org/10.1038/s41746-024-01095-8\n\n### Share this article\n\nAnyone you share the following link with will be able to read this content:\n\nSorry, a shareable link is not currently available for this article.\n\nProvided by the Springer Nature SharedIt content-sharing initiative\n\n### Subjects\n\n  * Health care\n  * Medical research\n  * Translational research\n\nDownload PDF\n\nAdvertisement\n\nnpj Digital Medicine (npj Digit. Med.) ISSN 2398-6352 (online)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n  * Cancel contracts here\n\n\u00a9 2024 Springer Nature Limited\n\nSign up for the Nature Briefing: Translational Research newsletter \u2014 top\nstories in biotechnology, drug discovery and pharma.\n\nGet what matters in translational research, free to your inbox weekly. Sign up\nfor Nature Briefing: Translational Research\n\n", "frontpage": true}
