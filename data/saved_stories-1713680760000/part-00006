{"aid": "40101670", "title": "Pandas API on Spark", "url": "https://spark.apache.org/pandas-on-spark/", "domain": "apache.org", "votes": 1, "user": "MrPowers", "posted_at": "2024-04-20 22:44:17", "comments": 0, "source_title": "pandas API on Spark | Apache Spark", "source_text": "pandas API on Spark | Apache Spark\n\n# pandas API on Spark\n\nThis page describes the advantages of the pandas API on Spark (\u201cpandas on\nSpark\u201d) and when you should use it instead of pandas (or in conjunction with\npandas).\n\npandas on Spark can be much faster than pandas and offers syntax that is\nfamiliar to pandas users. It offers the power of Spark with the familiarity of\npandas.\n\nHere are the main advantages of pandas on Spark:\n\n  * Faster query execution on single-machine workloads (because pandas on Spark uses all available cores, processes queries in parallel, and optimizes queries)\n  * pandas on Spark is scalable to multiple machines in a cluster and can process big data\n  * pandas on Spark allows queries to be run on larger than memory datasets\n  * pandas on Spark provides familiar syntax for pandas users\n\npandas has many limitations:\n\n  * pandas must load all data into memory before running a query which can be slow\n  * pandas cannot process datasets that are larger than the available memory on a single machine (because all data must be loaded into memory)\n  * pandas computations are run on a single core and don\u2019t leverage all available cores of a single machine\n  * pandas computations cannot be scaled out to multiple machines\n  * pandas doesn\u2019t have a query optimizer, so users have to manually code optimizations or suffer from slow code\n\nLet\u2019s look at some simple examples to get a better understanding of how pandas\non Spark overcomes the limitations of pandas. We\u2019ll also investigate the\nlimitations of pandas on Spark.\n\nAt the end of this page, you will see how you can use both pandas and pandas\non Spark in conjunction. It\u2019s not an either/or decision - in many situations\nusing both is a great option.\n\n## pandas on Spark example\n\nThis section demonstrates how pandas on Spark can run a query on a single file\non localhost faster than pandas. pandas on Spark isn\u2019t necessarily faster for\nall queries, but this example shows when it provides a nice speed-up.\n\nSuppose you have a Parquet file with 9 columns and 1 billion rows of data.\nHere are the first three rows of the file:\n\n    \n    \n    +-------+-------+--------------+-------+-------+--------+------+------+---------+ | id1 | id2 | id3 | id4 | id5 | id6 | v1 | v2 | v3 | |-------+-------+--------------+-------+-------+--------+------+------+---------| | id016 | id046 | id0000109363 | 88 | 13 | 146094 | 4 | 6 | 18.8377 | | id039 | id087 | id0000466766 | 14 | 30 | 111330 | 4 | 14 | 46.7973 | | id047 | id098 | id0000307804 | 85 | 23 | 187639 | 3 | 5 | 47.5773 | +-------+-------+--------------+-------+-------+--------+------+------+---------+\n\nHere is how you can use pandas on Spark to read the file and run a group by\naggregation.\n\n    \n    \n    import pyspark.pandas as ps df = ps.read_parquet(\"G1_1e9_1e2_0_0.parquet\")[ [\"id1\", \"id2\", \"v3\"] ] df.query(\"id1 > 'id098'\").groupby(\"id2\").sum().head(3)\n\nThis query runs in 62 seconds when executed on a 2020 M1 Macbook with 64 GB of\nRAM with Spark 3.5.0.\n\nLet\u2019s compare this with unoptimized pandas code.\n\n    \n    \n    import pandas as pd df = pd.read_parquet(\"G1_1e9_1e2_0_0.parquet\")[ [\"id1\", \"id2\", \"v3\"] ] df.query(\"id1 > 'id098'\").groupby(\"id2\").sum().head(3)\n\nThis query errors out because the machine with 64GB of RAM does not have\nenough space to store 1 billion rows of data in memory.\n\nLet\u2019s manually add some pandas optimizations to make the query run:\n\n    \n    \n    df = pd.read_parquet( \"G1_1e9_1e2_0_0.parquet\", columns=[\"id1\", \"id2\", \"v3\"], filters=[(\"id1\", \">\", \"id098\")], engine=\"pyarrow\", ) df.query(\"id1 > 'id098'\").groupby(\"id2\").sum().head(3)\n\nThis query runs in 275 seconds with pandas 2.2.0.\n\nCoding these optimizations manually with pandas can potentially give the wrong\nresults. Here\u2019s an example of a group by query that\u2019s correct, but with a row-\ngroup filtering predicate that\u2019s wrong:\n\n    \n    \n    df = pd.read_parquet( \"G1_1e9_1e2_0_0.parquet\", columns=[\"id1\", \"id2\", \"v3\"], filters=[(\"id1\", \"==\", \"id001\")], engine=\"pyarrow\", ) df.query(\"id1 > 'id098'\").groupby(\"id2\").sum().head(3)\n\nThis returns the wrong result, even though the group by aggregation logic is\nright!\n\nWith pandas, you need to manually apply column pruning and row-group filtering\nwhen reading a Parquet file. With pandas on Spark, the Spark optimizer\nautomatically applies these query enhancements, so you do not need to type\nthem manually.\n\nLet\u2019s investigate the advantages of pandas on Spark in more detail.\n\n## Advantages of pandas on Spark\n\nLet\u2019s recap the advantages of pandas on Spark:\n\nFaster query execution\n\npandas on Spark can execute queries faster than pandas because it uses all the\navailable cores to parallelize computations and optimizes queries before\nrunning them to allow for efficient execution.\n\npandas computations only run on a single core.\n\nScalable to larger-than-memory datasets\n\npandas loads data in memory before running queries, so it can only query\ndatasets that fit into memory.\n\nSpark can query datasets that are larger than memory by streaming the data and\nincrementally running computations.\n\npandas is notorious for erroring out when dataset sizes grow and Spark doesn\u2019t\nhave this limitation.\n\nRunnable on a cluster of many machines\n\nSpark can be run on a single machine or distributed to many machines in a\ncluster.\n\nWhen Spark is run on a single machine, the computations are run on all\navailable cores. This is often faster than pandas which only runs computations\non a single core.\n\nScaling computations on multiple machines is great for when you want to run\ncomputations on larger data sets or simply access more RAM/cores so the\nqueries run faster.\n\nFamiliar syntax for pandas users\n\npandas on Spark was designed to provide familiar syntax for pandas users.\n\nThe familiar syntax is the whole point - pandas on Spark provides the power of\nSpark with the same syntax that pandas users are accustomed to.\n\nProvides access to Spark\u2019s battle-tested query optimizer\n\npandas on Spark computations are optimized by Spark\u2019s Catalyst optimizer\nbefore they\u2019re executed.\n\nThese optimizations simplify queries and add optimizations.\n\nEarlier in this post, we saw how Spark automatically adds the column\npruning/row group filtering optimizations when reading Parquet files for\nexample. pandas doesn\u2019t have a query optimizer, so you need to add these\noptimizations yourself. Manually adding optimizations is tedious and error-\nprone. If you don\u2019t manually apply the right optimizations, your query will\nreturn the wrong result.\n\n## Limitations of pandas on Spark\n\npandas on Spark doesn\u2019t support all the APIs that pandas supports for two\nreasons:\n\n  * some features haven\u2019t been added to pandas on Spark yet\n\n  * some pandas features don\u2019t make sense with Spark\u2019s distributed, parallel execution model\n\nSpark breaks up DataFrames into multiple chunks so they can be processed in\nparallel, so certain pandas operations don\u2019t transition well to Spark\u2019s\nexecution model.\n\n## Using pandas on Spark in conjunction with regular pandas\n\nIt\u2019s often useful to use pandas on Spark and pandas to get the best of both\nworlds.\n\nSuppose you have a large dataset that you clean and aggregate into a smaller\ndataset that\u2019s passed into a scikit-learn machine learning model.\n\nYou can clean and aggregate the dataset with pandas on Spark to take advantage\nof fast query times and parallel execution. Once the dataset is processed, you\ncan convert it to a pandas DataFrame with to_pandas() and then run the machine\nlearning model with scikit-learn. This approach works well if the dataset can\nbe reduced enough to fit in a pandas DataFrame.\n\n## The pandas on Spark query execution model is different\n\npandas on Spark executes queries completely differently than pandas.\n\npandas on Spark uses lazy evaluation. It converts the query to an unresolved\nlogical plan, optimizes it with Spark, and only runs computations when results\nare requested.\n\npandas uses eager evaluation. It loads all the data into memory and executes\noperations immediately when they are invoked. pandas does not apply query\noptimization and all the data must be loaded into memory before the query is\nexecuted.\n\nWhen comparing pandas on Spark and pandas, you must be careful to factor in\nhow much time it takes to load the data into memory and how much time it takes\nto run the query. A lot of datasets take a long time to load into pandas.\n\nYou can also load data into memory with pandas on Spark, but this is often\nconsidered an anti-pattern. A dataset that\u2019s loaded into memory won\u2019t be\nupdated if the data in storage changes (via an append, merge, or deletion).\nPersisting a Spark DataFrame is wise in certain situations to speed up query\ntimes, but must be used carefully because it can cause incorrect query\nresults.\n\n## How pandas on Spark and PySpark differ\n\npandas on Spark and PySpark both take queries, convert them to unresolved\nlogical plans, and then execute them with Spark.\n\nPySpark and pandas on Spark both have similar query execution models.\nConverting a query to an unresolved logical plan is relatively quick.\nOptimizing the query and executing it takes much more time. So PySpark and\npandas on Spark should have similar performance.\n\nThe main difference between pandas on Spark and PySpark is just the syntax.\n\n## Conclusion\n\npandas on Spark is a great alternative for pandas users who would like to run\ntheir queries faster and want to leverage Spark\u2019s optimizer rather than\nwriting their own optimizations.\n\npandas on Spark uses syntax that\u2019s familiar to pandas users, so it\u2019s easy to\nlearn.\n\npandas on Spark is also a great technology to be used in conjunction with\npandas. You can use the big-data and performant processing capabilities of\npandas on Spark to process datasets before they\u2019re converted into pandas\nDataFrames that are compatible with other technologies.\n\nCheck out the docs to learn more about how to use pandas on Spark.\n\n##### Latest News\n\n  * Spark 3.4.3 released (Apr 18, 2024)\n  * Spark 3.5.1 released (Feb 23, 2024)\n  * Spark 3.3.4 released (Dec 16, 2023)\n  * Spark 3.4.2 released (Nov 30, 2023)\n\nArchive\n\nDownload Spark\n\nBuilt-in Libraries:\n\n  * SQL and DataFrames\n  * Spark Streaming\n  * MLlib (machine learning)\n  * GraphX (graph)\n\nThird-Party Projects\n\nApache Spark, Spark, Apache, the Apache feather logo, and the Apache Spark\nproject logo are either registered trademarks or trademarks of The Apache\nSoftware Foundation in the United States and other countries. See guidance on\nuse of Apache Spark trademarks. All other marks mentioned may be trademarks or\nregistered trademarks of their respective owners. Copyright \u00a9 2018 The Apache\nSoftware Foundation, Licensed under the Apache License, Version 2.0.\n\n", "frontpage": false}
