{"aid": "40091753", "title": "Databases: How they work, and a brief history (2019)", "url": "https://seldo.com/posts/databases_how_they_work_and_a_brief_history", "domain": "seldo.com", "votes": 1, "user": "ishandotpage", "posted_at": "2024-04-19 20:50:03", "comments": 0, "source_title": "Databases: how they work, and a brief history | Seldo.com", "source_text": "Databases: how they work, and a brief history | Seldo.com\n\n# Seldo.com\n\n19 November, 2019\n\n# Databases: how they work, and a brief history\n\nMy twitter-friend Simon had a simple question that contained much complexity:\nhow do databases work?\n\n> Ok, so databases really confuse me, like how do databases even work?\n>\n> \u2014 Simon Legg (@simonleggsays) November 18, 2019\n\nI don't have a job at the moment, and I really love databases and also\nteaching things to web developers, so this was a perfect storm for me:\n\n> To what level of detail would you like an answer? I love databases.\n>\n> \u2014 Laurie Voss (@seldo) November 18, 2019\n\nThe result was an absurdly long thread of 70+ tweets, in which I expounded on\nthe workings and history of databases as used by modern web developers, and\nSimon chimed in on each tweet with further questions and requests for\nclarification. The result of this collaboration was a super fun tiny\nexplanation of databases which many people said they liked, so here it is,\nlightly edited for clarity.\n\n## What is a database?\n\nLet's start at the very most basic thing, the words we're using: a \"database\"\nliterally just means \"a structured collection of data\". Almost anything meets\nthis definition \u2013 an object in memory, an XML file, a list in HTML. It's super\nbroad, so we call some radically different things \"databases\".\n\nThe thing people use all the time is, formally, a Database Management System,\nabbreviated to DBMS. This is a piece of software that handles access to the\npile of data. Technically one DBMS can manage multiple databases (MySQL and\npostgres both do this) but often a DBMS will have just one database in it.\n\nBecause it's so frequent that the DBMS has one DB in it we often call a DBMS a\n\"database\". So part of the confusion around databases for people new to them\nis because we call so many things the same word! But it doesn't really matter,\nyou can call an DBMS a \"database\" and everyone will know what you mean. MySQL,\nRedis, Postgres, RedShift, Oracle etc. are all DBMS.\n\nSo now we have a mental model of a \"database\", really a DBMS: it is a piece of\nsoftware that manages access to a pile of structured data for you. DBMSes are\noften written in C or C++, but it can be any programming language; there are\ndatabases written in Erlang and JavaScript. One of the key differences between\nDBMSes is how they structure the data.\n\n## Relational databases\n\nRelational databases, also called RDBMS, model data as a table, like you'd see\nin a spreadsheet. On disk this can be as simple as comma-separated values: one\nrow per line, commas between columns, e.g. a classic example is a table of\nfruits:\n\n    \n    \n    apple,10,5.00 orange,5,6.50\n\nThe DBMS knows the first column is the name, the second is the number of\nfruits, the third is the price. Sometimes it will store that information in a\ndifferent database! Sometimes the metadata about what the columns are will be\nin the database file itself. Because it knows about the columns, it can handle\nniceties for you: for example, the first column is a string, the second is an\ninteger, the third is dollar values. It can use that to make sure it returns\nthose columns to you correctly formatted, and it can also store numbers more\nefficiently than just strings of digits.\n\nIn reality a modern database is doing a whole bunch of far more clever\noptimizations than just comma separated values but it's a mental model of\nwhat's going on that works fine. The data all lives on disk, often as one big\nfile, and the DBMS caches parts of it in memory for speed. Sometimes it has\ndifferent files for the data and the metadata, or for indexes that make it\neasier to find things quickly, but we can safely ignore those details.\n\nRDBMS are older, so they date from a time when memory was really expensive, so\nthey usually optimize for keeping most things on disk and only put some stuff\nin memory. But they don't have to: some RDBMS keep everything in memory and\nnever write to disk. That makes them much faster!\n\nIs it still a database if all the structured data stays in memory? Sure. It's\na pile of structured data. Nothing in that definition says a disk needs to be\ninvolved.\n\nSo what does the \"relational\" part of RDBMS mean? RDBMS have multiple tables\nof data, and they can relate different tables to each other. For instance,\nimagine a new table called \"Farmers\":\n\nID| Name  \n---|---  \n1| bob  \n2| susan  \n  \nand we modify the Fruits table:\n\nFarmer ID| Fruit| Quantity| Price  \n---|---|---|---  \n1| apple| 10| 5.00  \n1| orange| 5| 6.50  \n2| apple| 20| 6.00  \n2| orange| 1| 4.75  \n  \nThe Farmers table gives each farmer a name and an ID. The Fruits table now has\na column that gives the Farmer ID, so you can see which farmer has which fruit\nat which price.\n\nWhy's that helpful? Two reasons: space and time. Space because it reduces data\nduplication. Remember, these were invented when disks were expensive and slow!\nStoring the data this way lets you only list \"susan\" once no matter how many\nfruits she has. If she had a hundred kinds of fruit you'd be saving quite a\nlot of storage by not repeating her name over and over. The time reason comes\nin if you want to change Susan's name. If you repeated her name hundreds of\ntimes you would have to do a write to disk for each one (and writes were very\nslow at the time this was all designed). That would take a long time, plus\nthere's a chance you could miss one somewhere and suddenly Susan would have\ntwo names and things would be confusing.\n\nRelational databases make it easy to do certain kinds of queries. For\ninstance, it's very efficient to find out how many fruits there are in total:\nyou just add up all the numbers in the Quantity column in Fruits, and you\nnever need to look at Farmers at all. It's efficient and because the DBMS\nknows where the data is you can say \"give me the sum of the quantity colum\"\npretty simply in SQL, something like SELECT SUM(Quantity) FROM Fruits. The\nDBMS will do all the work.\n\n## NoSQL databases\n\nSo now let's look at the NoSQL databases. These were a much more recent\ninvention, and the economics of computer hardware had changed: memory was a\nlot cheaper, disk space was absurdly cheap, processors were a lot faster, and\nprogrammers were very expensive. The designers of newer databases could make\ndifferent trade-offs than the designers of RDBMS.\n\nThe first difference of NoSQL databases is that they mostly don't store things\non disk, or do so only once in a while as a backup. This can be dangerous \u2013 if\nyou lose power you can lose all your data \u2013 but often a backup from a few\nminutes or seconds ago is fine and the speed of memory is worth it. A database\nlike Redis writes everything to disk every 200ms or so, which is hardly any\ntime at all, while doing all the real work in memory.\n\nA lot of the perceived performance advantages of \"noSQL\" databases is just\nbecause they keep everything in memory and memory is very fast and disks, even\nmodern solid-state drives, are agonizingly slow by comparison. It's nothing to\ndo with whether the database is relational or not-relational, and nothing at\nall to do with SQL.\n\nBut the other thing NoSQL database designers did was they abandoned the\n\"relational\" part of databases. Instead of the model of tables, they tended to\nmodel data as objects with keys. A good mental model of this is just JSON:\n\n    \n    \n    [ {\"name\":\"bob\"} {\"name\":\"susan\",\"age\":55} ]\n\nAgain, just as a modern RDBMS is not really writing CSV files to disk but is\ndoing wildly optimized stuff, a NoSQL database is not storing everything as a\nsingle giant JSON array in memory or disk, but you can mentally model it that\nway and you won't go far wrong. If I want the record for Bob I ask for ID 0,\nSusan is ID 1, etc..\n\nOne advantage here is that I don't need to plan in advance what I put in each\nrecord, I can just throw anything in there. It can be just a name, or a name\nand an age, or a gigantic object. With a relational DB you have to plan out\ncolumns in advance, and changing them later can be tricky and time-consuming.\n\nAnother advantage is that if I want to know everything about a farmer, it's\nall going to be there in one record: their name, their fruits, the prices,\neverything. In a relational DB that would be more complicated, because you'd\nhave to query the farmers and fruits tables at the same time, a process called\n\"joining\" the tables. The SQL \"JOIN\" keyword is one way to do this.\n\nOne disadvantage of storing records as objects like this, formally called an\n\"object store\", is that if I want to know how many fruits there are in total,\nthat's easy in an RDBMS but harder here. To sum the quantity of fruits, I have\nto retrieve each record, find the key for fruits, find all the fruits, find\nthe key for quantity, and add these to a variable. The DBMS for the object\nstore may have an API to do this for me if I've been consistent and made all\nthe objects I stored look the same. But I don't have to do that, so there's a\nchance the quantities are stored in different places in different objects,\nmaking it quite annoying to get right. You often have to write code to do it.\n\nBut sometimes that's okay! Sometimes your app doesn't need to relate things\nacross multiple records, it just wants all the data about a single key as fast\nas possible. Relational databases are best for the former, object stores the\nbest for the latter, but both types can answer both types of questions.\n\nSome of the optimizations I mentioned both types of DBMS use are to allow them\nto answer the kinds of questions they're otherwise bad at. RDBMS have \"object\"\ncolumns these days that let you store object-type things without adding and\nremoving columns. Object stores frequently have \"indexes\" that you can set up\nto be able to find all the keys in a particular place so you can sum up things\nlike Quantity or search for a specific Fruit name fast.\n\nSo what's the difference between an \"object store\" and a \"noSQL\" database? The\nfirst is a formal name for anything that stores structured data as objects\n(not tables). The second is... well, basically a marketing term. Let's digress\ninto some tech history!\n\n## The self-defeating triumph of MySQL\n\nBack in 1995, when the web boomed out of nowhere and suddenly everybody needed\na database, databases were mostly commercial software, and expensive. To the\nrescue came MySQL, invented 1995, and Postgres, invented 1996. They were free!\nThis was a radical idea and everybody adopted them, partly because nobody had\nany money back then \u2013 the whole idea of making money from websites was new and\nun-tested, there was no such thing as a multi-million dollar seed round. It\nwas free or nothing.\n\nThe primary difference between PostgreSQL and MySQL was that Postgres was very\ngood and had lots of features but was very hard to install on Windows (then,\nas now, the overwhelmingly most common development platform for web devs).\nMySQL did almost nothing but came with a super-easy installer for Windows. The\nresult was MySQL completely ate Postgres' lunch for years in terms of market\nshare.\n\nLots of database folks will dispute my assertion that the Windows installer is\nwhy MySQL won, or that MySQL won at all. But MySQL absolutely won, and it was\nbecause of the installer. MySQL became so popular it became synonymous with\n\"database\". You started any new web app by installing MySQL. Web hosting plans\ncame with a MySQL database for free by default, and often no other databases\nwere even available on cheaper hosts, which further accelerated MySQL's rise:\ndefaults are powerful.\n\nThe result was people using mySQL for every fucking thing, even for things it\nwas really bad at. For instance, because web devs move fast and change things\nthey had to add new columns to tables all the time, and as I mentioned RDBMS\nare bad at that. People used MySQL to store uploaded image files, gigantic\nblobs of binary data that have no place in a DBMS of any kind.\n\nPeople also ran into a lot of problems with RDBMS and MySQL in particular\nbeing optimized for saving memory and storing everything on disk. It made huge\ndatabases really slow, and meanwhile memory had got a lot cheaper. Putting\ntons of data in memory had become practical.\n\n## The rise of in-memory databases\n\nThe first software to really make use of how cheap memory had become was\nMemcache, released in 2003. You could run your ordinary RDBMS queries and just\nthrow the results of frequent queries into Memcache, which stored them in\nmemory so they were way, WAY faster to retrieve the second time. It was a\nrevolution in performance, and it was an easy optimization to throw into your\nexisting, RDBMS-based application.\n\nBy 2009 somebody realized that if you're just throwing everything in a cache\nanyway, why even bother having an RDBMS in the first place? Enter MongoDB and\nRedis, both released in 2009. To contrast themselves with the dominant \"MySQL\"\nthey called themselves \"NoSQL\".\n\nWhat's the difference between an in-memory cache like Memcache and an in-\nmemory database like Redis or MongoDB? The answer is: basically nothing. Redis\nand Memcache are fundamentally almost identical, Redis just has much better\nmechanisms for retrieving and accessing the data in memory. A cache is a kind\nof DB, Memcache is a DBMS, it's just not as easy to do complex things with it\nas Redis.\n\nPart of the reason Mongo and Redis called themselves NoSQL is because, well,\nthey didn't support SQL. Relational databases let you use SQL to ask questions\nabout relations across tables. Object stores just look up objects by their key\nmost of the time, so the expressiveness of SQL is overkill. You can just make\nan API call like get(1) to get the record you want.\n\nBut this is where marketing became a problem. The NoSQL stores (being in\nmemory) were a lot faster than the relational DBMS (which still mostly used\ndisk). So people got the idea that SQL was the problem, that SQL was why RDBMS\nwere slow. The name \"NoSQL\" didn't help! It sounded like getting rid of SQL\nwas the point, rather than a side effect. But what most people liked about the\nNoSQL databases was the performance, and that was just because memory is\nfaster than disk!\n\nOf course, some people genuinely do hate SQL, and not having to use SQL was\nattractive to them. But if you've built applications of reasonable complexity\non both an RDBMS and an object store you'll know that complicated queries are\ncomplicated whether you're using SQL or not. I have a lot of love for SQL.\n\nIf putting everything in memory makes your database faster, why can't you\nbuild an RDBMS that stores everything in memory? You can, and they exist!\nVoltDB is one example. They're nice! Also, MySQL and Postgres have kind of\ncaught up to the idea that machines have lots more RAM now, so you can\nconfigure them to keep things mostly in memory too, so their default\nperformance is a lot better and their performance after being tuned by an\nexpert can be phenomenal.\n\nSo anything that's not a relational database is technically a \"NoSQL\"\ndatabase. Most NoSQL databases are object stores but that's really just kind\nof a historical accident.\n\n## How does my app talk to a database?\n\nNow we understand how a database works: it's software, running on a machine,\nmanaging data for you. How does your app talk to the database over a network\nand get answers to queries? Are all databases just a single machine?\n\nThe answer is: every DBMS, whether relational or object store, is a piece of\nsoftware that runs on machine(s) that hold the data. There's massive\nvariation: some run on 1 machine, some on clusters of 5-10, some run across\nthousands of separate machines all at once.\n\nThe DBMS software does the management of the data, in memory or on disk, and\nit presents an API that can be accessed locally, and also more importantly\nover the network. Sometimes this is a web API like you're used to, literally\nmaking GET and POST calls over HTTP to the database. For other databases,\nespecially the older ones, it's a custom protocol.\n\nEither way, you run a piece of software in your app, usually called a Client.\nThat client knows the protocol for talking to the database, whether it's HTTP\nor WhateverDBProtocol. You tell it where the database server is on the\nnetwork, it sends queries over and gets responses. Sometimes the queries are\nliterally strings of text, like \"SELECT * FROM Fruits\", sometimes they are\nJSON payloads describing records, and any number of other variations.\n\nAs a starting point, you can think of the client running on your machine\ntalking over the network to a database running on another machine. Sometimes\nyour app is on dozens of machines, and the database is a single IP address\nwith thousands of machines pretending to be one machine. But it works pretty\nmuch the same either way.\n\nThe way you tell your client \"where\" the DB is is your connection credentials,\noften expressed as a string like \"http://username:password@mydb.com:1234\" or\n\"mongodb://...\". But this is just a convenient shorthand. All your client\nreally needs to talk to a database is the DNS name (like mydb.com) or an IP\naddress (like 205.195.134.39), plus a port (1234). This tells the network\nwhich machine to send the query to, and what \"door\" to knock on when it gets\nthere.\n\nA little about ports: machines listen on specific ports for things, so if you\nsend something to port 80, the machine knows the query is for your web server,\nbut if you send it to port 1234, it knows the query is for your database. Who\npicks 1234 (In the case of Postgres, it's literally 5432)? There's no rhyme or\nreason to it. The developers pick a number that's easy to remember between 1\nand 65,535 (the highest port number available) and hope that no other popular\npiece of software is already using it.\n\nUsually you'll also have a username and password to connect to the database,\nbecause otherwise anybody who found your machine could connect to your\ndatabase and get all the data in it. Forgetting that this is true is a really\ncommon source of security breaches!\n\nThere are bad people on the internet who literally just try every single IP in\nthe world and send data to the default port for common databases and try to\nconnect without a username or password to see if they can. If it works, they\ntake all the data and then ransom it off. Yikes! Always make sure your\ndatabase has a password.\n\nOf course, sometimes you don't talk to your database over a network. Sometimes\nyour app and your database live on the same machine. This is common in desktop\nsoftware but very rare in web apps. If you've ever heard of a \"database\ndriver\", the \"driver\" is the equivalent of the \"client\", but for talking to a\nlocal database instead of over a network.\n\n## Replication and scaling\n\nRemember I said some databases run on just 1 machine, and some run on\nthousands of machines? That's known as replication. If you have more than one\ncopy of a piece of data, you have a \"replica\" of that data, hence the name.\n\nBack in the old days hardware was expensive so it was unusual to have replicas\nof your data running at the same time. It was expensive. Instead you'd back up\nyour data to tape or something, and if the database went down because the\nhardware wore out or something, then you'd buy new hardware and (hopefully)\nreinstall your DBMS and restore the data in a few hours.\n\nWeb apps radically changed people's demands of databases. Before web apps,\nmost databases weren't being continuously queried by the public, just a few\nexperts inside normal working hours, and they would wait patiently if the\ndatabase broke. With a web app you can't have minutes of downtime, far less\nhours, so replication went from being a rare feature of expensive databases to\npretty much table stakes for every database. The initial form of replication\nwas a \"hot spare\".\n\nIf you ran a hot spare, you'd have your main DBMS machine, which handled all\nqueries, and a replica DBMS machine that would copy every single change that\nhappened on the primary to itself. Primary was called m****r and the replica\ns***e because the latter did whatever the former told it to do, and at the\ntime nobody considered how horrifying that analogy was. These days we call\nthose things \"primary/secondary\" or \"primary/replica\" or for more complicated\narrangements things like \"root/branch/leaf\".\n\nSometimes, people would think having a hot spare meant they didn't need a\nbackup. This is a huge mistake! Remember, the replica copies every change in\nthe main database. So if you accidentally run a command that deletes all the\ndata in your primary database, it will automatically delete all the data in\nthe replica too. Replicas are not backups, as the bookmarking site Magnolia\nfamously learned.\n\nPeople soon realized having a whole replica machine sitting around doing\nnothing was a waste, so to be more efficient they changed where traffic went:\nall the writes would go to the primary, which would copy everything to the\nreplicas, and all the reads would go to the replicas. This was great for\nscale!\n\nInstead of having 1 machine worth of performance (and you could swap to the\nhot spare if it failed, and still have 1 machine of performance with no\ndowntime) suddenly you had X machines of performance, where X could be dozens\nor even hundreds. Very helpful!\n\nBut primary/secondary replication of this kind has two drawbacks. First, if a\nwrite has arrived at the primary database but not yet replicated to all the\nsecondary machines (which can take half a second if the machines are far apart\nor overloaded) then somebody reading from the replica can get an answer that's\nout of date. This is known as a \"consistency\" failure, and we'll talk about it\nmore later.\n\nThe second flaw with primary/second replication is if the primary fails,\nsuddenly you can no longer write to your database. To restore the ability to\ndo writes, you have to take one of the replicas and \"promote\" it to primary,\nand change all the other replicas to point at this new primary box. It's time-\nconsuming and notoriously error-prone.\n\nSo newer databases invented different ways of arranging the machines, formally\ncalled \"network topology\". If you think of the way machines connect to each\nother as a diagram, the topology is the shape of that diagram.\nPrimary/secondary looks like a star. Root/branch/leaf looks like a tree. But\nyou can have a ring structure, or a mesh structure, or lots of others. A mesh\nstructure is a lot of fun and very popular, so let's talk about more about\nthem.\n\n## Mesh replication databases\n\nIn a mesh structure, every machine is talking to every other machine and they\nall have some portion of the data. You can send a write to any machine and it\nwill either store it, or figure out what machine should store it and send it\nto that machine. Likewise, you can query any machine in the mesh, and it will\ngive you the answer if it has the data, or forward your request to a machine\nthat does. There's no \"primary\" machine to fail. Neat!\n\nBecause each machine can get away with storing only some of the data and not\nall of it, a mesh database can store much, much more data than a single\nmachine could store. If 1 machine could store X data, then N machines could\ntheoretically store N*X data. You can almost scale infinitely that way! It's\nvery cool.\n\nOf course, if each record only existed on one machine, then if that machine\nfailed you'd lose those records. So usually in a mesh network more than one\nmachine will have a copy of any individual record. That means you can lose\nmachines without losing data or experiencing downtime; there are other copies\nlying around. In some mesh databases can also add a new machine to the mesh\nand the others will notice it and \"rebalance\" data, increasing the capacity of\nthe database without any downtime. Super cool.\n\nSo a mesh topology is a lot more complicated but more resilient, and you can\nscale it without having to take the database down (usually). This is very\nnice, but can go horribly wrong if, for instance, there's a network error and\nsuddenly half the machines can't see the other half of the machines in the\nmesh. This is called a \"network partition\" and it's a super common failure in\nlarge networks. Usually a partition will last only a couple of seconds but\nthat's more than enough to fuck up a database. We'll talk about network\npartitions shortly.\n\nOne important question about a mesh DB is: how do you connect to it? Your\nclient needs to know an IP address to connect to a database. Does it need to\nknow the IP addresses of every machine in the mesh? And what happens when you\nadd and remove machines from the mesh? Sounds messy.\n\nDifferent Mesh DBs do it differently, but usually you get a load balancer,\nanother machine that accepts all the incoming connections and works out which\nmachine in the mesh should get the question and hands it off. Of course, this\nmeans the load balancer can fail, hosing your DB. So usually you'll do some\nkind of DNS/IP trickery where there are a handful of load balancers all\nresponding on the same domain name or IP address.\n\nThe end result is your client magically just needs to know only one name or\nIP, and that IP always responds because the load balancer always sends you to\na working machine.\n\n## CAP theory\n\nThis brings us neatly to a computer science term often used to talk about\ndatabases which is Consistency, Availability, and Partition tolerance, aka CAP\nor \"CAP theory\". The basic rule of CAP theory is: you can't have all 3 of\nConsistency, Availability and Partition Tolerance at the same time. Not\nbecause we're not smart enough to build a database that good, but because\ndoing so violates physics.\n\nConsistency means, formally: every query gets the correct, most up-to-date\nanswer (or an error response saying you can't have it).\n\nAvailability means: every query gets an answer (but it's not guaranteed to be\nthe correct one).\n\nPartition Tolerance means: if the network craps out, the database will\ncontinue to work.\n\nYou can already see how these conflict! If you're 100% Available it means by\ndefinition you'll never give an error response, so sometimes the data will be\nout of date, i.e. not Consistent. If your database is Partition Tolerant, on\nthe other hand, it keeps working even if machine A can't talk to machine B,\nand machine A might have a more recent write than B, so machine B will give\nstale (i.e. not Consistent) responses to keep working.\n\nSo let's think about how CAP theorem applies across the topologies we already\ntalked about.\n\nA single DB on a single machine is definitely Consistent (there's only one\ncopy of the data) and Partition Tolerant (there's no network inside of it to\ncrap out) but not Available because the machine itself can fail, e.g. the\nhardware could literally break or power could go out.\n\nA primary DB with several replicas is Available (if one replica fails you can\nask another) and Partition Tolerant (the replicas will respond even if they're\nnot receiving writes from the primary) but not Consistent (because as\nmentioned earlier, the replicas might not have every primary write yet).\n\nA mesh DB is extremely Available (all the nodes always answer) and Partition\nTolerant (just try to knock it over! It's delightfully robust!) but can be\nextremely inconsistent because two different machines on the mesh could get a\nwrite to the same record at the same time and fight about which one is\n\"correct\".\n\nThis is the big disadvantage to mesh DBs, which otherwise are wonderful.\nSometimes it's impossible to know which of two simultaneous writes is the\n\"winner\". There's no single authority, and Very Very Complicated Algorithms\nare deployed trying to prevent fights breaking out between machines in the\nmesh about this, with highly variable levels of success and gigantic levels of\npain when they inevitably fail. You can't get all three of CAP and Consistency\nis what mesh networks lose.\n\nIn all databases, CAP isn't a set of switches where you are or aren't\nConsistent, Available, or Partition Tolerant. It's more like a set of sliders.\nSliding up the Partition Tolerance generally slides down Consistency, sliding\ndown Availability will give you more Consistency, etc etc.. Every DBMS picks\nsome combination of CAP and picking the right database is often a matter of\nchoosing what CAP combination is appropriate for your application.\n\n## Other topologies\n\nSome other terms you frequently hear in the world of databases are\n\"partitions\" (which are different from the network partitions of CAP theorem)\nand \"shards\". These are both additional topologies available to somebody\ndesigning a database. Let's talk about shards first.\n\nImagine a primary with multiple replicas, but instead of each replica having\nall the data, each replica has a slice (or shard) of the data. You can slice\nthe data lots of ways. If the database was people, you could have 26 shards,\none with all names starting with A, one with all the names starting with B,\netc..\n\nSharding can be helpful if the data is too big to all fit on one disk at a\ntime. This is less of a problem than it used to be because virtual machines\nthese days can effectively have infinity-sized hard drives.\n\nThe disadvantage of sharding is it's less Available: if you lose a shard, you\nlose everybody who starts with that letter! (Of course, your shards can also\nhave replicas...) Plus your software needs to know where all the shards are\nand which one to ask a question. It's fiddly. Many of the problems of sharded\ndatabases are solved by using mesh topologies instead.\n\nPartitions are another way of splitting up a database, but instead of\nsplitting it across many machines, it splits the database across many files in\na single machine. This is an old pattern that was useful when you had really\npowerful hardware and really slow disks, because you could install multiple\ndisks into a single machine and put different partitions on each one, speeding\nup your achingly slow, disk-based database. These days there's not a lot of\nreason to use partitions of this kind.\n\n## Fin\n\nThat concludes this impromptu Databases 101 seminar! I hope you enjoyed\nlearning a little bit more about this fantastically fun and critically\nimportant genre of software.\n\n\u00a9 2001 - 2024 Laurie Voss.\n\nHome | About me | Archive\n\n", "frontpage": false}
