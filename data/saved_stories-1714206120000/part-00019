{"aid": "40175393", "title": "A nerdsnipe led to a fast implementation of Game of Life", "url": "https://binary-banter.github.io/game-of-life/", "domain": "binary-banter.github.io", "votes": 1, "user": "Frotag", "posted_at": "2024-04-26 23:18:52", "comments": 0, "source_title": null, "source_text": "Binary Banter\n\n# Game of Life: How a nerdsnipe led to a fast implementation of game of life\n\n2023-06-29\n\n:: tags: #game-of-life #gpgpu #rust #simd\n\nInspired by an Advent of Code problem, we spontaneously decided to explore\nConway's Game of Life and discussed various techniques to solve it\nefficiently. This blogpost provides a concise summary of our weeks-long\njourney, where we delved into finding faster solutions for the Game of Life.\n\n# Table of Contents\n\n  * Table of Contents\n  * A Nerdsnipe\n  * Trivial simulation\n  * Unleashing the Power of Parallelism Using a Packed Representation\n\n    * A Packed Representation Using Bits\n    * A Packed Representation Using Nibbles\n    * Back to a Packed Representation Using Bits\n  * Embracing the Spectacle of Parallelism Using SIMD and Multi-Threading\n  * Ascending to the Apex of Parallelism Using the GPU\n\n    * OpenCL\n\n      * Multi-Step Simulation\n      * Shared Memory\n      * Work-Per-Thread\n    * Cuda\n\n      * LOP3\n      * Warp Shuffle\n  * Results and Previous Work\n  * Future Work\n  * Thanks\n\n# A Nerdsnipe\n\nOn a sunny afternoon, for no particular reason, we decided to discuss an\nAdvent of Code problem from a couple of years ago. This problem was about a\ncellular automaton which was strongly inspired by Conway's Game of Life. Here\nis an epic video showing what it looks like.\n\nObviously we had to discuss how we would solve this problem in the fastest\nmanner possible. One thing let to another, and needless to say, this\ndiscussion went on to consume weeks of our lives as we tried to find faster\nand faster solutions.\n\nThis blogpost is a short summary of all the techniques we've tried to speed up\nGame of Life.\n\nThe source code for this project is available on GitHub.\n\n# Trivial simulation\n\nBefore we move on to explaining the techniques we developed, we will first\ngive a short recap of how Game of Life works. The game is played on a\nrectangular grid populated by cells that can be either alive or dead. The\nstate of the cells are simultaneously updated in steps using the following\nrules:\n\n  1. Any live cell with fewer than two live neighbours dies, as if caused by under-population.\n  2. Any live cell with two or three neighbours lives on to the next generation.\n  3. Any live cell with more than three live neighbours dies, as if caused by over-population.\n  4. Any dead cell with exactly three live neighbours becomes a live cell, as if by reproduction.\n\nThe simplest possible simulation of these rules works on a 2D array of\nbooleans, literally following the rules above. For each cell the program\nevaluates its current state and number of live neighbours, which it uses to\ncompute the next state to be stored in a secondary grid. The secondary grid is\nnecessary because otherwise some cells might be using neighbours from the next\nstep. Assuming booleans are stored as bytes, we store 1 cell per byte and need\n9 memory accesses to determine its next state (1 for the center and 8 for its\nneighbours).\n\nAt the end of the step, the two grids are swapped either inefficiently by\ncopying the entire secondary grid back to the primary grid, or more\nefficiently by swapping their pointers.\n\n# Unleashing the Power of Parallelism Using a Packed Representation\n\nThe naive solution clearly doesn't make optimal use of memory. We not only\nneed an entire byte to store something that essentially only needs 1 bit to\nindicate whether it is dead or alive, but we also need many memory accesses on\ntop of that for just a single cell.\n\n## A Packed Representation Using Bits\n\nOur first idea to improve upon this solution was to use a packed\nrepresentation, where each bit represents a cell. It would then be possible to\nretrieve cells by using a mask and shifts, making this solution slower than\nour original solution.\n\n## A Packed Representation Using Nibbles\n\nWe can achieve a packed representation that is faster than our original by\nharnessing the power of parallelism. To make our lives easier for now, we\nstore each cell in a nibble (4 bits). We store these nibbles in an array of\nu64s, which we will call columns. Cells can then be stored in a column like\nthis:\n\n  * 1001 1011 0000 0011 -> 0x1001_1011_0000_0011\n\nOur goal is to find a way to simulate these 16 cells in parallel. Note that to\ncount the neighbours of a cell, we could simply add their corresponding\nnibbles. The difficulty lies in shifting the neighbours around in such a way\nthat they align, so that we can add them.\n\nTo get the top and bottom neighbours of a cell, we can simply load the columns\nabove and below the cell and add them. The neighbours on the sides are more\ntricky, shifting the columns left and right almost does the trick, but leaves\nout one nibble on either side that needs to be loaded from the neighbouring\ncolumn. The following two tables show the cells 0 through 7 that we want to\nalign with the center node c, so we can add them. The second table highlights\nthe case where some of the cells might be in the column to the left or right.\n\nleft| middle| right  \n---|---|---  \nxxxx xxxx xxxx xxxx| xxxx x012 xxxx xxxx| xxxx xxxx xxxx xxxx  \nxxxx xxxx xxxx xxxx| xxxx x3c4 xxxx xxxx| xxxx xxxx xxxx xxxx  \nxxxx xxxx xxxx xxxx| xxxx x567 xxxx xxxx| xxxx xxxx xxxx xxxx  \nleft| middle| right  \n---|---|---  \nxxxx xxxx xxxx xxx0| 12xx xxxx xxxx xxxx| xxxx xxxx xxxx xxxx  \nxxxx xxxx xxxx xxx3| c4xx xxxx xxxx xxxx| xxxx xxxx xxxx xxxx  \nxxxx xxxx xxxx xxx5| 67xx xxxx xxxx xxxx| xxxx xxxx xxxx xxxx  \n  \nIn pseudocode, we can find the count of each cell as follows:\n\n    \n    \n    top = grid[i - ROW] bottom = grid[i + ROW] // Note that we shift with 60 = 15 * 4 to align the columns to the left and right. left = (grid[i] >> 4) | (grid[i - 1] << 60) right = (grid[i] >> 4) | (grid[i - 1] << 60) top_left = (grid[i - ROW] >> 4) | (grid[i - ROW - 1] << 60) top_right = (grid[i - ROW] << 4) | (grid[i - ROW + 1] >> 60) bottom_left = (grid[i + ROW] >> 4) | (grid[i + ROW - 1] << 60) bottom_right = (grid[i + ROW] << 4) | (grid[i + ROW + 1] >> 60)\n\nAfter aligning the cells we can now simply add them together to find the\nnumber of neighbours for each cell. The following table show a grid of cells\nwhere 1 means alive and 0 dead. The bottom rows shows the number of neighbours\nfor each cell for the central column.\n\nleft| middle| right  \n---|---|---  \nxxxx xxxx xxxx xxx0| 1001 1011 1000 0011| 1xxx xxxx xxxx xxxx  \nxxxx xxxx xxxx xxx1| 1111 1000 0010 0110| 1xxx xxxx xxxx xxxx  \nxxxx xxxx xxxx xxx0| 1001 1000 1100 0011| 1xxx xxxx xxxx xxxx  \n\\-------------------| \\-------------------| \\-------------------  \n4446 5424 4411 1358  \n  \nThe final challenge is to use the current state and these neighbour counts to\ncompute the next state of the cells in parallel. The exact instructions may\nseem a bit weird, but they produce the same results as the rules laid out\nearlier.\n\n    \n    \n    // start with the previous state result = grid[i] // if we have 1,3,5,7 live neighbours, we are born result |= count // if we have 2,3,6,7 live neighbours, we survive result &= count >> 1 // if we have 0,1,2,3,8 live neighbours, we survive result &= !(count >> 2) // if we have 0-7 live neighbours, we survive result &= !(count >> 3) // clear all bits that don't represent a cell result &= 0x1111_1111_1111_1111\n\n## Back to a Packed Representation Using Bits\n\nSo far we have stored a cell in each nibble, but we can actually implement a\nparallel simulator that stores a cell in each bit. This makes the logic a lot\nmore complex to understand, but it does increase throughput.\n\nThe addition of the 8 neighbouring cells can be written as:\n\n    \n    \n    a b c d e f g h ---- + qrst\n\nWhere t is the least-significant bit of count, s the least-significant bit of\ncount >> 1, etc. We need to find a way to produce q, r, s, and t using bit-\nlevel instructions (AND, OR, NOT, XOR) instead of using the ADD instruction,\nso that we can simulate all 64 cells in parallel.\n\nLuckily we recently took a course on Computer Arithmetic, which taught us\nexactly how to do this! What we need to build is called an 8-bit counter as\nexplained in Parhami's Computer Arithmetic book. Technically this method only\nreally works well for hardware, but for this problem it actually works just as\nwell in software!\n\nWe designed the following counter, where square brackets indicate full- and\nhalf-adders which are to be emulated in software. The important thing to take\naway is that we reduce the 8 neighbours down to count = 4x + 2(y+z) + w.\n\n    \n    \n    [a| |b| |c] [d| |e| |f] [g| |h] --------- + [l|[i| |m||j| |n]|k] --------- + x y w z\n\nTo implement the above table in software, we first implement full- and half-\nadders in software:\n\n    \n    \n    fn half_adder(a,b): sum = a ^ b carry = a & b return carry, sum fn full_adder(a,b,c): temp = a ^ b sum = temp ^ c carry = (a & b) | (temp & c) return carry, sum\n\nNext, we use the adders to implement the table above:\n\n    \n    \n    // stage 0 l, i = full_adder(a,b,c) m, j = full_adder(d,e,f) n, k = half_adder(g,h) // stage 1 y, w = full_adder(i,j,k) x, z = full_adder(l,m,n)\n\nFinally, we use the results of the addition to update result:\n\n    \n    \n    // start with the previous state result = grid[i] // if we have 1,3,5,7 live neighbours, we are born result |= w; // if we have 2,3,6,7 live neighbours, we survive center &= (y ^ z); // if we have 0,1,2,3 live neighbours, we survive center &= ~x;\n\nSo we can now use this to simulate a fully packed u64 of cells in parallel.\n\n# Embracing the Spectacle of Parallelism Using SIMD and Multi-Threading\n\nThe story doesn't end there however! We have merely used parallelism on the\nscale of u64s, but most modern CPUs actually provide specialized instructions\nthat work on registers of multiple u64s. These instructions are commonly\nreferred to as Single-Instruction Multiple-Data, or SIMD for short.\n\nWe use Rust's new nightly std::simd API to achieve this. The API provides easy\nto use const-generic methods that abstract away the underlying intrinsics.\n\nFor the most part the translation from u64s as columns to N u64s as columns,\nwhere N is the number of SIMD lanes, was quite straight forward. The only real\nhurdle we came across is the lack of register-wide bit-shifts. Instead of\nshifting the entire register, the lanes themselves are shifted.\n\nTo remedy this problem we implemented helper functions. Below is our\nshift_left function.\n\n    \n    \n    pub fn shl_1<const N: usize>(v: Simd<u64, N>) -> Simd<u64, N> where LaneCount<N>: SupportedLaneCount, { let approx = (v << Simd::splat(1)); let mut mask = [0x00000_0000_0000_0001; N]; mask[N - 1] = 0; let rotate = (v >> Simd::splat(63)).rotate_lanes_left::<1>(); let neighbouring_bits = rotate & Simd::from_array(mask); approx | neighbouring_bits }\n\nThe function works as follows:\n\n  * First we approximate the shift, by shifting each lane left by 1. This works for all bits except the bits that should be shifted across lane boundaries.\n  * To get the remaining bits, we rotate the lanes left by one, then shift each lane right by 63.\n  * We mask the result of this so only the relevant bits are selected. Specifically, the lane that was rotated around the register should be ignored.\n  * Finally, we OR these results together.\n\nWith the hardware available to us, we can now simulate 256 cells in parallel!\nHowever, this is not where the parallelism stops, since CPUs have multiple\ncores, we can take advantage of multi-threading. During each step, we can\nsplit the rows of the field into N equal-sized groups, each of which is\nsimulated by a separate thread. We use a thread pool so the overhead of\nspawning the threads is amortized over the steps.\n\nWhile initially we thought that the Rust ownership model would hinder us when\nwriting the results to the same array from multiple threads, Rust has a\nbeautiful abstraction called chunks_mut that does exactly what we want. This\nsplits a slice into N equal-sized groups. The rest of this change was trivial,\nrequiring only a few changes in the way we index the arrays.\n\nNow we are fully using our CPU! But this is not the end of our journey...\n\n# Ascending to the Apex of Parallelism Using the GPU\n\nWe have now seen how we can efficiently pack cells into different\ndatastructures and quickly calculate the next state for each cell. At the end\nof the last chapter we made our solution multi-threaded. Modern CPUs have many\ncores that can perform work at the same time, but GPUs actually have thousands\nof cores that can perform simple tasks at the same time. GPUs however have a\nvastly different model of computation and memory, so running our solution on\nthe GPU is not trivial. Yet we were still determined to take a look into\nGeneral Purpose GPU (GPGPU).\n\nThere are two major frameworks for doing GPGPU: OpenCL and CUDA. Since CUDA is\nonly available for NVidia GPUs, we decided to first focus on porting our\nsolution to OpenCL.\n\n## OpenCL\n\nWe decided to use the opencl3 crate for our OpenCL implementation, since it\nprovides a simple interface for working with OpenCL. OpenCL makes a\ndistinction between the host (CPU) and device (usually GPU).\n\nThere are some key differences when comparing writing code for a CPU versus\nwriting OpenCL code. In particular, the memory of the device is usually\nseparate from that of the host, which means explicit calls need to be made to\nmove data back and forth.\n\nThe device has many work-items (equivalent to threads on the CPU), each of\nwhich can execute code. These are grouped into work-groups, allowing the work-\nitems inside the group to communicate. One of the ways work-items can\ncommunicate is via shared memory, which is memory that is only accessible by\nwork-items in the same work-group. This is in contrast to global memory which\nis slower but accessible by all work-items.\n\nFurthermore, kernels (i.e. programs on the device) need to be compiled and\nloaded into the device which are then all ran in parallel. During compilation\nthe work-group size and global work size (the total number of work-items)\nneeds to be specified.\n\nOur first implementation was quite straightforward - a kernel that simulates a\ncolumn for a single step with the same logic used on the CPU. During the step\nthe kernel will load its neighbouring columns from global memory. This kernel\ntakes two buffers of 32-bit unsigned integers field and new_field, which are\nswapped after the step has been computed for all columns. By repeatedly\nlaunching this kernel with the global work size equal to the dimensions of the\ngrid any number of steps can be simulated.\n\nThis basic implementation was already quite fast, but this was definitely not\nthe fastest it could go. We started experimenting using different techniques\nand benchmarked them using criterion in order to determine what works and\ndoesn't work. Benchmarking is important because the performance effects of\nchanges to the code may be very unpredictable, especially when working with\nGPUs.\n\nIn the following subsections we will discuss the techniques that worked:\n\n  * Multi-step simulation\n  * Use shared memory (also called local memory)\n  * Increase work-per-thread (WPT)\n\n### Multi-Step Simulation\n\nOne very effective technique to speed up computation involves simulating more\nthan one step per kernel call. This comes with the advantage of having\ndecreased kernel launching overhead and a reduced number of accesses to global\nmemory.\n\nBut how does multi-step simulation work? The core idea is to think about dirty\ncells, which are cells that may be incorrect after a given number of steps. If\nan area of n x m is simulated for t steps without using any information\noutside of that area, then only the inner area (n-t) x (m-t) will be correct,\nwhereas the cells outside this area will be dirty. The following example shows\nhow correctly simulated cells O turn into dirty cells X.\n\n    \n    \n    OOOOO -> XXXXX -> XXXXX OOOOO -> XOOOX -> XXXXX OOOOO -> XOOOX -> XXOXX OOOOO -> XOOOX -> XXXXX OOOOO -> XXXXX -> XXXXX\n\nThis example highlights that a tradeoff must be made when choosing the number\nof steps to simulate at once. If the number of steps is too large, then we\nwill be left with mostly dirty cells which will need to be computed by\noverlapping blocks. However, if the number of steps is too low then we might\nnot reap the benefits of multi-step simulation.\n\nThe first setup we tried is loading 3 columns, with the goal of simulating\nonly the middle one for 32 steps. This also requires a vertical padding of 32\ncells. We can calculate what our percentage of effective computation is. This\nis the percentage of cells that we simulate that actually end up being written\nback to global memory. Horizontally, a third of the cells are written to\nmemory, the two padding columns on the side are only used because we need to\nknow the value of them to simulate the 32 steps. Vertically, assuming a\nworkgroup size of 512, 2 * 32 / 512 = 8.75% of the computation is wasted.\nCombining these figures, (512 - 2 * 32) / 512 * (1/3) = 29.2% of the cells we\nsimulate are written back to global memory. That percentage is not that high,\nand we can do better.\n\nThe setup that we settled on is loading 3 columns, but simulating the middle\none for only 16 steps. This is achieved by shifting the columns a bit, so we\nare simulating only half of the outer 2 columns.\n\n    \n    \n    Loaded: XXXX_XXXX YYYY_YYYY ZZZZ_ZZZZ Left: XXXX_YYYY Right: YYYY_ZZZZ\n\nNow 50% of the simulation horizontally is effective, and again, this requires\n16 cells of padding vertically as well. Using the same logic as above, now\n46.9% of the cells we simulate are written back to global memory. This is the\nbest solution we found given the tradeoff discussed earlier. It has a good\nbalance between the number of steps and the amount of wasted computation.\n\n### Shared Memory\n\nNext, we noticed that work-items in the same work-group are communicating\nthrough global memory between each step. This is unnecessary, since we can\ninstead use shared memory, which is a lot faster with the restriction of being\nonly accessible by work-items in the same work-group.\n\nThe cells now only need to be read to shared memory at the start of each\nkernel call, and written back to global memory at the end of each kernel call.\nThis saves a lot of time that was previously wasted waiting for global memory.\n\n### Work-Per-Thread\n\nThe overhead of starting a new work-item is small but still significant, so we\nwant to minimize its impact. We do this by increasing the amount of work that\na single work-item performs.\n\nThe idea is that each work-item loads and simulates N vertically stacked\ncolumns instead of 1. An additional advantage of this approach is that only\nthe outermost columns needs to be communicated through shared memory, saving\nshared memory bandwidth.\n\nWe found that an N of approximately 16 is optimal depending on the GPU that is\nused, giving significant performance improvements.\n\n## Cuda\n\nOpenCL is a beautiful abstraction that allows you to write portable code that\nruns on a CPU, Amd GPU, Nvidia GPU, FPGA and more. This is an amazing feat\ngiven how different these architectures are, but this does mean that some of\nthe details are abstracted away. Details that, if used properly, could allow\nfor more optimizations!\n\n### LOP3\n\nOne of these details is knowledge about the CUDA PTX instruction set and\ncontrol over the instructions that are generated. We noticed while taking a\nlook at our code using Godbolt, that the compiler sometimes used an\ninstruction called lop3 (lookup-3). What we discovered is that this is an\namazing instruction! It is an instruction that takes 3 inputs a, b and c, an\noutput d and a look-up-table LUT which it uses to compute arbitrary bitwise\nlogical operations. The following table shows how the LUT = 0b1001_0110\nencodes the ternary bitwise XOR operator for inputs a, b and c.\n\na| b| c| d  \n---|---|---|---  \n0| 0| 0| 0  \n0| 0| 1| 1  \n0| 1| 0| 1  \n0| 1| 1| 0  \n1| 0| 0| 1  \n1| 0| 1| 0  \n1| 1| 0| 0  \n1| 1| 1| 1  \n  \nOur step operation is a function that takes 9 integers as input (the 8\nneighbours and the cell itself), and produces a new integer by looking at each\n9-tuple of bits. This means that if we can build our 9-input step function\nusing 3-input lop3 instructions, it could potentially be a lot faster.\n\nUsing our human ingenuity we managed to find a set of 11 lop3 instructions\nthat does exactly that:\n\n    \n    \n    a8 = lop3.b32 a2 a1 a0 0b10010110 b0 = lop3.b32 a2 a1 a0 0b11101000 a9 = lop3.b32 a5 a4 a3 0b10010110 b1 = lop3.b32 a5 a4 a3 0b11101000 aA = lop3.b32 a8 a7 a6 0b10010110 b2 = lop3.b32 a8 a7 a6 0b11101000 b3 = lop3.b32 b2 b1 b0 0b10010110 c0 = lop3.b32 b2 b1 b0 0b11101000 parity = lop3.b32 center aA a9 0b11110110 two_three = lop3.b32 b3 aA a9 0b01111000 center = lop3.b32 parity two_three c0 0b01000000\n\nThe code works as follows:\n\n  * The first 8 instructions are equivalent to the full-adder stages in the CPU code.\n\n    * 0b10010110 is a 3-bit XOR, producing the sum bit of the full-adder.\n    * 0b11101000 is an at-least-two operation, producing the carry bit of the full-adder.\n  * These instructions have reduced the information down to the bits c0, b3, a9, and aA, which represent count = 4*c0 + 2*b3 + (a9 + aA).\n  * parity has the following meaning:\n\n    * If center is on, stay on\n    * If the number of live neighbours is odd (determined from a9 and aA), turn on. This will make sure the cell turns on if it has 3 live neighbours. Note that this will also erroneously turn the cell on if 1, 5 or 7 neighbours are on, but in those cases the cell will be killed by the final instruction.\n  * two_three is on if the total value of b2, a9, and aA is 2 or 3. Remember that b2 has a weight of 2.\n  * Finally, the cell is live if:\n\n    * parity is on, meaning the cell is alive or the number of live neighbours is odd.\n    * two_three is on, meaning the cell has two or three live neighbours.\n    * c0 is off, meaning the cell does not have more than 4 live neighbours.\n\nThis means that we can do a single step using only 11 lop3 instructions, and\nthe instructions necessary to construct the a0-a7 neighbours, which are still\nthe same as before. However, we are not done yet! This is the solution that we\nas humans could come up with, but we gave the problem to a SAT-solver which\nfound a solution with 10 lop3 instructions. The first 6 instructions are the\nsame as ours, but the last 4 are complete magic to us, if you can understand\nwhat is happening here we're happy to hear it, but for now we've just accepted\nthat a SAT-solver has beaten us.\n\n    \n    \n    // reduction stage a8 = lop3.b32 a2 a1 a0 0b10010110 b0 = lop3.b32 a2 a1 a0 0b11101000 a9 = lop3.b32 a5 a4 a3 0b10010110 b1 = lop3.b32 a5 a4 a3 0b11101000 aA = lop3.b32 a8 a7 a6 0b10010110 b2 = lop3.b32 a8 a7 a6 0b11101000 // magic stage dreamt up by an insane SAT-solver magic0 = lop3.b32 a9 aA center 0b00111110 magic1 = lop3.b32 magic0 center b2 0b01011011 magic2 = lop3.b32 magic1 b1 b0 0b10010001 center = lop3.b32 magic2 magic0 magic1 0b01011000\n\nA blog post about how we managed to translate this problem (and others) to a\nform that a SAT-solver can work with is coming soon.\n\n### Warp Shuffle\n\nThe next observation we made is that the way we use shared memory is very\nregular, work items write to shared memory after each step, and at the start\nof the next step work items read the results of only their neighbouring work\nitems. This is something that can be efficiently encoded using a warp shuffle\ninstead. A warp is a set of upto 32 work items, which can send information to\neach other using a warp shuffle.\n\nAt the start of each step, each work item sends the state of the cells it is\nsimulating to each neighbour, while simultaneously receiving the state of the\nneighbouring cells. This completely eliminates the need for shared memory,\nallowing us to make use of the very fast register memory. One drawback of this\napproach is that we now have a maximum workgroup size of 32, while earlier we\nwere able to run with a workgroup size of 512. This is not as drastic as it\nmay seem, since each workgroup is split by the GPU into warps anyway, there is\nonly some extra overhead in creating the extra workgroups. Furthermore, due to\nthe earlier technique of increasing the work-per-thread, the percentage of\neffective computation remains approximately the same.\n\n    \n    \n    left[0] = __shfl_up_sync(-1, left[WORK_PER_THREAD], 1); right[0] = __shfl_up_sync(-1, right[WORK_PER_THREAD], 1); left[WORK_PER_THREAD + 1] = __shfl_down_sync(-1, left[1], 1); right[WORK_PER_THREAD + 1] = __shfl_down_sync(-1, right[1], 1);\n\n# Results and Previous Work\n\nSo how fast did we actually make it go? And was it significant? Thanks to our\nawesome university, we were allowed to use the high performance cluster (HPC)\nto experiment with several GPUs. Additionally, for completeness, we also\nincluded some CPU benchmarks. We measure performance in cell updates per\nsecond (CUpS).\n\nImplementation| Hardware| Performance (CUpS)  \n---|---|---  \nTrivial| i7 5820K| 0.219\u00d710^9  \ni7 6700K| 0.282\u00d710^9  \ni9 11900K| 0.378x10^9  \nPacked Nibbles| i7 5820K| 4.393\u00d710^9  \ni7 6700K| 5.065\u00d710^9  \nPacked Bits| i7 5820K| 6.539\u00d710^9  \ni7 6700K| 7.033\u00d710^9  \nSIMD| i7 5820K| 22.600x10^9  \ni7 6700K| 27.875x10^9  \ni9 11900K| 48.266x10^9  \nSIMD Multi-threaded| i7 5820K| 45.813x10^9  \ni7 6700K| 71.105x10^9  \ni9 11900K| 296.019x10^9  \nOpenCL| 1050 Notebook| 0.760\u00d710^12  \n1080Ti| 4.775\u00d710^12  \nV100| 6.678\u00d710^12  \n2080Ti| 6.783\u00d710^12  \nA40| 8.055\u00d710^12  \n7900XTX| 8.939\u00d710^12  \nCUDA| 1050 Notebook| 1.068\u00d710^12  \n1080Ti| 5.389\u00d710^12  \nV100| 9.871\u00d710^12  \n2080Ti| 9.666\u00d710^12  \nA40| 11.720x10^12  \n  \nWe compare our work with previous work from the Bitwise Parallel Bulk\nComputation paper. This paper performs some of the same optimizations that we\ndo:\n\n  * A packed representation using bits is used, similar to ours\n  * Bitwise operations are used to compute the new cells using the state from the neighbours. This uses 59 bitwise instructions, which is a lot more than our 10 lop3 instructions.\n  * Multi-step simulation is used, but a square layout is used for the multistep, while we use a line. The square has a higher percentage of effective computation, but this is still slower because more communication with neighbouring work items is needed.\n  * Warp shuffles are used as well, but because of the square layout mentioned above, more shuffles are required than in our implementation.\n\nThe final performance from the paper is:\n\nImplementation| Hardware| Performance (CUpS)  \n---|---|---  \nCPU| \"i7\"| 13.4\u00d710^9  \nGPGPU| Titan X| 1.990\u00d710^12  \n  \nWe could sadly not get our hands on a Titan X for a fair comparison, but out\nof the GPUs above the Titan X is most closely resembled by the 1080Ti. The\n1080Ti had a speed of 5.389\u00d710^12 CUpS, which means our solution shows a 2.7x\nimprovement in performance.\n\n# Future Work\n\nWe have left out two optimization techniques in our final implementation.\n\nFirstly, we did not implement the large universes method described in the BPBC\npaper. Currently, the entire field is stored in GPU VRAM. However, if the\nfield does not fit inside VRAM, we need to load and simulate overlapping parts\n(universes) of it for a certain number of steps. This is similar to the way\nmultistep simulation works, but on a way larger scale.\n\nSecondly, we did not implement sparse simulation. A sparse simulation is a\nsimulation which can detect that certain areas of the field are dead, meaning\nthese areas have reached a stable state. These areas can then be safely\nskipped during simulation. Sparse simulation can have an incredibly large\nimpact on simulation speed, but can also be incredibly hard to benchmark.\n\nBoth of these techniques could be added to our implementation, but we did not\nfind the time yet to do so.\n\n# Thanks\n\nThis post was written by Julia Dijkstra and Jonathan Brouwer.\n\nSpecial thanks to Mathijs Molenaar for providing some ideas for further\noptimizations.\n\n\u00a9 2023 Powered by Zola :: Theme: Terminimal by pawroman\n\n", "frontpage": false}
