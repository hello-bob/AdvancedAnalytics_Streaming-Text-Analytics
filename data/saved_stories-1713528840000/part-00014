{"aid": "40082491", "title": "Why and How to Rebuild Tables Online in PostgreSQL vs. Oracle", "url": "https://hexacluster.ai/postgresql/rebuilding-tables-online-using-pg_repack-in-postgresql/", "domain": "hexacluster.ai", "votes": 1, "user": "avivallssa", "posted_at": "2024-04-19 01:16:10", "comments": 0, "source_title": "Rebuilding Tables online using pg_repack in PostgreSQL - HexaCluster", "source_text": "Rebuilding Tables online using pg_repack in PostgreSQL - HexaCluster\n\n# Rebuilding Tables online using pg_repack in PostgreSQL\n\n  * April 15, 2024\n\n/\n\n  * Oracle to PostgreSQL, PostgreSQL, PostgreSQL Extensions, SQL Server to PostgreSQL\n\nAny database engine, including PostgreSQL, requires some routine maintenance\nto ensure optimal performance. Especially when there are massive delete or\npurge operations to delete rows from a Table, the fragmentation builds up\nwithin a table. Reclaim this fragmented space from disk requires a table to be\nrebuilt. At HexaCluster, we had many Customers asking us if there are similar\ncapabilities in PostgreSQL to rebuild tables online while migrating from\nOracle to PostgreSQL. In this article, we shall discuss about the extension\ncalled : pg_repack to rebuild tables online in PostgreSQL and see how it can\nbe compared with DBMS_REDEFINITION in Oracle, for rebuilding tables online.\nOracle Database 12c Release 2 offers much more efficient options for\nrebuilding tables and partitions online using its Online Table Move feature.\n\n### When and Why should we rebuild tables in PostgreSQL ?\n\nYou might have come across the term called VACUUM if you have deployed\nPostgreSQL as the database engine for your applications. VACUUM is one of the\nkey maintenance operations in PostgreSQL, responsible for cleaning up dead\ntuples from the table pages, thereby freeing up space. This space can either\nbe re-used for future insertions and updates or remain unused and lead to\nfragmentation. With massive data purge operations involving deletion of a\nsignificant percentage of data from a table, the amount of fragmentation may\nbe significantly larger. Eventually, these fragmented pages increase the size\nof the table and utilize more disk space than required. Such fragmentation\nmight distribute tuples across an unnecessarily large number of pages,\ncreating performance bottlenecks. To mitigate these issues, table rebuilds are\nsometimes necessary. Contact HexaCluster if you would like to get expert\nassistance here.\n\n### Why not use VACUUM FULL to rebuild tables in PostgreSQL ?\n\nVACUUM FULL is always the safest and the standard method for rebuilding tables\nin PostgreSQL. As an example, to rebuild a table called :\nmigrate_from_oracle_to_postgres using vacuum full, we can use the following\ncommand.\n\n    \n    \n    psql -U postgres -d hexacluster -p 5432 -c \"VACUUM FULL migrate_from_oracle_to_postgres\"\n\nSometimes, you may notice that it is faster to rebuild a table and its indexes\nin Postgres using \"VACUUM FULL\" compared to other extensions. However, this\ncommand exclusively locks a table from READ and WRITE operations during the\nrebuild and thus not always considered suitable by database engineers in\nproduction environments.\n\nBefore discussing about an interesting extension in PostgreSQL to rebuild\ntables online, let us see how the same can be performed in Oracle.\n\n### Does Oracle have a Package to rebuild tables online ?\n\nYes, Enterprise editions (versions) of Oracle supports a package called :\nDBMS_REDEFINITION that can help rebuilding tables online. Through this\npackage, Oracle also supports adding or dropping columns online and changes to\nthe partition strategies of a table.\n\nPlease Note: Oracle Database 12c Release 2 offers more simpler and efficient\noptions to move tables and their partitions online.\n\nHowever, there are a variety of manual steps an Oracle database engineer has\nto monitor and execute sequentially, to achieve the supported features. As an\nexample, to rebuild a table called : MIGRATE_TO_POSTGRES in Schema :\nHEXACLUSTER online using DBMS_REDEFINITION in Oracle, following steps are\ninvolved.\n\nPlease Note : There are 6 manual steps involved in successfully rebuilding a\nTable online in Oracle.\n\n  * Verify if the table can be rebuilt online\n    \n        BEGIN DBMS_REDEFINITION.CAN_REDEF_TABLE('HEXACLUSTER', 'MIGRATE_TO_POSTGRESQL'); END;\n\n  * Create an Interim Table with the same structure as the existing Oracle table.\n    \n        CREATE TABLE HEXACLUSTER.NEW_MIGRATE_TO_POSTGRESQL ( -- Define columns and data types as needed );\n\n  * Start the Redefinition process.\n    \n        BEGIN DBMS_REDEFINITION.START_REDEF_TABLE( uname => 'HEXACLUSTER', orig_table => 'MIGRATE_TO_POSTGRESQL', int_table => 'NEW_MIGRATE_TO_POSTGRESQL' ); END;\n\n  * Copy the dependents such as Triggers, Constraints, Indexes and Grants from the original tables to the interim table.\n    \n        BEGIN DBMS_REDEFINITION.COPY_TABLE_DEPENDENTS( uname => 'HEXACLUSTER', orig_table => 'MIGRATE_TO_POSTGRESQL', int_table => 'NEW_MIGRATE_TO_POSTGRESQL', copy_indexes => 1, copy_triggers => TRUE, copy_constraints => TRUE, copy_privileges => TRUE ); END;\n\n  * Synchronize the delta changes from the original table to the interim table. As the original table may be receiving changes due to live traffic, it is important to apply the changes to the interim table and keep it consistent.\n    \n        BEGIN DBMS_REDEFINITION.SYNC_INTERIM_TABLE( uname => 'HEXACLUSTER', orig_table => 'MIGRATE_TO_POSTGRESQL', int_table => 'NEW_MIGRATE_TO_POSTGRESQL' ); END;\n\n  * Once the above step is completed, we can complete the redefinition process by executing the following final step that acquires a lock, synchronizes the changes from original table to interim table and then swaps the tables.\n    \n        BEGIN DBMS_REDEFINITION.FINISH_REDEF_TABLE( uname => 'HEXACLUSTER', orig_table => 'MIGRATE_TO_POSTGRESQL', int_table => 'NEW_MIGRATE_TO_POSTGRESQL' ); END;\n\n  * Drop the interim table which is indeed the fragmented table, renamed due to swap performed in the previous step.\n\n### What about PostgreSQL ? Is there any Postgres extension to rebuild tables\nonline ?\n\nYes, there exists an extension in PostgreSQL called : pg_repack, that acts\nlike a smarter VACUUM FULL. It can be used to rebuild a table online and\nefficiently remove bloat and fragmentation while allowing concurrent\nread/write operations on that table.\n\n### Is rebuilding a table simple in PostgreSQL compared to Oracle ?\n\nDefinitely yes. It is simple to rebuild a table online in PostgreSQL compared\nto Oracle. This is because, all the 6 manual steps involved in\nDBMS_REDEFINITION for Oracle can be replaced with a single step in PostgreSQL\nusing pg_repack.\n\nAs an example, the following steps can be simply used to rebuidl a table\nonline in PostgreSQL using pg_repack.\n\n    \n    \n    -- To perform a dry-run and see whether we can rebuild table online postgres@hexacluster:~$ pg_repack --table users -d hexacluster --dry-run -- To execute and rebuild table online postgres@hexacluster:~$ pg_repack --table users -d hexacluster INFO: repacking table \"public.users\"\n\n### Steps involved in setting up pg_repack in PostgreSQL\n\nIn the following sections, we shall see the steps involved in setting up this\nextension and using it for rebuilding tables online.\n\n#### Pre-requisites\n\n  * To ensure efficient performance, the target table should possess either a primary key or at least a unique index covering a non-null column.\n  * The pg_repack binary and the extension version installed in the database should match.\n  * It only supports PostgreSQL 9.5 and above.\n  * Executing a complete table repack necessitates available disk space roughly twice the size of the target table(s) and their associated indexes.\n  * pg_repack will not work on postgresql catalog tables.\n  * Only table owners and superusers can use pg_repack to rebuild the table.\n\n#### Installing pg_repack extension\n\nInstalling pg_repack can be either done by using the package manager and\nPostgreSQL repository or we can do a source code installation.\n\n##### Installing on RedHat/Rocky/AlmaLinux/CentOS/Fedora/Oracle Linux\n\nOnce you set the PGDG repository for Centos/Redhat you can use the below\ncommand.\n\n    \n    \n    yum install pg_repack_<PG_MAJOR_VERSION>.x86_64 Eg: yum install pg_repack_16.x86_64\n\n##### Installing on Debian/Ubuntu systems\n\nSetup the latest PGDG repository in the ubuntu machine where you need to\ninstall pg_repack\n\n    \n    \n    # Create the file repository configuration: sudo sh -c 'echo \"deb https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list' # Import the repository signing key: wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - # Update the package lists: sudo apt-get update #install the software. Considering the target database as PostgreSQL 16, the command appears as following. sudo apt-get install postgresql-16-repack\n\n#### Creating the Extension in the database\n\nOnce the software is installed in the database server, we need to create the\nextension in the database in which we are going to rebuild the tables.\n\n    \n    \n    psql -U postgres -d hexacluster -p 5432 -c \"CREATE EXTENSION pg_repack\"\n\n#### Rebuilding tables online with pg_repack\n\nTo rebuild a table with pg_repack we use the below command. The pg_repack\nutility includes a feature that allows it to assess whether a table is\nsuitable for repacking through a dry run option.\n\n###### Dryrun\n\n    \n    \n    postgres@hexacluster:~$ pg_repack --table users -d hexacluster --dry-run INFO: Dry run enabled, not executing repack INFO: repacking table \"public.users\"\n\n###### Actual Execution\n\n    \n    \n    postgres@hexacluster:~$ pg_repack --table users -d hexacluster INFO: repacking table \"public.users\" postgres@hexacluster:~$\n\n###### Actual Execution with Detailed logging:\n\nExecuting pg_repack with detailed logging can be done by passing \u2013echo or -e\nto the command; this will show us the detailed steps that pg_repack does when\ntriggered on a table.\n\n    \n    \n    postgres@hexacluster:~$ pg_repack -e --table users -d hexacluster LOG: (query) SET search_path TO pg_catalog, pg_temp, public LOG: (query) SET search_path TO pg_catalog, pg_temp, public LOG: (query) select repack.version(), repack.version_sql() LOG: (query) SET statement_timeout = 0 LOG: (query) SET search_path = pg_catalog, pg_temp, public LOG: (query) SET client_min_messages = warning LOG: (query) SELECT r FROM (VALUES ($1, 'r')) AS given_t(r,kind) WHERE NOT EXISTS( SELECT FROM repack.tables WHERE relid=to_regclass(given_t.r)) AND NOT EXISTS( SELECT FROM pg_catalog.pg_class c WHERE c.oid=to_regclass(given_t.r) AND c.relkind = given_t.kind AND given_t.kind = 'p') LOG: (param:0) = users LOG: (query) SELECT t.*, coalesce(v.tablespace, t.tablespace_orig) as tablespace_dest FROM repack.tables t, (VALUES (quote_ident($1::text))) as v (tablespace) WHERE (relid = $2::regclass) ORDER BY t.relname, t.schemaname LOG: (param:0) = (null) LOG: (param:1) = users INFO: repacking table \"public.users\" LOG: (query) SELECT pg_try_advisory_lock($1, CAST(-2147483648 + $2::bigint AS integer)) LOG: (param:0) = 16185446 LOG: (param:1) = 16452 LOG: (query) BEGIN ISOLATION LEVEL READ COMMITTED LOG: (query) SET LOCAL lock_timeout = 100 LOG: (query) LOCK TABLE public.users IN ACCESS EXCLUSIVE MODE LOG: (query) RESET lock_timeout LOG: (query) SELECT pg_get_indexdef(indexrelid) FROM pg_index WHERE indrelid = $1 AND NOT indisvalid LOG: (param:0) = 16452 LOG: (query) SELECT indexrelid, repack.repack_indexdef(indexrelid, indrelid, $2, FALSE) FROM pg_index WHERE indrelid = $1 AND indisvalid LOG: (param:0) = 16452 LOG: (param:1) = (null) LOG: (query) SELECT repack.conflicted_triggers($1) LOG: (param:0) = 16452 LOG: (query) SELECT repack.create_index_type(16458,16452) LOG: (query) SELECT repack.create_log_table(16452) LOG: (query) CREATE TRIGGER repack_trigger AFTER INSERT OR DELETE OR UPDATE ON public.users FOR EACH ROW EXECUTE PROCEDURE repack.repack_trigger('user_id') LOG: (query) ALTER TABLE public.users ENABLE ALWAYS TRIGGER repack_trigger LOG: (query) SELECT repack.disable_autovacuum('repack.log_16452') LOG: (query) BEGIN ISOLATION LEVEL READ COMMITTED LOG: (query) SELECT pg_backend_pid() LOG: (query) SELECT pid FROM pg_locks WHERE locktype = 'relation' AND granted = false AND relation = 16452 AND mode = 'AccessExclusiveLock' AND pid <> pg_backend_pid() LOG: (query) COMMIT LOG: (query) BEGIN ISOLATION LEVEL SERIALIZABLE LOG: (query) SELECT set_config('work_mem', current_setting('maintenance_work_mem'), true) LOG: (query) SELECT coalesce(array_agg(l.virtualtransaction), '{}') FROM pg_locks AS l LEFT JOIN pg_stat_activity AS a ON l.pid = a.pid LEFT JOIN pg_database AS d ON a.datid = d.oid WHERE l.locktype = 'virtualxid' AND l.pid NOT IN (pg_backend_pid(), $1) AND (l.virtualxid, l.virtualtransaction) <> ('1/1', '-1/0') AND (a.application_name IS NULL OR a.application_name <> $2) AND a.query !~* E'^\\\\s*vacuum\\\\s+' AND a.query !~ E'^autovacuum: ' AND ((d.datname IS NULL OR d.datname = current_database()) OR l.database = 0) LOG: (param:0) = 112350 LOG: (param:1) = pg_repack LOG: (query) DELETE FROM repack.log_16452 LOG: (query) SELECT pid FROM pg_locks WHERE locktype = 'relation' AND granted = false AND relation = 16452 AND mode = 'AccessExclusiveLock' AND pid <> pg_backend_pid() LOG: (query) SET LOCAL lock_timeout = 100 LOG: (query) LOCK TABLE public.users IN ACCESS SHARE MODE LOG: (query) RESET lock_timeout LOG: (query) SELECT repack.create_table($1, $2) LOG: (param:0) = 16452 LOG: (param:1) = pg_default LOG: (query) INSERT INTO repack.table_16452 SELECT user_id,firstname,lastname,address,pincode,state,city,email,joining_date FROM ONLY public.users LOG: (query) SELECT repack.disable_autovacuum('repack.table_16452') LOG: (query) COMMIT LOG: (query) CREATE UNIQUE INDEX index_16458 ON repack.table_16452 USING btree (user_id) LOG: (query) SELECT repack.repack_apply($1, $2, $3, $4, $5, $6) LOG: (param:0) = SELECT * FROM repack.log_16452 ORDER BY id LIMIT $1 LOG: (param:1) = INSERT INTO repack.table_16452 VALUES ($1.*) LOG: (param:2) = DELETE FROM repack.table_16452 WHERE (user_id) = ($1.user_id) LOG: (param:3) = UPDATE repack.table_16452 SET (user_id, firstname, lastname, address, pincode, state, city, email, joining_date) = ($2.user_id, $2.firstname, $2.lastname, $2.address, $2.pincode, $2.state, $2.city, $2.email, $2.joining_date) WHERE (user_id) = ($1.user_id) LOG: (param:4) = DELETE FROM repack.log_16452 WHERE id IN ( LOG: (param:5) = 1000 LOG: (query) SELECT pid FROM pg_locks WHERE locktype = 'virtualxid' AND pid <> pg_backend_pid() AND virtualtransaction = ANY($1) LOG: (param:0) = {} LOG: (query) SAVEPOINT repack_sp1 LOG: (query) SET LOCAL lock_timeout = 100 LOG: (query) LOCK TABLE public.users IN ACCESS EXCLUSIVE MODE LOG: (query) RESET lock_timeout LOG: (query) SELECT repack.repack_apply($1, $2, $3, $4, $5, $6) LOG: (param:0) = SELECT * FROM repack.log_16452 ORDER BY id LIMIT $1 LOG: (param:1) = INSERT INTO repack.table_16452 VALUES ($1.*) LOG: (param:2) = DELETE FROM repack.table_16452 WHERE (user_id) = ($1.user_id) LOG: (param:3) = UPDATE repack.table_16452 SET (user_id, firstname, lastname, address, pincode, state, city, email, joining_date) = ($2.user_id, $2.firstname, $2.lastname, $2.address, $2.pincode, $2.state, $2.city, $2.email, $2.joining_date) WHERE (user_id) = ($1.user_id) LOG: (param:4) = DELETE FROM repack.log_16452 WHERE id IN ( LOG: (param:5) = 0 LOG: (query) SELECT repack.repack_swap($1) LOG: (param:0) = 16452 LOG: (query) COMMIT LOG: (query) BEGIN ISOLATION LEVEL READ COMMITTED LOG: (query) SAVEPOINT repack_sp1 LOG: (query) SET LOCAL lock_timeout = 100 LOG: (query) LOCK TABLE public.users IN ACCESS EXCLUSIVE MODE LOG: (query) RESET lock_timeout LOG: (query) SELECT repack.repack_drop($1, $2) LOG: (param:0) = 16452 LOG: (param:1) = 4 LOG: (query) COMMIT LOG: (query) BEGIN ISOLATION LEVEL READ COMMITTED LOG: (query) ANALYZE public.users LOG: (query) COMMIT LOG: (query) SELECT pg_advisory_unlock($1, CAST(-2147483648 + $2::bigint AS integer)) LOG: (param:0) = 16185446 LOG: (param:1) = 16452 postgres@hexacluster:~$\n\nWhen it is run on a table which does not contain either a primary key or a not\nnull unique key it throws an warning message like below and exits.\n\n    \n    \n    postgres@hexacluster:~$ pg_repack --table books -d hexacluster WARNING: relation \"public.books\" must have a primary key or not-null unique keys\n\nNote: When pg_repack is triggered for the entire database it first prepares a\nlist of tables that contains a PRIMARY KEY or a NOT NULL UNIQUE Constraint and\nworks only on those tables.\n\n#### Steps performed by pg_repack internally\n\nStep 1: Log Table Creation:\n\n  * The tool begins its process by creating a log table to track changes made to the original table.\n  * This log table will record INSERTs, UPDATEs, and DELETEs performed on the original table.\n\nStep 2: Trigger Addition:\n\n  * Next, pg_repack adds a trigger onto the original table.\n  * This trigger is responsible for capturing and logging any INSERTs, UPDATEs, and DELETEs made to the original table into the log table created in the previous step.\n\nStep 3: New Table Creation:\n\n  * The next step is to create a new table that contains all the rows from the old table.\n  * This new table serves as a fresh container for the data from the original table.\n\nStep 4: Index Building:\n\n  * After creating the new table, pg_repack constructs indexes on this new table.\n  * Indexes are crucial for optimizing query performance by speeding up data retrieval operations.\n\nStep 5: Applying Changes from Log Table:\n\n  * pg_repack then applies all the changes logged in the log table to the new table.\n  * This ensures that the new table is synchronized with the latest data modifications from the original table.\n\nStep 6: Table Swap:\n\n  * Once the new table is up-to-date with the changes from the log table, pg_repack swaps the tables.\n  * This swap involves replacing the old table with the new one, including all associated indexes and toast tables.\n  * The system catalogs are updated to reflect this table swap.\n\nStep 7: Original Table Drop:\n\n  * Finally, pg_repack drops the original table.\n  * The old table is no longer needed as its data has been successfully transferred to the new table.\n\n#### Locking Mechanism in pg_repack\n\n  * Throughout the process, pg_repack acquires and releases locks on the original table.\n  * Initially, it holds an ACCESS EXCLUSIVE lock during the setup (steps 1 and 2) and the final swap-and-drop phase (steps 6 and 7).\n  * However, for the majority of its operation, pg_repack only requires an ACCESS SHARE lock on the original table.\n  * This allows concurrent INSERTs, UPDATEs, and DELETEs to proceed without significant interruptions.\n\n#### Important Arguments that needs to be considered in a production\nenvironment\n\n  1. -T SECS, \u2013wait-timeout=SECS:\n\n     * Controls the duration (in seconds) pg_repack will wait to acquire exclusive locks at the beginning and end of the repacking process.\n     * If the lock cannot be obtained within this timeframe and the \u2013no-kill-backend option is not specified, pg_repack will forcibly cancel conflicting queries.\n     * Default value is 60 seconds.\n  2. -D, \u2013no-kill-backend:\n\n     * Allows pg_repack to skip repacking the table if the lock cannot be obtained within the duration specified by \u2013wait-timeout.\n     * By default, conflicting queries are canceled if the lock cannot be acquired.\n     * In a production system where the user queries should not get affected or terminated using this option is highly recommended along with the higher \u2013wait-timeout seconds.\n  3. -k, \u2013no-superuser-check:\n\n     * Skips the superuser checks in the client.\n     * Useful for running pg_repack on platforms that support non-superuser execution.\n     * This option is useful where users don\u2019t have superuser privileges. Ideally used in DBAAS environments like RDS etc.\n  4. -j, \u2013jobs:\n\n     * Specifies the number of extra connections to PostgreSQL to create.\n     * These extra connections are utilized to parallelize the rebuild of indexes on each table during the repacking process.\n     * Parallel index builds are supported only for full-table repacks, not with \u2013index or \u2013only-indexes options.\n     * If the PostgreSQL server has additional cores and available disk I/O, this option can significantly accelerate pg_repack\u2019s performance.\n\n#### Conclusion\n\nIn conclusion, pg_repack emerges as a valuable tool for PostgreSQL database\nadministrators seeking to manage disk space efficiently while minimizing\ndowntime and performance impacts. It provides a robust solution for PostgreSQL\nusers to maintain database health by efficiently rebuilding tables online,\naddressing fragmentation, and minimizing table locking.\n\nWith its capabilities, users can optimize database performance and storage\nutilization, ensuring smooth operation even in high-traffic environments.\n\nSeeking specialized PostgreSQL support or Database Migrations assistance ? Get\nexperts advice for effective data management, query tuning, database\nmigrations and database optimization. Click here for personalized expertise to\nelevate your database performance and meet your unique business needs.\n\nIn our next article we will cover about pg-osc a tool for making schema\nchanges in PostgreSQL tables with minimal locks, thus helping achieve zero\ndowntime schema changes against production workloads.\n\nSubscribe to our Newsletters and Stay tuned for more interesting topics.\n\n## Contact Us Today!\n\n## Author\n\n  * Manisankar K\n\nManisankar is the Director of Database Operations at HexaCluster. Before\njoining HexaCluster, he worked as a Senior Database Manager at MigOps and as a\nDatabase Consultant at HCL. As a PostgreSQL expert, Mani has supported\nnumerous customers in deploying PostgreSQL and published a number of technical\narticles in his free time. In addition to PostgreSQL, he is also skilled in\nother database technologies such as MySQL, Oracle, Cassandra and more.\nManisankar is also well-versed in development languages such as Python,\nGroovy, and Perl, empowering him to create solutions that automate complex\noperations for Customers.\n\n### Add A Comment Cancel reply\n\n## Our PostgreSQL Services\n\n  * Migrations to Open Source\n  * Advanced Remote DBA Services\n  * 24/7 PostgreSQL Support\n  * Architectural Health Audit\n  * PostgreSQL Trainings\n\nContact Us\n\nSubscribe to our Free Newsletters\n\n## Our Machine Learning Services\n\n  * Proof of Concept (POC) - Machine Learning\n  * Implementation of Use Cases - Machine Learning\n  * MLOps - Machine Learning\n\nContact Us\n\nShare article\n\nShareTweetShare\n\n### Get in Touch\n\n  * One Dundas Street West Suite 2500, Toronto, Ontario, M5G 1Z3, Canada\n  * connect@hexacluster.ai\n  * +1(902) 221-5976\n\n##### Explore\n\n  * Migrations to Open Source\n  * 24/7 PostgreSQL Support\n  * Implementation of Use Cases - Machine Learning\n\n##### Our products\n\n  * Ora2Pg\n  * PostgreSQL High Availability\n  * PostgreSQL Backup & Recovery\n  * Our Contributions\n\n\u00a9 2023 Company. All Rights Reserved. Privacy Policy.\n\nWe'd like to show you notifications for the latest news and updates.\n\n", "frontpage": false}
