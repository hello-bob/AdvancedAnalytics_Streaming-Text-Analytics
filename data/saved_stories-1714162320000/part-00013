{"aid": "40168848", "title": "Mixtures of Experts (2023)", "url": "https://blog.javid.io/p/mixtures-of-experts", "domain": "javid.io", "votes": 1, "user": "tosh", "posted_at": "2024-04-26 12:53:50", "comments": 0, "source_title": "Mixtures of Experts", "source_text": "Mixtures of Experts - Javid Lakha\n\n# Javid Lakha\n\nShare this post\n\n#### Mixtures of Experts\n\nblog.javid.io\n\n#### Discover more from Javid Lakha\n\nMachine learning and artificial intelligence research\n\nContinue reading\n\nSign in\n\n# Mixtures of Experts\n\n### How to scale up models without increasing compute\n\nJavid Lakha\n\nAug 30, 2023\n\n1\n\nShare this post\n\n#### Mixtures of Experts\n\nblog.javid.io\n\nShare\n\nWhen deploying machine learning models, inference costs hurt more than\ntraining costs. A model need only be trained once - and can be based on a\npredecessor - but the cost of running GPUs in production can be prohibitive.\nUnfortunately, the best neural networks have high parameter counts - and their\nperformance increases with scale. Consequently, sparse models - where, for a\nparticular input, only the most relevant parameters are used - have become\npopular. These enable the total parameter count to be increased, while holding\nconstant the number of floating point operations required at inference.\n\nA mixture of experts is a technique in which observations are routed to the\nparts of the model - the \u2018experts\u2019 - best positioned to deal with them. If not\nall experts are used, the model is sparse. Mixtures of experts train faster\nthan dense models, are more accurate and require less compute at inference\ntime. However, due to their high parameter count, more complex architecture\nand the need to distribute them across multiple GPUs, careful attention must\nbe paid to routing, load balancing, training instabilities and the inference-\ntime provisioning of experts across devices.\n\n###\n\nArchitecture\n\nIn principle, each expert can have its own architecture. Mostly, this is not\ndone - compared to other optimisations, the benefits are limited. While it\nmight help to use different architectures for different modalities, within\neach modality the default choices (Transformers for text, CNNs or Vision\nTransformers for images) tend to be sufficiently powerful. Moreover, there are\npractical reasons not to:\n\n  * At inference time, an expert might not be available - e.g. if it is oversubscribed. In this case, it helps if experts are not too divergent, so that the data can be routed to the next best expert.\n\n  * Using a slower expert will hold up the rest of the batch and harm GPU utilisation.\n\n####\n\nTransformers\n\nFigure 1: A mixture of experts implemented in a Transformer block\n\nIn a Transformer, a mixture of experts is normally implemented by replacing\nthe feedforward network in each block (or in every other block) with N\ndistinct feedforward networks and a lightweight router that sends data to at\nmost k of them, as shown in Figure 1.1 Under this setup, each Transformer\nblock has its own conditional computation step, and hence the expert networks\nare distributed throughout the stack. In each block, the attention heads first\nmix data between timesteps. Then, at each timestep, the k best experts are\nselected by the router; these operate on the timestep\u2019s hidden state\nindependently. The outputs from each of the activated experts - and the\nresidual bypass connection - are then combined and proceed to the next part of\nthe model.\n\nIf the data are multimodal, it might be worthwhile to also compute the\nattention heads conditionally. However, if the relationship between tokens are\nnot so domain-dependent that they need to be computed separately, then this\nspecialisation would be wasteful. A Transformer block typically contains\nmultiple attention heads - or, at least, multiple query matrices - so\ndifferent relationships can be learned. Though mixtures of experts do not\nrequire more computation, each specialised expert requires more memory - and\nGPU memory is expensive. The attention heads account for a high proportion of\na Transformer\u2019s parameters.2 Consequently, if there are fewer benefits to\nspecialising them, it makes sense not to.\n\nNonetheless, Transformers in which the attention heads are separated into\nexperts do exist. One notable example is the Branch-Train-Merge large language\nmodel, in which no parameters - except for the router - are shared. While this\nreduces or, if each expert is small enough, eliminates the costly\ncommunication overhead between GPUs at training and inference time, it is only\nviable in production if the demand for each expert can be accurately forecast.\nOtherwise, some models - and GPUs - will sit idle and others will be\noverloaded.\n\n###\n\nRouting\n\n####\n\nToken routing\n\nDetermining the best expert to process a token is a discrete optimisation\nproblem. Discrete optimisation problems are not differentiable and hence, to\nlearn the optimal allocation of tokens using backpropagation, a \u2018soft\u2019\nassignment needs to be made instead.3 This is done by weighting the output of\nthe experts E1, E2, ..., EN by the trainable function R. Let x be the token\nvector and y be the summed output of the expert layer. Then, not forgetting to\ninclude a residual connection,\n\nThe router function R is parametrised in terms of a trainable weight matrix W,\nwhose input dimension is equal to the dimension of the token vector x and\nwhose output dimension is equal to the number of experts N.4 A na\u00efve\nimplementation of R is as a softmax:\n\nHowever, in this form, each expert has a strictly positive weight R(x)i. This\nmeans that the mixture of experts is not sparse: every expert output Ei(x)\nmust be computed. Instead, R is modified so that the k highest values of R(x)\nare retained and the rest are set to 0:\n\nOriginally, it was thought to be necessary to route to k \u2265 2 experts, as the\nmodel could not possibly learn how to route tokens if it could not directly\ncompare experts. However, this is not so: k = 1 also works well. The reason\nfor this is the residual connection. If an expert hurts performance, its\nparameters are modified - so that next time, it hopefully performs better -\nand it is downweighted. If, for a particular subset of tokens, the expert\ncontinues to do badly, it will fall out of the top k and the next best expert\nwill be chosen instead.\n\n####\n\nLoad balancing\n\nIf too many tokens are allocated to a particular expert, the expert will be\noversubscribed and unable to process them all. When this happens, tokens can\neither be assigned to the next best expert or they can be \u2018dropped\u2019 - i.e.\nexpert computation is omitted and the token representation propagates to the\nnext layer through the residual connection. Even if expert capacity is\noverprovisioned or fully elastic, it is important to prevent a \u2018winners get\nbigger\u2019 effect: a positive feedback loop in which one expert becomes better\nand better at handling all tokens compared to the other untrained experts, so\nthat the mixture of experts model becomes functionally equivalent to a dense\nmodel with redundant parameters.\n\nTo encourage a roughly equal allocation of tokens across experts, a load\nbalancing term is added to the model\u2019s training loss. This is done by\npenalising high router weights. Let B be a batch consisting of T tokens, N be\nthe number of experts, f(i) be the fraction of tokens in the batch dispatched\nto expert i, p(i) be the fraction of the router weights assigned to expert i\nover the entire batch, R be the router weights outlined above. Fedus et al\n(2021) propose adding the following loss term for every expert layer:\n\nwhere \u03b1 is hyperparameter that reflects the importance assigned to load\nbalancing. The the fraction of tokens in the batch dispatched to expert i is\n\n11\n\nand the fraction of the router weights assigned to expert i over the batch is\n\nSo the loss term is, essentially, the sum of the k highest router weights for\neach token in the batch5, multiplied by the scaling factors N, k and T and the\nhyperparameter \u03b1. (These scaling factors are there to ensure that the loss is\ninvariant to changes in the total number of experts, the number of experts\nchosen for each token and the batch size.) The loss term is minimised when\nR(x)i = 1/N - i.e. tokens are routed to each expert with equal probability.\n\n####\n\nExpert choice\n\nInstead of routing each token to the top k experts, Zhou et al (2022) solve\nthe load balancing problem by letting each expert select the top t tokens from\neach sequence. This means that informative tokens can be processed by as many\nexperts as necessary and uninformative tokens - e.g. padding tokens or stop\nwords - can be ignored.\n\nDispensing with the requirement that the same amount of compute is used for\nevery token appears to be a productive strategy: expert choice models train\ntwice as fast as top-k experts models and perform better on GLUE and SuperGLUE\nlanguage understanding benchmarks.\n\nKomatsuzaki et al (2022) and Shen et al (2023) also observe that expert choice\nmodels outperform top-k experts models.\n\n####\n\nSoft mixtures of experts\n\nPuigcerver et al (2023) propose a variant of expert choice routing, which they\ncall a soft mixture of experts. In this model, experts act on sequences not\ntokens: each expert processes a weighted combination of all of the tokens in\nthe input sequence. The weights are unique to each expert and are learned.\nConceptually, the scientific expert places a greater emphasis on the\nscientific content in the sequence and the legal expert places a greater\nemphasis on the legal content.\n\nAs in expert choice models, each expert is utilised equally by design, so the\nload is never unbalanced: experts are never oversubscribed; nor do they sit\nidle. However, tokens are not processed equally: the emphasis an expert places\non each token depends upon the learned weights. As each token is fractionally\nprocessed by each expert, these models are technically not sparse. However, as\nthe emphasis on each token is uneven, they are not dense either.\n\nOne problem with this approach is that is solves load balancing by\ndiscouraging expert specialisation. Every sequence is processed by every\nexpert, and so experts are trained to be generally useful. When experts are\nconditionally activated, a pressure to generalise arises from the load\nbalancing loss term. However, in these models, equal utilisation is not a hard\nrequirement; the importance assigned to load balancing can be tuned using the\nloss term hyperparameter \u03b1.\n\n###\n\nTraining\n\n####\n\nPerformance and training speed\n\nSparse mixtures of experts train faster than dense models. Fedus et al (2021)\ncompare a dense Transformer (T5-Base) with various Switch Transformers, a\nmixture of experts model in which for the feedforward layers in each\nTransformer block tokens are either routed to their top expert or dropped.\nTrained using the same resources - 32 v3 TPUs - the mixtures of experts\nachieved the same performance the dense transformer in 1/7th of the time.\n\nFigure 2: Training times (in hours) and performance (negative log perplexity)\nof a dense transformer (T5-Base) and mixtures of experts with 32, 64 and 128\nexperts per layer. All models were trained on 32 TPUv3 - i.e. per hour of\ntraining time, each model was trained using equal compute. Source: Fedus et al\n(2021)\n\nMoreover, compared to a larger dense model (T5-Large), a Switch Transformer\nwith 64 experts trained 2.5 times faster, despite using only 2/7ths of the\ncompute.\n\nFigure 3: Training time (in hours) and performance (negative log perplexity)\nof two dense transformers (T5-Base and T5-Large) and a mixture of experts\nmodel (Switch-Base: 64e) with 64 experts per layer. T5-Large was trained with\n3.5x more compute than T5-Base or Switch-Base 64e. Source: Fedus et al (2021)\n\nChanging the routing model unlocks further performance gains. Zhou et al\n(2022) observe that expert choice models train twice as fast compared to top-k\nexpert routing models, including Switch Transformers.\n\n####\n\nScaling\n\nSparse models are promising because they enable the number of parameters to be\nincreased separately from the number of floating point operations required.\nDense models have predictable scaling laws: provided they are not bottlenecked\nby an absence of high-quality training data, as the number of parameters is\nincreased, the cross-entropy loss decreases as a power law.\n\nSparse models also demonstrate consistent performance increases when scaled:\n\nFigure 4: Sparse mixtures of experts demonstrate a power-law decrease in\ncross-entropy test loss as the number of experts is increased. All models were\ntrained using the same compute. Note the logarithmic scale on the x-axis.\nSource: Fedus et al (2021)\n\nClark et al (2022) observe that this relationship holds for other routing\nalgorithms (the Sinkhorn algorithm, reinforcement learning and hashing the\ntokens). However, the scaling law they fit deviate from the power-law\nrelationships observed in dense models. Instead, the authors find that the\nperformance improvement obtained by adding more experts decreases as the\nmodels become larger. Once a dense model obtains ~900 billion parameters, the\nauthors predict there will not be a further improvement to using a sparse\nmixture of experts. A strong caveat to this prediction must be made: the\nmodels used to fit the curve predate the Chinchilla paper, which counsels that\nparameters and training tokens should be scaled equally. Instead, each of the\nmodels were trained on 130 billion tokens.\n\nFigure 5: Sparse mixtures of experts decrease in language modelling (cross-\nentropy) loss as the number of experts is increased, for a number of routing\nmethods: Sinkhorn (S-BASE), reinforcement learning (RL-R) and token hashing\n(Hash). Note the logarithmic axes. All models were trained using the same\ncompute. Source: Clark et al (2022)\n\nIn dense models, increasing the number of parameters makes a model more sample\nefficient - i.e. it trains faster. As Figure 2 shows, this also obtains for\nmixtures of experts: as the number of experts - and thus parameters - is\nincreased, models require fewer samples to achieve the same level of\nperformance.\n\n####\n\nInstabilities\n\nCompared to dense transformers, sparse mixtures of experts are more prone to\nsuffering from training instabilities. These occur when, instead of\ndecreasing, the loss function diverges to infinity. Mixtures of experts are\nparticularly afflicted because they contain more exponential functions, which\ncompound the roundoff errors that arise from mixed precision training.\n\nTo reduce computation and communication costs, large neural networks store\nweights as float32 but operate on them as bfloat16. float32 uses 1 bit to\nrepresent the sign, 8 bits to represent the exponent and 23 bits to represent\nprecision. bfloat16 has the same range as float32, but uses 1 bit to represent\nthe sign, 8 bits to represent the exponent and only 7 bits to represent\nprecision. Consequently, the roundoff errors for bfloat16 are several orders\nof magnitude higher. Figure 6 shows how these errors are worse for large,\npositive numbers; a similar pattern exists for negative numbers.\n\nFigure 6: Maximum float32 and bfloat32 roundoff errors for selected positive\nnumber ranges. Adapted from Zoph et al (2022)\n\nMixtures of experts contain more exponential functions than dense models,\nbecause of the softmax function in their routers. Compared to the other\noperations in a neural network (e.g. addition or matrix multiplications),\nexponential functions are numerically unstable: a small change in the input -\nwhich might be caused by a roundoff error - can result in a large change in\nthe output. This problem is particularly acute for high magnitude inputs, as\nthe roundoff errors are larger.\n\nZoph et al (2022) demonstrate that penalising high magnitude inputs to the\nrouter\u2019s softmax function improves training stability without reducing the\nmodel\u2019s performance. Let T be the number of tokens in a batch and be N the\nnumber of experts. Let W be the router\u2019s weights and X be the token\nrepresentations in the batch. Then, letting \u03b2 be a hyperparameter that\nreflects its importance, define the auxiliary Z-loss term as follows:\n\nThis loss function is minimised when WX - the inputs to the router\u2019s softmax\nfunction - are small. Computation of the router probabilities depends only on\nthe relative magnitude of the logits, so penalising high inputs does not\nnecessarily change these. The router probabilities determine which experts\nprocess a token and how the expert outputs are scaled. Consequently, numerical\ninstabilities due to roundoff errors have a compounding effect.\n\nThe authors report that training instability can be further reduced by\n\n  * \u2018jittering\u2019 the inputs to the router\u2019s softmax by multiplying them by a number drawn uniformly at random from [0.99, 1.01]\n\n  * replacing multiplicative activations such as GELU gated linear units (GEGLUs) with rectified linear units and removing multiplicative normalisations such as root mean square error normalisation\n\n  * applying dropout throughout the model.\n\nUnfortunately, these methods also degrade the model\u2019s accuracy.\n\n####\n\nFine-tuning and instruction tuning\n\nSparse models perform well when training on large, diverse datasets. However,\nduring fine-tuning, they are susceptible to overfitting and consequently may\nperform worse than their dense counterparts.\n\nMitigating this problem requires setting noisier hyperparameters. Zoph et al\n(2022) demonstrate that, in contrast to dense models, sparse models perform\nbetter at fine-tuning when the learning rate is higher and the batch size is\nsmaller. Dropping out entire experts does not help, but increasing the dropout\nprobability within each expert has a moderate, positive effect.\n\nFigure 7: Sparse models benefit from smaller batch sizes and higher learning\nrates during fine-tuning. Dense models are the opposite. Both models are\ncompute-matched Transformers that were first pretrained for 500,000 steps in\nbatches of 1 million tokens on the CommonCrawl C4 dataset and then fine-tuned\non the SuperGlue benchmark dataset. Source: Zoph et al (2022)\n\nSparse mixtures of experts benefit more from instruction tuning than dense\nmodels. Shen et al (2023) establish that, in the absence of instruction\ntuning, mixtures of experts do worse than dense models when evaluated or\nfurther fine-tuned on downstream tasks. However, once both types of model are\ninstruction-tuned, mixtures of experts outperform dense models.\n\n####\n\nUpcycling\n\nA useful trick, pioneered by Komatsuzaki et al (2022), is to \u2018upcycle\u2019 an\nexisting dense model by initialising a mixture of experts from the dense model\ncheckpoint. In a Transformer, this is done by training the router from\nscratch, but initialising the experts from the weights of the feedforward\nlayers they replace. Holding compute constant and continuing training,\nupcycled mixtures of experts train faster than their dense base models.\n\nFigure 8: Upcycled models train faster than continuing to train the dense\nmodels on which they are based. The panel on the left shows performance on an\nimage classification dataset and the panel on the right shows token prediction\nperformance on CommonCrawl. B/32 is a Vision Transformer with patch size 32;\nB/16 and L/16 are Vision Transformers with patch size 16. Base, Large and XL\nare variants of the T5 language model. Source: Komatsuzaki et al (2022)\n\nSo, if there already exists a dense model checkpoint, upcycling it to a\nmixture experts might improve performance. And, if there does not, given that\ntraining a mixture of experts is more complicated, training a dense model and\nthen upcycling it might be a useful hedging strategy.\n\n####\n\nDistillation\n\nAfter training, a sparse mixture of experts model can be distilled into a\nsmaller, dense model. If all experts are equally utilised, a sparse model and\na dense model should require the same amount of GPU memory. (The sparse model\nwill have greater throughput and hence require large batch sizes.) However, if\nthey are not, then training a mixture of experts and then distilling it into a\ndense model is still worth doing, as some of the performance gain is retained.\nFedus et al (2021), for example, compare a sparse mixture of experts model to\na dense T5-Base model that is 100 times smaller. The mixture of experts\nnaturally performs better. However, distilling the weights of the mixture of\nexperts model into the dense model preserves 29% of the performance gain.\n\n###\n\nDeployment\n\nMixtures of experts typically contain outrageously high parameter counts.\nConsequently, though the computation requirements are low, the memory\nrequirements are extremely high and the model must be distributed across\nmultiple GPUs. To do this without incurring accuracy or latency penalties\nrequires paying close attention to expert provisioning and device allocation.\n\n####\n\nModel capacity\n\nDespite the load balancing objective that is present during training, at\ninference time, expert utilisation is extremely unbalanced. Huang et al (2023)\nexamine the expert activation patterns for state-of-the-art mixture of expert\nset-ups and observe that for language modelling and machine translation, a\nsmall number of experts are always allocated a large share of tokens; others\nare completely inactive. Machine translation is particularly bad, because\nthere is high temporal correlation: if one sequence makes use of a particular\nexpert, the probability that the next sequence will use that expert is higher.\n\nFigure 9: At inference time, expert utilisation is extremely unbalanced. Panel\n(a) shows a mixture of experts layer in a language model, evaluated on various\ndatasets. Panel (b) shows two mixture of experts layers from a machine\ntranslation model, evaluated on translations from English into French (fra),\nJapanese (jpn) and Asturian (ast). Each pixel represents a batch; a vertical\nline represents the entire dataset. Source: Huang et al (2023)\n\nThis is a problem, because it makes it difficult to predict the correct\ncapacity for each expert at inference time. If expert utilisation is roughly\nequal, then provisioning experts is straightforward. Let N be the number of\nexperts in a model, T be the number of tokens in each batch and C be the\ncapacity factor, a hyperparameter. Then, the na\u00efve implementation is to\nstatically provision on a GPU a fixed capacity of CT/N tokens for each expert.\nTo make efficient use of GPU parallelism, the expert feedforward networks on\neach device are stacked as tensors. If an expert is oversubscribed, the\nadditional tokens are dropped or routed to an expert with spare capacity. If\nan expert is undersubscribed, padding tokens are allocated instead. This means\nthat the token representations can also be stacked as tensors, improving\nefficiency. The dimensions of the expert tensor is num_experts_on_device \u00d7\nCT/N \u00d7 expert_output_dimension and the dimensions of the token representations\ntensor is num_experts_on_device \u00d7 CT/N \u00d7 token_representation_dimension. The\ntwo tensors are then multiplied together to compute the expert outputs.\n\nWhen expert allocation is unbalanced, the oversubscribed experts will drop (or\nreroute) tokens, hurting performance. The undersubscribed experts will waste\ncompute processing padding tokens. If the imbalance is slight, token dropping\ncan be reduced by increasing the capacity factor. However, for extreme\nimbalances, this is extremely wasteful.\n\nIf expert utilisation can be accurately predicted ahead of time, then\nindividual capacity factors can be set for each model. Huang et al (2023)\ndemonstrate how this can be done by examining historical load data. The\noptimal allocation of experts to GPUs is isomorphic to the multiway number\npartitioning problem, which is NP-hard. This can be approximated using a\ngreedy algorithm: at each step, the expert with the highest historical\nworkload is allocated to the GPU with the smallest load. Unfortunately, this\napproximation is less effective if sequences are temporally correlated, as in\nmachine translation. To fix this, an expert\u2019s predicted workload needs to be\nincreased by adding the historical workload of each of its correlated experts\nmultiplied by the strength of the correlation and a scaling factor.\n\n####\n\nCaching\n\nA further optimisation that can be made is to offload the least used experts\nonto the CPU. This can be implemented using a least recently used cache - and\nthis would account for temporal correlations.6 If there is a cache miss, there\nis a latency penalty, as the required expert must be copied to a GPU for\ninference. Huang et al (2023) find that for a machine translation model with\n128 experts, reducing the cache size from 128 to 64 decreases memory usage by\n~15% but increases latency by ~0.15 seconds (i.e. ~2.5 times).\n\n####\n\nBlock sparsity\n\nGPUs are designed to perform the same computation multiple times in parallel;\nhence, they are more efficient at dense matrix multiplications compared to\nsparser operations. To exploit this efficiency, deep learning libraries -\nincluding those that specialise in training mixtures of experts - make heavy\nuse of batched matrix multiplications. These require the batched matrices to\nhave the same shape - and some libraries even require that the shapes are\nstatically specified at compile time. This is the reason why, at inference\ntime, expert capacity must be accurately predicted and why the inputs to the\nexperts must be padded or truncated.\n\nMatrix multiplication consists of the repeated application of the dot product\nbetween the rows of the first matrix and the columns of the second. To\nmultiply large matrices together, the GPU breaks the input and output matrices\ninto tiles. The output tiles are computed by stepping through the inner\ndimension of the input matrices tile by tile, multiplying them together and\naccumulating the result.\n\nFigure 10: Tiled matrix multiplication. Source: NVIDIA\n\nGale et al (2022) observe that the forward and backward passes in a mixture of\nexperts feedforward layer can be expressed as block sparse matrix\nmultiplications. (In a block sparse matrix, the nonzero elements are clustered\ninto square submatrices.) This is useful, because block sparse matrix\nmultiplications can also be tiled, enabling them to exploit GPU parallelism.\nAs the empty blocks in the output matrix do not need to be computed, in\nprinciple block sparse matrix multiplication can achieve the same throughput\nas batched dense matrix multiplication.\n\nFigure 11: (A) A mixture of experts feedforward layer is normally implemented\nas a batched matrix multiplication. (B) Equivalently, this can be computed as\na block sparse matrix multiplication. (C) This enables imbalanced routing and\n- if desired - experts of different sizes. Source: Gale et al (2022)\n\nWhen implemented as a block sparse matrix multiplication, a mixture of experts\nfeedforward layer can flexibly and efficiently handle unbalanced token\nassignments without needing to drop tokens or waste compute processing padding\ntokens. Gale et al (2022) provide the CUDA kernels required to do this. Their\nimplementation achieves 98.6% of the throughput obtained by cuBLAS for batched\ndense matrix multiplications, evaluated on mixture of experts feedforward\nlayers.\n\nFigure 12: Block sparse matrix multiplications obtain, on average, 98.6% of\nthe throughput of cuBLAS batched dense matrix multiplications, when evaluated\non the forward (fwd) and backwards (gradw) passes of different mixture of\nexperts layers. MoE-XS, MoE-Small and MoE-Medium each contain 64 experts; have\n839, 3,693 and 13,041 parameters; and compute 316, 879 and 2,487 floating\npoint operations per batch, respectively. Source: Gale et al (2022)\n\nUsing block sparse matrix multiplications decreased the training time required\nby 1.38 times for the smallest model and 4.35 times for the largest.\n\n###\n\nHow to train a fast and accurate mixture of experts model\n\nThis advice will help you get to the state of the art as of August 2023.\n\n  * The optimal number of experts is between 64 and 128. Mixtures of experts, as with dense models, obey scaling laws: more parameters (i.e. more experts) result in better performance. However, there are diminishing returns: after ~128 experts the curve starts to taper.\n\n  * Use expert choice routing instead of routing each token to its top k experts. Zhou et al (2022), Komatsuzaki et al (2022) and Shen et al (2023) all find that this increases performance. If using top-k experts per token, Fedus et al (2021) and Clark et al (2022) find that k=1 is sufficient; this will reduce training and inference costs.\n\n  * Penalise high magnitude inputs into exponential functions - in particular, the softmax operation in the routers. Exponential functions are numerically unstable and the bfloat16 representation that is used in mixed precision training has large roundoff errors at high magnitudes. Zoph et al (2022) observe this reduces training instability, but does not harm performance.\n\n  * Train each expert on curated data from different distributions. Li et al (2022) note that experts trained on data split by provenance performed much better than experts trained on random data splits.\n\n  * If you have an existing dense model, \u2018upcycling\u2019 it into a mixture of experts might be more efficient than training a new model from scratch.\n\n  * Instruction-tune the model, but be aware that the optimal hyperparameters for training are not the optimal hyperparameters for fine-tuning. Shen et al (2023) establish that, in the absence of instruction tuning, mixtures of experts do worse than dense models on downstream tasks. However, once both types of model are instruction-tuned, mixtures of experts outperform dense models.\n\n  * Use the MegaBlocks CUDA kernels implemented by Gale et al (2022). This method of provisioning experts never drops tokens, eliminates wasteful computation and trains faster.\n\nShare\n\n###\n\nFurther reading\n\n####\n\nPapers\n\n  * Aidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, George van den Driessche, Eliza Rutherford, Tom Hennigan, Matthew Johnson, Katie Millican, Albin Cassirer, Chris Jones, Elena Buchatskaya, David Budden, Laurent Sifre, Simon Osindero, Oriol Vinyals, Jack Rae, Erich Elsen, Koray Kavukcuoglu and Karen Simonyan (2022), \u2018Unified Scaling Laws for Routed Language Models\u2019\n\n  * William Fedus, Barret Zoph and Noam Shazeer (2022), \u2018Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\u2019\n\n  * Trevor Gale, Deepak Narayanan, Cliff Young and Matei Zaharia (2022), \u2018MegaBlocks: Efficient Sparse Training with Mixture-of-Experts\u2019\n\n  * Haiyang Huang, Newsha Ardalani, Anna Sun, Liu Ke, Hsien-Hsin S. Lee, Anjali Sridhar, Shruti Bhosale, Carole-Jean Wu and Benjamin Lee (2023), \u2018Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference\u2019\n\n  * Aran Komatsuzaki, Joan Puigcerver, James Lee-Thorp, Carlos Riquelme Ruiz, Basil Mustafa, Joshua Ainslie, Yi Tay, Mostafa Dehghani and Neil Houlsby (2022), \u2018Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints\u2019\n\n  * Margaret Li, Suchin Gururangan, Tim Dettmers, Mike Lewis, Tim Althoff, Noah A. Smith and Luke Zettlemoyer, \u2018Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models\u2019\n\n  * Joan Puigcerver, Carlos Riquelme, Basil Mustafa and Neil Houlsby (2023), \u2018From Sparse to Soft Mixtures of Experts\u2019\n\n  * Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton and Jeff Dean (2017), \u2018Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\u2019\n\n  * Sheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne Longpre, Jason Wei, Hyung Won Chung, Barret Zoph, William Fedus, Xinyun Chen, Tu Vu, Yuexin Wu, Wuyang Chen, Albert Webson, Yunxuan Li, Vincent Zhao, Hongkun Yu, Kurt Keutzer, Trevor Darrell, Denny Zhou (2023), \u2018Mixture-of-Experts Meets Instruction Tuning: A Winning Combination for Large Language Models\u2019\n\n  * Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Zhifeng Chen, Quoc Le and James Laudon (2022), \u2018Mixture-of-Experts with Expert Choice Routing\u2019\n\n  * Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer and William Fedus (2022): \u2018ST-MoE: Designing Stable and Transferable Sparse Expert Models\u2019\n\n####\n\nBlog posts\n\n  * Davis Blalock (2022), \u20182022-12-4 arXiv roundup: New best MoE implementation, 3x faster transformer inference\u2019\n\n  * Davis Blalock (2023), \u20182023-3-19 arXiv roundup: GPT-4, Data deduplication, MoE optimizations\u2019\n\n  * Finbarr Timbers (2023), \u2018Papers I\u2019ve read this week, Mixture of Experts edition\u2019\n\n####\n\nVideos\n\n  * Barret Zoph and Irwan Bello (2022), \u2018Mixture of Experts (MoE) paradigm and the Switch Transformer\u2019, Stanford CS25: Transformers United\n\n  * Barret Zoph, William Fedus and Yannic Kilcher (2022), \u2018Sparse Expert Models (Switch Transformers, GLAM, and more... w/ the Authors)\u2019\n\n1\n\nZoph et al (2022) discovered that putting a dense feedforward layers\nimmediately before or after each sparse layer improves performance. This is\nnot due to the extra parameters, as adding dense feedforward layers elsewhere\nis less effective.\n\n2\n\nIn GPT-3, for instance, the attention heads account for ~1/3 of the\nparameters.\n\n3\n\nNon-gradient based methods of routing tokens to experts include reinforcement\nlearning, linear programmes and hash functions.\n\n4\n\nUsing more layers in the router does not increase performance. The reason for\nthis is because the router is not trained in isolation from the rest of the\nmodel: if more compute is needed, the other layers can adapt. William Fedus,\nBarret Zoph and Yannic Kilcher have a good discussion about improving the\nrouting function on Yannic\u2019s YouTube channel.\n\n5\n\nIn Fedus et al (2021), k = 1.\n\n6\n\nHuang et al (2023) adopt a different policy instead. First, experts that are\nnot active in the present batch are evicted. This is to account for the\ntemporal correlation of inputs. Next, experts are evicted using a last in,\nfirst out policy. This is to ensure the expert with the shortest reuse\ndistance remains in the cache.\n\n### Subscribe to Javid Lakha\n\nMachine learning and artificial intelligence research\n\n1 Like\n\n1\n\nShare this post\n\n#### Mixtures of Experts\n\nblog.javid.io\n\nShare\n\nBayesian Flow Networks\n\nExtending diffusion models to discrete data\n\nOct 5, 2023 \u2022\n\nJavid Lakha\n\nShare this post\n\n#### Bayesian Flow Networks\n\nblog.javid.io\n\nReady for more?\n\n\u00a9 2024 Javid Lakha\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n", "frontpage": false}
