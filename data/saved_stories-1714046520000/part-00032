{"aid": "40153815", "title": "Tiny GPU: A minimal GPU implementation in Verilog", "url": "https://github.com/adam-maj/tiny-gpu", "domain": "github.com/adam-maj", "votes": 5, "user": "fgblanch", "posted_at": "2024-04-25 05:36:05", "comments": 0, "source_title": "GitHub - adam-maj/tiny-gpu: A minimal GPU design in Verilog to learn how GPUs work from the ground up", "source_text": "GitHub - adam-maj/tiny-gpu: A minimal GPU design in Verilog to learn how GPUs\nwork from the ground up\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nadam-maj / tiny-gpu Public\n\n  * Notifications\n  * Fork 13\n  * Star 147\n\nA minimal GPU design in Verilog to learn how GPUs work from the ground up\n\n147 stars 13 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# adam-maj/tiny-gpu\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nadam-majUpdate README.mdApr 24, 2024de7cf22 \u00b7 Apr 24, 2024Apr 24, 2024\n\n## History\n\n83 Commits  \n  \n### docs/images\n\n|\n\n### docs/images\n\n| Update README.md| Apr 24, 2024  \n  \n### gds\n\n|\n\n### gds\n\n| Add GDS files| Apr 24, 2024  \n  \n### src\n\n|\n\n### src\n\n| Update documentation and remove bad solutions| Apr 24, 2024  \n  \n### test\n\n|\n\n### test\n\n| Add .gitkeep to logs| Apr 24, 2024  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| Add GDS files| Apr 24, 2024  \n  \n### Makefile\n\n|\n\n### Makefile\n\n| Add matmul kernel| Apr 20, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update README.md| Apr 24, 2024  \n  \n## Repository files navigation\n\n# tiny-gpu\n\nA minimal GPU implementation in Verilog optimized for learning about how GPUs\nwork from the ground up.\n\nBuilt with <15 files of fully documented Verilog, complete documentation on\narchitecture & ISA, working matrix addition/multiplication kernels, and full\nsupport for kernel simulation & execution traces.\n\n### Table of Contents\n\n  * Overview\n  * Architecture\n\n    * GPU\n    * Memory\n    * Core\n  * ISA\n  * Execution\n\n    * Core\n    * Thread\n  * Kernels\n\n    * Matrix Addition\n    * Matrix Multiplication\n  * Simulation\n  * Advanced Functionality\n  * Next Steps\n\n# Overview\n\nIf you want to learn how a CPU works all the way from architecture to control\nsignals, there are many resources online to help you.\n\nGPUs are not the same.\n\nBecause the GPU market is so competitive, low-level technical details for all\nmodern architectures remain proprietary.\n\nWhile there are lots of resources to learn about GPU programming, there's\nalmost nothing available to learn about how GPU's work at a hardware level.\n\nThe best option is to go through open-source GPU implementations like Miaow\nand VeriGPU and try to figure out what's going on. This is challenging since\nthese projects aim at being feature complete and functional, so they're quite\ncomplex.\n\nThis is why I built tiny-gpu!\n\n## What is tiny-gpu?\n\nImportant\n\ntiny-gpu is a minimal GPU implementation optimized for learning about how GPUs\nwork from the ground up.\n\nSpecifically, with the trend toward general-purpose GPUs (GPGPUs) and ML-\naccelerators like Google's TPU, tiny-gpu focuses on highlighting the general\nprinciples of all of these architectures, rather than on the details of\ngraphics-specific hardware.\n\nWith this motivation in mind, we can simplify GPUs by cutting out the majority\nof complexity involved with building a production-grade graphics card, and\nfocus on the core elements that are critical to all of these modern\nhardwareaccelerators.\n\nThis project is primarily focused on exploring:\n\n  1. Architecture - What does the architecture of a GPU look like? What are the most important elements?\n  2. Parallelization - How is the SIMD progamming model implemented in hardware?\n  3. Memory - How does a GPU work around the constraints of limited memory bandwidth?\n\nAfter understanding the fundamentals laid out in this project, you can\ncheckout the advanced functionality section to understand some of the most\nimportant optimizations made in production grade GPUs (that are more\nchallenging to implement) which improve performance.\n\n# Architecture\n\n## GPU\n\ntiny-gpu is built to execute a single kernel at a time.\n\nIn order to launch a kernel, we need to do the following:\n\n  1. Load global program memory with the kernel code\n  2. Load data memory with the necessary data\n  3. Specify the number of threads to launch in the device control register\n  4. Launch the kernel by setting the start signal to high.\n\nThe GPU itself consists of the following units:\n\n  1. Device control register\n  2. Dispatcher\n  3. Variable number of compute cores\n  4. Memory controllers for data memory & program memory\n  5. Cache\n\n### Device Control Register\n\nThe device control register usually stores metadata specifying how kernels\nshould be executed on the GPU.\n\nIn this case, the device control register just stores the thread_count - the\ntotal number of threads to launch for the active kernel.\n\n### Dispatcher\n\nOnce a kernel is launched, the dispatcher is the unit that actually manages\nthe distribution of threads to different compute cores.\n\nThe dispatcher organizes threads into groups that can be executed in parallel\non a single core called blocks and sends these blocks off to be processed by\navailable cores.\n\nOnce all blocks have been processed, the dispatcher reports back that the\nkernel execution is done.\n\n## Memory\n\nThe GPU is built to interface with an external global memory. Here, data\nmemory and program memory are separated out for simplicity.\n\n### Global Memory\n\ntiny-gpu data memory has the following specifications:\n\n  * 8 bit addressability (256 total rows of data memory)\n  * 8 bit data (stores values of <256 for each row)\n\ntiny-gpu program memory has the following specifications:\n\n  * 8 bit addressability (256 rows of program memory)\n  * 16 bit data (each instruction is 16 bits as specified by the ISA)\n\n### Memory Controllers\n\nGlobal memory has fixed read/write bandwidth, but there may be far more\nincoming requests across all cores to access data from memory than the\nexternal memory is actually able to handle.\n\nThe memory controllers keep track of all the outgoing requests to memory from\nthe compute cores, throttle requests based on actual external memory\nbandwidth, and relay responses from external memory back to the proper\nresources.\n\nEach memory controller has a fixed number of channels based on the bandwidth\nof global memory.\n\n### Cache (WIP)\n\nThe same data is often requested from global memory by multiple cores.\nConstantly access global memory repeatedly is expensive, and since the data\nhas already been fetched once, it would be more efficient to store it on\ndevice in SRAM to be retrieved much quicker on later requests.\n\nThis is exactly what the cache is used for. Data retrieved from external\nmemory is stored in cache and can be retrieved from there on later requests,\nfreeing up memory bandwidth to be used for new data.\n\n## Core\n\nEach core has a number of compute resources, often built around a certain\nnumber of threads it can support. In order to maximize parallelization, these\nresources need to be managed optimally to maximize resource utilization.\n\nIn this simplified GPU, each core processed one block at a time, and for each\nthread in a block, the core has a dedicated ALU, LSU, PC, and register file.\nManaging the execution of thread instructions on these resources is one of the\nmost challening problems in GPUs.\n\n### Scheduler\n\nEach core has a single scheduler that manages the execution of threads.\n\nThe tiny-gpu scheduler executes instructions for a single block to completion\nbefore picking up a new block, and it executes instructions for all threads\nin-sync and sequentially.\n\nIn more advanced schedulers, techniques like pipelining are used to stream the\nexecution of multiple instructions subsequent instructions to maximize\nresource utilization before previous instructions are fully complete.\nAdditionally, warp scheduling can be use to execute multiple batches of\nthreads within a block in parallel.\n\nThe main constraint the scheduler has to work around is the latency associated\nwith loading & storing data from global memory. While most instructions can be\nexecuted synchronously, these load-store operations are asynchronous, meaning\nthe rest of the instruction execution has to be built around these long wait\ntimes.\n\n### Fetcher\n\nAsynchronously fetches the instruction at the current program counter from\nprogram memory (most should actually be fetching from cache after a single\nblock is executed).\n\n### Decoder\n\nDecodes the fetched instruction into control signals for thread execution.\n\n### Register Files\n\nEach thread has it's own dedicated set of register files. The register files\nhold the data that each thread is performing computations on, which enables\nthe same-instruction multiple-data (SIMD) pattern.\n\nImportantly, each register file contains a few read-only registers holding\ndata about the current block & thread being executed locally, enabling kernels\nto be executed with different data based on the local thread id.\n\n### ALUs\n\nDedicated arithmetic-logic unit for each thread to perform computations.\nHandles the ADD, SUB, MUL, DIV arithmetic instructions.\n\nAlso handles the CMP comparison instruction which actually outputs whether the\nresult of the difference between two registers is negative, zero or positive -\nand stores the result in the NZP register in the PC unit.\n\n### LSUs\n\nDedicated load-store unit for each thread to access global data memory.\n\nHandles the LDR & STR instructions - and handles async wait times for memory\nrequests to be processed and relayed by the memory controller.\n\n### PCs\n\nDedicated program-counter for each unit to determine the next instructions to\nexecute on each thread.\n\nBy default, the PC increments by 1 after every instruction.\n\nWith the BRnzp instruction, the NZP register checks to see if the NZP register\n(set by a previous CMP instruction) matches some case - and if it does, it\nwill branch to a specific line of program memory. This is how loops and\nconditionals are implemented.\n\nSince threads are processed in parallel, tiny-gpu assumes that all threads\n\"converge\" to the same program counter after each instruction - which is a\nnaive assumption for the sake of simplicity.\n\nIn real GPUs, individual threads can branch to different PCs, causing branch\ndivergence where a group of threads threads initially being processed together\nhas to split out into separate execution.\n\n# ISA\n\ntiny-gpu implements a simple 11 instruction ISA built to enable simple kernels\nfor proof-of-concept like matrix addition & matrix multiplication\n(implementation further down on this page).\n\nFor these purposes, it supports the following instructions:\n\n  * BRnzp - Branch instruction to jump to another line of program memory if the NZP register matches the nzp condition in the instruction.\n  * CMP - Compare the value of two registers and store the result in the NZP register to use for a later BRnzp instruction.\n  * ADD, SUB, MUL, DIV - Basic arithmetic operations to enable tensor math.\n  * LDR - Load data from global memory.\n  * STR - Store data into global memory.\n  * CONST - Load a constant value into a register.\n  * RET - Signal that the current thread has reached the end of execution.\n\nEach register is specified by 4 bits, meaning that there are 16 total\nregisters. The first 13 register R0 - R12 are free registers that support\nread/write. The last 3 registers are special read-only registers used to\nsupply the %blockIdx, %blockDim, and %threadIdx critical to SIMD.\n\n# Execution\n\n### Core\n\nEach core follows the following control flow going through different stages to\nexecute each instruction:\n\n  1. FETCH - Fetch the next instruction at current program counter from program memory.\n  2. DECODE - Decode the instruction into control signals.\n  3. REQUEST - Request data from global memory if necessary (if LDR or STR instruction).\n  4. WAIT - Wait for data from global memory if applicable.\n  5. EXECUTE - Execute any computations on data.\n  6. UPDATE - Update register files and NZP register.\n\nThe control flow is laid out like this for the sake of simplicity and\nunderstandability.\n\nIn practice, several of these steps could be compressed to be optimize\nprocessing times, and the GPU could also use pipelining to stream and\ncoordinate the execution of many instructions on a cores resources without\nwaiting for previous instructions to finish.\n\n### Thread\n\nEach thread within each core follows the above execution path to perform\ncomputations on the data in it's dedicated register file.\n\nThis resembles a standard CPU diagram, and is quite similar in functionality\nas well. The main difference is that the %blockIdx, %blockDim, and %threadIdx\nvalues lie in the read-only registers for each thread, enabling SIMD\nfunctionality.\n\n# Kernels\n\nI wrote a matrix addition and matrix multiplication kernel using my ISA as a\nproof of concept to demonstrate SIMD programming and execution with my GPU.\nThe test files in this repository are capable of fully simulating the\nexecution of these kernels on the GPU, producing data memory states and a\ncomplete execution trace.\n\n### Matrix Addition\n\nThis matrix addition kernel adds two 1 x 8 matrices by performing 8 element\nwise additions in separate threads.\n\nThis demonstration makes use of the %blockIdx, %blockDim, and %threadIdx\nregisters to show SIMD programming on this GPU. It also uses the LDR and STR\ninstructions which require async memory management.\n\nmatadd.asm\n\n    \n    \n    .threads 8 .data 0 1 2 3 4 5 6 7 ; matrix A (1 x 8) .data 0 1 2 3 4 5 6 7 ; matrix B (1 x 8) MUL R0, %blockIdx, %blockDim ADD R0, R0, %threadIdx ; i = blockIdx * blockDim + threadIdx CONST R1, #0 ; baseA (matrix A base address) CONST R2, #8 ; baseB (matrix B base address) CONST R3, #16 ; baseC (matrix C base address) ADD R4, R1, R0 ; addr(A[i]) = baseA + i LDR R4, R4 ; load A[i] from global memory ADD R5, R2, R0 ; addr(B[i]) = baseB + i LDR R5, R5 ; load B[i] from global memory ADD R6, R4, R5 ; C[i] = A[i] + B[i] ADD R7, R3, R0 ; addr(C[i]) = baseC + i STR R7, R6 ; store C[i] in global memory RET ; end of kernel\n\n### Matrix Multiplication\n\nThe matrix multiplication kernel multiplies two 2x2 matrices. It performs\nelement wise calculation of the dot product of the relevant row and column and\nuses the CMP and BRnzp instructions to demonstrate branching within the\nthreads (notably, all branches converge so this kernel works on the current\ntiny-gpu implementation).\n\nmatmul.asm\n\n    \n    \n    .threads 4 .data 1 2 3 4 ; matrix A (2 x 2) .data 1 2 3 4 ; matrix B (2 x 2) MUL R0, %blockIdx, %blockDim ADD R0, R0, %threadIdx ; i = blockIdx * blockDim + threadIdx CONST R1, #1 ; increment CONST R2, #2 ; N (matrix inner dimension) CONST R3, #0 ; baseA (matrix A base address) CONST R4, #4 ; baseB (matrix B base address) CONST R5, #8 ; baseC (matrix C base address) DIV R6, R0, R2 ; row = i // N MUL R7, R6, R2 SUB R7, R0, R7 ; col = i % N CONST R8, #0 ; acc = 0 CONST R9, #0 ; k = 0 LOOP: MUL R10, R6, R2 ADD R10, R10, R9 ADD R10, R10, R3 ; addr(A[i]) = row * N + k + baseA LDR R10, R10 ; load A[i] from global memory MUL R11, R9, R2 ADD R11, R11, R7 ADD R11, R11, R4 ; addr(B[i]) = k * N + col + baseB LDR R11, R11 ; load B[i] from global memory MUL R12, R10, R11 ADD R8, R8, R12 ; acc = acc + A[i] * B[i] ADD R9, R9, R1 ; increment k CMP R9, R2 BRn LOOP ; loop while k < N ADD R9, R5, R0 ; addr(C[i]) = baseC + i STR R9, R8 ; store C[i] in global memory RET ; end of kernel\n\n# Simulation\n\ntiny-gpu is setup to simulate the execution of both of the above kernels.\nBefore simulating, you'll need to install iverilog and cocotb.\n\nOnce you've installed the pre-requisites, you can run the kernel simulations\nwith make test_matadd and make test_matmul.\n\nExecuting the simulations will output a log file in test/logs with the initial\ndata memory state, complete execution trace of the kernel, and final data\nmemory state.\n\nIf you look at the initial data memory state logged at the start of the\nlogfile for each, you should see the two start matrices for the calculation,\nand in the final data memory at the end of the file you should also see the\nresultant matrix.\n\nBelow is a sample of the execution traces, showing on each cycle the execution\nof every thread within every core, including the current instruction, PC,\nregister values, states, etc.\n\nFor anyone trying to run the simulation or play with this repo, please feel\nfree to DM me on twitter if you run into any issues - I want you to get this\nrunning!\n\n# Advanced Functionality\n\nFor the sake of simplicity, there were many additional features implemented in\nmodern GPUs that heavily improve performance & functionality that tiny-gpu\nomits. We'll discuss some of those most critical features in this section.\n\n### Multi-layered Cache & Shared Memory\n\nIn modern GPUs, multiple different levels of caches are used to minimize the\namount of data that needs to get accessed from global memory. tiny-gpu\nimplements only one cache layer between individual compute units requesting\nmemory and the memory controllers which stores recent cached data.\n\nImplementing multi-layered caches allows frequently accessed data to be cached\nmore locally to where it's being used (with some caches within individual\ncompute cores), minimizing load times for this data.\n\nDifferent caching algorithms are used to maximize cache-hits - this is a\ncritical dimension that can be improved on to optimize memory access.\n\nAdditionally, GPUs often use shared memory for threads within the same block\nto access a single memory space that can be used to share results with other\nthreads.\n\n### Memory Coalescing\n\nAnother critical memory optimization used by GPUs is memory coalescing.\nMultiple threads running in parallel often need to access sequential addresses\nin memory (for example, a group of threads accessing neighboring elements in a\nmatrix) - but each of these memory requests is put in separately.\n\nMemory coalescing is used to analyzing queued memory requests and combine\nneighboring requests into a single transaction, minimizing time spent on\naddressing, and making all the requests together.\n\n### Pipelining\n\nIn the control flow for tiny-gpu, cores wait for one instruction to be\nexecuted on a group of threads before starting execution of the next\ninstruction.\n\nModern GPUs use pipelining to stream execution of multiple sequential\ninstructions at once while ensuring that instructions with dependencies on\neach other still get executed sequentially.\n\nThis helps to maximize resource utilization within cores as resources are not\nsitting idle while waiting (ex: during async memory requests).\n\n### Warp Scheduling\n\nAnother strategy used to maximize resource utilization on course is warp\nscheduling. This approach involves breaking up blocks into individual batches\nof theads that can be executed together.\n\nMultiple warps can be executed on a single core simultaneously by executing\ninstructions from one warp while another warp is waiting. This is similar to\npipelining, but dealing with instructions from different threads.\n\n### Branch Divergence\n\ntiny-gpu assumes that all threads in a single batch end up on the same PC\nafter each instruction, meaning that threads can be executed in parallel for\ntheir entire lifetime.\n\nIn reality, individual threads could diverge from each other and branch to\ndifferent lines based on their data. With different PCs, these threads would\nneed to split into separate lines of execution, which requires managing\ndiverging threads & paying attention to when threads converge again.\n\n### Synchronization & Barriers\n\nAnother core functionality of modern GPUs is the ability to set barriers so\nthat groups of threads in a block can synchronize and wait until all other\nthreads in the same block have gotten to a certain point before continuing\nexecution.\n\nThis is useful for cases where threads need to exchange shared data with each\nother so they can ensure that the data has been fully processed.\n\n# Next Steps\n\nUpdates I want to make in the future to improve the design, anyone else is\nwelcome to contribute as well:\n\n  * Add a simple cache for instructions\n  * Build an adapter to use GPU with Tiny Tapeout 7\n  * Add basic branch divergence\n  * Add basic memory coalescing\n  * Add basic pipelining\n  * Optimize control flow and use of registers to improve cycle time\n  * Write a basic graphics kernel or add simple graphics hardware to demonstrate graphics functionality\n\nFor anyone curious to play around or make a contribution, feel free to put up\na PR with any improvements you'd like to add \ud83d\ude04\n\n## About\n\nA minimal GPU design in Verilog to learn how GPUs work from the ground up\n\n### Resources\n\nReadme\n\nActivity\n\n### Stars\n\n147 stars\n\n### Watchers\n\n3 watching\n\n### Forks\n\n13 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Languages\n\n  * SystemVerilog 74.3%\n  * Python 25.0%\n  * Makefile 0.7%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": true}
