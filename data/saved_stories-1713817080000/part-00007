{"aid": "40113883", "title": "Automated Stitching of Chip Images", "url": "https://www.bunniestudios.com/blog/2024/automated-stitching-of-chip-images/", "domain": "bunniestudios.com", "votes": 2, "user": "etiam", "posted_at": "2024-04-22 12:52:25", "comments": 0, "source_title": "Automated Stitching of Chip Images \u00ab bunnie's blog", "source_text": "Automated Stitching of Chip Images \u00ab bunnie's blog\n\n\u00ab Control and Autofocus Software for Chip-Level Microscopy\n\n## Automated Stitching of Chip Images\n\nThis is the final post in a series about non-destructively inspecting chips\nwith the IRIS (Infra-Red, in-situ) technique. Here are links to previous\nposts:\n\n  * IRIS project overview\n  * Methodology\n  * Light source electronics\n  * Fine focus stage\n  * Light source mechanisms\n  * Machine control & focus software\n\nThis post will cover the software used to stitch together smaller images\ngenerated by the control software into a single large image. My IRIS machine\nwith a 10x objective generates single images that correspond to a patch of\nsilicon that is only 0.8mm wide. Most chips are much larger than that, so I\ntake a series of overlapping images that must be stitched together to generate\na composite image corresponding to a full chip.\n\nThe un-aligned image tiles look like this:\n\nAnd the stitching software assembles it into something like this:\n\nThe problem we have to solve is that even though we command the microscope to\nmove to regularly spaced intervals, in reality, there is always some error in\nthe positioning of the microscope. The accuracy is on the order of 10\u2019s of\nmicrons at best, but we are interested in extracting features much smaller\nthan that. Thus, we must rely on some computational methods to remove these\nerror offsets.\n\nAt first one might think, \u201cthis is easy, just throw it into any number of\nimage stitching programs used to generate panoramas!\u201d. I thought that too.\n\nHowever, it turns out these programs perform poorly on images of chips. The\nmost significant challenge is that chip features tend to be large, repetitive\narrays. Most panorama algorithms rely on a step of \u201cfeature extraction\u201d where\nit uses some algorithms to decide what\u2019s an \u201cinteresting\u201d feature and line\nthem up between two images. These algorithms are tuned for aesthetically\npleasing results on images of natural subjects, like humans or outdoor\nscenery; they get pretty lost trying to make heads or tails out of the\ngeometrically regular patterns in a chip image. Furthermore, the alignment\naccuracy requirement for an image panorama is not as strict as what we need\nfor IRIS. Most panaroma stitchers rely on a later pass of seam-blending to\niron out deviations of a few pixels, yielding aesthetic results despite the\nmisalignments.\n\nUnfortunately, we\u2019re looking to post-process these images with an image\nclassifier to perform a gate count census, and so we need pixel-accurate\nalignment wherever possible. On the other hand, because all of the images are\ntaken by machine, we never have to worry about rotational or scale adjustments\n\u2013 we are only interested in correcting translational errors.\n\nThus, I ended up rolling my own stitching algorithm. This was yet another one\nof those projects that started out as a test program to check data quality,\nand suffered from \u201cjust one more feature\u201d-itis until it blossomed into the\nheaping pile that it is today. I wouldn\u2019t be surprised if there were already\ngood quality chip stitching programs out there, but, I did need a few bespoke\nfeatures and it was interesting enough to learn how to do this, so I ended up\nwriting it from scratch.\n\nWell, to be accurate, I copy/pasted lots of stackoverflow answers, LLM-\ngenerated snippets, and boilerplate from previous projects together with heaps\nof glue code, which I think qualifies as \u201cwriting original code\u201d these days?\nMaybe the more accurate way to state it is, \u201cI didn\u2019t fork another program as\na starting point\u201d. I started with an actual empty text buffer before I started\ncopy-pasting five-to-twenty line code snippets into it.\n\nSizing Up the Task\n\nA modestly sized chip of a couple dozen square millimeters generates a dataset\nof a few hundred images, each around 2.8MiB in size, for a total dataset of a\ncouple gigabytes. While not outright daunting, it\u2019s enough data that I can\u2019t\nbe reckless, yet small enough that I can get away with lazy decisions, such as\nusing the native filesystem as my database format.\n\nIt turns out that for my application, the native file system is a performant,\ninter-operable, multi-threaded, transparently memory caching database format.\nAlso super-easy to make backups and to browse the records. As a slight\noptimization, I generate thumbnails of every image on the first run of the\nstitching program to accelerate later drawing operations for preview images.\n\nEach file\u2019s name is coded with its theoretical absolute position on the chip,\nalong with metadata describing the focus and lighting parameters, so each file\nhas a name something like this:\n\nx0.00_y0.30_z10.00_p6869_i3966_t127_j4096_u127_a0.0_r1_f8.5_s13374_v0.998_d1.5_o100.0.png\n\nIt\u2019s basically an underscore separated list of metadata, where each element is\ntagged with a single ASCII character, followed by its value. It\u2019s a little\nawkward, but functional and easy enough to migrate as I upgrade schemas.\n\nCreating a Schema\n\nAll of the filenames are collated into a single Python object that tracks the\ntransformations we do on the data, as well as maintains a running log of all\nthe operations (allowing us to have an undo buffer). I call this the \u201cSchema\u201d\nobject. I wish I knew about dataframes before I started this project, because\nI ended up re-implementing a lot of dataframe features in the course of\nbuilding the Schema. Oh well.\n\nThe Schema object is serialized into a JSON file called \u201cdb.json\u201d that allows\nus to restore the state of the program even in the case of an unclean shutdown\n(and there are plenty of those!).\n\nThe initial state of the program is to show a preview of all the images in\ntheir current positions, along with a set of buttons that control the state of\nthe stitcher, select what regions to stitch/restitch, debugging tools, and\nfile save operations. The UI framework is a mix of PyQt and OpenCV\u2019s native UI\nfunctions (which afaik wrap PyQt objects).\n\nAbove: screenshot of the stitching UI in its initial state.\n\nAt startup, all of the thumbnails are read into memory, but none of the large\nimages. There\u2019s an option to cache the images in RAM as they are pulled in for\nprocessing. Generally, I\u2019ve had no trouble just pulling all the images into\nRAM because the datasets haven\u2019t exceeded 10GiB, but I suppose once I start\nstitching really huge images, I may need to do something different.\n\n...Or maybe I just buy a bigger computer? Is that cheating? Extra stick of RAM\nis the hundred-dollar problem solver! Until it isn\u2019t, I suppose. But, the good\nnews is there\u2019s a strong upper bound of how big of an image we\u2019d stitch (e.g.\nchips rarely go larger than the reticle size) and it\u2019s probably around 100GiB,\nwhich somehow seems \u201creasonable\u201d for an amount of RAM to put in one desktop\nmachine these days.\n\nAgain, my mind boggles, because I spend most of my time writing Rust code for\na device with 16MiB of RAM.\n\nAuto Stitching Flow\n\nAt the highest level, the stitching strategy uses a progressive stitch,\nstarting from the top left tile and doing a \u201clawn mower\u201d pattern. Every tile\nlooks \u201cleft and up\u201d for alignment candidates, so the very top left tile is\nconsidered to be the anchor tile. This pattern matches the order in which the\nimages were taken, so the relative error between adjacent tiles is minimized.\n\nBefore lawn mowing, a manually-guided stitch pass is done along the left and\ntop edges of the chip. This usually takes a few minutes, where the algorithm\nruns in \u201csingle step\u201d mode and the user reviews and approves of each alignment\nindividually. The reason this is done is if there are any stitching errors on\nthe top or left edge, it will propagate throughout the process, so these edges\nmust be 100% correct before the algorithm can run unattended. It is also the\ncase that the edges of a chip can be quite tricky to stitch, because arrays of\nbond pads can look identical across multiple frames, and accurate alignment\nends up relying upon random image artifacts caused by roughness in the die\u2019s\nphysical edges.\n\nOnce the left and top edges are fixed, the algorithm can start in earnest. For\neach tile, it starts with a \u201cguess\u201d of where the new tile should go based on\nthe nominal commanded values of the microscope. It then looks \u201cup\u201d and \u201cleft\u201d\nand picks the tile that has the largest overlapping region for the next step.\n\nAbove is an example of the algorithm picking a tile in the \u201cup\u201d direction as\nthe \u201cREF\u201d (reference) tile with which to stitch the incoming tile (referred to\nas \u201cSAMPLE\u201d). The image above juxtaposes both tiles with no attempt to align\nthem, but you can already see how the top of the lower image partially\noverlaps with the bottom of the upper image.\n\nTemplate Matching\n\nNext, the algorithm picks a \u201ctemplate\u201d to do template matching. Template\nmatching is an effective way to align two images that are already in the same\norientation and scale. The basic idea is to pick a \u201cunique\u201d feature in one\nimage, and convolve it with every point in the other image. The point with the\nhighest convolution value is probably going to be the spot where the two\nimages line up.\n\nAbove: an example of a template region automatically chosen for searching\nacross the incoming sample for alignment.\n\nIn reality, the algorithm is slightly more complicated than this, because the\nquality of the match greatly depends on the quality of the template. Thus we\nfirst have to answer the question of what template to use, before we get to\nwhere the template matches. This is especially true on chips, because there\nare often large, repeated regions that are impossible to uniquely match, and\nthere is no general rule that can guarantee where a unique feature might end\nup within a frame.\n\nThus, the actual implementation also searches for the \u201cbest\u201d template using\nbrute-force: it divides the nominally overlapping region into potential\ntemplate candidates, and computes the template match score for all of them,\nand picks the template that produces the best alignment of all the candidates.\nThis is perhaps the most computationally intensive step in the whole stitching\nprocess, because we can have dozens of potential template candidates, each of\nwhich must be convolved over many of the points in the reference image.\nComputed sequentially on my desktop computer, the search can take several\nseconds per tile to find the optimal template. However, Python makes it pretty\neasy to spawn threads, so I spawn one thread per candidate template and let\nthem duke it out for CPU time and cache space. Fortunately I have a Ryzen\n7900X, so with 12 cores and 12MiB of L2 cache, the entire problem basically\nfits entirely inside the CPU, and the multi-threaded search completes in a\nblink of the eye.\n\nThis is another one of those moments where I feel kind of ridiculous writing\ncode like this, but somehow, it\u2019s a reasonable thing to do today.\n\nThe other \u201csmall asterisk\u201d on the whole process is that it works not on the\noriginal image, but it works on a Gaussian-filtered, Laplacian-transformed\nversion of the images. In other words, instead of matching against the\ncontinuous tones of an image, I do the template match against the edges of the\nimage, making the algorithm less sensitive to artifacts such as lens flare, or\nglobal brightness gradients.\n\nAbove is an example of the output of the template matching algorithm. Most of\nthe region is gray, which indicates a poor match. Towards the right, you start\nto see \u201cripples\u201d that correspond to the matching features starting to line up.\nAs part of the algorithm, I extract contours to ring regions with a high\nmatch, and pick the center of the largest matching region, highlighted here\nwith the pink arrow. The whole contour extraction and center picking thing is\na native library in OpenCV with pretty good documentation examples.\n\nMinimum Squared Error (MSE) Cleanup\n\nTemplate matching usually gets me a solution that aligns images to within a\ncouple of pixels, but I need every pixel I can get out of the alignment,\nespecially if my plan is to do a gate count census on features that are just a\nfew pixels across. So, after template alignment, I do a \u201ccleanup\u201d pass using a\nminimum squared error (MSE) method.\n\nAbove: example of the MSE debugging output. This illustrates a \u201cgood match\u201d,\nbecause most of the image is gray, indicating a small MSE. A poor match would\nhave more image contrast.\n\nMSE basically takes every pixel in the reference image and subtracts it from\nthe sample image, squares it, and sums all of them together. If the two images\nwere identical and exactly aligned, the error would be zero, but because the\nimages are taken with a real camera that has noise, we can only go as low as\nthe noise floor. The cleanup pass starts with the initial alignment proposed\nby the template matching, and computes the MSE of the current alignment, along\nwith candidates for the image shifted one pixel up, left, right and down. If\nany of the shifted candidates have a lower error, the algorithm picks that as\nthe new alignment, and repeats until it finds an alignment where the center\npixel has the lowest MSE. To speed things up, the MSE is actually done at two\nlevels of shifting, first with a coarse search consisting of several pixels,\nand finally with a fine-grained search at a single pixel level. There is also\na heuristic to terminate the search after too many steps, because the\nalgorithm is subject to limit cycles.\n\nBecause each step of the search depends upon results from the previous step,\nit doesn\u2019t parallelize as well, and so sometimes the MSE search can take\nlonger than the multi-threaded template matching search, especially when the\ntemplate search really blew it and we end up having to search over a dozens of\npixels to find the true alignment (but if the template matching did it\u2019s job,\nthe MSE cleanup pass is barely noticeable).\n\nAgain, the MSE search works on the Gaussian-filtered Laplacian view of the\nimage, i.e., it\u2019s looking at edges, not whole tones.\n\nAfter template matching and MSE cleanup, the final alignment goes through some\nbasic sanity checks, and if all looks good, moves on to the next tile. If\nsomething doesn\u2019t look right \u2013 for example, the proposed offsets for the\nimages are much larger than usual, or the template matcher found too many good\nsolutions (as is the case on stitching together very regular arrays like RAM)\n\u2013 the algorithm stops and the user can manually select a new template and/or\nmove the images around to find the best MSE fit. This will usually happen a\ncouple of times per chip, but can be more frequent if there were focusing\nproblems or the chip has many large, regular arrays of components.\n\nAbove: the automatically proposed stitching alignment of the two images in\nthis example. The bright area is the overlapping region between the two\nadjacent tiles. Note how there is a slight left-right offset that the\nalgorithm detected and compensated for.\n\nOnce the stitching is all finished, you end up with a result that looks a bit\nlike this:\n\nHere, all the image tiles are properly aligned, and you can see how the\nJubilee machine (Jubilee is the motion platform on which IRIS was built) has a\nslight \u201cwalk off\u201d as evidenced by the diagonal pattern across the bottom of\nthe preview area.\n\nPotential Hardware Improvements\n\nThe Jubilee uses a CoreXY belt path, which optimizes for minimum flying mass.\nThe original designers of the Jubilee platform wanted it to perform well in 3D\nprinting applications, where print speed is limited by how fast the tool can\nmove. However, any mismatch in belt tension leads to the sort of \u201cwalk off\u201d\nvisible here. I basically need to re-tension the machine every couple of weeks\nto minimize this effect, but I\u2019m told that this isn\u2019t typical. It\u2019s possible\nthat I might have defective belts or more likely, sloppy assembly technique;\nor I live in the tropics and the room has 60% relative humidity even with air\nconditioning, causing the belts to expand slightly over time as they absorb\nmoisture. Or it could be that the mass of the microscope is pretty enormous,\nand that amplifies the effect of slight mismatches in tensioning.\n\nRegardless of the root cause, the Jubilee\u2019s design intent of performing well\nin 3D printing applications incurs some trade-off in terms of maintenance\nlevel required to sustain absolute accuracy. Since in the IRIS application,\nmicroscope head speed is not important, tool mass is already huge, and\nprecision is paramount, one of the mods I\u2019m considering for my version of the\nplatform is redoing the belt layout so that the drive is Cartesian instead of\nCoreXY. That should help minimize the walk-off and reduce the amount of\nmaintenance needed to keep it running in top-notch condition.\n\nEdge Blending for Aesthetics\n\nYou\u2019ll note that in the above image the overlap of the individual tiles is\nreadily apparent, due to slight variations in brightness across the imaging\nfield. This can probably be improved by adding some diffusers, and also\nimproving the alignment of the lights relative to the focal point (it\u2019s\ncurrently off by a couple of millimeters, because I designed it around the\nfocal point of a 20x objective, but these images were taken with a 10x\nobjective). Even then, I suspect some amount of tiling will always be visible,\nbecause the human eye is pretty sensitive to slight variations in shades of\ngray.\n\nMy working hypothesis is that the machine learning driven standard cell census\n(yet to be implemented!) will not care so much about the gradient because it\nonly ever looks at regions a few dozen pixels across in one go. However, in\norder to generate a more aesthetically pleasing image for human consumption, I\nimplemented a blending algorithm to smooth out the edges, which results in a\nfinal image more like this:\n\nClick the image to browse a full resolution version, hosted on siliconpr0n.\n\nThere\u2019s still four major stitch regions visible, and this is because OpenCV\u2019s\nMultiBandBlender routine seems to be limited to handle 8GiB-ish of raw image\ndata at once, so I can\u2019t quite blend whole chips in a single go. I tried\nrunning the same code on a machine with a 24GiB graphics card, and got the\nsame out of memory error, so the limit isn\u2019t total GPU memory. When I dug in a\nbit, it seemed like there was some driver-level limitation related to the\nmaximum number of pointers to image buffers that I was hitting, and I didn\u2019t\nfeel like shaving that yak.\n\nThe underlying algorithm used to do image blending is actually pretty neat,\nand based off a paper from 1983(!) by Burt and Adelson titled \u201cA\nMultiresolution Spline with Application to Image Mosaics\u201d. I actually tried\nimplementing this directly using OpenCV\u2019s Image Pyramid feature, mainly\nbecause I couldn\u2019t find any documentation on the MultiBandBlender routine. It\nwas actually pretty fun and insightful to play around with image pyramids;\nit\u2019s a useful idiom for extracting image features at vastly different scales,\nand for all its utility it\u2019s pretty memory efficient (the full image pyramid\nconsumes about 1.5x of the original image\u2019s memory).\n\nHowever, it turns out that the 1983 paper doesn\u2019t tell you how to deal with\nthings like non power of 2 images, non-square images, or images that only\npartially overlap...and I couldn\u2019t find any follow-up papers that goes into\nthese \u201cedge cases\u201d. Since the blending is purely for aesthetic appeal to human\neyes, I decided not to invest the effort to chase down these last details, and\nsettled for the MultiBandBlender, stitch lines and all.\n\nTouch-Ups\n\nThe autostitching algorithm isn\u2019t perfect, so I also implemented an interface\nfor doing touch-ups after the initial stitching pass is done. The interface\nallows me to do things like flag various tiles for manual review,\nautomatically re-stitch regions, and visualize heat maps of MSE and focus\nshifts.\n\nThe above video is a whistlestop tour of the stitching and touch-up interface.\n\nAll of the code discussed in this blog post can be found in the iris-stitcher\nrepo on github. stitch.py contains the entry point for the code.\n\nThat\u2019s a Wrap!\n\nThat\u2019s it for my blog series on IRIS, for now. As of today, the machine and\nassociated software is capable of reliably extracting reference images of\nchips and assembling them into full-chip die shots. The next step is to train\nsome CNN classifiers to automatically recognize logic cells and perform a\ncensus of the number of gates in a given region.\n\nSomeday, I also hope to figure out a way to place rigorous bounds on the\namount of logic that could be required to pass an electrical scan chain test\nwhile also hiding malicious Hardware Trojans. Ideally, this would result in\nsome sort of EDA tool that one can use to insert an IRIS-hardened scan chain\ninto an existing HDL design. The resulting fusion of design methodology, non-\ndestructive imaging, and in-circuit scan chain testing may ultimately give us\na path towards confidence in the construction of our chips.\n\nAnd as always, a big shout-out to NLnet and to my Github Sponsors for allowing\nme to do all this research while making it openly accessible for anyone to\nreplicate and to use.\n\nThis entry was posted on Monday, April 22nd, 2024 at 6:56 pm and is filed\nunder IRIS. You can follow any responses to this entry through the RSS 2.0\nfeed. You can leave a response, or trackback from your own site.\n\n### Leave a Reply\n\nClick here to cancel reply.\n\nbunnie's blog is proudly powered by WordPress Entries (RSS) and Comments\n(RSS).\n\n", "frontpage": false}
