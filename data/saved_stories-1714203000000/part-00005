{"aid": "40174087", "title": "Building an open data pipeline in 2024", "url": "https://blog.twingdata.com/p/building-an-open-data-pipeline-in", "domain": "twingdata.com", "votes": 12, "user": "dangoldin", "posted_at": "2024-04-26 20:47:38", "comments": 0, "source_title": "Building an open data pipeline in 2024", "source_text": "Building an open data pipeline in 2024 - by Dan Goldin\n\n# Twing Data\n\nShare this post\n\n#### Building an open data pipeline in 2024\n\nblog.twingdata.com\n\n#### Discover more from Twing Data\n\nNewsletter from Twing Data which will contain product and feature\nannouncements as well as thoughts on the data world.\n\nContinue reading\n\nSign in\n\n# Building an open data pipeline in 2024\n\n### Using Iceberg allows us to pick the optimal \"big data\" compute environment\nfor the specific requirements we have. There's no need to limit yourself to a\nsingle solution.\n\nDan Goldin\n\nApr 26, 2024\n\nShare this post\n\n#### Building an open data pipeline in 2024\n\nblog.twingdata.com\n\nShare\n\nIn a previous post on the commoditization of SQL, I touched on the concept of\nbuilding a data stack that harnesses this approach. One key element of this\narchitecture involves utilizing Iceberg as the core data storage layer, with\nthe flexibility to choose the most suitable compute environment depending on\nthe specific needs of your use case. For instance, if you\u2019re powering BI\nreporting you\u2019ll often want to prioritize speed so your customers can quickly\nget the data they need. Conversely, cost may be the biggest factor if you\u2019re\nworking on a batch data pipeline job. Additionally, you may have external data\ncustomers in which case you want to prioritize availability over everything\nelse. While there is no one-size-fits-all solution, understanding your unique\nuse cases and requirements allows you to tailor your approach accordingly.\n\n##\n\nUnderstand your requirements\n\nThere are a few dimensions here and I want to go through each of them\nseparately although in practice it\u2019s going to be a combination of factors.\n\n###\n\nData Scale\n\nWe live in a world of \u201cbig data\u201d but even within big data we have entirely\ndifferent tiers. Smaller data sets can be handled using DuckDB on a single\nnode instance. Larger data sets require some sort of map-reduce approach and\ncan leverage modern data warehouses, such as Snowflake or Databricks. And if\nyou\u2019re dealing with truly massive datasets you can take advantage of GPUs for\nyour data jobs.\n\n###\n\nLatency\n\nThere are two types of latency to consider. The first type is data processing,\nwhich refers to the time required to transform and process the data. The\nsecond type is the speed at which queries can be executed in interactive or\nreporting use cases. Imagine you have a service that collects e-commerce\ntransaction data to help customers understand their sales better. You receive\nan event for every transaction but given the scale you choose to aggregate the\ndata hourly and the aggregation takes 10 minutes to run. When the data is\naggregated users are then able to query it via a BI tool and 95% of queries\nexecute within 5 seconds. In this case, your data processing latency is at\nmost 70 minutes but your query latency is a few seconds.\n\nYou can improve data processing latency by running the jobs more frequently or\non larger hardware. To improve query latency you can have larger hardware and\nkeep as much data \u201chot\u201d and in memory rather than pulling it from \u201ccold\u201d\nstorage. As with most things, reducing latency will cost you more.\n\n###\n\nGovernance and access controls\n\nIn general, managed tools will give you stronger governance and access\ncontrols compared to open source solutions. For businesses dealing with\nsensitive data that requires a robust security model, commercial solutions may\nbe worth investing in, as they can provide an added layer of reassurance and a\nstronger audit trail.\n\nDifferent types of data will often have different requirements. Typically, raw\nevents often deal with personally identifiable information and have high\nsensitivity. However, as the data is rolled up and aggregated, the sensitivity\nlevel tends to decrease, allowing for more defined roles and responsibilities\naround the data sensitivity levels.\n\n###\n\nCost\n\nCost is a function of the above but is also a key dimension on its own.\nDepending on the economics of your business cost may actually be the\nconstraint that forces you to compromise on some of the other requirements. It\ncan be useful to think of the ideal architecture where cost is not a factor\nand then keep stripping and compromising as you layer on more and more cost\nconsiderations. That also gives you a good sense of your priorities and how to\ntradeoff between them. For instance, you might realize that achieving sub-\nsecond query latency is less crucial than establishing a robust security\nmodel.\n\nA helpful approach when thinking about cost is asking the question of what it\nwould look like if you had twice the budget? What about half the budget? That\ncan inspire some creative thinking and lead to solutions that weren\u2019t obvious\nat first.\n\n##\n\nPutting it all together\n\nBefore starting Twing Data, I gained valuable experience working at\nTripleLift, an advertising technology company, where I held several\nengineering leadership positions over the course of 10 years. During my time\nthere, we went through multiple iterations of a data pipeline. Drawing from\nthat experience, I wanted to utilize the framework mentioned above to create\nan architecture that is specifically tailored to accommodate diverse\nrequirements while leveraging Iceberg and the different compute environments\navailable.\n\nHere\u2019s a breakdown of how it fits together.\n\n  1. Log-level events are collected in Redpanda and persisted to Cloudflare R2 (better than AWS S3) using Parquet and Iceberg. There are hundreds of billions of events each day and range from information about the advertising auctions being run, to the winners of each auction, to collecting information around whether the ad was viewed and for how long.\n\n  2. Data modeling is done using SQLMesh and orchestrated in dagster. Complex transformations that require distributed work with large-scale joins run in Snowflake. One example is joining all the events from the first step in order to create a wide table where each row represents everything we know about a single auction.\n\n  3. DuckDB is used for simpler jobs that can run on a single node. For example, taking the wide and large table from the second step and coming up with smaller tables that have a subset of the dimensions and are highly aggregated.\n\n  4. Cube is used as a semantic layer to give us a standard way of defining the metrics we care about and ensuring they are consistent across multiple access paths.\n\n  5. One access path is Metabase which acts as our BI tool. By going through Cube we can ensure we use standard definitions and take advantage of Cube\u2019s pre-aggregation/caching layer.\n\n  6. Some of our engineers use Python directly and can also take advantage of the semantic layer offered by Cube.\n\n  7. We also want to allow power users, such as data scientists, the ability to query the data directly from R2 using whatever compute layer they want. They may want to use Spark or even Snowflake but they\u2019re able to because the data is stored using an open storage format, Iceberg.\n\nThis approach is centered around Iceberg and its open nature. In the above\nexample we can switch Snowflake with Databricks without any trouble. Moreover,\nwe have the flexibility to adopt different orchestration, data modeling, and\nsemantic layers as needed. The foundation of this flexibility lies in the fact\nthat the core of the system, the data itself, is not confined to a proprietary\nformat. This not only leads to cost-effectiveness but also fosters innovation\nas both you and the ecosystem expand and evolve.\n\n### Subscribe to Twing Data\n\nBy Dan Goldin \u00b7 Launched 6 months ago\n\nNewsletter from Twing Data which will contain product and feature\nannouncements as well as thoughts on the data world.\n\nShare this post\n\n#### Building an open data pipeline in 2024\n\nblog.twingdata.com\n\nShare\n\nComments\n\nReady for more?\n\n\u00a9 2024 Twing Data, Inc\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": true}
