{"aid": "40210421", "title": "The Absolute Minimum Every Software Developer Must Know About Unicode (2003)", "url": "https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/", "domain": "joelonsoftware.com", "votes": 2, "user": "tosh", "posted_at": "2024-04-30 12:43:28", "comments": 0, "source_title": "The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)", "source_text": "The Absolute Minimum Every Software Developer Absolutely, Positively Must Know\nAbout Unicode and Character Sets (No Excuses!) \u2013 Joel on Software\n\nSkip to content\n\n  * View menu\n  * View sidebar\n\n# Joel on Software\n\n## Your host\n\nI\u2019m Joel Spolsky, a software developer in New York City. More about me.\n\nRead the archives in dead-tree format! Many of these articles have been\ncollected into four books, available at your favorite bookstore. It\u2019s an\nexcellent way to read the site in the bath, or throw it at your boss.\n\n## Careers\n\nReady to level up? Stack Overflow Jobs is the job site that puts the needs of\ndevelopers first. Whether you want to take control of your search or let\nemployers discover you, we\u2019re on a mission to help every developer find a job\nthey love.\n\nLooking to hire smart programmers who get things done? Stack Overflow Talent\nis a fully-customized sourcing solution that helps you understand, reach, and\nattract developers on the platform they trust most. Find the right candidates\nfor your jobs. Learn more.\n\nFor my day job, I'm the co-founder and CEO of Stack Overflow, the largest\nonline community for programmers to learn, share their knowledge, and level\nup. Each month, more than 40 million professional and aspiring programmers\nvisit Stack Overflow to ask and answer questions and find better jobs. Stack\nOverflow is also the flagship site of the Stack Exchange network, 160+\nquestion and answer sites dedicated to all kinds of topics from cooking to\ngaming. According to Quantcast, Stack Overflow is the 30th largest web\nproperty in the United States and in the top 100 in the world.\n\nI also founded Fog Creek Software, one of the most influential small tech\ncompanies in the world. As an independent, privately-owned company, we\u2019ve been\nmaking customers happy since the turn of the century. We share what we've\nlearned about how to make great software, both by writing about our ideas and\nby creating products, like FogBugz, Trello and Gomix, that help others make\ngreat technology. As a result, Fog Creek's impact on the world of developers\nrivals companies a thousand times our size.\n\n## Twitter! Twitter!\n\nMy Tweets\n\nOctober 8, 2003November 17, 2017 by Joel Spolsky\n\n# The Absolute Minimum Every Software Developer Absolutely, Positively Must\nKnow About Unicode and Character Sets (No Excuses!)\n\n  * Top 10, New developer, News\n\nEver wonder about that mysterious Content-Type tag? You know, the one you\u2019re\nsupposed to put in HTML and you never quite know what it should be?\n\nDid you ever get an email from your friends in Bulgaria with the subject line\n\u201c???? ?????? ??? ????\u201d?\n\nI\u2019ve been dismayed to discover just how many software developers aren\u2019t really\ncompletely up to speed on the mysterious world of character sets, encodings,\nUnicode, all that stuff. A couple of years ago, a beta tester for FogBUGZ was\nwondering whether it could handle incoming email in Japanese. Japanese? They\nhave email in Japanese? I had no idea. When I looked closely at the commercial\nActiveX control we were using to parse MIME email messages, we discovered it\nwas doing exactly the wrong thing with character sets, so we actually had to\nwrite heroic code to undo the wrong conversion it had done and redo it\ncorrectly. When I looked into another commercial library, it, too, had a\ncompletely broken character code implementation. I corresponded with the\ndeveloper of that package and he sort of thought they \u201ccouldn\u2019t do anything\nabout it.\u201d Like many programmers, he just wished it would all blow over\nsomehow.\n\nBut it won\u2019t. When I discovered that the popular web development tool PHP has\nalmost complete ignorance of character encoding issues, blithely using 8 bits\nfor characters, making it darn near impossible to develop good international\nweb applications, I thought, enough is enough.\n\nSo I have an announcement to make: if you are a programmer working in 2003 and\nyou don\u2019t know the basics of characters, character sets, encodings, and\nUnicode, and I catch you, I\u2019m going to punish you by making you peel onions\nfor 6 months in a submarine. I swear I will.\n\nAnd one more thing:\n\nIT\u2019S NOT THAT HARD.\n\nIn this article I\u2019ll fill you in on exactly what every working programmer\nshould know. All that stuff about \u201cplain text = ascii = characters are 8 bits\u201d\nis not only wrong, it\u2019s hopelessly wrong, and if you\u2019re still programming that\nway, you\u2019re not much better than a medical doctor who doesn\u2019t believe in\ngerms. Please do not write another line of code until you finish reading this\narticle.\n\nBefore I get started, I should warn you that if you are one of those rare\npeople who knows about internationalization, you are going to find my entire\ndiscussion a little bit oversimplified. I\u2019m really just trying to set a\nminimum bar here so that everyone can understand what\u2019s going on and can write\ncode that has a hope of working with text in any language other than the\nsubset of English that doesn\u2019t include words with accents. And I should warn\nyou that character handling is only a tiny portion of what it takes to create\nsoftware that works internationally, but I can only write about one thing at a\ntime so today it\u2019s character sets.\n\nA Historical Perspective\n\nThe easiest way to understand this stuff is to go chronologically.\n\nYou probably think I\u2019m going to talk about very old character sets like EBCDIC\nhere. Well, I won\u2019t. EBCDIC is not relevant to your life. We don\u2019t have to go\nthat far back in time.\n\nBack in the semi-olden days, when Unix was being invented and K&R were writing\nThe C Programming Language, everything was very simple. EBCDIC was on its way\nout. The only characters that mattered were good old unaccented English\nletters, and we had a code for them called ASCII which was able to represent\nevery character using a number between 32 and 127. Space was 32, the letter\n\u201cA\u201d was 65, etc. This could conveniently be stored in 7 bits. Most computers\nin those days were using 8-bit bytes, so not only could you store every\npossible ASCII character, but you had a whole bit to spare, which, if you were\nwicked, you could use for your own devious purposes: the dim bulbs at WordStar\nactually turned on the high bit to indicate the last letter in a word,\ncondemning WordStar to English text only. Codes below 32 were called\nunprintable and were used for cussing. Just kidding. They were used for\ncontrol characters, like 7 which made your computer beep and 12 which caused\nthe current page of paper to go flying out of the printer and a new one to be\nfed in.\n\nAnd all was good, assuming you were an English speaker.\n\nBecause bytes have room for up to eight bits, lots of people got to thinking,\n\u201cgosh, we can use the codes 128-255 for our own purposes.\u201d The trouble was,\nlots of people had this idea at the same time, and they had their own ideas of\nwhat should go where in the space from 128 to 255. The IBM-PC had something\nthat came to be known as the OEM character set which provided some accented\ncharacters for European languages and a bunch of line drawing characters...\nhorizontal bars, vertical bars, horizontal bars with little dingle-dangles\ndangling off the right side, etc., and you could use these line drawing\ncharacters to make spiffy boxes and lines on the screen, which you can still\nsee running on the 8088 computer at your dry cleaners\u2019. In fact as soon as\npeople started buying PCs outside of America all kinds of different OEM\ncharacter sets were dreamed up, which all used the top 128 characters for\ntheir own purposes. For example on some PCs the character code 130 would\ndisplay as \u00e9, but on computers sold in Israel it was the Hebrew letter Gimel\n(), so when Americans would send their r\u00e9sum\u00e9s to Israel they would arrive as\nrsums. In many cases, such as Russian, there were lots of different ideas of\nwhat to do with the upper-128 characters, so you couldn\u2019t even reliably\ninterchange Russian documents.\n\nEventually this OEM free-for-all got codified in the ANSI standard. In the\nANSI standard, everybody agreed on what to do below 128, which was pretty much\nthe same as ASCII, but there were lots of different ways to handle the\ncharacters from 128 and on up, depending on where you lived. These different\nsystems were called code pages. So for example in Israel DOS used a code page\ncalled 862, while Greek users used 737. They were the same below 128 but\ndifferent from 128 up, where all the funny letters resided. The national\nversions of MS-DOS had dozens of these code pages, handling everything from\nEnglish to Icelandic and they even had a few \u201cmultilingual\u201d code pages that\ncould do Esperanto and Galician on the same computer! Wow! But getting, say,\nHebrew and Greek on the same computer was a complete impossibility unless you\nwrote your own custom program that displayed everything using bitmapped\ngraphics, because Hebrew and Greek required different code pages with\ndifferent interpretations of the high numbers.\n\nMeanwhile, in Asia, even more crazy things were going on to take into account\nthe fact that Asian alphabets have thousands of letters, which were never\ngoing to fit into 8 bits. This was usually solved by the messy system called\nDBCS, the \u201cdouble byte character set\u201d in which some letters were stored in one\nbyte and others took two. It was easy to move forward in a string, but dang\nnear impossible to move backwards. Programmers were encouraged not to use s++\nand s\u2013 to move backwards and forwards, but instead to call functions such as\nWindows\u2019 AnsiNext and AnsiPrev which knew how to deal with the whole mess.\n\nBut still, most people just pretended that a byte was a character and a\ncharacter was 8 bits and as long as you never moved a string from one computer\nto another, or spoke more than one language, it would sort of always work. But\nof course, as soon as the Internet happened, it became quite commonplace to\nmove strings from one computer to another, and the whole mess came tumbling\ndown. Luckily, Unicode had been invented.\n\nUnicode\n\nUnicode was a brave effort to create a single character set that included\nevery reasonable writing system on the planet and some make-believe ones like\nKlingon, too. Some people are under the misconception that Unicode is simply a\n16-bit code where each character takes 16 bits and therefore there are 65,536\npossible characters. This is not, actually, correct. It is the single most\ncommon myth about Unicode, so if you thought that, don\u2019t feel bad.\n\nIn fact, Unicode has a different way of thinking about characters, and you\nhave to understand the Unicode way of thinking of things or nothing will make\nsense.\n\nUntil now, we\u2019ve assumed that a letter maps to some bits which you can store\non disk or in memory:\n\nA -> 0100 0001\n\nIn Unicode, a letter maps to something called a code point which is still just\na theoretical concept. How that code point is represented in memory or on disk\nis a whole nuther story.\n\nIn Unicode, the letter A is a platonic ideal. It\u2019s just floating in heaven:\n\nA\n\nThis platonic A is different than B, and different from a, but the same as A\nand A and A. The idea that A in a Times New Roman font is the same character\nas the A in a Helvetica font, but different from \u201ca\u201d in lower case, does not\nseem very controversial, but in some languages just figuring out what a letter\nis can cause controversy. Is the German letter \u00df a real letter or just a fancy\nway of writing ss? If a letter\u2019s shape changes at the end of the word, is that\na different letter? Hebrew says yes, Arabic says no. Anyway, the smart people\nat the Unicode consortium have been figuring this out for the last decade or\nso, accompanied by a great deal of highly political debate, and you don\u2019t have\nto worry about it. They\u2019ve figured it all out already.\n\nEvery platonic letter in every alphabet is assigned a magic number by the\nUnicode consortium which is written like this: U+0639. This magic number is\ncalled a code point. The U+ means \u201cUnicode\u201d and the numbers are hexadecimal.\nU+0639 is the Arabic letter Ain. The English letter A would be U+0041. You can\nfind them all using the charmap utility on Windows 2000/XP or visiting the\nUnicode web site.\n\nThere is no real limit on the number of letters that Unicode can define and in\nfact they have gone beyond 65,536 so not every unicode letter can really be\nsqueezed into two bytes, but that was a myth anyway.\n\nOK, so say we have a string:\n\nHello\n\nwhich, in Unicode, corresponds to these five code points:\n\nU+0048 U+0065 U+006C U+006C U+006F.\n\nJust a bunch of code points. Numbers, really. We haven\u2019t yet said anything\nabout how to store this in memory or represent it in an email message.\n\nEncodings\n\nThat\u2019s where encodings come in.\n\nThe earliest idea for Unicode encoding, which led to the myth about the two\nbytes, was, hey, let\u2019s just store those numbers in two bytes each. So Hello\nbecomes\n\n00 48 00 65 00 6C 00 6C 00 6F\n\nRight? Not so fast! Couldn\u2019t it also be:\n\n48 00 65 00 6C 00 6C 00 6F 00 ?\n\nWell, technically, yes, I do believe it could, and, in fact, early\nimplementors wanted to be able to store their Unicode code points in high-\nendian or low-endian mode, whichever their particular CPU was fastest at, and\nlo, it was evening and it was morning and there were already two ways to store\nUnicode. So the people were forced to come up with the bizarre convention of\nstoring a FE FF at the beginning of every Unicode string; this is called a\nUnicode Byte Order Mark and if you are swapping your high and low bytes it\nwill look like a FF FE and the person reading your string will know that they\nhave to swap every other byte. Phew. Not every Unicode string in the wild has\na byte order mark at the beginning.\n\nFor a while it seemed like that might be good enough, but programmers were\ncomplaining. \u201cLook at all those zeros!\u201d they said, since they were Americans\nand they were looking at English text which rarely used code points above\nU+00FF. Also they were liberal hippies in California who wanted to conserve\n(sneer). If they were Texans they wouldn\u2019t have minded guzzling twice the\nnumber of bytes. But those Californian wimps couldn\u2019t bear the idea of\ndoubling the amount of storage it took for strings, and anyway, there were\nalready all these doggone documents out there using various ANSI and DBCS\ncharacter sets and who\u2019s going to convert them all? Moi? For this reason alone\nmost people decided to ignore Unicode for several years and in the meantime\nthings got worse.\n\nThus was invented the brilliant concept of UTF-8. UTF-8 was another system for\nstoring your string of Unicode code points, those magic U+ numbers, in memory\nusing 8 bit bytes. In UTF-8, every code point from 0-127 is stored in a single\nbyte. Only code points 128 and above are stored using 2, 3, in fact, up to 6\nbytes.\n\nThis has the neat side effect that English text looks exactly the same in\nUTF-8 as it did in ASCII, so Americans don\u2019t even notice anything wrong. Only\nthe rest of the world has to jump through hoops. Specifically, Hello, which\nwas U+0048 U+0065 U+006C U+006C U+006F, will be stored as 48 65 6C 6C 6F,\nwhich, behold! is the same as it was stored in ASCII, and ANSI, and every OEM\ncharacter set on the planet. Now, if you are so bold as to use accented\nletters or Greek letters or Klingon letters, you\u2019ll have to use several bytes\nto store a single code point, but the Americans will never notice. (UTF-8 also\nhas the nice property that ignorant old string-processing code that wants to\nuse a single 0 byte as the null-terminator will not truncate strings).\n\nSo far I\u2019ve told you three ways of encoding Unicode. The traditional store-it-\nin-two-byte methods are called UCS-2 (because it has two bytes) or UTF-16\n(because it has 16 bits), and you still have to figure out if it\u2019s high-endian\nUCS-2 or low-endian UCS-2. And there\u2019s the popular new UTF-8 standard which\nhas the nice property of also working respectably if you have the happy\ncoincidence of English text and braindead programs that are completely unaware\nthat there is anything other than ASCII.\n\nThere are actually a bunch of other ways of encoding Unicode. There\u2019s\nsomething called UTF-7, which is a lot like UTF-8 but guarantees that the high\nbit will always be zero, so that if you have to pass Unicode through some kind\nof draconian police-state email system that thinks 7 bits are quite enough,\nthank you it can still squeeze through unscathed. There\u2019s UCS-4, which stores\neach code point in 4 bytes, which has the nice property that every single code\npoint can be stored in the same number of bytes, but, golly, even the Texans\nwouldn\u2019t be so bold as to waste that much memory.\n\nAnd in fact now that you\u2019re thinking of things in terms of platonic ideal\nletters which are represented by Unicode code points, those unicode code\npoints can be encoded in any old-school encoding scheme, too! For example, you\ncould encode the Unicode string for Hello (U+0048 U+0065 U+006C U+006C U+006F)\nin ASCII, or the old OEM Greek Encoding, or the Hebrew ANSI Encoding, or any\nof several hundred encodings that have been invented so far, with one catch:\nsome of the letters might not show up! If there\u2019s no equivalent for the\nUnicode code point you\u2019re trying to represent in the encoding you\u2019re trying to\nrepresent it in, you usually get a little question mark: ? or, if you\u2019re\nreally good, a box. Which did you get? -> \ufffd\n\nThere are hundreds of traditional encodings which can only store some code\npoints correctly and change all the other code points into question marks.\nSome popular encodings of English text are Windows-1252 (the Windows 9x\nstandard for Western European languages) and ISO-8859-1, aka Latin-1 (also\nuseful for any Western European language). But try to store Russian or Hebrew\nletters in these encodings and you get a bunch of question marks. UTF 7, 8,\n16, and 32 all have the nice property of being able to store any code point\ncorrectly.\n\nThe Single Most Important Fact About Encodings\n\nIf you completely forget everything I just explained, please remember one\nextremely important fact. It does not make sense to have a string without\nknowing what encoding it uses. You can no longer stick your head in the sand\nand pretend that \u201cplain\u201d text is ASCII.\n\nThere Ain\u2019t No Such Thing As Plain Text.\n\nIf you have a string, in memory, in a file, or in an email message, you have\nto know what encoding it is in or you cannot interpret it or display it to\nusers correctly.\n\nAlmost every stupid \u201cmy website looks like gibberish\u201d or \u201cshe can\u2019t read my\nemails when I use accents\u201d problem comes down to one naive programmer who\ndidn\u2019t understand the simple fact that if you don\u2019t tell me whether a\nparticular string is encoded using UTF-8 or ASCII or ISO 8859-1 (Latin 1) or\nWindows 1252 (Western European), you simply cannot display it correctly or\neven figure out where it ends. There are over a hundred encodings and above\ncode point 127, all bets are off.\n\nHow do we preserve this information about what encoding a string uses? Well,\nthere are standard ways to do this. For an email message, you are expected to\nhave a string in the header of the form\n\n> Content-Type: text/plain; charset=\"UTF-8\"\n\nFor a web page, the original idea was that the web server would return a\nsimilar Content-Type http header along with the web page itself \u2014 not in the\nHTML itself, but as one of the response headers that are sent before the HTML\npage.\n\nThis causes problems. Suppose you have a big web server with lots of sites and\nhundreds of pages contributed by lots of people in lots of different languages\nand all using whatever encoding their copy of Microsoft FrontPage saw fit to\ngenerate. The web server itself wouldn\u2019t really know what encoding each file\nwas written in, so it couldn\u2019t send the Content-Type header.\n\nIt would be convenient if you could put the Content-Type of the HTML file\nright in the HTML file itself, using some kind of special tag. Of course this\ndrove purists crazy... how can you read the HTML file until you know what\nencoding it\u2019s in?! Luckily, almost every encoding in common use does the same\nthing with characters between 32 and 127, so you can always get this far on\nthe HTML page without starting to use funny letters:\n\n> <html> <head> <meta http-equiv=\"Content-Type\" content=\"text/html;\n> charset=utf-8\">\n\nBut that meta tag really has to be the very first thing in the <head> section\nbecause as soon as the web browser sees this tag it\u2019s going to stop parsing\nthe page and start over after reinterpreting the whole page using the encoding\nyou specified.\n\nWhat do web browsers do if they don\u2019t find any Content-Type, either in the http headers or the meta tag? Internet Explorer actually does something quite interesting: it tries to guess, based on the frequency in which various bytes appear in typical text in typical encodings of various languages, what language and encoding was used. Because the various old 8 bit code pages tended to put their national letters in different ranges between 128 and 255, and because every human language has a different characteristic histogram of letter usage, this actually has a chance of working. It\u2019s truly weird, but it does seem to work often enough that na\u00efve web-page writers who never knew they needed a Content-Type header look at their page in a web browser and it looks ok, until one day, they write something that doesn\u2019t exactly conform to the letter-frequency-distribution of their native language, and Internet Explorer decides it\u2019s Korean and displays it thusly, proving, I think, the point that Postel\u2019s Law about being \u201cconservative in what you emit and liberal in what you accept\u201d is quite frankly not a good engineering principle. Anyway, what does the poor reader of this website, which was written in Bulgarian but appears to be Korean (and not even cohesive Korean), do? He uses the View | Encoding menu and tries a bunch of different encodings (there are at least a dozen for Eastern European languages) until the picture comes in clearer. If he knew to do that, which most people don\u2019t.\n\nFor the latest version of CityDesk, the web site management software published\nby my company, we decided to do everything internally in UCS-2 (two byte)\nUnicode, which is what Visual Basic, COM, and Windows NT/2000/XP use as their\nnative string type. In C++ code we just declare strings as wchar_t (\u201cwide\nchar\u201d) instead of char and use the wcs functions instead of the str functions\n(for example wcscat and wcslen instead of strcat and strlen). To create a\nliteral UCS-2 string in C code you just put an L before it as so: L\"Hello\".\n\nWhen CityDesk publishes the web page, it converts it to UTF-8 encoding, which\nhas been well supported by web browsers for many years. That\u2019s the way all 29\nlanguage versions of Joel on Software are encoded and I have not yet heard a\nsingle person who has had any trouble viewing them.\n\nThis article is getting rather long, and I can\u2019t possibly cover everything\nthere is to know about character encodings and Unicode, but I hope that if\nyou\u2019ve read this far, you know enough to go back to programming, using\nantibiotics instead of leeches and spells, a task to which I will leave you\nnow.\n\n## Subscribe!\n\nYou\u2019re reading Joel on Software, stuffed with years and years of completely\nraving mad articles about software development, managing software teams,\ndesigning user interfaces, running successful software companies, and rubber\nduckies.\n\nIf you want to know when I publish something new, I recommend getting an RSS\nreader like NewsBlur and subscribing to my RSS feed.\n\n## About the author.\n\nIn 2000 I co-founded Fog Creek Software, where we created lots of cool things\nlike the FogBugz bug tracker, Trello, and Glitch. I also worked with Jeff\nAtwood to create Stack Overflow and served as CEO of Stack Overflow from\n2010-2019. Today I serve as the chairman of the board for Stack Overflow,\nGlitch, and HASH.\n\nProudly powered by WordPress\n\nNotifications\n\n", "frontpage": false}
