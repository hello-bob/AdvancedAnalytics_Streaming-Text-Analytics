{"aid": "40198959", "title": "Osquery: A Playground for SQL Noobs", "url": "https://bhmt.dev/blog/osquery/", "domain": "bhmt.dev", "votes": 2, "user": "chaosharmonic", "posted_at": "2024-04-29 14:42:44", "comments": 0, "source_title": "Osquery: a Playground for SQL noobs", "source_text": "BHMT\n\n# Osquery: a Playground for SQL noobs\n\nApril 28th, 2024\n\n(...like me!)\n\n## Hello there!\n\nSQL has been a longtime hole in my knowledge set -- data in my prior tech\nsupport life mostly used Excel, my first dev job (and the side project I built\nthereafter) used MongoDB, my next one didn't make me directly interact with a\ndata layer... And while I've been itching for an excuse to pick this up, my\ncurrent side project ended up opting for Deno KV. So for a while I've been\npoking around for some practical personal use tinker on, and stumbled acorss a\ntool called osquery.\n\nosquery takes a variety of different kinds of logging, that may be formatted\nin a variety of different ways, and exposes them virtually as a SQLite\ndatabase. So you'll have different collections (\"tables\" in SQL parlance) of\ndata like os_version, users, uptime, dns_resolvers, and numerous others. To\nrattle off a few more, you can query for packages installed across a variety\nof system- and language-level package managers, browser extensions, network\ninterfaces, users, processes... and so on. Plus, because it's all just a\nwrapper over your existing logging data, it's all read-only, so we can't break\nanything in the course of experimenting with this.\n\nTo be up front, I also taught myself this using chatbots -- using a variety of\nmodels both locally and on Chatbot Arena. It's largely been Mistral\nderivatives of varying finetunes and sizes, but also periodically included\nLlama 3 and Phi-3. This particular experiment started by asking one if it\ncould generate me some basic starting points specific to this tool. The ones\nit gave me weren't quite correct, but after a few changes they were enough to\nat least get me up and running. Overall, though, I had better success asking\nthem about the language as a whole, ranging from general learning paths to\nquestions about how specific parts of the language work. I then validated\nthese outputs by finding examples within the osquery shell to demonstrate them\nwith.\n\nSQL, or Structured Query Language, is a way to query relational databases.\nThese are databases where you store tables of of data -- sort of like a\nspreadsheet but with more definition. (And actually, SQLite can import CSVs\ndirectly and query them in the same language.) Each table holds columns, which\ncontain property names -- fields that, to grab an example like time, would\ninclude values like hour, minute, and so on. The columns also have data types:\nthe ones I just noted are all numbers, but a full system timestamp would be\nstored as text. The table then contains rows, which represent entries -- while\ntime has a single instance, representing the current system time, you'd have\nmore in something like, say, shell_history.\n\nThe table definition is called a schema, and is generally defined at its\ncreation. It can be modified, but since again osquery is just an interface\nover something other logging data and not an actual data store itself, we can\nassume that isn't happening here. We won't even be creating any of the tables\nourselves, as osquery handles this for us. (While I'm on the subject, note\nthat I'm mostly going to be talking about read operations throughout this\nwriteup. Sadly, we'll have to save our XKCD references for another\ndiscussion.)\n\n## Is it possible to learn this power?\n\n### An elegant weapon, for a more civilized age (getting started)\n\nosquery it's available on every major operating system and across a variety of\npackage managers, so I can't describe every install method, and will assume\nyou've followed the guide in their documentation.\n\nOnce it's installed, we can run queries using the osquery shell, running the\ncommand osqueryi in a terminal. This can run in two ways:\n\n  * you can run feed it a query directly: osqueryi \"SELECT * FROM users;\" and get its output in your terminal\n  * or, you can run just enter osqueryi and get an interactive shell\n\nThe latter will start by telling you to type .help if you need more\ninformation, which will then give you a list of other commands like it that\nwill help you navigate the database. For instance, you can enter .tables to\nget a list of tables, or .schema {table} to get the table's definition.\nAdditionally osqueryi will also take these from the command line. (This\nfunctionality, generally, is inherited from sqlite, but the command list\nitself is different.)\n\nLet's try this with .schema users. This outputs the CREATE TABLE statement\nused to define its columns and their data types. For clarity, I've also spaced\nit out into separate lines.\n\n    \n    \n    CREATE TABLE users( `uid` BIGINT, `gid` BIGINT, `uid_signed` BIGINT, `gid_signed` BIGINT, `username` TEXT, `description` TEXT, `directory` TEXT, `shell` TEXT, `uuid` TEXT, `type` TEXT HIDDEN, `is_hidden` INTEGER HIDDEN, `pid_with_namespace` INTEGER HIDDEN, PRIMARY KEY ( `uid`, `username`, `uuid`, `pid_with_namespace` ) ) WITHOUT ROWID;\n\nThis is a lot, particularly for this layout... And it'll vary on your system\n-- I'm on Linux, and not every table or column is relevant on every OS (as\nyou'll see with some of the hidden fields as you poke around) -- so let's go\nfor something simpler...\n\nFor an easy example, that won't break how this displays on mobile (I hope), we\ncan get the the hostnames stored in a system's hosts file.\n\nA quick .schema etc_hosts, and...\n\n    \n    \n    CREATE TABLE etc_hosts( `address` TEXT, `hostnames` TEXT, `pid_with_namespace` INTEGER HIDDEN, PRIMARY KEY ( `address`, `hostnames`, `pid_with_namespace` ) ) WITHOUT ROWID;\n\n...that'll be much easier to start with.\n\nI'm not going to cover everything in the above, but will note that the\ncapitalization is a convention for SQL keywords. The shell isn't actually\ncase-sensitive, but does use it in outputs, which I've kept here and will be\nsticking with for clarity.\n\nTo get the full contents, we can use a simple SELECT statement:\n\n    \n    \n    -- note: this can also be expressed here as `.all hosts` SELECT * FROM etc_hosts;\n\nWhich, on a machine where this hasn't been modified, will give you:\n\naddress| hostnames  \n---|---  \n127.0.0.1| localhost  \n  \nA couple of asides, before I get too deep:\n\n  * some of these examples are real -- it doesn't exactly matter if you know about the system uptime as I'm writing drafts of this -- but details like usernames are masked, and the command line tools are mostly just things you might find useful. I'd absolutly encourage you to go on Internet dives about them.\n  * You also don't even have to type these entire names, as the shell itself comes with tab completion.\n\nFor another example, let's look at uptime:\n\n    \n    \n    CREATE TABLE uptime( `days` INTEGER, `hours` INTEGER, `minutes` INTEGER, `seconds` INTEGER, `total_seconds` BIGINT );\n\nWe might not want the full table. In that case we don't have to use the\nwildcard *; instead, we can specifiy explicit fields by selecting columns,\nlike:\n\n    \n    \n    SELECT days, hours FROM uptime;\n\n### This is where the fun begins (operating on results)\n\nThis gives us:\n\ndays| hours  \n---|---  \n4| 4  \n  \n(And to keep these tables from overflowing on smaller devices, I'll be doing\nthis throughout.)\n\nWhat if we wanted total hours?\n\nThat's not in the schema above, but we can do math operations here and store\ntheir results in a column:\n\n    \n    \n    SELECT ((days * 24) + hours) AS total_hours FROM uptime;\n\nAnd we get:\n\ntotal_hours  \n---  \n100  \n  \nWe can use AS to rename existing fields in our queries, too:\n\n    \n    \n    SELECT days AS total_days FROM uptime;\n\ntotal_days  \n---  \n100  \n  \nWe can limit the output results:\n\n    \n    \n    -- get the first 10 commands in your shell history SELECT uid, command FROM shell_history LIMIT 10;\n\nuid| command  \n---|---  \n1000| tldr grep  \n1000| tldr  \n1000| osqueryi \".tables\"  \n1000| osqueryi \".all time\"  \n1000| deno  \n1000| exit  \n1000| nu  \n1000| osqueryi  \n  \nSort them:\n\n    \n    \n    -- get commands from shell history in reverse alphabetical order -- `DESC` indicates reverse order SELECT command FROM shell_history ORDER BY command DESC LIMIT 3;\n\ncommand  \n---  \nzsh  \nzsh  \nzsh  \n  \nFilter them:\n\n    \n    \n    -- get all running instances of osquery shell SELECT name, pid FROM processes WHERE name=\"osqueryi\";\n\nname| pid  \n---|---  \nosqueryi| 138464  \n      \n    \n    -- get `curl` calls invoked from an interactive shell -- % is a wildcard flag here -- it can be anything (or nothing) at all -- ergo: -- - \"{string}%\" is equivalent to \"starts with\" -- - \"%{string}\" means \"ends with\" -- - and \"%{string}%\" is \"includes\" -- so if, say, you got a URL from somewhere else, -- and piped it into `curl` -- it would still show up here -- (note that this wouldn't show calls made from within scripts) SELECT command FROM shell_history WHERE command LIKE \"%curl%\"\n\ncommand  \n---  \ncurl cheat.sh/sqlite3  \ncurl cheat.sh/ffmpeg  \n  \nAnd add conditional logic:\n\n    \n    \n    -- get processes using zsh or nushell SELECT name, pid FROM processes WHERE name=\"zsh\" OR name=\"nu\" LIMIT 3;\n\nname| pid  \n---|---  \nnu| 137609  \nzsh| 137211  \nzsh| 137416  \n  \n### Not just the men, but the women, and the children, too (grouping)\n\nWe can also aggregate results using functions like COUNT() to get total rows,\nas well as operate on numeric columns with ones like MIN() MAX(), SUM(), and\nAVG().\n\nFor a naive example, let's get some device data and operate on that:\n\n    \n    \n    SELECT models FROM usb_devices ORDER BY model;\n\nmodel  \n---  \n2.0 root hub  \n2.0 root hub  \n2.0 root hub  \n2.0 root hub  \n3.0 root hub  \n3.0 root hub  \n3.0 root hub  \n3.0 root hub  \nGoodix_USB2.0_MISC  \nHDMI_Expansion_Card  \nLaptop_Camera  \nWireless_Device  \n  \nWe can then roll that all into a single result:\n\n    \n    \n    -- get the number of USB SELECT COUNT(*) FROM usb_devices;\n\nCOUNT(*)  \n---  \n12  \n  \nWithout an AS, the function call itself will also be the column label. We can\nalso get counts for each unique value\n\n    \n    \n    SELECT COUNT(*) as total, COUNT(DISTINCT model) as models FROM usb_devices;\n\ntotal| models  \n---|---  \n12| 6  \n  \nBut we might also want to get totals for a given value. Say, total running\nprocesses per application:\n\n    \n    \n    SELECT name, COUNT() as total from processes GROUP BY name ORDER BY total DESC LIMIT 5\n\nname| total  \n---|---  \ncodium| 40  \nfirefox| 23  \nferdium| 19  \nchrome| 14  \nzsh| 10  \n  \n### Human/Cyborg relations (combining datasets)\n\nBut what if you wanted to use data from more than one table? What if you'd\nrather, say, associate processes with usernames instead of uids?\n\nHere's where the \"relational\" part comes in. You can use a join to get a\nsingle, merged result that pulls in data based on shared details like columns.\nFor instance, users also has a uid field, enabling us to...\n\n    \n    \n    -- get running processes by name, id, and user, -- substituting username for user id -- starting 25 results in SELECT username, name, pid FROM processes JOIN users ON processes.uid = users.uid OFFSET 25 LIMIT 2;\n\nusername| name| pid  \n---|---|---  \nryuzaki| ollama| 13523  \nryuzaki| koboldcpp| 13525  \n  \nJOINs will output a single set of results containing columns from both tables.\nBy default these are INNER JOINs, which only return rows where there are\nmatches on both sides. You can include other results from either table by\nusing an OUTER JOIN -- a LEFT OUTER JOIN will return all rows from the first\ntable regardless of matches, RIGHT will return all rows from the second, and\nFULL won't filter from either table.\n\nYou can also take results from multiple queries, across multiple tables, and\ncollect them into one output. Say you wanted to grab all of the packages\nyou've installed. You could make individual queries: select name FROM\npython_packages, select name FROM deb_packages, select tags FROM\ndocker_images... but you can also combine those into one set of results, with\nunion.\n\nLet's go for a simpler example, though. It's a lot of tables, and the default\ninstall on Linux is missing the Arch repos anyway.\n\nWe can also take and nest it into a subquery, to then operate on the entire\ncollected output:\n\n    \n    \n    -- output 2 results from the inner query -- parentheses (you know... `()`) will nest a query, -- and then you can `select` from its results -- the same way you can select from tables SELECT * FROM ( -- get all extensions installed on firefox -- or chromium-based browsers -- that can actually be a *lot* of things: -- brave, opera, *edge*... -- note: there's also a safari_extensions -- but I'm not on a mac SELECT name, browser_type AS source FROM chrome_extensions UNION SELECT name, 'firefox' AS source FROM firefox_addons ) LIMIT 2;\n\nname| source  \n---|---  \nPlasma Integration| chromium  \nBitwarden| firefox  \n  \nThe UNION operation takes similar sets of lists, from queries across separate\ntables, and combines them into one set of results. The key is that you're\nreturning similar overall shapes: you'll notice above that I'm getting the\nsame properties from each query, but AS you can see, I can rename other fields\nor assign manual values to shape the data however I might need.\n\nYou can even use subqueries to do it: for instance, while I could easily\npopulate the name field for Docker images with SELECT tags AS name, I could\nalso do it using SELECT (SELECT tags FROM docker_images) AS name, or then\ninsert that value into any other field of my choosing using AS to assign it to\na different column.\n\n## Perhaps the archives are incomplete (branching out)\n\n### Over six million different forms of communication (other output formats)\n\nYou can also get your results in CSV or JSON by adding --csv or --json flags\nwhen you run osqueryi. This can be useful for output to other applications or\nlanguages.\n\nSo then the etc_hosts example from earlier would show as:\n\n    \n    \n    address|hostnames 127.0.0.1|localhost\n\n...while the pid example above in JSON would look like:\n\n    \n    \n    [ {\"name\":\"zsh\",\"pid\":\"137211\"}, {\"name\":\"zsh\",\"pid\":\"137416\"}, {\"name\":\"nu\",\"pid\":\"137609\"} ]\n\nWhen invoking osqueryi directly on a single query, we can then use output\nredirection to save this to a file: osqueryi --csv '.all uptime' > uptime.csv\n\nSQLite itself can import CSVs to query them directly, and other SQL databases\nlike DuckDB can also do this with JSON. Speaking of which, like Markdown, SQL\nhas a variety of dialects, and not everything above will work in every kind of\ndatabase.\n\n### Let the past die. Kill it if you have to. (PRQL)\n\nIf you wanted to write complex queries, or work across multiple databases, you\nmight also try PRQL. It's a language that simplifies a variety of query\noperations, and compiles them to raw SQL in the dialect of your choosing.\n\nPRQL uses pipelines, represented either by the pipe character (|) or line\nbreaks, to transform data at each step of an operation. As an example:\n\n    \n    \n    from shell_history select { uid, command } filter command != 'chrome' take 10\n\nBut I'm not going to get too deep into this. As a fun bit of trivia, while\nSQL's pronunciation was never explicitly defined when the language was\ncreated, with ones spelling it out and ones calling it \"sequel\" both seeing\ncommon usage. With PRQL, though, calling it \"prequel\" is canonical.\n\n### Abilities some consider to be unnatural (some other playgrounds for SQL)\n\nSome other applications you might want to try include:\n\n  * Pretzel, a playground for PRQL and DuckDB that you can use to query local data. It runs in the browser using WebAssembly, and can can be installed locally to your system as a Progressive Web App. (Fair warning: to my knowledge, it has no dark mode)\n  * Using SteamPipe to query various web services. It has a plugin-based architecture enabling you to make SQL queries against API calls from a variety of diffrent kinds of publicly-accessible platforms, ranging from cloud infrastructure services to some social media platfoms to open data and standards like finance quotes and RSS feeds. It offers a similar shell that you can call with steampipe query.\n\nThere's also a plethora of sample datasets available for public consumption,\nfrom sources like Kaggle or data.gov. While I've personally soured on it as a\nplatform, there's also a subreddit for that. (Annoyingly, I haven't found a\nparticularly active Lemmy.)\n\nWith options that include JSON you could even just make requests from public\nAPIs to collect a dataset on something fun, like maybe data about Star Wars.\n\n...and if you're feeling some really unhinged enterprising, maybe you just end\nup writing 10000 words on Web scraping, as you fall down some rabbit hole just\nbecause you wanted a dataset of lines from Futurama and the ability to curate\nyour own jobs feed.\n\n", "frontpage": false}
