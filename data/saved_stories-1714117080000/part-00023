{"aid": "40162731", "title": "Llama 3: Get building with LLMs in 5 minutes", "url": "https://www.denoise.digital/llama-3-get-started-with-llms/", "domain": "denoise.digital", "votes": 1, "user": "rbanffy", "posted_at": "2024-04-25 20:41:06", "comments": 0, "source_title": "Llama 3: Get building with LLMs in 5 minutes", "source_text": "Llama 3: Get building with LLMs in 5 minutes\n\nAI\n\n# Llama 3: Get building with LLMs in 5 minutes\n\nGet started building transformative AI-powered features within 5 minutes using\nLlama 3, Ollama, and Python.\n\nby Leon Revill\n\nApril 16, 2024 \u2022 9 min read\n\nGetting started with LLMs locally, Llama 3 and Ollama \ud83e\udd99\n\nTo many developers, LLMs provide the opportunity to create AI-powered features\nthat were previously unimaginable but there is also a sense of scrambling to\navoid getting left behind in this persistent AI hype wave. This article will\ndemonstrate how easy it is to start building transformative AI-backed features\nwith LLMs using Ollama and Llama 3.\n\n## What you'll learn\n\n  * How to easily get Llama 3 running locally with Ollama\n  * How to interface with Llama 3 using the Ollama API and Python\n  * How to do basic model customisation to fine-tune its abilities\n\n## What is Llama 3?\n\nLarge Language Models (LLMs) have taken the AI world by storm, capable of\ngenerating human-quality text, translating languages, and writing different\ncreative content. Among these, Llama 3 shines as a powerful open-source option\ndeveloped by Meta AI.\n\nOne key advantage of Llama 3 is its accessibility. Unlike some proprietary\nLLMs with limited access and hefty fees, Llama 3 embraces an open-source\napproach. This allows developers to experiment, fine-tune the model for\nspecific tasks, and integrate it within their projects without barriers.\nAdditionally, Llama 3 boasts impressive performance on various benchmarks,\nrivaling or exceeding the capabilities of commercial models.\n\n## What is Ollama?\n\nOllama is a software tool specifically designed to streamline the process of\nrunning Large Language Models (LLMs) locally. Think of it as the bridge\nbetween complex AI models and your computer. With Ollama, you can take\nadvantage of powerful LLMs like Llama 3 without needing to navigate\ncomplicated technical setups.\n\nHere's how it works:\n\n  * Simplification: Ollama handles the intricacies of setting up dependencies, managing system requirements, and optimizing model configurations. This removes a significant barrier to entry for users who want to explore LLMs on their own machines.\n  * Variety of Models: Ollama supports a growing library of open-source LLMs, giving you the freedom to choose the models that best suit your projects.\n  * Customization: It allows fine-tuning of models to align them with your specific tasks. This unlocks the ability to tailor an LLM for unique use cases within your applications.\n  * Ways to interact: Ollama provides several interfaces to engage with LLMs. You can use a command-line interface (CLI) for quick interactions, utilize a REST API to integrate LLMs into your apps, or leverage the dedicated Python library for programmatic control.\n\nWith Ollama and LLama 3 combined, we can quickly get to work building\ncompelling solutions backed by a powerful LLM, let's get to it.\n\n## Get Llama 3 running in 3 easy steps\n\nFollow these simple steps to get Ollama and Llama 3 running on your local\nmachine so we can get to work.\n\n  1. Install Ollama: Ollama is available for MacOS, Windows, Linux and Docker. Follow the appropriate link to install the software.\n  2. Launch Llama 3: Once installed run ollama run llama3 from a terminal which will launch an interactive interface to the LLM plus an API\n  3. Test the API: Run the following command to test the API is available:\n\n    \n    \n    curl http://localhost:11434/api/generate -d '{\"model\": \"llama3\",\"prompt\":\"Who is Frodo Baggins?\", \"stream\": false}'\n\nIf you get something that looks like the following, we're ready to go!\n\nTesting the Ollama API and make sure everything is running correctly \ud83d\ude80\n\n## Using Python to interface with the Llama 3 Large Language Model\n\nNow we have the API running locally we can build an application that utilises\nthe LLMs capabilities. Ollama provides an easy-to-use Python and JavaScript\nlibrary to make integration easy. However, we'll be using simple HTTP requests\nas this is more back-end agnostic which is useful if we decide to move this\naway from our local environment not using Ollama.\n\nTo give this example more substance we'll also interface with a news API,\nfetch the latest news stories, and then ask the LLM to summarise them.\n\n  1. Grab an API key from newsdata.io so we can use their API to fetch news (it's completely free)\n  2. Create a new Python file we can run from the CLI: get_started.py\n  3. Enter the following code, replacing [API_KEY] with the API key, you've just acquired\n\n    \n    \n    # Import libraries to make the API requests and parse JSON import json import requests # Make a GET request to the news API to fetch the latest technology news newApiUrl = 'https://newsdata.io/api/1/news' urlParams = { 'apikey': '[API_KEY]', # API key 'language': 'en', # Restrict to english only 'category': 'technology' # Limit to the technology category } # Make the POST request newsApiResponse = requests.get(newApiUrl, urlParams) # Parse the JSON response and extract the articles data = newsApiResponse.json() articles = data['results'] print('Fetched {} articles, now asking the LLM to summerise them. This could take a minute or two...\\n'.format(len(articles))) # Loop through the articles and construct the prompt to ask the LLM to summerise them prompt = 'Summerise all these news articles in a single paragraph. Dont use bullet points:\\n\\n' # We're just pulling out the most relevant parts and formatting it in a way to make it easier for the model to understand for article in articles: prompt += 'Title: {}\\n'.format(article['title']) prompt += 'Description: {}\\n\\n'.format(article['description']) # Now we make a POST request to the LLM to generate the summary postData = { 'prompt': prompt, 'model': \"llama3\", 'stream': False # Disable streaming to get the entire response at once } ollamaUrl = 'http://localhost:11434/api/generate' # Make the POST request llmResponse = requests.post(ollamaUrl, data=json.dumps(postData)) # Parse the JSON response and extract the summary result = llmResponse.json() print('News Summary:\\n\\n', result['response'])\n\n  4. In a terminal run your simple script with python get_started.py, after a minute or so, you should get a summary of the latest tech news:\n\nLatest news summary using the Llama 3 Large Language Model (LLM)\n\nLet's break down the more interesting parts of the code and take a closer\nlook.\n\n    \n    \n    newApiUrl = \"https://newsdata.io/api/1/news\" urlParams = { 'apikey': '[API_KEY]', 'language': 'en', 'category': 'technology' } newsApiResponse = requests.get(newApiUrl, urlParams) data = newsApiResponse.json() articles = data['results']\n\nFirstly we're making a GET request to the news API to fetch the latest news\nwithin the technology category, limiting it to English content only. We're\nthen extracting the resulting news articles into a articles variable for use\nlater.\n\n    \n    \n    prompt = 'Summerise all these news articles in a single paragraph. Dont use bullet points:\\n\\n' for article in articles: prompt += 'Title: {}\\n'.format(article['title']) prompt += 'Description: {}\\n\\n'.format(article['description'])\n\nNext, we're setting up our prompt to send to the LLM. The main element of the\nprompt is to instruct the LLM to summarise the news articles we're going to\ngive it, and we're also giving it a strict instruction not to use bullet\npoints. An instruction it likes to regularly ignore \ud83e\udd2c.\n\nWe're then looping over the articles variable and pulling out each article\ntitle and description and appending it to the prompt string making it clear\nwhere each article starts and finishes. If you were to print out the prompt\nvariable at this point it'd look something like this:\n\n    \n    \n    Summerise all these news articles in a single paragraph. Dont use bullet points: Title: Google looking into reports of Phone Hub not working for some ChromeOS users Description: ChromeOS users have been frustrated by a persistent bug causing their Android phones to not be detected by the Phone Hub. This issue, which started plaguing users around after the release of ChromeOS 121, disrupts one of Google\u2019s key efforts to create a seamless integration between Chromebooks and Android devices akin to what Apple has [...] The post Google looking into reports of Phone Hub not working for some ChromeOS users appeared first on PiunikaWeb. Title: I woke up & found my dream car gone from my driveway \u2013 even though I paid down my loan and never missed a payment Description: None Title: Rajasekhar's remake film to stream on OTT soon? Description: None ...etc.\n\nWe then make a POST request to the Ollama API with the constructed prompt.\n\n    \n    \n    postData = { 'prompt': prompt, 'model': \"llama3\", 'stream': False } ollamaUrl = 'http://localhost:11434/api/generate' llmResponse = requests.post(ollamaUrl, data=json.dumps(postData)) result = llmResponse.json() print('News Summary:\\n\\n', result['response'])\n\nAs well as specifying the prompt as an argument in the POST request, we also\nspecify the model we want to use as llama3 and we also set streaming to False\nso we get the response back all at once. You can read the full documentation\non the Ollama API for more info.\n\nWe then finish by printing the response. Thats it! This demonstrates how easy\nit is to get started building things with LLMs thanks to the accessibility of\nLlama 3 via Ollama. But we're not done yet, keep reading to see how you can\nfine-tune the model ... and have a bit of fun at the same time.\n\n## Fine-tune the Llama 3 model to customise its capabilities\n\nThe above example is great, but of course, it'd be better if the LLM\nsummarised our news only as a pirate \ud83c\udff4\u2620\ufe0f\ud83e\udd9c!\n\nOllama allows us to easily tune a model to tailor its responses to our needs.\nWe could just update our prompt above and ask the LLM to respond as a pirate\nevery time, but if we want to create a consistent experience across all\nprompts as the application extends, fine-tuning is the way to go.\n\n  1. Run ollama pull llama3 to make sure you've got the llama3 model locally\n  2. Create a new Modelfile called pirate-news-Modelfile and add the following contents:\n\n    \n    \n    FROM llama3 PARAMETER temperature 1 SYSTEM \"\"\" You are a pirate from the Caribbean, you're always trying to get one over on anyone you meet. Always answer as a pirate. \"\"\"\n\n  3. Create a new model with the new Modelfile: ollama create llama3-pirate -f ./pirate-news-Modelfile\n  4. Run the new model: ollama run llama3-pirate\n  5. Update the Python script to interface with the new model:\n\n    \n    \n    postData = { 'prompt': prompt, 'model': \"llama3-pirate\", # <-- Specify the new model we've just created 'stream': False }\n\n  6. Run your Python script: python get_started.py. You should get something like this:\n\nThings are just better in \"Pirate\" \ud83e\udd23\n\nObviously, this is a silly (but fun!) example. Instead, we could have used\nthis method to fine-tune the model to ask it to never use bullet points\ninstead of adding this to the prompt, if we felt this was something we'd want\nto use across all prompts.\n\nOne final thing before we conclude, you may have noticed the temperate\nparameter in the Modelfile from earlier. By tweaking this we can also control\nhow creative the LLM gets with its responses. The higher the number the more\ncreative the LLM will be, the lower the number the more concise it'll be.\n\nRunning with a temperature of 10 I get something like the following, note the\nsudden use of emojis:\n\nLet the LLM be more creative and it'll start using emojis!\n\nUsing a temperature of 0.1 I get something more mundane like this:\n\nTweaking the temperate allows you to change how creative the LLM will be\n\nHopefully, that gives you greater insight into how you can fine-tune a model\nto behave more specifically to your needs. Take a look at the full\ndocumentation around Modelfiles for more info.\n\n## Conclusion\n\nAs this article demonstrates, building compelling AI features with LLMs has\nnever been easier. Tools like Ollama simplifies the process of running these\npowerful models locally, enabling developers to experiment and unlock new\npossibilities without complex technical setups. By combining Ollama's ease of\nuse with the power of Llama 3, you can start integrating intelligent features\ninto your applications today. Whether you want to summarize news articles,\nenhance customer support, or build entirely new AI-driven experiences, LLMs\nprovide a fertile ground for innovation.\n\n### Opportunities for Further Exploration\n\nThis article provided a foundational introduction. Here are a few exciting\ndirections you can explore next:\n\n  * Advanced Fine-tuning: Delve deeper into fine-tuning techniques to tailor models for even more specific use cases and domain-specific language.\n  * Real-World Applications: Consider how LLM-powered features could solve real problems in your projects. Perhaps it's building a more responsive chatbot for customer service or an AI-assisted tool for code generation.\n  * Ethical Considerations: As you develop LLM-powered features, keep issues of bias, fairness, and transparency in mind.\n\n### The Future is Bright\n\nThe field of LLMs is evolving rapidly. Stay curious, continue experimenting\nwith tools like Ollama, and push the boundaries of what's possible. The true\npotential of LLMs in software development is only just beginning to be\nrevealed.\n\nDon't miss the next one, show support by subscribing below \ud83d\udc47\n\n## Sign up for Denoise Digital: Decoding Tech Trends for Success\n\nCut through the noise with no-nonsense guides, insights, and interviews on all\nthings software development \ud83d\ude80\ud83d\udc53\u2615\ufe0f.\n\nNo spam. Unsubscribe anytime.\n\n### Share This Post\n\n## Check out these related posts\n\n### What is an AI Agent?\n\nby Leon Revill\n\n### AI Advantage: Use Sentiment Analysis to Improve UX\n\nby Leon Revill\n\nCut through the noise with no-nonsense guides, insights, and interviews on all\nthings software development \ud83d\ude80\ud83d\udc53\u2615\ufe0f.\n\n\u00a9 2024 Denoise Digital: Decoding Tech Trends for Success.\n\n", "frontpage": false}
