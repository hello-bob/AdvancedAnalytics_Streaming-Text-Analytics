{"aid": "40291598", "title": "IBM Granite: A Family of Open Foundation Models for Code Intelligence", "url": "https://github.com/ibm-granite/granite-code-models", "domain": "github.com/ibm-granite", "votes": 6, "user": "lukhas", "posted_at": "2024-05-07 21:16:36", "comments": 0, "source_title": "GitHub - ibm-granite/granite-code-models: Granite Code Models: A Family of Open Foundation Models for Code Intelligence", "source_text": "GitHub - ibm-granite/granite-code-models: Granite Code Models: A Family of\nOpen Foundation Models for Code Intelligence\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nibm-granite / granite-code-models Public\n\n  * Notifications\n  * Fork 3\n  * Star 84\n\nGranite Code Models: A Family of Open Foundation Models for Code Intelligence\n\nhuggingface.co/collections/ibm-granite/granite-code-\nmodels-6624c5cec322e4c148c8b330\n\n### License\n\nApache-2.0 license\n\n84 stars 3 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# ibm-granite/granite-code-models\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nmayank31398fix paperMay 7, 2024228f823 \u00b7 May 7, 2024May 7, 2024\n\n## History\n\n30 Commits  \n  \n### .github/ISSUE_TEMPLATE\n\n|\n\n### .github/ISSUE_TEMPLATE\n\n| doc updates: CoC, Contributing info, bug template| May 2, 2024  \n  \n### figures\n\n|\n\n### figures\n\n| updates: remove logo and paper update| May 6, 2024  \n  \n### CODE_OF_CONDUCT.md\n\n|\n\n### CODE_OF_CONDUCT.md\n\n| doc updates: CoC, Contributing info, bug template| May 2, 2024  \n  \n### CONTRIBUTING.md\n\n|\n\n### CONTRIBUTING.md\n\n| update link to license file in CONTRIBUTING md| May 2, 2024  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Create LICENSE| May 2, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| updates: remove logo and paper update| May 6, 2024  \n  \n### paper.pdf\n\n|\n\n### paper.pdf\n\n| fix paper| May 7, 2024  \n  \n## Repository files navigation\n\n\ud83d\udcda Paper | \ud83e\udd17 HugginFace Collection | \ud83d\udcac Discussions Page | \ud83d\udcf0 Blog (coming soon)\n\n## Introduction to Granite Code Models\n\nWe introduce the Granite series of decoder-only code models for code\ngenerative tasks (e.g., fixing bugs, explaining code, documenting code),\ntrained with code written in 116 programming languages. A comprehensive\nevaluation of the Granite Code model family on diverse tasks demonstrates that\nour models consistently reach state-of-the-art performance among available\nopen-source code LLMs.\n\nThe key advantages of Granite Code models include:\n\n  * All-rounder Code LLM: Granite Code models achieve competitive or state-of-the-art performance on different kinds of code-related tasks, including code generation, explanation, fixing, editing, translation, and more. Demonstrating their ability to solve diverse coding tasks.\n  * Trustworthy Enterprise-Grade LLM: All our models are trained on license-permissible data collected following IBM's AI Ethics principles and guided by IBM\u2019s Corporate Legal team for trustworthy enterprise usage. We release all our Granite Code models under an Apache 2.0 license license for research and commercial use.\n\nThe family of Granite Code Models comes in two main variants:\n\n  * Granite Code Base Models: base foundational models designed for code-related tasks (e.g., code repair, code explanation, code synthesis).\n  * Granite Code Instruct Models: instruction following models finetuned using a combination of Git commits paired with human instructions and open-source synthetically generated code instruction datasets.\n\nBoth base and instruct models are available in sizes of 3B, 8B, 20B, and 34B\nparameters.\n\n## Data Collection\n\nOur process to prepare code pretraining data involves several stages. First,\nwe collect a combination of publicly available datasets (e.g., GitHub Code\nClean, Starcoder data), public code repositories, and issues from GitHub.\nSecond, we filter the code data collected based on the programming language in\nwhich data is written (which we determined based on file extension). Then, we\nalso filter out data with low code quality. Third, we adopt an aggressive\ndeduplication strategy that includes both exact and fuzzy deduplication to\nremove documents having (near) identical code content. Finally, we apply a HAP\ncontent filter that reduces models' likelihood of generating hateful, abusive,\nor profane language. We also make sure to redact Personally Identifiable\nInformation (PII) by replacing PII content (e.g., names, email addresses,\nkeys, passwords) with corresponding tokens (e.g., \u27e8NAME\u27e9, \u27e8EMAIL\u27e9, \u27e8KEY\u27e9,\n\u27e8PASSWORD\u27e9). We also scan all datasets using ClamAV to identify and remove\ninstances of malware in the source code. In addition to collecting code data\nfor model training, we curate several publicly available high-quality natural\nlanguage datasets for improving the model\u2019s proficiency in language\nunderstanding and mathematical reasoning.\n\n## Pretraining\n\nThe Granite Code Base models are trained on 3-4T tokens of code data and\nnatural language datasets related to code. Data is tokenized via byte pair\nencoding (BPE), employing the same tokenizer as StarCoder. We utilize high-\nquality data with two phases of training as follows:\n\n  * Phase 1 (code only training): During phase 1, 3B and 8B models are trained for 4 trillion tokens of code data comprising 116 languages. The 20B parameter model is trained on 3 trillion tokens of code. The 34B model is trained on 1.4T tokens after the depth upscaling which is done on the 1.6T checkpoint of 20B model.\n  * Phase 2 (code + language training): In phase 2, we include additional high-quality publicly available data from various domains, including technical, mathematics, and web documents, to further improve the model\u2019s performance. We train all our models for 500B tokens (80% code-20% language mixture) in phase 2 training.\n\n## Instruction Tuning\n\nGranite Code Instruct models are finetuned on the following types of\ninstruction data: 1) code commits sourced from CommitPackFT, 2) high-quality\nmath datasets, specifically we used MathInstruct and MetaMathQA, 3) Code\ninstruction datasets such as Glaive-Code-Assistant-v3, Self-OSS-Instruct-SC2,\nGlaive-Function-Calling-v2, NL2SQL11 and a small collection of synthetic API\ncalling datasets, and 4) high-quality language instruction datasets such as\nHelpSteer and an open license-filtered version of Platypus.\n\n## Evaluation Results\n\nWe conduct an extensive evaluation of our code models on a comprehensive list\nof benchmarks that includes but is not limited to HumanEvalPack, MBPP, and\nMBPP+. This set of benchmarks encompasses different coding tasks across\ncommonly used programming languages (e.g., Python, JavaScript, Java, Go, C++,\nRust).\n\nOur findings reveal that Granite Code models outperform strong open-source\nmodels across model sizes. The figure below illustrates how Granite-8B-Code-\nBase outperforms Mistral-7B, LLama-3-8B, and other open-source models in three\ncoding tasks. We provide further evaluation results in our paper.\n\n## How to Use our Models?\n\nTo use any of our models, pick an appropriate model_path from:\n\n  1. ibm-granite/granite-3b-code-base\n  2. ibm-granite/granite-3b-code-instruct\n  3. ibm-granite/granite-8b-code-base\n  4. ibm-granite/granite-8b-code-instruct\n  5. ibm-granite/granite-20b-code-base\n  6. ibm-granite/granite-20b-code-instruct\n  7. ibm-granite/granite-34b-code-base\n  8. ibm-granite/granite-34b-code-instruct\n\n### Inference\n\n    \n    \n    from transformers import AutoModelForCausalLM, AutoTokenizer device = \"cuda\" # or \"cpu\" model_path = \"ibm-granite/granite-3b-code-base\" # pick anyone from above list tokenizer = AutoTokenizer.from_pretrained(model_path) # drop device_map if running on CPU model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device) model.eval() # change input text as desired input_text = \"def generate():\" # tokenize the text input_tokens = tokenizer(input_text, return_tensors=\"pt\") # transfer tokenized inputs to the device for i in input_tokens: input_tokens[i] = input_tokens[i].to(device) # generate output tokens output = model.generate(**input_tokens) # decode output tokens into text output = tokenizer.batch_decode(output) # loop over the batch to print, in this example the batch size is 1 for i in output: print(i)\n\n### Finetuning\n\nCodebase coming soon.\n\n## Model Cards\n\nThe model cards for each model variant are available in their respective\nHuggingFace repository. Please visit our collection here.\n\n## How to Download our Models?\n\nThe model of choice (granite-3b-code-base in this example) can be cloned\nusing:\n\n    \n    \n    git clone https://huggingface.co/ibm-granite/granite-3b-code-base\n\n## License\n\nAll Granite Code Models are distributed under Apache 2.0 license.\n\n## Would you like to provide feedback?\n\nPlease let us know your comments about our family of code models by visiting\nour collection. Select the repository of the model you would like to provide\nfeedback about. Then, go to Community tab, and click on New discussion.\nAlternatively, you can also post any questions/comments on our github\ndiscussions page.\n\n## About\n\nGranite Code Models: A Family of Open Foundation Models for Code Intelligence\n\nhuggingface.co/collections/ibm-granite/granite-code-\nmodels-6624c5cec322e4c148c8b330\n\n### Resources\n\nReadme\n\n### License\n\nApache-2.0 license\n\n### Code of conduct\n\nCode of conduct\n\nActivity\n\nCustom properties\n\n### Stars\n\n84 stars\n\n### Watchers\n\n3 watching\n\n### Forks\n\n3 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Contributors 2\n\n  * ameza13 Adriana Meza\n  * mayank31398 Mayank Mishra\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": true}
