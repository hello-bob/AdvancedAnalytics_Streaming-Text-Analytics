{"aid": "40080881", "title": "Lessons from customer evaluations of an AI product", "url": "https://generatingconversation.substack.com/p/lessons-from-customer-evaluations", "domain": "generatingconversation.substack.com", "votes": 10, "user": "vsreekanti", "posted_at": "2024-04-18 21:25:13", "comments": 0, "source_title": "Lessons from customer evaluations of an AI product", "source_text": "Lessons from customer evaluations of an AI product\n\n# Generating Conversation\n\nShare this post\n\n#### Lessons from customer evaluations of an AI product\n\ngeneratingconversation.substack.com\n\n#### Discover more from Generating Conversation\n\nThe latest in generative AI & LLMs across research and industry.\n\nOver 1,000 subscribers\n\nContinue reading\n\nSign in\n\n# Lessons from customer evaluations of an AI product\n\nVikram Sreekanti\n\nand\n\nJoseph E. Gonzalez\n\nApr 18, 2024\n\nShare this post\n\n#### Lessons from customer evaluations of an AI product\n\ngeneratingconversation.substack.com\n\nShare\n\nWe\u2019ve had RunLLM in users\u2019 hands for 4 months now, and we\u2019ve learned a lot.\nMost of what we\u2019ve learned has obviously turned into product improvements, but\nwe\u2019ve also started to notice some clear trends in how customers are evaluating\nproducts. These observations are mostly to help teams selling early-stage AI\nproducts, but we also hope that they will be gentle nudges for those on the\nother side of the table.\n\nSource: DALL-E 3.\n\nAs we\u2019ve talked about recently, evaluating LLMs and LLM-based products is\nhard, so we can\u2019t blame anyone evaluating a new tool for not having a clear\nframework. Unlike evaluations of a CRM or a database, there aren\u2019t set\ncriteria or neat feature matrices. In other words, we\u2019re all making it up as\nwe go. \ud83d\ude42\n\nYou learn by touching the stove. Perhaps the most interesting trend we\u2019ve\nfound is that some of our best customers have tended to be those who have\nalready tried to build a custom assistant in-house or with a third-party\nvendor. They\u2019ve seen what an AI product that\u2019s not fully matured looks like\nand how highly variable the responses can be, so they know what kinds of\nfailure modes to look for. They can also recognize higher quality responses\nmore quickly.\n\nThese have been the teams who have also shown up to our conversations with\npre-defined evaluation sets \u2014 they start by baselining their expectations of\nour assistant with that validation set and then continuing from there. Good\nperformance on the validation set is probably not a sign of good quality, but\npoor performance is an indicator of bad quality. The flip side is that teams\nwho haven\u2019t yet experienced an AI assistant tend to rely on vibes-only (more\nbelow).\n\nIt\u2019s all about the data. We wrote about this last week, but it\u2019s become so\nclear that it bears repeating. At every step of our pipeline \u2014 ingesting data\nsources, fine-tuning an LLM, processing user inputs, and generating responses\n\u2014 the quality and specificity of the data is the #1 determinant of the quality\nof our answers.\n\nIf a customer gives us feedback about an answer that could be improved, the\nfirst thing we do is look at our telemetry to understand why we didn\u2019t find\nthe right data. Once we understand that, we can update our data and inference\npipelines to address the issue. The simplest solution almost always tends to\nbe to improve the data processing. After tens of these revision cycles, we\u2019ve\nfound that a bad answer is usually a sign that we haven\u2019t ingested all the\nnecessary data for that question.\n\nManaging expectations is difficult. We\u2019ve generally found there two camps of\npeople unhappy with AI products. The first camp is those who think it\u2019s still\njust a party trick \u2014 something that generates useful answers sometimes but\ndoesn\u2019t have the potential to be consistently helpful. The second is those who\nexpect the world, asking a model to impute answers from information that might\nnot be written down anywhere.\n\nInterestingly, the skeptics are easier to convince than the maximalists. The\nformer, if you get them to invest some time into testing an assistant see\nvalue very quickly. The latter expect that any error is the product builder\u2019s\nfault rather than a limitation of the existing technology or the underlying\ndata. Convincing the maximalists means explaining to them that, yes, AI is a\npowerful tool but it still is only as good as the data you put into it. If\nyour documentation has gaps, so will the assistant\u2019s answers.\n\nVibes are still king. Vibes-based evals have gone from being a tongue-in-cheek\njoke to the dominant evaluation method for most LLM-based products. (They\nalways were, but we just never admitted it until recently.) If you\u2019re not\nfamiliar with the term, it\u2019s shorthand for \u201ctry the model out and see how good\nthe responses are.\u201d It\u2019s obviously not an empirical solution, and it\u2019s not\ngoing to give you pristine performance numbers \u2014 but the truth is that\npristine performance numbers aren\u2019t all that useful.\n\nWhile referring to them as \u2018vibes-based\u2019 may make them sound silly, this is\nstill a great way to evaluate an AI product. In our case, our customers know\nbetter than anyone the types of technical support questions they are getting,\nso their intuition about what our assistant should be able to answer is pretty\ngood. Most of the confidence that\u2019s built or lost during an evaluation comes\nfrom trying one-off questions and getting a sense for what the product can or\ncan\u2019t do.\n\nAt the end of the day, it\u2019s on us \u2014 the product builders \u2014 to prove value to\nour customers, from expectations to delivery. Vibes-based evals are great when\nexpectations are clearly set, but depending on the product, they can sell the\nproduct short (e.g., you don\u2019t realize you need more data to work well) or\noversell the product (e.g., the user happens to ask 10 questions you ace but\nthe not the 4 you might miss).\n\nWhat that means is that (in a still-forming market we need) to be thinking\nabout how to better evaluate our own tools and surface those insights to\ncustomers. For AI products, generic benchmarks like MMLU are going to tell us\nalmost nothing. To build customer confidence, that\u2019s going to mean developing\nproduct- and task-specific measures instead.\n\n### Subscribe to Generating Conversation\n\nLaunched 8 months ago\n\nThe latest in generative AI & LLMs across research and industry.\n\nShare this post\n\n#### Lessons from customer evaluations of an AI product\n\ngeneratingconversation.substack.com\n\nShare\n\nComments\n\nOpenAI is too cheap to beat\n\nData matters; infrastructure matters more\n\nOct 12, 2023 \u2022\n\nVikram Sreekanti\n\nand\n\nJoseph E. Gonzalez\n\n46\n\nShare this post\n\n#### OpenAI is too cheap to beat\n\ngeneratingconversation.substack.com\n\n15\n\nYou can't build a moat with AI\n\nIt's all about the data\n\nApr 11 \u2022\n\nVikram Sreekanti\n\nand\n\nJoseph E. Gonzalez\n\n18\n\nShare this post\n\n#### You can't build a moat with AI\n\ngeneratingconversation.substack.com\n\n7\n\nThe Easiest Part of LLM Applications is the LLM\n\nLLMs have brought thousands of developers into the machine learning world. The\nmodels themselves are, of course, impressive and key to the explosion of...\n\nAug 24, 2023 \u2022\n\nJoseph E. Gonzalez\n\nand\n\nVikram Sreekanti\n\n22\n\nShare this post\n\n#### The Easiest Part of LLM Applications is the LLM\n\ngeneratingconversation.substack.com\n\n2\n\nReady for more?\n\n\u00a9 2024 Joseph E. Gonzalez\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": true}
