{"aid": "40179457", "title": "Ollama with Llama3 and Code Interpreter", "url": "https://jonathanadly.com/ollama-with-llama3-and-code-interpreter", "domain": "jonathanadly.com", "votes": 1, "user": "jonathan-adly", "posted_at": "2024-04-27 12:33:41", "comments": 0, "source_title": "Ollama with Llama3 and Code Interpreter", "source_text": "Ollama with Llama3 and Code Interpreter\n\n#\n\nJonathan's blog\n\n# Jonathan's blog\n\n# Ollama with Llama3 and Code Interpreter\n\nJonathan Adly\n\n\u00b7Apr 26, 2024\u00b7\n\n5 min read\n\nI try to run an experiment once a week with open-source LLMs. This week\nexperiment was using Llama3 via Ollama and AgentRun to have an open-source,\n100% local Code Interpreter.\n\nThe idea is, give an LLM a query that is better answered via code execution\ninstead of its training. Run the code in AgentRun, then return the answer to\nthe user. It is more or less a proof of concept, that can be expanded on with\nadditional tools that an LLM can use.\n\nFor this experiment, I had Ollama installed and running as well as the\nAgentRun API. My goal was use code generated by an LLM it to answer some\nquestions that normally an LLM would struggle with. Like, what is 12345 *\n54321? Or what is the largest prime number under 1000?\n\nThe full code is available here: https://jonathan-\nadly.github.io/AgentRun/examples/ollama_llama3/\n\n# Step 1: Setting Up\n\nIf you don't have Ollama installed, first install it from here. Then, run a\ntest query to make sure everything is working.\n\n    \n    \n    curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"llama3\", \"prompt\":\"What is 1+1?\" }'\n\nNext, install AgentRun and have its REST API running. You will need docker\ninstalled to use docker-compose.\n\n    \n    \n    git clone https://github.com/Jonathan-Adly/agentrun cd agentrun/agentrun-api cp .env.example .env.dev docker-compose up -d --build\n\nAnd again, let's make a test request to make sure everything is running\ncorrectly.\n\n    \n    \n    curl -X GET http://localhost:8000/v1/health/ # {\"status\":\"ok\"}\n\nNext, we will run a Python script that will be our starting point to run\nqueries against Llama3 with Agentrun.\n\n    \n    \n    python -m venv agentrun-venv # windows: .\\agentrun-venv\\Scripts\\activate source agentrun-venv/bin/activate # windows: New-Item main.py -type file touch main.py pip install requests json_repair\n\nIn the file, we will start off by importing the necessary libraries. We'll\nneed json for handling data and requests for making HTTP calls. We\u2019re also\nusing a cool library called json_repair just in case our JSON data decides to\nact up and we need to fix it on the fly. This is especially the case if use 8B\nversion of Llama3 where the JSON sometimes is slightly broken.\n\n    \n    \n    import json import json_repair import requests\n\n### Step 2: Define the Function & Tools\n\nWe've crafted a simple function execute_python_code. This function is pretty\nstraightforward\u2014it sends a Python code snippet to a code execution environment\nprovided by AgentRun and fetches the output.\n\nHere's a quick peek at how this works:\n\n    \n    \n    def execute_python_code(code: str) -> str: code = json.dumps({\"code\": code}) response = requests.post( \"http://localhost:8000/v1/run/\", data=code, headers={\"Content-Type\": \"application/json\"}, ) print(code) output = response.json()[\"output\"] return output\n\nWe basically format the code snippet into JSON, send it off to our localhost\nwhere the magic happens, and get back the result. You can read more about how\nAgentRun works here.\n\nNext, we would use this function as our basis for defining the tool that we\nwant Llama3 to use. Here is what this looks like.\n\n    \n    \n    tools = [ { \"type\": \"function\", \"function\": { \"name\": \"execute_python_code\", \"description\": \"\"\"Sends a python code snippet to the code execution environment and returns the output. The code execution environment can automatically import any library or package by importing. The code snippet to execute must be a valid python code and must use print() to output the result.\"\"\", \"parameters\": { \"type\": \"object\", \"properties\": { \"code\": { \"type\": \"string\", \"description\": \"The code snippet to execute. Must be a valid python code. Must use print() to output the result.\", }, }, \"required\": [\"code\"], }, }, }, ]\n\nLastly, we will set up our model here. We can use the base Llama3 or any of\nthe finetunes provided by the community. For the sake of experimentations, I\nran my experiment using Dolphin-llama3 8b finetune.\n\n    \n    \n    # Ollama dolphin-llama3 page: https://ollama.com/library/dolphin-llama3 MODEL = \"dolphin-llama3\"\n\n### Step 3: The Integration with Ollama and Llama3\n\nMoving on to the cooler element\u2014integration with the Ollama and Llama3.\n\nHere\u2019s a how the query processing and tool selection works:\n\n    \n    \n    def generate_full_completion(prompt: str, model: str = MODEL) -> dict[str, str]: # setting up the parameters including our model params = { \"model\": model, \"prompt\": prompt, \"stream\": False, # seed and temperature for deterministic output \"temperature\": 0, \"seed\": 123, # format is JSON, since we are interested in tools/function calling \"format\": \"json\", } # making the post request and handling responses try: response = requests.post( f\"http://localhost:11434/api/generate\", headers={\"Content-Type\": \"application/json\"}, data=json.dumps(params), timeout=60, ) return json_repair.loads(response.text) except requests.RequestException as err: return {\"error\": f\"API call error: {str(err)}\"}\n\n### Step 4: Putting It All Together\n\nNow, that we have everything setup. We will simply use a prompt to nudge the\nmodel toward using our execute_python_code tool for its outputs.\n\n    \n    \n    def get_answer(query: str) -> str: functions_prompt = f\"\"\" You have access to the following tools: {tools} You must follow these instructions: If a user query requires a tool, you must select the appropriate tool from the list of tools provided. Always select one or more of the above tools based on the user query If a tool is found, you must respond in the JSON format matching the following schema: {{ \"tools\": {{ \"tool\": \"<name of the selected tool>\", \"tool_input\": <parameters for the selected tool, matching the tool's JSON schema }} }} If there are multiple tools required, make sure a list of tools are returned in a JSON array. If there is no tool that match the user request, you will respond with empty json. Do not add any additional Notes or Explanations. User Query: {query} \"\"\" r_dict = generate_full_completion(functions_prompt) r_tools = json_repair.loads(r_dict[\"response\"])[\"tools\"] code = r_tools[\"tool_input\"][\"code\"] response = execute_python_code(code) return response\n\nFinally, when you feed it a query like \"what's the 12312 * 321?\" the whole\nsystem whirls into action, the model figures out which tool and code snippet\nto use, executes it, and bam! You've got your answer.\n\n### Just to Show Off\n\nLet\u2019s see it in action with a couple of examples:\n\n    \n    \n    # 3952152 print(get_answer(\"what's 12312 *321?\")) # 500 print(get_answer(\"how many even numbers are there between 1 and 1000?\")) # Paris print(get_answer(\"what's the capital of France?\"))\n\nWe're blending advanced model integration with practical code execution.\nWhether you're automating tasks, building out a project, or just playing\naround to see the capabilities, this setup might just be your next go-to.\n\nAnd, there you go\u2014a delightful mix of Python, APIs, and some AI magic to\nstreamline how you handle and execute code snippets. As always, tweak, tinker,\nand tailor it to your needs. Happy coding, everyone!\n\n## Subscribe to my newsletter\n\nRead articles from Jonathan's blog directly inside your inbox. Subscribe to\nthe newsletter, and don't miss out.\n\nLlama3Pythonollama\n\n### Written by\n\n# Jonathan Adly\n\nShare this\n\n### More articles\n\nJonathan Adly\n\n# Open Sourcing a Python Project the Right Way in 2024\n\nEvery Python developer I've talked to has written some code that others would\nfind useful. At the sa...\n\nJonathan Adly\n\n# Using GPT-4 Over Email\n\nI recently came across a tweet from a founder in my network, who had an\ninteresting question on Twit...\n\nJonathan Adly\n\n# How to start a Python project in 2024\n\nWhen I begin a new Python project, one of the first steps I take is to create\na virtual environment....\n\n\u00a92024 Jonathan's blog\n\nArchive\u00b7Privacy policy\u00b7Terms\n\nWrite on Hashnode\n\nPowered by Hashnode - Home for tech writers and readers\n\n", "frontpage": false}
