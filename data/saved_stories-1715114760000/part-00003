{"aid": "40286125", "title": "RAG Evaluation Guide", "url": "https://www.pinecone.io/learn/series/vector-databases-in-production-for-busy-engineers/rag-evaluation/", "domain": "pinecone.io", "votes": 1, "user": "gk1", "posted_at": "2024-05-07 14:44:30", "comments": 0, "source_title": "RAG Evaluation: Don\u2019t let customers tell you first | Pinecone", "source_text": "RAG Evaluation: Don\u2019t let customers tell you first | Pinecone\n\nOpens in a new window Opens an external website Opens an external website in a\nnew window\n\nThis website utilizes technologies such as cookies to enable essential site\nfunctionality, as well as for analytics, personalization, and targeted\nadvertising purposes. You may change your settings at any time or accept the\ndefault settings. You may close this banner to continue with only essential\ncookies. Cookie Policy\n\nAnnouncementNew serverless free plan with 3x capacityLearn more\n\n# RAG Evaluation: Don\u2019t let customers tell you first\n\n  * RAG evaluation measures your pipeline\u2019s performance\n  * Understanding binary relevance metrics\n  * Understanding graded relevance metrics\n  * Determining which RAG evaluation framework is right for you\n  * Real-world tips on defining \u201cgood enough\u201d\n  * References\n\nRetrieval augmented generation (RAG) is an architecture that provides the most\nrelevant and contextually important proprietary, private, or dynamic data to\nyour Generative AI application's large language model (LLM) when performing\ntasks to enhance its accuracy and performance.\n\nYour application presents the prompt, which includes your end user\u2019s query and\nthe retrieved context, to your LLM at inference time:\n\nYour RAG pipeline is only as performant as your retrieval phase is accurate.\n\n## RAG evaluation measures your pipeline\u2019s performance\n\nTo evaluate the performance of Retrieval-Augmented Generation systems, we must\nunderstand how well the retrieval component selects and ranks relevant\ndocuments or data.\n\nRAG evaluation quantifies the accuracy of your retrieval phrase by calculating\nmetrics on the top results your system returns, enabling you to\nprogrammatically monitor your pipeline\u2019s precision, recall ability, and\nfaithfulness to facts.\n\nFirst, we\u2019ll examine some of the most commonly used metrics and how they are\nderived. Then, we\u2019ll survey frameworks and tooling that employ these metrics\nto quantify the performance of your RAG deployment.\n\nFinally, we\u2019ll help you choose the best framework and tooling for your use\ncase to ensure your RAG deployments consistently achieve your performance\ngoals.\n\nNote: this chapter focuses on RAG pipelines. For an in-depth treatment of\nInformation Retrieval metrics applied broadly, see Evaluating Measures in\nInformation Retrieval.\n\n## Understanding binary relevance metrics\n\nWhile different frameworks combine metrics and sometimes create custom\nmetrics, grounding in common information retrieval formulas will help you\nevaluate which frameworks best suit your use cases. The following is not an\nexhaustive list of available metrics but a good starting point.\n\nThe retrieval metrics we\u2019re examining first fit under the binary relevance\numbrella, where a result is either relevant or irrelevant. There are two\ncategories of metrics that observe binary relevance: Order-unaware and Order-\naware.\n\nOrder-unaware metrics examine if results are relevant and correct, regardless\nof which order they\u2019re in. In contrast, order-aware metrics penalize systems\nthat return less relevant results in the top positions.\n\nFor example, Precision and Average Precision are two information retrieval\nmetrics. Precision does not care about the result order, so it\u2019s called order-\nunaware. Conversely, Average Precision does consider the relative position of\nresults.\n\nIn the following example, a higher metric value is better:\n\n### Order-unaware metrics\n\n#### Precision@k\n\nPrecision@k=(true positives@k)+(false positives@k)true positives@k\n\nPrecision@k examines how many items in the result set are relevant, where K is\nthe number of results considered.\n\nPrecision@k is ideal for scenarios where the accuracy of each result is more\nimportant than finding every possible relevant result. It is beneficial when\nuser time is limited, reviewing irrelevant results is expensive, you prefer\nthe quality of results over quantity, or initial impressions matter.\n\nFor metrics such as Precision, which can be stated as precision@k, k operates\nlike a sliding window, allowing us to consider the value of the metric at a\ngiven position. We speak of precision at k of 1, or precision at k=3 in all of\nthe following examples.\n\nA key limitation of Precision@k is that it does not consider the relative\nposition of results. Thus, it would equally score two retrieval sets with\nthree relevant results, even if the second set had all three relevant results\nat the end.\n\n#### Recall@k\n\nRecall@k=(true positives@k)+(false negatives@k)true positives@k\n\nRecall@k determines how many relevant results your retrieval step returns from\nall existing relevant results for the query, where K is the number of results\nconsidered.\n\nTherefore, if we assume a k of 1, we\u2019re really only looking at 1 of the 3\ntotal relevant results for the query. This is why recall@k=1 is only 0.33 for\nthe following example:\n\nIf we consider the first 3 items instead, the recall@k score is improved\nbecause the sliding window of results we\u2019re observing now contains more of the\ntotal relevant results in the set.\n\nRecall is ideal for scenarios where capturing all relevant items in the result\nset is essential, even if this means including some irrelevant ones. It is\nhelpful when the cost of missing a relevant document is high, or the user is\nwilling to review more results.\n\n#### F1@k\n\nF1@k=(Precision@k)+(Recall@k)2\u2217(Precision@k)\u2217(Recall@k)\n\nHow can we accurately score retrieval when concerned with precision and\nrecall?\n\nThe F1 score combines precision and recall into a single metric. It is\nbeneficial in scenarios where you must balance retrieving all relevant items\n(recall) and ensuring they are applicable (precision).\n\nThis metric is helpful in situations where missing relevant documents or\nretrieving too many irrelevant items is costly. It is used by some of the\nframeworks and monitoring tools we discuss later on as a metric of overall\nretrieval performance.\n\n### Order-Aware Metrics\n\n#### Mean Reciprocal Rank (MRR)\n\nMRR=\u2223Q\u22231i=1\u2211\u2223Q\u2223ranki1\n\nMean Reciprocal Rank (MRR) is a metric well-suited for evaluating systems\nwhere the relevance of the top-ranked result is more important than the\nrelevance of subsequent results. MRR tells us the average position of the\nfirst relevant item across result sets, hence the \u201cmean.\u201d\n\nMRR ranges from 0 to 1, where a higher value indicates better performance and\nan MRR of 1 means the first result is always the correct answer for all\nqueries.\n\nMRR excels when the user receives one correct answer in fast-paced decision-\nmaking environments or when your goal is to bring as many relevant items as\nclose to the top of the results set as possible.\n\nIn the first example, our MRR is 1 because the first relevant result is at\nposition 1, whereas the MRR of the second example is 0.2. After all, the only\nappropriate result is at the end of the set.\n\n#### Average Precision (AP)\n\nAP=number of relevant items\u2211k=1n(P(k)\u2217rel(k))\n\nAverage Precision assesses the quality of results in a ranking system where\norder is important. It is especially significant when you expect multiple\nrelevant results across a list. Unlike MRR, which focuses on the position of\nthe first relevant item, Average Precision considers all relevant results.\n\nTherefore, Average Precision provides a robust measure of retrieval\neffectiveness across the entire result set, not just at the top.\n\nIn this example, we use Precision@K values for each result to calculate the\nAverage Precision:\n\nAP=3(1+2/3+3/5)=0.7555\n\nWe can see that when relevant results are placed higher in the results set,\nthe Average Precision metric improves:\n\nAP=3(1+1+1)=1\n\n## Understanding graded relevance metrics\n\nThe metrics we reviewed above deal with binary relevance: a result is either\nrelevant or irrelevant, green or red, or yes or no.\n\nWhat if we need to model shades of relevance, where one result might be\nextremely relevant while another is significantly less relevant but not\ntotally irrelevant?\n\nA given result could be assigned a relevance value ranging from 0 to 5, for\nexample:\n\nGraded relevance metrics address this spectrum.\n\nDiscounted Cumulative Gain and its counterpart, Normalized Discounted\nCumulative Gain, are graded relevance metrics.\n\n#### Discounted Cumulative Gain (DCG@k)\n\nDCG@k=i=1\u2211klog2(i+1)reli\n\nDCG is an order-aware metric that measures an item\u2019s usefulness based on its\nrank in the result set. It incorporates a logarithmic penalty to diminish the\nvalue of items lower in the order. This penalty adjusts the relevance of each\nresult, reflecting the intuition that top results are most valuable.\n\nHere\u2019s what that growing penalty looks like across our hypothetical set of\nfive results:\n\ni| Logarithmic Penalty calculation| Penalty  \n---|---|---  \n1| log2(1+1) = log2(2)| 1  \n2| log2(2+1) = log2(3)| 1.584  \n3| log2(3+1) = log2(4)| 2  \n4| log2(4+1) = log2(5)| 2.321  \n5| log2(5+1) = log2(6)| 2.584  \n  \nThe further a relevant result is from the top of the set, the higher the\npenalty applied.\n\nDiscounted Cumulative Gain is useful, but a key limitation is that it does not\naccount for the varying lengths of result sets and naturally favors longer\nsets. Therefore, it is difficult to fairly compare the DCG scores of two\nresult sets of different lengths.\n\nWe can see this effect at work when considering two result sets of varying\nlength, the first with relatively high relevance scores and the second with\nmore items total but not higher relevance scores:\n\nWe can calculate the Discounted Cumulative Gain for the top row of 3 results\nlike so:\n\nDCG=15+1.583+25\n\nWhich gives us a DCG metric of 9.40.\n\nConversely, calculating the DCG for the second row of 5 results which is not\ngenerally of higher quality overall:\n\nDCG=15+1.583+25+2.320+2.582\n\nWhich gives us 10.17!\n\nThis means that, even though the second longer result set was not\nsignificantly more relevant overall, it receives a higher DCG value due to\nhaving more items.\n\nThe Normalized Discounted Cumulative Gain (NDCG@k) metric addresses this\nlimitation by applying normalization.\n\n#### Normalized Discounted Cumulative Gain (DCG@k)\n\nNDCG@k=IDCG@kDCG@k\n\nTo compare DCG scores across queries fairly, we normalize them using the ideal\nDCG (IDCG) which assumes a perfect sort order by relevance:\n\nNDCG, the ratio of DCG to IDCG, allows us to evaluate the ranking of relevant\nitems near the top for sets of various lengths. NDCG thus provides a\nnormalized score, making it possible to compare the performance of queries\nthat return result sets of varying lengths.\n\nTo see how this plays out, we\u2019ll calculate the Ideal DCG (IDCG) for our sorted\nresult set above:\n\nPosition| Relevance| log2 (i + 1)| Rel i / log2 (i + 1)| IDCG@k  \n---|---|---|---|---  \n1| 5| log2(2) = 1| 5/1 = 5| 5  \n2| 5| log2(3) = 1.585| 5/1.585 = 3.154| 5 + 3.154 = 8.154  \n3| 3| log2(4) = 2| 3/2 = 1.5| 5 + 3.154 + 1.5 = 9.654  \n4| 2| log2(5) = 2.322| 2/ 2.3219 = 0.861| 5 + 3.154 + 1.5 + 0.861 = 10.515  \n5| 0| log2(6) = 2.585| 0/2.5849 = 0| 5 + 3.154 + 1.5 + 0.8516 + 0 = 10.515  \n  \nIn other words, our IDCG@k=3, considering the first shorter results set is\n9.654.\n\nThe IDCG@k=5 is 10.515.\n\nWe can now divide DCG@k by IDCG@k to obtain the NDCG@k as shown below:\n\nNDCG@3=9.6549.40=0.973\n\nNDCG@5=10.51510.17=0.967\n\nNormalized Discounted Cumulative Gain scores range from 0 to 1, meaning that\nwe can now fairly compare the relative quality of queries even when our result\nsets differ in length, and we can also see that this metric provides a more\nintuitive representation of which result set is more relevant.\n\n## Determining which RAG evaluation framework is right for you\n\nRAG evaluation frameworks range from proprietary paid solutions to open-source\ntools. Selecting the right solution requires balancing considerations around\nease of maintenance and operational burden, plus how well the metrics observed\nby the tool map to your Retrieval Augmented Generation pipeline\u2019s use case.\n\n#### Arize\n\nArize acts as a model monitoring platform and adapts well to evaluating RAG\nsystems by focusing on Precision, Recall, and F1 Score. It is beneficial in\nscenarios requiring ongoing performance tracking, ensuring RAG systems\nconsistently meet accuracy thresholds in real-time applications. Arize is a\nproprietary paid offering providing robust support and continuous updates for\nenterprise deployments.\n\n#### ARES\n\nARES leverages synthetic data and LLM judges, emphasizing Mean Reciprocal Rank\n(MRR) and Normalized Discounted Cumulative Gain (NDCG). It is ideal for\ndynamic environments where continuous training and updates are necessary to\nmaintain system relevance and accuracy. ARES is an open-source framework that\nprovides data sets to facilitate getting started.\n\n#### RAGAS\n\nRAGAS offers streamlined, reference-free evaluation focusing on Average\nPrecision (AP) and custom metrics like Faithfulness. It assesses how well the\ncontent generated aligns with provided contexts and is suitable for initial\nassessments or when reference data is scarce. RAGAS is an open-source tool,\nallowing for flexible adaptation and integration into diverse RAG systems\nwithout the financial overhead of licensed software.\n\nYou can read our complete RAGAS tutorial and video with sample code here.\n\n#### TraceLoop\n\nTraceLoop is an open-source RAG evaluation framework that focuses on tracing\nthe origins and flow of information throughout the retrieval and generation\nprocess.\n\n#### TruLens\n\nTruLens specializes in domain-specific optimizations for RAG systems,\nemphasizing accuracy and precision tailored to specific fields. It offers\ndetailed metrics to assess retrieval components' domain relevance. TruLens is\na proprietary tool for enterprises seeking specialized, high-performance RAG\nsolutions with robust customer support and regular updates to align with\nevolving domain-specific needs.\n\nYou can read our complete TruLens guide, complete with sample code and\nexplanations here.\n\n#### Galileo\n\nGalileo's RAG tool integrates advanced insights and metrics into users'\nworkflows, focusing on enhancing the performance and transparency of RAG\nsystems. It facilitates easy access to evaluation metrics and simplifies the\nmanagement of large-scale RAG deployments. Galileo offers its solutions as a\nproprietary service, which is ideal for businesses seeking comprehensive,\nscalable AI tools emphasizing usability and commercial application\nintegration.\n\nUltimately, your selection of evaluation framework will depend on your\nparticular use case, budget, and support needs.\n\nWe\u2019ve put together this table to help you determine which solution best maps\nto your use case:\n\nUse Case| Recommended Framework| Metrics Used| Reasoning  \n---|---|---|---  \nInitial RAG evaluations| RAGAS| Average Precision (AP), Faithfulness| RAGAS is\nideal for initial evaluations, especially in environments where reference data\nis scarce. It focuses on precision and how faithfully the response matches the\nprovided context.  \nDynamic, continuous RAG deployments| ARES| MRR, NDCG| ARES uses synthetic data\nand LLM judges, which are suitable for environments needing continuous updates\nand training and focusing on response ranking and relevance.  \nFull system traces including LLMs and Vector storage| TraceLoop| Information\nGain, Factual Consistency, Citation Accuracy| TraceLoop is best suited for\napplications where tracing the flow and provenance of information used in the\ngenerated output is critical, such as academic research or journalism.  \nReal-time RAG monitoring| Arize| Precision, Recall, F1| Arize excels in real-\ntime performance monitoring, making it perfect for deployments where immediate\nfeedback on RAG performance is essential  \nEnterprise-level RAG applications| Galileo| Custom metrics, Context Adherence|\nGalileo provides advanced insights and metrics integration for complex\napplications, ensuring RAG\u2019s adherence to context.  \nOptimizing RAG for specific domains| TruLens| Domain-specific accuracy,\nPrecision| TruLens is designed to optimize RAG systems within specific\ndomains, by enhancing the accuracy and precision of domain-relevant responses  \n  \n## Real-world tips on defining \u201cgood enough\u201d\n\nWhen choosing and prioritizing evaluation metrics, it's crucial to consider\nhow they align with your business objectives.\n\nFor example, if you primarily value customer satisfaction, prioritize metrics\nthat measure response accuracy and relevance (like MRR or Average Precision)\nbecause these directly affect user experience.\n\nWe also recommend approaching RAG evaluation iteratively because production\nRAG pipelines are complex, can involve large amounts of fast-moving data, and\ncan change over time in response to inputs or data store changes:\n\n  1. Instrument pipelines with metrics for observability\n  2. Monitor and observe performance trends over time\n  3. Iteratively make data-driven improvements to your pipeline\n\n#### Setting Baselines and Benchmarks\n\nEstablish performance benchmarks using metrics such as F1 Score and NDCG,\nadapting these as your system evolves and more data becomes available.\n\n#### Balancing Performance with Cost\n\nEvaluating the trade-offs between high metric scores and the associated costs\nis crucial. High MRR might be targeted but require substantial computational\nresources in some systems. Balance your need for observability with\ncomputational overhead.\n\n#### Iterative Improvement\n\nEncourage ongoing re-evaluation and refinement of RAG systems. Monitoring\ntools like Arize can track performance changes over time, facilitating data-\ndriven improvements.\n\n## References\n\n[1] Shahul Es, Jithin James, Luis Espinosa-Anke, Steven Schokaert, RAGAS:\nAutomated Evaluation of Retrieval Augmented Generation (2023)\n\n[2] A. Chaudhary, Evaluation Metrics for Information Retrieval (2020)\n\n[3] Y. Wang et al., A Theoretical Analysis of NDCG Ranking Measures (2013),\nJMLR\n\nShare via:\n\nVector Databases in Production for Busy Engineers\n\nChapters\n\n  1. Handling multi-tenancy\n  2. CI/CD for cloud-based vector databases\n  3. RAG Evaluation: Don't let customers tell you first\n\n     * RAG evaluation measures your pipeline\u2019s performance\n     * Understanding binary relevance metrics\n     * Understanding graded relevance metrics\n     * Determining which RAG evaluation framework is right for you\n     * Real-world tips on defining \u201cgood enough\u201d\n     * References\n\nProduct\n\nOverviewDocumentationIntegrationsTrust and Security\n\nSolutions\n\nCustomersRAGSemantic SearchMulti-Modal SearchCandidate\nGenerationClassification\n\nResources\n\nLearning CenterCommunityPinecone BlogSupport CenterSystem StatusWhat is a\nVector Database?What is Retrieval Augmented Generation (RAG)?\n\nCompany\n\nAboutPartnersCareersNewsroomContact\n\nLegal\n\nTermsPrivacyCookiesCookie Preferences\n\n\u00a9 Pinecone Systems, Inc. | San Francisco, CA\n\nPinecone is a registered trademark of Pinecone Systems, Inc.\n\n", "frontpage": false}
