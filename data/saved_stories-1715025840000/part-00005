{"aid": "40272398", "title": "Cert-Manager: All-in-One Kubernetes TLS Certificate Manager", "url": "https://developer-friendly.blog/2024/05/06/cert-manager-all-in-one-kubernetes-tls-certificate-manager/", "domain": "developer-friendly.blog", "votes": 1, "user": "meysamazad", "posted_at": "2024-05-06 08:19:29", "comments": 0, "source_title": "cert-manager: All-in-One Kubernetes TLS Certificate Manager - Developer Friendly Blog", "source_text": "cert-manager: All-in-One Kubernetes TLS Certificate Manager - Developer\nFriendly Blog\n\nSkip to content\n\n\ud83d\udd0a My latest book \ud83d\udcd6, Ultimate Docker for Cloud-Native Applications \ud83d\udc0b, is out!\nGrab your copy now! \ud83c\udff7\ufe0f\n\n# cert-manager: All-in-One Kubernetes TLS Certificate Manager\u00b6\n\nKubernetes is a great orchestration tool for managing your applications and\nall its dependencies. However, it comes with an extensible architecture and\nwith an unopinionated approach to many of the day-to-day operational tasks.\n\nOne of these tasks is the management of TLS certificates. This includes\nissuing as well as renewing certificates from a trusted Certificate Authority.\nThis CA may be a public internet-facing application or an internal service\nthat needs encrypted communication between parties.\n\nIn this post, we will introduce the industry de-facto tool of choice for\nmanaging certificates in Kubernetes: cert-manager. We will walk you through\nthe installation of the operator, configuring the issuer(s), and receiving a\nTLS certificate as a Kubernetes Secret for the Ingress or Gateway of your\napplication.\n\nFinally, we will create the Gateway CRD and expose an application securely\nover HTTPS to the internet.\n\nIf that gets you excited, hop on and let's get started!\n\n## Introduction\u00b6\n\nIf you have deployed any reverse proxy in the pre-Kubernetes era, you might\nhave, at some point or another, bumped into the issuance and renewal of TLS\ncertificates. The trivial approach, back in the days as well as even today,\nwas to use certbot^1. This command-line utility abstracts you away from the\ncomplexity of the underlying CA APIs and deals with the certificate issuance\nand renewal for you.\n\nCertbot is created by the Electronic Frontier Foundation (EFF) and is a great\ntool for managing certificates on a single server. However, when you're\nworking at scale with many applications and services, you will benefit from\nthe automation and integration that cert-manager^2 provides.\n\ncert-manager is a Kubernetes-native tool that extends the Kubernetes API with\ncustom resources for managing certificates. It is built on top of the Operator\nPattern^3, and is a graduated project of the CNCF^4.\n\nWith cert-manager, you can fetch and renew your TLS certificates behind\nautomation, passing them along to the Ingress^5 or Gateway^6 of your platform\nto host your applications securely over HTTPS without losing the comfort of\nhosting your applications in a Kubernetes cluster.\n\nWith that introduction, let's kick off the installation of cert-manager.\n\n## Pre-requisites\u00b6\n\nBefore we start, make sure you have the following set up:\n\n  * A Kubernetes cluster. We have a couple of guides in our archive if you need help setting up a cluster:\n\n    * Kubernetes the Hard Way\n    * Lightweight K3s installation on Ubuntu\n    * Azure AKS TF Module\n  * OpenTofu v1.7^9\n  * Although not required, we will use FluxCD as a GitOps approach for our deployments. You can either follow along and use the Helm CLI instead, or follow our earlier guide for introduction to FluxCD.\n  * Optionally, External Secrets Operator installed. We will use it in this guide to store the credentials for the DNS01 challenge.\n\n    * We have covered the installation of ESO in our last week's guide if you're interested to learn more: External Secrets Operator: Fetching AWS SSM Parameters into Azure AKS\n\n## Step 0: Installation\u00b6\n\ncert-manager comes with a first-class support for Helm chart installation.\nThis makes the installation rather straightforward.\n\nAs mentioned earlier, we will install the Helm chart using FluxCD CRDs.\n\ncert-manager/namespace.yml\n\n    \n    \n    apiVersion: v1 kind: Namespace metadata: name: cert-manager\n\ncert-manager/repository.yml\n\n    \n    \n    apiVersion: source.toolkit.fluxcd.io/v1beta2 kind: HelmRepository metadata: name: cert-manager spec: interval: 60m url: https://charts.jetstack.io\n\ncert-manager/release.yml\n\n    \n    \n    apiVersion: helm.toolkit.fluxcd.io/v2beta2 kind: HelmRelease metadata: name: cert-manager spec: chart: spec: chart: cert-manager sourceRef: kind: HelmRepository name: cert-manager version: v1.14.x interval: 30m maxHistory: 10 releaseName: cert-manager targetNamespace: cert-manager timeout: 2m valuesFrom: - kind: ConfigMap name: cert-manager-config\n\nAlthough not required, it is hugely beneficial to store the Helm values as it\nis in your VCS. This makes your future upgrades and code reviews easier.\n\n    \n    \n    helm repo add jetstack https://charts.jetstack.io helm repo update jetstack helm show values jetstack/cert-manager \\ --version v1.14.x > cert-manager/values.yml\n\ncert-manager/values.yml\n\n    \n    \n    # NOTE: truncated for brevity ... # In a production setup, the whole file will be stored in VCS as is! installCRDs: true\n\nAdditionally, we will use Kubernetes Kustomize^10:\n\ncert-manager/kustomizeconfig.yml\n\n    \n    \n    nameReference: - kind: ConfigMap version: v1 fieldSpecs: - path: spec/valuesFrom/name kind: HelmRelease\n\ncert-manager/kustomization.yml\n\n    \n    \n    configurations: - kustomizeconfig.yml configMapGenerator: - files: - values.yaml=./values.yml name: cert-manager-config resources: - namespace.yml - repository.yml - release.yml namespace: cert-manager\n\nNotice the namespace we are instructing Kustomization to place the resources\nin. The FluCD Kustomization CRD will be created in the flux-system namespace,\nwhile the Helm release itself is placed in the cert-manager namespace.\n\nUltimately, to create this stack, we will create a FluxCD Kustomization\nresource^11:\n\ncert-manager/kustomize.yml\n\n    \n    \n    apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: cert-manager namespace: flux-system spec: force: false interval: 10m0s path: ./cert-manager prune: true sourceRef: kind: GitRepository name: flux-system wait: true\n\nYou may either advantage from the recursive reconciliation of FluxCD, add it\nto your root Kustomization or apply the resources manually from your command\nline.\n\n    \n    \n    kubectl apply -f cert-manager/kustomize.yml\n\n## Step 1.0: Issuer 101\u00b6\n\nIn general, you can fetch your TLS certificate in two ways: either by\nverifying your domain using the HTTP01 challenge or the DNS01 challenge. Each\nhave their own pros and cons, but both are just to make sure that you own the\ndomain you're requesting the certificate for. Imagine a world where you could\nrequest a certificate for google.com without owning it!\n\nThe HTTP01 challenge requires you to expose a specific path on your web server\nand asking the CA to send a GET request to that endpoint, expecting a specific\nfile to be present in the response.\n\nThis is not always possible, especially if you're running a private service.\n\nOn a personal note, the HTTP01 feels like a complete hack to me.\n\nAs such, in this guide, we'll use the DNS01 challenge. This challenge will\ncreate a specific DNS record in your nameserver. You don't specifically have\nto manually do it yourself, as that is the whole point of automation that\ncert-manager will bring to the table.\n\nFor the DNS01 challenge, there are a couple of nameserver providers natively\nsupported by cert-manager. You can find the list of supported providers on\ntheir website^12.\n\nFor the purpose of this guide, we will provide examples for two different\nnameserver providers: AWS Route53 and Cloudflare.\n\nAWS services are the indudstry standard for many companies, and Route53 is one\nof the most popular DNS services (fame where it's due).\n\nCloudflare, on the other hand, is handling a significant portion of the\ninternet's traffic and is known for its networking capabilities across the\nglobe.\n\nIf you have other needs, you won't find it too difficult to find support for\nyour nameserver provider in the cert-manager documentation.\n\n## Step 1.1: AWS Route53 Issuer\u00b6\n\nThe developer-friendly.blog domain is hosted in Cloudflare and to demonstrate\nthe AWS Route53 issuer, we will make it so that a subdomain will be resolved\nby a Route53 Hosted Zone. That way, we can instruct the cert-manager\ncontroller to talk to the Route53 API for record creation and domain\nverfication.\n\nNameserver Diagrams\n\nhosted-zone/variables.tf\n\n    \n    \n    variable \"root_domain\" { type = string default = \"developer-friendly.blog\" } variable \"subdomain\" { type = string default = \"aws\" } variable \"cloudflare_api_token\" { type = string nullable = false sensitive = true }\n\nhosted-zone/versions.tf\n\n    \n    \n    terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"~> 5.47\" } cloudflare = { source = \"cloudflare/cloudflare\" version = \"~> 4.30\" } } } provider \"cloudflare\" { api_token = var.cloudflare_api_token }\n\nhosted-zone/main.tf\n\n    \n    \n    data \"cloudflare_zone\" \"this\" { name = var.root_domain } resource \"aws_route53_zone\" \"this\" { name = format(\"%s.%s\", var.subdomain, var.root_domain) } resource \"cloudflare_record\" \"this\" { for_each = toset(aws_route53_zone.this.name_servers) zone_id = data.cloudflare_zone.this.id name = var.subdomain type = \"NS\" value = each.value ttl = 1 depends_on = [ aws_route53_zone.this ] }\n\nhosted-zone/outputs.tf\n\n    \n    \n    output \"hosted_zone_id\" { value = aws_route53_zone.this.zone_id } output \"name_servers\" { value = aws_route53_zone.this.name_servers }\n\nTo apply this stack we'll use OpenTofu.\n\nWe could've either separated the stacks to create the Route53 zone beforehand,\nor we will go ahead and target our resources separately from command line as\nyou see below.\n\n    \n    \n    export TF_VAR_cloudflare_api_token=\"PLACEHOLDER\" export AWS_PROFILE=\"PLACEHOLDER\" tofu plan -out tfplan -target=aws_route53_zone.this tofu apply tfplan # And now the rest of the resources tofu plan -out tfplan tofu apply tfplan\n\nWe should have our AWS Route53 Hosted Zone created as you see in the\nscreenshot below.\n\nAWS Route53\n\nNow that we have our Route53 zone created, we can proceed with the cert-\nmanager configuration.\n\n### AWS IAM Role\u00b6\n\nWe now need an IAM Role with enough permissions to create the DNS records to\nsatisfy the DNS01 challenge^14.\n\nMake sure you have a good understanding of the OpenID Connect, the technique\nwe're employing in the trust relationship of the AWS IAM Role.\n\nroute53-iam-role/variables.tf\n\n    \n    \n    variable \"role_name\" { type = string default = \"cert-manager\" } variable \"hosted_zone_id\" { type = string description = \"The Hosted Zone ID that the role will have access to. Defaults to `*`.\" default = \"*\" } variable \"oidc_issuer_url\" { type = string description = \"The OIDC issuer URL of the cert-manager Kubernetes Service Account token.\" nullable = false } variable \"access_token_audience\" { type = string default = \"sts.amazonaws.com\" } variable \"service_account_name\" { type = string default = \"cert-manager\" description = \"The name of the service account.\" } variable \"service_account_namespace\" { type = string default = \"cert-manager\" description = \"The namespace of the service account.\" }\n\nroute53-iam-role/versions.tf\n\n    \n    \n    terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"~> 5.47\" } tls = { source = \"hashicorp/tls\" version = \"~> 4.0\" } } }\n\nroute53-iam-role/main.tf\n\n    \n    \n    data \"aws_iam_policy_document\" \"iam_policy\" { statement { actions = [ \"route53:GetChange\", ] resources = [ \"arn:aws:route53:::change/${var.hosted_zone_id}\", ] } statement { actions = [ \"route53:ChangeResourceRecordSets\", \"route53:ListResourceRecordSets\", ] resources = [ \"arn:aws:route53:::hostedzone/${var.hosted_zone_id}\", ] } statement { actions = [ \"route53:ListHostedZonesByName\", ] resources = [ \"*\", ] } } data \"aws_iam_policy_document\" \"assume_role_policy\" { statement { actions = [ \"sts:AssumeRoleWithWebIdentity\" ] effect = \"Allow\" principals { type = \"Federated\" identifiers = [ aws_iam_openid_connect_provider.this.arn ] } condition { test = \"StringEquals\" variable = \"${aws_iam_openid_connect_provider.this.url}:aud\" values = [ var.access_token_audience ] } condition { test = \"StringEquals\" variable = \"${aws_iam_openid_connect_provider.this.url}:sub\" values = [ \"system:serviceaccount:${var.service_account_namespace}:${var.service_account_name}\", ] } } } data \"tls_certificate\" \"this\" { url = var.oidc_issuer_url } resource \"aws_iam_openid_connect_provider\" \"this\" { url = var.oidc_issuer_url client_id_list = [ var.access_token_audience ] thumbprint_list = [ data.tls_certificate.this.certificates[0].sha1_fingerprint ] } resource \"aws_iam_role\" \"this\" { name = var.role_name inline_policy { name = \"${var.role_name}-route53\" policy = data.aws_iam_policy_document.iam_policy.json } assume_role_policy = data.aws_iam_policy_document.assume_role_policy.json }\n\nroute53-iam-role/outputs.tf\n\n    \n    \n    output \"iam_role_arn\" { value = aws_iam_role.this.arn } output \"service_account_name\" { value = var.service_account_name } output \"service_account_namespace\" { value = var.service_account_namespace } output \"access_token_audience\" { value = var.access_token_audience }\n    \n    \n    tofu plan -out tfplan -var=oidc_issuer_url=\"KUBERNETES_OIDC_ISSUER_URL\" tofu apply tfplan\n\nIf you don't know what OpenID Connect is and what we're doing here, you might\nwant to check out our ealier guides on the following topics:\n\n  * Establishing a trust relationship between bare-metal Kubernetes cluster and AWS IAM\n  * Same concept of trust relationship, this time between Azure AKS and AWS IAM\n\nThe gist of both articles is that we are providing a means for the two\nservices to talk to each other securely and without storing long-lived\ncredentials.\n\nIn essence, one service will issue the tokens (Kubernetes cluster), and the\nother will trust the tokens of the said service (AWS IAM).\n\n### Kubernetes Service Account\u00b6\n\nNow that we have our IAM role set up, we can pass that information to the\ncert-manager Deployment. This way the cert-manager will assume that role with\nthe Web Identity Token flow^15 (there are five flows in total).\n\nWe will also create a ClusterIssuer CRD to be responsible for fetching the TLS\ncertificates from the trusted CA.\n\nroute53-issuer/variables.tf\n\n    \n    \n    variable \"role_arn\" { type = string default = null } variable \"kubeconfig_path\" { type = string default = \"~/.kube/config\" } variable \"kubeconfig_context\" { type = string default = \"k3d-k3s-default\" } variable \"field_manager\" { type = string default = \"flux-client-side-apply\" } variable \"access_token_audience\" { type = string default = \"sts.amazonaws.com\" } variable \"chart_url\" { type = string default = \"https://charts.jetstack.io\" } variable \"chart_name\" { type = string default = \"cert-manager\" } variable \"release_name\" { type = string default = \"cert-manager\" } variable \"release_namespace\" { type = string default = \"cert-manager\" } variable \"release_version\" { type = string default = \"v1.14.x\" }\n\nroute53-issuer/versions.tf\n\n    \n    \n    terraform { required_providers { kubernetes = { source = \"hashicorp/kubernetes\" version = \"~> 2.29\" } helm = { source = \"hashicorp/helm\" version = \"~> 2.13\" } } } provider \"kubernetes\" { config_path = var.kubeconfig_path config_context = var.kubeconfig_context } provider \"helm\" { kubernetes { config_path = var.kubeconfig_path config_context = var.kubeconfig_context } }\n\nroute53-issuer/values.yml.tftpl\n\n    \n    \n    extraEnv: - name: AWS_ROLE_ARN value: ${sa_role_arn} - name: AWS_WEB_IDENTITY_TOKEN_FILE value: /var/run/secrets/aws/token volumeMounts: - name: token mountPath: /var/run/secrets/aws readOnly: true volumes: - name: token projected: sources: - serviceAccountToken: audience: ${sa_audience} expirationSeconds: 3600 path: token securityContext: fsGroup: 1001\n\nroute53-issuer/main.tf\n\n    \n    \n    data \"terraform_remote_state\" \"iam_role\" { count = var.role_arn != null ? 0 : 1 backend = \"local\" config = { path = \"../route53-iam-role/terraform.tfstate\" } } data \"terraform_remote_state\" \"hosted_zone\" { backend = \"local\" config = { path = \"../hosted-zone/terraform.tfstate\" } } locals { sa_audience = coalesce(var.access_token_audience, data.terraform_remote_state.iam_role[0].outputs.access_token_audience) sa_role_arn = coalesce(var.role_arn, data.terraform_remote_state.iam_role[0].outputs.iam_role_arn) } resource \"helm_release\" \"cert_manager\" { name = var.release_name repository = var.chart_url chart = var.chart_name version = var.release_version namespace = var.release_namespace reuse_values = true values = [ templatefile(\"${path.module}/values.yml.tftpl\", { sa_audience = local.sa_audience, sa_role_arn = local.sa_role_arn }) ] } resource \"kubernetes_manifest\" \"cluster_issuer\" { manifest = yamldecode(<<-EOF apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: route53-issuer spec: acme: email: admin@developer-friendly.blog enableDurationFeature: true privateKeySecretRef: name: route53-issuer server: https://acme-v02.api.letsencrypt.org/directory solvers: - dns01: route53: hostedZoneID: ${data.terraform_remote_state.hosted_zone.outputs.hosted_zone_id} region: eu-central-1 EOF ) }\n\nroute53-issuer/outputs.tf\n\n    \n    \n    output \"cluster_issuer_name\" { value = kubernetes_manifest.cluster_issuer.manifest.metadata.name }\n    \n    \n    tofu plan -out tfplan -var=kubeconfig_context=\"KUBECONFIG_CONTEXT\" tofu apply tfplan\n\nIf you're wondering why we're changing the configuration of the cert-manager\nDeployment with a new Helm upgrade, you will find an exhaustive discussion and\nmy comment on the relevant GitHub issue^16.\n\nThe gist of that conversation is that the cert-manager Deployment won't take\ninto account the eks.amazonaws.com/role-arn annotation on its Service Account,\nas you'd see the External Secrets Operator would. It won't even consider using\nthe ClusterIssuer.spec.acme.solvers[*].dns01.route53.role field for some\nreason!\n\nThat's why we're manually passing that information down to its AWS Go SDK^17\nusing the official environment variables^18.\n\nThis stack allows the cert-manager controller to talk to AWS Route53.\n\nNotice that we didn't pass any credentials, nor did we have to create any IAM\nUser for this communication to work. It's all the power of OpenID Connect and\nallows us to establish a trust relationship and never have to worry about any\ncredentials in the client service.\n\n### Is There a Simpler Way?\u00b6\n\nSure there is. If you don't fancy OpenID Connect, there is always the option\nto pass the credentials around in your environment. That leaves you with the\nburden of having to rotate them every now and then, but if you're cool with\nthat, there's nothing stopping you from going down that path. You also have\nthe possibility of automating such rotation using less than 10 lines of code\nin any programming language of course.\n\nAll that said, I have to say that I consider this to be an implementation\nbug^16. where cert-manager does not provide you with a clean interface to\neasily pass around IAM Role ARN. The cert-manager controller SHOULD be able to\nassume the role it is given with the web identity flow!\n\nRegardless of such shortage, in this section, I'll provide you a simpler way\naround this.\n\nBear in mind that I do not recommend this approach, and wouldn't use it in my\nown environments either.\n\nThe idea is to use our previously deployed ESO and pass the AWS IAM User\ncredentials to the cert-manager controller (easy peasy, no drama!).\n\niam-user/variables.tf\n\n    \n    \n    variable \"user_name\" { type = string default = \"cert-manager\" } variable \"hosted_zone_id\" { type = string description = \"The Hosted Zone ID that the role will have access to. Defaults to `*`.\" default = \"*\" }\n\niam-user/versions.tf\n\n    \n    \n    terraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"~> 5.47\" } } }\n\niam-user/main.tf\n\n    \n    \n    data \"aws_iam_policy_document\" \"iam_policy\" { statement { actions = [ \"route53:GetChange\", ] resources = [ \"arn:aws:route53:::change/${var.hosted_zone_id}\", ] } statement { actions = [ \"route53:ChangeResourceRecordSets\", \"route53:ListResourceRecordSets\", ] resources = [ \"arn:aws:route53:::hostedzone/${var.hosted_zone_id}\", ] } statement { actions = [ \"route53:ListHostedZonesByName\", ] resources = [ \"*\", ] } } resource \"aws_iam_user\" \"this\" { name = var.user_name } resource \"aws_iam_access_key\" \"this\" { user = aws_iam_user.this.name } resource \"aws_ssm_parameter\" \"access_key\" { for_each = { \"/cert-manager/access-key\" = aws_iam_access_key.this.id \"/cert-manager/secret-key\" = aws_iam_access_key.this.secret } name = each.key type = \"SecureString\" value = each.value }\n\niam-user/outputs.tf\n\n    \n    \n    output \"iam_user_arn\" { value = aws_iam_user.this.arn } output \"iam_access_key_id\" { value = aws_iam_access_key.this.id sensitive = true } output \"iam_access_key_secret\" { value = aws_iam_access_key.this.secret sensitive = true }\n\nAnd now let's create the corresponding ClusterIssuer, passing the credentials\nlike a normal human being!\n\nroute53-issuer-creds/variables.tf\n\n    \n    \n    variable \"kubeconfig_path\" { type = string default = \"~/.kube/config\" } variable \"kubeconfig_context\" { type = string default = \"k3d-k3s-default\" } variable \"field_manager\" { type = string default = \"flux-client-side-apply\" }\n\nroute53-issuer-creds/versions.tf\n\n    \n    \n    terraform { required_providers { kubernetes = { source = \"hashicorp/kubernetes\" version = \"~> 2.29\" } } } provider \"kubernetes\" { config_path = var.kubeconfig_path config_context = var.kubeconfig_context }\n\nroute53-issuer-creds/main.tf\n\n    \n    \n    data \"terraform_remote_state\" \"hosted_zone\" { backend = \"local\" config = { path = \"../hosted-zone/terraform.tfstate\" } } resource \"kubernetes_manifest\" \"external_secret\" { manifest = yamldecode(<<-EOF apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: route53-issuer-aws-creds namespace: cert-manager spec: data: - remoteRef: key: /cert-manager/access-key secretKey: awsAccessKeyID - remoteRef: key: /cert-manager/secret-key secretKey: awsSecretAccessKey refreshInterval: 5m secretStoreRef: kind: ClusterSecretStore name: aws-parameter-store target: immutable: false EOF ) } resource \"kubernetes_manifest\" \"cluster_issuer\" { manifest = yamldecode(<<-EOF apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: route53-issuer spec: acme: email: admin@developer-friendly.blog enableDurationFeature: true privateKeySecretRef: name: route53-issuer server: https://acme-v02.api.letsencrypt.org/directory solvers: - dns01: route53: hostedZoneID: ${data.terraform_remote_state.hosted_zone.outputs.hosted_zone_id} region: eu-central-1 accessKeyIDSecretRef: key: awsAccessKeyID name: route53-issuer-aws-creds secretAccessKeySecretRef: key: awsSecretAccessKey name: route53-issuer-aws-creds EOF ) }\n\nroute53-issuer-creds/outputs.tf\n\n    \n    \n    output \"external_secret_name\" { value = kubernetes_manifest.external_secret.manifest.metadata.name } output \"external_secret_namespace\" { value = kubernetes_manifest.external_secret.manifest.metadata.namespace } output \"cluster_issuer_name\" { value = kubernetes_manifest.cluster_issuer.manifest.metadata.name }\n\nWe're now done with the AWS issuer. Let's switch gear for a bit to create the\nCloudflare issuer before finally creating a TLS certificate for our desired\ndomain(s).\n\n## Step 1.2: Cloudflare Issuer\u00b6\n\nSince Cloudflare does not have native support for OIDC, we will have to pass\nan API token to the cert-manager controller to be able to manage the DNS\nrecords on our behalf.\n\nThat's where the External Secrets Operator comes into play, again. I invite\nyou to take a look at our last week's guide if you haven't done so already.\n\nWe will use the ExternalSecret CRD to fetch an API token from the AWS SSM\nParameter Store and pass it down to our Kubernetes cluster as a Secret\nresource.\n\nNotice the highlighted lines.\n\ncloudflare-issuer/externalsecret.yml\n\n    \n    \n    apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: cloudflare-issuer-api-token spec: data: - remoteRef: key: /cloudflare/api-token secretKey: cloudflareApiToken refreshInterval: 5m secretStoreRef: kind: ClusterSecretStore name: aws-parameter-store target: immutable: false\n\ncloudflare-issuer/clusterissuer.yml\n\n    \n    \n    apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: cloudflare-issuer spec: acme: email: meysam@licenseware.io enableDurationFeature: true privateKeySecretRef: name: cloudflare-issuer server: https://acme-v02.api.letsencrypt.org/directory solvers: - dns01: cloudflare: apiTokenSecretRef: key: cloudflareApiToken name: cloudflare-issuer-api-token email: admin@developer-friendly.blog\n\ncloudflare-issuer/kustomization.yml\n\n    \n    \n    resources: - externalsecret.yml - clusterissuer.yml namespace: cert-manager\n\ncloudflare-issuer/kustomize.yml\n\n    \n    \n    apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: cloudflare-issuer namespace: flux-system spec: interval: 5m path: ./cloudflare-issuer prune: true sourceRef: kind: GitRepository name: flux-system wait: true\n    \n    \n    kubectl apply -f cloudflare-issuer/kustomize.yml\n\nThat's all the issuers we aimed to create for today. One for AWS Route53 and\nanother for Cloudflare.\n\nWe are now equipped with enough access in our Kubernetes cluster to just\ncreate the TLS certificate and never have to worry about how to verify their\nownership.\n\nWith that promise, let's wrap this up with the easiest part!\n\n## Step 2: TLS Certificate\u00b6\n\nYou should have noticed by now that the root developer-friendly.blog will be\nresolved by Cloudflare as our initial nameserver. We also created a subdomain\nand a Hosted Zone in AWS Route53 to resolve the aws. subdomain using Route53\nas its nameserver.\n\nWe can now fetch a TLS certificate for each of them using our newly created\nClusterIssuer resource. The rest is the responsibility of the cert-manager to\nverify the ownership within the cluster through the DNS01 challenge and using\nthe access we've provided it.\n\ntls-certificates/aws-subdomain.yml\n\n    \n    \n    apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: aws-developer-friendly-blog spec: dnsNames: - '*.aws.developer-friendly.blog' issuerRef: kind: ClusterIssuer name: route53-issuer privateKey: rotationPolicy: Always revisionHistoryLimit: 5 secretName: aws-developer-friendly-blog-tls\n\ntls-certificates/cloudflare-root.yml\n\n    \n    \n    apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: developer-friendly-blog spec: dnsNames: - '*.developer-friendly.blog' issuerRef: kind: ClusterIssuer name: cloudflare-issuer privateKey: rotationPolicy: Always revisionHistoryLimit: 5 secretName: developer-friendly-blog-tls\n\ntls-certificates/kustomization.yml\n\n    \n    \n    resources: - cloudflare-root.yml - aws-subdomain.yml namespace: cert-manager\n\ntls-certificates/kustomize.yml\n\n    \n    \n    apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: tls-certificates namespace: flux-system spec: interval: 5m path: ./tls-certificates prune: true sourceRef: kind: GitRepository name: flux-system wait: true\n    \n    \n    kubectl apply -f tls-certificates/kustomize.yml\n\nIt'll take less than a minute to have the certificates issued and stored as\nKubernetes Secrets in the same namespace as the cert-manager Deployment.\n\nIf you would like the certificates in a different namespace, you're better off\ncreating Issuer instead of ClusterIssuer.\n\nThe final result will have a Secret with two keys: tls.crt and tls.key. This\nwill look similar to what you see below.\n\n    \n    \n    --- - apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: aws-developer-friendly-blog namespace: cert-manager spec: dnsNames: - \"*.aws.developer-friendly.blog\" issuerRef: kind: ClusterIssuer name: route53-issuer privateKey: rotationPolicy: Always revisionHistoryLimit: 5 secretName: aws-developer-friendly-blog-tls status: conditions: - lastTransitionTime: \"2024-05-04T05:44:12Z\" message: Certificate is up to date and has not expired observedGeneration: 1 reason: Ready status: \"True\" type: Ready notAfter: \"2024-07-30T04:44:12Z\" notBefore: \"2024-05-04T04:44:12Z\" renewalTime: \"2024-06-29T04:44:12Z\" --- apiVersion: v1 data: tls.crt: ...truncated... tls.key: ...truncated... kind: Secret metadata: annotations: cert-manager.io/alt-names: \"*.aws.developer-friendly.blog\" cert-manager.io/certificate-name: aws-developer-friendly-blog cert-manager.io/common-name: \"*.aws.developer-friendly.blog\" cert-manager.io/ip-sans: \"\" cert-manager.io/issuer-group: \"\" cert-manager.io/issuer-kind: ClusterIssuer cert-manager.io/issuer-name: route53-issuer cert-manager.io/uri-sans: \"\" labels: controller.cert-manager.io/fao: \"true\" name: aws-developer-friendly-blog-tls namespace: cert-manager type: kubernetes.io/tls\n\n## Step 3: Use the TLS Certificates in Gateway\u00b6\n\nAt this point, we have the required ingredients to host an application within\ncluster and exposing it securely through HTTPS into the world.\n\nThat's exactly what we aim for at this step. But, first, let's create a\nGateway CRD that will be the entrypoint to our cluster. The Gateway can be\nthought of as the sibling of Ingress resource, yet more handsome, more\nsuccessful, more educated and more charming^19.\n\nThe key point to keep in mind is that the Gateway API doesn't come with the\nimplementation. Infact, it is unopinionated about the implementation and you\ncan use any networking solution that fits your needs and has support for it.\n\nIn our case, and based on the personal preference and tendency of the author ,\nwe'll use Cilium as the networking solution, both as the CNI, as well as the\nimplementation for our Gateway API.\n\nWe have covered the Cilium installation before, but, for the sake of\ncompleteness, here's the way to do it^20.\n\ncilium/playbook.yml\n\n    \n    \n    - name: Bootstrap the Kubernetes cluster hosts: localhost gather_facts: false become: true environment: KUBECONFIG: ~/.kube/config vars: helm_version: v3.14.4 kube_context: k3d-k3s-default tasks: - name: Install Kubernetes library ansible.builtin.pip: name: kubernetes<30 state: present - name: Install helm binary ansible.builtin.shell: cmd: \"{{ lookup('ansible.builtin.url', 'https://git.io/get_helm.sh', split_lines=false) }}\" creates: /usr/local/bin/helm environment: DESIRED_VERSION: \"{{ helm_version }}\" - name: Install Kubernetes gateway CRDs kubernetes.core.k8s: src: \"{{ item }}\" state: present context: \"{{ kube_context }}\" loop: - https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.0.0/standard-install.yaml - https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.0.0/experimental-install.yaml - name: Install cilium block: - name: Add cilium helm repository kubernetes.core.helm_repository: name: cilium repo_url: https://helm.cilium.io - name: Install cilium helm release kubernetes.core.helm: name: cilium chart_ref: cilium/cilium namespace: kube-system state: present chart_version: 1.15.x kube_context: \"{{ kube_context }}\" values: gatewayAPI: enabled: true kubeProxyReplacement: true encryption: enabled: true type: wireguard operator: replicas: 1\n\nAnd now, let's create the Gateway CRD.\n\ngateway/gateway.yml\n\n    \n    \n    apiVersion: gateway.networking.k8s.io/v1 kind: Gateway metadata: name: developer-friendly-blog spec: gatewayClassName: cilium listeners: - allowedRoutes: namespaces: from: All name: http port: 80 protocol: HTTP - allowedRoutes: namespaces: from: All name: https port: 443 protocol: HTTPS tls: certificateRefs: - group: \"\" kind: Secret name: developer-friendly-blog-tls namespace: cert-manager - group: \"\" kind: Secret name: aws-developer-friendly-blog-tls namespace: cert-manager mode: Terminate\n\nNotice that we did not create the gatewayClassName. It comes as battery-\nincluded with Cilium. You can find the GatewayClass as soon as Cilium\ninstallation completes with the following command:\n\n    \n    \n    kubectl get gatewayclass\n\nGatewayClass is to Gateway as IngressClass is to Ingress.\n\nAlso note that we are passing the TLS certificates to this Gateway we have\ncreated earlier. That way, the gateway will terminate and offload the SSL/TLS\nencryption and your upstream service will receive plaintext traffic.\n\nHowever, if you have set up your mTLS the way we did with Wireguard encryption\n(or any other mTLS solution for that matter), node-to-node and/or pod-to-pod\ncommunications will also be encrypted.\n\ngateway/http-to-https-redirect.yml\n\n    \n    \n    apiVersion: gateway.networking.k8s.io/v1 kind: HTTPRoute metadata: name: https-redirect spec: parentRefs: - group: gateway.networking.k8s.io kind: Gateway name: developer-friendly-blog namespace: cert-manager sectionName: http rules: - filters: - requestRedirect: scheme: https statusCode: 301 type: RequestRedirect matches: - path: type: PathPrefix value: /\n\nThough not required, the above HTTP to HTTPS redirect allows you to avoid\naccepting any plaintext HTTP traffic on your domain.\n\ngateway/kustomization.yml\n\n    \n    \n    resources: - gateway.yml - http-to-https-redirect.yml namespace: cert-manager\n\ngateway/kustomize.yml\n\n    \n    \n    apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: gateway namespace: flux-system spec: interval: 5m path: ./gateway prune: true sourceRef: kind: GitRepository name: flux-system wait: true\n    \n    \n    kubectl apply -f gateway/kustomize.yml\n\n## Step 4: HTTPS Application\u00b6\n\nThat's all the things we aimed to do today. At this point, we can create our\nHTTPS-only application and expose it securely to the wild internet!\n\napp/deployment.yml\n\n    \n    \n    apiVersion: apps/v1 kind: Deployment metadata: name: echo-server spec: replicas: 1 selector: matchLabels: app: echo-server template: metadata: labels: app: echo-server spec: containers: - envFrom: - configMapRef: name: echo-server image: ealen/echo-server name: echo-server ports: - containerPort: 80 name: http securityContext: capabilities: add: - NET_BIND_SERVICE drop: - ALL readOnlyRootFilesystem: true securityContext: runAsGroup: 1000 runAsUser: 1000 seccompProfile: type: RuntimeDefault\n\napp/service.yml\n\n    \n    \n    apiVersion: v1 kind: Service metadata: name: echo-server spec: ports: - name: http port: 80 targetPort: http type: ClusterIP\n\napp/httproute.yml\n\n    \n    \n    apiVersion: gateway.networking.k8s.io/v1 kind: HTTPRoute metadata: name: echo-server spec: hostnames: - echo.developer-friendly.blog - echo.aws.developer-friendly.blog parentRefs: - group: gateway.networking.k8s.io kind: Gateway name: developer-friendly-blog namespace: cert-manager sectionName: https rules: - backendRefs: - group: \"\" kind: Service name: echo-server port: 80 weight: 1 filters: - responseHeaderModifier: set: - name: Strict-Transport-Security value: max-age=31536000; includeSubDomains; preload type: ResponseHeaderModifier matches: - path: type: PathPrefix value: /\n\napp/configs.env\n\n    \n    \n    PORT=80 LOGS__IGNORE__PING=false ENABLE__HOST=true ENABLE__HTTP=true ENABLE__REQUEST=true ENABLE__COOKIES=true ENABLE__HEADER=true ENABLE__ENVIRONMENT=false ENABLE__FILE=false\n\napp/kustomization.yml\n\n    \n    \n    resources: - deployment.yml - service.yml - httproute.yml images: - name: ealen/echo-server newTag: 0.9.2 configMapGenerator: - name: echo-server envs: - configs.env replacements: - source: kind: Deployment name: echo-server fieldPath: spec.template.metadata.labels targets: - select: kind: Service name: echo-server fieldPaths: - spec.selector options: create: true - source: kind: ConfigMap name: echo-server fieldPath: data.PORT targets: - select: kind: Deployment name: echo-server fieldPaths: - spec.template.spec.containers.[name=echo-server].ports.[name=http].containerPort namespace: default\n\napp/kustomize.yml\n\n    \n    \n    apiVersion: kustomize.toolkit.fluxcd.io/v1 kind: Kustomization metadata: name: app namespace: flux-system spec: interval: 5m path: ./app prune: true sourceRef: kind: GitRepository name: flux-system wait: true\n    \n    \n    kubectl apply -f app/kustomize.yml\n\nThat's everything we had to say for today. We can now easily access our\napplication as follows:\n\n    \n    \n    curl -v https://echo.developer-friendly.blog -sSo /dev/null\n\nor...\n\n    \n    \n    curl -v https://aws.echo.developer-friendly.blog -sSo /dev/null\n\nOutput of the curl command(s)\n\n    \n    \n    ...truncated... * expire date: Jul 30 04:44:12 2024 GMT ...truncated...\n\nBoth will show that the TLS certificate is present. signed by a trusted CA, is\nvalid and matches the domain we're trying to access.\n\nYou shall see the same expiry date on your certificate if accessing as\nfollows:\n\n    \n    \n    kubectl get certificate \\ -n cert-manager \\ -o jsonpath='{.items[*].status.notAfter}'\n\nOutput of kubectl command\n\n    \n    \n    2024-07-30T04:44:12Z\n\nAs you can see, the information we get from the publicly available certificate\nas well as the one we get internally from our Kubernetes cluster are the same\ndown to the second.\n\n## Conclusion\u00b6\n\nThese days, I am never spinning up a Kubernetes cluster without having cert-\nmanager installed on it as its day 1 operation task. It's such a life-saver\ntool to have in your toolbox and you can rest assured that the TLS\ncertificates in your cluster are always up-to-date and valid.\n\nIf you ever had to worry about the expiry date of your certificates before,\nthose days are behind you and you can benefit a lot by employing the cert-\nmanager operator in your Kubernetes cluster. Use it to its full potential and\nyou shall be served greatly.\n\nHope you enjoyed reading this material.\n\nUntil next time , ciao and happy hacking!\n\n  1. https://certbot.eff.org/ \u21a9\n\n  2. https://cert-manager.io/ \u21a9\n\n  3. https://kubernetes.io/docs/concepts/extend-kubernetes/operator/ \u21a9\n\n  4. https://www.cncf.io/ \u21a9\n\n  5. https://kubernetes.io/docs/concepts/services-networking/ingress/ \u21a9\n\n  6. https://gateway-api.sigs.k8s.io/ \u21a9\n\n  7. https://github.com/developer-friendly/blog/commit/eedf71d1f179a8a994a030e77c62f380440ed4d8 \u21a9\n\n  8. https://squidfunk.github.io/mkdocs-material \u21a9\n\n  9. https://github.com/opentofu/opentofu/releases/tag/v1.7.0 \u21a9\n\n  10. https://kubectl.docs.kubernetes.io/references/kustomize/kustomization/ \u21a9\n\n  11. https://fluxcd.io/flux/components/kustomize/kustomizations/ \u21a9\n\n  12. https://cert-manager.io/docs/configuration/acme/dns01/ \u21a9\n\n  13. https://developer.hashicorp.com/terraform/language/meta-arguments/for_each#limitations-on-values-used-in-for_each \u21a9\n\n  14. https://cert-manager.io/docs/configuration/acme/dns01/route53/#set-up-an-iam-role \u21a9\n\n  15. https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRoleWithWebIdentity.html \u21a9\n\n  16. https://github.com/cert-manager/cert-manager/issues/2147#issuecomment-2094066782 \u21a9\u21a9\n\n  17. https://github.com/aws/aws-sdk-go \u21a9\n\n  18. https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html \u21a9\n\n  19. https://gateway-api.sigs.k8s.io/ \u21a9\n\n  20. https://docs.cilium.io/en/stable/network/servicemesh/gateway-api/gateway-api/ \u21a9\n\nMay 6, 2024 May 2, 2024 GitHub\n\nCopyright \u00a9 Meysam Azad\n\nMade with Material for MkDocs and hosted by GitHub Pages We respect your\nprivacy. To manage your cookie preferences, click here. Licensed under the\nApache-2.0 license\n\n", "frontpage": false}
