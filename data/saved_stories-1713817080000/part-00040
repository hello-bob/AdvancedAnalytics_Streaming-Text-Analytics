{"aid": "40114249", "title": "Meta Llama 3 GitHub", "url": "https://github.com/meta-llama/llama3", "domain": "github.com/meta-llama", "votes": 1, "user": "adif_sgaid", "posted_at": "2024-04-22 13:40:01", "comments": 0, "source_title": "GitHub - meta-llama/llama3: The official Meta Llama 3 GitHub site", "source_text": "GitHub - meta-llama/llama3: The official Meta Llama 3 GitHub site\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nmeta-llama / llama3 Public\n\n  * Notifications\n  * Fork 973\n  * Star 13.1k\n\nThe official Meta Llama 3 GitHub site\n\n### License\n\nView license\n\n13.1k stars 973 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# meta-llama/llama3\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n19 Branches\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\njspisakMerge pull request #27 from ArtificialZeng/patch-1Apr 22, 202487d55e8 \u00b7\nApr 22, 2024Apr 22, 2024\n\n## History\n\n106 Commits  \n  \n### llama\n\n|\n\n### llama\n\n| Update tokenizer.py| Apr 15, 2024  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| .| Apr 3, 2024  \n  \n### CODE_OF_CONDUCT.md\n\n|\n\n### CODE_OF_CONDUCT.md\n\n| Create CODE_OF_CONDUCT.md| Apr 18, 2024  \n  \n### CONTRIBUTING.md\n\n|\n\n### CONTRIBUTING.md\n\n| Update CONTRIBUTING.md| Apr 18, 2024  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Update LICENSE| Apr 15, 2024  \n  \n### Llama3_Repo.jpeg\n\n|\n\n### Llama3_Repo.jpeg\n\n| Add files via upload| Apr 17, 2024  \n  \n### MODEL_CARD.md\n\n|\n\n### MODEL_CARD.md\n\n| Update MODEL_CARD.md| Apr 20, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update README.md| Apr 18, 2024  \n  \n### USE_POLICY.md\n\n|\n\n### USE_POLICY.md\n\n| Create USE_POLICY.md| Apr 15, 2024  \n  \n### download.sh\n\n|\n\n### download.sh\n\n| Set execute permissions on download script| Apr 18, 2024  \n  \n### eval_details.md\n\n|\n\n### eval_details.md\n\n| Update eval_details.md| Apr 18, 2024  \n  \n### example_chat_completion.py\n\n|\n\n### example_chat_completion.py\n\n| rope theta + nits| Apr 15, 2024  \n  \n### example_text_completion.py\n\n|\n\n### example_text_completion.py\n\n| rope theta + nits| Apr 15, 2024  \n  \n### requirements.txt\n\n|\n\n### requirements.txt\n\n| add blobfile| Apr 5, 2024  \n  \n### setup.py\n\n|\n\n### setup.py\n\n| Update setup.py| Apr 3, 2024  \n  \n## Repository files navigation\n\n\ud83e\udd17 Models on Hugging Face | Blog | Website | Get Started\n\n# Meta Llama 3\n\nWe are unlocking the power of large language models. Our latest version of\nLlama is now accessible to individuals, creators, researchers, and businesses\nof all sizes so that they can experiment, innovate, and scale their ideas\nresponsibly.\n\nThis release includes model weights and starting code for pre-trained and\ninstruction tuned Llama 3 language models \u2014 including sizes of 8B to 70B\nparameters.\n\nThis repository is intended as a minimal example to load Llama 3 models and\nrun inference. For more detailed examples, see llama-recipes.\n\n## Download\n\nIn order to download the model weights and tokenizer, please visit the Meta\nLlama website and accept our License.\n\nOnce your request is approved, you will receive a signed URL over email. Then\nrun the download.sh script, passing the URL provided when prompted to start\nthe download.\n\nPre-requisites: Make sure you have wget and md5sum installed. Then run the\nscript: ./download.sh.\n\nKeep in mind that the links expire after 24 hours and a certain amount of\ndownloads. If you start seeing errors such as 403: Forbidden, you can always\nre-request a link.\n\n### Access to Hugging Face\n\nWe are also providing downloads on Hugging Face, in both transformers and\nnative llama3 formats. To download the weights from Hugging Face, please\nfollow these steps:\n\n  * Visit one of the repos, for example meta-llama/Meta-Llama-3-8B-Instruct.\n  * Read and accept the license. Once your request is approved, you'll be granted access to all the Llama 3 models. Note that requests used to take up to one hour to get processed.\n  * To download the original native weights to use with this repo, click on the \"Files and versions\" tab and download the contents of the original folder. You can also download them from the command line if you pip install huggingface-hub:\n\n    \n    \n    huggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct --include \"original/*\" --local-dir meta-llama/Meta-Llama-3-8B-Instruct\n\n  * To use with transformers, the following pipeline snippet will download and cache the weights:\n    \n        import transformers import torch model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\" pipeline = transformers.pipeline( \"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", model_kwargs={\"torch_dtype\": torch.bfloat16}, device=\"cuda\", )\n\n## Quick Start\n\nYou can follow the steps below to quickly get up and running with Llama 3\nmodels. These steps will let you run quick inference locally. For more\nexamples, see the Llama recipes repository.\n\n  1. In a conda env with PyTorch / CUDA available clone and download this repository.\n\n  2. In the top-level directory run:\n    \n        pip install -e .\n\n  3. Visit the Meta Llama website and register to download the model/s.\n\n  4. Once registered, you will get an email with a URL to download the models. You will need this URL when you run the download.sh script.\n\n  5. Once you get the email, navigate to your downloaded llama repository and run the download.sh script.\n\n     * Make sure to grant execution permissions to the download.sh script\n     * During this process, you will be prompted to enter the URL from the email.\n     * Do not use the \u201cCopy Link\u201d option but rather make sure to manually copy the link from the email.\n  6. Once the model/s you want have been downloaded, you can run the model locally using the command below:\n\n    \n    \n    torchrun --nproc_per_node 1 example_chat_completion.py \\ --ckpt_dir Meta-Llama-3-8B-Instruct/ \\ --tokenizer_path Meta-Llama-3-8B-Instruct/tokenizer.model \\ --max_seq_len 512 --max_batch_size 6\n\nNote\n\n  * Replace Meta-Llama-3-8B-Instruct/ with the path to your checkpoint directory and Meta-Llama-3-8B-Instruct/tokenizer.model with the path to your tokenizer model.\n  * The \u2013nproc_per_node should be set to the MP value for the model you are using.\n  * Adjust the max_seq_len and max_batch_size parameters as needed.\n  * This example runs the example_chat_completion.py found in this repository but you can change that to a different .py file.\n\n## Inference\n\nDifferent models require different model-parallel (MP) values:\n\nModel| MP  \n---|---  \n8B| 1  \n70B| 8  \n  \nAll models support sequence length up to 8192 tokens, but we pre-allocate the\ncache according to max_seq_len and max_batch_size values. So set those\naccording to your hardware.\n\n### Pretrained Models\n\nThese models are not finetuned for chat or Q&A. They should be prompted so\nthat the expected answer is the natural continuation of the prompt.\n\nSee example_text_completion.py for some examples. To illustrate, see the\ncommand below to run it with the llama-3-8b model (nproc_per_node needs to be\nset to the MP value):\n\n    \n    \n    torchrun --nproc_per_node 1 example_text_completion.py \\ --ckpt_dir Meta-Llama-3-8B/ \\ --tokenizer_path Meta-Llama-3-8B/tokenizer.model \\ --max_seq_len 128 --max_batch_size 4\n\n### Instruction-tuned Models\n\nThe fine-tuned models were trained for dialogue applications. To get the\nexpected features and performance for them, a specific formatting defined in\nChatFormat needs to be followed: The prompt begins with a <|begin_of_text|>\nspecial token, after which one or more messages follow. Each message starts\nwith the <|start_header_id|> tag, the role system, user or assistant, and the\n<|end_header_id|> tag. After a double newline \\n\\n the contents of the message\nfollow. The end of each message is marked by the <|eot_id|> token.\n\nYou can also deploy additional classifiers for filtering out inputs and\noutputs that are deemed unsafe. See the llama-recipes repo for an example of\nhow to add a safety checker to the inputs and outputs of your inference code.\n\nExamples using llama-3-8b-chat:\n\n    \n    \n    torchrun --nproc_per_node 1 example_chat_completion.py \\ --ckpt_dir Meta-Llama-3-8B-Instruct/ \\ --tokenizer_path Meta-Llama-3-8B-Instruct/tokenizer.model \\ --max_seq_len 512 --max_batch_size 6\n\nLlama 3 is a new technology that carries potential risks with use. Testing\nconducted to date has not \u2014 and could not \u2014 cover all scenarios. In order to\nhelp developers address these risks, we have created the Responsible Use\nGuide.\n\n## Issues\n\nPlease report any software \u201cbug\u201d, or other problems with the models through\none of the following means:\n\n  * Reporting issues with the model: https://github.com/meta-llama/llama3/issues\n  * Reporting risky content generated by the model: developers.facebook.com/llama_output_feedback\n  * Reporting bugs and security concerns: facebook.com/whitehat/info\n\n## Model Card\n\nSee MODEL_CARD.md.\n\n## License\n\nOur model and weights are licensed for both researchers and commercial\nentities, upholding the principles of openness. Our mission is to empower\nindividuals, and industry through this opportunity, while fostering an\nenvironment of discovery and ethical AI advancements.\n\nSee the LICENSE file, as well as our accompanying Acceptable Use Policy\n\n## Questions\n\nFor common questions, the FAQ can be found here which will be kept up to date\nover time as new questions arise.\n\n## About\n\nThe official Meta Llama 3 GitHub site\n\n### Resources\n\nReadme\n\n### License\n\nView license\n\n### Code of conduct\n\nCode of conduct\n\nActivity\n\nCustom properties\n\n### Stars\n\n13.1k stars\n\n### Watchers\n\n119 watching\n\n### Forks\n\n973 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Contributors 21\n\n\\+ 7 contributors\n\n## Languages\n\n  * Python 94.3%\n  * Shell 5.7%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
