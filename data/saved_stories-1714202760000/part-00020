{"aid": "40173419", "title": "Ideological Asymmetries in Online Hostility", "url": "https://www.nature.com/articles/s41598-023-46574-2", "domain": "nature.com", "votes": 1, "user": "anigbrowl", "posted_at": "2024-04-26 19:51:07", "comments": 0, "source_title": "Ideological asymmetries in online hostility, intimidation, obscenity, and prejudice", "source_text": "Ideological asymmetries in online hostility, intimidation, obscenity, and prejudice | Scientific Reports\n\nSkip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n\n## Search\n\nAdvanced search\n\n### Quick links\n\n    * Explore articles by subject\n    * Find a job\n    * Guide to authors\n    * Editorial policies\n\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n\n  * Sign up for alerts\n  * RSS feed\n\nIdeological asymmetries in online hostility, intimidation, obscenity, and\nprejudice\n\nDownload PDF\n\nDownload PDF\n\n  * Article\n  * Open access\n  * Published: 15 December 2023\n\n# Ideological asymmetries in online hostility, intimidation, obscenity, and\nprejudice\n\n  * Vivienne Badaan^1,\n  * Mark Hoffarth^2,\n  * Caroline Roper^3,\n  * Taurean Parker^3 &\n  * ...\n  * John T. Jost ORCID: orcid.org/0000-0002-2844-4645^2,3\n\nScientific Reports volume 13, Article number: 22345 (2023) Cite this article\n\n  * 1291 Accesses\n\n  * 1 Altmetric\n\n  * Metrics details\n\n## Abstract\n\nTo investigate ideological symmetries and asymmetries in the expression of\nonline prejudice, we used machine-learning methods to estimate the prevalence\nof extreme hostility in a large dataset of Twitter messages harvested in 2016.\nWe analyzed language contained in 730,000 tweets on the following dimensions\nof bias: (1) threat and intimidation, (2) obscenity and vulgarity, (3) name-\ncalling and humiliation, (4) hatred and/or racial, ethnic, or religious slurs,\n(5) stereotypical generalizations, and (6) negative prejudice. Results\nrevealed that conservative social media users were significantly more likely\nthan liberals to use language that involved threat, intimidation, name-\ncalling, humiliation, stereotyping, and negative prejudice. Conservatives were\nalso slightly more likely than liberals to use hateful language, but liberals\nwere slightly more likely than conservatives to use obscenities. These\nfindings are broadly consistent with the view that liberal values of equality\nand democratic tolerance contribute to ideological asymmetries in the\nexpression of online prejudice, and they are inconsistent with the view that\nliberals and conservatives are equally prejudiced.\n\n### Similar content being viewed by others\n\n### Political polarization of news media and influencers on Twitter in the\n2016 and 2020 US presidential elections\n\nArticle Open access 13 March 2023\n\n### Measuring exposure to misinformation from political elites on Twitter\n\nArticle Open access 21 November 2022\n\n### Partisan asymmetries in exposure to misinformation\n\nArticle Open access 19 September 2022\n\n## Introduction\n\nIt is a common assumption in social science that, as Erikson and Tedin^1 put\nit, \u201cConservatives consider people inherently unequal and worthy of unequal\nrewards,\u201d whereas \u201cliberals are egalitarian\u201d (p. 69). Generations of\nphilosophers, social theorists, and political scientists have argued that a\nfundamental, if not the fundamental, difference between ideologues of the left\nand right concerns egalitarianism: liberal-leftists prioritize social,\neconomic, and political forms of equality, whereas conservative-rightists\naccept existing forms of hierarchy and inequality as legitimate and necessary,\nand perhaps even desirable (e.g.,^2,3,4,5,6,7,8). A stronger commitment to\nequality and tolerance explains evidence that has accumulated over several\ndecades that, on both implicit and explicit measures, political liberals\nexpress less hostility than conservatives toward a wide range of social groups\nthat are frequent targets of prejudice in\nsociety^5,8,9,10,11,12,13,14,15,16,17,18,19,20.\n\nRecently, however, the longstanding idea that liberals are more egalitarian,\nmore tolerant, and less prejudiced than conservatives has come under attack.\nIt has been argued that liberal-leftists are every bit as authoritarian,\nintolerant, and hostile toward dissimilar others as are conservative-\nrightists^21,22,23,24,25,26. The overarching claim is that leftists and\nrightists are equally biased, but they are just biased against different\ngroups^27. There is also an untested assumption in the literature on\n\u201cworldview conflict\u201d and prejudice^27 that conservatives are biased against\nBlack people and women not because of race or gender, but merely because they\nassume that Black people and women are liberal. Thus, whereas rightists are\nsaid to express prejudice against groups that are presumed to be left-leaning\n(such as Black people, atheists, and women), leftists are said to express\nprejudice against groups that are presumably right-leaning (such as\nbusinesspeople, Christians, and men).\n\nThe vast majority of evidence put forward on behalf of the ideological\nsymmetry perspective is based on self-reported attitudes, such as feeling\nthermometer ratings of how \u201ccold\u201d or \u201cwarm\u201d people feel toward specific target\ngroups. A typical, albeit unsurprising finding is that social conservatives\nfeel more warmth toward groups perceived as socially conservative (vs.\nliberal), whereas social liberals feel more warmth toward groups perceived as\nsocially liberal^28. However, we think that there are several major problems\nwith investigating ideological symmetries and asymmetries in prejudice this\nway^29.\n\nTo begin with, most of the research purporting to document ideological\nsymmetries in prejudice merely shows that liberals and conservatives sometimes\nexpress lukewarm attitudes toward specific groups. This body of work relies\nupon what we consider to be a watered-down definition of prejudice as any\n\u201cnegative evaluation... on the basis of group membership,\u201d which \u201cdoes not\ndepend on whether such a prejudice can be justified according to some moral\ncode\u201d (p. 359)^30. This conceptualization departs radically from \u201cclassic\u201d\ndefinitions of prejudice in social psychology, such as Gordon Allport\u2019s^31\ntreatment of prejudice as \u201cthinking ill of others without sufficient warrant\u201d\n(p. 6), that is, \u201can antipathy based upon a faulty and inflexible\ngeneralization... directed toward a group as a whole, or toward an individual\nbecause he is a member of that group\u201d (p. 9). Textbook definitions likewise\nemphasize \u201ca hostile or negative attitude toward a distinguishable group based\non generalizations derived from faulty or incomplete information\u201d (p. 231)^32,\nand \u201can unjustifiable (and usually negative) attitude toward a group and its\nmembers [involving] stereotyped beliefs, negative feelings, and a\npredisposition to discriminatory action\u201d (p. G\u201310)^33. When social scientists\nseek to understand and ameliorate prejudice, we expect that they are not\nconcerned merely with the expression of lukewarm attitudes but with the kind\nof intense, unwarranted negative affect that motivates hostility, hatred,\nintimidation, and discrimination (e.g.,^34).\n\nTo overcome limitations of previous research on the subject, and to\ninvestigate the hypothesis that liberal commitments to equality and democratic\ntolerance would contribute to an ideological asymmetry in expressions of\nhostility, intimidation, and prejudice, we conducted a large-scale\ninvestigation of naturally occurring social media behavior. Specifically, we\nharvested a large corpus of Twitter messages based on keywords that included\nsocial groups that, according to previous research, are common targets of\nliberal prejudice (e.g., Catholics, Whites, wealthy people, and conservatives)\nand conservative prejudice (e.g., Blacks, illegal immigrants, and liberals).\nIn addition, we implemented a Bayesian Spatial Following model to estimate the\nideological positions of Twitter users in our sample, so that we could compare\nthe online behavior of left- and right-leaning social media users. Finally, we\nused a combination of manual and automatic text-coding methods to investigate\nideological asymmetries in the use of language containing (1) threat and\nintimidation, (2) obscenity and vulgarity, (3) name-calling and humiliation,\n(4) hatred and racial, ethnic, or religious slurs, (5) stereotypic\ngeneralizations, and (6) negative prejudicial language. We hypothesized that:\n(HI) tweets mentioning liberal- or left-leaning target groups will contain\nmore expressions of online prejudice than tweets mentioning conservative- or\nright-leaning target groups; and (HII) tweets sent by conservative- and right-\nleaning users will contain more expressions of online prejudice than tweets\nsent by liberal- and left-leaning users.\n\n## Method\n\n### Data collection and inclusion criteria\n\nWe used a supervised machine-learning approach to analyze naturally occurring\nlanguage in a very large number of social media posts sent by liberal-leftists\nand conservative-rightists in reference to groups that have been identified as\nlikely targets of liberal and conservative bias. The population of interest\nwas the set of messages circulated in the U.S. Twittersphere. Between March\nand May 2016, we harvested 733,907 Twitter messages that included one or more\nof the 96 keywords listed in Table 1, including progressives, rightists,\nChristians, civil rights activists, Caucasians, Black people, destitute, and\nrich people. The selection of target groups was based on previous research by\nChambers et al.^23 and Brandt et al.^22, which sought to specify frequent\ntargets of \u201cliberal prejudice\u201d and \u201cconservative prejudice.\u201d For each of the\ntarget groups, we included synonyms, all of which were either hashtags or\nkeywords used on Twitter during the period of data collection. All search\nterms were manually inspected prior to data collection. Some of the terms were\ndeemed by the computer scientists implementing the queries as too common on\nTwitter to be included in the collection, so they were excluded. To filter out\ntweets that contained pornographic content and those written in languages\nother than English, respectively, we included pornography and non-English as\ncategories in the human coding and machine-learning phases. We excluded tweets\nthat, through machine-learning classification, had a probability of containing\npornographic content greater than 0.50 and being non-English greater than\n0.50. This left us with a total sample of 670,973 tweets that were eligible\nfor further analysis.\n\nTable 1 Keywords used to harvest tweets for the data collection.\n\nFull size table\n\n### Ideological estimation\n\nWe used Barber\u00e1\u2019s method of estimating left\u2013right (or liberal-conservative)\nideological positions of Twitter users^36. This method, which has been\nvalidated in a number of ways, employs a Bayesian Spatial Following model that\ntreats ideology as a latent variable estimated on the basis of follower\nnetworks, that is, the number of liberal and conservative political accounts\n(of well-known journalists, politicians, and other political actors) that the\nindividual follows. We were able to calculate point estimates for a total of\n325,717 Twitter users. Scores ranged from -2.611 (very liberal) to 4.668 (very\nconservative), with a mean of 0.369 (SD = 1.724). The mean indicated that, on\naverage, the users in our sample were moderate (neither liberal nor\nconservative). Using this method, 176,948 Twitter users in our sample were\nclassified as liberal-leaning (that is, below zero), and 148,769 were\nclassified as conservative-leaning (above zero).\n\n### Human coding phase\n\nTo train the automatic machine-learning algorithm to classify tweets, it was\nnecessary to first have a subset of them manually coded. Before rating the\ntweets that were used for the machine learning phase, all raters participated\nin a two-hour training session and were taught to follow the same standardized\nprotocol (see Human Coding Manual in Supplementary Material). In the pilot\ncoding phase, seven trained research assistants coded a total batch of 1000\ntweets (500 tweets each) to assess the appropriateness of the coding\ninstructions. We then used their feedback to make clarifications, minor\nrevisions, and edits to the coding manual. In the next phase, 11 trained\nundergraduate and graduate psychology students coded an additional set of 6000\ntweets. The final sample of manually coded tweets therefore consisted of N =\n7000 unique tweets, with each tweet coded by at least three independent\nraters.\n\n#### Coding categories\n\nTo establish our coding scheme, we conducted an extensive literature search on\nstudies of online incivility and the linguistic expression of prejudice.\nIncivility in online discourse is operationally defined in terms of the use of\ndisrespectful language^37,38. Disrespectful language can be broken down\nfurther into the use of obscene language and name-calling or attempts to\nhumiliate the target of the disrespectful language. In the context of\nintergroup relations, incivility may also include the use of aggressive,\nthreatening, or intimidating language. Because a main goal of our research\nprogram was to investigate ideological symmetries and asymmetries in\nprejudice, we estimated the prevalence of negative prejudicial language, which\nis underpinned by stereotypical categorical generalizations expressed in a way\nthat renders them largely immune to counterevidence^11,17,31,34,35. Thus, we\nsought to analyze prejudicial language directed at specific target groups that\nare typically perceived to be left- and right-leaning, respectively. Because\nour dataset was harvested before Twitter expanded its policies against hate\nspeech and hateful conduct in late 2019, we were able to investigate hatred\ndirected at various target groups.\n\nTherefore, research assistants coded the tweets on all of the following\ndimensions: (1) Threat/intimidation: language conveying a threat to use\nphysical violence or intimidation directed at an individual or group; (2)\nObscenity: an offensive word or phrase that would be considered inappropriate\nin professional settings; (3) Hatred: a communication that carries no meaning\nother than the expression of hatred for some social group; (4) Name-\ncalling/humiliation: language directed at an individual or group that is\ndemeaning, insulting, mocking, or intended to create embarrassment; (5)\nStereotypic generalization: false or misleading generalizations about groups\nexpressed in a manner that renders them largely immune to counterevidence; and\n(6) Negative prejudice: an antipathy based on group-based generalizations,\nthat is, an unfavorable feeling \u201ctoward a person or thing, prior to, or not\nbased on, actual experience\u201d (p. 6)^31.\n\nInter-rater reliability coefficients for each of these categories are provided\nin the Online Supplement (Tables S.1\u2013S.8). We used a majority voting method,\nso that if two or more of the three human coders agreed that a given tweet\ncontained hatred, obscenity, prejudice, and so on, it was classified as\nbelonging to the positive class. Coding frequencies estimated for the training\ndata set are summarized in Table S.9 of the Supplement for each of the six\ntheoretical categories (plus the two screening categories).\n\n### Machine-learning phase\n\nTraining, validation, and test sets for the machine-learning phase were based\non the 7000 human-coded tweets. We reserved 20% (1400) of the tweets to use as\na test set to evaluate final model performance. Of the other 5600 tweets, 20%\n(1100) were used for purposes of validation, leaving 4500 tweets with which to\ntrain the models. We used several different text classification strategies,\nincluding \u201cbag of words\u201d models such as the Support Vector Machine (SVM),\nneural networks such as Long Short-Term Memory (LSTM), and transfer learning\ntechniques such as Universal Language Model Fine-Tuning (ULMFiT) and\nBidirectional Encoder Representations from Transformers (BERT). We applied\neach of these strategies to classify the tweets according to the six\ndimensions of classification. For the sake of brevity, we report results from\nthe best performing model, namely BERT. Detailed information about all\nmachine-learning methods and results are provided in the Online Supplement,\nalong with a comparative analysis of the four machine learning models\nemployed.\n\n#### Bidirectional encoder representations from transformers\n\nBERT is an innovative state-of-the-art language representation model^39.\nDeveloped by researchers at Google AI Language, BERT creates a \u201cdeep\nbidirectional representation\u201d of language, which means that the representation\nof the language is contextualized, with each word conditioned on the preceding\nand succeeding words. A traditional language model is built by optimizing an\nobjective function that seeks to accurately predict the next word, given the\npreceding context. BERT instead randomly \u201cmasks\" words and seeks to predict\nthe masked word given the language that precedes and succeeds it.\n\nBERT uses units called transformers, as originally implemented by^40. The\ntransformer is an alternative to convolutional and recurrent architectures\nthat builds on the concept of multi-head attention. Traditional attention\nmechanisms in sequence-to-sequence models establish a correspondence between\nunits of the input and units of the output. Multi-head attention can relate\nparts of a single sequence to each other, within either the input or the\noutput. The BERT model also represents language as word parts, not just full\nword tokens. So, for example, it divides the word \u201cmongering\u201d into \u201cmon,\u201d\n\u201cger,\u201d and \u201cing.\u201d This use of bipartite encodings of words is common in NLP\nresearch, but it is especially important when analyzing Twitter data, which\noften contains misspellings and abbreviations. In addition, Twitter hashtags\nare often comprised of several words combined without a space, so tokenizing\nonly on words properly divided by spaces would be potentially problematic.\n\nTo implement our version of the BERT model, we used the publicly available\nPyTorch code. Although the original authors of BERT used TensorFlow, they have\nformally endorsed the PyTorch implementation, and experiments have verified\nthat it produces identical results^41. We started from the publicly available\nBERT model, pre-trained on the BooksCorpus (800 M words) and English Wikipedia\n(2500 M words). There are two publicly available versions of the BERT model.\nThe large version has 16 attention heads and 24 layers, whereas the base\nversion has 12 attention heads and 12 layers.\n\n## Results\n\n### BERT machine learning model\n\nThe results for tuning the BERT model are shown in Table 2. The creators of\nBERT recommend experimenting with batch sizes of 16 and 32, learning rates of\n5e^\u22125, 3e^\u22125 and 2e^\u22125, and epochs 3 and 4. We ran 6 of the 12 possible\ncombinations, and also experimented with choosing a smaller batch size and\nlearning rate than BERT\u2019s authors would typically recommend. All results\ndescribed below are based on the large pre-trained BERT model. An undefined\nF-score occurs when no correct positive class predictions are made. Because\nour classes were highly imbalanced, this usually indicates that the model did\nnot predict any positive incidences. The tuning results indicated that 3\nepochs, a learning rate of 2e^\u22125, and a batch size of 16 performed well.\nHowever, when we ran this tuned model on the other category labels, we\nencountered several degenerate results by using the \u201clarge\u201d model on a small\ndataset. Obscenity, name-calling, negative prejudice, and non-English all\nproduced undefined F-scores. The creators of BERT overcame the problem of\ndegenerate results by experimenting with several random initializations until\none version succeeded. We instead examined validation scores using the same\nparameters on the \u201cbase\u201d version of the model.\n\nTable 2 Validation F-scores from the BERT Model.\n\nFull size table\n\n### Hypothesis testing\n\nIn Table 3 we display the number and percentage of all tweets that, according\nto machine-learning analyses, contained each of the categories of linguistic\nbias. Here we define a tweet containing a positive instance as that with p\n(category) > 0.50. Negative prejudice\u2014the expression of hostile or unfavorable\nattitudes on the basis of categorical group membership\u2014was present in 13.0% of\nthe tweets in our sample (N = 87,250). Hateful speech was the least common\ncategory, with 2.20% of the Tweets (N = 14,690) containing positive instances.\n\nTable 3 Number and percentage of tweets containing positive instances of each\nlinguistic category according to machine-learning analyses of the complete\ndata set.\n\nFull size table\n\n#### Target group effects\n\nWe hypothesized that messages referring to liberal or left-leaning target\ngroups would contain more indicators of linguistic bias than messages\nreferring to conservative or right-leaning target groups. Because it was not\nnecessary to restrict this analysis to messages sent by users for whom we were\nable to classify their ideological position, we conducted this analysis based\non the larger sample of 670,973 tweets. The perceived ideological leanings of\nthe various target groups were estimated based on data from Chambers et al.\n(Sample 1)^22, as graphed by Brandt et al. (Fig. 2)^21.\n\nTarget ideology scores ranged from 1.29 (very liberal) to 4.65 (very\nconservative), with a mean of 2.876 (SD = 1.108). As hypothesized, target\nideology was significantly and negatively associated with each of the\nlinguistic bias categories (see Table 4). That is, the more liberal/leftist\nthe target group was perceived to be, the more likely it was for tweets\nmentioning that group to contain hatred, threatening language, obscenity,\nname-calling, stereotyping, and negative prejudice. Most of the correlations\nwere relatively small, but all were statistically significant at p < 0.001.\nThe two largest effect sizes were for name-calling (r = \u22120.146) and the\nexpression of negative prejudice (r = \u22120.126).\n\nTable 4 Bivariate correlations between target ideology (groups that were\nperceived as more conservative/rightist) and the expression of linguistic bias\noverall (N = 670,973 tweets).\n\nFull size table\n\n#### Communicator effects\n\nNext, we investigated the effects of user ideology on linguistic bias. This\nanalysis was based on the subset of messages (n = 325,717) sent by users who\ncould be classified as liberal or conservative. As shown in Table 5,\nconservative Twitter users were more likely than liberal Twitter users to\ncommunicate negative prejudice (r = 0.210), name-calling (r = 0.146),\nstereotypes (r = 0.110), and threatening language (r = 0.092), all ps < 0.001.\nConservatives were slightly more likely to use hateful language (r = 0.011),\nwhereas liberals were slightly more likely to use obscenity (r = \u22120.010); both\nof these effects were quite small but, because of the very large sample size,\nstill significant at p < 0.001.\n\nTable 5 Correlations between user ideology (twitter users who were classified\nas more conservative/rightist) and the expression of linguistic bias, both\noverall and against specific target groups.\n\nFull size table\n\n#### Communicator effects analyzed separately for liberal versus conservative\ntarget groups\n\nNext, we inspected correlations between user ideology and linguistic bias\ndirected at groups that were generally perceived to be liberal or left-leaning\nvs. conservative or right-leaning, respectively (see Table 5). For the\nsubsample of tweets that mentioned liberal-leftist groups (n = 229,788), which\ncomprised 70.5% of the total number of tweets in our collection, users who\nwere classified as more conservative were more likely to express negative\nprejudice (r = 0.247), to engage in name-calling (r = 0.191), and to include\nthreats (r = 0.123), stereotypes (r = 0.116), and hatred (r = 0.021), all ps <\n0.001. There was no effect of user ideology on the use of obscenity (r =\n0.003, p = 0.119).\n\nFor the much smaller subsample of tweets that mentioned conservative-rightist\ngroups (n = 95,929), more liberal users were slightly more likely to express\nobscenity (r = \u22120.047) and hatred (r = \u22120.026), both ps < 0.001. However, for\nthe remaining categories, conservative Twitter users were actually more likely\nthan liberal Twitter users to express linguistic bias. That is, even when\nwriting about groups that are generally considered to be right-leaning,\nconservatives were more likely to communicate negative prejudice (r = 0.118),\nstereotypes (r = 0.096), name-calling (r = 0.025), and threatening language (r\n= 0.021), all ps < 0.001.\n\n### Sensitivity analyses\n\nWe conducted additional sensitivity analyses to determine whether the results\nand their interpretation was impacted by analytic decisions. Specifically, we\nre-coded the continuous estimates for linguistic bias into binary, categorical\nvariables (< 50% probability = does not contain biased language, \u2265 50%\nprobability = does not contain biased language) and conducted regression\nanalyses. Results were very similar to those described above.\n\nTweets that mentioned liberal-leaning groups were more likely to contain\nhatred (b = \u2212 0.45, SE(b) = 0.008, Wald = 2833.26), threats (b = \u2212 0.26, SE(b)\n= 0.006, Wald = 1697.99), obscenity (b = \u2212 0.23, SE(b) = 0.006, Wald =\n1710.62), name calling (b = \u2212 0.36, SE(b) = 0.004, Wald = 6783.19),\nstereotypes (b = \u2212 0.22, SE(b) = 0.004, Wald = 2480.92), and negative\nprejudice (b = \u2212 0.272, SE(b) = 0.003, Wald = 6386.04), all ps < 0.001.\n\nWe also compared the frequencies (percentages) of messages about various\ntarget groups that contained each type of linguistic bias. Tweets about left-\nleaning (vs. right-leaning) groups were again more likely to contain hatred\n(3.5% vs. 0.5%, \u03c7^2 = 6882.25), threats (4.1% vs. 2.9%, \u03c7^2 = 749.48),\nobscenity (5.8% vs. 2.8%, \u03c7^2 = 3521.28), name calling (10.4% vs. 4.8%, \u03c7^2 =\n7342.45), stereotypes (7.9% vs. 5.7%, \u03c7^2 = 1254.16), and negative prejudice\n(15.4% vs. 10.1%, \u03c7^2 = 4205.00), all ps < 0.001.\n\nFinally, we examined whether user ideology was related to the percentage of\nmessages containing linguistic bias. Tweets sent by more conservative users\nhad a higher probability of containing hateful language (b = 0.049, SE(b) =\n0.008, Wald = 35.76), threats (b = 0.225, SE(b) = 0.005, Wald = 2055.62), name\ncalling (b = 0.210, SE(b) = 0.003, Wald = 3766.95), stereotypes (b = 0.134,\nSE(b) = 0.003, Wald = 1475.32), and negative prejudice (b = 0.26, SE(b) =\n0.003, Wald = 9125.68), all ps < 0.001. There was no statistically significant\neffect of user ideology in the use of obscene language (b = \u2212 0.007, SE(b) =\n0.006, Wald = 1.81, p = 0.179).\n\n### Ideology of the coders\n\nBecause we were concerned that the political orientations of the raters could\nbias their coding, we asked the research assistants to answer three questions\nabout their general political orientation (\u201cPlease indicate on the scale below\nhow liberal or conservative [in terms of your general outlook] you are\u201d),\nsocial attitudes (\u201cHow liberal or conservative do you tend to be when it comes\nto social policy?\u201d), and economic attitudes (\u201cHow liberal or conservative do\nyou tend to be when it comes to economic policy?\u201d). Responses could range from\n1 (very liberal) to 7 (very conservative). The 8 (of 11) raters who answered\nthese questions were liberal leaning on average, M = 2.46 (SD = 1.05).\n\nWe examined point-biserial correlations between coders\u2019 ideology scores and\ntheir rating of each linguistic category under study for every batch of\ntweets. We found that rater ideology was unrelated to the criterion linguistic\ncategory used to train the machine learning algorithm, i.e., hateful language\n(r = 0.009, p = 0.139). Rater ideology was also unrelated to the detection of\nthreatening language in the training tweets (r = 0.011, p = 0.079). At the\nsame time, the more conservative our raters were, the more likely they were to\ndetect obscenity (r = 0.022, p < 0.001), whereas the more liberal our raters\nwere, the more likely they were to detect name-calling (r = \u2212 0.028, p <\n0.001), stereotypes (r = \u2212 0.136, p < 0.001), and negative prejudice (r = \u2212\n0.111, p < 0.001). Thus, coder ideology was inconsistently related to the use\nof various coding categories. Most importantly, ideology of the raters was\nunrelated to their ratings of hatred, which was used as the base linguistic\nmodel for training the other categories. It is also worth highlighting the\nfact that the classification and labeling process for the machine learning\ntraining relied on majority voting, so that at least two annotators must have\nagreed that the tweets contained hatred, obscenity, etc., before it was\nlabeled as belonging to the positive class.\n\n## General discussion\n\n### Summary of findings and their implications\n\nIn this study, we investigated the question of whether online prejudice is\nsymmetrical or asymmetrical on the political left and right in the U.S. in a\nvery large sample of social media messages. We observed that Twitter messages\nmentioning targets perceived as liberal or left-leaning (such as Black\nAmericans and feminists) included higher levels of hate speech, threat,\nobscenity, name-calling, stereotyping, and negative prejudice, compared to\nTwitter messages mentioning targets perceived as conservative or right-leaning\n(such as conservatives and Christians). These results supported (HI).\n\nWe estimated user ideology scores based on Barber\u00e1\u2019s method^36 and observed\nthat whereas liberal users were slightly more likely than their conservative\ncounterparts to use obscene language, conservatives were more likely to use\nnegative prejudice, name-calling, and hateful and threatening language,\nalthough the effect sizes for the last two categories were very small. Perhaps\nthe most important finding is that conservatives were more likely than\nliberals to use negative prejudicial language, and that negative prejudice was\nexpressed more strongly in tweets mentioning purportedly left-leaning targets\nthan in tweets mentioning right-leaning targets. These results are clearly\nconsistent with (HII) and inconsistent with the alternative hypothesis that\nprejudice is symmetrical on the left and right^21,22,23,24,25,26,27,28,42.\nInstead, they reinforce the long-standing, empirically supported conclusion\nthat out-group prejudice is more prevalent on the right than the\nleft^9,10,11,12,13,14,15,17,18,19,29.\n\nBecause we measured the spontaneous use of language in a naturally occurring\n\u201creal-world\u201d setting, our results go well beyond what can be concluded based\non studies using feeling thermometer measures of prejudice, which are subject\nto norms of socially desirable responding (for a critique of previous research\nin this area, see^29). Our findings are also consistent with two other major\nstudies of prejudicial outcomes in society. First, an analysis of FBI hate-\ncrime data from 1996 to 2018 revealed that ostensibly left-leaning targets\nsuch as racial, religious, and sexual minorities were subjected to much higher\nlevels of hate crime than ostensibly right-leaning targets, such as racial,\nreligious, and sexual majorities^29. Thus, group-based discrimination, which\nis an obvious manifestation of out-group prejudice, disproportionately affects\ndisadvantaged target groups who are perceived as left-leaning in political\norientation. Second, a comprehensive study of political violence carried out\nin the US between 1948 and 2018 showed that individuals who were affiliated\nwith left-wing extremist movements had 68% lower odds of engaging in violent\nbehavior, compared to individuals affiliated with right-wing extremist\nmovements^13. Thus, in these previous investigations, and in our present\nstudy, rightists were much more likely to be perpetrators of prejudice, and\nleftists were much more likely to be victims of prejudice. This is consistent\nwith the view that substantial left\u2013right ideological asymmetries exist when\nit comes to the thoughts, feelings, and behaviors of individuals and the\nsocial groups to which they belong (see^5).\n\n### Strengths and limitations\n\nOne strength of the present research program, which we alluded to above, is\nthat it is high in external validity. This is because we unobtrusively\nobserved the spontaneous language used by liberals and conservatives in actual\nsocial media communications referring to target groups that are perceived as\nleft-leaning vs. right-leaning. Furthermore, by observing the expression of\nprejudice in vivo, focusing on naturally produced language, we avoided several\ncommon methodological artifacts that frequently hamper social psychological\nresearch on bias and prejudice, such as problems of experimenter bias and\nsocially desirable responding. Another advantage of this study is that the\nfinal sample size of messages analyzed was very large (N = 670,973), rendering\nour estimates both highly stable and robust.\n\nYet another strength of our study is that we used cutting-edge machine\nlearning methods in data science to investigate social psychological\nhypotheses and, in particular, to classify linguistic phenomena, such as the\nexpression of negative prejudice, that have historically been very difficult\nto classify using objective methods. In the process of developing our\ncomputational model, we generated a set of 7000 labelled tweets that is\navailable for future researchers to train their own machine learning models.\nAll of these tweets were rated by three different human coders, so that we\ncould ensure high levels of interrater reliability before training our various\nmachine-learning algorithms. Although the procedure was both time- and\nresource-intensive, it increased the accuracy of predictions made by the\nmachine-learning models. We have emphasized results based on the best-\nperforming algorithmic model (BERT) in this article, but the data scientists\non our team tested and fine-tuned four different classification models. The\nmethods and results associated with these other algorithms are described in\nthe Online Supplement.\n\nOf course, this study also has its limitations. For one thing, the Twitter API\nlimited the number of data queries we were able to submit during the period of\ndata collection, which means that the dataset does not include all potentially\nrelevant tweets sent during the period of investigation. However, we were able\nto collect a random sample of the total population of tweets sent during the\nperiod in question. The Twitter messages we harvested were from March to May\nof 2016. This was before the primary and presidential elections of 2016, which\nmeans that it was prior to Donald Trump\u2019s nomination and eventual election to\nthe presidency. Given the intensity of Trump\u2019s public rhetoric against many of\nthe left-leaning target groups listed in Table 1 (especially immigrants,\nracial minorities, liberals, and leftists), and the uptick in hate crimes and\nother cases of prejudice and discrimination that accompanied his presidency,\ne.g., see^43,44,45,46, the timing of our investigation means that we may have\nunderestimated the true extent of online bias and harassment committed by\nrightists against target groups that are perceived as left-leaning in the\nperiod that immediately followed our investigation.\n\nAnother technical limitation concerns the performance of our optimal machine\nlearning algorithm. Although the algorithm had high f-scores with respect to\nhatred, obscenity, and name calling, it performed less than optimally with\nrespect to the categories of negative prejudice, threat, and stereotyping.\nThis could be attributable to (a) the difficulty in detecting relatively\n\u201cfuzzy\u201d concepts; (b) the fact that our operationalization of stereotypes\nincluded all group-based generalizations, not only negative group-based\ngeneralizations; and/or (c) an insufficient amount of training data, although\nthe research team coded as many tweets as was logistically feasible giving\ntiming and other constraints. Future research would do well to overcome these\nlimitations by (a) using sentiment analysis to code the valence of the\nattitudes in the tweets; (b) focusing exclusively on negative stereotypes; and\n(c) annotating a larger corpus of training tweets. Despite the limitations of\nour study, we believe that it is the first of its kind to use robust machine-\nlearning models to assess multiple indicators of online prejudice.\n\nAs in every other study of social media communication, our analysis is highly\ndependent upon the selection of keywords and search terms used to construct\nthe data set. We first selected social groups based on previous research to\nidentify potential targets of \u201cliberal prejudice\u201d and \u201cconservative prejudice\u201d\nand then generated synonyms for those groups^22,23. However, some words and\nphrases (such as \u201cDemocrats\u201d and \u201cRepublicans\u201d) were determined by our\ncomputer technicians to occur too frequently in the total population of\ntweets; these were dropped to make the data collection more manageable.\nAlthough this did introduce some degree of selectivity in the search terms\nused, we note that the data set is based on 96 words and phrases, which is an\nextremely large sample of keywords compared to other studies of online\nhostility and prejudice.\n\nThe non-experimental study design prohibits the drawing of causal conclusions\nabout the nature of ideology and prejudice. Moreover, there are several third\nvariables\u2014such as intelligence, education, authoritarianism, social dominance\norientation, system justification, and the like\u2014that may help to explain why\nconservative-rightists express more online prejudice than liberal-leftists\n(e.g., see^5,8,9,10,12,47,48). Future research would do well to measure these\nas mediating or moderating variables.\n\nThe fact that our analyses are confined to a single social media platform is\nyet another limitation. Because Twitter changes its policies regarding the\nremoval of potentially prejudicial content every few years, our analysis was\nbounded by their terms of service during the period of data collection.\nAccording to the results of a Pew Survey in 2021, Twitter users tend to be\nyounger and more Democratic, compared to the public at large. Therefore,\nalthough our sample is much larger and more representative of the general\npopulation than in studies of prejudice based on convenience samples, we do\nnot know how well these results would generalize to the population of U.S.\nadults.\n\nIt would be useful to conduct parallel studies about the role of political\nideology in the expression of prejudice on other platforms, such as Facebook,\nInstagram, and Reddit, as well as social media channels that are favored by\nright-wingers, such as 4chan, Parler, and Trump\u2019s own social media platform,\nTruth Social. Some of these more recent social media platforms (especially\nParler and Truth Social) were created specifically to combat what right-wing\nopinion leaders claimed to be a crackdown on free speech. On such platforms,\nhateful and prejudicial language may be entirely unfiltered, making them well-\nsuited for empirical research into the connection between ideology and online\nprejudice.\n\n## Concluding remarks\n\nWe believe that it is an appropriate time for social scientists to take stock\nand reflect on the question of how and why it is we study prejudice and\ndiscrimination in the first place. Initially, research in this area arose from\nthe (belated) historical acknowledgement of exploitation and oppression faced\nby certain groups, such as racial, ethnic, religious, and sexual minorities,\nand perpetuated, generally speaking, by members of majority groups that were\nrelatively high in social status, power, and material resources\n(e.g.,^9,31,49). Many recent contributions to the debate about ideological\nsymmetry vs. asymmetry in bias and prejudice are strikingly ahistorical and,\nit seems to us, lacking an appreciation of structural inequalities in society\n(e.g.,^21,22,23,24,25,26,27,28,42). We contend that it is impossible to\nproperly understand these phenomena without appreciating the significance of\nboth longstanding and current imbalances of power and material resources in\nthe overarching social system (e.g., see^5,48). Our research program is\noffered as a wake-up call to those who would seek to strip the study of\nprejudice of its historical and social-structural origins in a na\u00efve and,\nindeed, we would argue, ultimately futile attempt to de-politicize and\n\u201cneutralize\u201d the subject matter (see also^50 for a similar critique of\nsymmetrical approaches to the study of political polarization).\n\n## Data availability\n\nData from the human coding phase, as well as the final dataset from the\nmachine learning phase are available via\nhttps://osf.io/6kj5s/?view_only=67175bcd980c444bbf47fb5b44dd8424.\n\n## References\n\n  1. Erikson, R. S. & Tedin, K. L. American Public Opinion: Its Origins, Content, and Impact (Routledge, 2019).\n\nBook Google Scholar\n\n  2. Bobbio, N. Left and Right: The Significance of Political Distinction (University of Chicago Press, 1996).\n\nGoogle Scholar\n\n  3. Inglehart, R. F. Culture Shift in Advanced Industrial Society (Princeton University Press, 1990).\n\nBook Google Scholar\n\n  4. Jacoby, W. G. Is there a culture war? Conflicting value structures in American public opinion. Am. Polit. Sci. Rev. 108, 754\u2013771. https://doi.org/10.1017/S0003055414000380 (2014).\n\nArticle Google Scholar\n\n  5. Jost, J. T. Left & Right: The Psychological Significance of Political Distinctions (Oxford University Press, 2021).\n\nGoogle Scholar\n\n  6. Lipset, S. M., Lazarsfelt, P., Barton, A. & Linz, J. The psychology of voting: An analysis of political behavior. In Handbook of Social Psychology (ed. Lindzey, G.) 1124\u20131175 (Addison Wesley, 1954/1962).\n\n  7. Lupton, R. N., Smallpage, S. M. & Enders, A. M. Values and political predispositions in the age of polarization: Examining the relationship between partisanship and ideology in the United States, 1988\u20132012. Br. J. Soc. Psychol. 50, 241\u2013260. https://doi.org/10.1017/S0007123417000837 (2020).\n\nArticle Google Scholar\n\n  8. Sidanius, J. & Pratto, F. Social Dominance: An Intergroup Theory of Social Hierarchy and Oppression (Cambridge University Press, 2001).\n\nGoogle Scholar\n\n  9. Adorno, T., Frenkel-Brenswick, E., Levinson, D. J. & Sanford, R. N. The Authoritarian Personality (Harpers, 1950).\n\nGoogle Scholar\n\n  10. Cunningham, W. A., Nezlek, J. B. & Banaji, M. R. Implicit and explicit ethnocentrism: Revisiting the ideologies of prejudice. Pers. Soc. Psychol. B 30, 1332\u20131346. https://doi.org/10.1177/0146167204264654 (2004).\n\nArticle Google Scholar\n\n  11. Federico, C. M. & Sidanius, J. Racism, ideology, and affirmative action revisited: The antecedents and consequences of \u201cprincipled objections\u201d to affirmative action. J. Pers. Soc. Psychol. 82, 488\u2013502. https://doi.org/10.1037/0022-3514.82.4.488 (2002).\n\nArticle PubMed Google Scholar\n\n  12. Hodson, G. & Busseri, M. A. Bright minds and dark attitudes: Lower cognitive ability predicts greater prejudice through right-wing ideology and low intergroup contact. Psychol. Sci. 23, 187\u2013195. https://doi.org/10.1177/0956797611421206 (2012).\n\nArticle PubMed Google Scholar\n\n  13. Jasko, K., LaFree, G., Piazza, J. & Becker, M. H. A comparison of political violence by left-wing, right-wing, and Islamist extremists in the United States and the world. Proc. Natl. Acad. Sci. USA 119, e2122593119. https://doi.org/10.1073/pnas.2122593119 (2022).\n\nArticle CAS PubMed PubMed Central Google Scholar\n\n  14. Jost, J. T., Banaji, M. R. & Nosek, B. A. A decade of system justification theory: Accumulated evidence of conscious and unconscious bolstering of the status quo. Polit. Psychol. 25, 881\u2013919. https://doi.org/10.1111/j.1467-9221.2004.00402.x (2004).\n\nArticle Google Scholar\n\n  15. Kite, M. E. & Whitley, B. E. Psychology of Prejudice and Discrimination (Routledge, 2016).\n\nBook Google Scholar\n\n  16. Rokeach, M. The Open and Closed Mind: Investigations into the Nature of Belief Systems and Personality Systems (Basic Books, 1960).\n\nGoogle Scholar\n\n  17. Sears, D. O. & Henry, P. J. The origins of symbolic racism. J. Pers. Soc. Psychol. 85, 259\u2013275. https://doi.org/10.1037/0022-3514.85.2.259 (2003).\n\nArticle PubMed Google Scholar\n\n  18. Sibley, C. G. & Duckitt, J. Personality and prejudice: A meta-analysis and theoretical review. Pers. Soc. Psychol. Rev. 12, 248\u2013279. https://doi.org/10.1177/1088868308319226 (2008).\n\nArticle PubMed Google Scholar\n\n  19. Sidanius, J., Pratto, F. & Bobo, L. Racism, conservatism, affirmative action, and intellectual sophistication: A matter of principled conservatism or group dominance?. J. Pers. Soc. Psychol. 70, 476\u2013490. https://doi.org/10.1037/0022-3514.70.3.476 (1996).\n\nArticle Google Scholar\n\n  20. Morehouse, K. N., Maddox, K. & Banaji, M. R. All human social groups are human, but some are more human than others: A comprehensive investigation of the implicit association of \u201cHuman\u201d to US racial/ethnic groups. PNAS https://doi.org/10.1073/pnas.2300995120 (2023).\n\nArticle PubMed PubMed Central Google Scholar\n\n  21. Brandt, M. J. Predicting ideological prejudice. Psychol. Sci. 28, 713\u2013722. https://doi.org/10.1177/0956797617693004 (2017).\n\nArticle PubMed Google Scholar\n\n  22. Brandt, M. J., Reyna, C., Chambers, J. R., Crawford, J. T. & Wetherell, G. The ideological-conflict hypothesis: Intolerance among both liberals and conservatives. Curr. Dir. Psychol. Sci. 23, 27\u201334. https://doi.org/10.1177/0963721413510932 (2014).\n\nArticle Google Scholar\n\n  23. Chambers, J. R., Schlenker, B. R. & Collisson, B. Ideology and prejudice: The role of value conflicts. Psychol. Sci. 24, 140\u2013149. https://doi.org/10.1177/0956797612447820 (2013).\n\nArticle PubMed Google Scholar\n\n  24. Conway, L. G. III., Houck, S. C., Gornick, L. J. & Repke, M. A. Finding the Loch Ness monster: Left-wing authoritarianism in the United States. Polit. Psychol. 39, 1049\u20131067. https://doi.org/10.1111/pops.12470 (2018).\n\nArticle Google Scholar\n\n  25. Costello, T. H., Clark, C. J. & Tetlock, P. E. Shoring up the shaky psychological foundations of a micro-economic model of ideology: Adversarial collaboration solutions. Psychol. Inq. 33, 88\u201394. https://doi.org/10.1080/1047840X.2022.2065130 (2022).\n\nArticle Google Scholar\n\n  26. Crawford, J. T. & Pilanski, J. M. Political intolerance, right and left. Polit. Psychol. 35, 841\u2013851. https://doi.org/10.1111/j.1467-9221.2012.00926.x (2014).\n\nArticle Google Scholar\n\n  27. Brandt, M. J. & Crawford, J. T. Worldview conflict and prejudice. Adv. Exp. Soc. Psychol. 61, 1\u201366. https://doi.org/10.1016/bs.aesp.2019.09.002 (2020).\n\nArticle CAS Google Scholar\n\n  28. Crawford, J. T., Brandt, M. J., Inbar, Y., Chambers, J. R. & Motyl, M. Social and economic ideologies differentially predict prejudice across the political spectrum, but social issues are most divisive. J. Pers. Soc. Psychol. 112, 383\u2013412. https://doi.org/10.1037/pspa0000074 (2017).\n\nArticle PubMed Google Scholar\n\n  29. Badaan, V. & Jost, J. T. Conceptual, empirical, and practical problems with the claim that intolerance, prejudice, and discrimination are equivalent on the political left and right. Curr. Opin. Behav. Sci. 34, 229\u2013238. https://doi.org/10.1016/j.cobeha.2020.07.007 (2020).\n\nArticle Google Scholar\n\n  30. Crandall, C. S., Eshleman, A. & Orien, L. Social norms and the expression and suppression of prejudice: The struggle for internalization. J. Pers. Soc. Psychol. 82, 359\u2013378. https://doi.org/10.1037/0022-3514.82.3.359 (2002).\n\nArticle PubMed Google Scholar\n\n  31. Allport, G. W. The Nature of Prejudice (Addison-Wesley, 1954/1990).\n\n  32. Aronson, E. The Social Animal (W. H. Freeman and Company, 1988).\n\nGoogle Scholar\n\n  33. Myers, D. G. Social Psychology (McGraw-Hill, 1995).\n\nGoogle Scholar\n\n  34. Dovidio, J. F., Gaertner, S. L. & Pearson, A. R. On the nature of prejudice: The psychological foundations of hate. In The Psychology of Hate (ed. Sternberg, R. J.) 211\u2013234 (American Psychological Association, 2005).\n\nChapter Google Scholar\n\n  35. Blum, L. Stereotypes and stereotyping: A moral analysis. Philos. Pap. 33, 251\u2013289. https://doi.org/10.1080/05568640409485143 (2004).\n\nArticle Google Scholar\n\n  36. Barber\u00e1, P. Birds of the same feather tweet together: Bayesian ideal point estimation using Twitter data. Polit. Anal. 23, 76\u201391. https://doi.org/10.1093/pan/mpu011 (2015).\n\nArticle Google Scholar\n\n  37. Coe, K., Kenski, K. & Rains, S. A. Online and uncivil? Patterns and determinants of incivility in newspaper website comments. J. Commun. 64, 658\u2013679 (2014).\n\nArticle Google Scholar\n\n  38. Rossini, P. G. C. Disentangling uncivil and intolerant discourse. In A Crisis of Civility? Contemporary Research on Civility, Incivility and Political Discourse (eds Boatright, R. G. et al.) 142\u2013215 (Routledge, 2019).\n\nGoogle Scholar\n\n  39. Devlin, J., Chang, M. W., Lee, K. & Toutanova, K. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv:1810.04805 (2018). https://doi.org/10.48550/arXiv.1810.04805\n\n  40. Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Process. Syst. 30, 5998\u20136008 (2017).\n\nGoogle Scholar\n\n  41. Wolf, T. & Sand, V. . PyTorch Pretrained BERT (2018). https://github.com/huggingface/pytorch-pretrained-BERT\n\n  42. Wetherell, G. A., Brandt, M. J. & Reyna, C. Discrimination across the ideological divide: The role of value violations and abstract values in discrimination by liberals and conservatives. Soc. Psychol. Pers. Sci. 4, 658\u2013667. https://doi.org/10.1177/1948550613476096 (2013).\n\nArticle Google Scholar\n\n  43. Crandall, C. S., Miller, J. M. & White, M. H. Changing norms following the 2016 US presidential election: The Trump effect on prejudice. Soc. Psychol. Pers. Sci. 9, 186\u2013192 (2018).\n\nArticle Google Scholar\n\n  44. Edwards, G. S. & Rushin, S. The effect of President Trump\u2019s election on hate crimes. SSRN. https://doi.org/10.2139/ssrn.3102652\n\n  45. Newman, B. et al. The Trump effect: An experimental investigation of the emboldening effect of racially inflammatory elite communication. Br. J. Polit. Sci. 51, 1138\u20131159. https://doi.org/10.1017/S0007123419000590 (2021).\n\nArticle Google Scholar\n\n  46. Ruisch, B. C. & Ferguson, M. J. Changes in Americans\u2019 prejudices during the presidency of Donald Trump. Nat. Hum. Behav. 6, 656\u2013665. https://doi.org/10.1038/s41562-021-01287-2 (2022).\n\nArticle PubMed Google Scholar\n\n  47. Ganzach, Y. & Schul, Y. Partisan ideological attitudes: Liberals are tolerant; the intelligent are intolerant. J. Pers. Soc. Psychol. 120, 1551\u20131566. https://doi.org/10.1037/pspi0000324 (2021).\n\nArticle PubMed Google Scholar\n\n  48. Jost, J. T. A Theory of System Justification (Harvard University Press, 2020).\n\nBook Google Scholar\n\n  49. Myrdal, G. An American Dilemma: The Negro Problem and Modern Democracy (Harper, 1944).\n\nGoogle Scholar\n\n  50. Kreiss, D. & McGregor, S. C. A review and provocation: On polarization and platforms. New Media Soc. 14, 61. https://doi.org/10.1177/1461444823116188 (2023).\n\nArticle Google Scholar\n\nDownload references\n\n## Acknowledgements\n\nThe authors gratefully acknowledge financial support from the INSPIRE program\nof the National Science Foundation (Award # SES-1248077 and SES-1248077-001).\nWe would also like to thank the 11 research assistants who manually annotated\nthousands of tweets for this project.\n\n## Author information\n\n### Authors and Affiliations\n\n  1. Department of Psychology, American University of Beirut, Beirut, Lebanon\n\nVivienne Badaan\n\n  2. Department of Psychology, New York University, 6 Washington Place, 5th Floor, New York, NY, 10003, USA\n\nMark Hoffarth & John T. Jost\n\n  3. Center for Data Science, New York University, 726 Broadway, 7th Floor, New York, NY, 10003, USA\n\nCaroline Roper, Taurean Parker & John T. Jost\n\nAuthors\n\n  1. Vivienne Badaan\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  2. Mark Hoffarth\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  3. Caroline Roper\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  4. Taurean Parker\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  5. John T. Jost\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n### Contributions\n\nV.B., J.T.J., and M.H. contributed to theorizing, research design, and writing\nof the manuscript. C.R. and T.P. were responsible for the machine-learning\nportion and drafting the corresponding section on methodology. V.B. and M.H.\nanalyzed the data. V.B prepared the tables, figures, and supplementary\nmaterials.\n\n### Corresponding author\n\nCorrespondence to Vivienne Badaan.\n\n## Ethics declarations\n\n### Competing interests\n\nThe authors declare no competing interests.\n\n## Additional information\n\n### Publisher's note\n\nSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional affiliations.\n\n## Supplementary Information\n\n### Supplementary Information.\n\n## Rights and permissions\n\nOpen Access This article is licensed under a Creative Commons Attribution 4.0\nInternational License, which permits use, sharing, adaptation, distribution\nand reproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the\nCreative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article's\nCreative Commons licence, unless indicated otherwise in a credit line to the\nmaterial. If material is not included in the article's Creative Commons\nlicence and your intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this licence, visit\nhttp://creativecommons.org/licenses/by/4.0/.\n\nReprints and permissions\n\n## About this article\n\n### Cite this article\n\nBadaan, V., Hoffarth, M., Roper, C. et al. Ideological asymmetries in online\nhostility, intimidation, obscenity, and prejudice. Sci Rep 13, 22345 (2023).\nhttps://doi.org/10.1038/s41598-023-46574-2\n\nDownload citation\n\n  * Received: 12 December 2022\n\n  * Accepted: 02 November 2023\n\n  * Published: 15 December 2023\n\n  * DOI: https://doi.org/10.1038/s41598-023-46574-2\n\n### Share this article\n\nAnyone you share the following link with will be able to read this content:\n\nSorry, a shareable link is not currently available for this article.\n\nProvided by the Springer Nature SharedIt content-sharing initiative\n\n### Subjects\n\n  * Human behaviour\n  * Psychology\n\n## Comments\n\nBy submitting a comment you agree to abide by our Terms and Community\nGuidelines. If you find something abusive or that does not comply with our\nterms or guidelines please flag it as inappropriate.\n\nDownload PDF\n\n## Associated content\n\nCollection\n\n### Social biases and discrimination\n\nAdvertisement\n\nScientific Reports (Sci Rep) ISSN 2045-2322 (online)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * protocols.io\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n  * Cancel contracts here\n\n\u00a9 2024 Springer Nature Limited\n\nSign up for the Nature Briefing newsletter \u2014 what matters in science, free to\nyour inbox daily.\n\nGet the most important science stories of the day, free in your inbox. Sign up\nfor Nature Briefing\n\n", "frontpage": false}
