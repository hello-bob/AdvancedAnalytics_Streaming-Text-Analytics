{"aid": "40081194", "title": "Imagine Flash: Accelerating Emu Diffusion Models with Backward Distillation", "url": "https://ai.meta.com/research/publications/imagine-flash-accelerating-emu-diffusion-models-with-backward-distillation/", "domain": "meta.com", "votes": 2, "user": "davidbarker", "posted_at": "2024-04-18 21:59:01", "comments": 0, "source_title": "Imagine Flash: Accelerating Emu Diffusion Models with Backward Distillation | Research - AI at Meta", "source_text": "Imagine Flash: Accelerating Emu Diffusion Models with Backward Distillation | Research - AI at Meta\n\n  * Our approach\n\n  * Research\n\n  * Meta AI\n\n  * Meta Llama\n\n  * Blog\n\n  * Try Meta AI\n\nBACK\n\n  * About us\n  * Responsibility\n  * People\n  * Careers\n\n  * Overview\n  * Infrastructure\n  * Resources\n  * Demos\n\nClear\n\n  * Clear\n\n  * Our approach\n\n>\n\n  * Research\n\n>\n\n  * Meta AI\n  * Meta Llama\n  * Blog\n\nTry Meta AI\n\n#### COMPUTER VISION\n\n# Imagine Flash: Accelerating Emu Diffusion Models with Backward Distillation\n\nApril 18, 2024\n\n## Abstract\n\nDiffusion models are a powerful generative framework, but come with expensive\ninference. Existing acceleration methods often compromise image quality or\nfail under complex conditioning when operating in an extremely low-step\nregime. In this work, we propose a novel distillation framework tailored to\nenable high-fidelity, diverse sample generation using just one to three steps.\nOur approach comprises three key components: (i) Backward Distillation, which\nmitigates training-inference discrepancies by calibrating the student on its\nown backward trajectory; (ii) Shifted Reconstruction Loss that dynamically\nadapts knowledge transfer based on the current time step; and (iii) Noise\nCorrection, an inference time technique that enhances sample quality by\naddressing singularities in noise prediction. Through extensive experiments,\nwe demonstrate that our method outperforms existing competitors in\nquantitative metrics and human evaluations. Remarkably, it achieves\nperformance comparable to the teacher model using only three denoising steps,\nenabling efficient high-quality generation.\n\nDownload the Paper\n\n#### AUTHORS\n\nWritten by\n\nJonas Kohler\n\nAlbert Pumarola\n\nEdgar Schoenfeld\n\nArtsiom Sanakoyeu\n\nRoshan Sumbaly\n\nPeter Vajda\n\nAli Thabet\n\nPublisher\n\nMeta\n\nResearch Topics\n\nComputer Vision\n\n### Related Publications\n\nMarch 20, 2024\n\n#### COMPUTER VISION\n\n#### SceneScript: Reconstructing Scenes With An Autoregressive Structured\nLanguage Model\n\nWe introduce SceneScript, a method that directly produces full scene models as\na sequence of structured language commands using an autoregressive, token-\nbased approach. Our proposed scene representation is inspired by recent\nsuccesses in transformers & LLMs, and departs from more traditional methods\nwhich commonly describe scenes as meshes, voxel grids, point clouds or\nradiance fields. Our method infers the set of structured language commands\ndirectly from encoded visual data using a scene language encoder-decoder\narchitecture. To train SceneScript, we generate and release a large-scale\nsynthetic dataset called Aria Synthetic Environments consisting of 100k high-\nquality indoor scenes, with photorealistic and ground-truth annotated renders\nof egocentric scene walkthroughs. Our method gives state-of-the art results in\narchitectural layout estimation, and competitive results in 3D object\ndetection. Lastly, we explore an advantage for SceneScript, which is the\nability to readily adapt to new commands via simple additions to the\nstructured language, which we illustrate for tasks such as coarse 3D object\npart reconstruction.\n\nArmen Avetisyan, Chris Xie, Henry Howard-Jenkins, Tsun-Yi Yang, Samir Aroudj,\nSuvam Patra, Fuyang Zhang, Duncan Frost, Luke Holland, Campbell Orme, Jakob\nJulian Engel, Edward Miller, Richard Newcombe, Vasileios Balntas\n\nMarch 20, 2024\n\nRead the Paper\n\nFebruary 13, 2024\n\n#### GRAPHICS\n\n#### COMPUTER VISION\n\n#### IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality\n3D Generation\n\nMost text-to-3D generators build upon off-the-shelf text-to-image models\ntrained on billions of images. They use variants of Score Distillation\nSampling (SDS), which is slow, somewhat unstable, and prone to artifacts. A\nmitigation is to fine-tune the 2D generator to be multi-view aware, which can\nhelp distillation or can be combined with reconstruction networks to output 3D\nobjects directly. In this paper, we further explore the design space of text-\nto-3D models. We significantly improve multi-view generation by considering\nvideo instead of image generators. Combined with a 3D reconstruction algorithm\nwhich, by using Gaussian splatting, can optimize a robust image-based loss, we\ndirectly produce high-quality 3D outputs from the generated views. Our new\nmethod, IM-3D, reduces the number of evaluations of the 2D generator network\n10-100x, resulting in a much more efficient pipeline, better quality, fewer\ngeometric inconsistencies, and a high yield of usable 3D assets.\n\nLuke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea\nVedaldi, Oran Gafni, Filippos Kokkinos\n\nFebruary 13, 2024\n\nRead the Paper\n\nJanuary 25, 2024\n\n#### COMPUTER VISION\n\n#### LRR: Language-Driven Resamplable Continuous Representation against\nAdversarial Tracking Attacks\n\nVisual object tracking plays a critical role in visual-based autonomous\nsystems, as it aims to estimate the position and size of the object of\ninterest within a live video. Despite significant progress made in this field,\nstate-of-the-art (SOTA) trackers often fail when faced with adversarial\nperturbations in the incoming frames. This can lead to significant robustness\nand security issues when these trackers are deployed in the real world. To\nachieve high accuracy on both clean and adversarial data, we propose building\na spatial-temporal implicit representation using the semantic text guidance of\nthe object of interest extracted from the language-image model (i.e., CLIP).\nThis novel representation enables us to reconstruct incoming frames to\nmaintain semantics and appearance consistent with the object of interest and\nits clean counterparts. As a result, our proposed method successfully defends\nagainst different SOTA adversarial tracking attacks while maintaining high\naccuracy on clean data. In particular, our method significantly increases\ntracking accuracy under adversarial attacks with around 90% relative\nimprovement on UAV123, which is close to the accuracy on clean data.\n\nFelix Xu, Di Lin, Jianjun Zhao, Jianlang Chen, Lei Ma, Qing Guo, Wei Feng,\nXuhong Ren\n\nJanuary 25, 2024\n\nRead the Paper\n\nDecember 08, 2023\n\n#### COMPUTER VISION\n\n#### Learning Fine-grained View-Invariant Representations from Unpaired Ego-\nExo Videos via Temporal Alignment\n\nThe egocentric and exocentric viewpoints of a human activity look dramatically\ndifferent, yet invariant representations to link them are essential for many\npotential applications in robotics and augmented reality. Prior work is\nlimited to learning view-invariant features from paired synchronized\nviewpoints. We relax that strong data assumption and propose to learn fine-\ngrained action features that are invariant to the viewpoints by aligning\negocentric and exocentric videos in time, even when not captured\nsimultaneously or in the same environment. To this end, we propose AE2, a\nself-supervised embedding approach with two key designs: (1) an object-centric\nencoder that explicitly focuses on regions corresponding to hands and active\nobjects; and (2) a contrastive-based alignment objective that leverages\ntemporally reversed frames as negative samples. For evaluation, we establish a\nbenchmark for fine-grained video understanding in the ego-exo context,\ncomprising four datasets---including an ego tennis forehand dataset we\ncollected, along with dense per-frame labels we annotated for each dataset. On\nthe four datasets, our AE2 method strongly outperforms prior work in a variety\nof fine-grained downstream tasks, both in regular and cross-view settings.\n\nSherry Xue, Kristen Grauman\n\nDecember 08, 2023\n\nRead the Paper\n\nSee All Papers\n\n## Help Us Pioneer The Future of AI\n\n##### We share our open source frameworks, tools, libraries, and models for\neverything from research exploration to large-scale production deployment.\n\nJoin our Team\n\nOur approach\n\nAbout AI at Meta\n\nResponsibility\n\nPeople\n\nCareers\n\nResearch\n\nInfrastructure\n\nResources\n\nDemos\n\nProduct experiences\n\nMeta AI\n\nLatest news\n\nBlog\n\nNewsletter\n\nFoundational models\n\nMeta Llama\n\nOur approach\n\nOur approachAbout AI at MetaResponsibilityPeopleCareers\n\nResearch\n\nResearchInfrastructureResourcesDemos\n\nProduct experiences\n\nMeta AI\n\nLatest news\n\nLatest newsBlogNewsletter\n\nFoundational models\n\nMeta Llama\n\nPrivacy Policy\n\nTerms\n\nCookies\n\nMeta \u00a9 2024\n\nAllow the use of cookies from Meta on this browser?\n\nWe use essential cookies and similar technologies to help:\n\nProvide and improve content on Meta Products\n\nProvide a safer experience by using information we receive from cookies on and\noff Meta Products\n\nProvide and improve Meta Company Products for people using a Meta or Oculus\naccount\n\nWe use tools on Meta from other companies that also use cookies. These tools\nare used for things like:\n\n  * Advertising and measurement services off of Meta Products\n  * Analytics\n  * Providing certain features\n  * Improving our services\n\nYou can allow the use of all cookies, just essential cookies or you can choose\nmore options below. You can learn more about cookies and how we use them, and\nreview or change your choice at any time in our Cookie Policy.\n\nEssential cookies\n\nThese cookies are required to use Meta Company Products. They\u2019re necessary for\nMeta websites to work as intended.\n\nOptional cookies\n\nOptional cookies from other companies\n\nWe use tools from other companies for advertising and measurement services off\nof Meta Company Products, analytics, and to provide certain features and\nimprove our services for you. These companies also use cookies.\n\nIf you allow these cookies:\n\n  * We\u2019ll be able to better personalize ads for you off of Meta Products, and measure their performance\n  * Features on our products will not be affected\n  * Other companies will receive information about you when you use cookies\n\nIf you don\u2019t allow these cookies:\n\n  * We won\u2019t use cookies from other companies to help personalize ads for you off of Meta Products or measure ads performance\n  * Some features on our products may not work\n\nOther ways you can control tracking\n\nAd settings\n\nIf you have added your Meta or Oculus account to the same Accounts Center as\nyour Facebook or Instagram account, you can manage how different data is used\nto personalize ads in ad settings. To show you better ads, we use data that\nadvertisers and other partners provide us about your activity off Meta Company\nProducts, including websites and apps. You can control whether we use this\ndata to show you ads in your ad settings.\n\nThe Facebook Audience Network is a way for advertisers to show you ads in apps\nand websites off the Meta Company Products. One of the ways Audience Network\nshows relevant ads is by using your ad preferences to determine which ads you\nmay be interested in seeing.\n\nAd preferences\n\nIn Ad preferences, you can choose whether we show you ads and make choices\nabout the information used to show you ads.\n\nYou can opt out of seeing online interest-based ads from Meta and other\nparticipating companies through the Digital Advertising Alliance in the US,\nthe Digital Advertising Alliance of Canada in Canada or the European\nInteractive Digital Advertising Alliance in Europe, or through your mobile\ndevice settings, if you are using Android, iOS 13 or an earlier version of\niOS. Please note that ad blockers and tools that restrict our cookie use may\ninterfere with these controls.The advertising companies we work with generally\nuse cookies and similar technologies as part of their services. To learn more\nabout how advertisers generally use cookies and the choices they offer, you\ncan review the following resources:\n\n  * Digital Advertising Alliance\n  * Digital Advertising Alliance of Canada\n  * European Interactive Digital Advertising Alliance\n\nYour browser or device may offer settings that allow you to choose whether\nbrowser cookies are set and to delete them. These controls vary by browser,\nand manufacturers may change both the settings they make available and how\nthey work at any time. As of 5 October 2020, you may find additional\ninformation about the controls offered by popular browsers at the links below.\nCertain parts of Meta Products may not work properly if you have disabled\nbrowser cookies. Please be aware these controls are distinct from the controls\nthat Instagram and Facebook offer.\n\n  * Google Chrome\n  * Internet Explorer\n  * Firefox\n  * Safari\n  * Safari Mobile\n  * Opera\n\n", "frontpage": false}
