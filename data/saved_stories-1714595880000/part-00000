{"aid": "40223847", "title": "Adapt-LLM: LLM That Decides If to Retrieve External Information or Not", "url": "https://didyouknowbg8.wordpress.com/2024/05/01/adapt-llm-llm-that-decides-if-to-retrieve-external-information-or-not/", "domain": "didyouknowbg8.wordpress.com", "votes": 2, "user": "allpaca", "posted_at": "2024-05-01 14:35:43", "comments": 0, "source_title": "ADAPT-LLM: LLM that Decides if to Retrieve External Information or Not", "source_text": "ADAPT-LLM: LLM that Decides if to Retrieve External Information or Not \u2013 Did\nyou know?\n\nDid you know?!\n\n# ADAPT-LLM: LLM that Decides if to Retrieve External Information or Not\n\nMay 1, 2024\n\nAI, LLM, Retrieval-Augmented Generation\n\nADAPT-LLM, Information Retrieval, IR, IR system, LLM, RAG\n\nLarge Language Models (LLMs) have become increasingly sophisticated, capable\nof answering our questions in informative ways. However, they often struggle\nwhen the answer isn\u2019t readily available within their internal knowledge base.\nThis limitation can be overcome by incorporating an Information Retrieval (IR)\nsystem that retrieves relevant text passages to supplement the LLM\u2019s\nknowledge. But how can we determine when this additional context is truly\nnecessary?\n\nMaybe introducing ADAPT-LLM, a type of LLM that can dynamically decide whether\nit needs to retrieve external information to answer a question.\n\nOh, well, said enough: introduction done! More details in:\n\n  1. Understanding Large Language Models (LLMs)\n  2. Introducing Information Retrieval (IR) Systems\n  3. The Challenge: When to Use Retrieved Information?\n  4. ADAPT-LLM: Deciding When to Retrieve Context\n  5. How Well Does ADAPT-LLM Perform?\n  6. Beyond the Basics: A Deeper Look into the Experiments\n  7. The Future of Adaptive Retrieval for LLMs\n  8. Conclusion\n\n### Understanding Large Language Models (LLMs)\n\nThis is something I\u2019ve already written dozens of times, but it\u2019s important to\nunderstand the rest, so... Let\u2019s do it again!\n\nAdvertisement\n\nPrivacy Settings\n\nImagine a vast library containing information on all sorts of topics: LLMs are\nsimilar to these libraries, but instead of storing physical books, they store\nand process information in a digital format. This information can include text\nfrom books, articles, code, and even conversations. LLMs are trained on\nmassive amounts of data, allowing them to recognize patterns and relationships\nbetween words. This enables them to perform various tasks, including\ngenerating different creative text formats, translating languages, and, of\ncourse, answering questions.\n\nHowever, LLMs have limitations: just like a library with a limited collection,\nan LLM\u2019s knowledge base can have gaps. If the answer to a question lies\noutside of this internal knowledge, the LLM may provide inaccurate or\nincomplete information.\n\n### Introducing Information Retrieval (IR) Systems\n\nInformation Retrieval (IR) systems act like assistants in a library. When you\nask a librarian a question, they can search the library\u2019s catalog and point\nyou to relevant books or articles. Similarly, IR systems can search through\nvast amounts of text data and retrieve passages that might be helpful for\nanswering a question.\n\nIn the context of LLMs, IR systems can be used to supplement the LLM\u2019s\ninternal knowledge. When an LLM encounters a question it cannot answer\nconfidently, the IR system can retrieve relevant text passages that the LLM\ncan then use to formulate a better response.\n\n### The Challenge: When to Use Retrieved Information?\n\nWhile IR systems offer a solution to LLMs\u2019 knowledge gaps, there\u2019s a catch:\nnot all questions require additional context! Including irrelevant information\ncan actually hinder the LLM\u2019s ability to answer the question correctly. Here\u2019s\nan analogy: imagine a librarian giving you a giant stack of books on every\ntopic imaginable, even if your question only requires a specific recipe.\nSorting through all that extra material would take a long time!\n\nThe key challenge is to determine when to use the retrieved information from\nthe IR system. This is where ADAPT-LLM, introduced in this paper, comes in.\n\n### ADAPT-LLM: Deciding When to Retrieve Context\n\nADAPT-LLM is an LLM specifically designed to address the challenge of\neffectively using retrieved information. It works in three key stages:\n\n  1. Evaluate the Question: First, ADAPT-LLM analyzes the question to assess if it has the necessary knowledge within its own memory to answer it accurately.\n  2. Direct Answer or Request for Context: If the answer is likely within its memory, ADAPT-LLM provides a direct answer. However, if it\u2019s unsure or lacks the required knowledge, it generates a special signal, denoted by the symbol \u27e8RET\u27e9, indicating it needs to retrieve additional context.\n  3. Answer with Retrieved Context: If \u27e8RET\u27e9 is generated, the IR system steps in. It retrieves potentially relevant passages based on the question. These retrieved passages are then provided to ADAPT-LLM, along with the original question. Finally, ADAPT-LLM uses this combination of information to formulate its answer.\n\nHere\u2019s a schematic representation of the ADAPT-LLM process:\n\nFrom this paper.\n\n### How Well Does ADAPT-LLM Perform?\n\nADAPT-LLM has been evaluated on a dataset of real-world questions along with\ntheir corresponding answers. The model was compared to two baseline models:\n\n  * Never-Retrieve (NR-LLM): This model always relies solely on its internal knowledge to answer questions, never using the IR system to retrieve additional context.\n  * Always-Retrieve (AR-LLM): This model always retrieves context for every question, regardless of whether it\u2019s necessary.\n\nThe evaluation focused on how accurately each model could answer the\nquestions. Here\u2019s what was found:\n\n  * ADAPT-LLM outperformed both baselines. This indicates that ADAPT-LLM\u2019s ability to selectively use retrieved context leads to better overall performance. The results are summarized in Table 1.\n  * ADAPT-LLM effectively determines when context is necessary. The model rarely requested additional information for questions it could answer confidently using its internal knowledge. On the other hand, it frequently requested context for questions that likely required external information for accurate answers.\n  * The choice of training data matters. The model performed slightly better when trained on a dataset more similar to the evaluation dataset in terms of question style and content.\n\nModel| Training Dataset| PopQA Test Set Accuracy  \n---|---|---  \nNever-Retrieve (NR-LLM)| SQuAD| 21.22%  \nAlways-Retrieve (AR-LLM)| SQuAD| 36.59%  \nADAPT-LLM| SQuAD| 38.15%  \nNever-Retrieve (NR-LLM)| NQ| 21.43%  \nAlways-Retrieve (AR-LLM)| NQ| 35.86%  \nADAPT-LLM| NQ| 36.77%  \nTable 1: Comparison of ADAPT-LLM with Baseline Models on PopQA Test Set using\nLlama-2 models trained on the NQ and SQuAD datasets\n\nThese findings demonstrate the effectiveness of ADAPT-LLM\u2019s approach. The\nmodel can accurately assess when retrieved information is helpful and avoid\nthe potential drawbacks of using irrelevant context.\n\n## Evaluating the Effectiveness of the \u27e8RET\u27e9 Token\n\nA key component of ADAPT-LLM is the \u27e8RET\u27e9 token, which acts as a signal for\nrequesting additional context from the IR system. To assess how effectively\nADAPT-LLM utilizes this token, the researchers conducted an analysis of its\nusage on the PopQA test set. Here\u2019s a breakdown of the results summarized in\nTable 2.\n\nTraining Dataset| Proportion of Questions with \u27e8RET\u27e9 Token| Acc. w/ Context\n(\u27e8RET\u27e9)| Acc. w/o Context (\u27e8RET\u27e9)| Acc. w/ Context (No \u27e8RET\u27e9)| Acc. w/o\nContext (No \u27e8RET\u27e9)  \n---|---|---|---|---|---  \nNQ| 82.26%| 33.04%| 14.65%| 55.72%| 62.36%  \nSQuAD| 83.93%| 33.40%| 9.94%| 57.73%| 62.92%  \nTable 2: Results of the Usage of the \u27e8RET\u27e9 Token in the ADAPT-LLM Model\n\nThe table showcases several key points:\n\n  * High Proportion of \u27e8RET\u27e9 Usage: The second column highlights the significant portion of questions (over 82%) for which ADAPT-LLM requests context retrieval (indicated by \u27e8RET\u27e9). This reinforces the notion that even real-world questions often require external information to achieve optimal answer accuracy for LLMs.\n  * Impact of Context on \u27e8RET\u27e9 Questions: The third and fourth columns (Acc. w/ Context and Acc. w/o Context for questions with \u27e8RET\u27e9) demonstrate the clear benefit of using retrieved context for questions where ADAPT-LLM requests it (\u27e8RET\u27e9). The accuracy jumps significantly when the model has access to relevant external information. This confirms the effectiveness of the \u27e8RET\u27e9 token in identifying situations where additional context is crucial for accurate answers.\n  * Performance on Questions without \u27e8RET\u27e9: The last two columns (Acc. w/ Context and Acc. w/o Context for questions without \u27e8RET\u27e9) showcase the model\u2019s capability when it decides to answer directly (no \u27e8RET\u27e9 token). Here, including retrieved context shows a significant negative impact on accuracy (around 5-7% decrease for both NQ and SQuAD datasets). This suggests that ADAPT-LLM can effectively determine when its internal knowledge is sufficient to answer a question accurately, avoiding the potential drawbacks of irrelevant retrieved information.\n\nSo, the analysis of the \u27e8RET\u27e9 token usage provides strong evidence for ADAPT-\nLLM\u2019s ability to make informed decisions about context retrieval. The model\neffectively identifies scenarios where additional information is essential and\nleverages it to significantly improve answer accuracy.\n\n### Beyond the Basics: A Deeper Look into the Experiments\n\nSeveral experiments have been done to assess ADAPT-LLM\u2019s capabilities in more\ndetail. Here are some key takeaways:\n\n  * Analyzing ADAPT-LLM\u2019s Decision Making: The analysis revealed that ADAPT-LLM requested context for a majority of the questions in the evaluation set. This suggests that even for real-world questions, LLMs often benefit from additional information to provide the most accurate answers.\n  * Importance of High-Quality Retrieved Context: The experiments highlighted the importance of using an effective IR system. In fact, when ADAPT-LLM was provided with gold passages containing the exact answer (instead of passages retrieved by the IR system), its performance significantly improved, as shown in Table 3. This suggests that further advancements in IR techniques are crucial for maximizing the effectiveness of ADAPT-LLM and similar approaches.\n  * ADAPT-LLM Learns from Experience: Even though ADAPT-LLM wasn\u2019t explicitly trained on a concept like question popularity, it showed a tendency to request context more often for questions with lower popularity scores (questions that are less frequently asked). This suggests that the model can learn effective strategies for context retrieval based on the data it is trained on.\n\nDataset| Passage Type| Accuracy  \n---|---|---  \nNQ| Gold Passages| 69.76%  \nNQ| Contriever Passages| 27.04%  \nSQuAD| Gold Passages| 89.42%  \nSQuAD| Contriever Passages| 22.49%  \nTable 3: Performance Comparison of ADAPT-LLM using Gold Passages vs Retrieved\nPassages\n\n### The Future of Adaptive Retrieval for LLMs\n\nADAPT-LLM represents a significant step forward in developing LLMs that can\neffectively utilize external information. This research opens doors to several\nexciting possibilities for the future:\n\n  * Improved Question Answering Systems: By incorporating adaptive retrieval techniques, question answering systems can become more reliable and accurate, especially when dealing with complex or open-ended questions.\n  * Enhanced Generalizability: ADAPT-LLM\u2019s ability to learn effective context retrieval strategies without relying on dataset-specific information suggests that the approach can be generalized to various question answering tasks.\n  * Exploration of Advanced Retrieval Techniques: The research highlights the need for more sophisticated IR systems. Future research can explore techniques like learnable sequential retrieval, which could further improve the quality of retrieved context and ultimately enhance the performance of ADAPT-LLM and similar models.\n\n### Conclusion\n\nIn conclusion, ADAPT-LLM presents a groundbreaking approach to using retrieved\ninformation in LLMs by dynamically deciding when to leverage external context,\ngiving more robust and reliable question answering systems. As research\nprogresses in IR techniques and LLM development, we can expect even more\nsignificant advancements in this field. Let\u2019s hope to see something more in\nthis direction soon!\n\n### Share this:\n\n  * Twitter\n  * Facebook\n\nLike Loading...\n\n\u2190Previous\n\nLike Loading...\n\n##### Subscribe for the latest breakthroughs and innovations shaping the\nworld!\n\n### Leave a comment Cancel reply\n\nBlog at WordPress.com.\n\nLoading Comments...\n\n  * Comment\n  * Reblog\n  * Subscribe Subscribed\n\n    * Did you know?\n    * Already have a WordPress.com account? Log in now.\n\n  * Privacy\n  *     * Did you know?\n    * Edit Site\n    * Subscribe Subscribed\n    * Sign up\n    * Log in\n    * Copy shortlink\n    * Report this content\n    * View post in Reader\n    * Manage subscriptions\n    * Collapse this bar\n\n%d\n\n%d\n\nDesign a site like this with WordPress.com\n\nGet started\n\n", "frontpage": false}
