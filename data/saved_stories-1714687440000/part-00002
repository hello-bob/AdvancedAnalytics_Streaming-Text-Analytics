{"aid": "40237791", "title": "Vercel AI SDK 3.1: ModelFusion joins the team", "url": "https://vercel.com/blog/vercel-ai-sdk-3-1-modelfusion-joins-the-team", "domain": "vercel.com", "votes": 1, "user": "MaxLeiter", "posted_at": "2024-05-02 16:03:15", "comments": 0, "source_title": "Vercel AI SDK 3.1: ModelFusion joins the team \u2013 Vercel", "source_text": "Vercel AI SDK 3.1: ModelFusion joins the team \u2013 Vercel\n\nSkip to content\n\nContactLog In\n\nSign Up\n\n\u2190 Back to Blog\n\nEngineering\n\nThursday, May 2nd 2024\n\n# Vercel AI SDK 3.1: ModelFusion joins the team\n\nOne step closer to a complete TypeScript framework for AI applications\n\nPosted by\n\nJared Palmer\n\nVP of Product, AI\n\nLars Grammel\n\nSoftware Engineer\n\nToday, we're releasing the AI SDK 3.1, with ModelFusion joining our team.\n\nThis release brings us one step closer to delivering a complete TypeScript\nframework for building AI applications. It is organized into three main parts:\n\n  * AI SDK Core: A unified API for generating text, structured objects, and tool calls with large language models (LLMs).\n  * AI SDK UI: A set of framework-agnostic hooks for quickly building chat interfaces.\n  * AI SDK RSC: A library to stream generative user interfaces with React Server Components (RSC).\n\nStreaming text with AI SDK\n\nStreaming text with AI SDK\n\n## AI SDK Core - A new foundation for AI\n\nDrawing inspiration from projects like Drizzle and Prisma, you can imagine the\nAI SDK Core as an ORM-style abstraction for LLMs.\n\nThese new APIs provide a set of unified, low-level primitives to work with\nLLMs in any JavaScript environment\u2014abstracting away the quirks between major\nmodel providers. This simplifies integrating LLMs down to just two decisions:\n\n  * What kind of data do you want to generate? (text or a structured object)\n  * How do you want it delivered? (incrementally streamed or all-at-once)\n\nYou can use the AI SDK Core API with any provider that implements the AI SDK\nLanguage Model Specification. We worked with major AI model providers to\niterate on the Vercel AI SDK Core, and already support:\n\n  * OpenAI\n  * Anthropic\n  * Google Gemini\n  * Mistral\n\nThe Language Model Specification is open-source. Anyone can now build AI\nprovider integrations that work seamlessly with the Vercel AI SDK, and our\namazing community has already started to add support for providers such as\nLLamaCpp.\n\n### Generating text\n\nHere\u2019s an example of generating text with AI SDK Core using the mistral-large-\nlatest model from Mistral AI.\n\n    \n    \n    import { generateText } from 'ai';\n    \n    import { mistral } from '@ai-sdk/mistral';\n    \n    const { text } = await generateText({\n    \n    model: mistral(\"mistral-large-latest\"),\n    \n    prompt: \"Generate a lasangna recipe.\",\n    \n    });\n\nIf you want to switch out the model for OpenAI\u2019s gpt-4-turbo, you can do so by\nchanging two lines of code.\n\n    \n    \n    import { generateText } from 'ai';\n    \n    import { openai } from '@ai-sdk/openai';\n    \n    const { text } = await generateText({\n    \n    model: openai(\"gpt-4-turbo\"),\n    \n    prompt: \"Generate a lasagna recipe.\",\n    \n    });\n\n### Generating Structured Data\n\nThe Vercel AI SDK standardizes structured object generation across model\nproviders with the generateObject and streamObject functions. Generating fully\ntyped objects is as simple as defining a Zod schema and passing it to your\nfunction call.\n\n    \n    \n    import { generateObject } from 'ai'\n    \n    import { z } from 'zod'\n    \n    import { openai } from '@ai-sdk/openai';\n    \n    const { object } = await generateObject({\n    \n    model: openai(\"gpt-4-turbo\"),\n    \n    schema: z.object({\n    \n    recipe: z.object({\n    \n    name: z.string().describe('name of recipe'),\n    \n    ingredients: z.array(\n    \n    z.object({\n    \n    name: z.string().describe('ingredient name'),\n    \n    amount: z.string().describe('amount of ingredient')\n    \n    })\n    \n    ),\n    \n    steps: z.array(z.string()).describe('steps to prepare recipe')\n    \n    })\n    \n    }),\n    \n    prompt: 'Generate a lasagna recipe.'\n    \n    })\n\nThe model returns a validated and type-safe object \u2014 in this case, a lasagna\nrecipe \u2014 based on your predetermined schema.\n\n    \n    \n    {\n    \n    \"name\": \"Classic Lasagna\",\n    \n    \"ingredients\": [\n    \n    {\n    \n    \"name\": \"Olive oil\",\n    \n    \"amount\": \"2 tablespoons\"\n    \n    },\n    \n    ...\n    \n    ],\n    \n    \"steps\": [\n    \n    \"Preheat oven to 375\u00b0F (190\u00b0C).\",\n    \n    \"In a large skillet, heat olive oil over medium heat. Add ground beef, onion, and garlic. Cook until beef is browned.\",\n    \n    ...\n    \n    ]\n    \n    }\n\n## AI SDK UI - Chat interface in seconds\n\nStreaming conversational text UIs (like ChatGPT) have gained massive\npopularity over the past year. However, a basic chat interface still requires\ncomplex boilerplate: state management (tracking client input, conversation\nhistory, loading state), logic to parse and process streaming text, lifecycle\nhooks to manage persistent data, and more.\n\nAI SDK UI simplifies the implementation of popular AI interfaces into three\nframework-agnostic hooks, useChat, useCompletion, and useAssistant. Together\nwith AI SDK Core\u2019s streamText function, you can build a streaming chatbot in\nless than 50 lines of code.\n\napp/api/chat/route.ts\n\n    \n    \n    import { streamText } from 'ai';\n    \n    import { openai } from '@ai-sdk/openai';\n    \n    export async function POST(req: Request) {\n    \n    const { messages } = await req.json();\n    \n    const result = await streamText({\n    \n    model: openai('gpt-4-turbo'),\n    \n    messages,\n    \n    });\n    \n    return result.toAIStreamResponse();\n    \n    }\n\napp/page.tsx\n\n    \n    \n    'use client'\n    \n    import { useChat } from 'ai/react'\n    \n    export default function Page() {\n    \n    const { messages, input, handleInputChange, handleSubmit } = useChat();\n    \n    return (\n    \n    <>\n    \n    {messages.map(message => (\n    \n    <div key={message.id}>\n    \n    {message.role === 'user' ? 'User: ' : 'AI: '}\n    \n    {message.content}\n    \n    </div>\n    \n    ))}\n    \n    <form onSubmit={handleSubmit}>\n    \n    <input\n    \n    name=\"prompt\"\n    \n    value={input}\n    \n    onChange={handleInputChange}\n    \n    id=\"input\"\n    \n    />\n    \n    <button type=\"submit\">Submit</button>\n    \n    </form>\n    \n    </>\n    \n    )\n    \n    }\n\nIn this Next.js App Router example, the useChat hook enables streaming chat\nmessages from OpenAI, manages the chat state, and updates the UI automatically\nas new messages arrive.\n\n## AI SDK RSC - Move beyond text\n\nWhile AI chatbots have made a profound impact, LLM applications have faced two\nimportant UX challenges:\n\n  * Limited or imprecise knowledge\n  * Plain text or markdown-only responses\n\nWith the introduction of Tools and Tool Calling, developers can build more\nrobust applications that fetch realtime data.\n\nWith Vercel AI SDK RSC, you can now move beyond text-based chatbots to give\nLLMs rich, component-based interfaces.\n\nIn today's release, we're adding streamUI, a new function that's compatible\nwith AI SDK Core Language Model Specification. This is the successor to render\n(which we plan to deprecate in the next minor release).\n\nLet\u2019s look at an example React Server Action that can retrieve the live\nweather and render a custom UI with streamUI.\n\n    \n    \n    import { streamUI } from 'ai/rsc'\n    \n    import { openai } from '@ai-sdk/openai'\n    \n    import { z } from 'zod'\n    \n    import { Spinner, Weather } from '@/components'\n    \n    import { getWeather } from '@/utils'\n    \n    async function submitMessage(userInput) { // 'What is the weather in SF?'\n    \n    'use server'\n    \n    const result = streamUI({\n    \n    model: openai('gpt-4-turbo'),\n    \n    messages: [\n    \n    { role: 'system', content: 'You are a helpful assistant' },\n    \n    { role: 'user', content: userInput }\n    \n    ],\n    \n    text: ({ content }) => <p>{content}</p>,\n    \n    tools: {\n    \n    get_city_weather: {\n    \n    description: 'Get the current weather for a city',\n    \n    parameters: z.object({\n    \n    city: z.string().describe('the city')\n    \n    }).required(),\n    \n    generate: async function* ({ city }) {\n    \n    yield <Spinner/>\n    \n    const weather = await getWeather(city)\n    \n    return <Weather info={weather} />\n    \n    }\n    \n    }\n    \n    }\n    \n    })\n    \n    return result.value\n    \n    }\n\nWhat is the weather in SF?\n\ngetWeather(\"San Francisco, CA\")\n\nThursday, March 7\n\n47\u00b0\n\nsunny\n\n7am\n\n48\u00b0\n\n8am\n\n50\u00b0\n\n9am\n\n52\u00b0\n\n10am\n\n54\u00b0\n\n11am\n\n56\u00b0\n\n12pm\n\n58\u00b0\n\n1pm\n\n60\u00b0\n\nThanks!\n\nWeather\n\nAn example of an assistant that renders the weather information in a streamed\ncomponent.\n\n## Towards a complete TypeScript AI framework\n\nThe Vercel AI SDK 3.1 marks an important step towards delivering a complete\nTypeScript AI Framework.\n\n  * With the AI SDK Core, you get a unified API for calling LLMs that works anywhere JavaScript or TypeScript runs.\n  * With the AI SDK UI, you can build chat interfaces in seconds with framework-agnostic hooks.\n  * Finally, with the AI SDK RSC, you can go beyond chat interfaces to deliver the next generation of AI native applications with Generative UI.\n\nYou can learn more in our new documentation or experiment with different\nmodels using the AI playground.\n\nExplore the possibilities\n\nTalk to our team to learn more about building AI-powered applications for your\norganization.\n\nGet started\n\nPosted by\n\nJared Palmer\n\nVP of Product, AI\n\nLars Grammel\n\nSoftware Engineer\n\nDevelop.Preview.Ship.\n\nVercel is the platform for frontend developers, providing the speed and\nreliability innovators need to create at the moment of inspiration.\n\nStart Deploying\n\nTour the Product\n\n", "frontpage": false}
