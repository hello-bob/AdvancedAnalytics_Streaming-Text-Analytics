{"aid": "40162370", "title": "Building a highly-available search engine using SQLite", "url": "https://www.philipotoole.com/building-a-highly-available-search-engine-using-sqlite/", "domain": "philipotoole.com", "votes": 2, "user": "otoolep", "posted_at": "2024-04-25 20:08:46", "comments": 0, "source_title": "Building a highly-available search engine using SQLite", "source_text": "Building a highly-available search engine using SQLite | Vallified\n\n# Vallified\n\nSearch\n\n# Building a highly-available search engine using SQLite\n\nApril 25, 2024 Philip O'Toole Leave a comment\n\nDid you know that SQLite contains a full text search engine? The SQLite FTS4\nand FTS5 Extensions allow you to perform full-text searches on documents\nloaded into SQLite.\n\nAnd since rqlite uses SQLite as its database engine, rqlite makes it really\neasy to deploy a highly-available search engine \u2013 and one you can interact\nwith using SQL and HTTP.\n\n## What is rqlite?\n\nrqlite is a lightweight, open-source distributed database that uses SQLite as\nits storage engine. It\u2019s designed for easy setup, allowing you to create a\ncluster in seconds. Once clustered, rqlite can handle the failure of\nindividual nodes without losing access to data, ensuring your system remains\nreliable and available when needed.\n\nAnd because rqlite is built on SQLite, once you form a rqlite cluster, it\u2019s\nmeans you\u2019ve got a highly-available search engine too.\n\n## Let\u2019s build a search engine\n\nLet\u2019s spin up a 3-node rqlite cluster \u2014 step-by-step directions for deploying\nthis test cluster are available on GitHub.\n\nThe step-by-step process is deliberately manual so you can see the details,\nbut rqlite has extensive support for automatic clustering and discovery when\nrunning on Kubernetes, as well as with Consul, and etcd. You can also run\nrqlite using Docker.\n\nFor this test, I deployed three virtual machines on Google Cloud Platform,\neach an e2-standard-4 type with persistent SSD storage. I placed each machine\nin a different zone to maximize fault tolerance, ensuring that the loss of one\ndata center wouldn\u2019t bring down the entire cluster.\n\n3 virtual machines running on GCP. These will form the 3-node rqlite cluster.\nNote that each machine is in a different zone, maximizing fault-tolerance.\n\n## Let\u2019s index some Apache logs\n\nLogs are a common use case for search engines. In this test, I used an Apache\nAccess log file containing 5 million records. I wrote a simple Python program\nto read these logs and write them to rqlite.\n\nWhat does the indexing program do? Not that much, which is part of its charm.\nIt creates a virtual FTS4 table in rqlite, reads the Apache log file, and\nwrites each line to rqlite. SQLite then does the indexing.\n\n#### Increasing indexing performance\n\nTo increase indexing speed, I used rqlite\u2019s Queued Writes, feature. It boosts\nwrite performance significantly, allowing the system to index logs much\nfaster, but with minor trade-offs in durability. I also ran two indexing\nprograms concurrently \u2014 with one instance writing the first half of the log\nfile, and the second writing the other half.\n\nI achieved indexing rates greater than 3,000 log lines per second, indexing 5\nmillion logs in under 30 minutes. This kind of performance would be more than\nenough for many moderately-popular websites.\n\n## And now let\u2019s search it\n\nFirst let\u2019s perform some initial examination of our data using the rqlite\nshell:\n\n    \n    \n    $ rqlite -H 34.67.9.228 Welcome to the rqlite CLI. Enter \".help\" for usage hints. Connected to http://34.67.9.228:4001 running version v8.23.3 34.67.9.228:4001> .schema +-----------------------------------------------------------------------+ | sql | +-----------------------------------------------------------------------+ | CREATE VIRTUAL TABLE logs USING fts5(entry) | +-----------------------------------------------------------------------+ | CREATE TABLE 'logs_data'(id INTEGER PRIMARY KEY, block BLOB) | +-----------------------------------------------------------------------+ | CREATE TABLE 'logs_idx'(segid, term, pgno, PRIMARY KEY(segid, term)) | +-----------------------------------------------------------------------+ | CREATE TABLE 'logs_content'(id INTEGER PRIMARY KEY, c0) | +-----------------------------------------------------------------------+ | CREATE TABLE 'logs_docsize'(id INTEGER PRIMARY KEY, sz BLOB) | +-----------------------------------------------------------------------+ | CREATE TABLE 'logs_config'(k PRIMARY KEY, v) WITHOUT ROWID | +-----------------------------------------------------------------------+ 34.67.9.228:4001>\n\nAs you can see, when an FTS table is created, SQLite actually creates\nadditional other tables.\n\nDirectly accessing the rqlite HTTP API is also easy. Let\u2019s confirm we have the\nright number of logs, by querying a node in the cluster:\n\n    \n    \n    $ curl -G 'localhost:4001/db/query?pretty&timings' --data-urlencode 'q=SELECT COUNT(*) FROM logs' { \"results\": [ { \"columns\": [ \"COUNT(*)\" ], \"types\": [ \"integer\" ], \"values\": [ [ 5000000 ] ], \"time\": 7.038763355 } ], \"time\": 7.041182592 }\n\nrqlite took about 7 seconds to return the count \u2014 and they\u2019re five million\nrecords as expected.\n\n### Needle-in-a-haystack\n\nBefore I kicked off the indexing processes I actually modified a single line\nat random in the Apache log file. I changed that line such that it contained\nthe string \u201cSuperSecretAgent\u201d. Let\u2019s see how long it takes for rqlite to\nlocate that line.\n\nWe will do this search in two ways \u2014 firstly using LIKE, which doesn\u2019t use\nfull-text search, and then using MATCH, which does. We will go back to using\nthe rqlite shell, and enable timings too.\n\n    \n    \n    $ rqlite -H 34.67.9.228 Welcome to the rqlite CLI. Enter \".help\" for usage hints. Connected to http://127.0.0.1:4001 running version v8.23.3 34.67.9.228:4001> .timer on 34.67.9.228:4001> SELECT * FROM logs WHERE entry LIKE '%SuperSecretAgent%' 47.39.156.135 - - [01/Apr/2022:16:48:22 +0200] \"HEAD /libraries/addurl.php HTTP/1.1\" 404 0 \"-\" \"SuperSecretAgent (http://www.owasp.org/index.php/Category:OWASP_DirBuster_Project)\" \"-\" Run Time: 2.458101 seconds 34.67.9.228:4001>\n\nI executed this query a few times, and it usually took around two second to\nreturn.\n\nNow, let\u2019s do a search.\n\n    \n    \n    $ rqlite -H 34.67.9.228 34.67.9.228:4001> SELECT * FROM logs WHERE entry MATCH 'SuperSecretAgent' | 47.39.156.135 - - [01/Apr/2022:16:48:22 +0200] \"HEAD /libraries/addurl.php HTTP/1.1\" 404 0 \"-\" \"SuperSecretAgent (http://www.owasp.org/index.php/Category:OWASP_DirBuster_Project)\" \"-\" Run Time: 0.000214 seconds 34.67.9.228:4001>\n\nThat\u2019s a big difference in speed \u2013 about 10,000 times faster! Of course,\nthat\u2019s the point of search, but it\u2019s compelling demonstration of how much\nfaster search can be relative to doing a full scan of the table.\n\n## Next Steps\n\nYou could test rqlite\u2019s resilience by taking one of the nodes offline; the\ncluster will still operate smoothly, continuing to index data and respond to\nsearch queries. Once the offline node is back online, it will automatically\nrejoin the cluster without any need for manual intervention.\n\nAdditionally, you can improve how you model data. For example, using a JSON\nschema for the log data means you could convert the Apache log lines into JSON\ndocuments on the fly before indexing. This change would let you run more\ndetailed queries such as sorting search results by timestamp, and performing\nbasic analytics on the log data.\n\n## Get Started\n\nTo see what rqlite can do, download it and join the Slack channel for more\ndiscussions, support, and to collaborate with others.\n\nFacebookTwitterEmailShare\n\ndatabaselogsprogrammingpythonrqlitesearchsqlite\n\n### Leave a Reply Cancel reply\n\n## Philip O'Toole\n\n# Summary\n\nMy name is Philip O\u2019Toole and I am an experienced software engineering leader\nfrom Ireland. Based in the Greater Pittsburgh area, I am the creator of\nrqlite, a distributed database built on SQLite. I also work at Google,\nmanaging software development teams building large scale Logging systems. I\nhave an interest in all things related to software development, particularly\nLinux system software, databases, and distributed systems.\n\n  * How I found a bug in SQLite\n  * The strange economics of open-source software\n  * What new development managers should know\n  * Software development: it\u2019s got nothing to do with computers\n  * 7 years of open-source database development: lessons learned\n  * What I learned from programming databases\n  * Coding like it\u2019s 1999\n  * 400 days of Go\n  * Is node.js just a stopgap?\n  * Replicating SQLite using Raft Consensus\n\n# Tags\n\n  * aws\n  * bleve\n  * book review\n  * C++\n  * cassandra\n  * cloud\n  * conference\n  * database\n  * data structures\n  * design\n  * distributed systems\n  * ec2\n  * ekanite\n  * elasticsearch\n  * fedora\n  * go\n  * google\n  * hashicorp\n  * influxdb\n  * java\n  * kafka\n  * kubernetes\n  * laptop\n  * leadership\n  * linux\n  * logs\n  * meetups\n  * node.js\n  * open-source\n  * operations\n  * percolate\n  * podcast\n  * programming\n  * python\n  * quality\n  * raft\n  * rqlite\n  * search\n  * speaking\n  * sqlite\n  * storm\n  * syslog\n  * time-series\n  * video\n  * whitepaper\n\n# Archives\n\n# Recent Comments\n\n  * HacKan on How I found a bug in SQLite\n  * HacKan on How I found a bug in SQLite\n  * Euphorbium on How I found a bug in SQLite\n  * Philip O'Toole on Replacing Postgres with rqlite\n  * Philip O'Toole on Replacing Postgres with rqlite\n\nAll opinions are strictly my own, and not of my employer.\n\nCopyright (c) Philip O\u2019Toole 2009-2023\n\n\u2713\n\nThanks for sharing!\n\nAddToAny\n\nMore...\n\n", "frontpage": false}
