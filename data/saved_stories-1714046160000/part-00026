{"aid": "40152554", "title": "llamafile v0.8", "url": "https://github.com/Mozilla-Ocho/llamafile/releases/tag/0.8", "domain": "github.com/mozilla-ocho", "votes": 1, "user": "birriel", "posted_at": "2024-04-25 02:13:59", "comments": 0, "source_title": "Release llamafile v0.8 \u00b7 Mozilla-Ocho/llamafile", "source_text": "Release llamafile v0.8 \u00b7 Mozilla-Ocho/llamafile \u00b7 GitHub\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nMozilla-Ocho / llamafile Public\n\n  * Notifications\n  * Fork 681\n  * Star 13.8k\n\n# llamafile v0.8\n\nLatest\n\nLatest\n\njart released this 24 Apr 22:05\n\n\u00b7 2 commits to main since this release\n\n0.8\n\n82f87bd\n\nllamafile lets you distribute and run LLMs with a single file\n\nllamafile is a local LLM inference tool introduced by Mozilla Ocho in Nov\n2023, which offers superior performance and binary portability to the stock\ninstalls of six OSes without needing to be installed. llamafile goes 2x faster\nthan llama.cpp and 25x faster than ollama for some use cases like CPU prompt\nevaluation. It has a fun web GUI chatbot, a turnkey OpenAI API compatible\nserver, and a shell-scriptable CLI interface which together put you in control\nof artificial intelligence.\n\nThis release further improves performance and introduces support for new\nmodels.\n\n  * Support for LLaMA3 is now available\n  * Support for Grok has been introduced\n  * Support for Mixtral 8x22b has been introduced\n  * Support for Command-R models has been introduced\n  * MoE models (e.g. Mixtral, Grok) now go 2-5x faster on CPU 4db03a1\n  * F16 is now 20% faster on Raspberry Pi 5 (TinyLLaMA 1.1b prompt eval improved 62 -> 75 tok/sec)\n  * F16 is now 30% faster on Skylake (TinyLLaMA 1.1b prompt eval improved 171 -> 219 tok/sec)\n  * F16 is now 60% faster on Apple M2 (Mistral 7b prompt eval improved 79 -> 128 tok/sec)\n  * Add ability to override chat template in web gui when creating llamafiles da5cbe4\n  * Improve markdown and syntax highlighting in server (#88)\n  * CPU feature detection has been improved\n\n### Downloads\n\nYou can download prebuilt llamafiles from:\n\n  * https://huggingface.co/jartine llamafiles quantized and compiled by us\n\n  * https://huggingface.co/models?library=llamafile llamafiles built by our user community\n\n### Errata\n\n  * The new web gui chat template override feature isn't working as intended. If you want to use LLaMA3 8B then you need to manually copy and paste the chat templates from our README into the llamafile web GUI.\n  * The llamafile-quantize program may fail with an assertion error when K-quantizing weights from an F32 converted file. You can work around this by asking llama.cpp's convert.py script to output an FP16 GGUF file, and then running lllamafile-quantize on that instead.\n\n10 people reacted\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
