{"aid": "40132194", "title": "AI and the Web newsletter [2024-04]: Burn, WebGPU and WASM news, Llama 3, Phi 3", "url": "https://ai-and-the-web.beehiiv.com/p/2024-04-nathaniel-simard-burn-webgpu-llama-3", "domain": "ai-and-the-web.beehiiv.com", "votes": 1, "user": "felladrin", "posted_at": "2024-04-23 14:09:28", "comments": 0, "source_title": "2024-04 edition of AI & the web", "source_text": "2024-04 edition of AI & the web\n\nThis Website Uses Cookies\n\nRead our privacy policy and terms of use for more information.\n\n  * AI & the Web\n  * Posts\n  * 2024-04 edition of AI & the web\n\n# 2024-04 edition of AI & the web\n\n## Nathaniel Simard on Burn, WebGPU & WASM developments, Llama 3 and Phi 3\nreleases\n\nJan Schulte April 23, 2024\n\n# tl;dr\n\nNathaniel Simard on Burn & Tracel AI | WebGPU gains further browser support | WASM gets better Tooling | Llama 3 & Phi 3 released | MuiscGen comes to the browser\n\n## Table of Contents\n\n  * tl;dr\n\n  * AI conversations\n\n    * Nathaniel Simard on Burn\n\n  * Latest developments\n\n    * WebGPU support\n\n    * WASM(I/O)\n\n    * Llama 3\n\n    * Phi 3\n\n    * wllama\n\n    * Ratchet\n\n  * Showcases\n\n    * MiniSearch\n\n    * MusicGen Web\n\n  * Upcoming conferences\n\n  * CHANGELOG.md\n\n  * On our own behalf\n\n# AI conversations\n\n## Nathaniel Simard on Burn\n\nNathaniel is founder & CEO of the startup Tracel AI and inventor of the burn\nML framework. On Friday we interviewed him on burn, his company and the state\nof AI in the web.\n\nJan: Nathaniel, we're excited to have you as our guest today! To kick off our\nconversation, could you please introduce yourself and share a bit about your\nbackground and what got you into ML?\n\nNathaniel: Sure, I'm the creator of Burn, a deep learning framework written in\nRust, and the founder of Tracel AI. I started coding in the first year of\nuniversity, where I was studying mechanical engineering, but I quickly\nswitched to software engineering after, since I instantly fell in love with\nprogramming. Then I explored different facets of the field from backend to\nfrontend development, and I decided to start my career as a consultant focused\non software quality. After some time, I wanted to go deeper into AI, since I\nwas always interested in the process of learning, so I enrolled for a master's\ndegree at MILA.\n\nJ: According to Github you started to work on burn in summer 2022. Since then\nhas already earned over 7000 stars on GH. What was your initial motivation to\ndevelop a new ML framework?\n\nN: I always had a side project going on, for fun mostly and to learn new\nthings. I wanted to explore asynchronous and sparse neural network\narchitectures, where each sub-network can learn and interact with other sub-\nnetworks asynchronously and independently. I wasn't able to actually create\nsomething useful because I needed fine control over the gradients and the\nconcurrency primitives, which is not easily done with Python and PyTorch. At\nthe same time, I was working on machine translation models at my current job,\nand it was quite painful to put models into production. I decided to switch my\nside project to a new deep learning framework, with more flexibility regarding\ngradients and concurrency primitives as well as being more reliable and easier\nto deploy on any system.\n\nJ: Amazing to see that this was born out of a side project! I feel the\nstruggle with concurrency in Python and this is something where Rust really\nshines. What are some of the other key features that makes burn special?\n\nN: I think there are two things that really set Burn apart. First, almost all\nneural network structures are generic over the backend. The goal is that you\ncan ship your model with almost no dependency, and anybody can run it on their\nhardware with the most appropriate backend, even embedded devices without an\noperating system. Second, Burn really tries to push the boundaries of what is\npossible in terms of performance and flexibility. It offers a fully eager API,\nbut also operation fusion and other optimizations that are normally only found\nin static graph frameworks. The objective is that you don't have to choose\nbetween portability, flexibility, and performance; you can have it all!\n\nJ: Let's dig a bit deeper into that. From a developer's point of view, how\nwould burn simplify my model code in comparison to let's say pytorch?\n\nN: It really depends on what you are building. For training, it's trivial to\nset up a multi-GPU setup on one node to implement data parallelism; no need to\nspawn multiple processes to execute the same program multiple times using low-\nlevel tensor synchronization. You can simply send a model to multiple threads\nwith their associated device and collect the gradients from each thread before\nupdating the reference model. It simplifies metrics logging, optimization,\neverything really. On top of that, it works with any hardware. For general\nuse, you don't have to worry about tensor layout or if a tensor is contiguous;\nwe take care of that part so that you can focus on the modeling part. The\ntensor API is fully functional and stateless, meaning that each operation\nreturns a new tensor, even operations that do semantically modify a tensor,\nlike slice_assign; the result will be returned. It may seem like a waste to\nnot reuse tensors, but in fact, we do reuse the tensor's data automatically,\nso users don't have to think about that. For instance, there is no in-place\noperation available in Burn; every place we can do an in-place operation or\nreuse a tensor's buffer, we do. Long story short, we provide clean APIs that\nautomatically handle a lot of the complexity of building models while\nperforming all sorts of optimization.\n\nJ: One of the recent exciting developments in burn was the development of a\nnew backend for JIT kernel fusion. You have written a very insightful\nexplanation about it on your blog (which I highly recommend our readers to\ncheck out). Can you tell us in a nutshell how it works and what performance\ngains you could achieve with it?\n\nN: Sure, the way fusion works in Burn is via a backend decorator that captures\nall tensor operations into a high-level representation without any data\ninvolved. Then the goal is to find potential optimizations on that\nrepresentation, which for now are mostly fusing element-wise operations\ntogether. The performance gains depend on the model you are building and how\nmany custom element-wise functions you are using. For the transformer provided\nin Burn, I think it's around a 20% speedup, but this will improve over time.\nThe biggest benefit is that you don't have to do many optimizations yourself\nand write custom GPU kernels; the compiler does that for you.\n\nJ: burn also runs in the browser (see e.g. MNIST demo https://burn.dev/demo/).\nWhat is the current state of browser support in burn and what are your plans\nfor the web?\n\nN: I think Burn has really great support for the Web, we even support WebGPU!\nWe currently don't have specific plans it should just work like any other\nWebAssembly application written in Rust. We might add more examples in the\nfuture based on community feedback.\n\nJ: What new features can we expect in burn throughout this year?\n\nN: First, we are working on adding more JIT runtimes, so you can expect even\nbetter hardware support than we currently have. But the biggest features are\ngoing to be quantization and distributed training/inference. The focus will\nthen be on improving and optimizing every part of Burn.\n\nJ: Distributed training/inference sounds exciting. Can you already share some\nmore information on that or is it still in the planning?\n\nN: We are going to implement it as a remote backend, generic over a protocol.\nTherefore, you'll be able to use it with any backend and even test your\ndistributed algorithms locally.\n\nJ: Let's talk about another topic. You founded Tracel AI last summer together\nwith your colleague Louis. Where are you standing now and what are your plans\nfor Tracel AI?\n\nN: I have big plans for Trace AI. The goal is to improve the current state of\nthe AI infrastructure landscape, starting with Burn. We are going to support\nthe community built around Burn with complementary products in the future.\nHopefully, we can make a positive impact in the field and help spread AI\nfurther.\n\nJ: A glance into the crystal ball: What big trends do you think we will see in\nthe next five years in AI in general and in Web AI specifically?\n\nN: I think models will need to become way more efficient; the framework is\nonly one part of it, but they should be able to adjust their compute and\nmemory requirements based on the task difficulty. Models could learn to\nincrease their operational efficiency. This could create a funny dynamic where\nthe more you train a network, the cheapest it is to deploy, but also fine-\ntune. Applied for the web, you could fine-tune a model with a way bigger cost\nto memory so that the model itself prunes some neurons to run efficiently on\nusers' hardware. In terms of application, I think we will see a lot of on-\ndevice AI models accelerated by NPU with privacy in mind. Probably the most\nimportant application will be in robotics, where AI will have a physical\npresence and impact in our world.\n\nJ: Nathaniel, thank you so much for joining us today and sharing your\nperspectives. We really appreciate your time and wish you all the best for\nyour future!\n\nN: Thanks a lot for having me!\n\n# Latest developments\n\n## WebGPU support\n\nThe WebGL + WebGPU was held in March 2024 on the state of WebGPU with some\ninteresting demos featuring WebGPU (though more focussed on graphics rather\nthan AI):\n\nThe most interesting piece of information to take away from this meetup was\nthe current WebGPU implementation status. WebGPU is now available in Safari\nTechnology Preview for testing and Firefox plans to ship to release by the end\nof this year. Exciting times ahead for GPU accelerated AI in the browser!\n\n## WASM(I/O)\n\nDevelopments in WASM are especially interesting for AI in the browser, since\nalmost all Web ML frameworks (onnx, tensorflow.js, ratchet, web-mlc, burn)\neither use a WASM backend when no GPU is available or are entirely written in\nC++ or Rust and then compiled down to WASM.\n\nIn March the WASM I/O conference was held in Barcelona with many captivating\ntalks. One to mention in particular was the talk by Google WASM @ Google on\nWASM support in Chrome, the tensorflow.js WASM backend and other insights and\ndemos:\n\nSome of the highlights:\n\n  * Google is working on tools that perform optimisations like tree-shaking or code splitting. This is great because WASM is notorious for having large binary sizes and therefore slow initial loading times.\n\n  * Garbage Collection support in WASM, aka WASM-GC, is going to ship in November, which might enable GC-based programming languages to build for the browser.\n\n  * Better support for SIMD in WASM, which e.g. powers the real-time background blur in Google Meet\n\n## Llama 3\n\nObviously I couldn't miss probably the most anticipated model release this\nyear: Llama 3. Read the full story here: Introducing Meta Llama 3: The most\ncapable openly available LLM to date.\n\nDwarkesh Patel had a very insightful interview with Marc Zuckerberg on Llama\n3. Remarkably, Marc also mentioned the plans to train a smaller Llama 3 model\nin the realm of 2B parameters, which would be much more usable in a browser\nenvironment.\n\n## Phi 3\n\nFresh from the oven: Microsoft just released Phi-3, a tiny yet powerful 3.8B\nparameter model which we will probably soon see in web frameworks like\ntransformers.js or ratchet.\n\nPaper page - Phi-3 Technical Report: A Highly Capable Language Model Locally\non Your Phone\n\nJoin the discussion on this paper page\n\nhuggingface.co/papers/2404.14219\n\n## wllama\n\nA new WASM build of Llama.cpp appeared that seems to be performing really well\n(see minisearch for a showcase):\n\nGitHub - ngxson/wllama: WebAssembly binding for llama.cpp - Enabling in-\nbrowser LLM inference\n\nWebAssembly binding for llama.cpp - Enabling in-browser LLM inference -\nngxson/wllama\n\ngithub.com/ngxson/wllama\n\n## Ratchet\n\nOur last episode's interviewee Christopher is making headway with Ratchet\nwhich now runs the Phi-2 at around 24 tok/sec (on an M3 Max). While there is\nstill some way to go to make it usable on lower end devices a very interesting\nfact is that this runs with only 15% performance loss in Chrome in comparison\nto running it natively. Chrome seems to be doing a great job on supporting\nWebGPU.\n\n# Showcases\n\n## MiniSearch\n\nVictor built MiniSearch which leverages the aforementioned wllama and uses a\nRAG pipeline to retrieve live search results. Given that this runs fully in\nthe browser and doesn't use WebGPU it runs surprisingly fast. Check it out\nhere\n\nMiniSearch\n\nMinimalist web-searching app with an AI assistant that is always available and\nruns directly from your browser.\n\nfelladrin-minisearch.hf.space\n\n## MusicGen Web\n\nJoshua added MusicGen support to transformers.js and built a Huggingface Space\nto experiment with it:\n\nMusicGen Web - a Hugging Face Space by Xenova\n\nIn-browser text-to-music w/ Transformers.js!\n\nhuggingface.co/spaces/Xenova/musicgen-web\n\nEnzo took that to a next level by creating a whole Jukebox running right in\nthe browser:\n\n\u2014 AI Jukebox \u2014 - a Hugging Face Space by enzostvs\n\nGenerate music powered by AI\n\nhuggingface.co/spaces/enzostvs/ai-jukebox\n\n# Upcoming conferences\n\nGoogle I/O 2024\n\nDon't miss our biggest developer conference, featuring product news and\ninnovations from Google. Tune in to I/O for livestreamed keynotes and\ntechnical sessions on demand.\n\nio.google/2024\n\n# CHANGELOG.md\n\n  * Burn v0.13.0 released - Many new features, improvements & bug fixes\n\n  * Transformers.js 2.17.0 released - Improves text generation and adds binary embedding quantization support\n\n# On our own behalf\n\nAre you working on a cool project with AI in the browser that you\u2019d like to\nshare? I\u2019d be especially interested in art projects using AI in the browser.\nSend us an email to ai-and-the-web@betalyra.pt.\n\n#### Keep reading\n\n## 2024-03 edition of AI & the web\n\nRatchet, ONNX WebGPU and more\n\nAI & the Web\n\nThe latest trends & advancements in Web AI\n\nHome\n\nPosts\n\n\u00a9 2024 Betalyra Soc. Unip. Lda..\n\nPrivacy Policy\n\nTerms of Use\n\nPowered by beehiiv\n\n", "frontpage": false}
