{"aid": "40175680", "title": "Conservatism predicts aversion to consequential Artificial Intelligence (2021)", "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8687590/", "domain": "nih.gov", "votes": 1, "user": "andy99", "posted_at": "2024-04-27 00:00:35", "comments": 0, "source_title": "Conservatism predicts aversion to consequential Artificial Intelligence", "source_text": "Conservatism predicts aversion to consequential Artificial Intelligence - PMC\n\nSkip to main content\n\nAn official website of the United States government\n\nThe .gov means it\u2019s official. Federal government websites often end in .gov or\n.mil. Before sharing sensitive information, make sure you\u2019re on a federal\ngovernment site.\n\nThe site is secure. The https:// ensures that you are connecting to the\nofficial website and that any information you provide is encrypted and\ntransmitted securely.\n\nLog in\n\n#### Account\n\nLogged in as: username\n\n  * Dashboard\n  * Publications\n  * Account settings\n  * Log out\n\nAccess keys NCBI Homepage MyNCBI Homepage Main Content Main Navigation\n\nPreview improvements coming to the PMC website in October 2024. Learn More or\nTry it out now.\n\n  * Advanced Search\n  * User Guide\n\n  * Journal List\n  * PLoS One\n  * PMC8687590\n\nAs a library, NLM provides access to scientific literature. Inclusion in an NLM database does not imply endorsement of, or agreement with, the contents by NLM or the National Institutes of Health. Learn more: PMC Disclaimer | PMC Copyright Notice\n\nPLoS One. 2021; 16(12): e0261467.\n\nPublished online 2021 Dec 20. doi: 10.1371/journal.pone.0261467\n\nPMCID: PMC8687590\n\nPMID: 34928989\n\n# Conservatism predicts aversion to consequential Artificial Intelligence\n\nNoah Castelo, Conceptualization, Formal analysis, Investigation, Methodology,\nProject administration, Writing \u2013 original draft, Writing \u2013 review & editing^\n1 ,^* and Adrian F. Ward, Conceptualization, Formal analysis, Methodology,\nWriting \u2013 review & editing^ 2\n\n### Noah Castelo\n\n^1 Department of Marketing, Business Economics, and Law, University of\nAlberta, Edmonton, Alberta, Canada\n\nFind articles by Noah Castelo\n\n### Adrian F. Ward\n\n^2 Department of Marketing, University of Texas at Austin, Austin, Texas,\nUnited States of America\n\nFind articles by Adrian F. Ward\n\nAli B. Mahmoud, Editor^\n\nAuthor information Article notes Copyright and License information PMC\nDisclaimer\n\n^1 Department of Marketing, Business Economics, and Law, University of\nAlberta, Edmonton, Alberta, Canada\n\n^2 Department of Marketing, University of Texas at Austin, Austin, Texas,\nUnited States of America\n\nSt John\u2019s University, UNITED KINGDOM\n\nCompeting Interests: The authors have declared that no competing interests\nexist.\n\n* E-mail: ac.atreblau@oletsacn\n\nReceived 2021 May 12; Accepted 2021 Dec 2.\n\nCopyright \u00a9 2021 Castelo, Ward\n\nThis is an open access article distributed under the terms of the Creative\nCommons Attribution License, which permits unrestricted use, distribution, and\nreproduction in any medium, provided the original author and source are\ncredited.\n\n## Associated Data\n\nSupplementary Materials\n\n    \n\nS1 File: (DOCX)\n\npone.0261467.s001.docx (69K)\n\nGUID: 52F6F0B9-BB4E-40CF-9B8C-35A06CEB85FC\n\nData Availability Statement\n\n    \n\nThe authors have uploaded their data to OSF at the following:\nhttps://osf.io/5wehr/?view_only=7f2ef4c8fcea4c0ea05a706987df9e96.\n\nGo to:\n\n## Abstract\n\nArtificial intelligence (AI) has the potential to revolutionize society by\nautomating tasks as diverse as driving cars, diagnosing diseases, and\nproviding legal advice. The degree to which AI can improve outcomes in these\nand other domains depends on how comfortable people are trusting AI for these\ntasks, which in turn depends on lay perceptions of AI. The present research\nexamines how these critical lay perceptions may vary as a function of\nconservatism. Using five survey experiments, we find that political\nconservatism is associated with low comfort with and trust in AI\u2014i.e., with AI\naversion. This relationship between conservatism and AI aversion is explained\nby the link between conservatism and risk perception; more conservative\nindividuals perceive AI as being riskier and are therefore more averse to its\nadoption. Finally, we test whether a moral reframing intervention can reduce\nAI aversion among conservatives.\n\nGo to:\n\n## Introduction\n\nArtificial intelligence (AI) has the potential to revolutionize society, with\nimpact ranging from the broadest reaches of industry and policy to the\nminutiae of daily life. Recent progress in AI research has enabled computers\nto perform tasks traditionally thought of as requiring human intelligence.\nToday, AI can drive cars by accurately tracking the environment and making\ndecisions in real time; it can diagnose diseases and make treatment\nrecommendations based on a comprehensive understanding of all published\nmedical research; it can even teach itself how to play complex games that\nrequire strategic planning. In all of these cases and many more it can match\nor outperform even the most expert humans [1\u20134]. AI has the potential to\nimprove outcomes for many people, in many domains, and indeed is already doing\nso. However, the benefits promised by AI are contingent on people accepting\nand using this technology\u2014and acceptance is far from guaranteed.\n\nIf people are averse to relying on AI, the ability of this technology to\npositively impact society as a whole could be hindered. For example, if AI can\noutperform humans at safely driving cars but people are averse to relying on\nAI for this task, traffic deaths might remain higher than they would be if\ndriverless cars were more readily embraced. More generally, active distrust or\naversion towards AI could impact the continued technical development and\nfunding of AI research, as well as the public policies that are created to\nregulate it, which could further curtail the technology\u2019s ability to\npositively impact society.\n\nIn this research, we first show that political conservatism is a strong\npredictor of AI aversion and then test interventions that can reduce this\naversion. As an individual difference factor, conservatism is both\npsychologically meaningful and practically relevant. At a psychological level,\nconservatism is not just about how one votes; it is about how one perceives\nthe world. Conservatives possess a heightened sensitivity to threat and\nuncertainty [5,6]; we find that this sensitivity (operationalized in terms of\nperceived risk) is responsible for the link between conservatism and AI\naversion. The potential consequences of this association are far-reaching. For\nexample, more positive perceptions of AI among liberals could lead liberal\nstates and citizens to adopt AI-based technologies more quickly than their\nconservative counterparts, and therefore to reap disproportionate benefits\nfrom those technologies. In addition to these potentially inequality-inducing\ndynamics, ideological or partisan gridlock on the issue of regulating AI could\nhamper effective and much-needed national legislation on issues such as\nautonomous vehicles and the use algorithms in hiring and sentencing decisions.\nUnderstanding ideological sources of aversion to AI is therefore an important\ntask for psychologists.\n\n### Aversion to algorithms and AI\n\nAn artificially intelligent agent is something that receives information from\nthe environment and uses that information to perform actions [7]. How do\npeople perceive these technologies, and how do those perceptions influence the\ndegree to which people are willing to use AI? The answers to these questions\nare important because AI\u2019s ability to positively impact society is partly a\nfunction of people\u2019s adoption and use. Note that our research focuses on\nspecific applications of AI such as driverless cars and automated medical\ndiagnoses\u2014often referred to as \u201cartificial narrow intelligence,\u201d as opposed to\na hypothetical AI that has human-like abilities in all areas, often referred\nto as \u201cartificial general intelligence.\u201d\n\nThe literature on individual\u2019s perceptions and adoption of AI (and of\nalgorithms more generally) generally shows that humans prefer to rely on other\nhumans rather than on algorithms to make decisions [8,9]. This tendency has\nbeen documented in many domains, such as admitting MBA students [10], hiring\nemployees [11], receiving medical treatment [12], and forecasting stock prices\n[13]\u2014despite the fact that algorithms outperform humans in these and many\nother domains [14]. Furthermore, this tendency exists when people actively\ndecide whether to rely on algorithms after seeing them perform [15] as well as\nwhen people consider algorithmic decision-making in prospect (i.e., in\nhypothetical scenarios) [16]. This literature suggests that people may be\nunlikely to rely on and trust AI as much as they would a human in many\ndomains. Although some research has identified situations in which people\nactually prefer to rely on algorithms more than on humans [17], this remains\nan exception rather than the rule in the literature.\n\nMore recent research has begun to explore potential moderators of this\naversion to relying on automated forms of decision making (including AI).\nResearch has shown that trust in algorithms can be increased by giving people\na small degree of control over the algorithm\u2019s output [15] or by explaining\nhow the algorithm works [18]. Furthermore, algorithm aversion is lower for\ntasks that seem highly objective (vs. subjective) in nature [8]. Our research\ncontributes to this stream of work, specifically focusing on how individual\ndifferences\u2014in this case, ideological conservatism\u2014impact perceptions of a\ntechnology with the proven potential to transform people\u2019s lives.\n\n### Conservatism and risk\n\nConservatism has been defined as \u201cthe tendency to prefer safe, traditional and\nconventional forms of institutions and behavior\u201d [19]. In the most basic\nsense, conservatism is associated with a preference for tradition and\nstability, whereas liberalism is associated with a preference for innovation\nand reform [20]. Supporting these classical conceptions of conservatism, a\nmore recent meta-analysis proposed the uncertainty-threat model of political\nconservatism, which states that conservatism consists of resistance to change\nand opposition to equality, both of which serve a fundamental need to reduce\nthreat and uncertainty [5,6].\n\nThis conceptualization is supported by research indicating that conservatism\nis fundamentally associated with a greater focus on negativity, fear, and\nrisk. A wealth of research has demonstrated that conservatives react more\nstrongly than liberals to environmental stimuli that are uncertain, ambiguous,\nor unexpected in nature, across both psychological and physiological measures\nof reactivity [21]. This tendency is especially pronounced for stimuli that\nare negative or potentially threatening [22]. For example, compared to\nliberals, conservatives have stronger physiological reactions (measured by\nelectrical skin conductance) to negatively valenced stimuli [23], are more\nlikely to perceive emotionally ambiguous faces as threatening [24], and are\nmore likely to believe warnings about potential hazards [25]. This research\nsuggests that conservatism reflects a fundamental mindset that structures how\npeople perceive and interact with the world around them, and that a heightened\nsensitivity to risk lies at the heart of this mindset.\n\nNote that conservatism can be broken down into social and fiscal conservatism\nor considered as a more general conservatism\u2014we focus primarily on social\nconservatism in our studies as this aspect has received the most attention in\npsychological research, where conservatism is explicitly conceptualized as a\nform of social cognition [6].\n\n### Sources of perceived risk\n\nSeveral prominent conceptions of risk perception share the central idea that\nperceived risk is jointly determined by two factors: the uncertainty of an\noutcome, and the importance of that outcome\u2019s consequences. The classic\nconsequentialist perspective holds that something\u2019s perceived risk is a\nfunction of the importance of its potential consequences, multiplied by the\nlikelihood of those consequences occurring [26,27]. For example, having a\ncomplex surgery has very important potential consequences (i.e., a surgical\nerror leading to disability or death), which makes the surgery potentially\nrisky; however, the overall perceived risk of that surgery will also depend on\nhow likely a surgical error is perceived to be (i.e., on the skill and track\nrecord of the surgeon). More recent approaches have found that perceived risk\nvaries along the similar dimensions of dread, which is affective and concerns\nperceived \u201ccatastrophic potential,\u201d and risk of the unknown, which is a more\ncognitive assessment of probabilities [28,29]. For example, nuclear war is\nhighly dreaded, but the perceived probability that it will occur is relatively\nlow [29].\n\nThis research examines how the dread component of risk, which parallels the\nimportance of potential consequences component, may amplify political\ndifferences in AI aversion for particularly consequential tasks. Our\ndefinition of consequentialness parallels the classic definition of this\nconcept in the risk perception literature: a task is more consequential if\nfailing the task has serious or significant consequences, which can include\nfinancial, physical, social, or psychological consequences. For example, using\nAI to control driverless cars clearly has more important potential\nconsequences than using AI to recommend a movie on Netflix; the risk of death\nis more consequential than the risk of watching a boring movie. In this\nresearch, we use between-task variations in consequentialness to examine how\nconsequentialness may amplify political differences in perceived risk\u2014and,\nconsequently, in willingness to rely on AI.\n\n### The present research\n\nThe preceding discussions of AI, conservatism, and risk suggest a conceptual\nmodel of how conservatism might affect aversion to AI, which we depict below\n(see Fig 1). Because conservatism is associated with greater propensity to\nperceive risk, we expect that more conservative people will be more likely to\nsee AI as risky. Because one of the major components of perceived risk is the\nconsequentialness of potential outcomes, we expect that the consequentialness\nof the task that AI is being used for will moderate the effect of conservatism\non perceived risk, such that the effect will be eliminated for tasks that are\nrelatively inconsequential. Finally, we expect that the perceived riskiness of\nAI will mediate the effect of conservatism on individuals\u2019 aversion to AI.\n\nOpen in a separate window\n\nFig 1\n\nA conceptual model of Conservatism\u2019s effects on AI aversion.\n\nThis model guides our research into how a common psychological variable\naffects people\u2019s attitudes towards a technology with perhaps unparalleled\npotential to improve outcomes across countless domains. Our focus on\nindividual differences contrasts with previous approaches to understanding\nreliance on algorithms, allowing us to explain some of the heterogeneity in\nresponses to emerging technologies and suggesting potential interventions for\nchanging those attitudes.\n\nWe test this model in 3 studies. The primary relationship that we focus on in\nthe paper is the relationship between conservatism and AI aversion. We\noperationalize aversion as comfort relying on AI in our first study (such that\nlower comfort reflects greater AI aversion), and as trust in AI in subsequent\nstudies (such that lower trust reflects greater AI aversion). Study 1 provides\ninitial evidence for this relationship and shows that perceived risk plays a\nmediating role. Study 2 shows that this pattern holds for consequential tasks,\nbut not for inconsequential tasks. Study 3 tests the full predicted pattern of\nmoderated mediation: conservatism predicts trust in AI because it predicts\nrisk; because risk is less relevant for inconsequential tasks, the divide\nbetween liberals and conservatives only exists for consequential tasks.\nFinally, Studies 4 and 5 test whether a moral reframing intervention [30,31]\ncan reduce AI aversion among conservatives without explicitly attempting to\nreduce perceived risk. In all studies, we report all measures, manipulations\nand exclusions. Sample sizes for each study were determined prior to any data\nanalysis.\n\nGo to:\n\n## Study 1\n\nIn our first study, we examine how conservatism affects AI aversion. We\noperationalize AI aversion by asking participants how comfortable they would\nfeel relying on AI for two tasks. We focus our examination on two particularly\npromising (and consequential) applications of AI: for self-driving cars, and\nfor diagnosing diseases. These applications were validated to be high in\nperceived consequentialness in the pretest to Study 2 (see Table 1). We test\nthe effect of conservatism on trust in AI both in isolation and alongside\nother potentially important demographic factors (e.g., income, education\nlevel). We then test for mediation via our proposed mechanism: perceived risk.\n\n### Table 1\n\nDirect and indirect effects of social conservatism on perceived risk of and\ncomfort with AI (standardized \u03b2 coefficients).\n\nEffect| Risk| Total| Comfort  \n---|---|---|---  \nDirect| Indirect  \nPath| a| c| c\u2019| b  \nMedical Diagnosis| .13^**| -.12^*| -.04| -.67^**  \nDriverless Cars| .21^**| -.20^**| 0.03| -.76^**  \nIn General| .06| -.14^**| -.09^*| -.58^**  \n  \nOpen in a separate window\n\nNote: Age, gender, education, and income are included as covariates in the\nmediation models reported above.\n\n^\u2020 = p < .10\n\n* = p < .05\n\n** = p < .01.\n\n### Method\n\n400 MTurk users (44% female, mean age = 35.3) read the following introduction\nto AI (see Appendix for complete stimuli):\n\n> \u201cEvery year, Artificial Intelligence (AI) becomes capable of performing new\n> tasks that only humans could do before, from beating chess grandmasters and\n> the best human players on the game show Jeopardy!, to driving cars and\n> diagnosing diseases.\n\n> Ongoing developments in Artificial Intelligence include the creation of\n> systems intended to handle increasingly complex tasks\u2014for example, reading\n> medical journals, diagnosing diseases, delivering treatment recommendations,\n> and even controlling automated driverless cars.\u201d\n\nIn this first study, we also wanted to ensure that participants were\nsufficiently engaged with the material and were thoughtfully considering the\ntechnology and its implications. In addition to the basic introduction to AI,\nwhich describes some of the technology\u2019s benefits, we therefore also described\nsome of the potential risks associated with AI. Note that future studies\nreplicate the results of this study without explicitly describing these risks.\nParticipants in this study therefore also read the following:\n\n> \u201cMany people are worried that developing AI is risky for several reasons.\n> For example, AI can already do many jobs better than humans can, and the\n> number of such jobs is growing all the time\u2014including jobs like truck\n> drivers and fast food workers and even some kinds of lawyers and doctors.\n> One risk is therefore that developing AI will put many humans out of a job.\n\n> Another risk that people worry about is that AI could potentially become\n> even more intelligent than humans and start to develop its own goals. This\n> could mean that AI decides humans are no longer helpful for its development\n> and could start to manipulate or hurt humans.\u201d\n\nParticipants then reported their perceptions of how risky it is to rely on AI\nfor \u201cmedical tasks such as diagnosing diseases and providing treatment\nrecommendations\u201d and for \u201ccontrolling driverless cars\u201d and how comfortable\nthey would be relying on AI for these tasks. We also asked participants how\nrisky it is to rely on AI in general and how comfortable they are with the\nidea of relying on AI. Finally, participants reported their social\nconservatism, fiscal conservatism, and conservatism in general, and reported\nage, gender, income, and education, which are demographic variables that are\nsometimes associated with conservatism and may also be associated with\nresponses to new technologies. All measures (except demographics) used 0\u2013100\nscales. This research was approved by the IRB at Columbia University.\nParticipants in all studies provided informed consent to participate in the\nsurveys by clicking a button indicating their consent. All data and R code for\nthis article are available at\nhttps://osf.io/5wehr/?view_only=7f2ef4c8fcea4c0ea05a706987df9e96.\n\n### Results\n\nWe first report results for how social conservatism affects comfort relying on\nAI for specific tasks, since these main effects are our primary focus. We then\nreport how conservatism affects perceived risks of AI, which is our proposed\nmechanism of the main effects. We then report mediation analyses testing\nwhether perceived risks mediate the effects of conservatism on comfort relying\non AI. For each analysis, we report how conservatism affects the dependent\nvariable while controlling for a range of other potentially relevant\ndemographic variables. Tables Tables22 and and33 display the effects of each\nof those variables, along with conservatism, on each of our dependent\nvariables. The Web Appendix reports results using general and fiscal\nconservatism instead of social conservatism.\n\n### Table 2\n\nConservatism-Only and Conservatism + Demographics Models of Comfort with AI,\nStudy 1.\n\nModel| Conservatism Only| Conservatism + Demographics  \n---|---|---  \nDomain| Medical Diagnoses| Self -Driving Cars| General| Medical Diagnoses|\nSelf-Driving Cars| General  \nConservatism| -.12 (.05)^*| -.20 (.05)^**| -.14 (.04)^**| -.13 (.05)^*| -.19\n(.05)^**| -.15 (.05)^**  \nEducation:  \nHigh school| 17.13 (17.10)| 28.66 (17.49)| 34.76 (15.21)^*  \nSome college| 11.82 (18.82)| 30.53 (17.19)^\u2020| 31.24 (14.95)^*  \n2-year college| 17.96 (17.05)| 36.05 (17.43)^*| 38.89 (15.16)^*  \n4-year college| 15.52 (16.75)| 32.47 (17.13)^\u2020| 36.45 (14.89)^*  \nGraduate degree| 18.07 (17.27)| 33.10 (17.66)^\u2020| 37.54 (15.35)^*  \nIncome| -.01 (.91)| .69 (.93)| -0.53 (.81)  \nAge| .08 (.14)| -.23 (.14)^\u2020| 0.04 (.12)  \nFemale| -9.83 (2.97)^**| -10.68 (3.03)^**| -10.03 (2.63)^**  \nIntercept| 40.43 (3.42)^*| 34.51 (3.56)^**| 44.47 (3.09)^**| 36.24 (18.16)^*|\n24.15 (18.62)| 24.38 (16.17)  \nR ^ 2| .01| .04| .02| .05| .09| .07  \n  \nOpen in a separate window\n\nGender (\u201cFemale\u201d) is dummy coded with \u201cMale\u201d as the reference group.\n\nEducation is a dummy coded variable with \u201cless than high school\u201d as the\nreference group.\n\n^\u2020 = p < .10\n\n* = p < .05\n\n** = p < .01.\n\n### Table 3\n\nConservatism-Only and Conservatism + Demographics Models of Perceived Risk of\nAI, Study 1.\n\nModel| Conservatism Only| Conservatism + Demographics  \n---|---|---  \nDomain| Medical Diagnoses| Self -Driving Cars| General| Medical Diagnoses|\nSelf-Driving Cars| General  \nSocial Conservatism| .13 (.04)^**| .23 (.05)^**| .05 (.05)| .14 (.05)^**| .22\n(.05)^**| .08 (.05)  \nEducation:  \nHigh school| -12.47 (15.52)| -21.22 (15.91)| -25.17 (14.98)^\u2020  \nSome college| -17.13 (15.26)| -30.32 (15.64)^\u2020| -28.96 (14.72)^*  \n2-year college| -16.77 (15.48)| -33.80 (15.86)^*| -31.65 (14.93)^*  \n4-year college| -16.27 (15.20)| -28.72 (15.59)^\u2020| -27.25 (14.67)^\u2020  \nGraduate degree| -16.47 (15.67)| -32.51 (16.06)^*| -25.97 (15.12)^\u2020  \nIncome| -.01 (.91)| -0.46 (.85)| -0.81 (.80)  \nAge| -0.10 (.12)| 0.12 (.13)| -0.19 (.12)  \nFemale| 10.97 (2.69)^**| 10.00 (2.75)^**| 7.24 (2.59)^**  \nIntercept| 67.28 (3.11)^**| 72.79 (3.22)^**| 62.12 (2.99)^**| 72.30\n(16.48)^**| 82.25 (16.89)^**| 91.01 (15.90)^**  \nR ^ 2| .02| .05| .001| .07| .11| .04  \n  \nOpen in a separate window\n\nGender (\u201cFemale\u201d) is dummy coded with \u201cMale\u201d as the reference group.\n\nEducation variables are dummy coded with \u201cless than high school\u201d as the\nreference group.\n\n^\u2020 = p < .10\n\n* = p < .05\n\n** = p < .01.\n\n### Comfort\n\nWe first performed a series of OLS regressions assessing the relationship\nbetween conservatism and comfort relying on AI (i) for medical diagnoses, (ii)\nfor self-driving cars, and (iii) in general. For each application of AI, we\nassessed the effect of conservatism both in isolation and alongside other\ndemographic variables that might reasonably predict comfort relying on AI:\nage, gender, income, and education level. Controlling for these other factors,\nincreasing levels of conservatism predict decreasing comfort with AI for\nmedical applications (\u03b2 = -.13, p = .014), driverless cars (\u03b2 = -.19, p <\n.001), and in general (\u03b2 = -.15, p = .004). Conservatism and comfort (as well\nas risk in the next section) were standardized so that beta coefficients\nrepresent effect sizes. Power analysis revealed that this sample size had 99%\npower to detect these effect sizes, and that a sample size of 62 would be\nrequired to detect these effect sizes with 80% power. The only other variable\nto consistently affect comfort was gender: for both uses of AI and for comfort\nwith AI in general, men felt significantly more comfortable than women (see\nTable 2 for the effects of all demographic variables, noting that the\ndemographic variables are not standardized).\n\n### Risk\n\nNext, we performed a series of OLS regressions assessing the relationship\nbetween conservatism and perceived risk associated with AI, controlling for\nthe demographic covariates. We found that increases in conservatism predicted\nincreases in perceived risk of using AI for both specific applications\nincluded in this study: medical diagnoses (\u03b2 = .14, p = .005) and driverless\ncars (\u03b2 = .22, p < .001). The effect of conservatism on perceived risk of AI\nin general was directionally consistent with the results found for specific\napplications, but not statistically significant (\u03b2 = .08, p = .112). These\nregression coefficients are standardized. Again, the only other variable to\naffect trust was gender: for both uses of AI and for AI in general, men\nperceived significantly less risk than women. Full results of these analyses\nare shown in Table 3. Risk, comfort, and conservatism are standardized. As\nwith our analysis of trust in AI, parallel analyses replacing social\nconservatism with overall conservatism and fiscal conservatism replicated\nthese results (see web appendix for full results).\n\n### Mediation\n\nWe assessed the relationship between conservatism, perceived riskiness of AI,\nand trust in AI in a series of mediation models. We included age, gender,\nincome, and education as covariates in each mediation analysis; parallel\nanalyses excluding these covariates yield nearly identical results (please see\nweb appendix for these analyses). First, we tested whether the perceived risk\nof using AI for medical applications mediated the total effect of social\nconservatism on comfort using AI for medical applications (\u03b2 = -.12, t(399) =\n2.45, p = .016). Using Hayes (2013) PROCESS Model 4, we found evidence of\nmediation with a significant indirect effect of conservatism on comfort\nthrough risk (B = -.08, SE = .03, 95% CI = -.15 to -.02) and a non-significant\ndirect effect after controlling for perceived risk (\u03b2 = -.04, t(399) = .97, p\n= .332).\n\nSecond, we tested whether the perceived risk of using AI for driverless cars\nmediated the total effect of social conservatism on comfort using AI for\ndriverless cars (\u03b2 = -.20, t(399) = 4.08, p < .001). We once again found\nevidence of mediation with a significant indirect effect of conservatism on\ncomfort through risk (\u03b2 = -.16, SE = .04, 95% CI = -.23 to -.08) and a non-\nsignificant direct effect after controlling for perceived risk (\u03b2 = -.04,\nt(399) = .95, p = .344).\n\nSince conservatism did not affect perceived risk of using AI in general, risk\ndid not mediate comfort with AI in general (B = -.03, SE = .03, 95% CI = -.09\nto .02). Please see Fig 2 for full results. Note that we cannot conclude that\nconservatism causes risk perception from these data, since we did not\nmanipulate conservatism. Thus, as in most research studying the psychology of\nconservatism, this relationship is measured in terms of correlation rather\nthan causation. Similarly, the relationship between perceived risk and comfort\nwith AI is also correlational. This is also true for the mediation analysis\nreported in Study 3. These mediation analyses are therefore compatible with\nonly one of several possible models. Finally, note that while our R^2 values\nare low, suggesting that the independent variables we measure explain only a\nsmall portion of the variance in our dependent variables, they are\nnevertheless above zero and in the normal range for social psychology research\n[32], therefore suggesting that conservatism in particular is indeed a\nsignificant factor in explaining attitudes towards AI.\n\nOpen in a separate window\n\nFig 2\n\nMediation model and standardized coefficients, Study 1.\n\n### Discussion\n\nStudy 1 provides evidence that conservatism is associated with aversion to AI.\nFor two potentially transformational applications of AI\u2014medical diagnoses and\nself-driving cars\u2014increasing conservatism predicts decreasing trust in AI.\nThis relationship cannot be explained by other potentially relevant\ndemographic variables that may be associated with conservatism and/or\nperceptions of AI, such as age, income, or education level; the conservatism-\ntrust relationship is robust across models that include and exclude these\ncovariates. Rather, results from our mediation analyses suggest that the\nrelationship between conservatism and trust in AI may be attributable to\ndifferences in perceived risk. We further explore the potential causal\nmechanism in Study 2.\n\nInterestingly, we found that the links between conservatism, risk, and trust\nin AI were stronger when participants considered specific applications of AI\nthan when they thought about AI in general. We suspect that this discrepancy\nmay reflect the fact that many of the tasks for which AI can be used are\nrelatively inconsequential\u2014and therefore possess negligibly low levels of\nrisk. If general considerations of AI bring to mind a mix of\napplications\u2014including not just advanced medical procedures and self-driving\ncars, but also music recommendations and smart thermostats\u2014this may undermine\nthe relationship between conservatism and perceived risk, thus weakening the\nrelationship between conservatism and trust. This would be consistent with our\ntheoretically derived prediction that task consequentialness should moderate\nthe relationship between conservatism and perceived risk of AI. We test this\nhypothesis in Studies 2 and 3.\n\nGo to:\n\n## Study 2\n\nAI can be used for a wide variety of tasks, and these tasks vary in terms of\nthe importance of the consequences of success and failure. For example, AI can\nbe used to recommend movies and music, or to diagnose diseases and drive cars.\nThe latter tasks are clearly much more consequential than the former. Since\nperceived risk is partly a function of the importance of consequences [26,29],\ntasks such as recommending movies and music should seem relatively low risk.\nConservatives should therefore be less likely to perceive the use of AI as\nrisky for such tasks, and more likely to feel comfortable relying on AI. Study\n2 tests whether the consequentialness of a task does indeed moderate the\neffect of conservatism on trust in AI. Recall that our proposed mechanism for\nthe relationship between conservatism and trust is perceived risk. Since\nperceived risk is partially determined by consequentialness, this study\ntherefore tests our theorized process using moderation.\n\n### Method\n\nIn Study 2, we ask people to evaluate AI in the context of 27 tasks that AI\ncan already accomplish, with varying levels of consequentialness; these tasks,\nwhich range from evaluating jokes to diagnosing diseases, are listed in Table\n4 below. We first assessed the perceived consequentialness of these tasks by\nasking 221 MTurk users to rate each task according to how \u201cconsequential\u201d each\nof the tasks seems, using the following wording: \"Please use the sliders to\nindicate how much each of the tasks below seems to be consequential vs.\ninconsequential. Consequential means the task is important and doing well has\nserious consequences. Inconsequential means the task is not very important and\ndoing well doesn\u2019t have serious consequences.\u201d This definition closely\nparallels the classic conception of the \u201cdread\u201d and the \u201cimportance of\noutcomes\u201d components of perceived risk [26,29]. We then divided these tasks\ninto two groups: those that were rated (1) above and (2) below the scale\nmidpoint (the scale endpoints were labeled \u201cvery inconsequential\u201d and \u201cvery\nconsequential\u201d). Each task was therefore classified as consequential or\ninconsequential depending on whether it was rated as above or below the scale\nmidpoint, which allowed us to create a binary measure of consequentialness.\n\n### Table 4\n\nTasks used in Study 2.\n\nDiagnose Disease (39)| Rec. Disease Treatment (38)| Fly Plane (35)  \n---|---|---  \nHire & Fire Employees (28)| Drive Car (26)| Drive Subway (25)  \nDrive Truck (24)| Predict Recidivism (23)| Buy Stocks (22)  \nAnalyze Data (21)| Predict Stock Market (20)| Rec. Marketing Strategy (18)  \nPred. Employee Success (17)| Give Directions (15)| Rec. Romantic Partners (14)  \nPredict Weather (9)| Write News Story (8)| Schedule Events (7)  \nPredict Elections (6)| Predict Student Success (6)| Rec Gift (2)  \nWrite Song (-13)| Recommend Restaurant (-14)| Recommend Movies (-17)  \nRecommend Music (-18)| Play Piano (-18)| Evaluate Jokes (-24)  \n  \nOpen in a separate window\n\nNote: Numbers in parentheses are the tasks\u2019 consequentialness ratings.\n\nRec. = Recommend; Pred. = Predict.\n\nWe then asked 417 MTurk users (44% female, average age = 36.0) to consider\nusing AI to perform each of these 27 tasks; tasks were presented in random\norder, and participants were not provided with any information about the rated\nconsequentialness of each task. AI was described as follows, without any\nmention of potential risks:\n\n> \u201cArtificial intelligence (AI) refers to a set of computer programs that can\n> be used to accomplish a task. Thanks to rapid progress in computer science,\n> AI can now be used to accomplish a wide range of tasks that humans would\n> generally do, without being explicitly instructed how to do so by humans.\u201d\n\nFor each task, participants were asked to indicate how much they would trust\nAI to perform the task (\u201cPlease indicate how much you would trust AI to\nperform each of the tasks below\u201d). As in Study 1, participants also reported\ntheir degree of social and fiscal conservatism, plus age and gender (all\nmeasures except age and gender used 0\u2013100 scales). Due to constraints on study\nlength, we did not measure perceived risk of using AI for each task in Study\n2.\n\n### Results\n\nBecause each participant reported trust in AI for each of the 27 tasks, we\nfirst converted the data from wide to long format in order to create a\nvariable for \u201ctask\u201d and a variable for \u201ctrust in AI.\u201d There were therefore 27\nrows for each participant (one per task), plus a \u201cparticipant\u201d variable. Each\nrow was assigned a 1 if it represented a consequential task, and a 0 if it\nrepresented an inconsequential task. This binary measure of consequentialism\nused the scale midpoint as the dividing point. See web appendix for\nalternative analyses using different dividing points (e.g., +10 and -10\ninstead of above and below 0), which replicate the results reported here.\n\nWe then ran an OLS regression using the binary measure of task\nconsequentialness (coded as 0 for inconsequential and 1 for consequential),\nsocial conservatism (standardized), and their interaction to predict trust in\nAI (standardized), including age, gender, and a participant fixed effect as\ncovariates. This revealed effects for gender, \u03b2 = .11, SE = .02, p < .001,\nsuch that females trusted AI less than males, age, \u03b2 = -.02, SE = .009 p =\n.011, such that older participants trusted AI less than younger participants,\nbut no effect of participant, \u03b2 < .001, SE < .001, p = .972. More pertinent to\nour theorizing, we observed no main effect of consequentialness, \u03b2 = .03, SE =\n.02, p = .127, no main effect of conservatism, \u03b2 = .02, p = .339; and a\nsignificant interaction between conservatism and task consequentialness, \u03b2 =\n.06, SE = .02, p = .003.\n\nBreaking down the interaction, we found that conservatism predicted trust in\nAI for consequential tasks, standardized \u03b2 = -.04, p < .001, but not for\ninconsequential tasks, \u03b2 = -.01, p = .310. Power analysis revealed that we\nhave 82% power to detect the effect size of -.04, and that a sample size of\n392 would be required to detect this effect with 80% power. Fig 3 below\ndisplays this interaction visually.\n\nOpen in a separate window\n\nFig 3\n\nThe effect of conservatism on trust in AI is significant for consequential but\nnot inconsequential tasks.\n\n### Discussion\n\nStudy 2 shows that the effect of conservatism on trust in AI is moderated by\nthe consequentialness of the application for which AI is being considered. For\nconsequential tasks, increasing conservatism predicts decreasing trust in (or\nincreasing aversion to) AI; for inconsequential tasks, conservatism has no\neffect on trust.\n\nImportantly, the potential of AI to improve outcomes is likely to be the\ngreatest for more consequential tasks. For example, replacing human-driven\ncars with driverless cars has the potential to save millions of lives that are\nlost every year to accidents caused by human error, fatigue, alcohol, and so\non. At the same time, these same consequential tasks face the greatest\nresistance to AI among conservatives, posing a major challenge for realizing\nthe potential benefits that AI can offer.\n\nGo to:\n\n## Study 3\n\nThis study is focused on testing the full predicted pattern of moderated\nmediation: conservatism predicts trust in AI because it predicts the perceived\nriskiness of AI; because perceived risks are lower for inconsequential tasks,\nthe divide between liberals and conservatives is stronger for tasks that have\nserious consequences. Study 1 provided mediation evidence for this process,\nwhile Study 2 provided moderation evidence. Study 3 combines mediation and\nmoderation to test the entire proposed conceptual model simultaneously.\n\n### Method\n\nOur sample for Study 3 consisted of 550 MTurk users (46% female, mean age =\n34.7). We excluded 48 participants from the analysis because they failed a\nbasic attention check, for a final sample of 502. Previous studies forced\nparticipants to answer this question correctly as an instructional\nmanipulation check [33], but in this study participants could proceed without\nanswering correctly. The results of this study are unchanged if we include\nthese participants. Note that no attention check was used in Studies 1 and 2.\n\nParticipants were randomly assigned to evaluate AI either for purposes of\nmusic recommendation (a task with relatively low consequences) or for use in a\nself-driving car (a task with relatively high consequences). We selected these\ntwo tasks because driving a car was rated as a highly consequential task in\nStudy 2 (M = 25.6 on a scale from -50 to 50), whereas recommending music was\nrated as a highly inconsequential task (M = -18.2). Both conditions began by\nhaving participants read the same brief introduction to AI that was used in\nStudy 2 (without mentioning any potential risks associated with AI; see\nappendix for full stimuli). In the \u201cmusic\u201d condition, participants then\nreported how risky it seems and how much they would trust AI to recommend\nmusic. In the \u201ccar\u201d condition, participants reported how risky it seems and\nhow much they would trust AI to control a driverless car. All measures were on\n0\u2013100 scales.\n\n### Results\n\nWe tested for moderated mediation using Model 8 in the PROCESS macro for SPSS\n(Hayes 2013). We specified participants\u2019 social conservatism as the\nindependent variable, perceived riskiness of using AI as the mediator, trust\nin AI as the dependent variable, and condition (music vs. car) as the\nmoderator. We also included age, gender, income, and education as covariates\nin the model (see web appendix for the analysis without covariates, and also\nwith fiscal and general conservatism instead of social conservatism.\nConservatism, trust, and risk are standardized variables. These analyses are\nconsistent with those reported here). Breaking down the model, we first focus\non perceived risk, the proposed mediator. Perceived risk was predicted by\ntask, such that it was higher for AI driving cars (M = 63.72) than for AI\nrecommending music (M = 21.06, t(501) = 17.28, p < .001). Conservatism also\npredicted perceived risk (\u03b2 = .50, p < .001). Furthermore, the interaction\nbetween task and conservatism was also significant (\u03b2 = -.23, p = .009).\nConservatism predicted perceived risk of AI driving a car, such that higher\nconservatism was associated with higher perceived risk (\u03b2 = .27, p < .001),\nbut conservatism had no effect on the perceived risk of AI recommending music\n(\u03b2 = .04, p = .480).\n\nWe then focus on trust in AI, the dependent variable. Trust was predicted by\nperceived risk (\u03b2 = -.61, p < .001), but not by task (\u03b2 < .001, SE = .07, p =\n.991), conservatism (\u03b2 = .17, p = .132), nor by the interaction between\nconservatism and task (\u03b2 = -.12, p = .093). Because perceived risk is a\nfunction of conservatism (for highly consequential tasks), these results\nindicate that conservatism predicts perceived risk (for highly consequential\ntasks), which in turn predicts trust. Indeed, the indirect effect of\nconservatism on trust, mediated by risk, was significant in the car condition\n(B = .23, SE = .05, 95% CI = .099 to .247), but not in the music condition (B\n= .01, SE = .02, 95% CI = -.043 to .110). Finally, the index of moderated\nmediation was significant (B = -.14, 95% CI = -.243 to -.037).\n\nPower analysis revealed that we had >99% power to detect an effect size of .22\nand that a sample size of 36 would be required to have 80% power to detect\nthis effect size.\n\n### Discussion\n\nThe results thus far have demonstrated that conservatism predicts perceived\nrisk, which in turn predicts trust in AI. Consistent with prior research on\nthe determinants of perceived risk, we have found that perceived risk is\npartially determined by consequentialness, and shown that conservatism only\npredicts trust in AI when the task for which AI is being used has significant\nconsequences. However, a problem remains. The tasks for which AI has the\ngreatest potential to meaningfully improve outcomes for individuals and\nsociety as a whole tend to have the greatest potential consequences: reducing\ntraffic fatalities by eliminating human error from car accidents, quickly and\naccurately diagnosing complex diseases and providing appropriate\nrecommendations for treatment, and so on. Thus, the tasks for which AI can\nprovide the most potential benefit are also the tasks for which there exists\nthe greatest resistance (among conservatives) to using AI.\n\nSo what, if anything, can be done to reduce this resistance and thereby\nincrease the likelihood that AI can achieve its full potential? We explore\nthis question in Study 4.\n\nGo to:\n\n## Study 4\n\nRelative to liberals, conservatives are known to interpret ambiguous stimuli\nas more threatening [34,35] and even have larger amygdalae suggesting a\nbiological basis for more sensitive risk and threat perception [36]. Trying to\nexplicitly convince conservatives to not see AI as risky therefore seems\nunlikely to succeed. However, conservatives tend to rely on different moral\nfoundations than liberals, often basing their judgments on principles of\nloyalty, authority, and purity, while liberals tend to emphasize principles of\nfairness and care [37]. Researchers have discovered that framing arguments in\nterms of conservative moral foundations can make those arguments much more\neffective at persuading conservatives, relative to framing arguments in terms\nof liberal moral foundations [31]. For example, conservatives are more likely\nto support universal health insurance in the United States if the arguments in\nfavor are based on the moral principle of purity (i.e., reducing the number of\ninfected, diseased people) than if they are based on the moral principle of\nfairness (i.e., everyone deserves to be healthy) [30].\n\nIn this study, we test whether this approach could also be effective at\nincreasing conservative support for AI.\n\n### Method\n\nWe recruited 400 American participants from Prolific Academic (42% female,\nmean age = 26). Participants read one of two arguments in favor of using AI in\nhealthcare, based either on the idea that doing so will reduce the number of\ndiseased and infected people, or on the idea that doing so will ensure more\nequitable access to quality healthcare (see Web Appendix for full arguments).\nWe then measured support for using AI in healthcare using three items (i.e.,\n\u201cI am in favor of using AI more often in healthcare,\u201d \u03b1 = .85), plus perceived\nrisk of using AI in healthcare and trust AI used in healthcare, and finally\nthe three dimensions of conservatism (social, fiscal, and general).\n\n### Results\n\nBecause the manipulation required reading a 200-word article, we first\nexcluded 32 participants who completed the entire survey in less than 1\nminute. We then tested whether any of the three dimensions of conservatism\ninteracted with argument condition in predicting support for healthcare AI. We\nonly found a significant general conservatism \u00d7 condition interaction (\u03b2 =\n.20, p = .043), a main effect of conservatism on support (\u03b2 = -.27, p < .001),\nand a main effect of condition, such that the purity condition was less\npersuasive (\u03b2 = -.32, SE = .11, p = .001). Neither social nor fiscal\nconservatism interacted with argument condition.\n\nResults in the fairness condition were consistent with those from Studies 1\u20133:\nconservatism predicted lower support for healthcare AI (standardized \u03b2 = .27,\np < .001). In the purity condition, however, conservatism did not predict\nsupport (standardized \u03b2 = .07, p = .361; see Fig 4). Power analysis revealed\nthat we have 99% power to detect an effect size of .27 and would need a sample\nsize of 29 to detect this effect size with 80% power. Critically, these\ndiverging patterns did not result from conservatives (defined as being below\nthe scale midpoint) being more persuaded by purity arguments (M = 6.76) than\nby fairness arguments (M = 6.51, t(52) = .47, p = .641). Rather, these\npatterns reflect reduced support among liberals (above the scale midpoint)\nshown purity arguments (M = 7.07) versus fairness arguments (M = 7.90, t(248)\n= 3.67, p < .001).\n\nOpen in a separate window\n\nFig 4\n\nFairness-based arguments persuade liberals more than purity-based arguments.\n\nSocial and fiscal conservatism did not interact with condition to predict\nsupport, and none of the three dimensions of conservatism interacted with\ncondition to predict perceived riskiness of or trust in healthcare AI.\n\nWe were therefore unable to shift conservatives\u2019 attitudes towards a\nconsequential form of AI using a moral reframing approach. Our final study\nattempts once more to achieve this goal using a revised moral reframing\nmanipulation.\n\nGo to:\n\n## Study 5\n\nThe previous study found that emphasizing different moral foundations shifted\nliberals\u2019 support for AI but did not change conservatives\u2019 attitudes. In Study\n5, we therefore focus specifically on attempting to shift conservatives\u2019\nattitudes. Specifically, we attempt to increase the persuasiveness of the\nmoral reframing approach by appealing to three typically conservative moral\nfoundations as opposed to just one as we did in Study 4.\n\n### Method\n\nProlific Academic pre-screens participants on a variety of factors including\npolitical ideology, asking them to self-identify as \u201cliberal,\u201d \u201cconservative,\u201d\nor \u201cmoderate.\u201d We therefore recruited 200 self-identified conservatives (48%\nfemale, mean age = 39) and asked them to read one of two ~150-word texts\narguing in favor of using AI in healthcare. The conservative text was based on\nprinciples of purity (AI will reduce the number of diseased Americans),\ntradition (technological innovation is the American way), and loyalty (it\u2019s a\npatriotic duty to ensure all Americans are healthy), while the liberal text\nwas based on principles of fairness and care (AI will ensure more equitable\naccess to high quality healthcare; see Web Appendix for full text).\nParticipants then completed the same measure of support for healthcare in AI\nused in Study 4, plus the risk and trust measures used throughout this paper.\n\n### Results\n\nWe first excluded 12 participants who completed the entire study in less than\n40 seconds (the cutoff used in this study was shorter because the manipulation\nword count was approximately 150 words in this study compared to 200 words in\nthe previous study). Among the remaining participants\u2014all self-identified\nconservatives\u2014condition had no effect on support for healthcare AI\n(M_conservative = 6.23, M_liberal = 6.26, t(186) = .11, p = .914, Cohen\u2019s d =\n.01). Condition also had no effect on the perceived risk of healthcare AI nor\non trust in its use (p\u2019s > .204).\n\nIt therefore appears that framing a consequential application of AI in terms\nof conservative moral foundations may not be effective at reducing\nconservative aversion to such uses of AI.\n\nGo to:\n\n## General discussion\n\nUsing artificial intelligence to automate complex tasks has the potential to\nradically improve outcomes in a fast-growing number of domains, from reducing\ntraffic accidents caused by human error to diagnosing diseases with greater\nspeed and accuracy. The extent to which these improved outcomes are realized,\nhowever, depends to a large degree on people\u2019s willingness to trust and rely\non AI rather than on themselves or other human beings for performing the task\nat hand. Understanding when and why people are willing to trust AI is\ntherefore an important task, especially if that understanding can shed light\non how to increase trust in AI when high trust is justified by superior\nperformance. This paper begins to make progress in that direction by\nhighlighting the role of a prominent psychological variable\u2013conservatism\u2014in\nshaping trust in AI. In addition to uncovering the basic relationship between\nconservatism and trust, we also provide evidence of the psychological process\nunderlying this relationship and show that it is resistant to a moral\nreframing intervention intended to reduce the effect.\n\nOur analyses cast doubt on two alternative explanations of our results. First,\nconservatism is often correlated with income, such that our results could be\ndriven by poorer individuals whose jobs are more immediately at risk of being\nautomated by AI, and who happen to also be more conservative. Second,\nconservatism is often correlated with level of education, such that our\nresults could be driven by less educated individuals who are less\nknowledgeable about technology and/or more likely to work in jobs that are at\nimmediate risk of automation. These explanations become less likely in light\nof the fact that the effect of social conservatism on the perceived riskiness\nof (and trust in) AI remains highly significant even after controlling for\nparticipants\u2019 income and level of education, and the fact that these other\ndemographic variables do not themselves have any effect on perceived risk and\ntrust.\n\nOur results contribute to a growing literature on the changing relationship\nbetween humans and technology. As technology becomes increasingly capable of\nautomating consequential tasks, research has found that humans are reluctant\nto trust and rely on technology for tasks traditionally performed by humans\n[8,10,12]. Our research has uncovered conservatism as an important driver of\nthis aversion to AI.\n\nPolitical or ideological sources of AI aversion are particularly important\ngiven the inextricable links between politics and the impacts that AI has on\nsociety. The relationship between conservatism and AI aversion that we have\nfocused on has political implications. For example, it seems likely that both\nconservative individuals as well as conservative state governments will be\nslower to adopt AI technologies compared to their liberal counterparts,\npotentially depriving conservatives of the benefits that AI can provide when\nit outperforms humans. Similarly, ideologically divided legislative bodies may\nbe unable to reach consensus on regulating AI in nationally important domains.\n\nFuture research can continue exploring factors that contribute to AI aversion.\nFor example, prior research has found that people who feel low in power also\nfeel a lower degree of control over anthropomorphized objects [38], which\nsuggests that feelings of power and control are likely to be relevant factors\nin shaping trust in AI. Indeed, giving people the ability to slightly modify\nthe output of an algorithm has been found to increase the use of that\nalgorithm, further pointing to the importance of perceived control over\ntechnology [15]. In light of the previously discussed relationships between\nconservatism, education, and income, which increase the likelihood that\nconservative individuals work in jobs that are at high risk of automation,\nthese findings suggest that perceived control over automation technologies\nsuch as AI may also contribute to our observed relationship between\nconservatism and distrust in AI.\n\nAnother factor that may be worth exploring in more detail is gender. Females\nperceived more risk in AI and trusted AI less than males in all of our\nstudies. Females are known to perceive more risk in general [39] and to take\nfewer risks than men [40]. This may be rooted in evolutionary pressures:\nwhereas males were traditionally hunters who needed to be risk-seeking in\norder to provide food for their families, females were traditionally child\ncare providers who needed to avoid harm [41,42]. Consistent with an account of\ngender differences rooted in risk perception, we also found that gender\npredicted perceived risks of and trust in AI for consequential tasks but not\nfor consequential tasks. Since consequential tasks are less risky, this\nsuggests that females may trust AI less than men because they perceive AI as\nmore risky, but that this gender difference is eliminated for inherently low-\nrisk tasks. However, none of the interventions that we tested in Study 4\ninteracted with gender. This highlights the need for future research to test\ndifferent interventions that are specifically rooted in a deeper understanding\nof gender differences.\n\nOur findings in Studies 4 and 5 suggest that moral reframing interventions may\nnot be universally successful across topic domains. Given that this approach\nhas been successful in increasing conservative support for typically liberal\ncauses such as the Affordable Care Act and environmentally sustainable\nbehaviors [35,43], it is perhaps surprising that it failed to shift\nconservative attitudes in this context. By opening our file drawer and\nreporting these null results, we hope to encourage further research on when\nand why moral reframing is most effective at mitigating ideological divides in\nattitudes. For example, it is possible that the kind of reframing\ninterventions we tested would be more effective if they were delivered by\ntrusted conservative sources rather than by university researchers in an\nonline survey.\n\nRelatedly, an important limitation of this research concerns the specific\ncontext in which AI is developed and marketed. Specifically, the primary\ndevelopers and marketers of AI are relatively liberal corporations such as\nGoogle and Facebook, which are often criticized for being hostile to\nconservative viewpoints. Our results cannot definitively rule out the notion\nthat conservatives are averse to AI because they are averse to the companies\nthat produce AI, rather than being averse to AI in particular for reasons\nunrelated to its developers.\n\nWe also acknowledge three further limitations of our research. First, we did\nnot provide a great level of detail to participants regarding the specific\ntasks that AI performs and how those tasks are shared with humans. For\nexample, AI in medical contexts may be used in collaboration with a human\ndoctor, and attitudes may be different in this context compared to if AI is\nused alone without human collaboration. Second, while we controlled for a\nrange of demographic factors in our analyses, we did not measure some that may\nbe relevant, including participants\u2019 occupation or specific field of training.\nFinally, the amount of variance that our regression models explain is\nrelatively modest, ranging from about 5\u201310%. While these numbers are typical\nfor psychological research, we must acknowledge that comfort with and\nperceived risks of AI are of course complex variables with many different\ninputs and influences. The effects of conservatism that we have uncovered are\ntherefore just one relatively small influence on our dependent variables of\ninterest.\n\nAs AI research continues to progress, the potential benefits that this\ntechnology can offer continue to grow. Increasing trust in AI can therefore\nincrease the probability that these benefits are realized. At the same time,\nAI also brings novel risks to people and to society, as recognized by many of\nthe participants in our studies. These risks range from mass unemployment and\nalgorithm-entrenched inequality and discrimination, to the loss of skills as\ntasks are offloaded to AI, and more [44,45]. Any efforts to increase trust in\nAI should therefore proceed mindfully\u2014not merely to increase trust in all\nforms of AI at all times, but to ensure that such efforts are focused on\napplications of AI that are clearly beneficial to society, and whose risks are\nminimized. This requires the companies developing AI and the governments\nregulating them to develop policies specifically addressing these and other\nrisks [46,47]. Understanding how people perceive AI can help inform the\ndevelopment of those policies and the development of AI and AI-based products\nmore broadly. The research presented in this paper contributes to such an\nunderstanding.\n\nGo to:\n\n## Supporting information\n\n#### S1 File\n\n(DOCX)\n\nClick here for additional data file.^(69K, docx)\n\nGo to:\n\n## Funding Statement\n\nThe author(s) received no specific funding for this work.\n\nGo to:\n\n## Data Availability\n\nThe authors have uploaded their data to OSF at the following:\nhttps://osf.io/5wehr/?view_only=7f2ef4c8fcea4c0ea05a706987df9e96.\n\nGo to:\n\n## References\n\n1\\. Simonite T. IBM Watson\u2019s Plan to End Human Doctors\u2019 Monopoly on Medical\nKnow-How. In: MIT Technology Review [Internet]. 2014 [cited 8 Apr 2015].\nhttp://www.technologyreview.com/news/529021/ibm-aims-to-make-medical-\nexpertise-a-commodity/.\n\n2\\. Esteva A, Kuprel B, Novoa RA, Ko J, Swetter SM, Blau HM, et al.\nDermatologist-level classification of skin cancer with deep neural networks.\nNature. 2017;542: 115\u2013118. doi: 10.1038/nature21056 [PubMed] [CrossRef]\n[Google Scholar]\n\n3\\. Silver D, Huang A, Maddison CJ, Guez A, Sifre L, van den Driessche G, et\nal. Mastering the game of Go with deep neural networks and tree search.\nNature. 2016;529: 484\u20139. doi: 10.1038/nature16961 [PubMed] [CrossRef] [Google\nScholar]\n\n4\\. Blanco M, Atwood J, Russell S, Trimble T, McClafferty J, Perez M.\nAutomated Vehicle Crash Rate Comparison Using Naturalistic Data. Virginia Tech\nTransportation Institute. 2016. doi: 10.13140/RG.2.1.2336.1048 [CrossRef]\n[Google Scholar]\n\n5\\. Jost JT, Napier JL, Thorisdottir H, Gosling SD, Palfai TP, Ostafin B. Are\nNeeds to Manage Uncertainty and Threat Associated With Political Conservatism\nor Ideological Extremity? Personality and Social Psychology Bulletin. 2007;33:\n989\u20131007. doi: 10.1177/0146167207301028 [PubMed] [CrossRef] [Google Scholar]\n\n6\\. Jost JT, Glaser J, Kruglanski AW, Sulloway FJ. Political conservatism as\nmotivated social cognition. Psychological Bulletin. 2003;129: 339\u2013375. doi:\n10.1037/0033-2909.129.3.339 [PubMed] [CrossRef] [Google Scholar]\n\n7\\. Russell SJ, Norvig P. Artificial Intelligence: A Modern Approach, 3rd\nedition. London: Pearson; 2009. [Google Scholar]\n\n8\\. Castelo N, Bos MW, Lehmann DR. Task-Dependent Algorithm Aversion. Journal\nof Marketing Research. 2019;56: 809\u2013825. doi: 10.1177/0022243719851788\n[CrossRef] [Google Scholar]\n\n9\\. Dawes RM, Faust D, Meehl PE. Clinical versus actuarial judgment. Science.\n1989;243: 1668\u20131674. doi: 10.1126/science.2648573 [PubMed] [CrossRef] [Google\nScholar]\n\n10\\. Dietvorst BJ, Simmons JP, Massey C. Algorithm Aversion: People\nErroneously Avoid Algorithms After Seeing Them Err. Journal of Experimental\nPsychology General. 2014;143: 1\u201313. doi: 10.1037/a0032259 [PubMed] [CrossRef]\n[Google Scholar]\n\n11\\. Diab DL, Pui S-Y, Yankelevich M, Highhouse S. Lay Perceptions of\nSelection Decision Aids in US and Non-US Samples. International Journal of\nSelection and Assessment. 2011;19: 209\u2013216. doi:\n10.1111/j.1468-2389.2011.00548.x [CrossRef] [Google Scholar]\n\n12\\. Longoni C, Bonezzi A, Morewedge CK. Resistance to Medical Artificial\nIntelligence. Journal of Consumer Research. 2019;46: 629\u2013650. doi:\n10.1093/jcr/ucz013 [CrossRef] [Google Scholar]\n\n13\\. \u00d6nkal D, Goodwin P, Thomson M, G\u00f6n\u00fcl S, Pollock A. The relative influence\nof advice from human experts and statistical methods on forecast adjustments.\nJournal of Behavioral Decision Making. 2009;22: 390\u2013409. doi: 10.1002/bdm.637\n[CrossRef] [Google Scholar]\n\n14\\. Grove WM, Zald DH, Lebow BS, Snitz BE, Nelson C. Clinical versus\nmechanical prediction: A meta-analysis. Psychological Assessment. 2000;12:\n19\u201330. doi: 10.1037/1040-3590.12.1.19 [PubMed] [CrossRef] [Google Scholar]\n\n15\\. Dietvorst BJ, Simmons JP, Massey C. Overcoming Algorithm Aversion: People\nWill Use Imperfect Algorithms If They Can (Even Slightly) Modify Them.\nManagement Science. 2016; mnsc.2016.2643. doi: 10.1287/mnsc.2015.2285 [PMC\nfree article] [PubMed] [CrossRef] [Google Scholar]\n\n16\\. Promberger M, Baron J. Do patients trust computers? Journal of Behavioral\nDecision Making. 2006;19: 455\u2013468. doi: 10.1002/bdm.542 [CrossRef] [Google\nScholar]\n\n17\\. Logg JM, Minson JA, Moore DA. Algorithm appreciation: People prefer\nalgorithmic to human judgment. Organizational Behavior and Human Decision\nProcesses. 2019;151: 90\u2013103. doi: 10.1016/j.obhdp.2018.12.005 [CrossRef]\n[Google Scholar]\n\n18\\. Yeomans M, Shah A, Mullainathan S, Kleinberg J. Making sense of\nrecommendations. Journal of Behavioral Decision Making. 2019;32: 403\u2013414. doi:\n10.1002/bdm.2118 [CrossRef] [Google Scholar]\n\n19\\. Wilson GD. The psychology of conservatism. Oxford, England: Academic\nPress; 1973. pp. xv, 277. [Google Scholar]\n\n20\\. Mill JS. On liberty and other essays. Oxford world\u2019s classics. 1998.\n[Google Scholar]\n\n21\\. Hibbing JR, Smith KB, Alford JR. Differences in negativity bias underlie\nvariations in political ideology. Behavioral and Brain Sciences. 2014;37:\n297\u2013307. doi: 10.1017/S0140525X13001192 [PubMed] [CrossRef] [Google Scholar]\n\n22\\. Lilienfeld SO, Latzman RD. Threat bias, not negativity bias, underpins\ndifferences in political ideology. Behavioral and Brain Sciences. 2014;37:\n318\u20139. doi: 10.1017/S0140525X1300263X [PubMed] [CrossRef] [Google Scholar]\n\n23\\. Dodd MD, Balzer A, Jacobs CM, Gruszczynski MW, Smith KB, Hibbing JR. The\npolitical left rolls with the good and the political right confronts the bad:\nconnecting physiology and cognition to preferences. Philosophical Transactions\nof the Royal Society B: Biological Sciences. 2012;367: 640\u2013649. doi:\n10.1098/rstb.2011.0268 [PMC free article] [PubMed] [CrossRef] [Google Scholar]\n\n24\\. Vigil JM. Political leanings vary with facial expression processing and\npsychosocial functioning. Group Processes & Intergroup Relations. 2010;13:\n547\u2013558. doi: 10.1177/1368430209356930 [CrossRef] [Google Scholar]\n\n25\\. Fessler DMT, Pisor AC, Holbrook C. Political Orientation Predicts\nCredulity Regarding Putative Hazards. Psychological Science. 2017;28: 651\u2013660.\ndoi: 10.1177/0956797617692108 [PubMed] [CrossRef] [Google Scholar]\n\n26\\. Bettman JR. Perceived Risk and Its Components: A Model and Empirical\nTest. Journal of Marketing. 1973;10: 184\u2013190. doi: 10.2307/3149824 [CrossRef]\n[Google Scholar]\n\n27\\. Jacoby J, Kaplan LB. The Components of Perceived Risk. ACR Special\nVolumes. 1972; 382\u2013393. [Google Scholar]\n\n28\\. Peters E, Slovic P. The Role of Affect and Worldviews as Orienting\nDispositions in the Perception and Acceptance of Nuclear Power1. Journal of\nApplied Social Psychology. 1996;26: 1427\u20131453. doi:\n10.1111/j.1559-1816.1996.tb00079.x [CrossRef] [Google Scholar]\n\n29\\. Slovic P, Weber EU. Perception of risk posed by extreme events. Risk\nManagement Strategies in an Uncertain World. 2002; 1\u201321. doi:\n10.1017/CBO9781107415324.004 [CrossRef] [Google Scholar]\n\n30\\. Feinberg M, Willer R. From Gulf to Bridge: When Do Moral Arguments\nFacilitate Political Influence? Pers Soc Psychol Bull. 2015;41: 1665\u20131681.\ndoi: 10.1177/0146167215607842 [PubMed] [CrossRef] [Google Scholar]\n\n31\\. Feinberg M, Willer R. Moral reframing: A technique for effective and\npersuasive communication across political divides. Social and Personality\nPsychology Compass. 2019;13: e12501. doi: 10.1111/spc3.12501 [CrossRef]\n[Google Scholar]\n\n32\\. Mahmoud AB, Hack-Polay D, Fuxman L, Nicoletti M. The Janus-faced effects\nof COVID-19 perceptions on family healthy eating behavior: Parent\u2019s negative\nexperience as a mediator and gender as a moderator. Scandinavian Journal of\nPsychology. 2021;62: 586\u2013595. doi: 10.1111/sjop.12742 [PMC free article]\n[PubMed] [CrossRef] [Google Scholar]\n\n33\\. Oppenheimer DM, Meyvis T, Davidenko N. Instructional manipulation checks:\nDetecting satisficing to increase statistical power. Journal of Experimental\nSocial Psychology. 2009;45: 867\u2013872. doi: 10.1016/j.jesp.2009.03.009\n[CrossRef] [Google Scholar]\n\n34\\. Jost JT, Federico CM, Napier JL. Political Ideology: Its Structure,\nFunctions, and Elective Affinities. Annual Review of Psychology. 2009;60:\n307\u2013337. doi: 10.1146/annurev.psych.60.110707.163600 [PubMed] [CrossRef]\n[Google Scholar]\n\n35\\. Kidwell B, Farmer A, Hardesty DM. Getting Liberals and Conservatives to\nGo Green: Political Ideology and Congruent Appeals. J Consum Res. 2013;40:\n350\u2013367. doi: 10.1086/670610 [CrossRef] [Google Scholar]\n\n36\\. Kanai R, Feilden T, Firth C, Rees G. Political Orientations Are\nCorrelated with Brain Structure in Young Adults. Current Biology. 2011;21:\n677\u2013680. doi: 10.1016/j.cub.2011.03.017 [PMC free article] [PubMed] [CrossRef]\n[Google Scholar]\n\n37\\. Graham J, Haidt J, Koleva S, Motyl M, Iyer R, Wojcik SP, et al. Chapter\nTwo\u2014Moral Foundations Theory: The Pragmatic Validity of Moral Pluralism. In:\nDevine P, Plant A, editors. Advances in Experimental Social Psychology.\nAcademic Press; 2013. pp. 55\u2013130. [Google Scholar]\n\n38\\. Kim S, McGill AL. Gaming with Mr. Slot or Gaming the Slot Machine? Power,\nAnthropomorphism, and Risk Perception. Journal of Consumer Research. 2011;38:\n94\u2013107. doi: 10.1086/658148 [CrossRef] [Google Scholar]\n\n39\\. Gustafson PE. Gender differences in risk perception: theoretical and\nmethodological perspectives. Risk analysis: an official publication of the\nSociety for Risk Analysis. 1998;18: 805\u2013811. doi:\n10.1023/b:rian.0000005926.03250.c0 [PubMed] [CrossRef] [Google Scholar]\n\n40\\. Byrnes JP, Miller DC, Schafer WD. Gender differences in risk taking: A\nmeta-analysis. Psychological Bulletin. 1999;125: 367\u2013383. doi:\n10.1037/0033-2909.125.3.367 [CrossRef] [Google Scholar]\n\n41\\. Dekel E, Scotchmer S. On the Evolution of Attitudes towards Risk in\nWinner-Take-All Games. Journal of Economic Theory. 1999;87: 125\u2013143. doi:\n10.1006/jeth.1999.2537 [CrossRef] [Google Scholar]\n\n42\\. Griskevicius V, Kenrick DT. Fundamental motives: How evolutionary needs\ninfluence consumer behavior. Journal of Consumer Psychology. 2013;23: 372\u2013386.\ndoi: 10.1016/j.jcps.2013.03.003 [CrossRef] [Google Scholar]\n\n43\\. Feinberg M, Willer R. The Moral Roots of Environmental Attitudes. Psychol\nSci. 2013;24: 56\u201362. doi: 10.1177/0956797612449177 [PubMed] [CrossRef] [Google\nScholar]\n\n44\\. O\u2019Neil C. Weapons of Math Destruction. Discover. 2016;37: 50\u201355. [Google\nScholar]\n\n45\\. Smith A, Anderson J. Digital Life in 2025: AI, Robotics and the Future of\nJobs. Pew Research Center. 2014; 67. doi: 10.1007/s13398-014-0173-7.2\n[CrossRef] [Google Scholar]\n\n46\\. Crawford K, Calo R. There is a blind spot in AI research. Nature. 2016.\npp. 311\u2013313. doi: 10.1038/538311a [PubMed] [CrossRef] [Google Scholar]\n\n47\\. Metz C. Inside OpenAI, Elon Musk\u2019s wild plan to set Artificial\nintelligence free. Wired. 2016; 1\u20137. [Google Scholar]\n\n  * Abstract\n  * Introduction\n  * Study 1\n  * Study 2\n  * Study 3\n  * Study 4\n  * Study 5\n  * General discussion\n  * Supporting information\n  * Funding Statement\n  * Data Availability\n  * References\n\nArticles from PLOS ONE are provided here courtesy of PLOS\n\n1\\. Simonite T. IBM Watson\u2019s Plan to End Human Doctors\u2019 Monopoly on Medical\nKnow-How. In: MIT Technology Review [Internet]. 2014 [cited 8 Apr 2015].\nhttp://www.technologyreview.com/news/529021/ibm-aims-to-make-medical-\nexpertise-a-commodity/. [Ref list]\n\n4\\. Blanco M, Atwood J, Russell S, Trimble T, McClafferty J, Perez M.\nAutomated Vehicle Crash Rate Comparison Using Naturalistic Data. Virginia Tech\nTransportation Institute. 2016. doi: 10.13140/RG.2.1.2336.1048 [CrossRef]\n[Google Scholar] [Ref list]\n\n5\\. Jost JT, Napier JL, Thorisdottir H, Gosling SD, Palfai TP, Ostafin B. Are\nNeeds to Manage Uncertainty and Threat Associated With Political Conservatism\nor Ideological Extremity? Personality and Social Psychology Bulletin. 2007;33:\n989\u20131007. doi: 10.1177/0146167207301028 [PubMed] [CrossRef] [Google Scholar]\n[Ref list]\n\n6\\. Jost JT, Glaser J, Kruglanski AW, Sulloway FJ. Political conservatism as\nmotivated social cognition. Psychological Bulletin. 2003;129: 339\u2013375. doi:\n10.1037/0033-2909.129.3.339 [PubMed] [CrossRef] [Google Scholar] [Ref list]\n\n7\\. Russell SJ, Norvig P. Artificial Intelligence: A Modern Approach, 3rd\nedition. London: Pearson; 2009. [Google Scholar] [Ref list]\n\n8\\. Castelo N, Bos MW, Lehmann DR. Task-Dependent Algorithm Aversion. Journal\nof Marketing Research. 2019;56: 809\u2013825. doi: 10.1177/0022243719851788\n[CrossRef] [Google Scholar] [Ref list]\n\n9\\. Dawes RM, Faust D, Meehl PE. Clinical versus actuarial judgment. Science.\n1989;243: 1668\u20131674. doi: 10.1126/science.2648573 [PubMed] [CrossRef] [Google\nScholar] [Ref list]\n\n10\\. Dietvorst BJ, Simmons JP, Massey C. Algorithm Aversion: People\nErroneously Avoid Algorithms After Seeing Them Err. Journal of Experimental\nPsychology General. 2014;143: 1\u201313. doi: 10.1037/a0032259 [PubMed] [CrossRef]\n[Google Scholar] [Ref list]\n\n11\\. Diab DL, Pui S-Y, Yankelevich M, Highhouse S. Lay Perceptions of\nSelection Decision Aids in US and Non-US Samples. International Journal of\nSelection and Assessment. 2011;19: 209\u2013216. doi:\n10.1111/j.1468-2389.2011.00548.x [CrossRef] [Google Scholar] [Ref list]\n\n12\\. Longoni C, Bonezzi A, Morewedge CK. Resistance to Medical Artificial\nIntelligence. Journal of Consumer Research. 2019;46: 629\u2013650. doi:\n10.1093/jcr/ucz013 [CrossRef] [Google Scholar] [Ref list]\n\n13\\. \u00d6nkal D, Goodwin P, Thomson M, G\u00f6n\u00fcl S, Pollock A. The relative influence\nof advice from human experts and statistical methods on forecast adjustments.\nJournal of Behavioral Decision Making. 2009;22: 390\u2013409. doi: 10.1002/bdm.637\n[CrossRef] [Google Scholar] [Ref list]\n\n14\\. Grove WM, Zald DH, Lebow BS, Snitz BE, Nelson C. Clinical versus\nmechanical prediction: A meta-analysis. Psychological Assessment. 2000;12:\n19\u201330. doi: 10.1037/1040-3590.12.1.19 [PubMed] [CrossRef] [Google Scholar]\n[Ref list]\n\n15\\. Dietvorst BJ, Simmons JP, Massey C. Overcoming Algorithm Aversion: People\nWill Use Imperfect Algorithms If They Can (Even Slightly) Modify Them.\nManagement Science. 2016; mnsc.2016.2643. doi: 10.1287/mnsc.2015.2285 [PMC\nfree article] [PubMed] [CrossRef] [Google Scholar] [Ref list]\n\n16\\. Promberger M, Baron J. Do patients trust computers? Journal of Behavioral\nDecision Making. 2006;19: 455\u2013468. doi: 10.1002/bdm.542 [CrossRef] [Google\nScholar] [Ref list]\n\n17\\. Logg JM, Minson JA, Moore DA. Algorithm appreciation: People prefer\nalgorithmic to human judgment. Organizational Behavior and Human Decision\nProcesses. 2019;151: 90\u2013103. doi: 10.1016/j.obhdp.2018.12.005 [CrossRef]\n[Google Scholar] [Ref list]\n\n18\\. Yeomans M, Shah A, Mullainathan S, Kleinberg J. Making sense of\nrecommendations. Journal of Behavioral Decision Making. 2019;32: 403\u2013414. doi:\n10.1002/bdm.2118 [CrossRef] [Google Scholar] [Ref list]\n\n19\\. Wilson GD. The psychology of conservatism. Oxford, England: Academic\nPress; 1973. pp. xv, 277. [Google Scholar] [Ref list]\n\n20\\. Mill JS. On liberty and other essays. Oxford world\u2019s classics. 1998.\n[Google Scholar] [Ref list]\n\n21\\. Hibbing JR, Smith KB, Alford JR. Differences in negativity bias underlie\nvariations in political ideology. Behavioral and Brain Sciences. 2014;37:\n297\u2013307. doi: 10.1017/S0140525X13001192 [PubMed] [CrossRef] [Google Scholar]\n[Ref list]\n\n22\\. Lilienfeld SO, Latzman RD. Threat bias, not negativity bias, underpins\ndifferences in political ideology. Behavioral and Brain Sciences. 2014;37:\n318\u20139. doi: 10.1017/S0140525X1300263X [PubMed] [CrossRef] [Google Scholar]\n[Ref list]\n\n23\\. Dodd MD, Balzer A, Jacobs CM, Gruszczynski MW, Smith KB, Hibbing JR. The\npolitical left rolls with the good and the political right confronts the bad:\nconnecting physiology and cognition to preferences. Philosophical Transactions\nof the Royal Society B: Biological Sciences. 2012;367: 640\u2013649. doi:\n10.1098/rstb.2011.0268 [PMC free article] [PubMed] [CrossRef] [Google Scholar]\n[Ref list]\n\n24\\. Vigil JM. Political leanings vary with facial expression processing and\npsychosocial functioning. Group Processes & Intergroup Relations. 2010;13:\n547\u2013558. doi: 10.1177/1368430209356930 [CrossRef] [Google Scholar] [Ref list]\n\n25\\. Fessler DMT, Pisor AC, Holbrook C. Political Orientation Predicts\nCredulity Regarding Putative Hazards. Psychological Science. 2017;28: 651\u2013660.\ndoi: 10.1177/0956797617692108 [PubMed] [CrossRef] [Google Scholar] [Ref list]\n\n26\\. Bettman JR. Perceived Risk and Its Components: A Model and Empirical\nTest. Journal of Marketing. 1973;10: 184\u2013190. doi: 10.2307/3149824 [CrossRef]\n[Google Scholar] [Ref list]\n\n27\\. Jacoby J, Kaplan LB. The Components of Perceived Risk. ACR Special\nVolumes. 1972; 382\u2013393. [Google Scholar] [Ref list]\n\n28\\. Peters E, Slovic P. The Role of Affect and Worldviews as Orienting\nDispositions in the Perception and Acceptance of Nuclear Power1. Journal of\nApplied Social Psychology. 1996;26: 1427\u20131453. doi:\n10.1111/j.1559-1816.1996.tb00079.x [CrossRef] [Google Scholar] [Ref list]\n\n29\\. Slovic P, Weber EU. Perception of risk posed by extreme events. Risk\nManagement Strategies in an Uncertain World. 2002; 1\u201321. doi:\n10.1017/CBO9781107415324.004 [CrossRef] [Google Scholar] [Ref list]\n\n30\\. Feinberg M, Willer R. From Gulf to Bridge: When Do Moral Arguments\nFacilitate Political Influence? Pers Soc Psychol Bull. 2015;41: 1665\u20131681.\ndoi: 10.1177/0146167215607842 [PubMed] [CrossRef] [Google Scholar] [Ref list]\n\n31\\. Feinberg M, Willer R. Moral reframing: A technique for effective and\npersuasive communication across political divides. Social and Personality\nPsychology Compass. 2019;13: e12501. doi: 10.1111/spc3.12501 [CrossRef]\n[Google Scholar] [Ref list]\n\n32\\. Mahmoud AB, Hack-Polay D, Fuxman L, Nicoletti M. The Janus-faced effects\nof COVID-19 perceptions on family healthy eating behavior: Parent\u2019s negative\nexperience as a mediator and gender as a moderator. Scandinavian Journal of\nPsychology. 2021;62: 586\u2013595. doi: 10.1111/sjop.12742 [PMC free article]\n[PubMed] [CrossRef] [Google Scholar] [Ref list]\n\n33\\. Oppenheimer DM, Meyvis T, Davidenko N. Instructional manipulation checks:\nDetecting satisficing to increase statistical power. Journal of Experimental\nSocial Psychology. 2009;45: 867\u2013872. doi: 10.1016/j.jesp.2009.03.009\n[CrossRef] [Google Scholar] [Ref list]\n\n34\\. Jost JT, Federico CM, Napier JL. Political Ideology: Its Structure,\nFunctions, and Elective Affinities. Annual Review of Psychology. 2009;60:\n307\u2013337. doi: 10.1146/annurev.psych.60.110707.163600 [PubMed] [CrossRef]\n[Google Scholar] [Ref list]\n\n35\\. Kidwell B, Farmer A, Hardesty DM. Getting Liberals and Conservatives to\nGo Green: Political Ideology and Congruent Appeals. J Consum Res. 2013;40:\n350\u2013367. doi: 10.1086/670610 [CrossRef] [Google Scholar] [Ref list]\n\n36\\. Kanai R, Feilden T, Firth C, Rees G. Political Orientations Are\nCorrelated with Brain Structure in Young Adults. Current Biology. 2011;21:\n677\u2013680. doi: 10.1016/j.cub.2011.03.017 [PMC free article] [PubMed] [CrossRef]\n[Google Scholar] [Ref list]\n\n37\\. Graham J, Haidt J, Koleva S, Motyl M, Iyer R, Wojcik SP, et al. Chapter\nTwo\u2014Moral Foundations Theory: The Pragmatic Validity of Moral Pluralism. In:\nDevine P, Plant A, editors. Advances in Experimental Social Psychology.\nAcademic Press; 2013. pp. 55\u2013130. [Google Scholar] [Ref list]\n\n38\\. Kim S, McGill AL. Gaming with Mr. Slot or Gaming the Slot Machine? Power,\nAnthropomorphism, and Risk Perception. Journal of Consumer Research. 2011;38:\n94\u2013107. doi: 10.1086/658148 [CrossRef] [Google Scholar] [Ref list]\n\n39\\. Gustafson PE. Gender differences in risk perception: theoretical and\nmethodological perspectives. Risk analysis: an official publication of the\nSociety for Risk Analysis. 1998;18: 805\u2013811. doi:\n10.1023/b:rian.0000005926.03250.c0 [PubMed] [CrossRef] [Google Scholar] [Ref\nlist]\n\n40\\. Byrnes JP, Miller DC, Schafer WD. Gender differences in risk taking: A\nmeta-analysis. Psychological Bulletin. 1999;125: 367\u2013383. doi:\n10.1037/0033-2909.125.3.367 [CrossRef] [Google Scholar] [Ref list]\n\n41\\. Dekel E, Scotchmer S. On the Evolution of Attitudes towards Risk in\nWinner-Take-All Games. Journal of Economic Theory. 1999;87: 125\u2013143. doi:\n10.1006/jeth.1999.2537 [CrossRef] [Google Scholar] [Ref list]\n\n42\\. Griskevicius V, Kenrick DT. Fundamental motives: How evolutionary needs\ninfluence consumer behavior. Journal of Consumer Psychology. 2013;23: 372\u2013386.\ndoi: 10.1016/j.jcps.2013.03.003 [CrossRef] [Google Scholar] [Ref list]\n\n43\\. Feinberg M, Willer R. The Moral Roots of Environmental Attitudes. Psychol\nSci. 2013;24: 56\u201362. doi: 10.1177/0956797612449177 [PubMed] [CrossRef] [Google\nScholar] [Ref list]\n\n44\\. O\u2019Neil C. Weapons of Math Destruction. Discover. 2016;37: 50\u201355. [Google\nScholar] [Ref list]\n\n45\\. Smith A, Anderson J. Digital Life in 2025: AI, Robotics and the Future of\nJobs. Pew Research Center. 2014; 67. doi: 10.1007/s13398-014-0173-7.2\n[CrossRef] [Google Scholar] [Ref list]\n\n46\\. Crawford K, Calo R. There is a blind spot in AI research. Nature. 2016.\npp. 311\u2013313. doi: 10.1038/538311a [PubMed] [CrossRef] [Google Scholar] [Ref\nlist]\n\n47\\. Metz C. Inside OpenAI, Elon Musk\u2019s wild plan to set Artificial\nintelligence free. Wired. 2016; 1\u20137. [Google Scholar] [Ref list]\n\n###### Other Formats\n\n  * PDF (759K)\n\n###### Actions\n\n  * Add to Collections\n\n###### Share\n\n  * Permalink\n\n###### RESOURCES\n\nCite\n\nDownload .nbib .nbib\n\nFollow NCBI\n\nConnect with NLM\n\nNational Library of Medicine 8600 Rockville Pike Bethesda, MD 20894\n\nWeb Policies FOIA HHS Vulnerability Disclosure\n\nHelp Accessibility Careers\n\nExternal link. Please review our privacy policy.\n\n", "frontpage": false}
