{"aid": "40113880", "title": "Scaling with Argo CD: Introducing the Apps Repo Architecture", "url": "https://skip.kartverket.no/blog/introducing-apps-repositories", "domain": "kartverket.no", "votes": 1, "user": "evenh", "posted_at": "2024-04-22 12:52:18", "comments": 0, "source_title": "Scaling with Argo CD: Introducing the Apps Repo Architecture", "source_text": "Scaling with Argo CD: Introducing the Apps Repo Architecture | SKIP\n\nSkip to main content\n\n# Scaling with Argo CD: Introducing the Apps Repo Architecture\n\nApril 22, 2024 \u00b7 20 min read\n\nEline Henriksen\n\nProduct Owner and Platform Developer\n\nArgo CD is an awesome tool. It helps teams de-mystify the deployment process\non Kubernetes by providing a visual representation of the deployments in the\ncluster, and GitOps methodologies gives a consistent and understandable\nstructure to your configuration files.\n\nBut what's the best way to scale when adding more teams? How can we make sure\nthat we're building our GitOps in a way that facilitates for self service and\nsecurity? That's what we'll discuss in this blog post.\n\nKartverket has been using Argo CD and GitOps for several years, and we've\nbuilt an architecture that solves our needs for scale and self-service. Here\nwe'll share our learnings and discuss why our teams are so happy with our Argo\nsetup.\n\n## Multi-tenancy in Argo CD\n\nSo you've deployed Argo CD on your multi-tenant cluster and given your teams\naccess to the user interface. Let's imagine we now have tens of teams and\nhundreds of applications in the Argo UI. When we start scaling out to more\nthan a handful of users we get into some issues with scale. Examples of these\nissues can be:\n\n  * How do you organize your apps and projects?\n  * How do you make sure no two teams accidentally (or maliciously) use the same namespace?\n  * How can we make sure teams clean up unused deployment resources?\n  * How do you seamlessly deploy to multiple clusters?\n\nAs a platform team we often find ourselves thinking that everyone loves\ninfrastructure and Kubernetes as much as we do. This is not the case! Most\npeople have not had the joy of having their childhood ruined by installing\nLinux on their school laptops and configuring WLAN drivers using ndiswrapper.\nBelieve it or not, most people just want tools to get out of their way and let\nthem do their job, be that programming, testing or anything else. Not every\nteam is going to be experts in Kubernetes and Argo. So should we expect all\nteams to know what a deletion finalizer is? What about the intricacies of\nserverside apply vs. clientside apply?\n\nIt's our responsibility as a platform team to make the user experience of\ndeploying to Kubernetes as user friendly as possible. After implementing an\narchitecture built with UX in mind we've had the joy of seeing people who are\nextremely skeptical of Kubernetes and the cloud be won over by how easy it is\nto get your workloads running on Kubernetes. This is thanks to the consistent\nuser experience and built-in best practices of the apps-repo architecture. But\nwe're getting ahead of ourselves, first we need to talk about a few\nabstractions that make this possible.\n\n## What are ApplicationSets?\n\nIn Argo CD there's an advanced feature that allows for automating creation of\nArgo CD Applications called ApplicationSets. Using an ApplicationSet we can\nessentially make a template that generates Argo CD applications based on files\nor folders in a Git repository, sort of like a ReplicaSet for Pods. Using\nApplicationSets we can build in features and assumptions and provide the teams\nwith a user experience that essentially boils down to \"add a file to a repo\nand it gets deployed to the cluster\". The purest form of GitOps. No messing\naround with Argo CD applications and projects.\n\nA core Argo CD component called the ApplicationSet controller will detect any\nApplicationSet resources deployed to the cluster and read them. After this, it\nwill periodically scan the a repo configured in the ApplicationSet resource\nand generate Application resources, which in turn scan a repo for manifest\nfiles and sync them to the cluster. So in other words: ApplicationSet ->\nApplication -> Deployments\n\nFor this to work you need a Git repo containing manifest files. You could have\nthe teams put these manifest files into their source code repositories, but\nthis is not considered best practice. Usually you would put your manifests\ninto a separate repo so that changes to the manifests don't conflict with\nchanges in the source code. At Kartverket we call this manifest repo an apps\nrepo.\n\n## Introducing apps repositories\n\nThe apps repo is where the product teams put their manifests. It has a\nconsistent structure and is designed to be read by an Argo CD ApplicationSet.\nIt also has a lot of nifty features that enable self-service which we'll get\nback to.\n\nFirst, let's have a look at the structure of an apps repo.\n\n    \n    \n    teamname-apps/ env/ clustername/ namespace/ example.yaml\n\nIn the simplest of terms, this tree describes where to deploy a given\nmanifest. By using a directory tree it makes setting up an ApplicationSet for\nthis repo trivial.\n\nConsider this example ApplicationSet:\n\n    \n    \n    apiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: exampleteam-apps namespace: argocd spec: generators: - git: directories: - path: env/*/* repoURL: 'https://github.com/kartverket/exampleteam-apps.git' revision: HEAD goTemplate: true goTemplateOptions: - missingkey=error template: metadata: name: '{{.path.basename}}' spec: destination: namespace: '{{ index .path.segments 2 }}' server: '{{ index .path.segments 1 }}' project: exampleteam source: path: '{{.path.path}}' repoURL: 'https://github.com/kartverket/exampleteam-apps.git' targetRevision: HEAD syncPolicy: syncOptions: - CreateNamespace=true automated: prune: true allowEmpty: true selfHeal: true\n\nWith this ApplicationSet any directory within env/*/* will be picked up by the\nApplicationSet controller and a new Argo CD Application will be created based\non the template in the template object. This enables a product team to create\nany number of applications for their products.\n\nAn example use for this is a product team wanting a namespace for each of\ntheir products. Instead of having to order a new namespace from the platform\nteam when they create a new product, they can simply create it themselves by\nadding a new directory with the same name as the namespace they want. A new\nKubernetes namespace will be automatically created thanks to the\nCreateNamespace=true sync option.\n\nEphemeral namespaces, aka. preview namespaces, is another usecase. Say a team\nwants to review a change before merging it to main. They could review the\nchange in the Pull Request, but this removes us from the end user's\nperspective and is not suitable for non-technical people. With a preview\nenvironment the team will automatically create a new directory in the apps\nrepo when a PR is created, and thus get a complete deployment with the change\nin question. This enables end-to-end testing in a browser, and also allows\nnon-technical people to do QA before a change is merged. When it is merged\nanother workflow can automatically delete the directory, which cleans up and\ndeletes the preview environment.\n\nOur convention is that namespaces are formatted with productname-branch. This\nallows teams to have multiple deploys per product, and also multiple products\nper team. So when a new PR is created all a team needs to do to automate the\ncreation of a new directory using CI tools like GitHub actions to create a new\ncommit in the apps-repo. This also enables the flexibility to create it as a\nPR in the apps-repo, but for ephemeral namespaces, this is usually not\nnecessary.\n\nFor example:\n\n    \n    \n    footeam-apps/ env/ foo-cluster/ foo-main/ app.yaml foo-feature-123/ app.yaml\n\n## Automating and avoiding duplication\n\nDepending on the complexity of the apps repo, the amount of products and\nbranches and a subjective \"ickyness\" with duplicating files (can you spell\nDRY?), you have several options on how to automate creating new namespaces.\n\nSimple repos will probably be fine with directories containing simple yaml-\nfiles that are synced to the cluster. Newer product teams especially\nappreciate the simplicity of this approach. To optimize for this you may\nconsider using a template directory at the base containing some example files\nthat are copied into the sub-directories. A pseudo-coded GitHub action that\nuses a frontend.yaml template from the templates directory could look like the\nfollowing:\n\n    \n    \n    jobs: build: # Build a container image and push it\n    \n    deploy: strategy: matrix: env: ['dev', 'test', 'prod'] steps: # .. Checkout repo & other setup ..\n    \n    - name: Deploy to ${{ matrix.version }} run: | namespace=\"myapp-${{ github.ref_name }}\" path=\"./env/atkv3-${{ matrix.env }}/$namespace\" mkdir -p $path cp -r templates/frontend.yaml $path/frontend.yaml kubectl patch --local \\ -f $path/frontend.yaml \\ -p '{\"spec\":{\"image\":\"${{needs.build.outputs.container_image_tag}}\"}}' \\ -o yaml git config --global user.email \"github-actions@github.com\" git config --global user.name \"GitHub Actions\" git commit -am \"Deploy ${{ matrix.env }} version ${{ github.ref_name }}\" git push\n\nThis works for most simple apps. Our experience, however, is that as a team\nmatures and gets more experienced with Kubernetes and Argo CD, they add more\ncomplexity and want more control. At this point most teams will migrate to\nusing jsonnet to enable referencing and extending a reusable library shared\nbetween multiple components. SKIP also provides some common manifests via\nArgoKit, a jsonnet library.\n\nKustomize is also a common choice, widely used by SKIP for our own\ninfrastructure, but not really widespread with other teams.\n\nDespite Argo supporting Helm we mostly avoid using it to create reusable\ntemplates due to the complexity of templating YAML. Jsonnet is superior in\nthis regard.\n\n> Fixing indentation errors in YAML templates in a Helm chart\n> pic.twitter.com/Dv2JUkCdiM\n>\n> \u2014 memenetes (@memenetes) December 8, 2022\n\n## Security considerations\n\nYou may be wondering: \"This seems great and all, but what about the security\nimplications of allowing teams to create and edit namespaces in a multi-tenant\ncluster? That seems really dangerous!\".\n\nFirst of all, I love you for thinking about security. We need more people like\nyou. Second, Argo CD has some great features we can leverage to make this work\nwithout removing the self-service nature of the apps repo architecture.\n\n### Prefixes\n\nIn order to make this work we need to give each team a set of prefixes. A\nprefix will usually be the name of a product that a product team has\nresponsibility for maintaining. The only important part is that it is unique\nand that no other teams have been allocated the same prefix. At Kartverket\nthis is done by the platform team as part of the team onboarding process.\n\nThe prefix is used as part of all namespaces that are created by the teams. In\nthe example namespace product-feature-123, product is the prefix. By giving\neach team a set of prefixes it helps them separate products into easily\nidentifiable namespaces and it ensures that a product team does not\naccidentally use another team's namespace.\n\nSince each product team has an apps repo with the ability to name their\ndirectories as they wish, how can we enforce this? This is where Argo CD's\nProjects come into play.\n\nArgo CD Projects provide a logical grouping of applications, which is useful\nwhen Argo CD is used by multiple teams. It also contains a field that allows\nallowlisting which clusters and namespaces are usable by a project.\n\nAdd the following to a Project to only allow this project to create and sync\nto namespaces prefixed with myprefix-.\n\n    \n    \n    metadata: name: exampleteam spec: destinations: - namespace: 'myprefix-*' server: '*'\n\nIf you scroll back up to the ApplicationSet example above, you will see that\nit only creates applications with the project exampleteam. This will\nautomatically wire any applications created to the destination rules we've\ndefined in this project and therefore deny any attempts by a team to use\nprefixes that they have not been allocated.\n\nThe crucial part here is that ApplicationSets and Projects are provisioned by\nthe platform team, and therefore build in these security features. These\nresources must not be accessible to the teams, or an attacker can simply add\nexclusions.\n\n### Namespace resources\n\nAnother way this could be abused is if a team is able to create Namespace\nresources in their apps repository. This should be denied using Argo and/or\ncluster policies.\n\nIf a team is able to create namespace resources (or other cluster scoped\nresources) in their namespace an attacked can use this to break their\nnamespace \"encapsulation\". Imagine for example if one could use their apps\nrepo to sync a namespace resource named kube-system into their env/foo-\ncluster/foo-main directory. Argo CD would allow this, as the manifests are\nread into an Argo CD application. Then the attacker could delete the namespace\nand take down the cluster.\n\nIt's useful in this multi-tenancy scenario to think of namespaces as resources\nowned by the platform team and namespace-scoped resources as owned by the\nproduct teams. This is considered a best practice, and was reiterated at\nKubeCon Europe 2024 by Marco De Benedictis. Allowing product teams to edit\nnamespaces can open up a ton of attack vectors, like disabling Pod Security\nAdmission controllers, allowing an attacker to create privileged containers\nwhich can compromise the host node.\n\nFriends don't let friends edit namespaces!\n\n## Self service customization\n\nSo we set up an ApplicationSet that configures best practices and secure\ndefaults for product teams! Great! But now that team with experienced cloud\nengineers really wants to customize their Argo configuration. Maybe they want\nto configure that one app has auto sync on, but another app has it turned off.\nMaybe they want to disable self-healing for a short period to manually edit in\nthe cluster. In any case, how can we let teams change this configuration self-\nservice when applications are provisioned by the ApplicationSet resource?\n\nWe could let the teams edit the ApplicationSet. In our case this would mean\nthe teams need to learn about the ApplicationSet abstraction, gotemplate and\nSKIP's internal GitOps repo structure. This is overkill when a team usually\njust wants to flip a flag between true or false for a directory. There could\nalso be security implications with allowing teams to edit ApplicationSet\nresources that could break encapsulation, which we want to avoid.\n\nAnother option would be to contact the platform team and tell us to change\nsome config for them. This is not in line with our thinking, as we want the\nteams to be able to work autonomously for most operations like this. It would\nalso mean we were given a lot of menial tasks which would mean we have less\ntime to do other more meaningful things or become a bottleneck for the teams.\n\nA third option is setting the ApplicationSet sync policy to create-only. This\nwould confifure the ApplicationSet controller to create Application resources,\nbut prevent any further modification, such as deletion, or modification of\nApplication fields. This would allow a team to edit the application in the UI\nafter creation, for example disabling auto sync. This last option is user\nfriendly, but in violation of GitOps principles where config lives in git and\nnot in a database. If you run Argo stateless like we do this would also mean\nthe changes disappear when the pod restarts.\n\nBecause none of these options seemed to be the best, we created a better\nsolution. By using a combination of generators and the new template patch\nfeature in Argo CD 2.8 we can look through every directory in the apps repo\nfor a configuration file called config.json.\n\nLet's look at an example config.json file. This example file is commited in\nthe apps repo to the env/foo-cluster/foo-main directory.\n\n    \n    \n    { \"tool\": \"kustomize\", \"autoSync\": false }\n\nThis file is not required, but if this file is found the values configured\nthere overrides a set of default values in the ApplicationSet template. These\nflags are then used to determine how the resulting Application will behave.\nThis means the team is able to change the values they care about per directory\nof their apps repo\n\n    \n    \n    footeam-apps/ env/ foo-cluster/ foo-main/ config.json app.yaml foo-feature-123/ config.json app.yaml foo-feature-with-default-config/ app.yaml\n\nAdditionaly, since the platform team is in control of the template we can\neliminate the ability to maliciously change the template by parsing the inputs\nin a secure way.\n\n### Example ApplicationSet\n\nLet's look at how we can write an ApplicationSet that allows us to use\nconfig.json files.\n\nFirst, we need to configure the ApplicationSet to look through all\ndirectories, and at the same time use a config.json file if it is found. This\nis perhaps the least intuitive part of this new ApplicationSet, so let's walk\nthrough it step by step.\n\nFirst we create a merge generator, which will merge two generators. The key\nthing here is that it only merges if the key matches in both generators, so\nthis allows us to first find all directories (the default), then directories\nthat contain config.json files (the override).\n\n    \n    \n    generators: - merge: generators: - # default - # override mergeKeys: - key\n\nNow we're going to add the generator from before into the default. The only\ndifference is we're doing this using a matrix generator. Doing this combines\nthe parameters generated by the two child generators, which gives us the\nvalues from the git generator like before, but also a set of default values we\ncan use in our template later if the config.json file is not provided.\n\nWe're also using a value from the git generator to assign a key that will\nuniquely identify this directory for the merge generator later.\n\n    \n    \n    generators: - merge: generators: - matrix: generators: - git: directories: - path: env/*/* repoURL: 'https://github.com/kartverket/exampleteam-apps.git' revision: HEAD - list: elements: - allowEmpty: false autoSync: true key: '{{ .path.basenameNormalized}}' prune: true selfHeal: true tool: directory - # override mergeKeys: - key\n\nNow we use a variant of the git generator to find all config.json files in the\nsame repo and extract the values from it. Again we're using the key field to\nuniquely identify this directory so that it will be merged with the correct\ndirectory in the merge generator.\n\nWe're repeating the default values here as well, since not all fields are\nrequired and we don't want them to be overwritten as null in the resulting\nmerge.\n\n    \n    \n    generators: - merge: generators: - matrix: generators: - git: directories: - path: env/*/* repoURL: 'https://github.com/kartverket/exampleteam-apps.git' revision: HEAD - list: elements: - allowEmpty: false autoSync: true key: '{{ .path.basenameNormalized}}' prune: true selfHeal: true tool: directory - matrix: generators: - git: directories: - path: env/*/*/config.json repoURL: 'https://github.com/kartverket/exampleteam-apps.git' revision: HEAD - list: elements: - allowEmpty: false autoSync: true key: '{{ .path.basenameNormalized}}' prune: true selfHeal: true tool: directory mergeKeys: - key\n\nThat's it for the generator! Now we can use these variables in the\ntemplatePatch field (and other fields). In this case we want to set syncPolicy\noptions, so we need to use the templatePatch, as gotemplates don't work for\nobjects.\n\nWe're also adding a special case where for directory sources (the default) we\nexclude config.json files, as we don't want to sync the config file with Argo.\nThis allows us to extend it later to add options for other tools like\nKustomize or Helm.\n\nKeep in mind that we don't want users to inject maliciously formed patches, so\nwe cast booleans to booleans.\n\n    \n    \n    templatePatch: | spec: source: directory: {{- if eq .tool \"directory\" }} exclude: config.json {{- end }} {{- if .autoSync }} syncPolicy: automated: allowEmpty: {{ .allowEmpty | toJson }} prune: {{ .prune | toJson }} selfHeal: {{ .selfHeal | toJson }} {{- end }}\n\n## Complete ApplicationSet\n\nHere is a complete ApplicationSet containing all the features we've discussed\nso far.\n\n    \n    \n    apiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: exampleteam-apps namespace: argocd spec: generators: - merge: generators: - matrix: generators: - git: directories: - path: env/*/* repoURL: 'https://github.com/kartverket/exampleteam-apps.git' revision: HEAD - list: elements: - allowEmpty: false autoSync: true key: '{{ .path.basenameNormalized}}' prune: true selfHeal: true tool: directory - matrix: generators: - git: directories: - path: env/*/*/config.json repoURL: https://github.com/kartverket/exampleteam-apps.git revision: HEAD - list: elements: - allowEmpty: false autoSync: true key: '{{ .path.basenameNormalized}}' prune: true selfHeal: true tool: directory mergeKeys: - key goTemplate: true goTemplateOptions: - missingkey=error template: metadata: name: '{{.path.basenameNormalized}}' spec: destination: namespace: '{{ index .path.segments 2 }}' server: '{{ index .path.segments 1 }}' project: exampleteam source: path: '{{.path.path}}' repoURL: 'https://github.com/kartverket/exampleteam-apps.git' targetRevision: HEAD syncPolicy: managedNamespaceMetadata: labels: app.kubernetes.io/managed-by: argocd pod-security.kubernetes.io/audit: restricted team: exampleteam syncOptions: - CreateNamespace=true - ServerSideApply=true - PrunePropagationPolicy=background templatePatch: | spec: source: directory: {{- if eq .tool \"directory\" }} exclude: config.json {{- end }} {{- if .autoSync }} syncPolicy: automated: allowEmpty: {{ .allowEmpty | toJson }} prune: {{ .prune | toJson }} selfHeal: {{ .selfHeal | toJson }} {{- end }}\n\n## Results\n\nWith Argo CD and the apps repo architecture, we've seen some real improvements\nin our deploy system. Teams find it to be incredibly intuitive to just update\na file in Git and have it be instantly reflected in Argo CD and Kubernetes,\nespecially when combined with Argo CD auto-sync.\n\nOnboarding new teams is quick and easy, since just putting files into a Git\nrepo is something most developers are already familiar with. We just show them\nthe structure of the apps repo and they're good to go. A team can go from not\nhaving any experience with Kubernetes to deploying their first application in\na matter of minutes.\n\nMigrating from one cluster to another is also a breeze. Just move manifests\nfrom one directory under env to another, and the ApplicationSet will take care\nof the rest. This is especially useful for teams that want to start developing\nwith new cloud native principles on-premises, modernizing the application and\neventually moving to the cloud.\n\nI feel the key part of this architecture is the config.json file. It allows a\ndegree of customization that is not possible with the default ApplicationSet\ntemplate and was to us the last missing piece. It allows teams to change\nconfiguration without needing to know about the ApplicationSet abstraction,\nand it allows the platform team to enforce security and best practices.\n\n### Tradeoffs\n\nBut of course, there are some drawbacks. Like always, it's tradeoffs all the\nway down.\n\nSince a product team uses an apps repo to organize their apps, moving apps\nfrom one team to another will require migrating files from one repo to\nanother. This will require some manual work to prevent Argo deleting the\nentire namespace when the directory is removed from the old repo. Usually this\nis not a big issue, and moving projects between teams happens very rarely, but\nit's something to keep in mind.\n\nThere is also a risk that a team could accidentally delete a namespace by\nremoving a directory in the apps repo. We have mitigated this by disabling\nauto-sync for most mission critical applications in production.\n\nAnd finally, projects that don't have clear ownership or shared ownership can\nbe tricky to place into a repo. You could make an apps repo for a \"pseudo-\nteam\" consisting of the teams that need access, but generally we find that\nit's better that all products have a clear singular main owner. This also\nprevents diffusion of responsibility.\n\n## Thank you for reading!\n\nWe hope you found this article helpful and informative. Getting into\nApplicationSets can be a bit tricky, so we hope we managed to convey the most\nimportant parts in a clear and understandable way. Thanks for reading!\n\nWe recently created a Mastodon account @kv_plattform! If you want to contact\nus or discuss this article, feel free to reach out to us there.\n\nTags:\n\n  * kubernetes\n  * argo-cd\n  * gitops\n\n  * Multi-tenancy in Argo CD\n  * What are ApplicationSets?\n  * Introducing apps repositories\n  * Automating and avoiding duplication\n  * Security considerations\n\n    * Prefixes\n    * Namespace resources\n  * Self service customization\n\n    * Example ApplicationSet\n  * Complete ApplicationSet\n  * Results\n\n    * Tradeoffs\n  * Thank you for reading!\n\nLenker\n\n  * Tilgjengelighetserkl\u00e6ring\n\nCopyright \u00a9 2024 Statens Kartverk\n\n", "frontpage": false}
