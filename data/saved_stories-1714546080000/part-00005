{"aid": "40216114", "title": "Five stages of accepting provably robust anonymization", "url": "https://desfontain.es/blog/five-stages.html", "domain": "desfontain.es", "votes": 1, "user": "p4bl0", "posted_at": "2024-04-30 20:44:48", "comments": 0, "source_title": "Ted is writing things", "source_text": "Five stages of accepting provably robust anonymization - Ted is writing things\n\n..@..\u2666.D.| About| Blog| Recipes  \n---|---|---|---  \nlatest \u2014 rss \u2014 archives \u2190 previous  \n  \n# Ted is writing things\n\nOn privacy, research, and privacy research.\n\n# Five stages of accepting provably robust anonymization\n\n2024-04-28\n\nThis post is a transcript of an invited talk I delivered to AnoSiDat in April\n2024.\n\nHi everybody! I\u2019m Damien.\n\nI\u2019m about to do something they always tell you not to do when you give\npresentations, and that\u2019s \u201cspeak about myself for a little while\u201d.\n\nI will to try to make it into a story though, so maybe it\u2019s OK. You tell me.\n\nIn 2016, I started splitting my time between my job at Google working in the\nprivacy team, and pursuing a PhD at ETH Z\u00fcrich.\n\nOn both sides, I was working on anonymization. And anonymization had something\nvery puzzling about it.\n\nAt Google, and in the rest of the industry, people mostly used notions like\nk-anonymity. The idea is to people into buckets of sufficiently many other\npeople that they\u2019re \u201cbasically anonymous\u201d.\n\nIt wasn\u2019t always k-anonymity. It could be other notions based on an intuitive\nidea of what anonymous data should look like. The general idea was to try and\nsee what could go wrong with a certain method, and if we couldn\u2019t think of\nanything, we\u2019d say: it\u2019s probably good enough.\n\nIn academia, though, especially among computer scientists, everyone seemed to\nhave converged on another notion: differential privacy. DP, as we like to call\nit, is not just a new criterion to decide whether data \u201clooks\u201d anonymous\nenough. Instead, it\u2019s a completely different approach, grounded in math. It\ntells you: there\u2019s bad news and good news.\n\n  * The bad news is that anything you publish will leak some individual information. You just can\u2019t escape that fact.\n  * The good news, is that you can quantify this leakage and limit it, regardless of the attack.\n\nA lot of people seemed to have seen this new concept and gone like: this is\nit. Not only is the math satisfying, but this is the right notion, on a\npolitical or even moral level. That\u2019s how we really protect the people in the\ndata.\n\nThat gap between practice and theory seemed weird to me. It looked like two\nseparate worlds who weren\u2019t really speaking to each other.\n\nSo I thought: could we try and bring the two worlds together somehow? That was\nthe big problem I wanted to solve during my PhD. I looked at older notions of\nanonymization, or practical tools used in industry, and asked: could you\nquantify their leakage in the language of DP? Maybe show that they\u2019re not that\nbad, and that they do provide good guarantees in practice?\n\nThat didn\u2019t quite work out.\n\nInstead, I became a lot like these folks in academia: I started focusing much\nmore heavily on differential privacy. I became convinced that this was the\nfuture, not just in academia, but for real-world use cases in industry as\nwell. At Google, my team even tried to make it into the default approach used\nfor anonymization. We made a ton of progress in that direction!\n\nNow, I\u2019m at Tumult Labs, a startup that tries to make that happen at the scale\nof the entire industry. Our goal \u2014 my job \u2014 is to make differential privacy\ninto this thing that everybody uses.\n\nKind of like cryptography. If you\u2019re not using HTTPS on your website today,\nthat\u2019s bad. People are like: \u201cYo, that\u2019s unsafe. You\u2019re putting your users at\nrisk. Do better.\u201d\n\nMy dream is to live in a world where if you\u2019re claiming to anonymize some\ndata, and you\u2019re not using DP... that\u2019s kind of suspicious.\n\nA few years ago, I thought this industry-wide shift would happen very soon.\nAny day now.\n\nExcept: it\u2019s not really happening. Or maybe it kind of is, but it\u2019s moving\nmuch more slowly than I expected. Take a random government agency that\npublishes statistics about people. Or a random company sharing anonymized data\nwith a third party. What are the chances they\u2019re using differential privacy?\nNot great.\n\nThat divergence between communities I observed 8 years ago? It\u2019s still there.\nIf anything, it got worse.\n\nMany academics and practitioners just... don\u2019t use newer, more robust notions.\nInstead, they keep doing the thing they\u2019ve done for decades! At least, that\u2019s\nwhat it looks like from where I\u2019m standing.\n\nAnd the differential privacy community \u2014 my community \u2014 has developed a sort\nof arrogance about people who haven\u2019t made the switch. To us, differential\nprivacy is so obviously the right approach. If you\u2019re not getting it, there\nmust be something wrong with you.\n\nIn fact, you might feel exactly this way about me based what I\u2019ve told you so\nfar. \u201cUrgh, another guy who\u2019s going to lecture me about how I do\nanonymization.\u201d\n\nOr maybe you are focusing on differential privacy, and I\u2019ve just called you\narrogant, and now you\u2019re like \u201cwho does this guy think he is?\u201d. I\u2019m making\neveryone unhappy. This talk is going great. You\u2019re so glad you\u2019re here.\n\nIn all seriousness though, I do think that as a community, we\u2019re not doing\nenough to understand people who keep using ad hoc anonymization. We\u2019re not\nreally listening to what they\u2019re telling us, or being empathetic to their\nexperiences. And... that doesn\u2019t feel right. That doesn\u2019t feel very\nproductive, either.\n\nSo in this talk, I\u2019ll try to do better.\n\nI\u2019ll attempt to truly engage with the arguments of critics of differential\nprivacy. Make sure I \u201cget it\u201d by making them mine. Fully understand the\nchallenges in the way of our grand vision, and try to figure out how we,\ntogether, can address them.\n\nI\u2019ve named this talk \u201cFive stages of accepting provably robust anonymization\u201d.\nThe stages, are, of course: denial, anger, bargaining, depression, and\nacceptance.\n\nIt\u2019s silly, of course, especially since my understanding is that these \u201cfive\nstages of grief\u201d model is actually kind of outdated in psychology.\n\nBut discussing questions like \u201chow do you define privacy\u201d can actually be\npretty emotional, for people like you and me. We care about this stuff. We do\nprivacy because we want to do the right thing. We want to build better, more\nrespectful ways of handling and sharing and publishing data. So when someone\ngoes and say \u201cthe way you\u2019re doing this is all wrong\u201d, it\u2019s not just an\nintellectual dispute. It makes us feel things.\n\nSo, we\u2019re going to go on a journey, and it\u2019s going to be an emotional journey.\n\nLet\u2019s dive in!\n\nThe first phase is Denial.\n\n> We don\u2019t actually need differential privacy. Whenever folks doing\n> differential privacy do their \u201canonymization 101\u201d lecture, it\u2019s always the\n> same story. First they come in, and start explaining some classical\n> reidentification attacks. The governor of Massachusetts reidentified by\n> Sweeney! AOL search queries! The New York taxi dataset! The Netflix prize\n> dataset! Then, they go: \u201cThis stuff is bad, and it could happen to you. You\n> need DP to make sure that you mitigate against that.\u201d\n>\n> But let\u2019s be honest. This isn\u2019t very convincing.\n\n> First, these attacks are honestly kind of underwhelming. The risk just seems\n> overblown. Who cares if we figure out that the person who watched some movie\n> also watched this other movie? Or if we can reidentify a couple of\n> celebrities\u2019 taxi trips? OK, this isn\u2019t supposed to happen, but... nobody\u2019s\n> actually harmed in real life by this stuff.\n>\n> Plus... this is purely academic work. In real life, people won\u2019t investigate\n> your data on the off-chance that they\u2019ll reidentify one person! Unless\n> they\u2019re people are academics trying to prove a point. What are the chances\n> this happens to me?\n>\n> And also, in all attacks, the datasets were obviously badly protected. Of\n> course removing names is not enough! Of course search engine queries are\n> reidentifying! Their problem wasn\u2019t that they weren\u2019t using differential\n> privacy. Their problem was that they didn\u2019t even try! If they had thought\n> about it for more than 5 minutes, and used any reasonable scheme to protect\n> data, they\u2019d probably have been fine.\n\nThese arguments are honestly pretty valid, and I wish we\u2019d admit this more\nopenly in the differential privacy community.\n\nFirst: most attacks, especially the shiny ones that got press coverage, were\ndone on datasets that were pretty embarrassing. It makes total sense to see\nthis and think: if I do my homework and try to do something reasonable to\nprotect my data, that should be enough to make sure bad things don\u2019t happen.\n\nThis can also be the case for compliance-based motivations for anonymization.\nYou know that joke about how, to escape a bear, you don\u2019t have to run faster\nthan the bear, you have to run faster than the slowest person running from the\nbear? Let\u2019s be real: some people feel this way about data protection\nauthorities. Regulators are still at the stage where they have to tell people\n\u201chashing identifiers is not anonymization\u201d! If you can demonstrate to a\nregulator that you did something reasonable, surely they\u2019re not going to be\nmad because you didn\u2019t use the gold standard, right?\n\nFinally, it\u2019s totally fair to say: \u201clook, I know I have to do cybersecurity\nstuff, I\u2019m going to get ransomware otherwise\u201d. The risk is real, it\u2019s\ntangible. We\u2019ve all received emails like \u201cwe deeply care about your privacy,\nalso all your data got stolen by random hackers\u201d. We know it happens all the\ntime. Re-identification attacks... not so much. The risk just isn\u2019t that huge.\n\nFirst, let\u2019s look at the last argument that risks only exist on \u201cobviously\nunsafe\u201d data releases. One piece of data that doesn\u2019t quite go in this\ndirection are reconstruction attacks. Like the one the U.S. Census did on\ntheir 2010 data.\n\nHere\u2019s a diagram giving a rough ideas of how it works. The idea is remarkably\nsimple: you consider every published statistic as an equation with a bunch of\nunknown variables \u2014 one per unknown attribute in each record. And then you\nsimply... solve that system of equations. If you have enough statistics, you\ncan just do that. This gives you reconstructed records. You went from\nstatistics to actual individual records. If a bunch of those are correct, that\nseems bad.\n\nThe attack doesn\u2019t stop there, though. It has a second step: you use an\nauxiliary dataset to link each reconstructed record with an actual person, and\nlearn something additional about this person.\n\nIt\u2019s fair to say that the attack was far easier, and far more successful, than\npeople expected. It was a real wake-up call inside the U.S. Census Bureau.\nPlus, it was done on the data from 2010, which used well-researched disclosure\navoidance techniques. So, that argument from earlier that attacks only happen\non datasets that are \u201cobviously\u201d badly protected... It\u2019s starting to look a\nlittle iffy.\n\nThere\u2019s still some debate about this attack, though. The initial description\nof the U.S. Census reconstruction attack did not give a lot of detail about\nhow it worked, and people had opinions about its validity. One common question\nwas around how to quantify the success of attacks, and against which baselines\nto compare it to. I\u2019m not going to relitigate the argument here. The Census\nrecently published a much more thorough technical description of what they\ndid, which I think puts to rest a lot of the technical debate. You should read\nit if you want to learn more.\n\nBut this debate undoubtedly raised some good points and questions around what\nit means for an attack to be successful, and how to actually demonstrate\nattack success.\n\nAnother way people have criticized the attack is by saying \u2014 let\u2019s be real.\nCensus data isn\u2019t that sensitive. It\u2019s just demographics: gender, age, racial\ninformation. It\u2019s not like medical data. Nobody would go through all this\ntrouble just to figure out \u201coh, this person\u2019s age is 42\u201d. Even if you\nreconstruct & reidentify a Census record, you don\u2019t really learn anything new\nand sensitive about that person.\n\nHere\u2019s something you can do with this statistical releases of this kind.\n\n  * You can take data from one year, reconstruct and reidentify some records...\n  * ... then you do the same attack on a more recent data release...\n  * ... and you answer a simple question: \u201cwho reported a different sex/gender attribute since last time\u201d?\n\nThat\u2019s exactly what two researchers did. They showed you could reidentify\nhundreds of trans people based on statistical releases. In the current\npolitical climate... I don\u2019t know about you, but that scares me.\n\nThere\u2019s a hard lesson in this: the real world will always find a way to\nsurprise you. Not just in \u201chow could an attacker do this nefarious thing\u201d, but\nalso in what nefarious thing can even be the goal. I\u2019ve seen some bad stuff\nworking on privacy in the industry for 10 years, but this attack still caught\nme by surprise.\n\nLet\u2019s look at the last point \u2014 nobody cares about reidentifying your data.\n\nHere\u2019s a real quote from a demographer in USA, complaining about the use of DP\nin the 2020 Census. \u201cIn past censuses, it was possible to create microdata\nfrom statistical tables, it was just a pain.\u201d\n\nThis is from a meeting Census folks had with some of their data users. This\nperson is just saying out loud what everyone is doing. People do reconstruct\ndata in order to do gerrymandering, political targeting, or any kind of\nresearch that would be made easier if we had microdata. This is an open\nsecret.\n\nSpeaking of open secrets... have you heard of the advertising industry?\nThere\u2019s a five-hundred-billion-dollar world who\u2019s predicated on showing ads to\nprecisely the right people, and having measurements that are as precise as\npossible. Data flows left and right, and companies have major financial\nincentives to run exactly this kind of attack, to get more information than\nwhat they\u2019re supposed to.\n\nThe fact that you don\u2019t hear about this kind of thing in the press doesn\u2019t\nmean it\u2019s not happening. Nobody has any interest in publicly talking about it\n\u2014 not the people doing it, not the people that sell data and then get it\nexploited, not the people buying or using it. So it\u2019s another one of these\nopen secrets.\n\nWant to know the worst thing I\u2019ve heard? There was a startup in the US that\nwas reconstructing & reidentifying Census records and linking them with\nmedical info to better target health insurance advertising. Again, the real\nworld keeps surprising you...\n\nHow can we move people past the denial stage?\n\nFirst, we need to do a better job at educating people, and especially\ndecision-makers \u2014 think data protection officers, regulators, product owners \u2014\nabout the risks of bad anonymization. Both about what can go unexpectedly\nwrong, and how that can happen. We can\u2019t keep using the same examples we used\n10 years ago.\n\nSecond, attacks are extremely valuable both to our academic understanding of\nreal-world privacy risk, and to the larger societal conversation about what\nanonymization should protect against. Go look at what your government is\npublishing, or what kind of data sharing is done as part of your healthcare\nsystem. Try to understand what makes sense as an attack, what makes sense as a\nbaseline, and how to actually demonstrate real-world risk. Just a few examples\ncan go a long way in making people aware of a problem they didn\u2019t think about\nso far.\n\nThe second stage, after denial, is Anger.\n\n> So, I\u2019m a data scientist, I\u2019ve been tasked to anonymize some data, so I\n> thought I\u2019d give differential privacy a try. I picked up an open-source\n> library that does differential privacy, and tried to use it to solve my\n> problem.\n>\n> And I have one question.\n\n> Why is it so hard to do anything?!\n>\n> Why do I need to learn so many new concepts? Why is this not integrated with\n> the tool I\u2019m normally using? Why is this so slow? OK, those were multiple\n> questions. What the hell, though?\n>\n> Also... it seems like I can only do simple analytics queries? Don\u2019t get me\n> wrong, a large part of what I need to do with data basically boils down to\n> group-by count or group-by sum queries. But sometimes I need to do more\n> complicated things! Like... Linear regression? Na\u00efve Bayes classification?\n> Anomaly detection? What am supposed to do if I need any of these?\n\nWho in the audience has tried using DP libraries? Do you relate with that\nexperience? I see some people nodding... Yeah. The anger is, honestly,\njustified.\n\nIt\u2019s our job to make this stuff easy to use, and we\u2019re simply not doing a\ngreat job. You can\u2019t just run an SQL query that does some simple aggregations\nand get good results. You still have to learn about epsilon, and probably\ndelta, and maybe even rho and other weird parameters.\n\nAnd even if you do, it\u2019s unlikely that you will be able to solve your problem\non your own. Real-world problems still often have requirements for which there\nis no good guidance, so you\u2019ll probably need to hire an expert to even think\nabout using the stuff. These can be because the necessary algorithms are still\ncutting-edge research, or simply because the context is somewhat unusual.\n\nEven in the best of cases, where the tool actually solves the problem, there\u2019s\nare big questions about how to optimize the algorithm and choose its privacy\nbudget. There\u2019s very little guidance and even less tooling that helps people\nunderstand trade-offs.\n\nAt my company, we develop Tumult Analytics, which we think is the best-in-\nclass DP framework. These problems are top of mind for us, and we try to\naddress them. Things are getting better!\n\nHere are some things we try to do to improve the status quo.\n\n  * Our tool is in Python, and its API mirrors the API of PySpark or Pandas, which helps people learn fast.\n  * We invested a lot of time making a user-friendly tutorial series that helps people learn what they need to use the tool.\n  * We have a number of unique features that our clients need in their production use cases, and the framework is built for extensibility, allowing ourselves to easily add more capabilities in the future.\n  * Finally, we just shipped the first version of our parameter tuning & optimization features to a customer. Stay tuned for the public launch! (Or if that sounds interesting to you, come say hi!)\n\nHow can you help here?\n\nFirst: talking to people. What do I mean by this? Things like: running\nusability studies to understand how to make tooling more user-friendly.\nInterviews with potential users to understand their needs and identify gaps.\nWriting more simple explanations of this stuff for wider audiences.\nCollaborating with people outside of your field, especially if you want to\napply DP to a new field of application. Building bridges. Solving other\npeople\u2019s problems.\n\nSecond, when you make a new thing... why not extend existing open-source\ntools? Some frameworks, like Tumult Analytics or OpenDP, are designed for\nextensibility, so adding new things is easier than you\u2019d think! The additional\nwork is not huge, compared to doing the actual research or writing a paper.\nAnd when it\u2019s super easy for people to reuse your work, you tend to get a lot\nmore exposure & citations!\n\nThird, if you make up new differential privacy mechanisms... make them hands-\nfree? What I mean by that is \u201cget rid of the hyperparameters\u201d. Every single\nparameter that needs to be set manually is a big usability hurdle. So it\u2019s\nworth figuring out \u2014 can we spend a little portion of the privacy budget to\nautomatically select good values for these hyperparameters, rather than asking\nthe user? End users will thank you!\n\nThe third stage is Bargaining.\n\n> I\u2019ve gone through denial already. I realize that there is a real problem,\n> and that it\u2019s worth fixing. After I got angry trying to use DP in practice,\n> I started looking for alternative solutions. And I think some of them make\n> sense! Hear me out.\n\n> First, I thought \u2014 what if I take some shortcuts? Maybe I can just add the\n> same kind of noise they use in DP, but without doing all the rest of the\n> annoying stuff? Like, adding some Gaussian samples in SQL or Excel to my\n> statistics should be good enough, right?\n>\n> But then I had an even better idea: synthetic data! If I make synthetic data\n> out of my original dataset, surely that\u2019s safe enough to share and publish,\n> right? The records aren\u2019t real! They don\u2019t actually correspond to anyone! It\n> seems to me that by definition, it breaks linkability, and nothing in there\n> can be reidentifiable.\n>\n> I had some lingering doubts about the safety of doing that, but then I\n> talked to a synthetic data vendor. And he had a really compelling argument:\n> empirical metrics. He said: \u201cI\u2019m not saying it\u2019s always safe, but we have\n> ways to verifying whether the data you generated it safe enough. We can run\n> tests and compute a privacy score that tells us whether the data you got is\n> anonymized enough.\u201d\n>\n> That sounds great to me!\n\nLet\u2019s be clear. This is already so much better than pretending the problem\ndoesn\u2019t exist.\n\nObviously, just adding Laplace noise of scale 1 to a sum of salaries in euros\nis not doing much. But if you try do DP, but maybe your noise is not\ncryptographically safe, or your sensitivity calculation is a little wrong...\nprobably it\u2019s better than not trying at all. Similarly, using synthetic data\nis much better than sharing the original dataset with the identifiers removed,\nor using a known broken approach. Academics will tell you: this isn\u2019t a silver\nbullet! There are attacks on synthetic data that can retrieve the original\ninformation! But it\u2019s also true that these attacks are not as convincing as\nthe ones on de-identified datasets, or even reconstruction attacks I talked\nabout earlier.\n\nIt\u2019s also understandable that people want to do that, when existing DP tools\ndon\u2019t solve their use case, or aren\u2019t integrated with their existing\nworkflows. Rolling your own anonymization by hand can seem much easier. Plus,\nsometimes, when you tell people that the first step to use DP is that they\nhave to aggregate their data, you kind of lose them from the start. Synthetic\ndata feels much more palatable, understandable. And the user experience can be\npretty smooth. Press a button, you\u2019re done.\n\nFinally, efforts to quantify privacy are laudable. Having a metric for\nsomething allows people to draw comparisons, make data-driven decisions, rely\non something concrete.\n\nPlus, when you\u2019re an engineer or a data scientist, a nice number just makes\nyou feel warm and fuzzy inside. And when the number go up or down? Brrrr.\nFeelings.\n\nI\u2019m joking, but this isn\u2019t a bad thing! It\u2019s great to use this as an emotional\nlever to encourage people to take better privacy decisions! We should\nabsolutely measure and quantify things!\n\nAsk a DP expert to push back against these and they\u2019ll tell you \u201cthere\u2019s\nnothing that shows it\u2019s safe, so we should assume it\u2019s not\u201d. Honestly, we find\nit even annoying that you\u2019d ask us to make this case. It\u2019s so obvious! Look at\ncryptography! They learned the hard way that everything should be grounded in\nmath. You made a new fancy encryption algorithm but you can\u2019t prove it\u2019s safe?\nGet out of here!\n\nBut let\u2019s be real: the outside world doesn\u2019t see things this way. Especially\nnot with anonymization. Not yet. We\u2019re just not going to win this argument on\nphilosophical grounds.\n\nSo instead, let\u2019s look more closely at these bargaining alternatives \u2014\nsynthetic data and empirical privacy metrics. We\u2019ll need to dive a little\ndeeper to figure out how to properly engage with this argument. How does one\nmeasure privacy, actually? Where do the warm and fuzzy numbers actually come\nfrom? As you can guess from the picture I chose there, it\u2019s going to be\nsomewhat of a rabbit hole. Bear with me.\n\nMost people use something that falls into the umbrella of similarity-based\nmetrics. The idea is relatively simple.\n\nFirst, you take your data and you split it in two parts \u2014 the train data and\nthe test data, kind of like you do in machine learning.\n\nThen, you use only the train data to generate your synthetic data.\n\nThen \u2014 and this is where it gets interesting \u2014 you compute the distance\nbetween the synthetic data and the train data. There are many ways to compute\na distance between two distributions; you end up with different metrics\ndepending on the distance you choose. But for this explanation, we\u2019ll ignore\nthe details, and focus on the intuition of this distance: it tries to answer\nthe question \u201cam I generating something that\u2019s suspiciously close to the real\ndata?\u201d\n\nBut you don\u2019t answer that question directly. Instead, you compute a second\ndistance, this time between the synthetic data and the test data. Now you\u2019ve\ngot two numbers, so you\u2019re doing the natural thing and compare them with each\nother: is the distance to the train data smaller than the distance with the\ntest data?\n\nIf yes, that\u2019s... Is that bad or good? Think about it.\n\nCorrect! That\u2019s bad. That means we generated records that are close, not just\nto the real data, but to the specific points that we used for generation. We\ndidn\u2019t just end up matching the distribution well, we overfit to individual\ndata points. That could be a sign that we leaked some private information. So,\nthat\u2019s bad.\n\nConversely, if the two numbers are roughly the same, or even if the distance\nto the train data is larger, that means we\u2019re fine. We didn\u2019t leak any\nsensitive data. Right?\n\n... right?\n\nWell... not really.\n\nFirst, it\u2019s easy to cheat at these metrics. All we need to do is to make the\ndistance to the training data large enough, and bam, the metric says we\u2019ve\ndone well. For example, with numerical data, I could just add a large constant\nnumber to every record. This way, the data I\u2019m generating is \u201cfar away\u201d from\nthe train data. The metric tells me I\u2019ve done a great job at privacy. But...\nall the sensitive information that was in the original dataset is still in the\n\u201csynthetic\u201d dataset. I\u2019ve leaked all the information. That\u2019s bad!\n\nOf course, you\u2019ll tell me: \u201cBut, Damien, that\u2019s unfair. We\u2019re not measuring\nthe privacy of evil algorithms made by attackers. These edge cases are\nmeaningless. We don\u2019t cheat in real life!\u201d\n\nTo which I say: you\u2019re using machine learning! You\u2019re giving your data to a\nneural network, you don\u2019t really understand how it works, but you tell it: \u201cGo\noptimize for these metrics. I want good utility and good privacy, and this is\nhow both of these things are defined. Find a good way of achieving both\nobjectives.\u201d\n\nGuess what? Neural networks are going to cheat! That\u2019s what they do! They\u2019re\njust doing it in a smarter way that avoids obvious detection! So gameability \u2014\nhow easy is it to cheat at a metric \u2014 is something you should be concerned\nabout.\n\nSecond: using distances is basically saying that the only thing the attacker\ncan do is \u201ctry to find records that are close to real records\u201d. As in, if\nthere\u2019s a synthetic record that matches a real record, then the attacker wins.\n\nBut... that doesn\u2019t make any sense! The attacker doesn\u2019t know which are real\ndata points! That doesn\u2019t correspond to something that people would actually\ndo to breach privacy! And attackers can do a ton of things: exploit subtle\ndetails of your algorithm, use auxiliary information, or even influence your\ndata sometimes. So using distances is restricting what the attacker is\n\u201callowed\u201d to do, without good justification.\n\nThird, you\u2019re computing a single number that captures a distance between\ndistributions. You got data points on the left, data points on the right, and\nyou\u2019re getting an averaged metric about your system. It\u2019s giving you, at best,\ninformation about the average level of \u201cprivacy\u201d provided by the synthetic\ndata generator.\n\nBut \u2014 and I cannot stress this enough \u2014 everyone needs privacy guarantees!\nIncluding outliers! Especially outliers! If your approach works fine for most\npeople, but leaks a ton of data for people in demographic minorities, that\u2019s\nbad! In fact, it\u2019s even worse than a system that leaks everyone\u2019s information!\nAt least you\u2019d notice and fix it!\n\nAaaargh! I thought we were past the anger stage but here you go, I got angry.\nI told you this stuff was emotional!\n\nFourth, and finally, something fun about the empirical metrics used in\npractice. Try to generate synthetic data and measure its privacy. Then do it\nagain, but with a different random seed, or different train/test data split.\n\nAre you obtaining the same number? Is the thing telling you whether it\u2019s \u201csafe\nenough\u201d giving you the same information both times?\n\nI\u2019ll spare you the experiment \u2014 the answer is \u201cprobably not\u201d. Researchers\ntried it on platforms offered by various synthetic data vendors and got widely\ninconsistent results. So even if the metric was more meaningful, it\u2019s really\nhard to trust it if the \u201cthings look OK!\u201d signal is super noisy.\n\nOK, so that\u2019s bad. Bad news, though: it gets worse. Because if you think about\nit, at its core, what are we trying to achieve?\n\nWe\u2019re trying to quantify risk. There\u2019s some kind of scale, some end of the\nscale is great, the other end is bad, and we want to know where we are on that\nscale. Well, we\u2019ve seen that maybe we\u2019re not exactly measuring risk, more like\n\u201crisk\u201d, but let\u2019s set that aside.\n\nPeople building and selling synthetic data are basically telling you: you\ngenerate some data and know where you are on the scale. Like, for example,\nthere. You\u2019re in the safe zone. You\u2019re fine. But that\u2019s not what empirical\nprivacy metrics can ever tell you, even if you fix all the problems I\nmentioned before!\n\nAt most, they can tell you something like \u201cyou\u2019re somewhere here.\u201d We know for\nsure that you\u2019re not on the left of this. Maybe we ran an attack and found\nthat this is the success rate of the attack. So it\u2019s at least that bad. We got\na lower bound on risk. But we don\u2019t know how much worse this can get! Maybe a\nbetter attack would have a much better success rate! We don\u2019t know!\n\nIt\u2019s critical to keep this framing in mind when you evaluate privacy claims\nbased on empirical metrics. There are still tons of papers that introduce a\nnew algorithm, compare it to prior work using empirical privacy metrics, and\nconclude something like \u201cour algorithm is better for privacy\u201d. No it\u2019s not!\nYou don\u2019t know that! At best, this specific attack seems to be mitigated more\neffectively in this specific context. There\u2019s a crucial difference!\n\nMitigating specific attacks is useful. By all means, we should do it. But we\ncan\u2019t jump from that to making general claims that we have no evidence for.\n\nOK, so that\u2019s depressing, and we\u2019re not even at the depression stage yet.\nLet\u2019s be a bit hopeful. What should we do?\n\nFirst, we need better metrics with better properties. Metrics should match\nspecific attacks, with clear assumptions about the attacker. They should\nreport the risk to the least well-protected people in the dataset, not the\naverage case. They shouldn\u2019t be too trivial to game, and so on. We\u2019re not yet\nat a stage where we know what these metrics should be. But the current ones\njust aren\u2019t cutting it.\n\nNot only do we need better tools, we need to use them better. We should be\nhonest about what they actually tell us, especially if we use them to\ncommunicate with people about privacy properties. And we should not use them\non their own, but in conjunction with upper bounds on the risk.\n\nWhich naturally leads me to synthetic data with differential privacy. There\u2019s\na lot of research on the topic already, but there\u2019s so much more to be done.\nBetter tools. Better usability. Better utility for various realistic\nworkloads. Better benchmarks. Capturing functional dependencies, time series.\nAnd so on, and so forth. More talking to people who actually need this\ntechnology, listening to their needs, addressing them.\n\nThere\u2019s a lot more I could say about synthetic data, but you probably don\u2019t\nwant me to at that point. Thanks for indulging me. Now, sadly, we have to move\non...\n\n... to Depression.\n\n> OK, Damien. I listened to you talk for 30 minutes, and then I was like:\n> Urgh, OK, fine. Stop talking. I\u2019ll do it the right way. I'll use\n> differential privacy, and do whatever is necessary to make it work. No\n> shortcuts.\n>\n> Except... I\u2019ve been trying for months now.\n\n> And there\u2019s just so much garbage in my data. It turns out, my use case is\n> complex. I do have lots of statistics and weird requirements.\n>\n> I tried all the things that they told me to try in the literature. I used\n> fancy techniques like zero-concentrated DP and dimensionality reduction and\n> adaptive algorithms. I even got some experts to help me. But even with their\n> help, it seems like my problem is impossible to solve. The noise is just too\n> bad.\n>\n> Unless... well, unless I use, like, an epsilon of 30 or anything. But sadly,\n> I understand the explanation of what epsilon means! Kind of. I know that\n> large privacy parameters are basically providing meaningless protection! So\n> why would I even bother, if the best I can do is a number that tells me that\n> the privacy levels are terrible?\n>\n> I should probably just stop trying to do anonymization altogether and go to\n> the mountains. Raise some goats or something. That seems nice.\n\nHonestly... yeah.\n\nFor many problems, the trade-off between privacy and utility can be frankly\nbrutal. That\u2019s just a fact.\n\nAnd if you give me an algorithm and the only thing you tell me is \u201cit\u2019s DP\nwith an epsilon of 20\u201d... it doesn\u2019t spark joy. There\u2019s no magic explanation\nthat I could give you that would make it acceptable.\n\nPlus, it\u2019s not like there\u2019s good guidance about this, either. If you can use a\nlow privacy budget, you can understand what\u2019s going on. You can rely on the\nmath. But when your epsilon is, like, 10... the math interpretation stops\nmaking sense, and what do you have to replace it?\n\nIn an ideal world, there\u2019d be some standard that tells you: in this context,\nunder these conditions, an epsilon of 10 is OK. In this other context, you can\ngo up to 15. Even if it feels a bit iffy from a mathematical standpoint, at\nleast there would be community agreement that this is acceptable. People could\ndefend their choices to their executives, to regulators, and to the people in\ntheir data. But there\u2019s nothing like that right now. So when people have to\nface hard decisions regarding parameter choices, of course that can feel\nhopeless.\n\nUntil we have some form of community agreement or standard on what parameters\nmake sense in which context, we won\u2019t solve that problem entirely. But in the\nmeantime, here are some ways we can look at the current situation under a more\nhopeful lens.\n\nFirst, here\u2019s an idea that has a lot of potential: privacy analyses don\u2019t need\nto be limited to a single number. You can do finer-grained things, like\nlooking at the privacy loss of individual attributes instead of entire\nrecords.\n\nSay your total epsilon is 10. That seems not so great. But maybe each person\nin your dataset has 20 attributes. And maybe you can show that if all you do\nis change a single attribute, then that information is protected with an\nepsilon or 0.5. Or maybe 1, or 2, depending on which attribute. That feels\nbetter: it gives you interpretable guarantees on smaller pieces of\ninformation. Maybe that\u2019s enough to give you some peace of mind.\n\nAnother idea that is largely uncharted territory today is to have the privacy\nloss vary across protected entities, instead of being constant on the entire\ndataset.\n\nThis is particularly useful for economic data that is typically very heavy-\ntailed: one large supermaket might have 1000 employees and millions in monthly\nrevenue, while the mom-and-pop store next door is 500 times smaller.\nProtecting both at the same time can be tricky, or outright impossible, if we\nprotect them in the exact same way. But it might make sense to adopt a privacy\nloss accounting framework where the smaller contributors have a better privacy\nparameter than the large contributors. We\u2019re doing things along these lines\nwith the U.S. Department of Agriculture and the economic branch of the U.S.\nCensus Bureau \u2014 come talk to me if you want to hear more.\n\nFinally, and this might sound funny considering everything I told you so far,\nbut... Even with very large epsilons, differential privacy seems to achieve\nbetter practical protection than other approaches?\n\nThis is particularly flagrant in machine learning use cases. If you want to\ntrain a deep neural network with differential privacy, you typically need very\nlarge epsilons to get decent results. But somehow, this seems to not only be\nenough to mitigate practical attacks \u2014 at least, those we know of today \u2014 but\nit also seems to work much better than alternative approaches that don\u2019t\nprovide any formal guarantees. I don\u2019t think we have a great explanation for\nthis. My guess is that DP forces you to do some things \u2014 like tracking and\nbounding sensitivity, and adding noise \u2014 that are inherently good for privacy?\n\nBut my point is that even if you\u2019re a empiricist through and through and you\ndon\u2019t care about formal guarantees, only about making practical decisions...\nYou may still want to consider using differential privacy! It just seems to\nwork better at mitigating attacks, even when it shouldn\u2019t.\n\nHow can you help?\n\nFirst, what I said about finer-grained privacy analysis \u2014 there\u2019s a lot to do\nhere. It\u2019s still largely uncharted territory.\n\n  * Some of it is systems design: how to make DP frameworks that make it easy to compute per-attribute privacy loss?\n  * Some of it is theory. This idea to vary the privacy loss across protected things: we don\u2019t know what are the best mechanisms to do that in practice, and what are the fundamental limitations of the approach. As always with new ideas, we\u2019ll probably bump into more open problems as we try to use this approach to address real-world use cases.\n\nAlso, it would be nice to understand better why DP is so effective at\nmitigating practical attacks. Maybe it\u2019s because it protects against worst-\ncase attackers, and real attackers are weaker than this? In which case,\nmodeling more realistic attackers seems worthwhile, maybe to gain a deeper\ntheoretical understanding of the advantage they can get. This idea is not new;\nwe actually tried to do exactly that during my PhD, but formalizing it and\nobtaining convincing results seems surprisingly difficult. There\u2019s probably a\nlot more to be done there.\n\nWe\u2019re reaching the end of this talk with the final stage: Acceptance.\n\nI think most people working on differential privacy in academia did not reach\nthis stage the hard way, by going through all the steps that came before, and\npersevering. Instead, a lot of them were attracted to the abstract properties\nof this framework. It felt elegant and robust.\n\n... we just like the math, you know?\n\nBut more and more, as we ship DP to practical use cases, we see practitioners\nget converted. Not because all the nice theory, but because of very practical\nconsiderations. I count four of them, but I\u2019m certainly missing some.\n\nThe first one is making data negotiations smoother. What do I mean by that?\n\nWe work with the IRS \u2014 the tax office in the US \u2014 to help them share data with\nthe Department of Education. The Department of Education wants to publish\nincome information depending on degree: when students go to this college to do\nthis degree, how much do they earn on average after 2 or 5 years? This is used\nby policymakers to better allocate federal resources, and by students to make\ndecisions about their education and the way they finance it.\n\nThe Department of Education wants as much income data as possible, as\nprecisely as possible. The IRS, on the other hand, has a constitutional duty\nto not leak tax information of specific people.\n\nBefore using DP, there would be dozens and dozens of emails and meetings back\nand forth where one side would ask for more statistics and the other would be\nlike \u201ceeeeeh, I don\u2019t know, this seems a bit scary, what if we do something\nelse instead\u201d, and so on.\n\nDifferential privacy aligned the incentives. It allowed the IRS to say: \u201cOK,\nhere is your total budget. We\u2019re OK with giving you this much epsilon. You\ndecide which queries you want to run, at which level of granularity, and which\nqueries are more important to you and require more budget.\u201d That saved them a\nton of time and led to both better data for the Department of Education, and\nbetter privacy outcomes for IRS.\n\nThe second also has to do with saving time and effort, in another context:\nautomated approval flows.\n\nWhen I was at Google, my team was in charge of evaluating and approving\nanonymization strategies for all other teams. People would come to us, and\nask: \u201cwe want to share statistics about user data with a third party, or keep\nsome metrics forever, can you please help us anonymize that data?\u201d\n\nWhen I started in this team, we were using ad hoc methods. Because these\nmethods only make sense depending on context, we had to ask a lot of questions\nto people. Can an attacker influence the data? Are you using windowed\nstatistics? Are you releasing multiple statistics about the same data point?\nWe had to verify a lot of this stuff by hand, and we could never know for sure\nthat a given launch would not have a bad privacy interaction with another\nlaunch from a different team using the same data.\n\nAfter investing in building DP libraries, we were able to tell people: \u201cUse\nthis library with these default parameters. If you\u2019re satisfied with the\nresult, we\u2019ll approve your launch super quickly.\u201d\n\nJust like before, it aligned incentives, putting individual teams in charge of\ntheir own success. You do the work because you have a vested interest in\nmaking it work for you. We save ourselves review time, which means we have\nmore time to improve the tooling, which benefits everyone.\n\nA third point is that DP allows you almost unlimited flexibility about what\nyou can do. It gives you a very clear, well-defined boundary: you have to\ntrack the privacy loss of everything. Within this boundary, you can go wild!\nYou don\u2019t need to second-guess yourself every time you introduce a new aspect\nto your algorithm, or optimize an existing approach. You just need to do the\nmath to quantify how much privacy budget each step costs. That\u2019s really\nconvenient.\n\nFinally, DP makes it super easy to build on other people\u2019s work. It composes\nnicely, so you can use individual ideas as building blocks, swap one out for\nanother. If you\u2019re using 5 different basic mechanisms in your overall strategy\nand one of them gets improved in a new paper, you can start using it and get\nimmediate benefits. And once you start having a good overview of what tools\nare out there, it\u2019s not super hard to start addressing novel use cases \u2014 the\nbasic ideas are very reusable and effective.\n\nIn conclusion.\n\nI do believe there that there are some real, scary issues with the use of\nunsafe anonymization practices. It\u2019s also clear to me that the future lies in\nprovably robust approaches, grounded in math, like differential privacy.\n\nIt\u2019s not just an intellectual preference or emotional investment \u2014 I believe\nthat this is truly the right thing to do from a moral and political\nstandpoint.\n\nBut we can\u2019t expect everyone to move there without solving many major\nproblems, like the ones I tried to talk about today.\n\nI think we can actually do this. And I sure hope that you will help!\n\nThe inspiration from this talk came from a talk by Andrej Bauer, who used this\n\"Five stages\" structure for a lecture about constructive mathematics, later\nconverted into a paper. I heartily recommend both. I hope I did justice to the\nformat.\n\nThanks a ton for listening. Here\u2019s my email address, LinkedIn, and Mastodon if\nyou want to stay in touch, and a link to my blog post series about\ndifferential privacy if you want to learn more about this stuff.\n\nI'm very grateful to Esfandiar Mohammadi for inviting me to speak to this\ncongress, to Gerome Miklau for supporting my preparing and delivering this\ntalk, and to Hari Kumar for his fantastic feedback on this presentation.\n\nThanks as well to Antoine Amarilli for his helpful comments on this\ntranscript.\n\nThe BibTeX entry was copied to your clipboard.\n\nAll opinions here are my own, not my employer's. | Feedback on these posts is very welcome! Please reach out via e-mail (se.niatnofsed@neimad) or Twitter (@TedOnPrivacy) for comments and suggestions. | Interested in deploying formal anonymization methods? My colleagues and I at Tumult Labs can help. Contact me at oi.tlmt@neimad, and let's chat!\n\nby Damien Desfontaines \u2014 \u2014 propulsed by Pelican\n\n", "frontpage": false}
