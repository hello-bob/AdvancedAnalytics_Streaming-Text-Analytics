{"aid": "40165032", "title": "Logging Implicit Human Feedback", "url": "https://foyle.io/docs/blog/logfeedback/", "domain": "foyle.io", "votes": 2, "user": "sebg", "posted_at": "2024-04-26 01:35:20", "comments": 0, "source_title": "Logging Implicit Human Feedback", "source_text": "Logging Implicit Human Feedback | Foyle\n\nView page source Edit this page Create child page Create documentation issue\nCreate project issue Print entire section\n\n# Logging Implicit Human Feedback\n\nBy Jeremy Lewi | Wednesday, April 24, 2024\n\nFoyle is an open source assistant to help software developers deal with the\npain of devops. Developers are expected to operate their software which means\ndealing with the complexity of Cloud. Foyle aims to simplify operations with\nAI. One of Foyle\u2019s central premises is that creating a UX that implicitly\ncaptures human feedback is critical to building AIs that effectively assist us\nwith operations. This post describes how Foyle logs that feedback.\n\n## The Problem\n\nAs software developers, we all ask AIs (ChatGPT, Claude, Bard, Ollama,\netc....) to write commands to perform operations. These AIs often make\nmistakes. This is especially true when the correct answer depends on internal\nknowledge, which the AI doesn\u2019t have.\n\n  * What region, cluster, or namespace is used for dev vs. prod?\n  * What resources is the internal code name \u201ccaribou\u201d referring to?\n  * What logging schema is used by our internal CICD tool?\n\nThe experience today is\n\n  * Ask an assistant for one or more commands\n  * Copy those commands to a terminal\n  * Iterate on those commands until they are correct\n\nWhen it comes to building better AIs, the human feedback provided by the last\nstep is gold. Yet today\u2019s UX doesn\u2019t allow us to capture this feedback easily.\nAt best, this feedback is often collected out of band as part of a data\ncuration step. This is problematic for two reasons. First, it\u2019s more expensive\nbecause it requires paying for labels (in time or money). Second, if we\u2019re\ndealing with complex, bespoke internal systems, it can be hard to find people\nwith the requisite expertise.\n\n## Frontend\n\nIf we want to collect human feedback, we need to create a single unified\nexperience for\n\n  1. Asking the AI for help\n  2. Editing/Executing AI suggested operations\n\nIf users are copying and pasting between two different applications the\nlikelihood of being able to instrument it to collect feedback goes way down.\nFortunately, we already have a well-adopted and familiar pattern for combining\nexposition, commands/code, and rich output. Its notebooks.\n\nFoyle\u2019s frontend is VSCode notebooks. In Foyle, when you ask an AI for\nassistance, the output is rendered as cells in the notebook. The cells contain\nshell commands that can then be used to execute those commands either locally\nor remotely using the notebook controller API, which talks to a Foyle server.\nHere\u2019s a short video illustrating the key interactions.\n\nCrucially, cells are central to how Foyle creates a UX that automatically\ncollects human feedback. When the AI generates a cell, it attaches a UUID to\nthat cell. That UUID links the cell to a trace that captures all the\nprocessing the AI did to generate it (e.g any LLM calls, RAG calls, etc...).\nIn VSCode, we can use cell metadata to track the UUID associated with a cell.\n\nWhen a user executes a cell, the frontend sends the contents of the cell along\nwith its UUID to the Foyle server. The UUID then links the cell to a trace of\nits execution. The cell\u2019s UUID can be used to join the trace of how the AI\ngenerated the cell with a trace of what the user actually executed. By\ncomparing the two we can easily see if the user made any corrections to what\nthe AI suggested.\n\n## Traces\n\nCapturing traces of the AI and execution are essential to logging human\nfeedback. Foyle is designed to run on your infrastructure (whether locally or\nin your Cloud). Therefore, it\u2019s critical that Foyle not be too opinionated\nabout how traces are logged. Fortunately, this is a well-solved problem. The\nstandard pattern is:\n\n  1. Instrument the app using structured logs\n  2. App emits logs to stdout and stderr\n  3. When deploying the app collect stdout and stderr and ship them to whatever backend you want to use (e.g. Google Cloud Logging, Datadog, Splunk etc...)\n\nWhen running locally, setting up an agent to collect logs can be annoying, so\nFoyle has the built-in ability to log to files. We are currently evaluating\nthe need to add direct support for other backends like Cloud Logging. This\nshould only matter when running locally because if you\u2019re deploying on Cloud\nchances are your infrastructure is already instrumented to collect stdout and\nstderr and ship them to your backend of choice.\n\n## Don\u2019t reinvent logging\n\nUsing existing logging libraries that support structured logging seems so\nobvious to me that it hardly seems worth mentioning. Except, within the AI\nEngineering/LLMOps community, it\u2019s not clear to me that people are reusing\nexisting libraries and patterns. Notably, I\u2019m seeing a new class of\nobservability solutions that require you to instrument your code with their\nSDK. I think this is undesirable as it violates the separation of concerns\nbetween how an application is instrumented and how that telemetry is stored,\nprocessed, and rendered. My current opinion is that Agent/LLM observability\ncan often be achieved by reusing existing logging patterns. So, in defense of\nthat view, here\u2019s the solution I\u2019ve opted for.\n\nStructured logging means that each log line is a JSON record which can contain\narbitrary fields. To Capture LLM or RAG requests and responses, I log them;\ne.g.\n\n    \n    \n    request := openai.ChatCompletionRequest{ Model: a.config.GetModel(), Messages: messages, MaxTokens: 2000, Temperature: temperature, } log.Info(\"OpenAI:CreateChatCompletion\", \"request\", request)\n\nThis ends up logging the request in JSON format. Here\u2019s an example\n\n    \n    \n    { \"severity\": \"info\", \"time\": 1713818994.8880482, \"caller\": \"agent/agent.go:132\", \"function\": \"github.com/jlewi/foyle/app/pkg/agent.(*Agent).completeWithRetries\", \"message\": \"OpenAI:CreateChatCompletion response\", \"traceId\": \"36eb348d00d373e40552600565fccd03\", \"resp\": { \"id\": \"chatcmpl-9GutlxUSClFaksqjtOg0StpGe9mqu\", \"object\": \"chat.completion\", \"created\": 1713818993, \"model\": \"gpt-3.5-turbo-0125\", \"choices\": [ { \"index\": 0, \"message\": { \"role\": \"assistant\", \"content\": \"To list all the images in Artifact Registry using `gcloud`, you can use the following command:\\n\\n```bash\\ngcloud artifacts repositories list --location=LOCATION\\n```\\n\\nReplace `LOCATION` with the location of your Artifact Registry. For example, if your Artifact Registry is in the `us-central1` location, you would run:\\n\\n```bash\\ngcloud artifacts repositories list --location=us-central1\\n```\" }, \"finish_reason\": \"stop\" } ], \"usage\": { \"prompt_tokens\": 329, \"completion_tokens\": 84, \"total_tokens\": 413 }, \"system_fingerprint\": \"fp_c2295e73ad\" } }\n\nA single request can generate multiple log entries. To group all the log\nentries related to a particular request, I attach a trace id to each log\nmessage.\n\n    \n    \n    func (a *Agent) Generate(ctx context.Context, req *v1alpha1.GenerateRequest) (*v1alpha1.GenerateResponse, error) { span := trace.SpanFromContext(ctx) log := logs.FromContext(ctx) log = log.WithValues(\"traceId\", span.SpanContext().TraceID())\n\nSince I\u2019ve instrumented Foyle with open telemetry(OTEL), each request is\nautomatically assigned a trace id. I attach that trace id to all the log\nentries associated with that request. Using the trace id assigned by OTEL\nmeans I can link the logs with the open telemetry trace data.\n\nOTEL is an open standard for distributed tracing. I find OTEL great for\ninstrumenting my code to understand how long different parts of my code took,\nhow often errors occur and how many requests I\u2019m getting. You can use OTEL for\nLLM Observability; here\u2019s an example. However, I chose logs because as noted\nin the next section they are easier to mine.\n\n### Aside: Structured Logging In Python\n\nPython\u2019s logging module supports structured logging. In Python you can use the\nextra argument to pass an arbitrary dictionary of values. In python the\nequivalent would be:\n\n    \n    \n    logger.info(\"OpenAI:CreateChatCompletion\", extra={'request': request, \"traceId\": traceId})\n\nYou then configure the logging module to use the python-json-logger formatter\nto emit logs as JSON. Here\u2019s the logging.conf I use for Python.\n\n### Logs Need To Be Mined\n\nPost-processing your logs is often critical to unlocking the most valuable\ninsights. In the context of Foyle, I want a record for each cell that captures\nhow it was generated and any subsequent executions of that cell. To produce\nthis, I need to write a simple ETL pipeline that does the following:\n\n  * Build a trace by grouping log entries by trace ID\n  * Reykey each trace by the cell id the trace is associated with\n  * Group traces by cell id\n\nThis logic is highly specific to Foyle. No observability tool will support it\nout of box.\n\nConsequently, a key consideration for my observability backend is how easily\nit can be wired up to my preferred ETL tool. Logs processing is such a common\nuse case that most existing logging providers likely have good support for\nexporting your logs. With Google Cloud Logging for example it\u2019s easy to setup\nlog sinks to route logs to GCS, BigQuery or PubSub for additional processing.\n\n### Visualization\n\nThe final piece is being able to easily visualize the traces to inspect what\u2019s\ngoing on. Arguably, this is where you might expect LLM/AI focused tools might\nshine. Unfortunately, as the previous section illustrates, the primary way I\nwant to view Foyle\u2019s data is to look at the processing associated with a\nparticular cell. This requires post-processing the raw logs. As a result, out\nof box visualizations won\u2019t let me view the data in the most meaningful way.\n\nTo solve the visualization problem, I\u2019ve built a lightweight progressive web\napp(PWA) in Go (code) using maxence-charriere/go-app. While I won\u2019t be winning\nany design awards, it allows me to get the job done quickly and reuse existing\nlibraries. For example, to render markdown as HTML I could reuse the Go\nlibraries I was already using (yuin/goldmark). More importantly, I don\u2019t have\nto wrestle with a stack(typescript, REACT, etc...) that I\u2019m not proficient in.\nWith Google Logs Analytics, I can query the logs using SQL. This makes it very\neasy to join and process a trace in the web app. This makes it possible to\nview traces in real-time without having to build and deploy a streaming\npipeline.\n\n## Try Foyle\n\nPlease consider following the getting started guide to try out an early\nversion of Foyle and share your thoughts by email(jeremy@lewi.us) on\nGitHub(jlewi/foyle) or on twitter (@jeremylewi)!\n\n## About Me\n\nI\u2019m a Machine Learning platform engineer with over 15 years of experience. I\ncreate platforms that facilitate the rapid deployment of AI into production. I\nworked on Google\u2019s Vertex AI where I created Kubeflow, one of the most popular\nOSS frameworks for ML.\n\nI\u2019m open to new consulting work and other forms of advisory. If you need help\nwith your project, send me a brief email at jeremy@lewi.us.\n\n## Acknowledgements\n\nI really appreciate Hamel Husain and Joseph Gleasure reviewing and editing\nthis post.\n\n  * \u2190Previous\n  * Next\u2192\n\nLast modified April 25, 2024: Fix link. (#80) (8fa0e0f)\n\n\u00a9 2024\u20132024 Jeremy Lewi | CC BY 4.0 |All Rights ReservedPrivacy Policy\n\n", "frontpage": false}
