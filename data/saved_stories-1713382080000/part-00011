{"aid": "40063522", "title": "Show HN: Privacy-first analytics in natural language in the browser", "url": "https://tobilg.com/chat-with-a-duck", "domain": "tobilg.com", "votes": 1, "user": "tobilg", "posted_at": "2024-04-17 12:14:48", "comments": 0, "source_title": "Chat with a Duck", "source_text": "Chat with a Duck\n\n#\n\ntobilg.com\n\n# tobilg.com\n\n# Chat with a Duck\n\n## Browser-first analytics in natural language with DuckDB and Ollama\n\nTobias M\u00fcller\n\n\u00b7Apr 16, 2024\u00b7\n\n5 min read\n\nA while ago I published sql-workbench.com and the accompanying blog post\ncalled \"Using DuckDB-WASM for in-browser Data Engineering\". The SQL Workbench\nenables its users to analyze local or remote data directly in the browser.\n\nThis lowers the bar regarding the infrastructure needed to get started with\nData Analysis and Data Engineering. No databases must be installed on servers\nor developer machines, no data is sent to the internet and SaaS or Cloud\nproviders. The sole interface to the data you want to analyze is SQL.\n\nBut what if a user is new in this space, and has no SQL skills yet? Or, if\nhe/she has SQL skills, but wants to query the data without having to\nunderstand the data model first?\n\n## Enter DuckDB-NSQL\n\nLLMs (Large Language Models) can be used to generate code from natural\nlanguage questions. Popular examples include GitHub Copilot, OpenAI's GPT-4 or\nMeta's Code Llama.\n\nDuckDB-NSQL is a Text-to-SQL model created by NumberStation for MotherDuck.\nIt's hosted on HuggingFace and on Ollama, and can be use with different LLM\nruntimes. There are also some nice blog posts that are worth a read:\n\n  * How to quack in SQL\n\n  * AI that quacks\n\nThe model was specifically trained for the DuckDB SQL syntax with 200k DuckDB\nText-to-SQL pairs, and is based on the Llama-2 7B model.\n\n## Bring your own AI\n\nIf you want to enable SQL Workbench's privacy first AI integration for SQL\ngeneration, you first have to install Ollama on your local machine.\n\nOnce you installed Ollama, you can either download the relevant DuckDB-NSQL\nmodel beforehand, or have it automatically downloaded on the first usage.\n\nIf you want to pull the model yourself, you can do this in your terminal\n(after you installed Ollama) by issuing the following command:\n\n    \n    \n    ollama pull duckdb-nsql:7b\n\nPlease be aware that the default model has a size of 3.8GB, which can take a\nwhile to download, depending on your internet connection speed. There are\nsmaller quantized models as well, but be aware that the answer quality might\nbe lower with them.\n\nOnce the model is downloaded, Ollama can be started from your terminal:\n\n    \n    \n    OLLAMA_ORIGINS=\"https://sql-workbench.com\" ollama serve\n\nSetting the OLLAMA_ORIGINS environment variable to https://sql-workbench.com\nis necessary to enable CORS from the SQL Workbench running in your browser for\nyour locally running Ollama server.\n\nYou can enable the AI feature in SQL Workbench:\n\nThen, you can ask your questions after a specific comment string like below:\n\n    \n    \n    --ai your natural language question\n\nTo execute the prompt, you have two options:\n\n  * Press ALT + g to generate the SQL\n\n  * Press ALT + r to run the generated SQL directly\n\nSo if you created a table named \"locations\" beforehand, that has a column\nnamed \"country\", the following would generate an appropriate SQL:\n\n    \n    \n    --ai distinct country from locations SELECT distinct country FROM locations\n\nThe generated SQL is automatically inserted below the closest prompt comment\nstring. In case you have multiple comment strings in the current SQL Workbench\ntab, the one closest to the actual cursor position is used.\n\nYou can also ask questions regarding about remote files (Parquet, CSV), but\ndon't expect the answer quality to be very good, because the schema will be\nunknown to the model.\n\n## Explore AWS IAM data with the help of AI\n\nAWS publishes its Service Authorization Reference documentation, and there's a\nGithub repository that transforms the published data automatically to Parquet,\nCSV and JSON data formats every night at 4AM UTC.\n\nEach Parquet file represents a table in a relational data model that looks\nlike this:\n\nWith the help of SQL Workbench and its underlying DuckDB-WASM instance, it's\npossible to load the remote data from GitHub into our browser's memory.\n\nTo do this, the following SQL statements need to be executed, either by copy &\npasting the code, or automatically via the SQL Workbench's shareable query\nfeature by clicking on the duck below (this will open in a new browser\nwindow/tab, redirected via dub.co):\n\nThe create schema will then be used as input for the prompt which is sent to\nOllama in the background by the browser.\n\n### SQL script\n\n    \n    \n    CREATE TABLE services ( service_id INTEGER PRIMARY KEY, \"name\" VARCHAR, prefix VARCHAR, reference_url VARCHAR ); CREATE TABLE actions ( action_id INTEGER PRIMARY KEY, service_id INTEGER, \"name\" VARCHAR, reference_url VARCHAR, permission_only_flag BOOLEAN, access_level VARCHAR, FOREIGN KEY (service_id) REFERENCES services (service_id) ); CREATE TABLE condition_keys ( condition_key_id INTEGER PRIMARY KEY, \"name\" VARCHAR, reference_url VARCHAR, description VARCHAR, \"type\" VARCHAR ); CREATE TABLE resource_types ( resource_type_id INTEGER PRIMARY KEY, service_id INTEGER, \"name\" VARCHAR, reference_url VARCHAR, arn_pattern VARCHAR, FOREIGN KEY (service_id) REFERENCES services (service_id) ); CREATE TABLE resource_types_condition_keys ( resource_type_condition_key_id INTEGER PRIMARY KEY, resource_type_id INTEGER, condition_key_id INTEGER, FOREIGN KEY (resource_type_id) REFERENCES resource_types (resource_type_id), FOREIGN KEY (condition_key_id) REFERENCES condition_keys (condition_key_id) ); CREATE TABLE actions_resource_types ( action_resource_type_id BIGINT PRIMARY KEY, action_id INTEGER, resource_type_id INTEGER, required_flag BOOLEAN, FOREIGN KEY (action_id) REFERENCES actions (action_id) ); CREATE TABLE actions_condition_keys ( action_condition_key_id BIGINT PRIMARY KEY, action_resource_type_id BIGINT, action_id INTEGER, condition_key_id INTEGER, FOREIGN KEY (action_id) REFERENCES actions (action_id), FOREIGN KEY (condition_key_id) REFERENCES condition_keys (condition_key_id) ); CREATE TABLE actions_dependant_actions ( action_dependent_action_id INTEGER PRIMARY KEY, action_resource_type_id BIGINT, action_id INTEGER, dependent_action_id INTEGER, FOREIGN KEY (action_id) REFERENCES actions (action_id), FOREIGN KEY (action_resource_type_id) REFERENCES actions_resource_types (action_resource_type_id) ); INSERT INTO services SELECT * FROM 'https://raw.githubusercontent.com/tobilg/aws-iam-data/main/data/parquet/aws_services.parquet'; INSERT INTO resource_types SELECT * FROM 'https://raw.githubusercontent.com/tobilg/aws-iam-data/main/data/parquet/aws_resource_types.parquet'; INSERT INTO condition_keys SELECT * FROM 'https://raw.githubusercontent.com/tobilg/aws-iam-data/main/data/parquet/aws_condition_keys.parquet'; INSERT INTO actions SELECT * FROM 'https://raw.githubusercontent.com/tobilg/aws-iam-data/main/data/parquet/aws_actions.parquet'; INSERT INTO resource_types_condition_keys SELECT * FROM 'https://raw.githubusercontent.com/tobilg/aws-iam-data/main/data/parquet/aws_resource_types_condition_keys.parquet'; INSERT INTO actions_resource_types SELECT * FROM 'https://raw.githubusercontent.com/tobilg/aws-iam-data/main/data/parquet/aws_actions_resource_types.parquet'; INSERT INTO actions_condition_keys SELECT * FROM 'https://raw.githubusercontent.com/tobilg/aws-iam-data/main/data/parquet/aws_actions_condition_keys.parquet'; INSERT INTO actions_dependant_actions SELECT * FROM 'https://raw.githubusercontent.com/tobilg/aws-iam-data/main/data/parquet/aws_actions_dependant_actions.parquet';\n\n### Example prompts & results\n\nPrompt\n\n    \n    \n    --ai distinct service names that contain 'S3'\n\nResult\n\nPrompt\n\n    \n    \n    --ai count distinct action names for service name contains S3\n\nResult\n\nPrompt\n\n    \n    \n    --ai show all 'Write' access level action for service 'Amazon S3'\n\nResult\n\nPrompt\n\n    \n    \n    --ai first 10 actions for service 'Amazon CloudFront'\n\nResult\n\nPrompt\n\n    \n    \n    --ai service name with least actions\n\nResult\n\nPrompt\n\n    \n    \n    --ai service name with most resource types\n\nResult\n\nPrompt\n\n    \n    \n    --ai count actions names of service 'Amazon S3'\n\nResult\n\nPrompt\n\n    \n    \n    --ai top 10 resource types by services\n\nResult\n\n## Demo video\n\nhttps://youtu.be/rTuCec_fhlk\n\n## Summary\n\nUsing a locally hosted LLM together with an in-browser SQL Workspace enables a\ncost-effective and privacy-friendly way to use state of the art tools without\nneeding to rely on third party services.\n\n## Subscribe to my newsletter\n\nRead articles from tobilg.com directly inside your inbox. Subscribe to the\nnewsletter, and don't miss out.\n\nduckDBllmAISQL\n\n### Written by\n\n# Tobias M\u00fcller\n\nShare this\n\n### More articles\n\nTobias M\u00fcller\n\n# Using DuckDB-WASM for in-browser Data Engineering\n\nIntroduction DuckDB, the in-process DBMS specialized in OLAP workloads, had a\nvery rapid growth duri...\n\nTobias M\u00fcller\n\n# Retrieving Lambda@Edge CloudWatch Logs\n\nWhat is Lambda@Edge AWS Lambda@Edge is an extension of the traditional AWS\nLambda service, but with ...\n\nTobias M\u00fcller\n\n# List of free AWS Knowledge Badges\n\nAs the Skillbuilder website is sometimes a bit hard to navigate, here's the\nfull list of free badges...\n\n\u00a92024 tobilg.com\n\nArchive\u00b7Privacy policy\u00b7Terms\n\n", "frontpage": false}
