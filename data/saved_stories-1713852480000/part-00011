{"aid": "40120767", "title": "From Ground Zero to Production: Go's Journey at Google", "url": "https://i-admin.cetico.org/posts/early-days-golang-google/", "domain": "cetico.org", "votes": 1, "user": "vrosas", "posted_at": "2024-04-22 21:05:13", "comments": 0, "source_title": "From Ground Zero to Production: Go's Journey at Google", "source_text": "From Ground Zero to Production: Go's Journey at Google - Yves Junqueira's blog\n\nYves Junqueira's blog\n\n# From Ground Zero to Production: Go's Journey at Google\n\nPosted on Mar 8, 2024\n\nRecently, Jeremy Mason and Sameer Ajmani wrote about the saga to make Go one\nof Google\u2019s internal languages. Go is currently the 8th most popular\nprogramming language in the world and it is still growing, so people are\ninterested to learn about the early days and how we got here.\n\nI thought I would write about the perspective of an SRE, framework developer\nand early adopter. All information I\u2019m sharing is related to systems that\nGoogle already documented publicly, so I don\u2019t think I\u2019m revealing any\nsecrets. There are some important parts of this story (e.g: envelope) that I\nhaven\u2019t seen mentioned elsewhere so I won\u2019t discuss them.\n\n## Breaking the Ice: My Introduction to Go Programming at Google\n\nI started looking at Go before it was released publicly, and when it launched,\nI became an instant fan and an early user inside Google. I loved its\nsimplicity.\n\nI worked a bit on core libraries and was active in the community, helping\nusers in the go-nuts mailing list early on and writing open-source libraries.\nLater on, I helped lead the Go Meetup in Seattle and co-organized a beloved\nconference, Go Northwest.\n\nTo the best of my knowledge, I wrote the first production-critical tool and,\nlater on, the first user-facing service in Go at Google.\n\nThe first one was a service used to monitor the health of Google+\u2019s Bigtable\nservers. That was one of my jobs as a SRE. Bigtable had detailed internal\nstats about the performance of each tablet, but sometimes we needed to\nunderstand why a certain tablet was so overloaded and what was happening\nelsewhere in the system, so we could understand the root causes. We needed to\ncollect that data over time and analyse it. So I built a crawler that would\ninspect many thousands of servers and present detailed stats in a global\ndashboard.\n\nIn 2011, Andrew Gerrand gave an interview to The Register where he mentioned\nthis work. He confirmed to me at the time that this referred to my project. I\nwas thrilled! He said this in the interview:\n\n> \u201cGoogle has people who administer apps and services, and they need to, say,\n> write tools that say scrape a few thousand machines statuses and aggregate\n> the data,\u201d he says. \u201cPreviously, these operations people would write these\n> in Python, but they\u2019re finding that Go is much faster in terms of\n> performance and time to actually write the code.\u201d\n\nGo was indeed faster to run and faster to write. Most importantly, it felt fun\nto use. It made me more productive, so I got hooked!\n\n## Low-level libraries: node authentication and RPCs\n\nWhen Go started, it couldn\u2019t talk to Google\u2019s internal infrastructure.\n\nFirst, the team had to build a protocol-buffers-based stubby RPC system. This\nrequired implementing LOAS to encrypt and authenticate communication with\nremote nodes, and Chubby for name resolution (similar to etcd, used in\nkubernetes).\n\nStubby and Chubby are notoriously complex. Stubby requires a complicated state\nmachine for managing connections but most of the heavy lifting is done by\nChubby, which needs to provide a consistent view of the world even when Borg\nnodes run out of CPU, or are temporarily disconnected from the network because\nsomeone is running a MapReduce and eating all your rack\u2019s switch bandwidth.\nIt\u2019s easy to end-up with deadlocks or reliability issues.\n\nDue to Hyrum\u2019s law, which states that all observable behaviors of your system\nwill be depended on by somebody, the team had to make sure to match the exact\nbehaviour expected by the existing production network and watch out for corner\ncases. For example, healthchecking is notoriously easy to get wrong, and\nshould not be too strict otherwise they leave the door open for cascading\nfailures when a part of the network is temporarily overloaded or disconnected\nfrom the other part. Other tricky distributed system features that had to be\nimplemented, such as backend subsetting and load-balancing. We needed to\ndiagnose when things went wrong, so logging and metrics libraries were added\nearly on.\n\nIn order to find which host:port to talk to, services used Chubby for name\nresolution. It worked as a fully-consistent storage system for small amounts\nof data, and its most used feature was to resolve BNS addresses - similar to\nwhat you would see today with etcd in kubernetes.\n\nSystems sent and received data to and from other services using the Stubby\nprotocol. In Stubby (like in gRPC), messages were encoded using the protocol-\nbuffers wire format. Creating protocol-buffers payload at runtime using\nreflection would be too slow and resource intensive. Engineers would also miss\nthe lack of feedback from a strong-typed system. For those reasons, Google\nused generated code libraries for all languages. Luckily, protocol buffers are\nlanguage-agnostic. The team just had to write Blaze extensions to the existing\nbuild system logic and, voila, we had high-quality client library code for all\ninternal RPC services.\n\nCuriously, there is a small incremental build time cost to generate code for\nanother language, and Google had many, many thousands of RPC services.\nTherefore, it was decided that owners of each RPC service had to opt-in to\nallow the build system to generate Go code for their particular service.\nAlthough a little bit bureacratic, over time we saw thousands of CLs (google\u2019s\nequivalent Pull Requests) flying around to add Go to the set of generated code\nfor each service. This served as a silly but fun measure of progress for our\ncommunity since we could count the number of instances of the \u201cenable Go\u201d\nflags in the code base.\n\n## Influencing the Global Master Selection and Executing Bigtable Drainage\n\nAs an early adopter of those early libraries and an engineer focused in\nproduction systems, I was able to learn how internal systems worked. I helped\ndebug and fix many weird problems. Over time, I acquired the confidence to\nbuild systems to automate operational SRE work. Noticing that most of the\nuser-facing outages on our services happened in the storage layer (Bigtable or\nColossus), I had the idea to create a control system that would monitor the\nhealth of Bigtable partitions and carefully drain them in GSLB when it\ndetected problems. At the time, when an outage occurred, an SRE would get\npaged and, after confirming it was a storage issue, they would simply drain\nthe cluster and go back to sleep.\n\nI wanted to replace this manual whackamole with a proper control system.\nDraining traffic could lead to cascading failures so it was a dangerous\noperation. At the time, most SREs did not want to take this kind of risk with\nan automated system. Luckily, I had a good team. They carefully reviewed my\nproposal, provided lots of feedback about potential failure modes, and we\neventually came up with a design that we were confident enough with. We needed\nto carefully aggregate information from different monitor systems (which could\nfail or provide incorrect data), use the global load balancer to safely\ntraffic away from a cluster, then finally open a ticket in Buganizer for the\noncall SRE to clean-up during work hours.\n\nThe system needed multiple replicas to be always on to react to an outage, but\nit was critical that a single replica remained live at a time. To support\nthat, I wrote a global \u201cmaster election\u201d library for Go that would ensure a\nsingle replica of the system would be active at a time. It used the global\nchubby lock service to provide a high-level library to tell the application to\nstart operating or shut itself down if it can\u2019t prove that we hold the \u201cglobal\nlock\u201d.\n\nTo support this work, I also wrote minor utilities here and there, and worked\nwith the Go team to fix bugs. I reported issues I\u2019d find, and they fixed them.\n\nAt the time, the Go\u2019s team focus was on external users. All their attention\nwas on releasing Go 1.0. It was a small team with few resources, but their\n\u201csecret sauce\u201d is that they were exceptional engineers and the team was very\nproductive. Somehow, they supported internal users very well even though their\ntime was so limited. The internal mailing list was very active with googlers\nmostly playing around with Go for side-projects, but the Go team adopted very\nrobust internal processes to make things run smoothly. They reviewed\neveryone\u2019s code carefully and helped build a strong internal code quality\nculture. Whenever they released a new candidate version of Go, they would\nrebuild all internal projects with the new version and re-run our tests to\nmake sure things were OK. They always did things the right way.\n\n## Initial Insights from JID Proxy Deployment in Production\n\nA few months later I wrote the first user-facing service in Go at Google. By\nuser-facing I mean that if it stopped working many user-facing products would\nstop working. It was a simple RPC service, but it was used by all Google\nmessaging services.\n\nThis service converted data to and from JID format based on internal user ids\nobtained from another RPC service. The service was simple but it was massive,\ndoing hundreds of thousands of requests per second at the time. It was\ncritical to the core of Google\u2019s messaging services that powered Android,\nHangouts and other products.\n\nThis migration was a very important test bed for Go at Google. Critically, it\ngave us an incredible base to compare the performance of Go vis-a-vis our\nother production languages - specifically Java. This service was replacing a\nJava-based one that was difficult to maintain (not because of Java but for\nother reasons) so we ran both of them at the same time with real production\ntraffic and compared their performance closely.\n\nWe learned important lessons from that first large-scale experiment: Go used\nmore CPU cores than Java to serve the same traffic, but the garbage collection\n(GC) pauses were extremely short. As an SRE that worked hard to reduce GC\npauses to improve tail latency in user-facing services, that was very\npromising to see. The Go team was happy with that result but they weren\u2019t\nsurprised: Go was just doing what it was designed to do!\n\nIn fact, years later when the SRE leadership officially reviewed Go\u2019s\nreadiness for production and asked the Go team to ensure Go had good GC\nperformance, I think it was largely pro-forma. Go had proved early on that Go\nhad exceptional GC performance, and it kept getting better over the years.\n\n## Encountering Absent In-house Libraries\n\nIn those early days, before flywheel, before the dl.google.com service, before\nVitess, Go was ignored by most of the engineers at Google. If someone wanted\nto ship a product to users, they would first have to write the basic building\nblocks that let them connect to other services at Google. That was a non-\nstarter for most.\n\nThe lower-level libraries for the lock service (chubby) and the RPC system\n(stubby) popped up relatively quickly (again, the Go team was extremely good),\nthe most important libraries at Google were the interfaces with our storage\nsystems: Bigtable, Megastore, Spanner, Colossus. If you wanted to read or\nwrite data, you basically couldn\u2019t use Go yet. But, slowly, the Go team,\nsometimes in partnership with core infrastructure teams, started to tackle\nthis challenge.\n\nOne by one, they eventually created libraries for Bigtable, Colossus and even\nSpanner (not Megastore, since it was largely a library that was replaced by\nSpanner). That was a major achievement.\n\nUsage at Google was still limited but our community was growing. I gave the\nfirst official Introduction to Go Programming class at Google and helped\npeople in Zurich find interesting projects to work in Go. Around this time I\nfinally got \u201creadability\u201d in Go, and later joined the Go readability team.\n\n## The need for Site Reliability Engineers to guide application functionality\n\nThe other thing missing in Go were production-related features that we learned\nover the years were necessary for production teams. That is, if you want to\nrun large-scale systems without constantly being in ops mode, fighting fires.\n\nWhenever an outage occurs and we diagnose the root causes, over time we learn\nthe weaknesses in the system that should be improved. The goal is to reduce\noutages and operational overhead. Often times, to make the system more\nreliable, we have to make changes to the application runtime. It\u2019s hard\nappreciate the depth of details we need to observe and control a system to\ntruly make it reliable.\n\nFor example, we need to make sure that, in addition to logging incoming\nrequests, applications should also log details about outgoing requests that\nwere involved in that operation. This way, we can pinpoint with certainty\nthat, say, our \u201cCallBob\u201d service became slow at 11:34am because of increased\nlatency to the \u201cFindAddress\u201d calls. When we operate large-scale systems, we\ncan\u2019t be satisfied with guess work and weak correlations. There are too many\nred herrings and \u201coptimistic\u201d root cause assignment. We need to higher\ncertainty about the causes: we want to see that the specific requests that\nfailed did experience high-latency, and exclude other explanations (i.e:\nincoming requests that did not trigger a slow FindAddress call shouldn\u2019t be\nfailing).\n\nSimilarly, over the years we noticed that a large chunk of SRE time was spent\ncoordinating between teams to determine the exact number of connections and\nrequests per second that one service should send to another, and how those\nconnections should be established exactly. For example, if several services\nwant to connect to a backend, we want to be smart about which exact nodes are\nconnecting to which other nodes. This is called backend subsetting. It needs\nto be carefully adjusted considering the overall system health and not just of\none node or a node pair, but of the entire network. Excessively large subsets\ncause too much resource usage, and excessively small subsets can lead to load\nimbalances. For this reason, over time, SRE teams started to help maintain the\nclient libraries used to talk to their service, so they could instrument what\nwas happening and retain some control over how other nodes talked to their\nsystems.\n\n## Unveiling the Magic: a server toolkit for Go\n\nThe model where SRE co-owned the client libraries worked very well in pratice,\nand over time we learned that it was a great idea to also add traffic and load\nmanagement to these libraries.\n\n  * What do you do to incoming RPCs when your system is starting to overload?\n  * Should you hold those requests in a queue, or immediately reject them?\n  * What metrics should you use to determine that your system is overloaded?\n  * How can you avoid entering a cascading failure when too many parts of the system think they are overloaded?\n\nAlejo Forero Cuervo wrote about the lessons learned in the SRE book chapter\nHandling Overload, it\u2019s worth reading. One by one, we added careful logic to\nthe libraries to automatically set these parameters based on experience and\ninternal sensors.\n\nIn The Evolving SRE Engagement Model, my former colleague Ashish Bhambhani and\nmy former boss Acacio Cruz explained that we eventually evolved the SRE\nengagement model to include work and adoption of Server Frameworks. This model\nallowed SREs to directly influence the behaviour of systems in nuanced areas\nthat benefitted from our extensive field experience.\n\nMy SRE team and I wanted to bring these features to Go, but they were too\nexotic and specialized for the Go team to handle. I set up a 20% project\n(which later became a full-time project) and recruited a bunch of experienced\nengineers that wanted to contribute. I flew to New York and met with a very\nawesome Go team member and we worked together to build a roadmap for a \u201cserver\nframework\u201d in Go.\n\nThe Go team was reluctant at first with our approach. The whole \u201cframework\u201d\nconcept was a little bit of a red flag for them. This could have become a\nreligous war of sorts, but the Go team took the time to explain in detail why\nthey were concerned. Sameer specifically has an uncanny ability to reflect and\nexplain why he thinks something works better one way vs another, in technical\nterms.\n\nSameer felt strongly that Go should not have inconsistent developer\nexperiences, internal vs external, with or without a \u201cframework\u201d. It would be\na disservice to the internal Go community if there were different ways to\nbuild Go applications at Google. In agreement with his concern, our ragtag\nteam of 20%-ers went through great lengths to ensure our \u201cframework\u201d felt more\nlike just another library, not a framework, and that it would not introduce a\ndifferent programming model for Go. The goal was to introduce our reliability\nfeatures with a simple library import. If you wrapped your Go HTTP or Stubby\nserver with our libraries, everything would look the same in code, but you\nmagically got logging, instrumentation, load-shedding, traffic management and\neven per-request experimentation support, out of the box.\n\nIn order to create this magic library that made services better, we had to\nmake significant changes to Google\u2019s internal RPC libraries and even to the\nbuild system - to enable our framework team to create arbitrary \u201cextensions\u201d\nto the RPC systems that would operate seamlessly without significant\nperformance overhead when receiving and sending requests.\n\nThe result was worth it. It worked really well. Our project made services\nsignificantly easier to manage, without imposing a programming style that was\ndifferent from what the Go team wanted. We called it a server \u201ctoolkit\u201d to\navoid confusion, and it the became the Right Way to build production-ready\nsystems at Google. People often cite our internal server framework in their\nLinkedIn profile :). It was called Goa, not to be confused with the unrelated\nexternal Goa framework. Here's an example from someone's LinkedIn profile:\n\nWith its production-readiness features, our Go toolkit removed a major\nroadblock for Go\u2019s internal growth. Engineers could now be confident that\ntheir Go projects would perform as well as, and be as debuggable as, their\nolder Java and C++ siblings. That said, growth didn\u2019t quite happen yet. Go\nneeded a killer use-case to become popular at Google.\n\n### Go's Adoption Across Several SRE Teams\n\nAt the time, the SRE team I was part of was a special place at Google, the\nSocial SRE team. We had great engineers and exceptional management in both SWE\nand SRE. So we were able to do things the right way. Some SRE teams were\nchasing their tails fighting fires, but we had the luxury of engineering\nthings properly. That created a virtuous cycle where we continuously fixed\nproblems before they became big, which means we had time to optimize\noperations further, and so on.\n\nAs a result, our SRE team wrote a lot of useful code. Like my fellow senior\nengineers, I helped people find things to do, so I helped kickstart many early\nproduction-related tools in Go. One of those tools would automatically, and\nsafely, remove traffic away from an entire Bigtable cluster if it noticed\nsomething wrong.\n\nThere were also other projects, in Java and C++, related to traffic and load\nmanagement, led by other senior engineers. This innovative environment\nattracted talent and we continued to deliver good results, so our SRE team\ngrew.\n\nOur engineering director Acacio Cruz (responsible for many of the positive\nthings happening with our team, along with his peers in Mountain View) was\nvery attuned to engineering efficiency: are we using our engineering time for\nthe most impactful things? He understood that there is efficiency in\nstandardization, and he saw that our engineers were happy and productive. He\nhad the idea of pushing for Go to become the tool of choice for any automation\nwithin our team. The proposal was to avoid Python and use Go to write\nproduction tooling. To my surprise, none of my teammates objected. That\naccelerated the usage of Go within our social SRE team, and soon folks outside\nof our area took notice.\n\nThe core libraries, server framework, the successful production tools and the\nsocial SRE standardization around Go - they all contributed to a changing\nperception that Go was becoming a serious language at Google.\n\nAt the same time, SREs had seen a couple of generations of tools written in\nPython that worked extremely well but became very difficult to maintain over\ntime. Google SREs enjoyed Python, we wrote a ton of Python code.\nUnfortunately, at the time, the lack of types and compile-time syntax error\nchecks caused many hard-to-fix issues:\n\n  1. When you work on a project that someone else started, that project may or may not have good test coverage. It\u2019s difficult to add tests for code you didn\u2019t write. You don\u2019t really know what is being used and how. So you end up testing too many things or testing too little. In production-critical tools we can\u2019t take risks when making changes.\n  2. At the time, people generally wrote code in one moment and ran the tests in another moment. If you only realize you have syntax errors when you run tests, maybe you already context switched to doing something else, so now you have to go back and fix it. That wastes time and adds uncertainty.\n\nAs more and more SREs started to write automation in Go, it became clear that\nthose teams were happy and productive and were less likely to get stuck with\nhard-to-maintain code. It started to dawn on people that Go projects are\neasier to evolve and maintain, and that it was not just an effect of those\nprojects being newer, cleaner or just better engineered.\n\nSRE leadership noticed this effect and decided to take action and communicate\nvery broadly within the organization: SRE teams should preferably use Go for\nproduction-related projects, and avoid Python. I don\u2019t know if this is now\nseen at Google as something dictatorial, but at the time I think it just felt\nlike good org-wide communication and decision-making.\n\n## Go Production Platform and explosive growth\n\nThings accelerated quickly after that. We created a production platform that\nhad strong support for Go since the early days and replaced a lot of\nboilerplate configuration and repetitive procedures with high-level\nabstractions. This platform saw strong growth and eventually other platforms\nsurfaced. Go and our server framework became ubiquitous. I eventually left\nGoogle but I remember those days with joy.\n\nWhile I was just a user of the language, the experience of watching a project\ngo from zero to being a top-10 programming language has taught me a lot. I\ncould see with my own eyes that a strong team, surrounded by a strong\ncommunity, can really make big things.\n\n## Observing Go's Ascend to Prominence\n\nMy time working with Go programming at Google has been a game-changer, giving\nme a great understanding of the project's technical side and how a world-\nfamous team operates. As the project went on, I could clearly see how Go can\nmake project and team scaling easier.\n\nGo's emphasis on minimalistic design facilitated uniform coding, making it\neasy to integrate new programmers into the project, a feature particularly\nuseful in projects on a tight schedule. As the project grew, new libraries and\ntoolkits also emerged, increasing its popularity and facilitating its adoption\nby several big tech companies including Apple, Facebook, and Docker.\n\nDespite Rust having an extensive range of features, the widespread acceptance\nof Go across various industries shows that powerful software doesn't\nnecessarily need to be complicated.\n\nLooking back, it's clear that while our journey was filled with challenges,\neach twist and turn, each adjustment and advancement, was key to shaping\ntoday's Go. As the community moves forward, I am excited to see where we Go\nnext.\n\n2024 \u00a9 Yves Junqueira |\n\n", "frontpage": false}
