{"aid": "40157248", "title": "OSS Decorator to Trace LLM Apps \u2013 Integrated with LlamaIndex, LangChain, OAI SDK", "url": "https://langfuse.com/blog/2024-04-python-decorator", "domain": "langfuse.com", "votes": 6, "user": "marcklingen", "posted_at": "2024-04-25 13:23:36", "comments": 0, "source_title": "Trace complex LLM applications with the Langfuse decorator (Python) - Langfuse Blog", "source_text": "Trace complex LLM applications with the Langfuse decorator (Python) - Langfuse\nBlog\n\nWe use essential cookies to make our site work. With your consent, we may also\nuse non-essential cookies to improve user experience and analyze website\ntraffic. By clicking \u201cAccept,\u201d you agree to our website's cookie use as\ndescribed in our Cookie Policy. You can change your cookie settings at any\ntime by clicking \u201cPreferences.\u201d\n\nLangfuse Launch Week 1 \u2192Langfuse Launch Week, Day 4: Datasets v2 \u2192\n\nCTRL K\n\nApril 24, 2024\n\n# Trace complex LLM applications with the Langfuse decorator (Python)\n\nWhen building RAG or agents, lots of LLM calls and non-LLM inputs feeds into\nthe final output. The Langfuse decorator allows you to trace and evaluate\nholistically.\n\nMarc Klingen\n\nHassieb Pakzad\n\nWhen we initially built complex agents for web scraping and code generation\nwhile in Y Combinator, we quickly recognized the need for new LLM-focused\nobservability to truly understand how our applications produced their outputs.\nIt wasn't just about the LLM calls, but also the retrieval steps, chaining of\nseparate calls, function calling, obtaining the right JSON output, and various\nAPI calls that the agents should perform behind the scenes. In the end, all\nthese steps led to them breaking many times on our initial users.\n\nThis insight prompted us to start working on Langfuse. As many teams were\nexperimenting, we prioritized easy integrations with frameworks like LangChain\nand LlamaIndex, as well as wrapping the OpenAI SDK to log individual LLM\ncalls. As applications become increasingly complex and agents are deployed in\nproduction, the ability to trace complex applications is even more crucial.\n\nTraces are at the core of the Langfuse platform. Until now, generating a trace\nwas straightforward if you used a framework. However, if you didn't, you had\nto use the low-level SDKs to manually create and nest trace objects, which\nadded verbose instrumentation code to a project. Inspired by the developer\nexperience of tools we admire, such as Sentry and Modal, we created the\n@observe() decorator for Langfuse to make tracing your Python code as simple\nas possible. Note: we plan to revamp our JS/TS tracing as well.\n\nThis post is a deep dive into the @observe() decorator, our design objectives,\nchallenges we faced when implementing it, and how it can help you trace\ncomplex LLM applications.\n\ntldr:\n\n## Introduction\n\nIn this video, we introduce the decorator and why we think it's an awesome\nabstraction to merge all Langfuse integrations. If you are familiar with it,\nskip this video and jump to the next section.\n\nDecorator Integration\n\n## Goals\n\nWhen starting to work on the decorator, we wanted to make everything simple\nthat's complex when manually instrumenting your code. Consider this real-world\ntrace of a complex agent, where maintaining the nesting hierarchy manually\nusing the low-level SDK used to come with additional complexity and\nboilerplate code.\n\nExample of a nested trace with multiple LLM, non-LLM calls and a LangChain\nchain\n\nThe goal of the @observe() decorator is to abstract away the complexity of\ncreating and nesting traces and spans, and to make it easy to trace complex\napplications.\n\nThe decorator should:\n\n  * Trace all function calls and their outputs in a single trace while maintaining the nesting hierarchy\n  * Be easy to use and require minimal changes to your code\n  * Automatically capture the function name, arguments, return value, streamed completions, exceptions, execution time, and nesting of functions\n  * Be fully compatible with the native Langfuse integrations for LangChain, LlamaIndex, and the OpenAI SDK\n  * Encourage reusable abstractions across an LLM-based application without needing to consider how to pass trace objects around\n  * Support async environments for our many users that run performance-optimized LLM apps\n\n## Design decisions\n\n  1. The decorator reuses the low-level SDK to create traces and asynchronously batch them to the Langfuse API. This implementation was derived from the PostHog SDKs and is tested to have little to no impact on the performance of your application.\n  2. The decorator maintains a call stack internally that keeps track of nested function calls to reflect the observation hierarchy in the trace.\n  3. To be async-safe, the decorator leverages Python Contextvars (opens in a new tab) for managing its state.\n  4. The same observe() decorator is used to create a trace (outermost decorated function) and to add spans to the trace (inner decorated functions). This way, functions can be used in multiple traces without needing to be strictly a \"trace\" or a \"span\" function.\n\n## Limitation: Python Contextvars and ThreadPoolExecutors\n\nThe power of observability is most visible in complex applications with\nproduction workloads. Async and concurrent environments are common in these\napplications, and the decorator should work seamlessly in these environments.\nThe decorator uses Python's contextvars to store the current trace context and\nto ensure that the observations are correctly associated with the current\nexecution context. This allows you to use the decorator in reliably in async\nfunctions.\n\nHowever, an important exception are Python's ThreadPoolExecutors and\nProcessPoolExecutors. The decorator will not work correctly in these\nenvironments, as the contextvars are not correctly copied to the new threads\nor processes. There is an existing issue (opens in a new tab) in Python's\nstandard library and a great explanation (opens in a new tab) in the fastapi\nrepo that discusses this limitation.\n\nIn short, the decorator will work correctly in async environments, but not in\nThreadPoolExecutors or ProcessPoolExecutors.\n\n## Before and after\n\n### Status quo: Low-level SDK\n\nThe low-level SDK is very flexible but it is also very verbose and requires\npassing of Langfuse objects.\n\n    \n    \n    from langfuse import Langfuse from langfuse.openai import openai # OpenAI integration langfuse = Langfuse() def story(trace): span = trace.span(name=\"story\") output = openai.chat.completions.create( model=\"gpt-3.5-turbo\", max_tokens=100, messages=[ {\"role\": \"system\", \"content\": \"You are a great storyteller.\"}, {\"role\": \"user\", \"content\": \"Once upon a time in a galaxy far, far away...\"} ], trace_id=trace.id, parent_observation_id=span.id ).choices[0].message.content span.end(output=output) return output def main(): trace = langfuse.trace(\"main\") return story(trace)\n\n### @observe() decorator to the rescue\n\nAll complexity is abstracted away and you can focus on your business logic.\nThe OpenAI SDK wrapper is aware that it is run within a decorated function and\nautomatically adds its logs to the trace.\n\n    \n    \n    from langfuse.decorators import observe from langfuse.openai import openai # OpenAI integration @observe() def story(): return openai.chat.completions.create( model=\"gpt-3.5-turbo\", max_tokens=100, messages=[ {\"role\": \"system\", \"content\": \"You are a great storyteller.\"}, {\"role\": \"user\", \"content\": \"Once upon a time in a galaxy far, far away...\"} ], ).choices[0].message.content @observe() def main(): return story() main()\n\n## Interoperability\n\nThe decorator completely replaces the need to use the low-level SDK. It allows\nfor the creation and manipulation of traces, and you can add custom scores and\nevaluations to these traces as well. Have a look at the extensive\ndocumentation for more details.\n\nLangfuse is natively integrated with LangChain, LlamaIndex, and the OpenAI SDK\nand the decorator is fully compatible with these integrations. As a result,\nyou can, in a single trace, use the decorator on the outermost function,\ndecorate function calls and API calls that are non-LLM related, and use the\nnative instrumentation for the OpenAI SDK, LangChain and Llama Index.\n\nExample (cookbook):\n\n    \n    \n    from langfuse.openai import openai from langfuse.decorators import observe @observe() def openai_fn(calc: str): res = openai.chat.completions.create( model=\"gpt-3.5-turbo\", messages=[ {\"role\": \"system\", \"content\": \"You are a very accurate calculator. You output only the result of the calculation.\"}, {\"role\": \"user\", \"content\": calc}], ) return res.choices[0].message.content @observe() def llama_index_fn(question: str): # Set callback manager for LlamaIndex, will apply to all LlamaIndex executions in this function langfuse_handler = langfuse_context.get_current_llama_index_handler() Settings.callback_manager = CallbackManager([langfuse_handler]) # Run application index = VectorStoreIndex.from_documents([doc1,doc2]) response = index.as_query_engine().query(question) return response @observe() def langchain_fn(person: str): # Get Langchain Callback Handler scoped to the current trace context langfuse_handler = langfuse_context.get_current_langchain_handler() # Pass handler to invoke method of chain/agent chain.invoke({\"person\": person}, config={\"callbacks\":[langfuse_handler]}) @observe() def main(): output_openai = openai_fn(\"5+7\") output_llamaindex = llama_index_fn(\"What did he do growing up?\") output_langchain = langchain_fn(\"Feynman\") return output_openai, output_llamaindex, output_langchain main();\n\n## Outlook\n\nThe decorator drives open tracing for teams that don't want to commit to a\nsingle application framework or ecosystem, but want to easily switch between\nframeworks while relying on Langfuse as a single platform for all\nexperimentation, observability and evaluation needs.\n\nRoadmap: The decorator is currently only available for Python. We will add a\nsimilar implementation for JS/TS.\n\n## Add-on\n\nIf you want to built complex applications while being able to easily switch\nbetween models, we strongly recommend using this stack:\n\n  * Langfuse Decorator for tracing\n  * Langfuse OpenAI SDK Wrapper for automatic instrumentation of OpenAI calls\n  * LiteLLM Proxy for standardization of 100+ models on the OpenAI API\n\nHave a look at this cookbook to see an end-to-end example \u2013 we really think\nyou'll like this stack and there are lots of teams in the Langfuse Community\nwho built on top of it.\n\n## Thank you\n\nThank you to everyone who tested the decorator during the beta phase and\nprovided feedback. We've received several Gists from community members\nshowcasing their own decorator implementations built using Langfuse before the\ndecorator was released as an official integration. We're excited to see what\nyou create with it!\n\n## Get Started\n\nRun the end-to-end cookbook on your Langfuse traces or learn more about model-\nbased evals in Langfuse.\n\nDocsExample notebookVideo introduction\n\nLast updated on April 24, 2024\n\nLangfuse Launch Week #1\n\n### Was this page useful?\n\n### Questions? We're here to help\n\nGitHub Q&AEmailTalk to sales\n\n### Subscribe to updates\n\nPlatform\n\n  * LLM Tracing\n  * Prompt Management\n  * Evaluation\n  * Datasets\n  * Metrics\n\nIntegrations\n\n  * Python SDK\n  * JS/TS SDK\n  * OpenAI SDK\n  * Langchain\n  * Llama-Index\n  * Litellm\n  * Flowise\n  * Langflow\n  * API\n\nResources\n\n  * Documentation\n  * Interactive Demo\n  * Changelog\n  * Roadmap\n  * Pricing\n  * Status\n  * Self-hosting\n  * Open Source\n\nAbout\n\n  * Blog\n  * Careers\n  * About us\n  * Support\n  * Why Langfuse?\n  * Schedule Demo\n  * OSS Friends\n\nLegal\n\n  * Security\n  * Imprint\n  * Terms\n  * Privacy\n\n\u00a9 2022-2024 Finto Technologies\n\n", "frontpage": false}
