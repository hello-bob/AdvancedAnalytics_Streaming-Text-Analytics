{"aid": "40291577", "title": "Why Is Everyone Suddenly Furious About AI Regulation?", "url": "https://asteriskmag.com/issues/06/why-is-everyone-suddenly-furious-about-ai-regulation", "domain": "asteriskmag.com", "votes": 2, "user": "mitchbob", "posted_at": "2024-05-07 21:14:50", "comments": 2, "source_title": "Why Is Everyone Suddenly Furious About AI Regulation?\u2014Asterisk", "source_text": "Why Is Everyone Suddenly Furious About AI Regulation?\u2014Asterisk\n\nSo what does the bill actually do?\n\nMajor misconceptions\n\nWhat are the real problems?\n\nConclusion\n\n# Why Is Everyone Suddenly Furious About AI Regulation?\n\n## Zvi Mowshowitz\n\nPlease tell us, because we're stumped.\n\nThis February, California state senator Scott Wiener (D \u2013 San Francisco)\nintroduced Senate Bill 1047, which, if passed, would require the companies\nbehind the world\u2019s largest, most advanced AI models to take steps to guarantee\ntheir safety before releasing the models to the public. For a piece of state\nlegislation, the bill has unusual international importance. Most of the\ncompanies making such models are headquartered in California.\n\nDespite write-ups in the San Francisco Chronicle and Washington Post, the bill\ninitially received relatively little attention. That was until last week, when\n\u2014 as these things go \u2014 a spate of strong objections to it exploded across\nTwitter and Substack.\n\nSome of these objections were helpful critiques pointing to potential problems\nwith SB 1047\u2019s current form. Some were based on a failure to understand how\nthe law works, a failure to carefully read the bill, or both. And some were\nalarmist rhetoric with little tether to the bill as it\u2019s actually written.\n\nNothing is a substitute for reading the actual bill. The purpose of this\npiece, a condensed version^ 1 of the one I published on my blog, is to respond\nto some of the most serious misconceptions \u2014 and to suggest concrete changes\nthat address some of the real concerns.\n\n## So what does the bill actually do?\n\nSB 1047 only applies to the most powerful new AI models, which it defines as\n\u201ccovered models.\u201d Let\u2019s say you\u2019d like to build a new model. What counts?\n\nCovered models are those trained on more than 1026 flops (a measure of\ncomputing power that, at current prices, is estimated to cost between tens and\nhundreds of millions of dollars), or projected to have similar performance on\nbenchmarks used to evaluate state of the art foundation models. If your model\nwas trained on less than 1026 flops and doesn't outperform those that were? It\nis not a covered model. You do not need to do anything at all. Note that this\n1026 threshold would, according to our best estimates, exclude every released\nmodel including GPT-4, Claude Opus and the current versions of Google Gemini.\nNone of them would count as covered models under this bill.\n\nWhat if your model is a derivative of an existing model, e.g. of an open-\nsource model? You also do not need to do anything. Right now, no current open-\nsource models are anywhere near the threshold, other than Meta\u2019s prospective\nLlama-3 400B, which may or may not hit it. But even if Llama-3 400B is\ncovered, developers who use or modify it would be unaffected. All the\nrequirements in the bill fall on the original developer \u2014 in this case, Meta.\n\nNow let\u2019s say you\u2019ve secured access to enough computing power to meet the\ncompute threshold, or you can be reasonably sure your model will be 2024 state\nof the art. Then you are training a covered model, and you will need to adhere\nto the safety requirements laid out in the bill.\n\nDuring training, you will need to a) ensure no one can hack it (while it\nremains in your possession), b) make sure it can be shut down, c) implement\ncovered guidance (here meaning guidance issued by the National Institute of\nStandards and Technology and by the newly created Frontier Model Division, as\nwell as \u201cindustry best practices\u201d), and d) implement a written and separate\nsafety and security protocol which can provide reasonable assurance that the\nmodel lacks hazardous capabilities, or \u2014 if it has them \u2014 that it can\u2019t use\nthem. You will also need to include the testing procedure you will use to\nidentify the hazardous capabilities \u2014 and say what you will do if you find\nthem. Notably, the bill doesn\u2019t specify what any of this looks like.\nDevelopers create and implement the plans; the government does not dictate\nwhat they are.\n\nHazardous capabilities are set at an extremely high threshold. We are not\ntalking about hallucinations, bias, or Gemini generating images of diverse\nsenators from the 1800s \u2014 or even phishing attacks, scams, or other serious\nfelonies. The bill specifies hazardous capabilities as the ability to directly\nenable a) the creation or use of weapons of mass destruction; b) at least $500\nmillion of damage through cyberattacks on critical infrastructure via a single\nincident or multiple related incidents; c) the same amount of damage,\nperformed autonomously, in conduct that would violate the Penal Code; or d)\nother threats to public safety of comparable severity.\n\nYou train your covered model and now you want to release it. What then? You\nmust implement \u2018reasonable safeguards and requirements\u2019 to prevent anyone from\nbeing able to use its hazardous capabilities, if it has them. After\ndeployment, you need to periodically reevaluate your safety protocols, and\nfile an annual report that says you\u2019re doing so.\n\nIf you don\u2019t want to comply with these requirements, you can apply for a\nlimited duty exemption.\n\nYou have two ways to do this. One is for you, the developer, to provide\nreasonable assurance that your model (and any derivative models based on it)\nwon\u2019t have hazardous capabilities. Or you can show that your model will be no\nmore capable than existing noncovered models which themselves lack hazardous\ncapabilities, or which have their own limited duty exemption. Meeting either\nrequirement allows you to train and release your model without additional\nsafeguards.\n\nThe point of the limited duty exemption is to reduce the regulatory burden on\nfuture, more capable models: in the coming years, we expect to learn more\nabout which classes of models are safe. The limited duty exemption would\nremove most safety requirements for developers replicating models that are\nalready proven safe. Let\u2019s say it\u2019s clear in 2025 that state of the art\ncovered models from 2024 don\u2019t have hazardous capabilities \u2014 these models, and\nothers based on them, could get an exemption and be released without\nsafeguards.\n\nDoes anyone need to get a limited duty exemption? No. Labs are entirely free\nto deploy models that don\u2019t qualify for an exemption, as long as they still\ncomply with safety requirements.\n\n## Major misconceptions\n\nIs this an existential threat to California\u2019s AI industry?\n\nNo. The bill has zero or minimal impact on most of California\u2019s AI industry,\nand this is unlikely to change for years. Few companies will want to train\ncovered models that attempt to compete with Google, Anthropic and OpenAI.\nThere\u2019s no requirement for developers of noncovered models to do anything at\nall \u2014 even fill out paperwork. The bill relies entirely on AI labs to self-\nreport if their models are large enough to qualify. For those who do train\ncovered models, there will be increasing ability to get limited duty\nexemptions that make the requirements trivial.\n\nEven for leading labs, the compliance costs imposed by the bill are trivial\ncompared to the compute costs of training the model. As a practical matter, I\nbelieve that I could give reasonable assurance, right now, that all publically\navailable models (including GPT-4, Claude 3, and Gemini Advanced 1.0 and Pro\n1.5) lack hazardous capability.\n\nThe people who made them think so too. While the safety requirements in the\nbill are more stringent, and their final details have yet to be determined,\nthey\u2019re not drastically different from what industry leaders already do. When\nGPT-4 was released, OpenAI tested its ability to contribute to the creation of\nbiological, nuclear, and chemical weapons, and its ability to carry out\ncyberattacks. They also partnered with METR (Model Evaluation and Threat\nResearch), a nonprofit team at the forefront of assessing whether AI systems\npose catastrophic risks, to test if the model was capable of \u201cescaping\u201d its\ndatacenter and copying itself into the wild (it wasn\u2019t). Anthropic regularly\ntests their models for biological and cyber capabilities as well as autonomous\nreplication. Both labs have a Responsible Scaling Policy (Anthropic) or a\nPreparedness Framework (OpenAI) that details their plans for testing their\nmodels and what level of risk would make them stop.^ 2\n\nThese measures are already the most rigorous in the business, and they clearly\nhaven\u2019t destroyed either lab\u2019s ability to make leading-edge models.\n\nDoes the bill create a new regulatory agency?\n\nNo. It creates the Frontier Model Division within the California Department of\nTechnology.^ 3 The new division will issue guidance, allow coordination on\nsafety procedures, appoint an advisory committee on (and to assist) open\nsource, publish incident reports and process certifications. That\u2019s all.\n\nSome critics of SB 1047 have greatly exaggerated the powers of the FMD. In an\nimpact analysis, Brian Chau, executive director of the advocacy group Alliance\nfor the Future, writes that the bill \u201cconcentrates power (even military power)\nin a small, minimally accountable Frontier Model Division\u201d and that the FMD\nwould be \u201ca new regulatory entity in California.\u201d A call to action circulated\nby AFTF states that it would have \u201cpolice powers,\u201d and would be able to \u201cthrow\nmodel developers in jail for the thoughtcrime of doing AI research.\u201d\n\nAll of this is \u2014 to put it mildly \u2014 false. SB 1047 will be enforced by the\nstate Attorney General, not the FMD. The FMD has no power to enforce\nregulations, impose new ones, or block the release of new models. And neither\nthe Attorney General nor anyone else can throw people in jail for violating\nthe bill (unless they intentionally lie to the government in writing), let\nalone wield military power.\n\nAre the burdens here overly onerous to small developers, researchers, or\nacademics?\n\nRight now rather obviously not, since the burdens do not apply to them. The\nsubstantial burdens only apply if you train a covered model, from scratch,\nthat can\u2019t get a limited duty exemption. A derivative model never counts. A\nChatGPT wrapper definitely does not count. Research on any existing models\ndoes not count.\n\nThis will not apply to a small developer for years. At the point that it does,\nyes, if you make a GPT-5-level model from scratch, I think you can owe us some\nreports.\n\nWhich models are covered depends not just on size but on performance \u2014 models\nwith similar capabilities to what one could have trained with 1026 flops in\n2024 would also need to comply with safety regulations. Neil Chilson, Head of\nAI Policy at the newly formed Abundance Institute, argues that this clause is\nanti-competitive, with its purpose being to ensure that if someone creates a\nsmaller model that has similar performance to the big boys, it would not have\ncheaper compliance costs.\n\nNo. The point is to ensure the safety of models with advanced capabilities.\nThe reason to use a 1026 flops threshold is that this is the best\napproximation we have for \u201clikely to have sufficiently advanced capabilities.\u201d\n\nAre regulatory requirements capable of contributing to moats? Yes, of course.\nAnd it is possible this will happen here to a non-trivial degree, among those\ntraining frontier foundation models in particular. But \u2014 as I say above \u2014 I\nexpect the costs involved to be a small fraction of the compute costs of\ntraining such models, or the cost of actual necessary safety checks.\n\nFinally, does the bill, as claimed by (again) Brian Chau \u201c\u2018literally specify\nthat they want to regulate models capable of competing with OpenAI?\u201d\n\nNo, of course not. To the people spreading this claim: You can do better. You\nneed to do better.\n\nThere are legitimate reasons one could think this bill would be a net negative\neven if its particular implementation issues are fixed. There are also\nparticular details that need (or at least would benefit from) fixing. Healthy\ndebate is good.\n\nThis kind of hyperbole, and a willingness to repeatedly signal boost it, is\nnot.\n\nDoes SB 1047 target open source AI?\n\nDean Ball, a research fellow at George Mason University\u2019s Mercatus Center,\nclaims that the bill would \u201ceffectively outlaw all new open source AI models.\u201d\n\nIt won\u2019t. No existing open source model would count as a covered model. Maybe\nsome will in the future. But this very clearly does not \u201cban all open source.\u201d\nThere are zero existing open weights models that this bans. There are a\nhandful of open source developers that might plausibly have obligations under\nthis bill in the future, but we\u2019re talking about companies like Meta and\nperhaps Mistral, not small start-ups.\n\nAnother set of concerns involves the bill\u2019s shutdown requirement: developers\nof covered models must be able to shut down all copies of the model on\ncomputers that they control. This provision has been misread to refer to all\ncopies in existence. This would of course make releasing a model\u2019s weights\nillegal, since it\u2019s impossible for a company to shut down every copy of a\nmodel downloaded by anyone anywhere. However, that\u2019s not what the law says.\nEmphasis mine:\n\n22602 (m): \u201cFull shutdown\u201d means the cessation of operation of a covered\nmodel, including all copies and derivative models, on all computers and\nstorage devices within custody, control, or possession of a person, including\nany computer or storage device remotely provided by agreement.\n\nIf they had meant \u201cfull shutdown\u201d to mean \u201cno copies of the model are now\nrunning\u201d then this would not be talking about custody, control or possession\nat all. Instead, if the model has open weights and has been downloaded by\nothers, the developer is off the hook.\n\nRather than a clause that is impossible for an open model to meet, this is a\nclause where open models are granted extremely important special treatment, in\na way that seems damaging to the core needs of the bill.\n\nAre developers guilty of perjury for being wrong?\n\nOther critics have claimed that the bill would jail developers for making\ngood-faith mistakes about the capabilities of their models. Not unless they\nare willfully defying the rules and outright lying, in writing, to the\ngovernment.\n\nEven then, mostly no. It is extremely unlikely that perjury charges would be\npursued unless there was clear bad faith and lying, and even if that happened,\nit still seems unlikely. California\u2019s perjury statute requires you to know\nyour statement to be false. There were only a handful of prosecutions in all\nof 2022. People almost never get prosecuted for perjury.\n\n## What are the real problems?\n\nNone of this is to say that SB 1047 is perfect. There are two known\nimplementation problems with the bill as written.\n\nThe most serious is that the bill\u2019s definition of \u201cderivative models\u201d is too\nbroad. As written, derivative models can include unlimited additional\ntraining. This creates a loophole: it\u2019s possible to take a less capable base\nmodel and make it significantly more capable without being subject to any\nsafety requirements. In that situation, responsibility would still rest with\nthe original developer of the base model.\n\nThis could be fixed by adding a section to make clear that if a sufficiently\nlarge amount of compute (I suggest 25% of original training compute or 1026\nflops, whichever is lower) is spent on additional training and fine-tuning of\nan existing model, then the resulting model is now nonderivative. The new\ndeveloper has all the responsibilities of a covered model, and the old\ndeveloper is no longer responsible.\n\nThe other problem is that, in determining if a model empowered a human to\ncause catastrophic harm, the bill compares an AI-assisted human to someone who\nlacks access to any covered models at all. But this isn\u2019t realistic: over\ntime, access to covered models will become more widespread and may make\nessentially every complex task substantially easier. This can be fixed by\nchanging the baseline for comparison to what a human can do with access to\ncovered models established as safe.\n\n## Conclusion\n\nHopefully this has cleared up a lot of misconceptions about SB 1047.\n\nCalifornia AI companies are investing billions of dollars in AI, and talking\nabout making that trillions. Policymakers have been caught repeatedly off\nguard by the capabilities of the models they develop. SB 1047 is an admirable\neffort to get ahead of the ball, and make sure companies that spend tens or\nhundreds of millions of dollars on a new model are checking if their models\ncan commit catastrophic large-scale crimes.\n\nThis bill would have zero impact on every model currently available outside\nthe three big labs of Anthropic, OpenAI and Google DeepMind, and at most one\nother model known to be in training, Llama-3 400B. If you build a \u201cderivative\u201d\nmodel, meaning you are working off of someone else\u2019s foundation model, you\nhave to do nothing.\n\nIn addition, if in the future you build something that is substantially above\nGPT-4 level, matching the best anyone will do in 2024, then so long as you are\nbehind existing state of the art your requirements will be minimal.\n\nThis bill is not all upside, nor is it in ideal condition. But with some\nchanges, it seems to be a mostly excellent version of what it is attempting to\nbe.\n\nThat does not mean it could not be improved further. It certainly does not\nmean we will not want to make changes over time as AI rapidly evolves, nor\nthat it would be sufficient if passed in identical form at the federal level.\nFor all the misguided talk of how this bill would supposedly destroy the\nentire AI industry in California, it is easy to see ways the bill could prove\ninadequate to future safety needs. What this does seem to be is a good\nbaseline from which to encourage and gain visibility on basic safety\nprecautions, which puts us in a better position to assess future unpredictable\nsituations.\n\nThere is room \u2014 and we should make room \u2014 for good faith disagreements over SB\n1047 and what effect it will have in practice. But those disagreements should\nbe based on what the bill actually says and does.\n\n  1. And therefore does not cover every key detail of the bill \u21a9\n\n  2. I\u2019ve commented extensively the Anthropic responsible scaling policy (RSP) and OpenAI\u2019s preparedness framework elsewhere. \u21a9\n\n  3. See section 4, 11547.6(c) \u21a9\n\nZvi Mowshowitz writes at Don't Worry About the Vase.\n\nPublished May 2024\n\nHave something to say? Email us at letters@asteriskmag.com.\n\nNext Golden States\n\n## About Highlights\n\nBy highlighting text and \u201cstarring\u201d your selection, you can create a personal\nmarker to a passage.\n\nWhat you save is stored only on your specific browser locally, and is never\nsent to the server. Other visitors will not see your highlights, and you will\nnot see your previously saved highlights when visiting the site through a\ndifferent browser.\n\nTo add a highlight: after selecting a passage, click the star . It will add a\nquick-access bookmark.\n\nTo remove a highlight: after hovering over a previously saved highlight, click\nthe cross . It will remove the bookmark.\n\nTo remove all saved highlights throughout the site, you can click here to\ncompletely clear your cache. All selections have been cleared.\n\nSubscribe\n\nBy clicking \u201cAccept All Cookies\u201d, you agree to the storing of cookies on your\ndevice to enhance site navigation, analyze site usage, and assist in our\nmarketing efforts.\n\n## Privacy Preference Center\n\nWhen you visit any website, it may store or retrieve information on your\nbrowser, mostly in the form of cookies. This information might be about you,\nyour preferences or your device and is mostly used to make the site work as\nyou expect it to. The information does not usually directly identify you, but\nit can give you a more personalized web experience. Because we respect your\nright to privacy, you can choose not to allow some types of cookies. Click on\nthe different category headings to find out more and change our default\nsettings. However, blocking some types of cookies may impact your experience\nof the site and the services we are able to offer. More information\n\n### Manage Consent Preferences\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched\noff in our systems. They are usually only set in response to actions made by\nyou which amount to a request for services, such as setting your privacy\npreferences, logging in or filling in forms. You can set your browser to block\nor alert you about these cookies, but some parts of the site will not then\nwork. These cookies do not store any personally identifiable information.\n\n#### Performance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure\nand improve the performance of our site. They help us to know which pages are\nthe most and least popular and see how visitors move around the site. All\ninformation these cookies collect is aggregated and therefore anonymous. If\nyou do not allow these cookies we will not know when you have visited our\nsite, and will not be able to monitor its performance.\n\n#### Functional Cookies\n\nThese cookies enable the website to provide enhanced functionality and\npersonalisation. They may be set by us or by third party providers whose\nservices we have added to our pages. If you do not allow these cookies then\nsome or all of these services may not function properly.\n\n#### Targeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They\nmay be used by those companies to build a profile of your interests and show\nyou relevant adverts on other sites. They do not store directly personal\ninformation, but are based on uniquely identifying your browser and internet\ndevice. If you do not allow these cookies, you will experience less targeted\nadvertising.\n\n### Performance Cookies\n\nlabel\n\nConsent Leg.Interest\n\nlabel\n\nlabel\n\nlabel\n\n", "frontpage": false}
