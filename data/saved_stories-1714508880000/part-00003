{"aid": "40211271", "title": "Benchmarking PostgreSQL Connection Poolers: PgBouncer, PgCat and Supavisor", "url": "https://tembo.io/blog/postgres-connection-poolers", "domain": "tembo.io", "votes": 6, "user": "riv991", "posted_at": "2024-04-30 14:27:00", "comments": 0, "source_title": "Benchmarking PostgreSQL connection poolers: PgBouncer, PgCat and Supavisor | Tembo", "source_text": "Benchmarking PostgreSQL connection poolers: PgBouncer, PgCat and Supavisor | Tembo\n\n# Benchmarking PostgreSQL connection poolers: PgBouncer, PgCat and Supavisor\n\nFeb 13, 2024 \u2022 10 min read\n\n# Binidxaba\n\nCommunity contributor\n\nCreating a connection to your Postgres database to execute a short-lived query\nis expensive. Several people have measured the overhead of Postgres\nconnections and some locate them in the range of 1.3MB of memory per\nconnection and others in the range of 2MB. In addition, there is also the\noverhead of having to fork a new process in the database server.\n\nTo alleviate these problems, typically people use a connection pooler.\nHowever, for Postgres, there are several options available. Some popular ones\nare Pgbouncer, Pgcat, Odyssey, pgagroal, Pgpool-II and Supavisor.\n\nBut which one should you choose? Should you go with the battle-tested\nPgbouncer? Or should you go with the internet-scale, cloud-native Supavisor?\nOr the sharding and load balancing aware pgCat?\n\nIn this post, we will compare these three popular alternatives:\n\n  * PgBouncer\n  * PgCat\n  * Supavisor\n\nLet\u2019s quickly describe each of them.\n\n## Quick overview of PgBouncer, PgCat and Supavisor\n\n### PgBouncer\n\nPgBouncer is a lightweight connection pooler and probably the most popularly\nused.\n\nIt can be quickly installed with your OS package manager (e.g. apt) and the\nconfiguration involves adjusting two files: pgbouncer.ini and an\nauthentication file. After that, you just need to change your connection\nstrings to point to the PgBouncer instance.\n\nOne of its limitations often discussed on the internet is its lack of support\nfor replica failover and its limited support for load balancing.\n\n### PgCat\n\nPgBouncer\u2019s limitations are addressed in PgCat, which is described as a\nconnection pooler and proxy that allows sharding, load balancing, failover and\nmirroring.\n\nIn contrast with Pgbouncer, PgCat allows spreading queries to a sharded\ndatabase. Plus, it is multithreaded, thus allowing to easily exploit hardware\npotential.\n\nThe README is well-written, making the installation easier. Like PgBouncer,\nyou just need to update a configuration file (in this case, pgcat.toml); and\nthen redirect your application to PgCat.\n\n### Supavisor\n\nAnother option is Supavisor, the connection pooler developed by Supabase.\n\nIt is described as a scalable, cloud-native Postgres connection pooler. In the\nlaunching post, it is mentioned that it can handle millions of connections and\nwill replace PgBouncer in Supabase\u2019s managed Postgres offering as it is\nintended to provide zero downtime when scaling a server, and handling of\nmodern connection demands such as those seen in serverless environments.\n\nIts setup is a bit more involved, as it requires an additional auxiliary\ndatabase to store tenants information, plus there are more listening ports in\nplay. Here is a diagram I drew to better understand the architecture:\n\n## Criteria for comparing PostgreSQL connection poolers\n\nChoosing a connection pooler requires careful consideration of various\nfactors.\n\nThe first one is how easy it is to set it up. Do you need to play with a bunch\nof knobs just to get started? Do you need to change your application code, or\ndo you just need to change a connection string?\n\nThe second one is how much it can scale. Can it handle hundreds, thousands, or\na million concurrent connections?\n\nA third one is the overhead. How much memory does it require for each\nconnection in the pool (e.g., what metadata does it save)? What\u2019s the latency\nof acquiring a connection when available connections are in the pool?\n\nOne more could be: Can it adapt to a varying number of concurrent connections\nand gracefully handle a spike in the number of connection attempts?\n\nWe try to answer some of these questions in the sections below.\n\n## Description of experiments\n\nTo compare the three connection poolers, I set up a couple of VMs in Google\nCloud. My intention was to evaluate some of the factors I described above.\n\nSo, my setup was as follows:\n\nI used separate machines to give the connection pooler and pgbench enough\nCPU/Memory resources.\n\nThe VMs were Google Cloud\u2019s E2-standard-8 (8 vCPUs, 4 cores, 32GB memory), all\nin the same zone. The OS was Debian 11.8. Postgres version was 15.5.\n\nI configured the three poolers to create a pool of 100 connections and the\nworkload was generated using PgBench. In all cases, the pooler was configured\nfor Transaction mode. I collected the outputs and analyzed them. For the exact\ndetails of my experiments to replicate them, please refer to this github\nrepository.\n\n## Results\n\n### Throughput and Latency for small connection count\n\nThe first thing I wanted to see was whether using a connection pooler could\nmitigate the need to create new connections every time the app executed a\nquery.\n\nThe following two graphs show the behavior of the connection poolers when the\nnumber of clients (<=100) is smaller than the number of connections in the\npool. As a baseline, I am considering direct connections to the database\n(i.e., no pooler in between). For that, I added the numbers of: (1) when\nclients connect only once and execute all their workload, and (2) when clients\ndirectly connect to Postgres, execute some workload, and then disconnect.\n\nA few things to notice:\n\n  * The baselines serve as lower and upper bounds for the poolers\u2019 behavior.\n  * All connection poolers had worse latency than connecting the client directly to the database once and leaving the connection open for the duration of its workload. That\u2019s expected because, in contrast, the poolers need to execute additional logic to hand connections to the client application.\n  * All connection poolers had better latency numbers than connecting the client directly to the Database, running a query, and then disconnecting. This is because the poolers remove the time-to-connect from the application\u2019s execution path.\n  * For lower connection counts, PgBouncer has the lowest latency and best throughput. But as we go over 50 clients, PgCat starts having better latency and throughput.\n  * Since PgBouncer is single-threaded, maybe beyond 25 connections, it is necessary to spawn another PgBouncer instance.\n  * In contrast, PgCat and Supavisor continue to deliver more throughput, although PgCat performs better up to this point (number of clients <= 100).\n  * For all cases, PgCat and PgBouncer have somewhat comparable latency (in the range of -17% and +24% difference), but Supavisor has much higher compared to the other two (in the range of 80-160%).\n\n### Throughput and Latency for large connection count\n\nLet\u2019s see what happens when we stress the connection poolers using more\nclients. The following two graphs show my results when the number of\nconcurrent clients varied in the range of 250-2500:\n\nFor the three poolers, the latency of executing a simple query continues to\ngrow. This is because, beyond a certain point, the CPU becomes a bottleneck,\nand connection requests begin to pile up. I suspect this could be mitigated\nwith a more powerful machine.\n\nWe can see that PgCat performs better (higher throughput, lower latency),\nreaching 59K tps. In contrast, PgBouncer peaks at 44,096 tps and degrades to a\nsteady state of 25,000-30,000 tps beyond 75 concurrent connections. Supavisor\npeaks at about 21,700 tps, but remains in steady state.\n\nAs suggested in Supabase\u2019s blog post, beyond this point, one alternative is to\nscale Supavisor horizontally by adding more instances. This case, though, is\nbeyond the scope of my experiments.\n\n### Latency at maximum throughput\n\nNow, let us see what happens with the latency when each of the poolers are\nexercised at their maximum throughput. For PgBouncer that is 50 clients; for\nPgCat, 1250 clients; and for Supavisor, 100 clients.\n\nPgBouncer\u2019s latency (clients = 50) is generally below 4 ms for 99% of the\nconnection requests, and (as shown earlier) the mean is around 1 ms.\n\nFor PgCat (clients=1250), the latency at maximum throughput is higher than\nPgBouncer\u2019s, as shown in the following picture, but it is also dealing with a\nmuch larger number of connections:\n\nBut of the three poolers, the worst latency was obtained with Supavisor\n(clients=100):\n\nInterestingly, when considering the 99 percentile, the latency sees some\nfrequent spikes. Pgbench shows that in the form of standard deviation:\n\nI don\u2019t have enough knowledge to understand the root cause of this. One\nhypothesis could be the inherent complexity of Supavisor\u2019s architecture. The\nother one is that I may be misusing the pooler \ud83d\ude42\n\n### CPU Utilization at maximum throughput\n\nDuring the experiments, I also collected information about CPU utilization.\nLet\u2019s see how the poolers use the CPU when exercised at their maximum\nthroughput.\n\nThe following plot demonstrates that with 50 clients, PgBouncer\u2019s CPU\nutilization reaches ~100% (i.e., full utilization in one core). We again see\nthe downside of its single-threaded implementation. The mitigation could be to\nadd more PgBouncer instances.\n\nIn contrast, PgCat uses CPU most efficiently by being able to support 1,250\nconnections within 400% CPU utilization, whereas Supavisor uses 700% for 100\nclients. In theory, PgCat and Supavisor have more room for better scalability\ndue to their ability to utilize more cores.\n\n### Latency with 1250 clients\n\nBefore finishing, let me show you what the latencies look like when we use\n1250 clients with all the poolers. PgCat has the maximum throughput with this\nnumber of clients, and so the following graph shows a fair comparison with\nsimilar workload.\n\nEven for the median, Supavisor shows the noisy behavior I mentioned\npreviously. PgCat exhibits the best latency.\n\nThe following graph shows the latency for p99, and we see the same trend:\n\n## Summary of Results\n\nFor convenience, the following table summarizes the qualitative attributes of\nthe three poolers:\n\nPgBouncer| PgCat| Supavisor  \n---|---|---  \nCurrent Version (Maturity)| v1.21| v1.1.1| v1.1.13  \nRepo| pgbouncer| pgcat| supavisor  \nRepo Popularity (Stars in GH)| 2.5K| 2.4K| 1.4K  \nLanguage of implementation| C| Rust| Elixir  \nInstallation/Setup Complexity| Easy| Easy| Medium, several moving parts.  \nMulti-threaded| No| Yes| Yes  \nApplication Changes| Connection string| Connection string| Connection string  \n  \nAnd this table summarizes the results from my experiments with the\nE2-standard-8 vms:\n\nPgBouncer| PgCat| Supavisor  \n---|---|---  \nMax Concurrent Clients Tested| 2500| 2500| 2500  \nMax Throughput| 44,096 tps @ 50 clients| 59,051 tps @ 1,250 clients| 21,708\ntps @ 100 clients  \nLatency @ 1,250 concurrent clients| 47.2 ms| 21.1 ms| 64.37 ms  \n  \nIn this post, we compared connection poolers for Postgres across different\naxes.\n\nOf the three, Supavisor is the one that requires more steps to begin using it.\nHowever, the way it handles tenants is convenient for modern cloud\nenvironments. Other than that, once the poolers are adequately set up, the\nonly change required in your application is the connection string.\n\nFrom my experiments, I found that PgCat performs better as it delivers higher\nthroughput while supporting more concurrent clients. With more than 750\nclients, PgCat achieves more than 2X qps compared to PgBouncer and Supavisor.\n\nPgBouncer offers the best latency for low (<50) connection counts. However,\nits downsides are: (1) it is single-threaded, preventing it from fully\nutilizing the machine with a single instance, and (2) if there is a surge in\nthe number of connection requests, clients would right away notice some\nperformance degradation (at least in my environment). In comparison, PgCat and\nSupavisor keep their tps numbers when more clients are added.\n\nSo, according to these experiments, PgCat shows like it\u2019s the best option when\nyou have high connection counts.\n\nThank you for reading until this point, dear reader. I hope you find these\ninsights helpful. In a follow up post, we shall compare the three connection\npoolers using prepared statements. In the meantime, I am curious: which\nconnection pooler are you currently using and why? What other factors did you\nconsider before making that decision? Let us know your comments at @tembo_io.\n\nUpdate (2024/02/14): Made it clear that the experiments use Transaction mode\nand added note about prepared statements.\n\n## Appendix\n\nIf you are interested in the exact details of the configurations used for the\nexperiments presented in this post, please refer to the github repository.\n\n### What's next?\n\nTry Tembo Cloud for free\n\nStar on Github\n\nSubscribe on Youtube\n\nView our RSS feed\n\nFollow on X\n\nNext post\n\n#### MongoDB capabilities on Postgres with Managed FerretDB on Tembo Cloud\n\nFeb 15, 2024\n\npostgres connection pool pgbouncer pgcat supavisor pgbench\n\n## On this page\n\nQuick overview of PgBouncer, PgCat and\nSupavisorPgBouncerPgCatSupavisorCriteria for comparing PostgreSQL connection\npoolersDescription of experimentsResultsThroughput and Latency for small\nconnection countThroughput and Latency for large connection countLatency at\nmaximum throughputCPU Utilization at maximum throughputLatency with 1250\nclientsSummary of ResultsAppendix\n\n## Share this article\n\n###### Company\n\nDocs Blog Pricing Product Cloud Trunk Roadmap Changelog\n\n###### Connect\n\nGithub Twitter LinkedIn Youtube Tembo Slack Trunk Slack\n\n###### Resources\n\nCareers\n\nPrivacy policy Terms of service\n\nSOC2 Type 1 compliant\n\n", "frontpage": true}
