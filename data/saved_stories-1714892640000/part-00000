{"aid": "40261550", "title": "Understanding Stein's Paradox (2021)", "url": "https://joe-antognini.github.io/machine-learning/steins-paradox", "domain": "joe-antognini.github.io", "votes": 5, "user": "robertvc", "posted_at": "2024-05-05 01:01:05", "comments": 0, "source_title": "Understanding Stein's paradox \u2013 Joe Antognini", "source_text": "Understanding Stein's paradox \u2013 Joe Antognini\n\n# Joe Antognini\n\n\u2630\n\n# Understanding Stein's paradox\n\nJanuary 2, 2021\n\n## The paradox\n\nStein\u2019s paradox is among the most surprising results in statistics. The basic\nidea is easily stated, but it is difficult to understand how it could possibly\nbe true. The premise is this: suppose that I have a Gaussian distribution with\na variance of unity and some mean which I don\u2019t tell you. I then draw a single\nsample from this distribution, give it to you, and ask you to guess the mean.\nWhat do you do? Well, you don\u2019t have a lot of information to go on here, so\nyou just guess that the mean is the number I gave you. This is a good guess!\n(We will make the notion of a \u201cgood guess\u201d a little more precise later on.)\n\nNo big surprise there. Now we play again, but this time, my distribution is a\ntwo-dimensional Gaussian. The covariance is the identity matrix (so this is\nequivalent to sampling from two independent one-dimensional Gaussians). But\nagain I have not told you the mean (which is now a two-dimensional vector).\nOnce more I draw a single sample from the distribution, hand it over to you,\nand ask you to guess the mean. You simply guess that the mean is the sample I\nhave given you. Once more you have guessed well!\n\nNow we do the same thing in three dimensions. I draw a single sample, hand it\nover to you, and ask you to guess the mean. Just as before, you guess that the\nmean is the sample I gave you. But this is no longer a good guess! Stein\u2019s\nparadox is that if we play this game in three dimensions or more, a better\nguess is to say that the mean is this:\n\n\u02c6\u03bc=ReLU(1\u2212D\u22122|x|2)x,\n\n\u03bc^=ReLU(1\u2212D\u22122|x|2)x,\n\nwhere DD is the dimensionality of the Gaussian, and xx is the sample drawn\nfrom the distribution. This is the so-called \u201cJames-Stein estimator.\u201d\n\nWho would have thought! What is going on here?\n\n## What makes a guess good?\n\nBefore we go on, we should clarify exactly what we mean by a \u201cgood guess.\u201d We\nare trying to do what is called \u201cparameter estimation\u201d in statistics \u2014 based\non a sample from a distribution, we want to infer some underlying parameter\n(or parameters) of the distribution. (In this case the parameter we are\ninterested in is the mean.) In order to quantify how good or bad our estimate\nis we choose a function called a \u201closs function.\u201d There is some freedom in\nchoosing a loss function, but the mean squared error is a common choice and\nhas a lot of valuable properties. Stein\u2019s paradox assumes that we are using\nthe mean squared error. So if we guess that the mean is \u02c6\u03bc\u03bc^ and the true\nvalue of the mean is \u03bc\u03bc, then the loss is\n\nL=|\u02c6\u03bc\u2212\u03bc|2.\n\nL=|\u03bc^\u2212\u03bc|2.\n\nNow, of course, we need some rule to go from the sample xx to the estimate\n\u02c6\u03bc\u03bc^. This rule is just a function of some kind, say, f(x)f(x). This function\nhas the special name of an \u201cestimator.\u201d We can choose whatever function we\nwant here. Our original guess was to just use f(x)=xf(x)=x. But another choice\nhere is to say f(x)=x+7f(x)=x+7, or f(x)=sin(x)/x71f(x)=sin(x)/x71, or even\njust f(x)=31f(x)=31. It doesn\u2019t take much imagination to see that there are an\ninfinite number of possible choices. But presumably some of these choices are\nbetter than others. How do we know which ones are good?\n\nStatisticians use the concept of risk for this purpose. Risk is simply the\nexpected value of your loss function. One thing that can be a little confusing\nis that the risk is a function of both your choice of estimator and the true\nvalue of the parameter itself. So in the original game where you\u2019re guessing\nthe mean of a one-dimensional Gaussian, the risk will be a function of\nwhatever rule you decide to use and the actual, unknown value of the mean.\n\nThe fact that the risk is a function of the true value of the parameter makes\nthings a little tricky. If you\u2019re trying to decide between two estimators, you\nmight find that one estimator works better for certain values that the\nparameter can take, and the other works better for others. As a dumb example,\nlet\u2019s go back to guessing the mean of a one-dimensional Gaussian. Our original\nestimator was \u02c6\u03bc=x\u03bc^=x. But another, perfectly valid, estimator is \u02c6\u03bc=7\u03bc^=7.\nIn other words we ignore the sample entirely and say that the mean is 7 no\nmatter what. Generally this doesn\u2019t seem like a smart thing to do. But if the\nmean turns out to actually be pretty close to 7, on average this will be the\nbetter guess! Specifically, the risk of our initial estimator is\n\nRx=E[(x\u2212\u03bc)2]=1\u221a2\u03c0\u222b(x\u2212\u03bc)2e\u2212(x\u2212\u03bc)2/2dx=1.\n\nAnd the risk on our second, dumb estimator is\n\nRdumb=E[(7\u2212\u03bc)2]=1\u221a2\u03c0\u222b(7\u2212\u03bc)2e\u2212(x\u2212\u03bc)2/2dx=(7\u2212\u03bc)2.\n\nAs long as the true mean, \u03bc, happens to be between 6 and 8, the dumb estimator\nof just saying 7 actually has lower risk!\n\nBased on this example it might seem that we\u2019re stuck. Since we don\u2019t know the\ntrue value of the mean, we can\u2019t generally say if one estimator is better than\nanother. And indeed this is often the case. But there are certain situations\nwhere this is not true. If we have two estimators and one of them has a lower\nrisk for any possible value the parameter can take, we can say that one is\ndefinitively better than the other. In statistical parlance, we say that the\nworse estimator is \u201cinadmissable.\u201d\n\nIn more precise terms, Stein\u2019s paradox states that in three dimensions or\nmore, the naive estimator (just guessing that the mean is x) is inadmissable\nbecause the risk of the James-Stein estimator is lower for any possible mean I\ncould choose.\n\n## What is the James-Stein estimator doing?\n\nBefore we can understand why the James-Stein estimator gives you a better\nguess than the naive estimator x, we should understand what exactly it\u2019s\ndoing. The idea is that we take the naive guess x and then we scale it towards\nthe origin by some amount. The factor by which we scale it is:\n\nReLU(1\u2212D\u22122|x|2).\n\nThe ReLU function simply takes the maximum of its argument and zero, so this\nscale factor will be either positive or zero. Let\u2019s suppose that it\u2019s\npositive. In this case we scale it towards the origin more if the magnitude of\nthe sample is smaller and less if it is lower. In the limit of |x|\u2192\u221e we don\u2019t\nchange it at all and our guess reduces to the naive estimator x.\n\nIn the other limit, if |x| is very small, then the ReLU function will kick in\nand just set the scale factor to zero. Hence anytime we get a sufficiently\nsmall sample we throw it out and just guess that the mean is zero instead. And\nall else being equal we will shrink our estimate more in higher dimensional\nspaces than in lower dimensional spaces.\n\nSo that is what the James-Stein estimator is doing. Why does it work? Before\nwe can answer that we need to take a quick detour.\n\n## Samples in high dimensional spaces\n\nHigh dimensional spaces are counterintuitive. One of the counteruintitive\nproperties of high dimensional distributions is this: a sample from a\nsymmetric high dimensional distirbution is highly likely to be further from\nthe origin than the mean. Specifically, for an isotropic D-dimensional\nGaussian, the difference between the average distance to a sample and the\ndistance to the mean grows as \u223c\u221aD.\n\nIt\u2019s a little strange to put it this way, but isn\u2019t so surprising with a\nlittle bit of thought. Even in two dimensions we can see that this is the case\njust by drawing it:\n\nThe shaded area of the circle is less than half the area, so we are less\nlikely to choose a sample closer to the origin than the mean. What perhaps\nmakes this counterintuitive is that in two dimensions a fairly large fraction\nof the circle is still shaded. But as the dimensionality increases, this\nfraction decreases exponentially. Once the dimensionality is even moderately\nlarge we are highly unlikely to sample a point in this shaded region.\n\nOne caveat here is that this effect decreases the larger the mean is. You can\nimagine that as we move the circle further away from the origin, the shaded\nfraction gets closer and closer to 1\u20442. So long as the mean is sufficiently\nlarge, the probability of sampling a point closer to the origin than the mean\ncan be close to 1\u20442 even in high dimensional spaces; it just requires a very\nlarge mean. We can start to see here the connection to the James-Stein\nestimator, which also gets very close to the naive estimator x as the mean\n(and hence |x|) gets very large.\n\nWhat we are doing by shrinking the estimate towards the origin is correcting\nfor the tendency of the typical sample to be slightly further away from the\norigin than the mean. This correction allows us to reduce the overall risk of\nthe estimate. [^1]\n\n## How arbitrary is the origin, really?\n\nStein\u2019s paradox is particularly strange because there are actually two\ncounterintuitive things going on:\n\n  1. The origin is arbitrary, so why does moving your estimate towards the origin help?\n  2. Why does this not work in one or two dimensions?\n\nLet\u2019s take a look at the first of these. A central principle in physics is\nthat of relativity \u2014 coordinate systems are arbitrary, so the laws of physics\nmust be valid in all of them. Surely this is also true in statistics as well.\nWe can choose the origin to be wherever we like, so it cannot contain any\ninformation. But this sensible assertion is false. Statistics is not physics.\n\nIf we truly had no information about the mean, what value would the sample\nhave? If it could really be anything then presumably its value would be\nexceedingly large. After all, there are a lot of numbers, almost all of them\nare too big to write down, and the mean could be absolutely any of them. But\nif we pick a sample and find that its distance from the origin is 3.72 there\nmust be something fishy going on. Clearly we have, in fact, managed to embed\nsome information in our choice of coordinate system. The only reason we have\ngotten out sensible values in our sample is because we have some prior as to\nwhere the mean is expected to be.\n\nIn fact, in the limit of no information |x| will be infinitely large and the\nJames-Stein estimator will reduce to the naive estimator, x. So it was our\nclever choice of origin that smuggled information into our estimator and\nallowed us to do better than the naive estimate.\n\nIt\u2019s important to distinguish between the direction of the origin and the\ndistance between the origin and the mean. The direction of the origin is\nunimportant. In fact, we could shrink our estimate in any direction and we\nwould still get the benefits of the James-Stein estimator. The important thing\nis the distance between the origin and the mean \u2014 this is what has encoded\nsome prior information that we can exploit to make a better prediction.\n\n## The bias-variance tradeoff\n\nThe way the James-Stein estimator works can be understood by looking at the\nbias-variance tradeoff. The bias-variance tradeoff states that the risk of an\nestimator can be decomposed into two components: a constant \u201cbias\u201d term, which\nreflects how far off the average value of the estimate is from the correct\nvalue; and an unbiased \u201cvariance\u201d term, which accounts for the randomness of\nthe sample.\n\nThe naive estimator x is unbiased but has a high variance. One reason that\nStein\u2019s paradox seems unnatural is that we tend to confuse unbiased estimators\nwith estimators that minimize risk. But in high dimensional spaces, samples\nfrom an isotropic Gaussian encompass a tremendous volume. Although our naive\nestimate is unbiased it has very high variance.\n\nWhat the James-Stein estimator does is scale the overall distribution towards\nthe origin, thereby shrinking the volume of the distribution (and hence its\nvariance), at the cost of introducing a little bit of bias. Although the\nestimator is now biased its overall risk is lower.\n\n## Deriving the James-Stein estimator geometrically\n\nAt this point we have some idea as to why shrinking the estimate towards the\norigin could be helpful, but we haven\u2019t yet figured out why the specific form\nof the James-Stein estimator seems to work. To do this we\u2019ll follow an\nargument presented by Brown & Zhao (2012).\n\nRemember that the direction of the coordinate system is arbitrary. So let\u2019s\nrotate into a new one where one axis points directly toward the mean, and the\nother D\u22121 axes are pointed in arbitrary (orthogonal) directions. (Of course,\nyou can\u2019t do this because you don\u2019t know where the true mean is. But I do, and\nI\u2019ve decided to help you out.) In this coordinate system the mean is just\n(|\u03bc|,0,...,0). Let\u2019s write the sample in this coordinate system as\n(\u03b61,\u03b62,...,\u03b6D).\n\nThe loss for our guess can now be broken into two orthogonal components: one\ncomponent, (\u03b61\u2212|\u03bc|)2, which tells us how far off our estimate is from the true\nvalue when it\u2019s projected onto the correct direction of the mean; and a second\ncomponent, \u2211Di=2\u03b62i, which tells us how far off our estimate is from this\ncorrect direction. Let\u2019s call this second component the \u201cresidual\u201d component\nand define it as\n\n\u03c1\u2261\u221aD\u2211i=2\u03b62i.\n\nOur underlying distribution is isotropic, so rotating the coordinate system\ndoesn\u2019t change the fact that each coordinate \u03b6i is a sample from an\nindependent one-dimensional Gaussian with unit variance. And if i>1 then the\nmean of this distribution is zero as well. \u03c1 therefore follows a \u03c7\ndistribution with D\u22121 degrees of freedom.\n\nLet\u2019s suppose that we\u2019ve gone and chosen a sample from our distribution and by\ngood fortune \u03b61 happens to be exactly equal to |\u03bc|. In general the rest of the\n\u03b6i in the sample won\u2019t be exactly zero and so \u03c1 will be positive. If we plot\n\u03b61 on the x-axis and \u03c1 on the y-axis, the situation will look like this:\n\nThe sample from our distribution is the point at the top right of the\ntriangle. If we just use this sample as our estimate of the mean then the loss\nof this estimate will simply be the squared distance between the point and the\nbottom right corner of the triangle, or \u03c12.\n\n### How can we transform the sample?\n\nWe only have two points in this problem: the origin, and the sample x. By\nsymmetry this means that the only way by which we can make an estimate is to\nchoose a point somewhere on the line between the origin and x. [^2] In other\nwords, you don\u2019t actually know what direction any of the \u03b6i are in so you\ncan\u2019t simply move your guess down the right side of the triangle to reduce \u03c1.\nThe only direction you are allowed to move your sample is along the\nhypoteneuse of this triangle.\n\n### How should we transform the sample?\n\nThe beauty of the James-Stein estimator is that even though we are constrained\nto move the sample along the hypoteneuse, we can nevertheless reduce the\ndistance between our guess and the true mean by shrinking it until the\ndirection between our guess and the mean is perpendicular to the hypoteneuse.\n\nSome simple geometry reveals that this new point is located at:\n\n(1\u2212\u03c1|\u03bc|2+\u03c12)x=(1\u2212\u03c1|x|2)x\n\nThis is looking like the beginnings of the James-Stein estimator!\n\nWhat exactly is \u03c1? Unfortunately \u03c1 is random variable, but let\u2019s represent it\nby some central point of its distribution. Now \u03c1 follows a \u03c7 distribution with\nD\u22121 degrees of freedom and the mode of this distribution is D\u22122. So if we\nsimply represent this distribution by its mode, our estimator becomes\n\n(1\u2212D\u22122|x|2)x,\n\nwhich is the James-Stein estimator without the ReLU.\n\nTo be clear, this is a very hand-wavey argument. Representing the entire\ndistribution by a single point is not particularly sophisticated, and there is\nno special reason to choose to use the mode instead of the mean or median\nexcept that it happens to give an answer that corrseponds to the James-Stein\nestimator. [^3]\n\n### Can we derive the James-Stein estimator rigorously?\n\nGiven how hand-wavey this argument was, you might wonder whether it\u2019s possible\nto rigorously derive the James-Stein estimator by explicitly calculating the\nrisk given the Gaussian and \u03c7 distributions, and then finding the function\nthat minimizes the risk using the calculus of variations. However this\napproach will not work. The reason is that the James-Stein optimizer does not\nactually minimize the risk! In fact, despite its fame, the James-Stein\nestimator is itself inadmissible.\n\nAs we\u2019ll see below, we can improve our estimator by wrapping it in a ReLU\nfunction. But the ReLU function introduces some difficulties, because there is\na very general theorem which states that any admissible estimator must be a\nBayes estimator for some prior, or the limit of Bayes estimators. [^4] This\nends up implying that the estimator must be a smooth function and the ReLU\nfunction is not smooth. So while the James-Stein estimator beats the naive\nestimator x, there exists some other estimator which beats it!\n\n### Recovering the ReLU\n\nThere is one last piece to tackle: the ReLU function. Where does that come\nfrom?\n\nThe ReLU has no effect on our estimate as long as |x|\u2265D\u22122. This will generally\nbe the case if |\u03bc| is large. But if |\u03bc| is small, then some of the points\nsampled will have |x|<D\u22122. If we were to just use the same scaling we derived\nabove, we would reflect our estimate through the origin and make it negative!\nThis estimate has a worse loss than an estimate at the origin.\n\nThe reason for this is that once we have shrunk the sample all the way down to\nthe origin we have already traded away all the variance for bias. In other\nwords, we started with an estimate that had no bias and high variance, and\nended up with an estimate that had high bias and no varaiance. But if we were\nto keep going past the origin, we\u2019d continue to increase the bias but would\nstart to increase the variance as well! This is strictly worse than keeping\nour estimate at the origin, so we are better off clamping the estimate at zero\nwith a ReLU function.\n\n## Why does Stein\u2019s paradox not hold in two dimensions?\n\nLet\u2019s now turn to the second counterintuitive property of Stein\u2019s paradox:\nwhat\u2019s so special about three dimensions? It is not hard to see why the James-\nStein estimator doesn\u2019t help in one dimension \u2014 we arrived at the James-Stein\nestimator by separating the sample into two components, one along the\ndirection to the true mean, and the other as a residual component\nperpendicular to the first. But in one dimension there is no residual\ncomponent! By shrinking your estimate towards the origin you introduce some\nbias, but this is not counterbalanced by the reduction in variance.\n\nBut what about the two dimensional case? Here again we unfortunately must wave\nour hands. In the two dimensional case we do, in fact, reduce the variance by\nshrinking our estimate towards the origin, just not by enough to offset the\nbias we introduce. While the orthogonal component, \u03c1, is not identically zero\nas it was in the one-dimensional case, it is strongly bunched up close to zero\nin the two dimensional case. The idea of the James-Stein estimator is to note\nthat a certain proportion of a sample\u2019s magnitude is due to a component\northogonal to the mean and to shrink the estimate to reduce that othogonal\ncomponent somewhat. But if the magnitude of the orthogonal component is too\nsmall, this won\u2019t always work. There are some values of the mean for which the\nJames-Stein estimator is better, but others where it is worse. It is not the\nbetter estimator no matter what.\n\n### A digression on random walks\n\nAs an aside, there is a deep correspondence found by Lawrence Brown between\nrandom walks and the admissibility of the naive estimator, x. Brown showed\nthat the naive estimator is admissible if and only if a random walk returns to\nthe origin an infinite number of times. Random walks in one or two dimensions\ndo this, but random walks in three or more dimensions do not. At a high level\nthis is because the random walk drifts away from the origin at a rate linear\nin the dimensionality of the space, but the volume subtended by the origin at\na fixed distance decays exponentially with dimension. In one or two dimensions\nthe volume subtended by the origin is large enough that you are guaranteed to\neventually make your way back to it. But in higher dimensions, the volume\nsubtended is smaller, so that as time progresses you are less and less likely\nto make your back and the probability approaches zero even in the limit of an\ninfinite number of steps.\n\nWhile rigorously proving this correspondance is non-trivial, both problems\nhave at their core a comparison between the distance between a point and the\norigin and the volume of the unit sphere in the space.\n\n## There be dragons far from the origin\n\nSo that, at a high level, is why the James-Stein estimator works. We\u2019ve been\nfocused in this post on the simplest form of Stein\u2019s paradox, which is a bit\nof a toy problem. But the artificiality of the problem shouldn\u2019t befog us as\nto the significance of these ideas. The artificiality eases the analysis, but\nin fact the concepts that Stein\u2019s paradox illustrates run much deeper through\nstatistics and machine learning.\n\nAs we discussed earlier on, while we usually don\u2019t think that there is\nanything special about our choice of origin, we do neveretheless use it to\nsmuggle in some priors, even if those priors are weak and we perhaps do so\nunwittingly. This is also true of machine learning models, maybe even more so.\nFor most machine learning models the origin really is special. For example, at\nthe origin most garden-variety neural networks will exhibit particularly\nsimple behavior \u2014 they will output all zeros indpendent of the input. [^5] Any\nmovement away from this special point will introduce complexity into the\nfunctional behavior of the model. Neural networks are, of course, non-linear,\nso it is not always true that points further from the origin in parameter\nspace are more complicated, but it is generally true.\n\nMost ML practitioners appreciate that techniques like L2 regularization have\nthe effect of making models simpler, and therefore less likely to overfit. But\nStein\u2019s paradox dramatically illustrates the relationship between the risk of\nan estimate and the dimensionality of the space \u2014 in high dimensional spaces\nthere is much more volume further away from the origin than closer to it. We\nare oftentimes better off introducing bias in order to reduce variance because\nin high dimensional spaces shrinking a small amount towards the origin reduces\nan enormous volume of parameter space. In other words, for a large machine\nlearning model, there are vastly more ways to overfit than there are to\nunderfit so we should bias our models towards underfitting because that is a\nsimpler problem to solve than overfitting. (Or maybe to put it another way,\nunderfitting is a small number of problems, whereas overfitting is a\nstupendous number of problems.)\n\nThis phenomenon can manifest itself during neural network training. Suppose we\nhave converged on a local minimum in the loss landscape after training with\nsome varaint of stochastic gradient descent. At convergence there will be\nequality between the stochastic component of SGD, for which the random walk is\ncausing the parameters to drift away from the minimum, and the true gradient,\nwhich is pushing the parameters towards the minimum. [^6] At this point the\nneural network is wandering on some high-dimensional ellipsoid around the\nminumum. But by the nature of high dimensional spaces, the neural network will\nalways be further from the origin than the minimum, and therefore on the side\nof too much complexity. Thus this converged model will always slightly\noverfit.\n\nThe further the model recedes from the origin, the more inexplicable its\nfunctional behavior can become. The moral of this story is that far from the\norigin there be dragons, and in a high dimensional space, moving just a little\nfurther away from the origin introduces lots and lots of dragons. The James-\nStein estimator advises you to stay close to the origin to keep your risk down\nand the dragons at bay.\n\n## Footnotes\n\n  1. It\u2019s important to note that this does not imply that our original estimate is biased. In fact the naive estimator is unbiased and shrinking it towards the origin introduces bias. It is the distance of the naive estimator from the origin that is biased high. But by correcting for this bias we happen to reduce the risk. \u21a9\n\n  2. While there\u2019s nothing stopping us from choosing some other random direction, this adds no new information and so is effectively the same as choosing a different origin. \u21a9\n\n  3. The median and mean of the \u03c7 distribtion are more complicated but are asymptotically equal to the mode in the limit of a large number of degrees of freedom. But the asymptotic limit does not help you prove the case for D=3.\n\nIn fact Brown & Zhao (2012) chose to use the mean instead and derive a\n(D\u22121)/|x|2 term instead of (D\u22122)/|x|2 as we do from the mode. They then argue\nthat the extra \u22121/|x|2 is due to the variance in \u03b61. I am not fully convinced\nof this argument; it seems to me that once one has represented the entire\ndistribution by a central point one is already waving one\u2019s hands. \u21a9\n\n  4. A Bayes estimator is determined by a particular choice of prior. A \u201climit of Bayes estimators\u201d is found by taking the limit of the Bayes estimators found from a sequence of priors as the sequence tends to infinity. \u21a9\n\n  5. Of course the origin is not the only point that exhibits this behavior. I speculate that if we shrank to any point for which the model had functional simplicity we would observe the same benefits as shrinking towards the origin. \u21a9\n\n  6. Learning rate decay also mitigates this issue. \u21a9\n\n  * Contact Me\n  * @joe_antognini\n\n\u00a9 2024 Joe Antognini. Powered by Jekyll using the Balzac theme.\n\n", "frontpage": true}
