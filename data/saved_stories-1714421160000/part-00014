{"aid": "40196590", "title": "Unleashing Resilience: A Guide to Chaos Engineering in Serverless Architectures", "url": "https://www.ranthebuilder.cloud/post/unleashing-resilience-a-practical-guide-to-chaos-engineering-in-serverless-architectures", "domain": "ranthebuilder.cloud", "votes": 1, "user": "cebert", "posted_at": "2024-04-29 10:25:03", "comments": 2, "source_title": "Unleashing Resilience: A Practical Guide to Chaos Engineering in Serverless Architectures", "source_text": "Unleashing Resilience: A Practical Guide to Chaos Engineering in Serverless\nArchitectures\n\ntop of page\n\nSearch\n\n  * koby aharon\n  *     * Apr 15\n    * 11 min read\n\n# Unleashing Resilience: A Practical Guide to Chaos Engineering in Serverless\nArchitectures\n\nA Practical Guide to Chaos Engineering in Serverless Architectures\n\nChaos engineering is a proactive methodology that intentionally introduces\ncontrolled disruptions and failures into a system to uncover weaknesses,\nenhance resilience, and ensure robust performance in the face of unforeseen\noutages.\n\nIn this post, we will explore chaos engineering practices and provide an\nexample code for running a chaos experiment within a serverless architecture\ndeployed in AWS.\n\nAdditionally, we will leverage AWS FIS (Fault Injection Service) for enhanced\nexperimentation.\n\nThis is the second part of a series of posts discussing chaos engineering in\nserverless architectures. It applies the concepts introduced in Part 1 of this\nseries.\n\n## Guest writer introduction\n\nKoby Aharon is a Principal Software Architect at CyberArk. He is a hands-on\nprofessional engineer who loves using technology to solve complex problems.\n\nYou can follow Koby on his LinkedIn and Medium accounts.\n\n## Table of Contents\n\n  1. Chaos Engineering Definition\n\n    1. Chaos Experiment Steps\n\n  2. Preparing for Our First Experiment\n\n    1. Defining the Experiment\n\n    2. AWS FIS Concepts\n\n  3. Experiment Steps\n\n    1. 1 - Form a Hypothesis\n\n    2. 2 - Introduce Faults (Chaos)\n\n    3. 3 - Observe\n\n    4. 4 - Improve\n\n  4. Unleash Chaos Onto Our Architecture\n\n    1. Validating the Fix\n\n  5. Summary\n\n## Chaos Engineering Definition\n\nCloud providers like AWS manage serverless architectures, but ensuring that\nyour serverless applications are resilient is still essential. For instance,\nin an outage in one region, you want to ensure that your architecture is\nresilient and that your customers in other regions are unaffected.\n\nChaos engineering can help you simulate these outages and validate the\nresilience of your serverless applications. Following its principles, you can\nconduct chaos experiments that simulate a region failure and verify the system\nis functioning as expected.\n\nBefore diving into code, let\u2019s briefly recap our sample architecture and the\nsteps of an experiment we covered in Part 1.\n\nWe presented the following architecture:\n\nPicture 1: Sample Serverless Architecture\n\nThe architecture diagram presents a classic serverless architecture that runs\non top of AWS. It consisted of the following resources:\n\n  * API hosted in Amazon API gateway. Our API is deployed into two regions, us-east-1 and eu-west-1, and exposes two paths: \u2018/api/health\u2019 for health checking and \u2018/api/hello\u2019 as our service API. Our health returns \u201cRegion {region} is healthy,\u201d while our service API returns: \u201cHello from region {region}\u201d (region marks the endpoint that handled the request, either \u2018us-east-1\u2019 or \u2018eu-west-1\u2019).\n\n  * There are two Lambda functions in each region. Our API triggers the functions (we have a dedicated Lambda per API). There is more on using Lambda with AWS API Gateway here.\n\n  * Custom domain registered in Amazon Route 53. Every regional API will get a dedicated custom domain stating the relevant region. For example: \u201cexample.{region}.mydomain.com\u201d (the region should be either \u2018us\u2019 or \u2018eu,\u2019 e.g., \u201cexample.us.mydomain.com\u201d).\n\n  * Route53\u2019s Latency-based routing points to our endpoints in both regions. The entry address should match \u201cexample.mydomain.com.\u201d\n\n  * Route53\u2019s health check to check the health of our APIs. Every regional API will have a dedicated health check that points to the relevant domain address: \u201chttps://exmaple.{region}.mydomain.com/api/health.\u201d\n\n### Chaos Experiment Steps\n\nWe presented the four steps of a chaos experiment:\n\n  1. Form a hypothesis (plan)\n\n  2. Introduce stress/faults (do)\n\n  3. Observe (check)\n\n  4. Improve (act)\n\nAnd defined the following hypothesis:\n\nGiven: We have a multi-region API gateway behind latency-based routing.\n\nHypothesis: Failure in one region does not affect other regions. Moreover,\nRoute 53 failover should promptly detect the issue and reroute traffic to the\nhealthy region.\n\nThis post will present how to experiment with AWS using AWS FIS to test our\nhypothesis. During our experiment, we will use a Lambda extension to inject\nfaults into our Lambda functions (more on this below). See the first part for\nmore information on the different approaches to injecting faults.\n\nAfter presenting our hypothesis and explaining how we will inject faults into\nthe Lambda function, we will start our experiment by first understanding how\nto validate our hypothesis during our Chaos experiment.\n\n## Preparing for Our First Experiment\n\nBefore running our experiment, it is essential to validate that the\napplication behaves as expected and simulate user behavior within the system.\nAdditionally, determining how the system functions from a customer\u2019s point of\nview is crucial.\n\nMore specifically, if the US region is down, we need to be able to verify that\nit is down and that our EU customers are unaffected.\n\nWe will use Amazon CloudWatch to validate the application\u2019s behavior and\ncreate a dashboard with a mixture of built-in and custom metrics. Using those\nmetrics, we can monitor our application from both a system operator\u2019s and a\ncustomer\u2019s point of view. The metrics and dashboard are central to our\nexperiment\u2019s \u201cobserve\u201d step. We will talk about those metrics in detail later\nin this post.\n\n### Defining the Experiment\n\nOur experiment has two main parts. The first simulates customer behavior,\nwhile the second runs it and injects faults.\n\nCustomer Behavior Simulator: Simulating customer behavior is crucial as we\nwant to first test our hypothesis in a controlled environment without\naffecting real customers. To simulate customer behavior, we will use an\nupdated version of this application called \u201cload-gen,\u201d taken from AWS Chaos\nEngineering Workshop. This application can be deployed as a Lambda function\nand invoked to simulate load on a given API. For more information on the\napplication and how to use it, see AWS Chaos Engineering Workshop.\n\nRunning our Experiment: We will use AWS FIS (Fault Injection Service) to run\nour experiment. AWS FIS is a fully managed service for running fault injection\n(Chaos) experiments in AWS. Unlike external tools, FIS injects faults directly\nthrough the AWS control plane, allowing it to mimic real-world issues specific\nto AWS services.\n\nIn addition to AWS FIS, we will use AWS System Manager, a service that manages\nyour AWS resources and applications on-premises or in the cloud. We will write\na custom automation document in AWS System Manager and use it from FIS. We\nwill go into more detail later in this post.\n\n### AWS FIS Concepts\n\nHere\u2019s a breakdown of key AWS FIS concepts and how to leverage them for chaos\nengineering in our serverless environment:\n\n  * Experiment Template: This blueprint defines our experiment. It specifies the resources (e.g., Lambda functions) we want to target, the faults to inject (latency, errors), and stop conditions (duration, error thresholds).\n\n  * Actions: These are specific ways to disrupt our resources, like throttling network traffic or introducing delays. FIS provides pre-built actions or allows us to define custom ones. See here for a list of supported actions.\n\n  * Targets: We want to inject faults into these AWS resources, like our Lambda functions.\n\n  * Stop Conditions: Define when the experiment should automatically stop. You can base the stop condition on a time limit, specific errors encountered, or exceeding resource utilization thresholds.\n\nNow that we understand the services we will use to conduct our experiment\nlet\u2019s review the experiment stages and explain how we will use AWS FIS.\n\n## Experiment Steps\n\n### 1 - Form a Hypothesis\n\nQuick reminder: here is our hypothesis:\n\nGiven: We have a multi-region API gateway behind latency-based routing.\n\nHypothesis: Failure in one region does not affect other regions. Moreover,\nRoute 53 failover should promptly detect the issue and reroute traffic to the\nhealthy region.\n\n### 2 - Introduce Faults (Chaos)\n\nOne can choose to inject different fault types into a Lambda function. This\npost will use the Latency fault type, which is simple and intuitive. This\nfault type introduces artificial delays in the Lambda execution by adding\nlatency, which eventually causes failures due to timeouts. More on this below.\n\nAs of writing this blog, AWS FIS does not have pre-built actions for Lambda\nservice, so we must resort to other approaches. We will use a Lambda extension\nto inject a Latency fault into our Lambda. Once connected to our Lambda, we\ncan enable it by setting our Lambda environment variables and configuring it\nto add artificial delays during the Lambda invocation. You can read more on\nthe extension in the official README file.\n\nTo connect the extension to our Lambda, we will use FIS\u2019s aws:ssm:start-\nautomation-execution action, which enables us to invoke an AWS System Manager\nautomation document. For our purposes, we will develop an automation document\nthat will:\n\n  * Get a Lambda and a Lambda extension ARN\u2019s as parameters.\n\n  * Attach the extension to the Lambda.\n\n  * Enable fault injection (Chaos) by setting the Lambda\u2019s environment variables.\n\n  * (Optional) Update a Lambda alias to point to our new version (more on this below).\n\nSee here for more information about AWS System Manager automation and here for\nmore information about writing custom automation documents.\n\nYou can write automation documents in either JSON or YAML format. See the\nfollowing YAML snippet for the list of parameters we will use in our document:\n\nAs you can see in the snippet, we have the following parameters:\n\n  * FunctionName \u2014 The name of the lambda function to add the layer.\n\n  * LayerArn \u2014 The ARN of the layer to add to the lambda function.\n\n  * AutomationAssumeRole \u2014 The ARN of the role that allows Automation to perform the actions on your behalf.\n\n  * ChaosMode \u2014 (Optional) Whether we want to enable or disable Chaos (defaults to enable).\n\n  * AliasName \u2014 (Optional) The name of the lambda alias that needs to be updated. A Lambda alias is a pointer to a function version, and we usually use it when connecting a Lambda function to an API Gateway. If you don\u2019t use a Lambda alias, you can ignore this parameter and leave it empty.\n\nThe document takes the parameters and runs a Python script. You can see the\nfollowing snippet containing the \u201chandler\u201d function, which is the main part of\nour script:\n\nThis Lambda function connects the extension and adds environment variables to\nenable Chaos (inject latency fault).\n\nOur script also supports disabling Chaos (stopping the injection of faults) by\ndetaching the extension and removing the added environment variables.\n\nYou can view the full version of the SSM document here.\n\n#### FIS Experiment Definition\n\nAfter reviewing our automation document, let\u2019s revisit our FIS experiment\ntemplate, which is a blueprint that defines our experiment. You can see the\nfollowing example snippet of the FIS template we will use (partial):\n\nIn the snippet above, we can see our experiment contains the following actions\n(lines 4\u201335):\n\n  * \u201cS00_AttachExtensionToLambda\u201d (lines 5\u201313) - Enable Chaos in our Lambda by invoking our automation document. We will use the extension mentioned above while enabling the latency response type configured with 60 seconds. Enabling the latency response type will cause one of the following:\n\n  1. Function timeout - this will happen if we configure our Lambda to run under 60 seconds.\n\n  2. API Gateway API timeout \u2014 API Gateway has a maximum integration timeout of 30 seconds (hard limit).\n\n  * \u201cS01_Wait\u201d (lines 14\u201322) - Wait 10 minutes. This action is essential as we must wait for enough traffic to validate our hypothesis later.\n\n  * \u201cS02_DetachExtensionFromLambda\u201d (lines 23\u201334) - Disable Chaos by invoking our automation document with the \u201cChaosMode\u201d parameter set to \u201cDISABLED.\u201d This will revert our Lambda to its original state and complete the experiment.\n\nIt's important to note the snippet above focused on updating a single Lambda.\nHowever, in our case, we should extend it to inject faults to both Lambda\nfunctions in the US region: health check and service. Otherwise, we won\u2019t\nsimulate a regional failure in Lambda service.\n\nFor more information on AWS FIS experiment templates, see here.\n\n### 3 - Observe\n\nAs mentioned above, to verify our hypothesis during the experiment, we will\ncreate a CloudWatch dashboard showing a mixture of built-in and custom metrics\ncontaining a dimension. You can read more on custom metrics and dimensions\nhere. The dashboard should provide us visibility of both regions and present\nthe response code our customer receives. It will contain the following\nmetrics:\n\n  * Route53 HealthCheckStatus \u2014 Using this built-in AWS metric, we can verify that our endpoint is considered unhealthy from a system operator\u2019s point of view. We will monitor the metrics for our API in both regions.\n\n  * \u201cregion_{region}\u201d - a custom metric to mark the number of requests a particular region handles. We can calculate this value by parsing the response payload as our API returns \u201cHello from region {region}\u201d in case of an HTTP 200 response code.\n\n  * API invocation return code \u2014 several custom metrics to mark the HTTP response code returned from calling our API. We will have the following metrics:\n\n  1. status_2xx - number of 2xx responses.\n\n  2. status_4xx - number of 4xx responses.\n\n  3. status_5xx - number of 5xx responses.\n\nOur custom behavior simulator mentioned above (the \u201cload-gen\u201d application)\nwill publish our custom metrics and contain the invoked API as a dimension.\nUsing our custom metrics, we can understand if a customer experiences a\nfailure while calling our API and which API endpoint handled the request (\u2018us-\neast-1\u2019 or \u2018eu-west-1\u2019 region).\n\nFor example, in case our API URL is \u201chttps://example.mydomain.com\u201d and we get\nan HTTP 200 response code while calling the API by reaching the \u2018us-east-1\u2019\nregion, we will have the following metrics:\n\n  * region_us-east-1: API - \u201chttps://example.mydomain.com\u201d, value - 1\n\n  * status_2xx: API - \u201chttps://example.mydomain.com\u201d, value - 1\n\n  * status_4xx: API - \u201chttps://example.mydomain.com\u201d, value - 0\n\n  * status_5xx: API - \u201chttps://example.mydomain.com\u201d, value - 0\n\n### 4 - Improve\n\nOnce the experiment finishes running, we will view our dashboard and validate\nour hypothesis. We must fix any issue we discover and rerun our experiment.\n\n## Unleash Chaos Onto Our Architecture\n\nNow that we have reviewed all the steps needed to conduct our experiment, we\nwill go over the exact steps we will take to unleash Chaos:\n\n1\\. Deploy our sample architecture presented above.\n\n2\\. Start running our customer simulator (the load-gen application described\nabove). We can run it as a Lambda function in the EU and US regions while\nensuring both instances call the latency-based URL:\n\u201chttps://example.mydomain.com/api/hello.\u201d\n\n3\\. Open our CloudWatch dashboard and verify everything is working correctly.\nWe expect both our Route53 endpoints to be healthy and the clients in both\nregions to get an HTTP 200 response code.\n\n4\\. Start our FIS experiment.\n\n5\\. Once finished, look at our dashboard and validate our hypothesis. If we\ndon\u2019t see the expected behavior in the dashboard, we should check why, fix the\nproblem, and rerun the experiment by returning to step 2.\n\nAfter reviewing the steps we should take, let\u2019s look at the following\ndashboard that monitored an experiment we conducted:\n\nExperiment Dashboard \u2014 Error\n\nLet\u2019s review the widgets in the dashboard from left to right:\n\n  1. Health \u201cus-east-1\u201d - Displays the health of our US region endpoint (1 \u2014 healthy, 0 \u2014 unhealthy). This widget uses Route53 HealthCheckStatus.\n\n  2. \u201cus-east-1\u201d #requests\u2014Displays the number of requests that reached the US region. This widget uses our region value in our custom metric presented above.\n\n  3. \u201ceu-west-1\u201d #requests \u2014 Same as widget 2 for the EU region.\n\nThis dashboard shows a hidden problem we have. Pause for a second and think:\nWhat might be the problem in our case?\n\nWe have about 44 requests in our US and about 29 in our EU regions. Once we\nstart our experiment (marked in the red arrow), we see a decline in requests\nreaching the US region while the EU remains constant. We don\u2019t see an increase\nin requests reaching the EU region, as we expect Route 53 failover to kick in\nand route traffic from our unhealthy region.\n\nThe clue to the problem is that our US endpoint is considered healthy\nthroughout our experiment (widget 1); however, we expect it to be unhealthy.\nAs both endpoints are deemed healthy, Route53 keeps pointing US customers to\nthe US endpoint, causing them to fail instead of moving them to the EU region.\n\nWhat is the problem? We accidentally configured our US health check to point\nto \u201cexample.eu.mydomain.com\u201d instead of \u201cexample.us.mydomain.com.\u201d\n\nIt\u2019s great! We found a problem, and the experiment was beneficial!\n\nOur US customers would have experienced an outage during an actual regional\noutage instead of being moved to the European region.\n\nLet\u2019s fix it and rerun the experiment.\n\n### Validating the Fix\n\nAfter fixing the problem, let\u2019s look at the following dashboard:\n\nExperiment Dashboard - Success\n\nIn this dashboard, we added another widget (the last one in the first row),\npresenting the status codes returned by invoking our APIs. It uses our API\ninvocation custom metric mentioned above.\n\nWe see everything is working as expected: once we start our experiment (red\narrow), we see a decrease in us-east-1 requests with a matching increase in\nthe number of HTTP 5xx status codes (the last widget on the first row). After\na few minutes, Route53 failover kicks in, marking our US endpoint as unhealthy\n(green arrow in the first widget in the first row) and routing all traffic to\nthe EU region. We can validate it by seeing an increase in requests reaching\nthe EU region (the last widget in the second row) and a matching decrease in\nthe HTTP 5xx response codes (status codes component in the first row).\n\n## Summary\n\nThat\u2019s it for now. In this post, we took the example architecture and concepts\npresented in the first part and put them into practice. We ran an experiment\nusing AWS FIS and a custom SSM document, found a misconfiguration, and fixed\nit. We are now ready for a regional downtime.\n\nI hope you find this post (and the previous one) useful and practical and\nencourage you to test your serverless architectures. You might be surprised by\nwhat you\u2019ll see :)\n\n## Special Thanks\n\nI'd like to thank Ran Isenberg and Maxim Drobachevsky for taking the time to\nreview this post and providing their valuable feedback.\n\nTags:\n\n  * Serverless\n  * \u2022\n  * Lambda\n\n## Related Posts\n\nSee All\n\nAWS Lambda Cookbook \u2014 Part 1 \u2014 Logging Best Practices\n\nAWS Lambda Cookbook \u2014 Part 2 \u2014 Observability Best Practices\n\nIntroduction to Chaos Engineering in Serverless Architectures\n\nSubscribe\n\nBuy Me Coffee\n\n\u00a92024 by Ran The Builder, Ran Isenberg\n\nran.isenberg@ranthebuilder.cloud\n\nPrivacy Policy\n\nbottom of page\n\n", "frontpage": false}
