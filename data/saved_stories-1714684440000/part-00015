{"aid": "40236104", "title": "Learning Geometry-Guided Semantics for Exploring Unconstrained Photo Collections", "url": "https://tau-vailab.github.io/HaLo-NeRF/", "domain": "tau-vailab.github.io", "votes": 1, "user": "PaulHoule", "posted_at": "2024-05-02 13:37:23", "comments": 0, "source_title": "HaLo-NeRF: Learning Geometry-Guided Semantics for Exploring Unconstrained Photo Collections", "source_text": "HaLo-NeRF: Learning Geometry-Guided Semantics for Exploring Unconstrained\nPhoto Collections\n\nHaLo-NeRF: Learning Geometry-Guided Semantics for Exploring Unconstrained\nPhoto Collections\n\n# HaLo-NeRF\ud83d\ude07 Learning Geometry-Guided Semantics for Exploring Unconstrained\nPhoto Collections\n\n## Eurographics 2024\n\nChen Dudai*^1\n\nMorris Alper*^1\n\nHana Bezalel^1\n\nRana Hanocka^2\n\nItai Lang^2\n\nHadar Averbuch-Elor^1\n\n^* Denotes equal contribution\n\n^1Tel Aviv University ^2University of Chicago\n\nPaper Code Supplementary Data\n\n## TL;DR We learn a semantic localization field for textual descriptions over\ncollections of in-the-wild images depicting a large-scale scene.\n\nAs illustrated for St. Paul's Cathedral above, our approach enables generating\nnovel views with controlled appearances of these semantic regions of interest.\n\n## Abstract\n\nInternet image collections containing photos captured by crowds of\nphotographers show promise for enabling digital exploration of large-scale\ntourist landmarks. However, prior works focus primarily on geometric\nreconstruction and visualization, neglecting the key role of language in\nproviding a semantic interface for navigation and fine-grained understanding.\nIn more constrained 3D domains, recent methods have leveraged modern vision-\nand-language models as a strong prior of 2D visual semantics. While these\nmodels display an excellent understanding of broad visual semantics, they\nstruggle with unconstrained photo collections depicting such tourist\nlandmarks, as they lack expert knowledge of the architectural domain and fail\nto exploit the geometric consistency of images capturing multiple views of\nsuch scenes. In this work, we present a localization system that connects\nneural representations of scenes depicting large-scale landmarks with text\ndescribing a semantic region within the scene, by harnessing the power of SOTA\nvision-and-language models with adaptations for understanding landmark scene\nsemantics. To bolster such models with fine-grained knowledge, we leverage\nlarge-scale Internet data containing images of similar landmarks along with\nweakly-related textual information. Our approach is built upon the premise\nthat images physically grounded in space can provide a powerful supervision\nsignal for localizing new concepts, whose semantics may be unlocked from\nInternet textual metadata with large language models. We use correspondences\nbetween views of scenes to bootstrap spatial understanding of these semantics,\nproviding guidance for 3D-compatible segmentation that ultimately lifts to a\nvolumetric scene representation. To evaluate our method, we present a new\nbenchmark dataset containing large-scale scenes with ground-truth\nsegmentations for multiple semantic concepts. Our results show that HaLo-NeRF\ncan accurately localize a variety of semantic concepts related to\narchitectural landmarks, surpassing the results of other 3D models as well as\nstrong 2D segmentation baselines.\n\n## Neural 3D Localization Results of our Method\n\nWe demonstrate sample results from our HolyScenes benchmark (depicting Milan\nCathedral on top and Badshahi Mosque on bottom), visualizing segmentation maps\nover input images from test landmarks. As illustrated above, HaLo-NeRF\nsuccessfully segments regions corresponding to a given text input. Moreover,\nour method can differentiate between concepts with similar semantic\ndescriptions, such as Portal and Window\n\n## Method\n\nOur goal is to perform text-driven neural 3D localization for landmark scenes\ncaptured by collections of Internet photos. In other words, given this\ncollection of images and a text prompt describing a semantic concept in the\nscene, we would like to know where it is located in 3D space. These images are\nin the wild, meaning that they may be taken in different seasons, time of day,\nviewpoints, and distances from the landmark, and may include transient\nocclusions.\n\nIn order to localize unique architectural features landmarks in 3D space, we\nleverage the power modern foundation models for visual and textual\nunderstanding. Despite progress in general multimodal understanding, modern\nVLMs struggle to localize fine-grained semantic concepts on architectural\nlandmarks, as we show extensively in our results. The architectural domain\nuses a specialized vocabulary, with terms being rare in general usage.\n\nTo address these challenges, we design a three-stage system: (a) We extract\nsemantic pseudo-labels from noisy Internet image metadata using a large\nlanguage model (LLM). (b) We use these pseudo-labels and correspondences\nbetween scene views to learn image-level and pixel-level semantics. In\nparticular, we fine-tune an image segmentation model (CLIPSegFT) using multi-\nview supervision\u2014where zoomed-in views and their associated pseudo-labels\n(such as image on the left associated with the term \u201ctympanum\u201d) provide a\nsupervision signal for zoomed-out views. (c) We then lift this semantic\nunderstanding to learn volumetric probabilities over new, unseen landmarks\n(such as the St. Paul\u2019s Cathedral depicted on the right), allowing for\nrendering views of the segmented scene with controlled viewpoints and\nillumination settings.\n\n## Visualizations\n\nIn addition, we show below visualizations, comparing HaLo-NeRF (left) with the\nBaseline model (right), which uses the CLIPSeg model without finetuning. Both\nvideos show the same temporal sequence of RGB renderings, varying only in the\nprobabilities depicted (taken either from our model or the baseline). Note\nthat once zoomed-in, we turn off the probabilities for both models, allowing\nto better view the target semantic region. The target text prompt is written\nabove each video, with the name of the landmark on the right. As illustrated\nbelow, our model yields significantly cleaner probabilities that better\nlocalize the semantic regions, particularly for unique concepts that are less\ncommon outside of the domain of architectural landmarks. We also visualize the\nzoomed-in region with multiple appearance (for our model, keeping the\nappearance of the baseline model fixed). Results over additional prompts and\nlandmarks from the HolyScenes benchmark are illustrated in the main paper.\n\n#### \"Portals\" (a grand entrance to a cathedral), Notre-Dame Cathedral\n\nHaLo-NeRF Baseline\n\n\u276e \u276f\n\n## Citation\n\n    \n    \n    @InProceedings{dudai2024halonerf, author = {Dudai, Chen and Alper, Morris and Bezalel, Hana and Hanocka, Rana and Lang, Itai and Averbuch-Elor, Hadar}, title = {HaLo-NeRF: Learning Geometry-Guided Semantics for Exploring Unconstrained Photo Collections}, booktitle = {Proceedings of the Eurographics Conference (EG)}, year = {2024} }\n\n", "frontpage": false}
