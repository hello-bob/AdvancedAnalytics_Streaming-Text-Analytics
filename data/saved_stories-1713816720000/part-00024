{"aid": "40113196", "title": "Cria \u2013 Run AI locally and programmatically, with as little friction as possible", "url": "https://github.com/leftmove/cria", "domain": "github.com/leftmove", "votes": 2, "user": "anonyonoor", "posted_at": "2024-04-22 11:11:47", "comments": 0, "source_title": "GitHub - leftmove/cria: Run LLMs locally with as little friction as possible.", "source_text": "GitHub - leftmove/cria: Run LLMs locally with as little friction as possible.\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nleftmove / cria Public\n\n  * Notifications\n  * Fork 0\n  * Star 3\n\nRun LLMs locally with as little friction as possible.\n\npypi.org/project/cria/\n\n### License\n\nMIT license\n\n3 stars 0 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# leftmove/cria\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n4 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nleftmoveupdate readmeApr 22, 2024fea8810 \u00b7 Apr 22, 2024Apr 22, 2024\n\n## History\n\n15 Commits  \n  \n### .github\n\n|\n\n### .github\n\n| update readme| Apr 21, 2024  \n  \n### src\n\n|\n\n### src\n\n| update readme| Apr 22, 2024  \n  \n### tests\n\n|\n\n### tests\n\n| message history| Apr 22, 2024  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| update readme| Apr 22, 2024  \n  \n### LICENSE.md\n\n|\n\n### LICENSE.md\n\n| published package| Apr 21, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| update readme| Apr 22, 2024  \n  \n### poetry.lock\n\n|\n\n### poetry.lock\n\n| update readme| Apr 21, 2024  \n  \n### pyproject.toml\n\n|\n\n### pyproject.toml\n\n| update readme| Apr 22, 2024  \n  \n## Repository files navigation\n\nCria, use Python to run LLMs with as little friction as possible.\n\nCria is a library for programmatically running Large Language Models through\nPython. Cria is built so you need as little configuration as possible \u2014 even\nwith more advanced features.\n\n  * Easy: No configuration is required out of the box. Getting started takes just five lines of code.\n  * Concise: Write less code to save time and avoid duplication.\n  * Efficient: Use advanced features with your own ollama instance, or a subprocess.\n\n## Guide\n\n  * Quick Start\n  * Installation\n\n    * Windows\n    * Mac\n    * Linux\n  * Advanced Usage\n\n    * Custom Models\n    * Streams\n    * Closing\n    * Message History\n\n      * Follow-Up\n      * Clear Message History\n      * Passing In Custom Context\n    * Multiple Models and Parallel Conversations\n\n      * Models\n      * With\n      * Standalone\n    * Running Standalone\n  * Contributing\n  * License\n\n## Quickstart\n\nRunning Cria is easy. After installation, you need just five lines of code \u2014\nno configurations, no manual downloads, and no servers to worry about.\n\n    \n    \n    import cria ai = cria.Cria() prompt = \"Who is the CEO of OpenAI?\" for chunk in ai.chat(prompt): print(chunk, end=\"\") # The CEO of OpenAI is Sam Altman! ai.close() # Not required, but best practice.\n\nAnother example.\n\n    \n    \n    import cria with cria.Model() as ai: prompt = \"Who is the CEO of OpenAI?\" response = ai.chat(prompt, stream=False) # The CEO of OpenAI is Sam Altman!\n\nIf no model is configured, Cria runs the default model: llama3:8b. If the\ndefault model is not installed on your machine, Cria will install it\nautomatically.\n\nImportant: llama:8b is about 4.7GB, and will likely take a while to download.\n\n## Installation\n\n  1. Cria uses ollama, to install it, run the following.\n\n### Windows\n\nDownload\n\n### Mac\n\nDownload\n\n### Linux\n\n    \n        curl -fsSL https://ollama.com/install.sh | sh\n\n  2. Install Cria with pip.\n    \n        pip install cria\n\n## Advanced Usage\n\n### Custom Models\n\nTo run other LLMs, pass them into your ai variable.\n\n    \n    \n    import cria ai = cria.Cria(\"llama2\") prompt = \"Who is the CEO of OpenAI?\" for chunk in ai.chat(prompt): print(chunk, end=\"\") # The CEO of OpenAI is Sam Altman. He co-founded OpenAI in 2015 with...\n\nYou can find available models here.\n\n### Streams\n\nStreams are used by default in Cria, but you can turn them off by passing in a\nboolean for the stream parameter.\n\n    \n    \n    prompt = \"Who is the CEO of OpenAI?\" response = ai.chat(prompt, stream=False) print(response) # The CEO of OpenAI is Sam Altman!\n\n### Closing\n\nBy default, models are closed when you exit the Python program, but closing\nthem manually is a best practice.\n\n    \n    \n    ai.close()\n\nYou can also use with statements to close models automatically (recommended).\n\n### Message History\n\n#### Follow-Up\n\nMessage history is automatically saved in Cria, so asking follow-up questions\nis easy.\n\n    \n    \n    prompt = \"Who is the CEO of OpenAI?\" response = ai.chat(prompt, stream=False) print(response) # The CEO of OpenAI is Sam Altman. prompt = \"Tell me more about him.\" response = ai.chat(prompt, stream=False) print(response) # Sam Altman is an American entrepreneur and technologist who serves as the CEO of OpenAI...\n\n#### Clear Message History\n\nClear history by running the clear method.\n\n    \n    \n    prompt = \"Who is the CEO of OpenAI?\" response = ai.chat(prompt, stream=False) print(response) # Sam Altman is an American entrepreneur and technologist who serves as the CEO of OpenAI... ai.clear() prompt = \"Tell me more about him.\" response = ai.chat(prompt, stream=False) print(response) # I apologize, but I don't have any information about \"him\" because the conversation just started...\n\n#### Passing In Custom Context\n\nYou can also create a custom message history, and pass in your own context.\n\n    \n    \n    context = \"Our AI system employed a hybrid approach combining reinforcement learning and generative adversarial networks (GANs) to optimize the decision-making...\" messages = [ {\"role\": \"system\", \"content\": \"You are a technical documentation writer\"}, {\"role\": \"user\", \"content\": context}, ] prompt = \"Write some documentation using the text I gave you.\" for chunk in ai.chat(messages=messages, prompt=prompt): print(chunk, end=\"\") # AI System Optimization: Hybrid Approach Combining Reinforcement Learning and...\n\nIn the example, instructions are given to the LLM as the system. Then, extra\ncontext is given as the user. Finally, the prompt is entered (as a user). You\ncan use any mixture of roles to specify the LLM to your liking.\n\nThe available roles for messages are:\n\n  * user - Pass prompts as the user.\n  * system - Give instructions as the system.\n  * assistant - Act as the AI assistant yourself, and give the LLM lines.\n\nThe prompt parameter will always be appended to messages under the user role,\nto override this, you can choose to pass in nothing for prompt.\n\n### Multiple Models and Parallel Conversations\n\n#### Models\n\nIf you are running multiple models or parallel conversations, the Model class\nis also available. This is recommended for most use cases.\n\n    \n    \n    import cria ai = cria.Model() prompt = \"Who is the CEO of OpenAI?\" response = ai.chat(prompt, stream=False) print(response) # The CEO of OpenAI is Sam Altman.\n\nAll methods that apply to the Cria class also apply to Model.\n\n#### With Model\n\nMultiple models can be run through a with statement. This automatically closes\nthem after use.\n\n    \n    \n    import cria prompt = \"Who is the CEO of OpenAI?\" with cria.Model(\"llama3\") as ai: response = ai.chat(prompt, stream=False) print(response) # OpenAI's CEO is Sam Altman, who also... with cria.Model(\"llama2\") as ai: response = ai.chat(prompt, stream=False) print(response) # The CEO of OpenAI is Sam Altman.\n\n#### Standalone Model\n\nOr, models can be run traditionally.\n\n    \n    \n    import cria prompt = \"Who is the CEO of OpenAI?\" llama3 = cria.Model(\"llama3\") response = llama3.chat(prompt, stream=False) print(response) # OpenAI's CEO is Sam Altman, who also... llama2 = cria.Model(\"llama2\") response = llama2.chat(prompt, stream=False) print(response) # The CEO of OpenAI is Sam Altman. # Not required, but best practice. llama3.close() llama2.close()\n\n### Generate\n\nCria also has a generate method.\n\n    \n    \n    prompt = \"Who is the CEO of OpenAI?\" for chunk in ai.generate(prompt): print(chunk, end=\"\") # The CEO of OpenAI (Open-source Artificial Intelligence) is Sam Altman. promt = \"Tell me more about him.\" response = ai.generate(prompt, stream=False) print(response) # I apologize, but I think there may have been some confusion earlier. As this...\n\n### Running Standalone\n\nWhen you run cria.Cria(), an ollama instance will start up if one is not\nalready running. When the program exits, this instance will terminate.\n\nTo prevent this behavior, either run your own ollama instance in another\nterminal, or run a managed subprocess.\n\n#### Running Your Own Ollama Instance\n\n    \n    \n    ollama serve\n    \n    \n    ai = cria.Cria() prompt = \"Who is the CEO of OpenAI?\" with cria.Model(\"llama3\") as llama3: response = llama3.generate(\"Who is the CEO of OpenAI?\", stream=False) print(response)\n\n#### Running A Managed Subprocess (Reccomended)\n\n    \n    \n    ai = cria.Cria(standalone=True, close_on_exit=False) prompt = \"Who is the CEO of OpenAI?\" # Ollama will already be running. with cria.Model(\"llama2\") as llama2: response = llama2.generate(\"Who is the CEO of OpenAI?\", stream=False) print(response) with cria.Model(\"llama3\") as llama3: response = llama3.generate(\"Who is the CEO of OpenAI?\", stream=False) print(response) quit() # Olama will keep running, and be used the next time this program starts.\n\n## Contributing\n\nIf you have a feature request, feel free to make an issue!\n\nContributions are highly appreciated.\n\n## License\n\nMIT\n\n## About\n\nRun LLMs locally with as little friction as possible.\n\npypi.org/project/cria/\n\n### Topics\n\npython llama llm ollama\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\nActivity\n\n### Stars\n\n3 stars\n\n### Watchers\n\n2 watching\n\n### Forks\n\n0 forks\n\nReport repository\n\n## Releases 3\n\nCria v1.4.1 Latest\n\nApr 22, 2024\n\n\\+ 2 releases\n\n## Sponsor this project\n\n  * ko-fi.com/wallstreetlocal\n\n## Packages 0\n\nNo packages published\n\n## Languages\n\n  * Python 100.0%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
