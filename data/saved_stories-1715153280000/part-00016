{"aid": "40290630", "title": "New open source AI model for document segmentation and unstructured ETL", "url": "https://www.aryn.ai/post/new-open-source-ai-model-for-document-segmentation-and-unstructured-etl", "domain": "aryn.ai", "votes": 3, "user": "bsowell", "posted_at": "2024-05-07 19:43:57", "comments": 0, "source_title": "New open source AI model for document segmentation and unstructured ETL", "source_text": "New open source AI model for document segmentation and unstructured ETL\n\ntop of page\n\nGet Started\n\n  * The Aryn Team\n  *     * 6 hours ago\n\n# New open source AI model for document segmentation and unstructured ETL\n\nBy Jon Fritz and Ben Sowell\n\nAt Aryn, we spend time helping users build search and analytics applications\nover complex unstructured datasets \u2014 an extension of RAG that we call LUnA\n(LLM-powered unstructured analytics). We\u2019ve found that the way in which you\nsegment, enrich, and chunk complex, unstructured data is critical for\ngenerating high-quality answers. Sycamore, our open source platform, includes\na data processing framework called Sycamore ETL that\u2019s purpose built for this.\nInstead of trying to process a document all at once, Sycamore ETL first\ndecomposes it into its constituent components, which can be of various types\nsuch as paragraphs, tables, images, and more. Then, it can apply the best AI\nmodel for each component based on its type to extract and process it with high\nfidelity.\n\nIn this blog post, we\u2019re excited to show you how to use our new Sycamore\nPartitioner to accomplish these ETL tasks. We are also excited to share that\nwe\u2019ve released a new open source, Apache v2.0 AI model for high-fidelity\ndocument segmentation, the first step of partitioning. We then show you how\nyou can use component specific AI models for high-quality table extraction,\nOCR, and summarizing figures.\n\n### Sycamore DocSets and document partitioning\n\nSycamore ETL gives users a scalable and robust abstraction for document\nprocessing: DocSets. A DocSet is like a Python DataFrame in Spark, but instead\nof operating on structured records, you can transform and manipulate\ncollections of unstructured documents.\n\nDocSets are schema-free, thus each document in a DocSet can have a different\nstructure.\n\nThe DocSet represents each document as a tree of elements, where each element\nrepresents a chunk or component of the document. Elements can represent\ndifferent types of components, such as text or images. Documents and elements\nalso have additional associated metadata called properties. Because the\nelements are stored as a tree, Sycamore can process each element individually\nwhile retaining its context in the whole document. Supported transformations\non DocSets range from simple formatting to complex AI-based techniques. And,\nwith DocSets, scalability and fault-tolerance are built-in.\n\nTo process raw PDFs and bring them into DocSets, Sycamore must first segment\nthe document and label each element, such as headings, tables, and figures.\nThis process is called document segmentation, and is a critical step in\nprocessing unstructured data. Partitioner transforms can use a variety of\ntechniques, from hardcoded heuristics to advanced AI models, to identify\nbounding boxes and label unstructured components.\n\nWith Sycamore, users can choose from multiple partitioners and their\nrespective configurations. We initially included off-the-shelf open source\npartitioners as part of our stack. However, for real world use cases, we\nquickly found that our previously included partitioners lacked the fidelity\nand accuracy we needed for getting high quality results for RAG and\nunstructured analytics. So, we built our own partitioner powered by our new\ndocument segmentation AI model.\n\nWe are excited to introduce the Sycamore Partitioner in the latest release.\nThe first version is focused on PDFs, and it includes a newly trained object\ndetection model that provides better accuracy in labelling and segmenting\ndocuments.\n\nBut most importantly, the Sycamore Partitioner and associated AI models are\n100% open source with the Apache v2.0 license. You don\u2019t need to sign-up or\npay for anything \u2013 it\u2019s free to use with Sycamore or your own projects. We\nbelieve the data community should have access to the latest-and-greatest tech\nin open source. However, we hope you do join our Sycamore Slack group and\nshare your feedback (or contribute!).\n\n### Using object detection for better segmentation\n\nYou\u2019ve likely seen complex PDFs \u2013 ranging from technical manuals for equipment\nto scientific journals. Sub-headings might have other associated headings,\ntables can have rows spanning multiple columns, and text might be broken up\ninto multiple columns and separated by images. These complicated layouts tend\nto confuse standard OCR or extraction techniques, and will leave you with a\nmess.\n\nMany segmentation approaches use an AI model for object detection and\nlabelling to try to solve this problem. Object detection models take an image\nas input, and then generate bounding boxes to identify specific parts of that\nimage. The bounding boxes will correlate to what the model was trained to\nidentify. Some object detection models can also label the different bounding\nboxes. Data preparation systems can then use this information to merge\nelements into the right size chunks, enrich with metadata, or do OCR.\nTherefore, the quality of your semantic search or RAG application is directly\ncorrelated to how well your unstructured data is segmented by your\npartitioner, and ultimately prepared.\n\nWhen experimenting with various open source partitioners with different object\ndetection models, we found that there were issues consistently segmenting and\nlabeling complex PDF documents across various use cases. Furthermore, these\npartitioners didn\u2019t allow for using the relevant state-of-the-art AI model for\neach document component type to get the best extraction.\n\nSo, we embarked on a journey to create our own partitioner with a new object\ndetection model at its core. First, we wanted to start with a model built with\nthe latest AI architectures, and we chose the Deformable DEtection TRansformer\n(DETR) model. However, existing DETR models were not trained on enterprise\ndata and business documents, and we realized that we\u2019d need to train our own.\n\nEnter DocLayNet \u2013 an open source, human-annotated document layout segmentation\ndataset containing tens of thousands of pages from a broad variety of document\nsources. We used this dataset to train our DETR model, and the result was much\nbetter object detection and labeling for enterprise documents. More\ninformation about the model is on our Hugging Face model page.\n\nHere is the previous partitioner that relies on a rules-based extraction\nmethod and our new Sycamore Partitioner (with DETR model) segmenting and\nlabeling a document:\n\nWith the new DETR model, the Sycamore Partitioner accurately draws the proper\nbounding boxes around each component, and correctly labels the type of\ncomponent. On the other hand, we see the other partitioner over-segmenting\nmost of the document, and not identifying the chart and table as unique\ncomponent types.\n\nOur trained DETR model is at the heart of the Sycamore Partitioner, and it\nenables a variety of downstream ETL. Within our partitioner, we also included\nhigh-quality table extraction, image processing, and OCR features in our\ninitial release. We discuss and show examples of these below.\n\n### Easily extract your tables with high-fidelity\n\nWhen building the new Sycamore Partitioner, we saw an opportunity to include\nhigh-quality table extraction. When our DETR model identifies and labels a\ntable, the Sycamore Partitioner will then use the Table Transformer model to\noutline each cell in the table. We then use PDF Miner to extract the text from\nthe page, and use the DocSet\u2019s table representation to intersect each\nextracted element with the outline of the cell in the table. With this\nrepresentation, Sycamore stores the contents of each cell and table\nproperties, and allows you to manipulate the table in subsequent transforms.\nSycamore can even convert the table to different formats, like HTML, CSV,\nPandas, and more!\n\nFor a quick demo, we ran a Sycamore job on the document we showed above, which\ncontains a table, graph, and various headers and text blocks. It\u2019s easy to\nconfigure partitioning and table extraction:\n\n    \n    \n    ds = context.read.binary(paths=[\"/path/to/folder\"],\\ binary_format=\"pdf\")\\ .partition(partitioner=SycamorePartitioner\\ (extract_table_structure=True))\n\nBelow, we visualize the output of the Sycamore Partitioner. It correctly\nidentifies and labels each part of the document, including the multi-colored\ntable in the top right:\n\nThe partitioner then uses PDF Miner to extract the text, and then finds the\nintersection of the text and the cell bounding boxes to construct the table as\nan element in the DocSet.\n\nIn cases where a table has a column that spans several rows (or other complex\nformatting), the model will identify this and properly represent the table.\nHere is an example showing a table a column header (\u201dUSD billion\u201d) that spans\nthree columns and three sub-headers (\u201dRevenue by business,\u201d \u201cBy region,\u201d and\n\u201cBy Industry\u201d) that span four columns:\n\nThe Sycamore Partitioner draws the correct bounding boxes across the cells in\nthe table. We can explore the table\u2019s representation to see how Sycamore\nextracted the cells and stored the table. We can also use the convert_to_html\ntransform, and then visualize the HTML output for the table (an excerpt is\nbelow):\n\nWith the table extraction process, the Sycamore Partitioner has information\nabout every cell and header in the table. For instance, it can correctly place\nthe spanning rows like \u201cRevenue by Business.\u201d\n\n### Identifying and processing images with multi-modal LLMs\n\nSimilar to tables, the Sycamore Partitioner can identify images in your\ndocuments and process them in various ways. Using multi-modal LLMs, it can\ncreate summary descriptions in words and extract metadata from images. This\nenables you to load images with relevant metadata in your vector and keyword\nindexes, and allows you to retrieve them during a search query. DocSets have a\nspecific element type for images that handles metadata differently. For\ninstance, when creating vector embeddings for an image, Sycamore can instead\nuse the supplied image summary (in text) to create the vectors.\n\nYou can choose to send images along with a prompt to a multi-modal LLM like\nGPT-4-turbo to create helpful metadata like summaries or classifications:\n\n    \n    \n    context = sycamore.init() doc = context.read.binary(paths=paths, binary_format=\"pdf\")\\ .partition(partitioner=SycamorePartitioner(extract_images=True))\\ .transform(SummarizeImages)\\ .show()\n\nFrom the example document above, we can extract and process this image of a\ngraph:\n\nUsing GPT-4V, the SummarizeImages transform gets this description of the image\nand add it as metadata in the DocSet:\n\n    \n    \n    { 'is_graph': True, 'x-axis': 'Total Investment (in 'Billions of U.S. Dollars)', 'y-axis': 'Country', 'summary': \"The bar graph displays the total investment in billions of U.S. dollars by various countries. The United States leads with a significant margin at approximately 47.36 billion dollars, followed by China with about 13.41 billion dollars, and the United Kingdom with around 4.37 billion dollars. Other countries like Israel, India, and South Korea also show investments ranging from 3 to 4 billion dollars. The graph clearly illustrates the disparity in investment amounts among the countries, with the United States investing more than three times the amount of the second highest, China.\" }\n\nThe summarization gives additional context to the image in text, which is\nhelpful for downstream search and analytics queries on this data.\n\n### Processing documents that require OCR\n\nMany datasets consist of images of documents, such as signature-based\ndocuments. For document images, you need to include an OCR step in your\nprocessing pipeline to extract text from the image. Using an accurate\nsegmentation model is important for this, because AI models for OCR pull the\ntext from each labeled part of the image. Because it can accurately segment\nimages of documents, the Sycamore Partitioner\u2019s DETR model is a great fit in\ncombination with open source models like Easy OCR. Also, the Sycamore\nPartitioner can do OCR on tables.\n\nBelow is a snippet from an example notebook using the OCR feature to extract\ntext from a scanned PDF. Each element in the document was identified and\nsegmented using the DETR model:\n\n    \n    \n    ds = context.read.binary(paths=[str(path)], binary_format=\"pdf\")\\ .partition(SycamorePartitioner(use_ocr=True))\\ .explode()\n\nThe output from the notebook is a DocSet that includes the extracted text as\npart of each element in the DocSet.\n\n### Bringing it all together in a DocSet\n\nHere\u2019s what it looks like if you want to run all of these operations on the\ndocument in one go:\n\n    \n    \n    ds = context.read.binary(paths=[str(path)], binary_format=\"pdf\")\\ .partition(SycamorePartitioner(extract_table_structure=True, use_ocr=True,\\ extract_images=True))\\ .transform(SummarizeImages)\\ .explode()\n\nOnce you have the partitioned documents in a DocSet, associated with each\nelement is its type (e.g. table), relevant properties and metadata, and its\ncontext within the rest of the document. Using the show() function in a\nnotebook, we can show the elements from the prior document. In the screenshot\nbelow, you can see some of the elements with bounding boxes, text, and other\nproperties for headers, footers, and a table:\n\nNow, you can take this DocSet and continue your ETL processing. For example,\nyou can extract and add more metadata (blog for example schema extraction),\ntransform specific elements, assemble chunks, and choose what to create vector\nembeddings for (and how). To get high-quality answers from your RAG or\nunstructured analytics pipelines, these steps are often needed to add\nadditional context for better retrieval and more accurate answers.\n\nBut, it all starts with high-fidelity partitioning.\n\n### Get started with the Sycamore Partitioner\n\nThe new Sycamore Partitioner is included in the latest containerized release\nand Sycamore ETL Python library, and currently operates on PDFs. You can use\nthe partitioner with CPU or an NVDIA GPU, though performance on larger\nprocessing jobs can be faster if you use a GPU. Our DETR AI model that powers\nthe partitioner is 100% open source with an Apache v2.0 license, and we\nencourage you to try it out in Sycamore (also open source) or use it in your\nown projects. And, feel free to improve on it! You can fine tune it for your\nown purposes, or contribute back so that we can make it better for everyone to\nuse.\n\nTo learn more about the Sycamore Partitioner\u2019s Deformable DETR model, visit\nAryn\u2019s Hugging Face model page. To learn more about the Sycamore Partitioner,\nvisit the Sycamore Documentation. To get started with Sycamore, visit the\ngetting the started guide.\n\nJoin the Sycamore Slack: https://join.slack.com/t/sycamore-\nulj8912/shared_invite/zt-23sv0yhgy-MywV5dkVQ~F98Aoejo48Jg\n\n## Recent Posts\n\nSee All\n\nWhen RAG runs out of steam, use schema extraction and analytics with Sycamore\n\nNear-Duplicate Detection in Sycamore: What Is It Good For?\n\nRAG is a band-aid; we need LLM-powered Unstructured Analytics \u2014 LUnA\n\nContact\n\n756 California St., Unit A\n\nMountain View, CA 94041\n\nContact us: info@aryn.ai\n\nQuick Links\n\nBlog\n\nLinkedIn\n\nDocs\n\nGitHub\n\n\u00a9 2024 by Aryn.\n\nbottom of page\n\n", "frontpage": false}
