{"aid": "40188038", "title": "Take the tools out of 'Data', but don't take the data out of the tools", "url": "https://csvbase.com/blog/1", "domain": "csvbase.com", "votes": 1, "user": "tosh", "posted_at": "2024-04-28 12:14:44", "comments": 0, "source_title": "Take the tools out of 'Data', but don't take the data out of the tools", "source_text": "Take the tools out of 'Data', but don't take the data out of the tools\n\ncsvbase is a simple web database. Learn more on the about page.\n\n# Take the tools out of 'Data', but don't take the data out of the tools\n\nUsing dataframes to write smaller, faster programs\n\n2023-01-11\n\nby Cal Paterson\n\nThe Data team are using a strange and weird datastructure: the dataframe.\nDataframes are different to the usual nested hashtables and arrays. They are\noptimised for bulk operations - those are the operations you tend to do when\nanalysing data.\n\nWhen you write code with objects they call it \"object-oriented programming\". I\nthink when you code with dataframes they should probably call it \"dataframe-\noriented programming\". For whatever reason they tend to call it \"data science\"\ninstead.\n\n## Worked examples of \"dataframe-oriented programming\"\n\nProgramming with dataframes is probably best illustrated by worked example.\n\nHere's a sample problem:\n\nStarting from some published data about stock exchanges (meripaterson/stock-\nexchanges), print an alphabetically sorted list of \"MIC codes\".\n\nMIC codes are a bit like airport codes but for stock exchanges (in fact: for\nany kind of \"trading venue\"). Instead of \"LHR\" referring to Heathrow, the\nLondon Stock Exchange is referred to by \"XLON\". Of course, some stock\nexchanges in the published dataset don't have a MIC code entered. Not all\nstock exchanges have been assigned one. These \"blanks\" will need to be\nremoved.\n\nHere's how you do it with pandas, which is probably the most widespread\ndataframe library:\n\n    \n    \n    #!/usr/bin/env python3 from sys import stdout # 'pip install pandas' first import pandas as pd df = ( # read csv from csvbase pd.read_csv( # csvbase does HTTP content negotiation to allow pandas to use the same # url as a browser - magic! \"https://csvbase.com/meripaterson/stock-exchanges\", # and restrict to just the 'MIC' column usecols=[\"MIC\"] ) # drop nulls .dropna() # sort .sort_values(\"MIC\") ) # write to stdout (as a single column csv with no header) df.to_csv(stdout, index=False, header=False)\n\nThat looks a bit different to how you might usually do it in Python.\n\nThere are no for-loops or no if statements: it's all whole-dataframe\noperations. Those are more efficient and that matters more as the dataframe\ngrows. It's also more readable, once you're used to the change in style.\n\nSome will have experience programming this way when using SQL queries. Here's\nhow you could do the same thing with SQL:\n\n    \n    \n    #!/usr/bin/env sh curl -s https://csvbase.com/meripaterson/stock-exchanges \\ | sqlite3 -csv ':memory:' '.import /dev/stdin t' \\ \"SELECT MIC FROM t WHERE MIC != '' ORDER BY MIC ASC\"\n\nSQL comes out pretty terse for this problem and in fact sqlite makes a decent\ndataframe library. Again: no for loops or if statements: SQL doesn't even have\na for-loop in the language and the \"if statement\" (CASE) doesn't allow you to\ncontrol program flow. The tricky part with SQL is that, because the language\nisn't easily composable (notoriously, people just concat text strings),\ncomplicated queries can balloon out in size as the problem grows.\n\nAnother dataframe library: Apache Spark. Spark acts as a dataframe API on top\nof Hadoop. Hadoop has a (well deserved) reputation for Cthulhu-tier complexity\nbut using Apache Spark is actually not too much harder than using Pandas:\n\n    \n    \n    #!/usr/bin/env python3 from sys import stdout import csv # 'pip install pyspark' for these from pyspark import SparkFiles from pyspark.sql import SparkSession # make a spark \"session\". this creates a local hadoop cluster by default (!) spark = SparkSession.builder.getOrCreate() # put the input file in the cluster's filesystem: spark.sparkContext.addFile(\"https://csvbase.com/meripaterson/stock-exchanges.csv\") # the following is much like for pandas df = ( spark.read.csv(f\"file://{SparkFiles.get('stock-exchanges.csv')}\", header=True) .select(\"MIC\") .na.drop() .sort(\"MIC\") ) # pyspark has no easy way to write csv to stdout - use python's csv lib csv.writer(stdout).writerows(df.collect())\n\nHelpfully, the python spark libraries will start up your own personal hadoop\ncluster by default. That makes getting rolling much easier. Admittedly, there\nis more noise in this version - but most of that is setup and you can still\nsee the dataframe operations clearly in the middle of the program. Again: no\nloops, no ifs.\n\nJust one more, in R:\n\n    \n    \n    #!/usr/bin/env Rscript mics <- read.csv('https://csvbase.com/meripaterson/stock-exchanges')[\"MIC\"] non_nulls <- sort(mics[!apply(mics == \"\", 1, all),]) write.table(non_nulls, \"\", quote=F, row.names=FALSE, col.names=FALSE)\n\nR is pretty terse too - and yet again, no ifs, no loops. I'm a bit of a\nbeginner with R so there are probably ways to golf the above program down\nfurther (if so: please tell me how).\n\nDataframes allow the programs which mainly manipulate data to be smaller,\nquicker to write and easier to follow - that's why the data team like them.\nIt's just a lot easier to read through a (usually short) sequence of whole-\ndataframe operations than to read some free-flowing imperative code.\n\nThe key to legible (and efficent) dataframe code is to leave the data in the\ndataframe. Use the operations the dataframe library provides to work on the\ndata. Avoid the temptation to rip the data out and operate on it with your\nprogramming language's native functions. Operate on the data where it already\nis!\n\nDataframes have wider applicability than just data teams, though. I think they\nshould be used more often than they are. In the rest of the article I am going\nto give some hints on how to get started using dataframes. The hints will be\nmostly generic across all tools but from now on I'll give code samples for\npandas.\n\n## Hint 1: Pre-processing matters\n\nOne of the fairly bleak realisations that comes when working with data is how\nmuch of your time is going to be spent cleaning it and fixing it up before you\ncan actually use it. It's crucial to become good at pre-processing the data.\n\n### Reshaping\n\nThe first and most important sort of pre-processing is reshaping. People\nrelease data in all sorts of weird and wonderful shapes but usually you only\nwant one specific kind of shape: long - as opposed to wide.\n\nHere's an example wide table:\n\nSchool| Average mark (2007)| Average mark (2008)| Average mark (2009)  \n---|---|---|---  \nHogwarts| 74| 75| 71  \nSmeltings| 55| 65| 59  \nSt Brutus's Secure Centre for Incurably Criminal Boys| 34| 40| 42  \n  \nThis layout can be a real pain. Aggregation functions don't work well. Imagine\ntrying to calculate the average mark across all schools and all years - you'd\nhave to first stack the all the marks into a single column before you could\ntake the average by running mean. And of course each time a new year of data\nis added to your dataset you'd have to go back and correct your stacking code.\nIf you forget to do so, then you get the wrong answer.\n\nAnd here's what the same data looks like in \"long\" format:\n\nSchool| Year| Average mark  \n---|---|---  \nHogwarts| 2007| 74  \nHogwarts| 2008| 75  \nHogwarts| 2009| 71  \nSmeltings| 2007| 55  \nSmeltings| 2008| 65  \nSmeltings| 2009| 59  \nSt Brutus's Secure Centre for Incurably Criminal Boys| 2007| 34  \nSt Brutus's Secure Centre for Incurably Criminal Boys| 2008| 40  \nSt Brutus's Secure Centre for Incurably Criminal Boys| 2009| 42  \n  \nNow taking the average is easy, it's just potter_schools_long[\"Average\nmark\"].mean(). Getting the average for a single year is no more difficult, you\njust filter out other years first.\n\nWhat you usually want is for each individual data point to be in it's own row.\nThat just makes life easier. You can, in general, think of a good dataframe as\nbeing some \"Y\" over an \"X\" - just like X and Y in a graph. Sometimes X can be\ncomposite: as in this case, it is (School, Year).\n\nSome other examples of well shaped dataframes:\n\n  * bond price (Y) over time (X)\n  * the emissions of a car (Y) by model and production year (both consitutents of the \"X\")\n  * gini index (Y) by country (X)\n\nSimple, X to Y, relationships are the easiest to work with - \"long format\".\nThe problem is that people keep publishing data in other shapes! How to solve\nthe problem in general?\n\nThe solution is a library function that is usually called \"melt\" (named after\nthe kind of person who publishes data in wide format). pandas has a melt\nfunction. melt turns your wide, multiple-Y dataframe into a narrower,\ncomposite-X dataframe. No custom code required.\n\nThe converse is often called casting but in pandas is instead called pivot,\nprobably taking after Excel, where such a result is called a \"pivot table\".\nPivot tables aren't useless - sometimes they are handy to find trends, like a\nprimitive tabular form of graph - but they aren't a good way to store or\nexchange data.\n\nWhen you recieve data that is in a bad shape you should mutter a hex under\nyour breath, reshape with the standard tools from whatever library you're\nusing and move on with you life. Don't write custom code.\n\n### Flattening\n\nThe next most common issue is that the data you've been given is not tabular\nin nature. You don't have a CSV, you have some JSON or - christ preserve you -\nsome XML. Such \"documents\" are pain in the bum because they are fundamentally\nnested and that doesn't correspond well to our dataframe model.\n\nNo bother: you just flatten it. Pandas, for example, has a function called\njson_normalize which takes nested dictionaries (they need not be from JSON)\nand traverses them, building a dataframe for you. In other libraries this can\nbe called \"unnesting\", splitting or \"exploding\".\n\nHere's an example which turns some data from the reddit API into a dataframe:\n\n    \n    \n    >>> import pandas as pd, requests >>> r = requests.get(\"https://www.reddit.com/r/CasualUK/top.json?limit=10&t=year\") >>> # flatten: >>> top_posts = pd.json_normalize(c[\"data\"] for c in r.json()[\"data\"][\"children\"]) >>> # parse the unixtimes as datetimes: >>> top_posts = top_posts.assign(created_utc=pd.to_datetime( ... top_posts[\"created_utc\"], unit=\"s\", origin=\"unix\")) >>> top_posts[[\"author\", \"created_utc\", \"upvote_ratio\"]] author created_utc upvote_ratio 0 MellotronSymphony 2022-11-23 08:44:19 0.88 1 Daz-Gregory1337 2022-11-04 09:23:00 0.90 2 Pestish 2022-09-21 16:10:51 0.95 3 f3361eb076bea 2022-08-26 13:42:04 0.93 4 kopsy 2022-10-30 15:29:04 0.84 5 MrClaretandBlue 2022-12-05 09:41:52 0.94 6 going10-1 2022-10-01 10:50:59 0.95 7 zembo12 2022-05-21 07:58:24 0.93 8 vela025 2022-10-22 11:00:24 0.92 9 halosos 2022-05-20 08:16:17 0.88\n\nSometimes half the battle in programming is knowing what already exists. I\nhave seen people write some substantial pre-processing programs to pointlessly\nduplicate this common built-in function - they simply did not know it was\nthere for the taking.\n\n### Null, NA, \"N/A\", \"\", etc \u2014 and existential doubt\n\nAnother perennial issue is that many there tend to be nulls in data. Null gets\nwritten in all sorts weird and wonderful ways, including:\n\n  * null\n  * NA\n  * N/A\n  * not applicable\n  * the empty string\n  * whitespace\n  * and many, many more\n\nIt's important to catch these when you're loading your dataframe in. Nulls get\nspecial treatment in many operations and you want your dataframe library to\nknow that a certain value is a null.\n\nWhen you read a csv file in pandas, you can tell what nulls look like in\nadvance:\n\n    \n    \n    df = pd.read_csv(\"my-weirdo-data.csv\", na_values=[\"not a bean\"])\n\nDon't let the strange and weird textual forms of \"no value\" propagate through\nyour dataframe program. Convert them when you parse. Then you can get on with\nthe existentially difficult program of working out what a null means in your\ncontext (harder than it sounds).\n\n### Don't write special code\n\nThe most important hint for pre-processing is that you don't should try to\navoid writing special code where you can. You are not the first person who has\nhad to melt some stupidly wide csv file. You also aren't the first person\nwho's recieved a file where null is written as N/applicable.\n\nWhen I was in primary school an art teacher told my class that the children\nwho where the quickest to mix paints became the best painters. Children who\nwere slow to mix each week got progressively less practice than their peers at\nthe painting bit compared to the children who mixed quickly and got on with\nit.\n\nThe same is true of pre-processing: you will have to do a lot of it and it is\nbest that you start seeing melting, casting, flattening, and so on as basic\noperations that will be provided by your library. Pre-process quickly so you\ncan spend the balance of your time doing data analysis.\n\n## Hint 2: Dataframe libraries differ, a bit\n\nDespite the basic concepts remaining the same, dataframe libraries do differ.\nThere is, sadly, no intergalactic standard dataframe API. Not even SQL, which,\nwhile notionally portable differs a lot in practice between implementations.\n\nBecause dataframe APIs are (mostly) incompatible you need to pick on before\nyou start. Best not discover that your data is too big for memory after having\nwritten a long pandas program.\n\n### Column-oriented vs row-oriented\n\nMany traditional SQL databases are \"row-based\", meaning all the data of each\nrow is stored together. That seems obvious and natural but there is another\nway: \"column-based\", where you split out each column and keep them all\nseperate.\n\nRow-based tools have the advantage when it comes to transactional updates: you\ncan more easily lock access to a single row while it's being changed. But data\nanalysis usually doesn't care much about transactions - most data is\nhistorical and even if the source data is still being updated most of the time\nyou can just take a \"cut\" of it and being a day or so out of date is not\nreally a problem.\n\nColumn-based tools have many advantages when it comes to reads. Most\noperations touch on only a subset of columns, and if your columns are stored\nseparately there are fewer bytes to read. That makes it quicker. Columns also\ntend to compress better when done separately.\n\nPandas is column oriented, as is Spark and as is R. (Traditional) SQL\ndatabases like Postgres and sqlite are not.\n\n### Lazy vs strict evaluation\n\nSome dataframes are lazily evaluated: meaning that nothing happens until you\nask for the final result. The advantage here is that when the library knows\nthe whole program that will be run that it has more options for optimisation.\nOften this comes in the form of combining multiple passes over the data into\none, but it's a big topic.\n\nOthers are strictly evaluated and do each operation, in order, as you issue\nthe instructions. The advantage here is that strict evaluation is easier to\nunderstand and debug - reading query plans from lazy dataframe tools is often\nan art in itself. \"My dataframe program is slow\" is a much easier problem to\nresolve when the dataframe is strictly evaluated.\n\nPandas is strictly evaluated. So is R. Apache Spark and almost all SQL-based\nsystems are lazy.\n\n### Single-node vs multi-node\n\nSome dataframe librares are really aimed at running only on a single node.\nThis tends to make them easier to start with and much easier to use for ad hoc\nanalysis: you don't need to first set up some cluster thing and no one is\ngoing to ask for your credit card number.\n\nOthers are of the big, complicated, distributed, multi-node etc kind. Those\nscale to much bigger datasets. Usually they are actually slower in straight-\nline performance than the single-node kind but if your data is big you have no\nother option.\n\nPandas, R and sqlite are single-node options. Apache Spark and many of the\ncloud-hosted services like BigQuery are multi-node. There are of course some\noptions to make single-node dataframe code work transparently across multiple\nnodes - see Dask.\n\n## Hint 3: Speed does matter\n\nA common mantra among software engineers is:\n\n> Make it work, make it right, make it fast\n\nThis is, in the main, great advice. But it causes grief in data engineering\nbecause the size of data means that slow code can easily be so slow that it\nhinders improvement - the n in O(n) is typically a big number. When each run\ntakes hours (or even days) you are not going to be bold and try to improve it:\nmanagement are already highly stressed and keen that nothing interfere with\n\"the big run\".\n\nSo I suggest a modified version of the mantra for dataframe-oriented\nprogramming:\n\n> Make it fast, make it work, make it right\n\nYou need at least a little bit of \"fast\" in order to have the time and space\nto add more correctness. Projects which have short feedback loops (read\nresults, edit code, run code) will, over time, tend to end up more developed\nand sophisticated. Projects with long feedback loops become fraught - and\npeople avoid making any changes to them.\n\nMy hints, then, for speeding it up:\n\n### No loops\n\nThe first and most important tip when writing dataframe code is never to try\nto loop over it.\n\nLooping is generally a disaster for performance. For example, Pandas is\nintentionally built upon numpy arrays - a datastructure designed for\nefficiency and vectorisation. When you iterate over them in a Python loop, you\nthrow those advantages away and instead operate at the speed of Python,\nwhich...is less than stellar.\n\nHere's a simple example using Pandas:\n\n    \n    \n    import timeit import pandas as pd import numpy as np def pd_add_one(df): df[\"a\"] += 1 def python_add_one(df): for index, row in df.iterrows(): df.loc[index] = row[0] + 1 df = pd.DataFrame({\"a\": np.random.rand(1000)}) print(\"python takes: %f secs\" % timeit.timeit( \"python_add_one(df)\", globals=globals(), number=100)) print(\"pandas takes: %f secs\" % timeit.timeit( \"pd_add_one(df)\", globals=globals(), number=100))\n\nOn my machine, this program prints:\n\n    \n    \n    python takes: 4.007588 secs pandas takes: 0.007870 secs\n\nLooping in this case is around 500 times slower. The example might seem\nartificially constructed to favour pandas by deliberately using a simple\noperation (addition) but actually it's heavily weighted in favour of Python.\nThe more complicated the operation to be executed and the larger the dataset\n(this is just 1000 integers) the more any comparison will favour in-dataframe\noperations over looping. And there aren't a lot of things where you can bear\nto get a few hundred times slower.\n\n### Avoid UDFs\n\nThe next thing people try when told not to loop are UDFs - user defined\nfunctions. Again, in Pandas, these are pieces of Python code and the only\ndifference is that the dataframe runs the code itself. An example:\n\n    \n    \n    import timeit import pandas as pd import numpy as np def apply_add_one(df): def add_one(a): return a + 1 df = df.apply(add_one) df = pd.DataFrame({\"a\": np.random.rand(1000)}) print(\"apply takes: %f secs\" % timeit.timeit(\"apply_add_one(df)\", globals=globals(), number=100))\n\nWhich prints:\n\n    \n    \n    apply takes: 0.013365 secs\n\nSo yes, in this case, a UDF is lot faster than looping, though still\nconsiderably slower - nearly twice - than using pandas operations directly.\nUser-defined functions aren't always even so performant as this - the slowdown\ncan be particularly bad in lazily evaluted dataframe libraries because UDFs\nmake life hard for the query optimiser.\n\nIt's operative to leave the data in the dataframe: don't keep pulling it out\nto work on it in your programming language. Use the built-in dataframe\nfunctions: they are a lot more effective.\n\n### Smaller is better\n\nThere is a certain glamour to working on some enormous dataset: you get to\ntell everyone else that you're working on \"Big Data\". But much \"Big\" data is\nless big than bloated: filled with repetition, values that use a bigger\nrepresentation than necessary, pre-normalised, accidentally cartesian, and so\non. The list of ways to make medium-sized data into big data is long.\n\nRepresentation matters a lot. An common example, especially where some of the\ndata has passed through excel, are booleans. Excel typically represents true\nand false with the strings TRUE and FALSE, which often weigh 8 and 10 bytes\nrespectively (in UTF-16LE - which is what Excel outputs when you save to csv).\nA boolean is ideally a single bit so the word FALSE is 80 times bigger than\nnecessary. Over a large number of rows the pain of bloated data representation\nreally mounts.\n\nThe same thing happens with integers. The default integer size in many tools\nis often 8 bytes wide (or 64 bits). That's a lot of extra bytes if your column\nis only going to be used to carry integers between 1 and 30. Usually there is\na smaller integer size that is smaller, either two bytes or even a single byte\n- try to use that instead.\n\nSome columns are de-facto enums - they only contain a fixed set of options.\nE-commerce businesses often have a simple state machine for customer orders -\nPICKING, PACKING, SHIPPING, COMPLETED etc. Putting these as strings massively\nbloats out the column. A few libraries have support for automatically changing\nthe representation of these - for example Pandas does - but in other scenarios\nyou can just map them to integers and use a small integer column to store\nthose.\n\nAll of these optimisations seem small individually: shaving one shaves off\nonly a few bytes per row, but when you have a billion rows, each byte shaved\noff is 1 GB off the size of your dataset. It's not unusual to find a dataset\nthat seems too big for memory only to discover that, after optimising it a\nlittle, that suddenly it fits into the memory of a well-specced desktop PC.\nAnything you do to shrink a dataset will make life easier.\n\n## Hint 4: Use csvbase.com\n\nThe final tip, because yes, this is an advert, is to: try csvbase.\n\ncsvbase is an open source website for sharing data. It makes it really easy to\nshare data between people - everyone can export in their preferred format:\ndata scientists get parquet, programmers get a REST API and those would take\njoy in MS Excel can get XLSX with properly formatted cells. And of course you\ncan see your table in a web browser:\n\nWhat it looks like\n\nThe REST API is pretty easy. For example, GET a row with curl:\n\n    \n    \n    curl https://csvbase.com/meripaterson/stock-exchanges/rows/227\n\nYou get:\n\n    \n    \n    { \"row\": { \"Continent\": \"Western Europe\", \"Country\": \"United Kingdom\", \"Last changed\": \"2017-08-16\", \"MIC\": \"XLON\", \"Name\": \"London Stock Exchange\" }, \"row_id\": 227, \"url\": \"https://csvbase.com/meripaterson/stock-exchanges/rows/227\" }\n\nAnd you can get started as easily as pasting cells from excel:\n\nPasting is easy\n\nI'm looking for ideas to improve it. If you have any, please write to me at\ncal@calpaterson.com.\n\n## The future of the past/past of the future\n\nWhy aren't dataframes more widely used, outside the data team? Perhaps one of\nthe reasons is that there was never a time when they seemed new and hip. That\nearly and trendy period is a crucial one that allows technologies to gain\ntheir advocates, and with them, its adoption.\n\nInstead, many software engineers are suspicious that dataframes are largely a\nrebrand of the old, SQL, idea of \"tables\". That is roughly the truth - and SQL\nis old by the standards of computing. The original - and still central - ideas\nabout relational data and normal form are dated 1970.\n\nIt seems reasonable that the solutions of the past would have solved their\nproblems in the past. Not in this case: eight years ago some bright spark\nnoticed that the relational model hadn't made it to the field of statistics,\nso he reworded part of it to help the statisticians better organise their\ndata.\n\nThat became the tidyverse and has been massively influential and had a huge\npositive effect on data exchange, not just in statistics, but in many fields.\nThe past is still here. It's just not evenly distributed.\n\n## See also\n\nYou can find more of my blogposts (of which only a small number are thinly\nveiled adverts such as this one) on my website: https://calpaterson.com. You\ncan sign up to get an email when I write something new and there is an RSS\nfeed as well.\n\nIf you want to get started with \"dataframe-oriented programming\" the best\nthing to do is to learn the pandas library and for that, the best place to\nbegin reading is Chapter 5 of Python for Data Analysis.\n\nIf you want to learn SQL well, I always recommend C. J. Date's books, which I\nlove but I realise that almost everyone else hates. Regardless, the best start\non that front is SQL and Relational Theory.\n\nI've written in the past about the dataframe libraries that certain big\ninvestment banks use. I still think those are good and I wish the banks would\nopen source something for a change.\n\nOne of the things that makes dataframes popular is that SQL gets harder and\nharder to use as your program grows. My favourite article explaining the\nissues with SQL is Against SQL.\n\n  * Source code\n  * Privacy policy\n  * Terms\n\n", "frontpage": false}
