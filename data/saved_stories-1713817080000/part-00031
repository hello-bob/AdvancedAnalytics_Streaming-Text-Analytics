{"aid": "40114145", "title": "Matryoshka and Binary vectors: Slash vector search costs with Vespa", "url": "https://blog.vespa.ai/combining-matryoshka-with-binary-quantization-using-embedder/", "domain": "vespa.ai", "votes": 2, "user": "cmcollier", "posted_at": "2024-04-22 13:27:42", "comments": 0, "source_title": "Matryoshka \ud83e\udd1d Binary vectors: Slash vector search costs with Vespa", "source_text": "Matryoshka \ud83e\udd1d Binary vectors: Slash vector search costs with Vespa | Vespa Blog\n\nMatryoshka \ud83e\udd1d Binary vectors: Slash vector search costs with Vespa | Vespa Blog\n\n# Vespa Blog\n\nAI + data, online\n\nShare\n\nJo Kristian Bergum Follow Vespa Solutions Architect\n\n22 Apr 2024\n\n# Matryoshka \ud83e\udd1d Binary vectors: Slash vector search costs with Vespa\n\nWe announce support for combining matryoshka and binary quantization in\nVespa\u2019s native hugging-face embedder (Vespa >= 8.332.5). In addition, we\ndiscuss how these techniques slash costs compared to float text embeddings.\n\nRecent advances in text embedding models include matryoshka representation\nlearning(MRL), which creates a hierarchy of embeddings with flexible\ndimensionalities, and binary quantization learning (BQL), which learns to\nencode float dimensions to 1-bit representation, representing text as a binary\nvector.\n\nBoth MRL and BQL are instances of deep representational learning, albeit with\nvariations in their representations.\n\nInstead of representing text as a large vector of floats, the combination of\nMRL and BQL encodes text as a binary vector, no longer than a SHA-512 hash\n(512 bits).\n\nThe emphasis of both MRL and BQL is on sacrificing accuracy by a few % in\nexchange for a much lower cost. By using a compact representation of the text\nembeddings, the systems can run on less expensive hardware or require less\nmemory, resulting in cost savings. We can quantify the memory-related savings\nusing Vespa basic plan pricing, where memory (GB) is priced at $ 0.01 / hour.\nIn the following table, we calculate the memory price for 1 Billion\n1024-dimensional vectors using different Vespa tensor precision types.\n\nVector precision| Vector dimensions| Bytes per vector| GB for 1B vectors| $\nhourly  \n---|---|---|---|---  \nfloat| 1024| 4096| 3814| 38.14  \nbfloat16| 1024| 2048| 1907| 19.07  \nint8 (scalar quantization)| 1024| 1024| 953| 9.54  \nint8 (binary quantization)| 128| 128| 119| 1.19  \nint8 (binary quantization + mrl)| 64| 64| 60| 0.59  \n  \nAssuming the use case is within Vespa\u2019s vector streaming search capabilities,\nwhich is suitable for scenarios with partitioned data (e.g., by user ID) and\ndisk-based storage, we can significantly reduce the hourly pricing. In such\ncases, the $0.01/hour cost can be reduced to $0.0004/hour, effectively\nreducing the above hourly prices by 25. Moreover, similarity searches over\nthese binary representations are fast in Vespa, which slashes the CPU-related\ncosts, demonstrated in later sections.\n\nIn addition to cost savings for existing use cases, MRL and BQL will unlock\nnew ones that are no longer prohibitively costly when vector size drops much\nbelow the original text size. This binary text embedding paradigm shift will\nmake more unstructured data useful with binary embedding representations.\n\nSince both techniques are simple post-processing steps over the embedding\nvector representation, we can produce multiple representations with a single\nmodel inference call. One inference pass is vital because model inference is a\nsignificant cost driver for embedding retrieval systems.\n\nFor MRL, the post-processing stage retains the first k dimensions of the\noriginal n dimensions; in contrast, binary quantization retains the\ndimensionality but converts the float dimension values into a 1-bit\nrepresentation via thresholding normalized embeddings at 0.\n\n  * MRL provides flexibility in the number of dimensions\n  * BQL provides per-dimension precision flexibility\n\nMRL retains only the first k-dimensions of the original vector representation,\nwhile BLQ learns a binary representation.\n\nAn exciting new direction in embedding models is to combine these two\ntechniques. From Binary and Scalar Embedding Quantization for Significantly\nFaster & Cheaper Retrieval:\n\n> Additionally, we are excited that embedding quantization is fully\n> perpendicular to Matryoshka Representation Learning (MRL). In other words,\n> it is possible to shrink MRL embeddings from e.g. 1024 to 128 (which usually\n> corresponds with a 2% reduction in performance) and then apply binary or\n> scalar quantization.\n\nThis turned out to be a solid direction, as demonstrated by mixedbread.ai in\ntheir follow-up blog post: 64 bytes per embedding, yee-haw.\n\nIllustration from mixedbread.ai blog post. Notice the small gap between the\nbinary representation and the float representation. At 512 dimensions, the\nbinary representation is 64 bytes.\n\nMixedbread.ai\u2019s combination of MRL and BQL retains 90% accuracy (on MTEB\nRetrieval task) using 64-dimensional int8 (512 bits) binarized from 512 float\ndimensions (first 512 out of 1024). This binary representation reduces\nstorage-related costs by 64 compared to the baseline using 1024 floats.\n\nNotably, the text embedding storage requirement drops to 64 bytes per vector\nembedding; the same size as a SHA-512 hash:\n\n    \n    \n    20e00cff08039a4ef0815bbf5dd53db9b34fd2c7f5f723bb9467effe9ff5 bed946f0829843036cde4414617311f12465ec8e15dab30a9a63e8a6f228a8b1ba8e\n\nHex representation of a 64-dimensional int8 vector, 128 characters.\nIllustration of how compact state-of-the-art text binary embeddings are. Vespa\nsupports hex format of binary vectors in both queries and in documents.\nCompare this compact representation with wrestling with thousands of float\ndimensions..\n\n## Adapting MRL and BQL with embedding inference in Vespa\n\nVespa exposes core functionality for embedding text to vector representations,\nin queries and documents. This functionality allows developers to embed (pun\nintended) their chosen text embedding models in Vespa, scale inference volume,\nand reduce query latency by avoiding sending large vector payloads over remote\nnetworks. As a bonus, it eliminates wrestling with additional infrastructure\nfor embedding inference.\n\nWith the increased interest in MRL and BQL we have added support for obtaining\nMRL and BQL embeddings to the native Vespa hugging-face-embedder.\n\nWe demonstrate the new functionality using mixedbread.ai\u2019s text embedding\nmodel which has been trained with both a MRL and BQL objective. The\nmixedbread.ai embedding model has 1024 float dimensions originally, but offers\ndimension flexibility via MRL and precision flexibility via BQL. The\nmixedbread.ai model uses CLS pooling, and the float version uses the\nprenormalized-angular distance metric where vectors are normalized to unit\nlength (1) and where the cosine similarity equals the dot product between two\nvectors. For the binarized version, hamming distance is used as distance-\nmetric. See the adaptive retrieval section for more details on these two\ndistance metrics.\n\nFirst, we add the component of type hugging-face-embedder to the Vespa\napplication package services.xml,. Notice pooling-strategy and normalization.\n\n    \n    \n    <component id=\"mxbai\" type=\"hugging-face-embedder\"> <transformer-model url=\"https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1/resolve/main/onnx/model_fp16.onnx\"/> <tokenizer-model url=\"https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1/raw/main/tokenizer.json\"/> <pooling-strategy>cls</pooling-strategy> <normalize>true</normalize> </component>\n\nFor GPU instances in Vespa Cloud, we recommend the fp16 model for 3x\nthroughput over the standard model (fp32). For CPU instances, use the\nquantized model version.\n\nThe following demonstrates the new functionality for embedding texts using the\nVespa hugging-face-embedder. All the schema examples below use a single text\nfield. The embed functionality can also handle arrays of strings with multi-\nvector indexing support.\n\n    \n    \n    schema doc { document doc { field text type string {..} } field embedding type tensor<float>(x[1024]) { indexing: input text | embed mxbai | attribute | index attribute { distance-metric: prenormalized-angular } } }\n\nIn the example above, we retain the full precision representation (1024\nfloats), signaled by having a destination tensor type tensor<float>(x[1024].\nThe index is optional and will build a HNSW graph for the tensor. HNSW index\nenables efficient but approximate nearest neighbor search.\n\nIf we want to use the first 256 dimensions instead (MRL), we can signal that\nby changing the tensor definition from x[1024] to x[256] and the embedder will\nuse the first 256 dimensions of the model\u2019s original output dimensions.\n\n    \n    \n    schema doc { document doc { field text type string {..} } field mrl_embedding type tensor<float>(x[256]) { indexing: input text | embed mxbai | attribute | index attribute { distance-metric: prenormalized-angular } } }\n\nWe can also change the tensor cell type to bfloat16 instead of float, which\nreduces storage by 2x over float (but is computationally approximately 20%\nslower than float on CPU).\n\n    \n    \n    field mrl_embedding type tensor<bfloat16>(x[256]) { indexing: input text | embed mxbai | attribute | index attribute { distance-metric: prenormalized-angular } }\n\nWe can also obtain multiple representations. The schema below will only\ninvolve a single inference call to the model because it uses the same input\ntext and the same embedder id.\n\n    \n    \n    schema doc { document doc { field text type string {..} } field mrl_embedding type tensor<bfloat16>(x[256]) { indexing: input text | embed mxbai | attribute | index attribute { distance-metric: prenormalized-angular } } field embedding type tensor<bfloat16>(x[1024]) { indexing: input text | embed mxbai | attribute attribute { paged distance-metric: prenormalized-angular } } }\n\nThis allows for using the shorter MRL representation combined with HNSW\nindexing. The float representation is used in ranking phases. This example\nalso demonstrates offloading the full embedding version to disk (signaled by\nthe paged attribute setting).\n\nNotice also that we don\u2019t specify an index on the embedding field that we\nintend to use only in ranking phases. Adding HNSW indexes costs resources; if\nwe don\u2019t intend to retrieve efficiently over it, adding an HNSW index adds\ncosts without benefits.\n\nAt query time, we can perform a Vespa query request like this where we\nretrieve using the MRL shortened representation, but where the embedder\nproduces both representations so that we can use the float version in Vespa\nranking phases. Similar to embedding expressed in the schema, the below will\nonly cause a single inference call at query time, as the input text and model\nid are the same.\n\n    \n    \n    { \"yql\": \"select * from doc where {targetHits:200}nearestNeighbor(mrl_embedding, mrl_q)\", \"input.query(mrl_q)\": \"embed(mxbai,@text)\", \"input.query(q)\": \"embed(mxbai, @text)\", \"text\": \"The string to embed\" }\n\nThe input query tensors assigned by the embed invocations must be defined in\nthe schema as inputs.\n\n    \n    \n    rank-profile adaptive-rescoring { inputs { query(mrl_q) tensor<bfloat16>(x[256]) query(q) tensor<bfloat16>(x[1024]) } first-phase { .. } second-phase { .. } global-phase { .. } }\n\nAll the above examples demonstrate using MRL flexibility; the following\ndemonstrates BQL and combining MRL with BQL.\n\nThe following signals that we want the BQL representation by changing the\ntensor type definition to int8. The output of the internal embedder inference\nis a 1024-dimensional vector with float values. Each dimension value is\nconverted to a 1/0 representation using the threshold function (>0). Finally,\nthose 1024 bits are packed into a 128-dimensional int8 vector representation.\n\n    \n    \n    schema doc { document doc { field text type string {..} } field bq_embedding type tensor<int8>(x[128]) { indexing: input text | embed mxbai | attribute | index attribute { distance-metric: hamming } } }\n\nNotice that for BQL representation, we use the hamming distance metric.\n\nWe can combine MRL with BQL by defining fewer int8 dimensions, here using 64:\n\n    \n    \n    schema doc { document doc { field text type string {..} } field mrl_bq_embedding type tensor<int8>(x[64]) { indexing: input text | embed mxbai | attribute | index attribute { distance-metric: hamming } } }\n\nIn the example above, the embedder uses the first 8*64 = 512 dimensions and\nquantizes those 512 floats into 512 bits, packed into a 64-dimensional int8\nvector.\n\n    \n    \n    schema doc { document doc { field text type string {..} } field mrl_bq_embedding type tensor<int8>(x[32]) { indexing: input text | embed mxbai | attribute | index attribute { distance-metric: hamming } } field mrl_embedding type tensor<bfloat16>(x[512]) { indexing: input text | embed mxbai | attribute attribute { paged distance-metric: prenormalized-angular } } }\n\nIn the above case, we use 32*8 = 256 first float dimensions before\nbinarization of those into a 32-dimensional int8 vector. This example also\nproduces a 512-dimensional bfloat16 version (again using paged) for use in\nranking phases. As in the previous examples, there is one embedding model\ninference pass since the input text and embedder id are the same.\n\n## Adaptive retrieval\n\nThe multiple vector representations from the same model inference can be used\nin an adaptive retrieval pipeline. In such a pipeline, the coarse-level\nrepresentation is used for efficient candidate retrieval and the finer\nrepresentation in ranking phases.\n\nThis type of ranking pipeline optimizes the amount of computation involved and\nallows flexibility in storage tiering. The coarse-levelrepresentation can be\nstored in-memory, while finer (but larger) representations can be stored on\ndisk and paged on-demand in a ranking phase that typically handles just a few\nhundred random page-ins.\n\nA promising storage-effective way to retain higher accuracy is to add a re-\nranking phase that uses the full float version of the query vector but without\nadditional storage penalty other than the binarized document vector\nrepresentation. From Binary and Scalar Embedding Quantization for\nSignificantly Faster & Cheaper Retrieval:\n\n> Yamada et al. (2021) introduced a rescore step, which they called rerank, to\n> boost the performance. They proposed that the float32 query embedding could\n> be compared with the binary document embeddings using dot-product. In\n> practice, we first retrieve rescore_multiplier * top_k results with the\n> binary query embedding and the binary document embeddings \u2013 i.e., the list\n> of the first k results of the double-binary retrieval \u2013 and then rescore\n> that list of binary document embeddings with the float32 query embedding.\n\nWe discussed this paper and this type of phased pipeline using coarse-level\nhamming distance for candidate retrieval in our series on billion-scale vector\nsearch. In 2021, there weren\u2019t any available text embedding models that were\ntrained with these objectives. That has changed with both Cohere binary\nembeddings and mixedbread.ai\u2019s binary embeddings.\n\nThe inverted hamming distance (1/(1 + hamming_distance)) is an approximation\nof the float dotproduct between the normalized query and document vectors\n(which equals the cosine similarity for normalized vectors).\n\nHamming distance is a coarse-level metric since it only takes N discrete\nunique values, where N is the number of bits. The re-scoring phase described\nabove re-orders the retrieved vectors from the coarse-level search and lifts\naccuracy retention to 95-96% of using the original float representations of\nboth the query and document.\n\nVespa has a built-in ranking function for unpacking the int8 representation to\na float vector representation.\n\nIllustration of binarization performed by the Vespa hugging-face-embedder and\nhow we can use the unpack_bits ranking function in ranking expressions.\n\nThe following example presents a comprehensive schema where the Hamming\ndistance is the coarse-level search metric. The most promising candidates from\nthe coarse-level search are re-ranked utilizing a second-phase expression that\nincorporates the unpacked float representation of the binary vector.\n\n    \n    \n    schema doc { document doc { field text type string {..} } field embedding type tensor<int8>(x[64]) { indexing: input text | embed mxbai | attribute | index attribute { distance-metric: hamming } } } rank-profile adaptive-rescoring { inputs { query(q) tensor<float>(x[512]) query(q_binary) tensor<int8>(x[64]) } function unpack_to_float() { expression: 2*unpack_bits(attribute(embedding), float)-1 } first-phase { expression: closeness(field, embedding) # inverted hamming } second-phase { expression: sum(query(q) * unpack_to_float) # dot product } }\n\nWith this schema, we can adaptively re-rank using the full precision query\nvector representation (MRL chopped to 512) with the unpacked (lossy) float\nversion.\n\nAt query time, we produce two query vectors with a single inference pass.\n\n    \n    \n    { \"yql\": \"select * from doc where {targetHits:200}nearestNeighbor(embedding,q_binary)\", \"input.query(q_binary)\": \"embed(mxbai,@text)\", \"input.query(q)\": \"embed(mxbai, @text)\", \"text\": \"my query string that is embedded\", \"ranking\": \"adaptive-rescoring\" }\n\nThe targetHits is the number of hits we want to expose to the configurable\nranking phases (per node involved in the query).\n\n## Serving Performance & Cost Reduction\n\nReducing the dimensionality and type of the vectors slashes memory and storage\ncosts, however, it also has a substantial effect on similarity search\nperformance. The employed similarity metric is at the core of embedding-based\nretrieval (which must match the metric used when learning the\nrepresentations).\n\nThe time required to compute the distance metric between two vectors (v1, v2)\naffects the overall performance of both approximate and exact nearest neighbor\nsearch algorithms. The main difference lies in the number of comparisons\nneeded. With MRL, where only the first k dimensions are retained, the\nperformance improvement is linearly proportional to the number of dimensions.\nFor example, reducing the number of float dimensions from 1024 to 512 halves\nthe complexity, while reducing from 1024 to 256 reduces it by a factor of\nfour.\n\nThe above is from a Vespa performance test (GitHub) that uses the\nmixedbread.ai embedding model with support for MRL and BQL and reports the\nend-to-end (average) latency (ms) for exact nearest neighbor search\n(targetHits = 100). This test uses the most optimized distance metric for\nfloat vectors in Vespa, prenormalized-angular , which can only be used for\nvectors that are normalized to the same length. The blue line represents\n1024-dimensional vectors, while the orange line represents 512-dimensional\nvectors. It is important to note that this is an exact search, which performs\nup to 100K comparisons. This is different from approximate searches, which are\nmuch faster due to significantly fewer distance computations (with a\ndegradation in accuracy). The graph above demonstrates that reducing the float\ndimensions via MRL from 1024 to 512 results in a two-fold speedup.\n\nWhen converting from float vectors with cosine similarity to binary vectors,\nwe also transition from one distance metric to another. With binary vectors,\nthe Hamming distance becomes the appropriate measure. The Hamming distance can\nbe calculated using efficient CPU popcount instructions over the result of a\nbitwise exclusive OR (XOR) operation between the two binary bit vectors.\nCompared to floating-point dot products, hamming involves fewer CPU\ninstructions and less data to move between the CPU(s) and memory.\n\nSimilar to the one comparing float representations, the performance test\ndemonstrates that calculating the hamming distance over 512-dimensional bit\nvectors is significantly more efficient than floating-point multiplications.\n\nCalculating the hamming distance is approximately 20 times faster (2ms),\nenabling users to experience faster search and higher query throughput with\nthe same resources. In practical terms, organizations can reduce CPU-related\ncosts by 20x while maintaining the same query throughput as when using float\nembeddings.\n\nDue to the low computing complexity and low latency search, many use cases\nwill not need to enable HNSW indexing; an exact search might be all you need.\n\nThe above is part of the same test, which tests query throughput, and where\nthe hamming metric and binary representations are much more efficient than\ntheir float counterparts. The tests run on the same hardware, embedding the\nsame data. Approaching 10,000 queries per second with 100,000 vectors equals\nclose to 1B hamming distance computations per second!\n\nThe huggingface-embedder functionality mentioned in the blog post makes it\npossible to conduct these experiments by creating multiple vector\nrepresentations from the same inference call, eliminating additional inference\ncosts (and time). The efficiency ensures that the test remains within the\nscope of the CI/CD pipeline at Vespa, where builds undergo performance testing\n(as seen on the x-axis above).\n\n## Summary\n\nIn this blog post, we have discussed the benefits of using Matryoshka\nRepresentation Learning (MRL) and Binary Quantization Learning (BQL) for\nembedding inference and vector indexing in Vespa. We demonstrated how to use\nboth techniques in Vespa, with open-source models that you can embed in Vespa\nusing the Vespa hugging-face-embedder functionality. Vespa also has first-\nclass support for binary vectors if you want to use embedding provider APIs,\nsee the comprehensive resources on using Cohere binary embeddings with Vespa.\n\nThe unique aspect of MRL and BQL is that they introduce minimal computational\noverhead during embedding model training. Both techniques are post-processing\nsteps performed after model inference. The loss function must consider the\ndifferent representations and similarities, but the additional loss function\ncost is insignificant compared to the model forward pass. With this negligible\nadditional training cost, learning multiple representations instead of a\nsingle float vector representation allows for significantly cheaper and more\npractical retrieval implementations for online serving.\n\nWe are very excited about this direction, as it unlocks many new use cases\nthat are no longer prohibitively expensive to serve in production\u2014making more\nunstructured data useful.\n\nThe new Vespa embedder functionality for MRL and BQL is available in Vespa\n8.332.5 and above; Vespa cloud tenants are already upgraded to this version.\n\n## FAQ\n\nHow does MRL/BQL compare with other retrieval and ranking strategies?\n\nBoth MRL and BQL prioritize reducing costs, even if it means sacrificing a bit\non accuracy. For instance, these can be employed for initial retrieval,\nfollowed by re-ranking utilizing more advanced techniques. In the context of\nsearch, MRL and BQL serve to lower the computational and storage costs\nassociated with the initial retrieval stage. This reduced latency (and\ncompute) also allows pipelines to incorporate more complex models during re-\nranking while adhering to overall latency SLAs (service level agreements).\n\nDoes Vespa support combining filtering with nearestNeighbors and hamming\ndistance?\n\nYes, hamming distance over binary vectors represented in int8 is a native (and\nmature) feature in Vespa. Hamming is exposed as a native distance metric. The\nnearest neighbor operator can be combined with filters in the Vespa query\nlanguage. See the practical guide to using Vespa nearest neighbor search.\n\nCan Vespa combine MRL/BQL with multi-vector indexing?\n\nYes, it is also one of the most exciting use cases for binarized vectors.\nLonger texts must be chunked to fit into the embedding model capability\n(longer text gives a more diluted representation).\n\nWith the low cost of binary vectors, we can represent longer texts with more\nchunks as vectors without significantly increasing costs. All the new\nhuggingface-embedder functionality also works for array<string> inputs to\nrepresent chunks in the same retrievable unit (document).\n\nSee Multilingual Hybrid Search with Cohere binary embeddings and Vespa for an\nexample that uses multi-vector binary vector indexing.\n\nCan you combine binary vectors with regular text search to implement hybrid\nsearch?\n\nYes, here is a concrete example using Cohere binary embeddings.\n\nHow does binary quantization relate to scalar quantization?\n\nBoth compression techniques aim to reduce storage and computation costs.\n\nScalar quantization (SQ) compresses float dimensions into int8 by utilizing a\nsample of vectors, preserving dimensionality but only achieving a compression\nfactor of 4. It involves mapping a continuous range of float values to a set\nof discrete int8 values, representing 256 distinct levels. SQ requires a large\nset of vectors to learn the conversion for each dimension.\n\nWith a BQL-optimized embedding model, this type of analysis is included in the\nmodel training phase, as it has learned a BQ-compatible representation.\n\nI\u2019m confused about int8 and using it to represent binary vectors\n\nint8 vectors can represent two distinct types: binary vectors and vectors with\nthe same dimensionality as the original float representation using scalar\nquantization. Notably, the latter approach utilizes the same distance metric\nas the float representation, such as the angular distance. This latter\napproach reduces the storage footprint by 4x, not 32x, as with BQL.\n\nHow does binary quantization relate to product quantization?\n\nProduct quantization (PQ) is used in several ANN (approximate nearest\nneighbor) search algorithms to compress large batches of vectors, exploiting\nthe data distribution in these vectors. With BQL-optimized embedding models,\nwe don\u2019t need any post-processing of the vectors, as the model has been\ntrained to output vectors that are compatible with Binary Quantization (BQ).\nIn addition, BQL enables us to recreate the original lossy float\nrepresentation by unpacking the bit representation, which can be used for re-\nscoring or as input to a DNN ranking model.\n\nI\u2019ve read that BQ is lossy and that the quality and accuracy is bad?\n\nBQ is lossy because it converts a continuous range of float values into bits,\nand using it on vectors from an embedding model trained without the BQL\nobjective may significantly reduce accuracy. Employing these techniques on\nvectors from a model that lacks training with BQL or MRL objectives would\nresult in a more significant degradation in accuracy compared to models\ntrained with such an objective.\n\nI\u2019ve read that HNSW uses a lot of memory, how does binary vectors impact HNSW?\n\nContrary to popular belief, the HNSW graph has a minimal impact on total\nmemory consumption. Instead, the source vectors are the primary contributors\nto memory usage. Binarization offers a practical solution by reducing the\nstorage requirements of the source vectors. It\u2019s worth noting that both read\n(queries) and insert (add/update) operations necessitate graph traversal,\nwhich entails similarity calculations involving the retrieval of stored\nvectors.\n\nDue to the random access pattern (to fetch source vectors) caused by HNSW\ngraph traversal, keeping the vectors in memory is crucial for achieving\nmeaningful insert and read performance. With binarization we both reduce the\nmemory footprint of the vectors (32/64x) and also increase both search and\nindexing performance (since distance calculations are so much cheaper). In\naddition, there are many cases where you don\u2019t need to build a HNSW index for\nretrieval, but instead use the vector representations during ranking phases.\nSee Redefining Hybrid Search Possibilities with Vespa for a discussion on\nthis.\n\nHow does BQ compare with the compression technique used with ColBERT?\n\nVespa\u2019s ColBERT embedder (blog post) also supports binarization to reduce the\nstorage footprint of the token-level vectors. It\u2019s essentially the same type\nof compression, thresholding normalized float vectors at 0 and packing them\ninto int8. mixedbread.ai has also trained a ColBERT checkpoint, using this in\ncombination with their binarized single-vector model is an attractive\npipeline, where retrieval phase can use the binarized single-vector\nrepresentation, and ColBERT can be used to re-rank the candidates retrieved by\nthe coarse-level hamming search.\n\nI want to use an embedding provider instead and feed the vectors to Vespa, how\ncan I do that?\n\nMixedbread.ai offers an embedding API for getting multiple vector\nrepresentations for the same inference call. This includes binary embeddings (\nSee the encoding_format parameter).\n\nCohere has embedding models and API that support obtaining multiple vector\nrepresentations for the same inference API call (including binary int8 and\nscalar quantized int8). See our comprehensive guideon using Cohere embeddings\nin combination with Vespa.\n\nI use sentence-transformers, how can I use sentence-transformers in\ncombination with Vespa?\n\nSentence-transformers have great support for quantization and binarization and\nthis is a practical example of how to use sentence-transformers in combination\nwith Vespa.\n\nI have more questions; I want to learn more! For those interested in learning\nmore about Vespa or binary vectors and Vespa\u2019s capabilities, join the Vespa\ncommunity on Slack or Discord to exchange ideas, seek assistance from the\ncommunity, or stay in the loop on the latest Vespa developments.\n\n  * #embeddings\n\n\u00ab Vespa Newsletter, April 2024\n\nNever miss a story from us, subscribe to our newsletter\n\nCopyright \u00a9 2024 Vespa Blog - Subscribe to updates - Vespa Blog RSS feed\n\nMediumish Jekyll Theme by WowThemes.net\n\n", "frontpage": false}
