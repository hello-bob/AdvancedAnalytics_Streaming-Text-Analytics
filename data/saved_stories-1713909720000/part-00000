{"aid": "40133454", "title": "Make the Most of Retrieval Augmented Generation", "url": "https://vectorize.io/make-the-most-of-retrieval-augmented-generation-rag/", "domain": "vectorize.io", "votes": 2, "user": "bytearray", "posted_at": "2024-04-23 15:59:20", "comments": 0, "source_title": "Make the Most of Retrieval Augmented Generation - Vectorize", "source_text": "Make the Most of Retrieval Augmented Generation - Vectorize\n\nWe use essential cookies to make our site work. With your consent, we may also\nuse non-essential cookies to improve user experience, personalize content, and\nanalyze website traffic. For these reasons, we may share your site usage data\nwith our analytics partners. By clicking \u201cAccept,\u201d you agree to our website's\ncookie use as described in our Cookie Policy. You can change your cookie\nsettings at any time by clicking \u201cPreferences.\u201d\n\n  * Use Cases\n\n    * Question Answering Systems\n    * AI Copilots\n    * Call Center Automation\n    * Content Automation\n    * Hyper-personalization\n  * Blog\n  * About\n  * Learn\n\n    * Prompt Engineering\n    * Retrieval Augmented Generation (RAG)\n    * Vector Database Guide\n  * Contact\n\nContact Us\n\n#### Be on of the first to try Vectorize!\n\nEdit Content\n\nRAG\n\n# Make the Most of Retrieval Augmented Generation\n\nApril 23, 2024 Hadi Azzouni No comments yet\n\nRetrieval Augmented Generation (RAG) is emerging as the de facto architecture\nfor building applications with LLMs. There are three layers to an LLM app:\n\n  1. The LLM itself. Examples: Llama 2, GPT4, etc.\n  2. Business data\n  3. Business logic\n\nMost modern LLMs are trained on datasets that roughly contain the whole text\navailable on the web. This makes these LLMs great for generic tasks that\ninvolve language understanding, reasoning over text data, etc.\n\nHowever, most business use cases involve internal business data. For example,\nto build a custom medical chatbot assistant for a hospital, you would need to\nsupply proprietary medical data that only the hospital has access to and is\nnot available on the web. Similarly, to build a devops assistant for your\ninfrastructure, you would need to supply your internal infrastructure data to\nthe LLM.\n\nThis is where RAG comes in. The basic idea behind RAG is to extend the context\nof the core LLM with business data and business logic to build a useful\nbusiness app.\n\nThat said, the question \u201cshould I adopt a RAG architecture?\u201d does not make\nsense anymore. If you want to use an LLM to perform business-related tasks\nlike question answering, recommendations, providing assistance, etc., then you\nHAVE to use some sort of RAG.\n\nThat is the line of thinking one should have about RAG. Now, let\u2019s dig deeper\ninto the specifics:\n\n  1. Extend LLM context to private data: As mentioned before, LLMs are trained on public datasets collected from the internet; hence, they never saw your proprietary medical documents or industrial designs.\n\n  2. Enhance accuracy and reliability: Some tasks are generic and can be performed by an LLM without additional context data. However, adding that private context data would greatly improve:\n\n    1. Accuracy: the LLM would answer more questions correctly when additional context is provided.\n    2. Reliability: LLMs hallucinate, and there is nothing we can do to completely eliminate that. Even the best and largest LLMs out there would spit nonsense at times. Grounding the LLM\u2019s answers in context data works like a guardrail against hallucinations.\n  3. Provide up-to-date information: LLMs are trained once, then put in inference mode after that. Hence, the data cut-off is a major characteristic of an LLM. As of today, the latest training data for OpenAI models, for example, dates back to December 2023. The consequence is that these models are pretty useless in answering anything around the recent Iran-Israel events, for example (date: April 13, 2024). To augment GPT4 with this data, you would need to hook it to real-time news articles, for example.\n\nlllm list\n\n4\\. Enhance user trust: Say you ask GPT4, \u201cWhat is a major event that happened\non October 7th, 2023?\u201d. Since the data cut-off is December 2023, GPT4 may\nanswer, \u201cA missile attack was launched on Israel at 7 a.m.,\u201d for example.\nHowever, although the main answer is correct, the specifics (missile attack, 7\na.m.) need some support to be trusted. RAG allows you to provide such support\nby ingesting news articles, prompting GPT4 to answer from these articles and\ncite their sources.\n\nSome epic LLM hallucinations and bad citation examples include this one: In\nthe legal case of Mata v. Avianca, a New York attorney used ChatGPT for legal\nresearch, which led to the inclusion of fabricated citations and quotes in a\nfederal court case. The attorney, Steven Schwartz, admitted to using ChatGPT,\nwhich highlights the direct consequences of relying on AI-generated content\nwithout grounding in context data (and without verification either in this\ncase).\n\n5\\. Reduce costs: This is more relevant in the case of deploying and managing\nyour own LLM. Since RAG extends the LLM knowledge and context with external\ndata, you remove the need for retraining or fine-tuning the LLM based on\nrecent data. You also don\u2019t need to fine-tune with your proprietary datasets.\nTraining and fine-tuning LLMs can be very costly and may take days to months\nto complete. Another interesting aspect of RAG is that you are technically\noffloading the retrieval step from internal LLM retrieval (KV cache) to\nexternal retrieval. Recent LLMs have a larger context, which allows them to\ningest large data segments and ask questions about them; however, inferences\non large chunks of data are both slower and costlier than external RAG\nretrieval.\n\n6\\. Improve Speed: As we already explained, when data is larger than what we\ncan input to the LLM, we don\u2019t have options but to use RAG. Now, say you have\na large context LLM, like Gemini 1.5 with a 1 million token context window,\nthen you may think of giving it all the data at once (if less than 1 million\ntokens). In this case, inference would take a long time and cost you much more\ncompared to doing external RAG retrieval.\n\n## Some examples of RAG systems in production\n\nPerplexity (.ai): Perplexity is a good example of a web RAG. It is an answer\nengine that answers user queries from the web and cites sources.\n\nCursor (.sh): Cursor is a great coding assistant. It is a RAG system that\nindexes your code base, allowing you to ask questions about your code and get\ncontextual answers.\n\nHeyCloud (.ai): HeyCloud is a specialized AI assistant for DevOps teams. It\nintegrates with your cloud stack (like AWS or Kubernetes) and allows you to\nask DevOps and cloud-related questions like:\n\n  * How many users on my AWS account have permissions to see billing information?\n  * How much do I spend on EC2 in the London region per month?\n  * Create an EKS cluster with 20 nodes for under $100 per month.\n\n## Conclusion\n\nIn this article, we went through the main reasons RAG is relevant (or\nnecessary) in almost any LLM app. One more thing I want to say here: always\nstart simple. RAG systems can quickly get out of control and become\nuntraceable; start with a tiny part of the data, preferably with an API-\nprovided LLM like GPT4. Once you have the first toy version complete and a\nbaseline performance, then start adding more advanced algorithms and\nstrategies.\n\n### Share this:\n\n  * Twitter\n  * LinkedIn\n  * Facebook\n  * Reddit\n  * Pinterest\n  * Threads\n  * X\n\n### Related\n\n### Leave a ReplyCancel reply\n\n#### Search\n\n#### Categories\n\n#### Recent posts\n\n  * Make the Most of Retrieval Augmented Generation\n\n  * Triumph Over Data Obstacles In RAG: 8 Expert Tips\n\n  * How to build a better RAG pipeline\n\n#### Tags\n\nRAG Retrieval Augmented Generation\n\nThe easiest, fastest way to connect your data to your LLMs.\n\n##### Resources\n\n  * Support center\n  * Documentation\n  * Community\n  * Hosting\n\n##### Company\n\n  * About us\n  * Latest news\n  * Contact us\n  * Resources\n\n\u00a9 Vectorize AI, Inc, All Rights Reserved.\n\n  * Terms & Conditions\n  * Privacy Policy\n\n## Discover more from Vectorize\n\nSubscribe now to keep reading and get access to the full archive.\n\nContinue reading\n\nLoading Comments...\n\n", "frontpage": false}
