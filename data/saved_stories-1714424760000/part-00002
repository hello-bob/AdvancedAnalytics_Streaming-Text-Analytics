{"aid": "40199253", "title": "Lambda, Kappa, Delta Architectures for Data", "url": "https://subrabytes.dev/dataarchitectures", "domain": "subrabytes.dev", "votes": 1, "user": "acossta", "posted_at": "2024-04-29 15:04:50", "comments": 0, "source_title": "Lambda, Kappa, Delta Architectures for Data", "source_text": "Lambda, Kappa, Delta Architectures for Data | The Pipeline\n\nLambda, Kappa, Delta Architectures for Data - The Pipeline\n\n# Lambda, Kappa, Delta Architectures for Data\n\nPosted by Venkatesh Subramanian on April 21, 2024 \u00b7 8 mins read\n\nIn this post we will review few Data architecture patterns that started\nevolving during the big data era till recently, and their uses.\n\nA historical timeline of these patterns and a few surrounding events as\nfollows to set the context. 2004 - Apache Hadoop created. Hadoop distributed\nfile system and MapReduce become the foundation of big data processing. 2008 -\nApache Cassandra developed as distributed database to handle large amounts of\ndata across commodity servers. 2011 - Lambda Data architecture introduced by\nNathan Marz. 2012 - Apache Kafka developed, providing a distributed event\nstreaming platform. 2014 - Jay Kreps, one of the co-founders of Apache Kafka,\nproposed the Kappa Architecture. 2016 - Apache Flink was developed, providing\nframework and distributed processing engine for stateful computations over\nunbounded and bounded data streams. 2019 - Databricks introduced Delta lake,\nan open-source storage layer that brings ACID transactions to Apache Spark and\nbig data workloads. 2021 - Delta lake architectures mature, and widely adopted\nfor simplifying data engineering needs.\n\nLambda architecture is a good starting point to trace this evolution.\n\nFig 1: Lambda architecture\n\nIn this pattern we have provision for dealing with both batch processing and\nstream processing, in an elegant manner. As we can see from fig 1, batch and\nstream pipelines are kept separate. This separation is good as it is more\nefficient to process real time only when you have to, else it is better to\ntolerate the latency and do it in batch mode. When doing in batch mode you can\neven pick up schedules when shared server utilization is low, thereby avoid\nspinning up additional servers when their utilization is high.\n\nThe batch layer in Lambda will have a set batch size and a cutoff time. This\ndata will be first appended without any modifications in the master system of\nrecord -immutable appends of any changes. So we don\u2019t update or delete the\noriginal row, instead add an extra entry indicating its update with a time\nstamp. This way the master data stores the unfiltered source of truth of all\ntransactions. Next, there is a serving layer that will create views or\naggregations of most commonly used data access patterns by queries. So, this\nway the queries can benefit from a pre-computation, which can significantly\nreduce latencies.\n\nThe stream data will always have the last 1-2 hours window of data only. So if\nthe query needs something recent in this window then it will get the answer\nfrom the streaming layer versus the views of serving layer above.\n\nThis pattern evolved in context of big data, however it can also be used with\nsmall data. It is also simple to implement in the latter scenarios without\nneeding specialized frameworks. However it does have some limitations. In case\nof medium to big data scenarios you will need separate batch and streaming ETL\npipelines. So now code is split into 2 parts for serving and may get difficult\nto maintain. Architecture will always have tradeoffs, so we just need to\nevaluate if a given set of tradeoffs will be fine for your team context. The\nplus in this pattern is that it is so intiutive as to how the pieces work, so\none can debug issues at runtime in a transparent manner. I also like the fact\nthat concerns of batch and streaming are separate and can be separately\noptimized based on the dynamics of each type of processing.\n\nThe next pattern is called Kappa. This came after Lambda and proposed to do\naway with the batch layer, and instead do all processing in a single stream\nlayer built largely on top of Kafka. So this is very advantageous if real time\nprocessing of data such as those coming from IoT sensors and time sensitive\nalerts are the primary part of the application. It also simplifies maintenance\nby localizing the code in one streaming framework. However, this also has a\nlimitation. Streaming data frameworks use queues to store incoming messages\nand then process. This will work for limited data size, however if there is\nlarge data needing pre-processing then the queues will not be able to handle\nthe load. Besides, not all queries need low latency stream response - which\ncan be unnecessary over-engineering.\n\nFig 2: Kappa architecture\n\nThe latest pattern which aims to overcomes limitation of Lambda and Kappa is\nthe Delta architecture. This brings to table an unified platform for both\nbatching and streaming. It achieves this by leveraging micro-batching\ntechniques- and is also the basis of modern Lakehouse architectures. Delta\narchitecture simplifies by unifying batch and streaming data\u2019s ingestion,\nprocessing, storing, and management in same pipeline by using a continuous\ndata flow model.\n\nFig 3: Delta architecture Source of this image: Databricks\n\nAs we can see from this diagram, there is a bronze, silver, and gold layer-\ncorresponding to ingested, refined, and ready to use data. Delta lake is built\non top of Parquet files with transaction log and ACID (Atomicity, Consistency,\nIsolation, Durability) transaction abilities, enabling robust and reliable\ndata management. It also leverages Apache Spark compute power with the Delta\nlake storage management for achieving unified analytics capabilities. Platform\nproviders such as Databricks and Snowflake provide managed services of this\nDelta architecture for easy consumption by development teams. This\nsimplification for the end user is achieved by heavy lifting by Apache\nframwork and managed services providers. While this is elegant for big data,\nit may still be an overkill for small data pipelines that need to deal with\nbatch and streaming flows. For these small data scenarios Lambda can be easily\nadapted by developers, even without any heavy framework dependencies. In big\ndata scenarios one can use Delta architectures and still control when batch\njobs run. So you could use a combination of data filtering and scheduling for\ncertain batch data, while the rest of the data can be continuously processed\nin micro-batches via the unified delta architectures. This way developer can\nget best of both the worlds - simplified unified processing and differential\ntreatment of certain batch loads that can be scheduled when optimal to do so.\nOne additional point is the testing of latencies in unified platforms, as\nbatch and stream are combined. Although modern frameworks like Spark optimize\nthe unification, there could still be some impact on streaming latency based\non the workload dynamics.\n\nIn conclusion I will say that Lambda, Kappa, and Delta are all relevant and\ntimeless architectures, each with their strengths and limitations - based on\nthe specific requirements of your use case. Look at the nature of your data,\nrequirements of data processing tasks, and resources available to you- and\nthen decide the architecture pieces to deliver the exact solution needed by\nyour customers.\n\n\u2190 Previous Post\n\nCopyright \u00a9 Venkatesh Subramanian 2024\n\n", "frontpage": false}
