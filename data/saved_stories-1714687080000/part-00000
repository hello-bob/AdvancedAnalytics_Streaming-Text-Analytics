{"aid": "40237577", "title": "An Evaluation of RAG Retrieval Chunking Methods", "url": "https://superlinked.com/vectorhub/articles/evaluation-rag-retrieval-chunking-methods", "domain": "superlinked.com", "votes": 1, "user": "sorenbs", "posted_at": "2024-05-02 15:47:42", "comments": 0, "source_title": "An evaluation of RAG Retrieval Chunking Methods | VectorHub by Superlinked", "source_text": "An evaluation of RAG Retrieval Chunking Methods | VectorHub by Superlinked\n\nVDB Comparison\n\nBuilding BlocksArticlesContributingVDB Comparison\n\nMost Recent\n\nTable of Contents\n\nDatasets\n\nChunking, with different parameters and embedding models\n\nSummary of outcomes\n\nModel\n\nChunking\n\nReranking\n\nDataset HotpotQA results\n\nDataset SQUAD results\n\nDataset QuAC results\n\nIn sum...\n\nContributors\n\n# An evaluation of RAG Retrieval Chunking Methods\n\nLast Update: April 10, 2024\n\nChoosing a RAG Retrieval method that suits your use case can be daunting. Are\nsome methods better suited to specific tasks and types of datasets than\nothers? Are there trade-offs between performance and resource requirements you\nneed to be aware of? How do different chunking techniques, embedding models,\nand reranking interact to impact performance results? Evaluation can help\nanswer these questions.\n\nTo evaluate the relative performance of several different, prominent chunking\nmethods within the Retrieval component of a RAG system, we looked at how they\nperformed 1) on different leaderboard datasets, 2) using different parameters\nand embedding models, and 3) along several ranking metrics - MRR, NDCG@k,\nRecall@k, Precision@k, MAP@k and Hit-Rate, with k\u2019s of 1, 3, 7, and 10.\n\nBelow, we present our datasets, chunking methods, embedding models, rerankers,\nand, finally, the outcomes of our research, within each dataset, and across\nall datasets.\n\n## Datasets\n\nWe performed our evaluation of chunking methods on the following three\ndatasets:\n\n  1. Dataset HotpotQA\n  2. Dataset SQUAD\n  3. Dataset QuAC\n\nThese datasets are widely used benchmarks in the field of Question Answering.\nThey also contain a wide variety of questions, so we can use them to evaluate\nthe performance of different methods across different query types.\n\n## Chunking, with different parameters and embedding models\n\nWe experimented with LlamaIndex- and LangChain-implemented chunking methods,\nusing different parameter combinations and embedding models:\n\n  1. SentenceSplitter\n  2. SentenceWindowNodeParser\n  3. SemanticSplitterNodeParser\n  4. TokenTextSplitter\n  5. RecursiveCharacterTextSplitter (LangChain)\n\nWe chose embedding models from the top ranks of the MTEB Leaderboard:\n\n  1. BAAI/bge-small-en-v1.5\n  2. BAAI/bge-large-en-v1.5\n  3. BAAI/bge-m3\n  4. sentence-transformers/all-distilroberta-v1\n  5. WhereIsAI/UAE-Large-V1\n\nIn addition to the above single-vector representation models, we also tested a\nmulti-vector embedding and Retrieval model, ColBERT v2 using RAGatouille.\n\nColBERT v2 embeds each text as a matrix of token-level embeddings, permitting\nmore fine-grained interactions between parts of the text than with single-\nvector representation. RAGatouille provides optional chunking using LlamaIndex\nSentenceSplitter.\n\nFinally, we also tested the effect on performance of different rerankers after\nRetrieval:\n\n  1. cross-encoder/ms-marco-TinyBERT-L-2\n  2. sentence-transformers/all-mpnet-base-v2\n  3. BAAI/bge-reranker-base\n  4. BAAI/bge-reranker-large\n  5. WhereIsAI/UAE-Large-V1\n\n## Summary of outcomes\n\n### Model\n\nColBERT-based embedding - using SentenceSplitter - and Retrieval proved to be\nthe most efficient method of improving result accuracy and relevance, with an\naverage performance advantage of about 10% over the second best method. This\nmethod's performance superiority held for all the datasets we tested.\n\n### Chunking\n\nThe SentenceSplitter performed better than the other chunking methods,\ncontrary to our expectations. We had intuitively assumed that the naive\nSentenceSplitter (which takes in chunk_size and overlap parameters) would be\nthe least efficient chunking method, and that instead the\nSemanticSplitterNodeParser\u2019s performance would be superior - because, for any\ngiven sentence, the latter method creates chunks based on breakpoints computed\nfrom semantic dissimilarities of the preceding and succeeding sentences. This\nassumption turned out to be false. Why?\n\nThe superior performance of SentenceSplitter over SemanticSplitterNodeParser\nappears to illustrate that, despite the advantages of semantic evaluation:\n\n  1. a sentence is a very natural level of granularity for containing meaningful information, and\n  2. high semantic similarity measures can result from noise rather than meaningful similarities.\n\nVector representations of semantically similar sentences (provided there is no\nword overlap) are often more distant from each other than we might expect\nbased on the sentences\u2019 meaning. Words can mean different things in different\ncontexts, and word embeddings basically encode the \"average\" meaning of a\nword, so they can miss context-specific meaning in particular instances. In\naddition, sentence embeddings are aggregates of word embeddings, which further\n\"averages away\" some meaning.\n\nAs a result, retrieval performance can suffer when we 1) do our chunking by\nsplitting text on the basis of something other than the borders of sentences,\n2) merge text segments into groups on the basis of semantic similarity rather\nthan using a fixed length (i.e., basically 1-2 sentences).\n\n### Reranking\n\nUsing rerankers after retrieval (as a post-processing step) was very good at\nimproving result accuracy and relevance. In particular, reranker model cross-\nencoder/ms-marco-TinyBERT-L-2-v2, with only 4.3M parameters, was highly\nefficient in terms of inference speed, and also outperformed the larger models\nconsistently across all three datasets.\n\nNow, let\u2019s take a look at our dataset-specific outcomes.\n\n## Dataset HotpotQA results\n\nChunking methods performance results on HotpotQA dataset (above)\n\nOn the HotpotQA dataset, the best performance came from ColBERT v2, which used\nthe default SentenceSplitter chunker from LlamaIndex, with a\nmax_document_length of 512. This method achieved an MRR of 0.3123 and\nRecall@10 of 0.5051.\n\nThe second best performance came from using SentenceSplitter with a chunk size\nof 128, embedding model WhereIsAI/UAE-Large-V1, with 335M parameters, and\nreranker cross-encoder/ms-marco-TinyBERT-L-2-v2. In fact, all the other\nsingle-vector embedding models, combined with SentenceSplitter chunking and\nthe TinyBERT reranker, performed about as well as WhereIsAI/UAE-Large-V1, with\nminor differences. This includes model BAAI/bge-small-en-v1.5; it performed on\npar with the larger models despite being only 1/10th their size.\n\nThe single-vector embedding models performed about as well as each other\nwhether reranking was applied or not. Reranking improved their performance by\nabout the same percentage for all these models. This was true not just for\nthis dataset, but also across our other datasets (SQUAD and QuAC).\n\n## Dataset SQUAD results\n\nChunking methods performance results on SQUAD dataset (above)\n\nOn the SQUAD dataset, the best ColBERT experiment produced an MRR of 0.8711\nand Recall@10 of 0.9581. These values are very high. We think this may suggest\nthat the model was trained on SQUAD, though the ColBERT v2 paper mentions only\nevaluation of the Dev partition of SQUAD, which we didn't use.\n\nOn this dataset, the BAAI/bge-m3 model, using the same cross-encoder/ms-marco-\nTinyBERT-L-2-v2 reranker, produced the second best results - an MRR of 0.8286\nand Recall@10 of 0.93. Without a reranker, BAAI/bge-m3\u2019s MRR was 0.8063 and\nRecall@10 was 0.93.\n\nBAAI/bge-m3\u2019s scores are also (like ColBERT\u2019s) high. It\u2019s possible that this\nmodel was also trained on SQUAD, but Huggingface doesn\u2019t provide an exhaustive\nlist of BAAI/bge-m3\u2019s training datasets.\n\nWe tested multiple rerankers on this dataset of 278M-560M parameters, but they\nperformed significantly worse than the small (TinyBERT) model, in addition to\nhaving much slower inference speeds.\n\n## Dataset QuAC results\n\nChunking methods performance results on QuAC dataset (above)\n\nOn the QuAC dataset, the ColBERT experiment achieved an MRR of 0.2207 and\nRecall@10 of 0.3144.\n\nThe second best performing model was BAAI/bge-large-en-v1.5 with\nSentenceSplitter, chunk size of 128 and chunk overlap of 16, combined with the\nsame TinyBERT reranker. The other models, when using the same reranker,\nperformed roughly on par with this model.\n\nWithout the reranker, the different chunking methods, with the exception of\nthe SemanticSplitter, would produce comparable results.\n\n## In sum...\n\nHere\u2019s a tabular summary of our best performing methods for handling RAG\nRetrieval.\n\nDataset| Model| Chunker| Reranker| MRR| Recall@10  \n---|---|---|---|---|---  \nAll datasets| ColBERT v2| SentenceSplitter| None| \\+ 8%| \\+ 12%  \nHotpotQA| ColBERT v2| SentenceSplitter| None| 0.3123| 0.5051  \nHotpotQA| WhereIsAI/UAE-Large-V1| SentenceSplitter| TinyBERT-L-2-v2| 0.2953|\n0.4257  \nSQUAD| ColBERT v2| SentenceSplitter| None| 0.8711| 0.9581  \nSQUAD| BAAI/bge-m3| SentenceSplitter| TinyBERT-L-2-v2| 0.8286| 0.93  \nSQUAD| BAAI/bge-m3| SentenceSplitter| None| 0.8063| 0.93  \nQuAC| ColBERT v2| SentenceSplitter| None| 0.2207| 0.3144  \nQuAC| BAAI/bge-large-en-v1.5| SentenceSplitter| TinyBERT-L-2-v2| 0.1975|\n0.2766  \n  \nOur best performing method for handling RAG Retrieval on all datasets was\nColBERT v2 with SentenceSplitter chunking.\n\nOur other (single-vector) embedding models, though trailing in performance\nbehind ColBERT v2 (with SentenceSplitter), tended to perform about the same as\neach other, both when they were combined with reranking and when they weren\u2019t,\nacross all three datasets.\n\nSentenceSplitter chunking surprised us by outperforming\nSemanticSplitterNodeParser, but upon further reflection, these outcomes\nsuggest that sentences are natural delimiters of meaning, and semantic\n\u201caveraging\u201d of meaning may miss context-specific relevance.\n\nFinally, reranker model TinyBERT proved to be the most efficient at improving\nmodel performance, outperforming even the larger rerankers.\n\n## Contributors\n\n  * Krist\u00f3f Horv\u00e1th, author\n  * M\u00f3r Kapronczay, contributor\n  * Robert Turner, contributor-editor\n\nStay updated with VectorHub\n\nContinue Reading\n\nBy subscribing, you agree to our Terms and Conditions.\n\nAbout\n\nCompanyCareers\n\nSupport\n\nContact UsTerms of UsePrivacy PolicyCookie Policy\n\nSocial\n\nGithubX (Twitter)LinkedIn\n\n", "frontpage": false}
