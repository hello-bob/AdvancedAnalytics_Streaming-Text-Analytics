{"aid": "40096284", "title": "Production-Scale Datacenter Profile", "url": "https://github.com/elastic/otel-profiling-agent", "domain": "github.com/elastic", "votes": 1, "user": "adif_sgaid", "posted_at": "2024-04-20 10:37:06", "comments": 0, "source_title": "GitHub - elastic/otel-profiling-agent: The production-scale datacenter profiler", "source_text": "GitHub - elastic/otel-profiling-agent: The production-scale datacenter\nprofiler\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nelastic / otel-profiling-agent Public\n\n  * Notifications\n  * Fork 200\n  * Star 1.7k\n\nThe production-scale datacenter profiler\n\n### License\n\nApache-2.0 license\n\n1.7k stars 200 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# elastic/otel-profiling-agent\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nrockdabootTemporary information for 3rd party pull requests (#21)Apr 18,\n20243b12f0d \u00b7 Apr 18, 2024Apr 18, 2024\n\n## History\n\n13 Commits  \n  \n### .github\n\n|\n\n### .github\n\n| Temporary information for 3rd party pull requests (#21)| Apr 18, 2024  \n  \n### config\n\n|\n\n### config\n\n| Initial commit| Apr 15, 2024  \n  \n### containermetadata\n\n|\n\n### containermetadata\n\n| Initial commit| Apr 15, 2024  \n  \n### debug/log\n\n|\n\n### debug/log\n\n| Initial commit| Apr 15, 2024  \n  \n### docs\n\n|\n\n### docs\n\n| Initial commit| Apr 15, 2024  \n  \n### host\n\n|\n\n### host\n\n| Initial commit| Apr 15, 2024  \n  \n### hostmetadata\n\n|\n\n### hostmetadata\n\n| Initial commit| Apr 15, 2024  \n  \n### interpreter\n\n|\n\n### interpreter\n\n| license: Add preamble to non-eBPF C/include files| Apr 15, 2024  \n  \n### legal\n\n|\n\n### legal\n\n| Initial commit| Apr 15, 2024  \n  \n### libpf\n\n|\n\n### libpf\n\n| license: Add preamble to non-eBPF C/include files| Apr 15, 2024  \n  \n### lpm\n\n|\n\n### lpm\n\n| Initial commit| Apr 15, 2024  \n  \n### maccess\n\n|\n\n### maccess\n\n| Initial commit| Apr 15, 2024  \n  \n### metrics\n\n|\n\n### metrics\n\n| Initial commit| Apr 15, 2024  \n  \n### pacmask\n\n|\n\n### pacmask\n\n| Initial commit| Apr 15, 2024  \n  \n### proc\n\n|\n\n### proc\n\n| Initial commit| Apr 15, 2024  \n  \n### processmanager\n\n|\n\n### processmanager\n\n| Initial commit| Apr 15, 2024  \n  \n### proto\n\n|\n\n### proto\n\n| Initial commit| Apr 15, 2024  \n  \n### reporter\n\n|\n\n### reporter\n\n| license: Add preamble to non-eBPF C/include files| Apr 15, 2024  \n  \n### support\n\n|\n\n### support\n\n| Makefile: allow docker targets to work on macOS (#17)| Apr 18, 2024  \n  \n### testsupport\n\n|\n\n### testsupport\n\n| Initial commit| Apr 15, 2024  \n  \n### tpbase\n\n|\n\n### tpbase\n\n| license: Add preamble to non-eBPF C/include files| Apr 15, 2024  \n  \n### tracehandler\n\n|\n\n### tracehandler\n\n| Initial commit| Apr 15, 2024  \n  \n### tracer\n\n|\n\n### tracer\n\n| Initial commit| Apr 15, 2024  \n  \n### utils\n\n|\n\n### utils\n\n| license: Add preamble to non-eBPF C/include files| Apr 15, 2024  \n  \n### .dockerignore\n\n|\n\n### .dockerignore\n\n| Run build container as user (#10)| Apr 18, 2024  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| Run build container as user (#10)| Apr 18, 2024  \n  \n### .golangci.yml\n\n|\n\n### .golangci.yml\n\n| Initial commit| Apr 15, 2024  \n  \n### AUTHORS\n\n|\n\n### AUTHORS\n\n| Initial commit| Apr 15, 2024  \n  \n### Dockerfile\n\n|\n\n### Dockerfile\n\n| Run build container as user (#10)| Apr 18, 2024  \n  \n### KNOWN_KERNEL_LIMITATIONS.md\n\n|\n\n### KNOWN_KERNEL_LIMITATIONS.md\n\n| Initial commit| Apr 15, 2024  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Initial commit| Apr 15, 2024  \n  \n### Makefile\n\n|\n\n### Makefile\n\n| Run build container as user (#10)| Apr 18, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| [README.md] Encourage 3rd parties to contribute PRs (#20)| Apr 18, 2024  \n  \n### cli_flags.go\n\n|\n\n### cli_flags.go\n\n| Initial commit| Apr 15, 2024  \n  \n### cli_flags_internal.go\n\n|\n\n### cli_flags_internal.go\n\n| Initial commit| Apr 15, 2024  \n  \n### go.mod\n\n|\n\n### go.mod\n\n| Initial commit| Apr 15, 2024  \n  \n### go.sum\n\n|\n\n### go.sum\n\n| Initial commit| Apr 15, 2024  \n  \n### main.go\n\n|\n\n### main.go\n\n| Initial commit| Apr 15, 2024  \n  \n### main_test.go\n\n|\n\n### main_test.go\n\n| Initial commit| Apr 15, 2024  \n  \n## Repository files navigation\n\nNote\n\nPlease be aware that we currently won't merge 3rd party PRs because this\nrepository is temporary. We are waiting for the decision of the OpenTelemetry\ntechnical commitee on the donation of this repository.\n\nIn case the donation gets accepted, this repository will move to the GitHub\nopen-telemetry organization, which requires signing a different CLA. At that\npoint we will start working on reviewing and merging 3rd party PRs.\n\n# Introduction\n\nThis repository implements a whole-system, cross-language profiler for Linux\nvia eBPF. The repository serves as a staging space in the process of donating\nthe agent to OpenTelementry.\n\n## Core features and strengths\n\n  * Implements the experimental OTel profiling signal\n  * Very low CPU and memory overhead (1% CPU and 250MB memory are our upper limits in testing and the agent typically manages to stay way below that)\n  * Support for native C/C++ executables without the need for DWARF debug information (by leveraging .eh_frame data as described in US11604718B1)\n  * Support profiling of system libraries without frame pointers and without debug symbols on the host.\n  * Support for mixed stacktraces between runtimes - stacktraces go from Kernel space through unmodified system libraries all the way into high-level languages.\n  * Support for native code (C/C++, Rust, Zig, Go, etc. without debug symbols on host)\n  * Support for a broad set of HLLs (Hotspot JVM, Python, Ruby, PHP, Node.JS, V8, Perl), .NET is in preparation.\n  * 100% non-intrusive: there's no need to load agents or libraries into the processes that are being profiled.\n  * No need for any reconfiguration, instrumentation or restarts of HLL interpreters and VMs: the agent supports unwinding each of the supported languages in the default configuration.\n  * ARM64 support for all unwinders except NodeJS.\n  * Support for native inline frames, which provide insights into compiler optimizations and offer a higher precision of function call chains.\n\n## Building\n\nNote\n\nIf you simply wish to take the agent for a spin with minimal effort, you can\nalso immediately jump to the \"Visualizing data locally\" section, launch\ndevfiler and follow the download links for agent binaries within its \"Add\ndata\" dialogue.\n\nThe agent can be built without affecting your environment by using the\nprovided make targets. You need to have docker installed, though. Builds on\namd64 and arm64 architectures are supported.\n\nThe first step is to build the Docker image that contains the build\nenvironment:\n\n    \n    \n    make docker-image\n\nThen, you can build the agent:\n\n    \n    \n    make agent\n\nThe resulting binary will be in the current directory as otel-profiling-agent.\n\nAlternatively, you can build without Docker. Please see the Dockerfile for\nrequired dependencies.\n\nAfter installing the dependencies, just run make to build.\n\n## Running\n\nYou can start the agent with the following command:\n\n    \n    \n    sudo ./otel-profiling-agent -collection-agent=127.0.0.1:11000 -disable-tls\n\nThe agent comes with a functional but work-in-progress / evolving\nimplementation of the recently released OTel profiling signal.\n\nThe agent loads the eBPF program and its maps, starts unwinding and reports\ncaptured traces to the backend.\n\n## Visualizing data locally\n\nWe created a desktop application called \"devfiler\" that allows visualizing the\nprofiling agent's output locally, making it very convenient for development\nuse. devfiler spins up a local server that listens on 0.0.0.0:11000.\n\nTo run it, simply download and unpack the archive from the following URL:\n\nhttps://upload.elastic.co/d/0891b6a006b1ee8224e638d2454b967f7c1ac596110b5c149ac7c98107655d9b\n\nAuthentication token: a217abfdd7c438e9\n\nThe archive contains a build for each of the following platforms:\n\n  * macOS (Intel)\n  * macOS (Apple Silicon)\n  * Linux AppImage (x86_64)\n  * Linux AppImage (aarch64)\n\nNote\n\ndevfiler is currently in an experimental preview stage.\n\n### macOS\n\nThis build of devfiler is currently not signed with a globally trusted Apple\ndeveloper ID, but with a developer certificate. If you simply double-click the\napplication, you'll run into an error. Instead of opening it with a double\nclick, simply do a right-click on devfiler.app, then choose \"Open\". If you go\nthis route, you'll instead be presented with the option to run it anyway.\n\n### Linux\n\nThe AppImages in the archive should run on any Linux distribution with a\nreasonably modern glibc and libgl installation. To run the application, simply\nextract the archive and then do:\n\n    \n    \n    ./devfiler-appimage-$(uname -m).AppImage\n\n## Agent internals\n\nThe host agent is a Go application that is deployed to all machines customers\nwish to profile. It collects, processes and pushes observed stack traces and\nrelated meta-information to a backend collector.\n\n### Concepts\n\n#### File IDs\n\nA file ID uniquely identifies an executable, kernel or script language source\nfile.\n\nFile IDs for native applications are created by taking the SHA256 checksum of\na file's head, tail, and size, then truncating the hash digest to 16 bytes\n(128 bits):\n\n    \n    \n    Input \u2190 Concat(File[:4096], File[-4096:], BigEndianUInt64(Len(File))) Digest \u2190 SHA256(Input) FileID \u2190 Digest[:16]\n\nFile IDs for script and JIT languages are created in an interpreter-specific\nfashion.\n\nFile IDs for Linux kernels are calculated by taking the FNV128 hash of their\nGNU build ID.\n\n#### Stack unwinding\n\nStack unwinding is the process of recovering the list of function calls that\nlead execution to the point in the program at which the profiler interrupted\nit.\n\nHow stacks are unwound varies depending on whether a thread is running native,\nJITed or interpreted code, but the basic idea is always the same: every\nlanguage that supports arbitrarily nested function calls needs a way to keep\ntrack of which function it needs to return to after the current function\ncompletes. Our unwinder uses that same information to repeatedly determine the\ncaller until we reach the thread's entry point.\n\nIn simplified pseudo-code:\n\n    \n    \n    pc \u2190 interrupted_process.cpu.pc sp \u2190 interrupted_process.cpu.sp while !is_entry_point(pc): file_id, start_addr, interp_type \u2190 file_id_at_pc(pc) push_frame(interp_type, file_id, pc - start_addr) unwinder \u2190 unwinder_for_interp(interp_type) pc, sp \u2190 unwinder.next_frame(pc, sp)\n\n#### Symbolization\n\nSymbolization is the process of assigning source line information to the raw\naddresses extracted during stack unwinding.\n\nFor script and JIT languages that always have symbol information available on\nthe customer machines, the host agent is responsible for symbolizing frames.\n\nFor native code the symbolization occurs in the backend. Stack frames are sent\nas file IDs and the offset within the file and the symbolization service is\nthen responsible for assigning the correct function name, source file and\nlines in the background. Symbols for open-source software installed from OS\npackage repos are pulled in from our global symbolization infrastructure and\nsymbols for private executables can be manually uploaded by the customer.\n\nThe primary reason for doing native symbolization in the backend is that\nnative executables in production will often be stripped. Asking the customer\nto deploy symbols to production would be both wasteful in terms of disk usage\nand also a major friction point in initial adoption.\n\n#### Stack trace representation\n\nWe have two major representations for our stack traces.\n\nThe raw trace format produced by our BPF unwinders:\n\nhttps://github.com/elastic/otel-profiling-\nagent/blob/0945fe6/host/host.go#L60-L66\n\nThe final format produced after additional processing in user-land:\n\nhttps://github.com/elastic/otel-profiling-\nagent/blob/0945fe6/libpf/libpf.go#L458-L463\n\nThe two might look rather similar at first glance, but there are some\nimportant differences:\n\n  * the BPF variant uses truncated 64-bit file IDs to save precious kernel memory\n  * for interpreter frames the BPF variant uses the file ID and line number fields to store more or less arbitrary interpreter-specific data that is needed by the user-mode code to conduct symbolization\n\nA third trace representation exists within our network protocol, but it\nessentially just a deduplicated, compressed representation of the user-land\ntrace format.\n\n#### Trace hashing\n\nIn profiling it is common to see the same trace many times. Traces can be up\nto 128 entries long, and repeatedly symbolizing and sending the same traces\nover the network would be very wasteful. We use trace hashing to avoid this.\nDifferent hashing schemes are used for the BPF and user-mode trace\nrepresentations. Multiple 64 bit hashes can end up being mapped to the same\n128 bit hash, but not vice-versa.\n\nBPF trace hash (64 bit):\n\n    \n    \n    H(kernel_stack_id, frames_user, PID)\n\nUser-land trace hash (128 bit)\n\n    \n    \n    H(frames_user_kernel)\n\n### User-land sub-components\n\n#### Tracer\n\nThe tracer is a central user-land component that loads and attaches our BPF\nprograms to their corresponding BPF probes during startup and then continues\nto serve as the primary event pump for BPF <-> user-land communication. It\nfurther instantiates and owns other important subcomponents like the process\nmanager.\n\n#### Trace handler\n\nThe trace handler is responsible for converting traces from the BPF format to\nthe user-space format. It receives raw traces tracer, converts them to the\nuser-space format and then sends them on to the reporter. The majority of the\nconversion logic happens via a call into the process manager's ConvertTrace\nfunction.\n\nSince converting and enriching BPF-format traces is not a cheap operation, the\ntrace handler is also responsible for keeping a cache (mapping) of trace\nhashes: from 64bit BPF hash to the user-space 128bit hash.\n\n#### Reporter\n\nThe reporter receives traces and trace counts in the user-mode format from the\ntrace handler, converts them to the gRPC representation and then sends them\nout to a backend collector.\n\nIt also receives additional meta-information (such as metrics and host\nmetadata) which it also converts and sends out to a backend collector over\ngRPC.\n\nThe reporter does not offer strong guarantees regarding reliability of network\noperations and may drop data at any point, an \"eventual consistency\" model.\n\n#### Process manager\n\nThe process manager receives process creation/termination events from tracer\nand is responsible for making available any information to the BPF code that\nit needs to conduct unwinding. It maintains a map of the executables mapped\ninto each process, loads stack unwinding deltas for native modules and creates\ninterpreter handlers for each memory mapping that belongs to a supported\nlanguage interpreter.\n\nDuring trace conversion the process manager is further responsible for routing\nsymbolization requests to the correct interpreter handlers.\n\n#### Interpreter handlers\n\nEach interpreted or JITed language that we support has a corresponding type\nthat implements the interpreter handler interface. It is responsible for:\n\n  * detecting the interpreter's version and structure layouts\n  * placing information that the corresponding BPF interpreter unwinder needs into BPF maps\n  * translating interpreter frames from the BPF format to the user-land format by symbolizing them\n\n#### Stack delta provider\n\nUnwinding the stack of native executables compiled without frame pointers\nrequires stack deltas. These deltas are essentially a mapping from each PC in\nan executable to instructions describing how to find the caller and how to\nadjust the unwinder machine state in preparation of locating the next frame.\nTypically these instructions consist of a register that is used as a base\naddress and an offset (delta) that needs to be added to it -- hence the name.\nThe stack delta provider is responsible for analyzing executables and creating\nstack deltas for them.\n\nFor most native executables, we rely on the information present in .eh_frame.\n.eh_frame was originally meant only for C++ exception unwinding, but it has\nsince been repurposed for stack unwinding in general. Even applications\nwritten in many other native languages like C, Zig or Rust will typically come\nwith .eh_frame.\n\nOne important exception to this general pattern is Go. As of writing, Go\nexecutables do not come with .eh_frame sections unless they are built with CGo\nenabled. Even with CGo the .eh_frame section will only contain information for\na small subset of functions that are either written in C/C++ or part of the\nCGo runtime. For Go executables we extract the stack delta information from\nthe Go-specific section called .gopclntab. In-depth documentation on the\nformat is available in a separate document).\n\n### BPF components\n\nThe BPF portion of the host agent implements the actual stack unwinding. It\nuses the eBPF virtual machine to execute our code directly in the Linux\nkernel. The components are implemented in BPF C and live in the otel-\nprofiling-agent/support/ebpf directory.\n\n#### Limitations\n\nBPF programs must adhere to various restrictions imposed by the verifier. Many\nof these limitations are significantly relaxed in newer kernel versions, but\nwe still have to stick to the old limits because we wish to continue\nsupporting older kernels.\n\nThe minimum supported Linux kernel versions are\n\n  * 4.19 for amd64/x86_64\n  * 5.5 for arm64/aarch64\n\nThe most notable limitations are the following two:\n\n  * 4096 instructions per program A single BPF program can consist of a maximum of 4096 instructions, otherwise older kernels will refuse to load it. Since BPF does not allow for loops, they instead need to be unrolled.\n  * 32 tail-calls Linux allows BPF programs to do a tail-call to another BPF program. A tail call is essentially a jmp into another BPF program, ending execution of the current handler and starting a new one. This allows us to circumvent the 4096 instruction limit a bit by doing a tail-call before we run into the limit. There's a maximum of 32 tail calls that a BPF program can do.\n\nThese limitations mean that we generally try to prepare as much work as\npossible in user-land and then only do the minimal work necessary within BPF.\nWe can only use algorithms at worst and try to stick with for most things. All\nprocessing that cannot be implemented like this must be delegated to user-\nland. As a general rule of thumb, anything that needs more than 32 iterations\nin a loop is out of the question for BPF.\n\n#### Unwinders\n\nUnwinding always begins in native_tracer_entry. This entry point for our\ntracer starts by reading the register state of the thread that we just\ninterrupted and initializes the PerCPURecord structure. The per-CPU record\npersists data between tail-calls of the same unwinder invocation. The\nunwinder's current PC, SP etc. values are initialized from register values.\n\nAfter the initial setup the entry point consults a BPF map that is maintained\nby the user-land portion of the agent to determine which interpreter unwinder\nis responsible for unwinding the code at PC. If a record for the memory region\nis found, we then tail-call to the corresponding interpreter unwinder.\n\nEach interpreter unwinder has their own BPF program. The interpreter unwinders\ntypically have an unrolled main loop where they try to unwind as many frames\nfor that interpreter as they can without going over the instruction limit.\nAfter each iteration the unwinders will typically check whether the current PC\nvalue still belongs to the current unwinder and tail-call to the right\nunwinder otherwise.\n\nWhen an unwinder detects that we've reached the last frame in the trace,\nunwinding is terminated with a tail call to unwind_stop. For most traces this\ncall will happen in the native unwinder, since even JITed languages usually\ncall through a few layers of native C/C++ code before entering the VM. We\ndetect the end of a trace by heuristically marking certain functions with\nPROG_UNWIND_STOP in the BPF maps prepared by user-land. unwind_stop then sends\nthe completed BPF trace to user-land.\n\nIf any frame in the trace requires symbolization in user-mode, we additionally\nsend a BPF event to request an expedited read from user-land. For all other\ntraces user-land will simply read and then clear this map on a timer.\n\n#### PID events\n\nThe BPF components are responsible for notifying user-land about new and\nexiting processes. An event about a new process is produced when we first\ninterrupt it with the unwinders. Events about exiting processes are created\nwith a sched_process_exit probe. In both cases the BPF code sends a perf event\nto notify user-land. We also re-report a PID if we detect execution in\npreviously unknown memory region to prompt re-scan of the mappings.\n\n### Network protocol\n\nAll collected information is reported to a backend collector via a push-based,\nstateless, one-way gRPC protocol.\n\nAll data to be transmitted is stored in bounded FIFO queues (ring buffers).\nOld data is overwritten when the queues fill up (e.g. due to a lagging or\noffline backend collector). There is no explicit reliability or redundancy\n(besides retries internal to gRPC) and the assumption is that data will be\nresent (eventually consistent).\n\n### Trace processing pipeline\n\nThe host agent contains an internal pipeline that incrementally processes the\nraw traces that are produced by the BPF unwinders, enriches them with\nadditional information (e.g. symbols for interpreter frames and container\ninfo), deduplicates known traces and combines trace counts that occurred in\nthe same update period.\n\nThe traces produced in BPF start out with the information shown in the\nfollowing diagram.\n\nOur backend collector expects to receive trace information in a normalized and\nenriched format. This diagram below is relatively close to the data-structures\nthat are actually sent over the network, minus the batching and domain-\nspecific deduplication that we apply prior to sending it out.\n\nThe diagram below provides a detailed overview on how the various components\nof the host agent interact to transform raw traces into the network format. It\nis focused around our data structures and how data flows through them. Dotted\nlines represent indirect interaction with data structures, solid ones\ncorrespond to code flow. \"UM\" is short for \"user mode\".\n\n### Testing strategy\n\nThe host agent code is tested with three test suites:\n\n  * Go unit tests Functionality of individual functions and types is tested with regular Go unit tests. This works great for the user-land portion of the agent, but is unable to test any of the unwinding logic and BPF interaction.\n  * coredump test suite The coredump test suite (utils/coredump) we compile the whole BPF unwinder code into a user-mode executable, then use the information from a coredump to simulate a realistic environment to test the unwinder code in. The coredump suite essentially implements all required BPF helper functions in user-space, reading memory and thread contexts from the coredump. The resulting traces are then compared to a frame list in a JSON file, serving as regression tests.\n  * BPF integration tests A special build of the host agent with the integration tag is created that enables specialized test cases that actually load BPF tracers into the kernel. These test cases require root privileges and thus cannot be part of the regular unit test suite. The test cases focus on covering the interaction and communication of BPF with user-mode code, as well as testing that our BPF code passes the BPF verifier. Our CI builds the integration test executable once and then executes it on a wide range of different Linux kernel versions via qemu.\n\n### Probabilistic profiling\n\nProbabilistic profiling allows you to reduce storage costs by collecting a\nrepresentative sample of profiling data. This method decreases storage costs\nwith a visibility trade-off, as not all Profiling Host Agents will have\nprofile collection enabled at all times.\n\nProfiling Events linearly correlate with the probabilistic profiling value.\nThe lower the value, the fewer events are collected.\n\n#### Configure probabilistic profiling\n\nTo configure probabilistic profiling, set the -probabilistic-threshold and\n-probabilistic-interval options.\n\nSet the -probabilistic-threshold option to a unsigned integer between 1 and 99\nto enable probabilistic profiling. At every probabilistic interval, a random\nnumber between 0 and 99 is chosen. If the probabilistic threshold that you've\nset is greater than this random number, the agent collects profiles from this\nsystem for the duration of the interval. The default value is 100.\n\nSet the -probabilistic-interval option to a time duration to define the time\ninterval for which probabilistic profiling is either enabled or disabled. The\ndefault value is 1 minute.\n\n#### Example\n\nThe following example shows how to configure the profiling agent with a\nthreshold of 50 and an interval of 2 minutes and 30 seconds:\n\n    \n    \n    sudo ./otel-profiling-agent -probabilistic-threshold=50 -probabilistic-interval=2m30s\n\n# Legal\n\n## Licensing Information\n\nThis project is licensed under the Apache License 2.0 (Apache-2.0). Apache\nLicense 2.0\n\nThe eBPF source code is licensed under the GPL 2.0 license. GPL 2.0\n\n## Licenses of dependencies\n\nTo display a summary of the dependencies' licenses:\n\n    \n    \n    make legal\n\nDetails can be found in the generated deps.profiling-agent.csv file.\n\nAt the time of writing this, the summary is\n\n    \n    \n    Count License 52 Apache-2.0 17 BSD-3-Clause 17 MIT 3 BSD-2-Clause 1 ISC\n\n## About\n\nThe production-scale datacenter profiler\n\n### Resources\n\nReadme\n\n### License\n\nApache-2.0 license\n\n### Security policy\n\nSecurity policy\n\nActivity\n\nCustom properties\n\n### Stars\n\n1.7k stars\n\n### Watchers\n\n17 watching\n\n### Forks\n\n200 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Contributors 17\n\n\\+ 3 contributors\n\n## Languages\n\n  * Go 83.4%\n  * C 14.7%\n  * Python 0.6%\n  * Java 0.3%\n  * Shell 0.3%\n  * Makefile 0.2%\n  * Other 0.5%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
