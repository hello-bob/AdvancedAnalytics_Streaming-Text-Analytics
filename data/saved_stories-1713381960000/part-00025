{"aid": "40063209", "title": "GPT-4-turbo-2024-04-09 \"wins\" simple evals benchmark", "url": "https://github.com/openai/simple-evals", "domain": "github.com/openai", "votes": 2, "user": "zurfer", "posted_at": "2024-04-17 11:44:47", "comments": 0, "source_title": "GitHub - openai/simple-evals", "source_text": "GitHub - openai/simple-evals\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nopenai / simple-evals Public\n\n  * Notifications\n  * Fork 80\n  * Star 744\n\n### License\n\nMIT license\n\n744 stars 80 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# openai/simple-evals\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nyuchenhe07Merge pull request #5 from eltociear/patch-2Apr 13, 2024267835b \u00b7\nApr 13, 2024Apr 13, 2024\n\n## History\n\n5 Commits  \n  \n### sampler\n\n|\n\n### sampler\n\n| init| Apr 11, 2024  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| init| Apr 11, 2024  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| init| Apr 11, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update README.md| Apr 12, 2024  \n  \n### common.py\n\n|\n\n### common.py\n\n| init| Apr 11, 2024  \n  \n### demo.py\n\n|\n\n### demo.py\n\n| init| Apr 11, 2024  \n  \n### drop_eval.py\n\n|\n\n### drop_eval.py\n\n| init| Apr 11, 2024  \n  \n### gpqa_eval.py\n\n|\n\n### gpqa_eval.py\n\n| init| Apr 11, 2024  \n  \n### humaneval_eval.py\n\n|\n\n### humaneval_eval.py\n\n| init| Apr 11, 2024  \n  \n### math_eval.py\n\n|\n\n### math_eval.py\n\n| fix path| Apr 12, 2024  \n  \n### mgsm_eval.py\n\n|\n\n### mgsm_eval.py\n\n| init| Apr 11, 2024  \n  \n### mmlu_eval.py\n\n|\n\n### mmlu_eval.py\n\n| init| Apr 11, 2024  \n  \n### types.py\n\n|\n\n### types.py\n\n| init| Apr 11, 2024  \n  \n## Repository files navigation\n\n# Overview\n\nThis repository contains a lightweight library for evaluating language models.\nWe are open sourcing it so we can be transparent about the accuracy numbers\nwe're publishing alongside our latest models (starting with\ngpt-4-turbo-2024-04-09).\n\nEvals are sensitive to prompting, and there's significant variation in the\nformulations used in recent publications and libraries. Some use few-shot\nprompts or role playing prompts (\"You are an expert software programmer...\").\nThese approaches are carryovers from evaluating base models (rather than\ninstruction/chat-tuned models) and from models that were worse at following\ninstructions.\n\nFor this library, we are emphasizing the zero-shot, chain-of-thought setting,\nwith simple instructions like \"Solve the following multiple choice problem\".\nWe believe that this prompting technique is a better reflection of the models'\nperformance in realistic usage.\n\nWe will not be actively maintaining this repository and monitoring PRs and\nIssues. In particular, we're not accepting new evals. Here are the changes we\nmight accept.\n\n  * Bug fixes (hopefully not needed!)\n  * Adding adapters for new models\n  * Adding new rows to the table below with eval results, given new models and new system prompts.\n\nThis repository is NOT intended as a replacement for\nhttps://github.com/openai/evals, which is designed to be a comprehensive\ncollection of a large number of evals.\n\n## Evals\n\nThis repository currently contains the following evals:\n\n  * MMLU: Measuring Massive Multitask Language Understanding, reference: https://arxiv.org/abs/2009.03300, https://github.com/hendrycks/test, MIT License\n  * MATH: Measuring Mathematical Problem Solving With the MATH Dataset, reference: https://arxiv.org/abs/2103.03874, https://github.com/hendrycks/math, MIT License\n  * GPQA: A Graduate-Level Google-Proof Q&A Benchmark, reference: https://arxiv.org/abs/2311.12022, https://github.com/idavidrein/gpqa/, MIT License\n  * DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs, reference: https://arxiv.org/abs/1903.00161, https://allenai.org/data/drop, Apache License 2.0\n  * MGSM: Multilingual Grade School Math Benchmark (MGSM), Language Models are Multilingual Chain-of-Thought Reasoners, reference: https://arxiv.org/abs/2210.03057, https://github.com/google-research/url-nlp, Creative Commons Attribution 4.0 International Public License (CC-BY)\n  * HumanEval: Evaluating Large Language Models Trained on Code, reference https://arxiv.org/abs/2107.03374, https://github.com/openai/human-eval, MIT License\n\n## Samplers\n\nWe have implemented sampling interfaces for the following language model APIs:\n\n  * OpenAI: https://platform.openai.com/docs/overview\n  * Claude: https://www.anthropic.com/api\n\nMake sure to set the *_API_KEY environment variables before using these APIs.\n\n## Setup\n\nDue to the optional dependencies, we're not providing a unified setup\nmechanism. Instead, we're providing instructions for each eval and sampler.\n\nFor HumanEval (python programming)\n\n    \n    \n    git clone https://github.com/openai/human-eval pip install -e human-eval\n\nFor the OpenAI API:\n\n    \n    \n    pip install openai\n\nFor the Anthropic API:\n\n    \n    \n    pip install anthropic\n\n## Demo\n\n    \n    \n    python -m simple-evals.demo\n\nThis will launch evaluations through the OpenAI API.\n\n## Benchmark Results\n\nModel| Prompt| DROP(f1)| GPQA%| MATH%| MGSM%| MMLU%| HumanEval%  \n---|---|---|---|---|---|---|---  \nGPT4s  \ngpt-4-turbo-2024-04-09| chatgpt^1| 85.4| 49.1| 72.2| 88.6| 86.5| 87.6  \ngpt-4-turbo-2024-04-09| assistant^2| 86.0| 49.3| 73.4| 89.6| 86.7| 88.2  \ngpt-4-1106(-vision)-preview| chatgpt| 81.3| 42.1| 64.1| 86.5| 84.6| 82.2  \ngpt-4-1106(-vision)-preview| assistant| 83.2| 42.5| 64.3| 87.1| 84.7| 83.7  \ngpt-4-0125-preview| chatgpt| 83.4| 39.7| 64.2| 83.7| 84.8| 88.2  \ngpt-4-0125-preview| assistant| 81.5| 41.4| 64.5| 85.1| 85.4| 86.6  \nREFERENCE  \nClaude-3-Opus (rerun w/ api)| empty^3| 79.0| 49.7| 63.2| 89.7| 84.1| 84.8  \nClaude-3-Opus (rerun w/ api)| lmsys^4| 77.1| 50.7| 63.8| 89.2| 84.2| 82.9  \nClaude-3-Opus (report^5)| unknown| 83.1| 50.4| 60.1| 90.7| 86.8| 84.9  \nGemini-Ultra-1.0 (report^6)| unknown| 82.4| n/a| 53.2| 79.0| 83.7| 74.4  \nGemini-Pro-1.5 (report^6)| unknown| 78.9| n/a| 58.5| 88.7| 81.9| 71.9  \n  \n## Legal Stuff\n\nBy contributing to evals, you are agreeing to make your evaluation logic and\ndata under the same MIT license as this repository. You must have adequate\nrights to upload any data used in an eval. OpenAI reserves the right to use\nthis data in future service improvements to our product. Contributions to\nOpenAI evals will be subject to our usual Usage Policies:\nhttps://platform.openai.com/docs/usage-policies.\n\n## Footnotes\n\n  1. chatgpt system message: \"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture.\\nKnowledge cutoff: 2023-12\\nCurrent date: 2024-04-01\" \u21a9\n\n  2. assistant system message in OpenAI API doc: \"You are a helpful assistant.\" . \u21a9\n\n  3. claude-3 empty system message: suggested by Anthropic API doc, and we have done limited experiments due to rate limit issues, but we welcome PRs with alternative choices. \u21a9\n\n  4. claude-3 lmsys system message: system message in LMSYS Fast-chat open source code: \"The assistant is Claude, created by Anthropic. The current date is {{currentDateTime}}. Claude's knowledge base was last updated ... \". We have done limited experiments due to rate limit issues, but we welcome PRs with alternative choices. \u21a9\n\n  5. claude-3 reports: https://www.anthropic.com/news/claude-3-family. \u21a9\n\n  6. gemini-1.5 reports: https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/, we dont have rerun results due to rate_limit issues and paid-as-you-go version are still \"coming soon\" by the time of this study on 04/02. \u21a9 \u21a9^2\n\n## About\n\nNo description, website, or topics provided.\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\nActivity\n\nCustom properties\n\n### Stars\n\n744 stars\n\n### Watchers\n\n14 watching\n\n### Forks\n\n80 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Contributors 2\n\n  * eltociear Ikko Eltociear Ashimine\n  * yuchenhe07\n\n## Languages\n\n  * Python 100.0%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
