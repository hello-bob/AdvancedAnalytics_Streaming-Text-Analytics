{"aid": "40092773", "title": "Some Volumes Were Slow and We Figured Out Why", "url": "https://community.fly.io/t/some-volumes-were-slow-and-we-figured-out-why/19394", "domain": "fly.io", "votes": 14, "user": "mxstbr", "posted_at": "2024-04-19 22:47:51", "comments": 0, "source_title": "Some Volumes Were Slow And We Figured Out Why - Fresh Produce - Fly.io", "source_text": "Some Volumes Were Slow And We Figured Out Why - Fresh Produce - Fly.io\n\nSkip to main content\n\nWelcome to the Fly.io community forum. You\u2019re probably here because you\u2019re\ntrying to figure something out. That\u2019s great. We also offer email support, for\nthe apps you really care about.\n\n#\n\nSome Volumes Were Slow And We Figured Out Why\n\nFresh Produce\n\nYou have selected 0 posts.\n\nselect all\n\ncancel selecting\n\njssjr\n\n3h\n\n## A Bug Report\n\nFly Volumes are fast. That sounds like a brag, but the truth is, we made\ntradeoffs to end up with fast Volumes. We back them with a pool of locally-\nattached NVMe drives, which means they\u2019re pinned to specific physical servers,\nand while we do back them up, you generally want to be doing something at an\nupper layer to replicate them. They can lose data! But, the flip side is:\nthey\u2019re very fast.\n\nSo it was jarring, earlier this week, to get reports from folks experiencing\nwhat appeared to be I/O performance problems. We make it easy to spot I/O\nissues: you can just click out from our dashboard to Metrics, and look at the\nI/O Utilization percentage, which should be low.\n\nOne tricky thing about doing infra ops for a public cloud is that every\npossible thing can go wrong. Our customers exercise our hardware in every\nconceivable way. A performance problem could be on our side, or it could be an\napp stuck in an expensive tight loop. We started digging, but didn\u2019t see any\npatterns.\n\nThen our metrics cluster started dragging. Well, we\u2019re confident in the\nperformance envelope of that system. We built it to scale. And we were seeing\nthe Fly Machines running it grinding to a halt. Our digging gained some\nurgency.\n\nThen a customer reported the same grinding halt. We worked around the problem\nwith them, by re-creating machines (hold that thought, it\u2019s relevant later).\nBut something was obviously up: as the saying goes, \u201cOnce is happenstance.\nTwice is coincidence. Three times is an incident.\u201d\n\n### Digging deeper\n\nThe way Fly.io 1 works is, an orchestrator service on our physical servers,\ncalled flyd, spawns and manages KVM virtual machines using Firecracker and\nCloud-Hypervisor. Both are lightweight and super fast, we we run lots and lots\nof them on any given physical server.\n\nWe know that customer Fly Machines overwhelmingly aren\u2019t having problems, but\nwe also know something is up. We want to catch a Fly Machine in the act. So we\nset up a tiny script to alert us when any Firecracker process gets stuck in\n\u201cuninterruptible sleep\u201d (D) for more than a few seconds. That\u2019s the sign that\nthe process is stalling on I/O. Stalling for multiple seconds, or even just\ncoming up in D state multiple times back to back, means something\u2019s off.\n\nStalking a few of these troublesome processes gives us candidates to inspect.\nLinux makes this easy: you just cat /proc/$pid/stack. Here\u2019s what we see:\n\n    \n    \n    [<0>] blk_io_schedule+0x22/0x40 [<0>] __blkdev_direct_IO_simple+0x222/0x320 [<0>] blkdev_direct_IO+0x71/0x80 [<0>] generic_file_read_iter+0x9c/0x150\n\nThis is a normal stack trace: a process waiting for the completion of a\nscheduled I/O operation. But every process we looked at had the exact same\ntrace. That\u2019s a smell: blk_io_schedule should be super fast, unless something\nis getting in the way. This is like catching a whole room full of people\nblinking at exactly the same time.\n\n### The obvious question\n\nWhat changed?\n\nOur hardware didn\u2019t change. And there\u2019s no correlation of these events in\nparticular regions. We\u2019re in the middle of a round of Firecracker version\nupdates (ironically, to improve I/O performance!) but the new version is\nfeature flagged. And we haven\u2019t changed how volumes work.\n\nExcept we did, and didn\u2019t realize it.\n\nEarlier this week, we shipped a small feature to allow first class support for\nswap devices 3. This has the effect of moving swap off the root filesystem\ndevice and onto a dedicated swap device.\n\nThe thing about swap is, if you\u2019re using it, you\u2019re probably using it pretty\nhard. Because of the way swap memory works, mostly invisibly to your apps, it\ncan get pretty thrashy, as the OS lies to your dev framework about how much\nmemory there is and covers its tracks with disk operations.\n\nSo: we limit the IO bandwidth swap devices. This is good. Swap is available,\nto keep your app from crashing in a transient spike, but it\u2019s limited, so it\ndoesn\u2019t inadvertently create noisy neighbor problems. All is in balance, all\nis right with the world.\n\nYou see where this is going.\n\n2068\u00d7410 355 KB\n\nIt\u2019s easy enough to check which devices we\u2019re rate limiting:\n\n    \n    \n    $ find /sys/fs/cgroup/ -name blkio.throttle.write_bps_device | xargs egrep '253' /sys/fs/cgroup/blkio/blkio.throttle.write_bps_device:253:179 16777216 /sys/fs/cgroup/blkio/blkio.throttle.write_bps_device:253:101 16777216 /sys/fs/cgroup/blkio/blkio.throttle.write_bps_device:253:233 16777216 /sys/fs/cgroup/blkio/blkio.throttle.write_bps_device:253:250 16777216 /sys/fs/cgroup/blkio/blkio.throttle.write_bps_device:253:86 16777216 /sys/fs/cgroup/blkio/blkio.throttle.write_bps_device:253:114 16777216 ...\n\nMost of those major,minor pairs were associated with swap devices. \u201cMost\u201d is\nnot the answer you want here. Now we know what\u2019s happening: some small number\nof Fly Machines are getting throttled by an incorrect swap rate limit.\n\nThe remaining question is \u201cwhy\u201d. And a quick audit of the flyd code that sets\nup the rate limits yields an answer:\n\n    \n    \n    writeDev.Major = int64(rdev / 256) writeDev.Minor = int64(rdev % 256)\n\nTo work with a device given its path on the filesystem, you call stat(2) on\nit. stat gives us the device major,minor numbers packed into a dev_t, which\nyou get to unpack. Easy enough: the major is the top 8 bits, the minor the\nbottom, of a 16 bit value...\n\n... if it\u2019s 2003.\n\nCheck linux/kdev_t.h:\n\n    \n    \n    #define MINORBITS 20\n\nOur orchestrator was potentially setting the blkio limits on the wrong device,\nlimiting it to 16MiB/s, and sending any IO-intensive applications straight\ninto their worst nightmare.\n\nThe probability of hitting this was not huge. Which explained why the elevated\niowait times seemed so sporadic. But once in a million events happen\nconstantly at Fly.io 1 scale.\n\nYou can see now why re-creating a Fly Machine worked around this issue: it\ngenerated a new swap device, and a new root filesystem device, and those were\nunlikely to lose the block device number lottery.\n\n### What we did\n\nClearing the bad rate limits was easy enough.\n\nSo was fixing the flyd code that handled st_rdev.\n\nMeanwhile, we\u2019re investigating better telemetry for container process states.\nIt\u2019s not certain that a machine stuck for an extended period of time in an\nuninterruptible state is something that warrants us taking action or if it\u2019s\njust a false positive event, but it might give us some signal to help detect\npotential issues.\n\nWe\u2019d also like to flat out make Firecracker\u2019s IO layer faster. I don\u2019t think\nthat would have helped with this bug, but knowing with greater certainty what\nthe expected IO performance for a machine should be may let us determine if\nthe observed performance is affected by changes to our platform. We\u2019ll write\nmore about this soon.\n\n### In short\n\nYou\u2019re hosting your app on Fly.io 1 and may have seen abnormally slow IO for\nthe past few days. A change we made was the reason that happened and we are\nbummed if your app was affected. Bare metal performance is a feature of our\nplatform and we\u2019re pretty passionate about it.\n\nAs always, if you\u2019re running into issues or see something funky, you should\nget in touch. Despite millions of metrics being spun through the heuristics\nthat drive our alerts, you have an app you care about and we want to know if\nyou\u2019re hitting any problems. Our users helped us identify this issue and we\u2019re\ngrateful for your engagement.\n\n  * #### created\n\n3h\n\n  * #### last reply\n\n2h\n\n  * 1\n\n#### reply\n\n  * 285\n\n#### views\n\n  * 2\n\n#### users\n\n  * 10\n\n#### likes\n\novaistariq\n\n2h\n\nIt\u2019s great to see the transparency. I can imagine how tricky this would have\nbeen to debug.\n\n### Related Topics\n\nTopic| Replies| Views| Activity  \n---|---|---|---  \nAutomatic volume snapshots are incredible!| 217| Nov 2022  \nWould love to see more data about the platformQuestions / Help| 260| Mar 2023  \nFly delpoy painfully slowBuild debuggingregistry| 187| Jan 10  \nFly stability| 585| Dec 2022  \nWireGuard Peers Got Faster Again, Again| 774| Sep 2022\n\n", "frontpage": true}
