{"aid": "40180376", "title": "Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking", "url": "https://marqo.ai/blog/generalized-contrastive-learning-for-multi-modal-retrieval-and-ranking", "domain": "marqo.ai", "votes": 1, "user": "jdp15", "posted_at": "2024-04-27 14:49:34", "comments": 0, "source_title": "Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking", "source_text": "Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking\n\nGetting Started\n\n# Generalized Contrastive Learning for Multi-Modal Retrieval and Ranking\n\nApril 23, 2024\n\n5\n\nmin read\n\nTL;DR We generalize the popular training method of CLIP to accommodate any\nnumber of text and images when representing documents and also encode\nrelevance (or rank) to provide better first stage retrieval. Known as\nGeneralized Contrastive Learning (GCL), our results show that GCL achieves a\n94.5% increase in NDCG@10 and 504% for ERR@10 for in-domain and 26.3 - 48.8%\nand 44.3 - 108.0% increases in NDCG@10 and ERR@10 respectively for cold-start\nevaluations, measured relative to the CLIP baseline. When compared to a\nkeyword search only baseline of BM25, there is an improvement of 300 - 750%\nfor in-domain and cold start respectively across NDCG@10 and ERR@10. Finally\nwe contribute a multi-modal benchmark dataset of 10M rows, across 100k queries\nand ~5M products each with ranking data for training and evaluation. Read more\nin the pre-print, GitHub repository, or below.\n\n### 1\\. Introduction to vector search\n\nVector search works by representing data as learned vectors - embeddings.\nNearest neighbor search is then used to return the closest (i.e most relevant)\nresults. The efficacy of the method relies on producing high quality\nembeddings. High quality embeddings are ones that have the desired similarity\naccurately encoded in the underlying vector space - things that are close\ntogether in the vector space are \u201crelevant\u201d in the downstream application.\nHowever, real-world use cases are more complex than single document, single\nquery schemes with embedding models trained on binary relevance relationships.\n\nAn animation demonstrating a simplified 2-D example of vector similarity. The\npoints represent vectors in 2D. A query vector is represented by the orange\ndot. Finding relevant results is done by calculating the distance of the query\nvector with its neighbors and sorting by the closest distance.\n\n### 2\\. Limitations of current embedding models for vector search\n\nAlthough vector search is very powerful and enables searching across just\nabout any data, the current methods have some limitations. The prevailing\nmethods for training embedding models are largely disconnected from the end\nuse-case (like search), the vector database, and the requirements of users.\nThis means that a lot of the potential of vector search is being unmet. Some\nof the current challenges are described below.\n\n### 2.1 Restricted to using a single piece of information to represent a\ndocument\n\nCurrent models encode and represent one piece of information with one vector.\nThe reality is that often there are multiple pieces of pertinent information\nfor a document that may span multiple modalities. For example, in product\nsearch there may be a title, description, reviews, and multiple images, each\nwith its own caption. Furthermore, different documents will have different\naffinities for different queries and can be quantified using various\nengagement, behavioral or judgement metrics. Existing training methods require\nbinary relationships of the same magnitude only and cannot use this important\ninformation. GCL generalises embedding model training to use as many pieces of\ninformation as is desired.\n\nStandard model training assumes relationships are 1-1. However, in the real-\nworld there are often multiple pieces of useful information that can be used.\nGCL extends CLIP by allowing the models to be directly optimized to use these\nadditional data.\n\n### 2.2 No notion of rank when dealing with degenerate queries\n\nFor many retrieval applications where a query is asking a question or the\nquery will only have a single correct result, training models on binary\nrelationships works well. The model only needs to learn how to match the query\nwith the document. However, when there are degenerate queries - multiple\nresults that satisfy some criteria of relevance - the ordering of the results\nis only ever learned indirectly from the many binary relationships. In\nreality, the ordering of results matters, even for first stage retrieval. GCL\nallows for the magnitude of query-document specific relevance to be encoded in\nthe embeddings and improves ranking of candidate documents.\n\nAn example of searching for an answer to a question compared to searching when\nthere are multiple relevant results.\n\n### 2.3 Poor text understanding when using CLIP like methods\n\nFor multi-modal models like CLIP, these are trained to only work from image to\ntext (and vice versa). The text-text understanding is not as good as text only\nmodels due to the text-text relationships being learned indirectly through\nimages. For many applications, having both inter- and intra-modality\nunderstanding is required. GCL allows for any combination of inter- and intra-\nmodal understanding by directly optimizing for this.\n\nIntra-modality understanding in methods like CLIP are only ever learned\nindirectly through other modalities. With GCL, the intra-modal understanding\nis optimized for directly, improving performance on these tasks.\n\n### 2.4 Lack of representative datasets to develop methods for vector search\n\nIn developing GCL, it became apparent there was a disconnect with publicly\navailable datasets for embedding model training and evaluation for real-world\nuse cases. Existing benchmarks are typically text only or inter-modal only and\nfocus on the 1-1 query-result paradigm. Additionally, existing datasets have\nlimited notions of relevance, with the majority encoding it as a binary\nrelationship while several use (up-to) a handful of discrete categorizations\noften on the test set only. This differs from a typical real-world use cases\nwhere relevance can be both hard binary relationships or come from continuous\nvariables. To help with this we compiled a dataset of 10M (ranked) product-\nquery pairs, across ~100k queries, nearly 5M products, and four evaluation\nsplits (available here).\n\nAn example from the Marqo-GS-10M dataset. Each query has 100 results with\nranking information across 100 discrete values.\n\n### 2.5 Unoptimized methods for unified embeddings\n\nHaving the ability to use all or subsets of data to represent documents is\ncritical for users. Some documents might be represented by a single piece of\ninformation or multiple pieces spanning multiple modalities. GCL extends\ncontrastive methods to include the ability to directly optimize for single\nunified embeddings from single or multiple modalities. GCL also allows for\noptimzing for the exact same structure as is stored in the vector database.\nFor example, optimizing the embeddings to perform well when used individually\nor as fused representations with variable constituent embeddings (e.g. to\nconserve memory).\n\nGCL allows for the additional optimization of unified embeddings. This can\ncome from a pooling mechanism or another learned model.\n\n### 3\\. Generalised contrastive learning\n\nAs highlighted in the previous section, there are several short-comings of\nexisting embedding model training methods with respect to vector search. To\novercome these limitations, Generalized Contrastive Learning (GCL) was\ndeveloped alongside a 10M row dataset for development and evaluation.\n\n### 3.1 Method\n\nGCL works by extending existing CLIP training to define equality relationships\nbetween pieces of information. For CLIP, there are text captions T and images\nV and it works by learning embeddings by satisfying a relationship like T = V.\nGCL extends this by generalising it to a left-hand side (LHS, L) and right-\nhand side (RHS, R), i.e L = R. Instead of restricting the LHS or RHS to a\nsingle section of text or an image, GCL allows these to be any number of text\nor images with a magnitude to define the strength of the relationship. This is\nhighlighted in the example below where we are defining an equality that says\non one side we have a query (\u201dlunar new year outfit for pet\u201d) and it is being\nequated to a document that is made up of a title (\"Pet Scarf, Red Chinese New\nYear Themed Pet Clothes Accessory\"),an image, and this relationship is defined\nby a weight w1.\n\nSchematic illustrating how GCL can be used for multiple pieces of information\nwhile leveraging non-binary relevance signals.\n\nThe weights are converted from ground-truth relevance score by a score-to-\nweight function. GCL makes it possible to learn ranking based on historical\ndata. For example, we can attribute a higher weight to a query-document pair\nif many people have downloaded the document after searching with this query.\nMoreover, GCL generalizes traditional single-field learning by training with\nmultiple fields, merging elements such as title and product image into a fused\nembedding. This approach of utilizing simple mean embedding is consistent with\npractices in popular vector databases (e.g. to reduce memory). This is easily\nextended to more complex learned approaches, for example using another learned\nhead to unify the embeddings.\n\n### 3.2 Marqo-GS-10M\n\nIn addition to the GCL framework, we have created and released a dataset to\ndevelop and benchmark approaches on. The Marqo-GS-10M consists of ~10M query-\nproduct pairs, each containing a query, title, image and rank. The dataset is\nfurther split into training and multiple evaluation splits. For a vector\nsearch system, there are varying definitions of what would constitute \u201cin-\ndomain\u201d and \u201cout-of-domain\u201d. For example, some use cases might have completely\nknown and fixed set of queries and documents while others will be dominated by\ncompletely unseen queries and documents. Most systems will be some combination\nof these and the composition of this will likely change over time. Marqo-\nGS-10M defines four distinct evaluation splits so model performance is much\nbetter understood. The splits are as follows:\n\n  1. Training split with 80% of queries and 50% of documents.\n  2. Novel query split with the other 20% of queries and the same documents as the training split.\n  3. Novel corpus split with the same queries as the training split and unseen documents with the equal size of the training corpus.\n  4. Zero-shot split with both unseen queries and documents.\n\nA diagram illustrating the composition of the different test splits (left) and\ndifferent rank-to-weight functions that can be used during training.\n\n### 3.3 Results\n\nThe table below shows the results for the different splits outlined above.\nBoth text only and multimodal models were rank-tuned on Marqo-GS-10M starting\nfrom pre-trained base models. The metrics shown are normalised discounted\ncumulative gain (nDCG), rank-biased precision (RBP) and expected reciprocal\nrank (ERR). See the Appendix for metric descriptions.\n\nAll metrics are between 0 and 1, with higher being better.\n\nText only. The BM25 baseline is shown and compared to the pre-trained\nbaselines (), text only contrastive learning (Cross E.) and GCL.*Image only:\nThe pre-trained baselines (), CLIP training and GCL.*Multi-modal: Trained\nmodels for CLIP training and GCL.\n\n### 3.4 Extensions to Matryoshka and binary embeddings\n\nIn addition to what has been described above, GCL is also compatible with\nrecent popular methods like binary and Matryoshka embeddings. Testing (un-\nreleased) has shown that GCL still outperforms CLIP based training on Marqo-\nGS-10M when using binary or Matryoshka methods during training. Below are the\nresults for Matryoshka GCL compared to GCL. Reducing the dimension by a factor\nof 2 results in no performance degradation while reducing it by 4x has minimal\ndegradation (>95%).\n\nZero-shot evaluation results for unseen queries and documents for GCL and\nMatryoshka GCL for three different embedding dimensions.\n\nBelow are results comparing different training and evaluation regimes for\nusing binary embeddings with GCL. The comparison is for GCL training using\nfloat embeddings for both training and evaluation. This is compared to models\nthat were trained to target both float and binary embeddings while evaluated\nusing either float or binary embeddings. This shows that the performance\nmatches or exceeds the pure float embedding training while preserving most\n(>90%) of the relevance when using binary embeddings in a single stage of\nretrieval.\n\nZero-shot evaluation results for unseen queries and documents for GCL and\nbinary embeddings trained using GCL.\n\n### 4\\. Conclusion\n\nGCL extends the benefits of CLIP for multi-modal contrastive learning but adds\nin flexibility to deal with many aspects of real world data like continuous\nrelevance and varying data sources. To evaluate the methods we also developed\na 10M row dataset with multi-modal data along with ranking information.\nFinally, GCL is compatible with other embedding training methods like\nMatryoshka and binary embeddings. Read the paper here and the dataset here.\n\n### 5\\. Appendix\n\nMetric descriptions.\n\nnDCG: evaluates by considering both the relevance and the rank of the\ndocuments returned. It first computes the Discounted Cumulative Gain (DCG),\nwhich sums the graded relevance scores of documents adjusted for their\nposition in the result list.\n\nRBP: measures by simulating a user\u2019s likelihood of continuing to view\nsuccessive search results. It uses a persistence parameter p to weigh the\nrelevance of each document, with earlier results receiving more weight. This\nmetric reflects the probability that a user will keep looking through the\nlist, focusing on the relevance of documents and the typical depth of a search\nsession.\n\nERR: a metric that combines the concept of reciprocal rank with multiple\nrelevance levels. It calculates the expected position of finding a\nsatisfactory search result, adjusted by a user's likelihood of stopping after\nencountering relevant documents. ERR effectively measures how well results\nsatisfies user queries with highly ranked and relevant results.\n\nJesse Clark\n\nJesse is a co-founder and the CTO at Marqo, he leads the applied sciences\ndivision performing R&D in AI for search and recommendations.\n\nMore Posts\n\nGetting Started\n\nWhy I Joined Marqo\n\nApril 19, 2024\n\nTips\n\nBenchmarking Models for Multi-modal Search\n\nApril 10, 2024\n\nShowcase\n\nMarqo V2: Performance at Scale, Predictability, and Control\n\nMarch 1, 2024\n\nShowcase\n\nBuilding the Next Generation of Vector Search\n\nFebruary 13, 2024\n\nTips\n\nUI Concepts for Vector Search\n\nOctober 20, 2023\n\nShowcase\n\nMarqo Cloud is Generally Available\n\nAugust 16, 2023\n\nShowcase\n\n\u201cContext Is All You Need\u201d - Multimodal Vector Search with Personalization\n\nJune 8, 2023\n\nShowcase\n\nRefining Image Quality and Eliminating NSFW Content with Marqo\n\nMay 24, 2023\n\nShowcase\n\nSpeech processing for context aware Q&A with Marqo and ChatGPT\n\nApril 12, 2023\n\nShowcase\n\nCombining stable diffusion with semantic search: generating and categorising\n100k hot dogs\n\nFebruary 7, 2023\n\nShowcase\n\nUsing Marqo and GPT3 for topical news summarisaiton\n\nFebruary 7, 2023\n\nShowcase\n\nFrom \u201ciron manual\u201d to \u201cIron Man\u201d \u2014 Augmenting GPT for fast editable memory to\nenable context aware question & answering\n\nFebruary 6, 2023\n\nShowcase\n\nImage search with localization and open-vocabulary reranking using Marqo,\nyolox, CLIP and OWL-ViT\n\nJanuary 24, 2023\n\nTips\n\nHow to dramatically improve search speed in Marqo\n\nAugust 29, 2022\n\nGetting Started\n\nMarqo: An Introduction\n\nAugust 29, 2022\n\nShowcase\n\nHow I used Marqo to create a multilingual legal database in 5 key lines of\ncode\n\nAugust 29, 2022\n\nGetting Started\n\nWhat is tensor search?\n\nAugust 29, 2022\n\nTips\n\nHow to implement text-to-image search on Marqo \u2014 in 5 lines of code\n\nAugust 29, 2022\n\n## Subscribe to our mailing list.\n\nThank you! Your submission has been received!\n\nOops! Something went wrong while submitting the form.\n\nMarqo is more than a vector database, it's an end-to-end vector search engine.\n\nFor media inquiries, please email press@marqo.ai\n\nRESOURCES\n\nPricingDocumentationBlogAPI ReferenceStatus Page\n\nCONNECT\n\nSupportGitHubSlackCommunity\n\nCOMPANY\n\nContact usCareersTerms and ConditionsSLA and Support PolicyPrivacy Policy\n\n\u00a9 Copyright Marqo.ai 2024. All Rights Reserved.\n\n", "frontpage": false}
