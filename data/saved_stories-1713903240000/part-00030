{"aid": "40131599", "title": "Compare models while accounting for hyperparameter tuning effort", "url": "https://nicholaslourie.github.io/opda/tutorial/examples.html", "domain": "nicholaslourie.github.io", "votes": 1, "user": "nicholaslourie", "posted_at": "2024-04-23 13:14:38", "comments": 0, "source_title": "opda", "source_text": "Examples - opda v0.6.1\n\nopda v0.6.1\n\nopda v0.6.1\n\nDocumentation\n\n  * Setup\n  * Usage\n  * Examples\n  * Citation\n  * API Reference\n\n    * opda.approximation module\n    * opda.exceptions module\n    * opda.nonparametric module\n    * opda.parametric module\n    * opda.random module\n    * opda.utils module\n  * Changelog\n\nLinks\n\n  * Source\n  * Issues\n\nContributing\n\n  * Development\n  * Release\n\nBack to top\n\n# Examples#\n\nThe following examples show how to solve common tasks using opda.\n\nTo dive deeper, checkout Usage or the API Reference.\n\n## Compare Models#\n\nLet\u2019s compare two models while accounting for hyperparameter tuning effort. We\ncould compare a new model against a baseline or perhaps against an ablation of\nsome component. Either way, the comparison works pretty much the same.\n\n### Models with Similar Costs#\n\nIf the models have similar costs (e.g., training compute), then compare their\nperformance as a function of hyperparameter search iterations:\n\n    \n    \n    >>> from matplotlib import pyplot as plt >>> import numpy as np >>> >>> from opda.nonparametric import EmpiricalDistribution >>> >>> generator = np.random.default_rng(0) # Set the random seed. >>> uniform = generator.uniform >>> >>> # Simulate results from random search. >>> ys0 = uniform(0.70, 0.90, size=48) # scores from model 0 >>> ys1 = uniform(0.75, 0.95, size=48) # scores from model 1 >>> >>> # Plot tuning curves and confidence bands for both models in one figure. >>> fig, ax = plt.subplots() >>> ns = np.linspace(1, 10, num=1_000) >>> for name, ys in [(\"baseline\", ys0), (\"model\", ys1)]: ... # Construct the confidence bands. ... dist_lo, dist_pt, dist_hi = EmpiricalDistribution.confidence_bands( ... ys=ys, # accuracy results from random search ... confidence=0.80, # confidence level ... a=0., # (optional) lower bound on accuracy ... b=1., # (optional) upper bound on accuracy ... ) ... # Plot the tuning curve. ... ax.plot(ns, dist_pt.quantile_tuning_curve(ns), label=name) ... # Plot the confidence bands. ... ax.fill_between( ... ns, ... dist_hi.quantile_tuning_curve(ns), ... dist_lo.quantile_tuning_curve(ns), ... alpha=0.275, ... label=\"80% confidence\", ... ) ... # Format the plot. ... ax.set_xlabel(\"search iterations\") ... ax.set_ylabel(\"accuracy\") ... ax.set_title(\"Model Comparison\") ... ax.legend(loc=\"lower right\") [<matplotlib... >>> # plt.show() or fig.savefig(...)\n\nBreaking down this example, first we construct confidence bands for each\nmodel\u2019s CDF using confidence_bands():\n\n    \n    \n    >>> for name, ys in [(\"baseline\", ys0), (\"model\", ys1)]: ... # Construct the confidence bands. ... dist_lo, dist_pt, dist_hi = EmpiricalDistribution.confidence_bands( ... ys=ys, # accuracy results from random search ... confidence=0.80, # confidence level ... a=0., # (optional) lower bound on accuracy ... b=1., # (optional) upper bound on accuracy ... )\n\nThen, we compute the tuning curves via the quantile_tuning_curve() method. The\nlower CDF band gives the upper tuning curve band, and the upper CDF band gives\nthe lower tuning curve band. In this way, you can plot the tuning curve with\nconfidence bands:\n\n    \n    \n    ... # Plot the tuning curve. ... ax.plot(ns, dist_pt.quantile_tuning_curve(ns), label=name) ... # Plot the confidence bands. ... ax.fill_between( ... ns, ... dist_hi.quantile_tuning_curve(ns), ... dist_lo.quantile_tuning_curve(ns), ... alpha=0.275, ... label=\"80% confidence\", ... )\n\nThe rest just makes the plot look pretty, then shows it or saves it to disk.\n\n### Models with Different Costs#\n\nWhen models have different costs, it\u2019s more difficult to make a comparison.\nUse your judgment and tailor the analysis to the situation.\n\nOne general approach is: first rescale the models so they have similar\ninference cost, then adjust the tuning curves to match the training cost. To\nadjust the tuning curves, just multiply the search iterations by their average\ncost (e.g., in FLOPs, GPU hours, dollars, and so on).\n\nLet\u2019s revisit the previous example. This time, assume the models have similar\nsizes; however, while the baseline trains for 1 epoch, the new model trains\nfor 1 to 5:\n\n    \n    \n    >>> # Compute the average cost per training run. >>> avg_epochs0 = 1 # train model 0 for 1 epoch >>> avg_epochs1 = np.mean([1, 2, 3, 4, 5]) # train model 1 for 1-5 epochs >>> >>> # Plot tuning curves and confidence bands for both models in one figure. >>> fig, ax = plt.subplots() >>> ns = np.linspace(1, 30, num=1_000) >>> for name, avg_epochs, ys in [ ... (\"baseline\", avg_epochs0, ys0), ... ( \"model\", avg_epochs1, ys1), ... ]: ... # Construct the confidence bands. ... dist_lo, dist_pt, dist_hi = EmpiricalDistribution.confidence_bands( ... ys=ys, # accuracy results from random search ... confidence=0.80, # confidence level ... a=0., # (optional) lower bound on accuracy ... b=1., # (optional) upper bound on accuracy ... ) ... # Plot the tuning curve. ... ax.plot( ... avg_epochs * ns, ... dist_pt.quantile_tuning_curve(ns), ... label=name, ... ) ... # Plot the confidence bands. ... ax.fill_between( ... avg_epochs * ns, ... dist_hi.quantile_tuning_curve(ns), ... dist_lo.quantile_tuning_curve(ns), ... alpha=0.275, ... label=\"80% confidence\", ... ) ... # Format the plot. ... ax.set_xlim(1, 30) ... ax.set_xlabel(\"total training epochs\") ... ax.set_ylabel(\"accuracy\") ... ax.set_title(\"Model Comparison\") ... ax.legend(loc=\"lower right\") [<matplotlib... >>> # plt.show() or fig.savefig(...)\n\nThe main difference is that we multiply the number of search iterations by\ntheir average cost. First, we compute the average cost per training run:\n\n    \n    \n    >>> # Compute the average cost per training run. >>> avg_epochs0 = 1 # train model 0 for 1 epoch >>> avg_epochs1 = np.mean([1, 2, 3, 4, 5]) # train model 1 for 1-5 epochs\n\nWhile the baseline trains for 1 epoch, the new model trains for 1 to 5 at\nrandom. In general, pick the cost measure and way to compute the average\nthat\u2019s most appropriate for your problem. Here, we use total training epochs\nand the formula for a mean. If we compared two optimizers instead, we might\nuse FLOPs and either calculate the average theoretically[1] or estimate it\nempirically based on the results from our random search\u2014but, be careful! Since\nwe won\u2019t account for uncertainty in the average cost, you must use a high\nquality estimate.\n\nWhen you plot the tuning curve, multiply the search iterations by the average\ncost per training run:\n\n    \n    \n    ... # Plot the tuning curve. ... ax.plot( ... avg_epochs * ns, ... dist_pt.quantile_tuning_curve(ns), ... label=name, ... ) ... # Plot the confidence bands. ... ax.fill_between( ... avg_epochs * ns, ... dist_hi.quantile_tuning_curve(ns), ... dist_lo.quantile_tuning_curve(ns), ... alpha=0.275, ... label=\"80% confidence\", ... )\n\nNote that we only multiply the x values by the average cost. The\nquantile_tuning_curve() method still expects the number of search iterations\nas input.\n\nAnd that\u2019s it! We now have a fair comparison between models based on our\ntuning budget.\n\nTo learn more, checkout EmpiricalDistribution in the reference documentation\nor get interactive help in a Python REPL by running\nhelp(EmpiricalDistribution).\n\n## Analyze a Hyperparameter#\n\nLet\u2019s determine whether a specific hyperparameter is important to tune and\nthen dig into how it affects performance. We might, for example, do this after\ncomparing a model against a baseline in order to understand the new model or\nprovide advice on tuning its hyperparameters.\n\n### Hyperparameter Importance#\n\nImagine we\u2019re pretraining a language model. We\u2019re interested in the weight\ndecay. First, let\u2019s ask: how important is this hyperparameter? Weerts et al.\n(2020) give a practical and intuitive definition of hyperparameter importance\nin terms of tuning risk: the difference in test performance between tuning the\nhyperparameter and leaving it at the default value. We\u2019ll operationalize this\nidea by comparing the tuning curve from when we do tune the hyperparameter to\nthe one where we don\u2019t:\n\n    \n    \n    >>> from matplotlib import pyplot as plt >>> import numpy as np >>> >>> from opda.nonparametric import EmpiricalDistribution >>> >>> generator = np.random.default_rng(0) # Set the random seed. >>> normal, uniform = generator.normal, generator.uniform >>> >>> # Design the experiment. >>> n = 48 # Decide the number of search iterations. >>> search_space = { # Define the search space. ... \"learning_rate\": {\"bounds\": [1e-5, 1e-1], \"default\": 1e-3}, ... \"weight_decay\" : {\"bounds\": [1e-6, 1e-2], \"default\": 1e-4}, ... } >>> >>> # Run random search on the hyperparameters. >>> def pretrain(learning_rate, weight_decay): ... xentropy = 1. \\ ... + (np.log10(learning_rate) - -2)**2 / 2 \\ ... + (np.log10( weight_decay) - -6)**2 / 7 ... return xentropy + normal(0, 0.1, size=xentropy.size) >>> >>> ys_default = pretrain( # Set weight decay to default. ... learning_rate=np.exp(uniform(np.log(1e-5), np.log(1e-1), size=n)), ... weight_decay=1e-4, ... ) >>> ys_tuned = pretrain( # Tune weight decay. ... learning_rate=np.exp(uniform(np.log(1e-5), np.log(1e-1), size=n)), ... weight_decay =np.exp(uniform(np.log(1e-6), np.log(1e-2), size=n)), ... ) >>> >>> # Plot tuning curves and confidence bands for both conditions. >>> fig, ax = plt.subplots() >>> ns = np.linspace(1, 10, num=1_000) >>> conditions = [ ... ( \"default (1e-4)\", ys_default), ... (\"tuned (1e-6 to 1e-2)\", ys_tuned), ... ] >>> for name, ys in conditions: ... # Construct the confidence bands. ... dist_lo, dist_pt, dist_hi = EmpiricalDistribution.confidence_bands( ... ys=ys, # cross-entropy results from random search ... confidence=0.80, # confidence level ... a=0., # (optional) lower bound on cross-entropy ... b=np.inf, # (optional) upper bound on cross-entropy ... ) ... # Plot the tuning curve. ... ax.plot( ... ns, ... dist_pt.quantile_tuning_curve(ns, minimize=True), ... label=name, ... ) ... # Plot the confidence bands. ... ax.fill_between( ... ns, ... dist_hi.quantile_tuning_curve(ns, minimize=True), ... dist_lo.quantile_tuning_curve(ns, minimize=True), ... alpha=0.275, ... label=\"80% confidence\", ... ) ... # Format the plot. ... ax.set_xlabel(\"search iterations\") ... ax.set_ylabel(\"cross-entropy\") ... ax.set_title(\"Weight Decay Importance\") ... ax.legend(loc=\"upper right\") [<matplotlib... >>> # plt.show() or fig.savefig(...)\n\nBreaking down this example, first we run a (hypothetical) random search fixing\nthe hyperparameter to its default:\n\n    \n    \n    >>> ys_default = pretrain( # Set weight decay to default. ... learning_rate=np.exp(uniform(np.log(1e-5), np.log(1e-1), size=n)), ... weight_decay=1e-4, ... )\n\nNext, we run a (hypothetical) random search tuning the hyperparameter:\n\n    \n    \n    >>> ys_tuned = pretrain( # Tune weight decay. ... learning_rate=np.exp(uniform(np.log(1e-5), np.log(1e-1), size=n)), ... weight_decay =np.exp(uniform(np.log(1e-6), np.log(1e-2), size=n)), ... )\n\nNote that we run random search on a log scale by sampling uniformly in log-\nspace and then exponentiating back. An appropriate scale should be chosen for\neach hyperparameter.\n\nAfter collecting results from random search, we construct the confidence bands\nfor the CDF of the score distribution using confidence_bands():\n\n    \n    \n    >>> for name, ys in conditions: ... # Construct the confidence bands. ... dist_lo, dist_pt, dist_hi = EmpiricalDistribution.confidence_bands( ... ys=ys, # cross-entropy results from random search ... confidence=0.80, # confidence level ... a=0., # (optional) lower bound on cross-entropy ... b=np.inf, # (optional) upper bound on cross-entropy ... )\n\nAnd then we plot the tuning curves. The lower CDF band gives the upper tuning\ncurve band, and the upper CDF band gives the lower tuning curve band:\n\n    \n    \n    ... # Plot the tuning curve. ... ax.plot( ... ns, ... dist_pt.quantile_tuning_curve(ns, minimize=True), ... label=name, ... ) ... # Plot the confidence bands. ... ax.fill_between( ... ns, ... dist_hi.quantile_tuning_curve(ns, minimize=True), ... dist_lo.quantile_tuning_curve(ns, minimize=True), ... alpha=0.275, ... label=\"80% confidence\", ... )\n\nBecause we\u2019re minimizing cross-entropy (rather than maximizing it), we must\npass minimize=True to the quantile_tuning_curve() method.\n\nAfter plotting the tuning curves, just format the plot and then save or show\nit.\n\n### Hyperparameter Effect#\n\nOnce we\u2019ve assessed hyperparameter importance, we might ask: how exactly does\nthis hyperparameter affect performance? Continuing the previous example, we\u2019ll\ngenerate tuning curves for different values of the hyperparameter:\n\n    \n    \n    >>> # Run random search with various values for weight decay. >>> weight_decays = [1e-6, 1e-4, 1e-2] >>> conditions = [] >>> for weight_decay in weight_decays: ... name = f\"weight decay: {weight_decay:.0e}\" ... ys = pretrain( ... learning_rate=np.exp(uniform(np.log(1e-5), np.log(1e-1), size=n)), ... weight_decay=weight_decay, ... ) ... conditions.append((name, ys)) >>> >>> # Plot tuning curves and confidence bands for all conditions. >>> fig, ax = plt.subplots() >>> ns = np.linspace(1, 10, num=1_000) >>> for name, ys in conditions: ... # Construct the confidence bands. ... dist_lo, dist_pt, dist_hi = EmpiricalDistribution.confidence_bands( ... ys=ys, # cross-entropy results from random search ... confidence=0.80, # confidence level ... a=0., # (optional) lower bound on cross-entropy ... b=np.inf, # (optional) upper bound on cross-entropy ... ) ... # Plot the tuning curve. ... ax.plot( ... ns, ... dist_pt.quantile_tuning_curve(ns, minimize=True), ... label=name, ... ) ... # Plot the confidence bands. ... ax.fill_between( ... ns, ... dist_hi.quantile_tuning_curve(ns, minimize=True), ... dist_lo.quantile_tuning_curve(ns, minimize=True), ... alpha=0.275, ... label=\"80% confidence\", ... ) ... # Format the plot. ... ax.set_xlabel(\"search iterations\") ... ax.set_ylabel(\"cross-entropy\") ... ax.set_title(\"Effect of Weight Decay\") ... ax.legend(loc=\"upper right\") [<matplotlib... >>> # plt.show() or fig.savefig(...)\n\nBreaking down this example, first we choose values at which to probe the\nhyperparameter:\n\n    \n    \n    >>> weight_decays = [1e-6, 1e-4, 1e-2]\n\nThen, we fix the hyperparameter to each value (1e-6, 1e-4, 1e-2) in a separate\nrandom search:\n\n    \n    \n    >>> conditions = [] >>> for weight_decay in weight_decays: ... name = f\"weight decay: {weight_decay:.0e}\" ... ys = pretrain( ... learning_rate=np.exp(uniform(np.log(1e-5), np.log(1e-1), size=n)), ... weight_decay=weight_decay, ... ) ... conditions.append((name, ys))\n\nAfter collecting the results, we construct confidence bands:\n\n    \n    \n    >>> for name, ys in conditions: ... # Construct the confidence bands. ... dist_lo, dist_pt, dist_hi = EmpiricalDistribution.confidence_bands( ... ys=ys, # cross-entropy results from random search ... confidence=0.80, # confidence level ... a=0., # (optional) lower bound on cross-entropy ... b=np.inf, # (optional) upper bound on cross-entropy ... )\n\nAnd then plot the tuning curves:\n\n    \n    \n    ... # Plot the tuning curve. ... ax.plot( ... ns, ... dist_pt.quantile_tuning_curve(ns, minimize=True), ... label=name, ... ) ... # Plot the confidence bands. ... ax.fill_between( ... ns, ... dist_hi.quantile_tuning_curve(ns, minimize=True), ... dist_lo.quantile_tuning_curve(ns, minimize=True), ... alpha=0.275, ... label=\"80% confidence\", ... )\n\nLast, we just format the plots then save or show them.\n\nFor more information, checkout EmpiricalDistribution in the reference\ndocumentation or get interactive help in a Python REPL by running\nhelp(EmpiricalDistribution).\n\nFootnotes\n\n[1]\n\nFor a great example calculating these kinds of quantities for a transformer,\nsee Section 2 of Scaling Laws for Neural Language Models (Kaplan et al.,\n2020).\n\nNext\n\nCitation\n\nPrevious\n\nUsage\n\nCopyright \u00a9 2023, Nicholas Lourie\n\nMade with Sphinx and @pradyunsg's Furo\n\nOn this page\n\n  * Examples\n\n    * Compare Models\n\n      * Models with Similar Costs\n      * Models with Different Costs\n    * Analyze a Hyperparameter\n\n      * Hyperparameter Importance\n      * Hyperparameter Effect\n\n", "frontpage": false}
