{"aid": "40236456", "title": "A transformer walk-through, with Gemma", "url": "https://graphcore-research.github.io/posts/gemma/", "domain": "graphcore-research.github.io", "votes": 1, "user": "sebg", "posted_at": "2024-05-02 14:03:30", "comments": 0, "source_title": "A transformer walk-through, with Gemma", "source_text": "A transformer walk-through, with Gemma - Graphcore Research Blog\n\n### Douglas Orr\n\nResearch Team Lead at Graphcore\n\n  * GitHub\n  * LinkedIn\n  * Personal Blog\n  * Twitter\n  * Email\n\n# A transformer walk-through, with Gemma\n\n36 minute read\n\nTransformer-based LLMs seem mysterious, but they don\u2019t need to. In this post,\nwe\u2019ll walk through a modern transformer LLM, Google\u2019s Gemma, providing bare-\nbones PyTorch code and some intuition for why each step is there. If you\u2019re a\nprogrammer and casual ML enthusiast, this is written for you.\n\nOur problem is single-step prediction: take a string e.g. \u201cI want to move\u201d,\nand use an already-trained language model (LM) to predict what could come\nnext. This is the core component of chatbots, coding assistants, etc, since we\ncan easily chain these predictions together to generate long strings.\n\nWe\u2019ll walk through this example using Gemma 2B, with an accompanying notebook\n(github, Colab) which provides unadorned code for everything we\u2019ll see.\n\nThere are two ways to read this post: the main text describes the\nimplementation, or what is required to run Gemma. The expandable sections add\ndetails about why the code does this, with some of the machine learning\nintuition behind the code.\n\n## TokenizationPermalink\n\nTokenize the input string, splitting & mapping it to token IDs.\n\n    \n    \n    tokenizer = transformers.AutoTokenizer.from_pretrained(\"google/gemma-2b\") input_ids = tokenizer(\"I want to move\").input_ids # input_ids = [2, 235285, 1938, 577, 3124]\n\nThe model works on sequences of subword tokens, represented as integers in the\nrange [0, 256000), such as 1938 (=> \u201c\u2581want\u201d). Since the input comes in as a\nsingle long string, we use a tokenizer to first split the string into tokens,\nsecond to map these tokens to numeric IDs.\n\n## Embedding lookupPermalink\n\nConvert token IDs to 2048-element vectors by looking them up in an embedding\ntable.\n\n    \n    \n    hiddens = p.embedding[input_ids] # p.embedding.shape = (256000, 2048) # input_ids.shape = (5,) # hiddens.shape = (5, 2048)\n\nFor each input token ID (e.g. 1938 => \u201c\u2581want\u201d), select the corresponding row\nfrom the large embedding table. The result, which we\u2019ll call hiddens, has\nshape (5, 2048), where there are 5 tokens along the sequence dimension and\n2048 components along the hidden dimension.\n\n## Post-embedding rescalingPermalink\n\nScale up hiddens by sqrt(2048).\n\n    \n    \n    hiddens *= hiddens.shape[-1] ** 0.5\n\nIn Gemma, embedding parameters are shared between the input embedding lookup\nand the output embedding projection. This is a reasonable design, but some\ncare is required to give appropriate scale to inputs and outputs. The scaling\nfactor of sqrt(hidden_size) makes inputs larger without interfering with the\noutput projection.\n\n## Transformer layerPermalink\n\nFor each of Gemma\u2019s 18 transformer layers, add attention(norm(hiddens)) then\nmlp(norm(hiddens)) back into hiddens.\n\n    \n    \n    for p_attn, p_mlp in p.layers: hiddens += attention(p_attn, rms_norm(p_attn.norm, hiddens)) hiddens += mlp(p_mlp, rms_norm(p_mlp.norm, hiddens))\n\nHere, Gemma uses the concept of residual layers, which build deep (many-\nlayered) functions using the iteration x = x + layer(norm(x)) rather than the\nsimpler MLP iteration x = layer(x).\n\n## RMSNormPermalink\n\nDivide out the root-mean-squared \u201cRMS\u201d from each token to reset it to 1. Then\nmultiply each element by a learned scaling parameter.\n\n    \n    \n    def rms_norm(x: Tensor) -> Tensor: # x.shape = (5, 2048) z = x.to(torch.float32, copy=True) z /= torch.sqrt((z ** 2).mean(-1, keepdim=True) + p.eps) z *= (1 + p.weight.float()) # p.weight.shape = (2048,) return z.to(x.dtype) # return.shape = (5, 2048) z = rms_norm(hiddens)\n\nIt\u2019s worth considering shapes and data types carefully. If the input shape\nx.shape is (5, 2048), then the normaliser torch.sqrt(...) has shape (5, 1).\nThis is because we\u2019re doing a mean over the last axis, processing each token\nin the sequence independently. The division z /= torch.sqrt(...) performs\nbroadcasting, dividing each element of a 2048-vector by the same value\n(different for each token).\n\nSince some data types have limited range (e.g. 65e3 in float16 versus 3e48 in\nfloat32), it may not be safe to do x ** 2 directly. If abs(x) > 256, this will\noverflow and cause an error. Therefore, we cast to float32 at the beginning,\nand back to the original x.dtype at the end of the op. There is also a small\nepsilon value added to the denominator as sqrt(... + p.eps), which helps\nprotect against division by a very small number.\n\n## AttentionPermalink\n\nThe first type of residual layer in Gemma is attention. It combines\nrepresentations across the sequence dimension.\n\n    \n    \n    def self_attn(q: Tensor, k: Tensor, v: Tensor) -> Tensor: # t=target, s=source, n=kv-heads, m=q-heads-per-kv, d=head-dim a = torch.einsum(\"tnmd, snd -> nmts\", q, k) / sqrt(q.shape[-1]) a += torch.full(a.shape[-2:], -torch.inf, dtype=a.dtype).tril_(-1).T a = a.softmax(dim=-1) return torch.einsum(\"nmts, snd -> tnmd\", a, v) def attention(x: Tensor) -> Tensor: # x.shape = (5, 2048) q = (x @ p.q_proj.T).unflatten(-1, (1, 8, 256)) k = (x @ p.k_proj.T).unflatten(-1, (1, 256)) v = (x @ p.v_proj.T).unflatten(-1, (1, 256)) # q.shape = (5, 1, 8, 256) # k.shape = (5, 1, 256) # v.shape = (5, 1, 256) o = self_attn(q, k, v) # o.shape = (5, 1, 8, 256) return o.flatten(1) @ p.o_proj.T # return.shape = (5, 2048)\n\nThe first step in attention is a projection (dot product with a parameter\nmatrix) from the 2048-dimension input to 8 separate 256-dimension query heads,\none 256-dimension key and one 256-dimension value. This projection is done\nindependently but in parallel for each token using the dot product operator @.\nNote that PyTorch typically keeps projections in transposed layout, so we need\nto transpose q_proj etc via q_proj.T before the dot product.\n\nThe 8 query heads are processed independently but in parallel by self_attn.\n\n  1. (Einsum does a dot product but allows dimensions to be reordered.) Einsum tnmd, snd -> nmts to take the first argument of shape (target, kv-heads, q-heads-per-kv, head-dim) and second of shape (source, kv-heads, head-dim) and run dot product over head-dim (256) to get a result of shape (kv-heads, q-heads-per-kv, target, source). We can think of this as a (target, source) attention matrix per query head.\n  2. Scale the attention matrix by /sqrt(256).\n  3. Create a causal mask using torch.full(...), setting all source positions that are to the right of (i.e. after) target in the (target, source) matrix to -infinity. This prevents future tokens from influencing past ones.\n  4. Use softmax(x) to exponentiate z = exp(x) then normalise z /= sum(z) to get attention weights in the range [0, 1]. This is run independently for each target, normalising to sum to 1 over all sources. The causal-masked positions are now exp(-infinity) == 0.\n  5. Run a final einsum, nmts, snd -> tnmd to take the first argument of shape (kv-heads, q-heads-per-kv, target, source) and second of shape (source, kv-heads, head-dim) and run dot product over source (of size sequence length, i.e. 5), to get a result of shape (target, kv-heads, q-heads-per-kv, head-dim). This is a weighted sum over the sequence, where the weights come from the (target, source) attention matrix.\n\nFinally, the outputs of the heads are mixed by flattening them from (5, 1, 8,\n256) -> (5, 2048) then following a final output projection, o_proj.\n\n## Rotary positional encoding (RoPE)Permalink\n\nWhen we described attention, we omitted one line from the definition, so we\ncould talk about it now. After projecting to get q and k, Gemma replaces them\nusing rotary positional encoding, which allows attention to vary depending on\nrelative position.\n\n    \n    \n    def rotate(z: Tensor, cos: Tensor, sin: Tensor) -> Tensor: zx, zy = z.unflatten(-1, (2, -1)).movedim(-2, 0) return torch.cat([zx * cos - zy * sin, zy * cos + zx * sin], -1) def embed_rotate(q: Tensor, k: Tensor, theta: float) -> Tensor: # q.shape = (5, 1, 8, 256) # k.shape = (5, 1, 256) d = q.shape[-1] freq = theta ** -(torch.arange(0, d, 2, dtype=torch.float) / d) angle = torch.arange(q.shape[0])[:, None] * freq cos = angle.cos().to(q.dtype) sin = angle.sin().to(q.dtype) return ( rotate(q, cos[:, None, None, :], sin[:, None, None, :]), rotate(k, cos[:, None, :], sin[:, None, :]), ) # In attention() q, k = embed_rotate(q, k, theta=p.rope_theta)\n\nRotary positional encoding transforms q and k, keeping them the same shape. It\ndoes the same thing for every attention head, so we can think of it as mapping\nfrom (5, 256) -> (5, 256). It transforms these in pairs, treating the\n256-vector as 128 \u00d7 2-vectors.\n\n  1. Generate an array of 128 different angular frequencies from 1 to 1/theta = 1/10000.\n  2. For each frequency and position in the sequence, calculate the phase angle freq * position. This is the angle (in radians) that we will rotate the corresponding 2-vector by.\n  3. Compute cos and sin of that angle for the rotation.\n  4. Rotate each head within q and k independently. Use the first 128 components of each head as x-coordinates, and the second 128 components as y-coordinates to give the 2-vectors to be rotated. Then use trigonometry x' = x cos a - y sin a, y' = y cos a + x sin a to calculate the rotation.\n  5. Concatenate x' and y' back into their original position in q or k.\n\nInserting this transformation between the calculation of q,k and the self_attn\nfunction that uses them to compute attention is sufficient to give Gemma the\nability to deliberately attend to tokens based on position, not just content.\n\n## Multi-layer perceptron (MLP, GeGLU)Permalink\n\nThe second type of residual layer in Gemma is the multi-layer perceptron\n(MLP), specifically the Gaussian error gated linear unit (GeGLU). This is a\nbit simpler than attention:\n\n    \n    \n    def mlp(x: Tensor) -> Tensor: # x.shape = (5, 2048) gate = x @ p.gate_proj.T up = x @ p.up_proj.T z = nn.functional.gelu(gate) * up # {gate, up, z}.shape = (5, 16384) return z @ p.down_proj.T # return.shape = (5, 2048)\n\nThe MLP takes input representations for each token as a 2048-vector and then\napplies the same transformation to each of the tokens independently.\n\nFirst, make two separate projections (dot products with trained parameter\nmatrices) to get two 16384-vectors. One of these vectors, called the gate is\npassed through a nonlinear function called gelu that applies to each element.\nWe\u2019ll treat the function as a black box, but to a first approximation, gelu(x)\nis quite close to max(0, x). The gate and other vector are multiplied element\nby element, then another trained down-projection produces the result, a\n2048-vector.\n\n## Final normPermalink\n\nBefore we predict the output tokens, we run RMSNorm one final time on the\ntoken representations:\n\n    \n    \n    hiddens = rms_norm(p.final_norm, hiddens)\n\n## Output projectionPermalink\n\nFinally, project the representation 2048-vector up to a 256000-vector,\nindependently but in parallel for each token in the sequence, producing a\nscore for every possible next-token prediction.\n\n    \n    \n    logits = hiddens @ p.embedding.T # hiddens.shape = (5, 2048) # logits.shape = (5, 256000)\n\nThis operation shares the same parameters embedding used in the embedding\nlookup step earlier. This is just as well, as it consumes a chunky 0.5B\nparameters of the 2B parameter model.\n\n## That\u2019s it!Permalink\n\nCongratulations, you\u2019ve done it \u2014 you\u2019ve made it to the end of Gemma. I hope\nit has been a fun ride (I have certainly enjoyed it!) There\u2019s not much of a\nconclusion, but if forced, I\u2019d offer:\n\n  1. Much of the transformer\u2019s operation comes down to the humble dot product.\n  2. As algorithms go, the transformer isn\u2019t all that complex.\n\nThat said, this was just a taster; here are a few out-of-scope topics I think\nare very interesting, in no particular order:\n\n  * Training/fine-tuning.\n  * Efficient implementation (especially parallel and distributed).\n  * Low-precision computation (training and inference).\n  * Integration into a full inference system, with batching, masking & search.\n  * Evaluation methods.\n  * Sparse computation.\n  * Alternative architectures (e.g. RNN, S4).\n  * Application to other domains.\n  * Practical applications & alignment.\n\nThere\u2019s plenty to learn, then. I hope that we can make some progress together!\n\nThis article was originally published by Graphcore researcher Douglas Orr on\nhis personal blog, Doug\u2019s Diversions in April 2024. It is reproduced here with\nhis permission.\n\nWritten by: Douglas Orr\n\nCategories: posts\n\nUpdated: April 24, 2024\n\n#### Share on\n\nTwitter Facebook LinkedIn\n\n#### Comments\n\n## You May Also Enjoy\n\n## February Papers: Longer RoPEs & Better Quantisation\n\n17 minute read\n\nImproving LLM inference is a key research topic at the moment, and something\nwe\u2019re particularly interested in at Graphcore because of its hardware\nimplicatio...\n\n## March Papers: Low-Rank Galore & 1.58-Bit Weights\n\n17 minute read\n\nMarch was a fruitful month for AI research, with plenty of papers for us to\nchoose from. A trend in the work we\u2019ve selected is the pushing of previously\npubl...\n\n## April Papers: TriForce, QuaRot & Mixture-of-Depths\n\n1 minute read\n\nFor our April selection of AI research papers, there is a clear common thread:\nefficient LLM inference. But as it happens, ML researchers are showing there\na...\n\n  * Twitter\n  * GitHub\n  * Subscribe\n  * Our Papers\n  * Feed\n\n\u00a9 2024 Graphcore Research Blog. Powered by Jekyll & Minimal Mistakes.\n\nThis website stores a cookie on your computer to enable google analytics by\ndefault. If you decline this information will not be stored. All data is\nanonymized and will not be shared. Accept Deny\n\n", "frontpage": false}
