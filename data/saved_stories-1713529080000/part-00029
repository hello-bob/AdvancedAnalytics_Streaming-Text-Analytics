{"aid": "40083598", "title": "What Does a Confidence Interval Mean?", "url": "https://www.allendowney.com/blog/2024/04/17/what-does-a-confidence-interval-mean/", "domain": "allendowney.com", "votes": 2, "user": "RafelMri", "posted_at": "2024-04-19 05:07:58", "comments": 0, "source_title": "What does a confidence interval mean?", "source_text": "What does a confidence interval mean? \u2013 Probably Overthinking It\n\n# Probably Overthinking It\n\n## Data science, Bayesian Statistics, and other ideas\n\nWhat does a confidence interval mean?\n\n# What does a confidence interval mean?\n\n##### April 17, 2024 AllenDowney\n\nHere\u2019s another installment in Data Q&A: Answering the real questions with\nPython. In general, I will try to focus on practical problems, but this one is\na little more philosophical.\n\nconfidence\n\n# What does a confidence interval mean?\u00b6\n\nHere\u2019s a question from the Reddit statistics forum (with an edit for clarity):\n\n> Why does a confidence interval not tell you that 90% of the time, [the true\n> value of the population parameter] will be in the interval, or something\n> along those lines?\n>\n> I understand that the interpretation of confidence intervals is that with\n> repeated samples from the population, 90% of the time the interval would\n> contain the true value of whatever it is you\u2019re estimating. What I don\u2019t\n> understand is why this method doesn\u2019t really tell you anything about what\n> that parameter value is.\n\nThis is, to put it mildly, a common source of confusion. And here is one of\nthe responses:\n\n> From a frequentist perspective, the true value of the parameter is fixed.\n> Thus, once you have calculated your confidence interval, one if two things\n> are true: either the true parameter value is inside the interval, or it is\n> outside it. So the probability that the interval contains the true value is\n> either 0 or 1, but you can never know which.\n\nThis response is the conventional answer to this question \u2014 it is what you\nfind in most textbooks and what is taught in most classes. And, in my opinion,\nit is wrong. To explain why, I\u2019ll start with a story.\n\nSuppose Frank and Betsy visit a factory where 90% of the widgets are good and\n10% are defective. Frank chooses a part at random and asks Betsy, \u201cWhat is the\nprobability that this part is good?\u201d\n\nBetsy says, \u201cIf 90% of the parts are good, and you choose one at random, the\nprobability is 90% that it is good.\u201d\n\n\u201cWrong!\u201d says Frank. \u201cSince the part has already been manufactured, one of two\nthings must be true: either it is good or it is defective. So the probability\nis either 100% or 0%, but we don\u2019t know which.\u201d\n\nFrank\u2019s argument is based on a strict interpretation of frequentism, which is\na particular philosophy of probability. But it is not the only interpretation,\nand it is not a particularly good one. In fact, it suffers from several flaws.\nThis example shows one of them \u2014 in many real-world scenarios where it would\nbe meaningful and useful to assign a probability to a proposition, frequentism\nsimply refuses to do so.\n\nFortunately, Betsy is under no obligation to adopt Frank\u2019s interpretation of\nprobability. She is free to adopt any of several alternatives that are\nconsistent with her commonsense claim that a randomly-chosen part has a 90%\nprobability of being functional.\n\nNow let\u2019s see how this story relates to confidence intervals.\n\nClick here to run this notebook on Colab\n\nI\u2019ll start by importing the usual libraries.\n\nIn [1]:\n\n    \n    \n    import numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns\n\n## Generating a confidence interval\u00b6\n\nSuppose that Frank is a statistics teacher and Betsy is one of his students.\nOne day Frank teaches the class a process for computing confidence intervals\nthat goes like this:\n\n  1. Collect a sample of size n.\n\n  2. Compute the sample mean, m, and the sample standard deviation, s.\n\n  3. If those estimates are correct, the sampling distribution of the mean is a normal distribution with mean m and standard deviation s/\u221an.\n\n  4. Compute the 5th and 95th percentiles of this sampling distribution. The result is a 90% confidence interval.\n\nAs an example, Frank generates a sample with size 100 from a normal\ndistribution with known parameters mean \u03bc=10 and standard deviation \u03c3=3.\n\nIn [2]:\n\n    \n    \n    from scipy.stats import norm mu = 10 sigma = 3 np.random.seed(17) data = norm.rvs(mu, sigma, size=100)\n\nThen Betsy uses the following function to compute a 90% CI.\n\nIn [3]:\n\n    \n    \n    def compute_ci(data): n = len(data) m = np.mean(data) s = np.std(data) sampling_dist = norm(m, s / np.sqrt(n)) ci90 = sampling_dist.ppf([0.05, 0.95]) return ci90\n\nIn [4]:\n\n    \n    \n    ci90 = compute_ci(data) ci90\n\nOut[4]:\n\n    \n    \n    array([ 9.78147291, 10.88758585])\n\nIn this example, we know that the actual population mean is 10 so we can see\nthat this CI contains the population mean. But if we draw another sample, we\nmight get a sample mean that is substantially higher or lower than \u03bc, and the\nCI we compute might not contain \u03bc.\n\nTo see how often that happens, we\u2019ll use this function, which generates a\nsample, computes a 90% CI, and checks whether the CI contains \u03bc.\n\nIn [5]:\n\n    \n    \n    def run_experiment(mu, sigma): data = norm.rvs(mu, sigma, size=100) low, high = compute_ci(data) return low < mu < high\n\nIf we run this function 1000 times, we can count how often the CI contains \u03bc.\n\nIn [6]:\n\n    \n    \n    np.mean([run_experiment(mu, sigma) for i in range(1000)]) * 100\n\nOut[6]:\n\n    \n    \n    90.60000000000001\n\nThe answer is close to 90% \u2014 that is, if we run this process many times, 90%\nof the CIs it generates contain \u03bc and 10% don\u2019t. So the CI-computing process\nis like a factory where 90% of the widgets are good and 10% are defective.\n\nNow suppose Frank chooses a different value of \u03bc and does not tell Betsy what\nit is. To simulate that scenario, I\u2019ll choose a value from a random number\ngenerator with a specific seed.\n\nIn [7]:\n\n    \n    \n    np.random.seed(17) unknown_mu = np.random.uniform(10, 20)\n\nAnd just for good measure, I\u2019ll generate a random value for \u03c3, too.\n\nIn [8]:\n\n    \n    \n    unknown_sigma = np.random.uniform(2, 3)\n\nNext Frank generates a sample from a normal distribution with those\nparameters, and gives the sample to Betsy.\n\nIn [9]:\n\n    \n    \n    data2 = norm.rvs(unknown_mu, unknown_sigma, size=100)\n\nAnd Betsy uses the data to compute a CI.\n\nIn [10]:\n\n    \n    \n    compute_ci(data2)\n\nOut[10]:\n\n    \n    \n    array([12.81278165, 13.73152148])\n\nNow suppose Frank asks, \u201cWhat is the probability that this CI contains the\nactual value of \u03bc that I chose?\u201d\n\nBetsy says, \u201cWe have established that 90% of the CIs generated by this process\ncontain \u03bc, so the probability that this CI contains \u03bc is 90%.\u201d\n\nAnd of course Frank says \u201cWrong! Now that we have computed the CI, it is\nunknown whether it contains the true parameter, but it is not random. The\nprobability that it contains \u03bc is either 100% or 0%. We can\u2019t say it has a 90%\nchance of containing \u03bc.\u201d\n\nOnce again, Frank is asserting a particular interpretation of probability \u2014\none that has the regrettable property of rendering probability nearly useless.\nFortunately, Betsy is under no obligation to join Frank\u2019s cult.\n\nUnder most reasonable interpretations of probability, you can say that a\nspecific 90% CI has a 90% chance of containing the true parameter. There is no\nreal philosophical problem with that.\n\nBut there might be practical problems.\n\n## Practical problems\u00b6\n\nThe process we use to construct a CI takes into account variability due to\nrandom sampling, but it does not take into account other problems, like\nmeasurement error and non-representative sampling. To see why that matters,\nlet\u2019s consider a more realistic example.\n\nSuppose we want to estimate the average height of adult male residents of the\nUnited States. If we define terms like \u201cheight\u201d, \u201cadult\u201d, \u201cmale\u201d, and\n\u201cresident of the United States\u201d precisely enough, we have defined a population\nthat has a true, unknown average height. If we collect a representative sample\nfrom the population and measure their heights, we can use the sample mean to\nestimate the population mean and compute a confidence interval.\n\nTo demonstrate, I\u2019ll use data from the Behavioral Risk Factor Surveillance\nSystem (BRFSS). Here\u2019s an extract I prepared for Elements of Data Science,\nbased on BRFSS data from 2021.\n\nIn [11]:\n\n    \n    \n    from os.path import basename, exists def download(url): filename = basename(url) if not exists(filename): from urllib.request import urlretrieve local, _ = urlretrieve(url, filename) print(\"Downloaded \" + str(local)) return filename download('https://github.com/AllenDowney/ElementsOfDataScience/raw/v1/data/brfss_2021.hdf')\n\nOut[11]:\n\n    \n    \n    'brfss_2021.hdf'\n\nIn [12]:\n\n    \n    \n    brfss = pd.read_hdf('brfss_2021.hdf', 'brfss')\n\nIt includes data from 203,760 male respondents.\n\nIn [13]:\n\n    \n    \n    male = brfss.query('_SEX == 1') len(male)\n\nOut[13]:\n\n    \n    \n    203760\n\nFor 193,701 of them, we have their self-reported height recorded in\ncentimeters.\n\nIn [14]:\n\n    \n    \n    male['HTM4'].count()\n\nOut[14]:\n\n    \n    \n    193701\n\nWe can use this data to compute a sample mean and 90% confidence interval.\n\nIn [15]:\n\n    \n    \n    m = male['HTM4'].mean() ci90 = compute_ci(male['HTM4']) m, ci90\n\nOut[15]:\n\n    \n    \n    (178.14807357731763, array([178.11896943, 178.17717773]))\n\nBecause the sample size is so large, the confidence interval is quite small \u2014\nits width is only 0.03% of the estimate.\n\nIn [16]:\n\n    \n    \n    np.diff(ci90) / m * 100\n\nOut[16]:\n\n    \n    \n    array([0.03267411])\n\nSo there is very little variability in this estimate due to random sampling.\nThat means the estimate is precise, but that doesn\u2019t mean it\u2019s accurate.\n\nFor one thing, the measurements in this dataset are self-reported. If people\ntend to round up \u2014 and they do \u2014 that would make the estimated mean too high.\n\nFor another thing, it is difficult to construct a representative sample of a\npopulation as large as the United States. The BRFSS is run by people who know\nwhat they are doing, but nothing is perfect \u2014 it is likely that some groups\nare systematically overrepresented or underrepresented. And that could make\nthe estimated mean too high or too low.\n\nGiven that there is almost certainly some measurement error and some sampling\nbias, it is unlikely that the actual population falls in the very small\nconfidence interval we computed.\n\nAnd that\u2019s true in general \u2014 when the sample size is large, variability due to\nrandom sampling is small, which means that other sources of error are likely\nto be bigger. So as sample size increases, the probability decreases that the\nCI contains the true value.\n\n## Summary\u00b6\n\nThe way confidence intervals are taught in most statistics class is based on\nthe frequentist interpretation of probability. But you are not obligated to\nadopt that interpretation, and there are good reasons you should not.\n\nSome people will say that confidence intervals are a frequentist method that\nis inextricable from the frequentist interpretation. I don\u2019t think that\u2019s true\n\u2014 there is nothing about the computation of a confidence interval that depends\non the frequentist interpretation. So you are free to interpret the CI under\nany philosophy of probability you like.\n\nIf you want to say that a 90% CI has a 90% chance of containing the true\nvalue, there is nothing wrong with that, philosophically. I think it is a\nmeaningful and useful probabilistic claim.\n\nHowever, it is only true if other sources of error \u2014 like sampling bias and\nmeasurement error \u2014 are small compared to variability due to random sampling.\n\nFor that reason, I think the best interpretation of a confidence interval, for\npractical purposes, is that it quantifies the precision of the estimate but\nsays nothing about its accuracy.\n\nCredit: I borrowed Frank and Betsy from my friend Ted Bunn. They first\nappeared in his blog post Who knows what evil lurks in the hearts of men? The\nBayesian doesn\u2019t care..\n\n### Share this:\n\n  * Click to share on Twitter (Opens in new window)\n  * Click to share on Facebook (Opens in new window)\n\n### Related\n\nData Q&AApril 9, 2024Similar post\n\nStandard deviation of a countApril 13, 2024Similar post\n\nWhat does skew mean?March 16, 2023Similar post\n\nUncategorized\n\nComments are closed.\n\nProbably Overthinking It is available now. You can get a 30% discount if you\norder from the publisher and use the code UCPNEW. You can also order from\nAmazon or, if you want to support independent bookstores, from Bookshop.org.\n\n#### About me\n\nAllen Downey is a curriculum designer at Brilliant, professor emeritus at Olin\nCollege, and author of Think Python, Think Bayes, and other books available\nfrom Green Tea Press.\n\nI am working on a book, also called Probably Overthinking It, which is about\nusing evidence and reason to answer questions and guide decision making.\n\nProbably Overthinking It is available for preorder now!\n\n#### Search this blog\n\n#### Subscribe via Email\n\n#### Archives\n\n  * April 2024 (3)\n  * March 2024 (1)\n  * February 2024 (5)\n  * January 2024 (3)\n  * December 2023 (4)\n  * November 2023 (3)\n  * October 2023 (5)\n  * September 2023 (2)\n  * August 2023 (2)\n  * July 2023 (3)\n  * June 2023 (2)\n  * May 2023 (2)\n  * April 2023 (3)\n  * March 2023 (3)\n  * February 2023 (3)\n  * January 2023 (4)\n  * November 2022 (2)\n  * October 2022 (1)\n  * September 2022 (3)\n  * August 2022 (3)\n  * July 2022 (1)\n  * May 2022 (3)\n  * April 2022 (1)\n  * December 2021 (1)\n  * September 2021 (1)\n  * August 2021 (3)\n  * July 2021 (1)\n  * June 2021 (1)\n  * May 2021 (4)\n  * April 2021 (5)\n  * January 2021 (2)\n  * November 2020 (1)\n  * October 2020 (2)\n  * September 2020 (1)\n  * July 2020 (2)\n  * June 2020 (1)\n  * April 2020 (1)\n  * February 2020 (2)\n  * January 2020 (7)\n  * December 2019 (3)\n  * October 2019 (2)\n  * September 2019 (1)\n  * August 2019 (4)\n  * July 2019 (4)\n  * June 2019 (1)\n  * May 2019 (1)\n  * April 2019 (1)\n  * March 2019 (2)\n  * February 2019 (3)\n  * January 2019 (2)\n  * December 2018 (5)\n  * October 2018 (3)\n  * September 2018 (3)\n\n#### Older articles\n\nOlder articles are available from this original site.\n\n#### My books\n\nThink Python 2e\n\nThink Bayes\n\nThink Complexity 2e\n\nThink Stats 2e\n\nThink DSP\n\nThink Java\n\nThink Data Structures\n\n\u00a9 2024 | Proudly Powered by WordPress | Theme: Nisarg\n\n", "frontpage": false}
