{"aid": "40102303", "title": "Scaling your models to 0 with Fly.io", "url": "https://xeiaso.net/talks/2024/ollama-fly-gpu/", "domain": "xeiaso.net", "votes": 1, "user": "xena", "posted_at": "2024-04-21 00:40:20", "comments": 0, "source_title": "Scaling your models to 0 with Fly.io - Xe Iaso", "source_text": "Scaling your models to 0 with Fly.io - Xe Iaso\n\n# Scaling your models to 0 with Fly.io\n\nMon Feb 12 2024\n\nWant to watch this in your video player of choice? Take this:\nhttps://cdn.xeiaso.net/file/christine-static/talks/2024/ollama-fly-\ngpu/index.m3u8\n\nOllama makes it easy to get started on ridiculous ventures like making your\nown personal assistant catgirl. At Fly.io, we're making it just as easy to get\nyour models and projects running in the cloud, so you don't have to burn your\nbattery when you're hacking things up. Here's how to set up your own private\nOllama server on Fly.io that turns off by itself when you're done with it.\n\nTo do all this, we're gonna need to do a couple things. First, you need to\ncreate a WireGuard connection to your private Fly network. Then we'll create\nthe app and give it a Flycast address. Finally we'll deploy it and you have\nyour own Ollama server!\n\nFirst, install the WireGuard app on your computer, or the wireguard-tools\npackage on Linux. This gets you a persistent connection to your private Fly\nnetwork.\n\nThen run fly wireguard create, import the config with the gui or wg-quick, and\nactivate it.\n\nIf you can ping _api.internal, you're in. You'll have a private connection to\nyour Fly network. You can use this to poke your applications directly, such as\nwith your Ollama server.\n\nNow that's set, create a new app in a new folder with fly launch. Give it a\nname and leave the defaults alone. Open fly.toml in your favourite editor,\nsuch as Vim.\n\nNext, we need to create a private IPv6 address for the app so that the\nplatform doesn't try to put it on the public internet by default. That would\nlet anyone access it, which is the opposite of private.\n\nRunning fly ips allocate-v6 --private will let you create a private flycast\naddress internal to your network. This is what we will use to contact the app.\nYou don't need to write it down, that's what DNS is for.\n\nThe region you choose will be based on what type of GPU you want. The\ndocumentation will have more details. I'll link to it in the writeup on the\nblog.\n\nThen change the VM size to the GPU you want. At the time of me speaking,\nthere's three options:\n\n  * a100-40gb\n  * a100-80gb\n  * l40s\n\nThese correlate to Ampere A100 at 40 and 80 GB of vram respectively and the\nLovelace L40s with 48 GB of vram. The most A100-40GB cards are in ORD, and the\nmost L40s cards are in SEA.\n\nNext, go back to your fly.toml file and add the build section and the mounts\nsection. The build section tells the platform to pull the Ollama image from\nthe Docker Hub. The mount section will create a 100GB persistent folder to\nstore all of your downloaded models in. It's easy as pi, except it's 100\ngigabytes instead of 3.14.\n\nFinally, set up a HTTP service block to point the platform to the Ollama port\nand enable automatic stopping of machines with a minimum of zero. This will\nmake the platform automatically spin down your GPU instance when you're not\nusing it, saving you money. Why pay for compute you're not using?\n\nNow that's all said and done, let's deploy it. Run fly deploy, hit enter, and\nwatch it scale right up.\n\nNow when you want to use ollama on your cloud GPU, set the Ollama host\nenvironment variable to your-app.flycast. Then any ollama commands will hit\nyour big chungus instead of your macbook. My local MacBook has a few models\nsuch as Llama 2 and Mistral, but the big chungus in the cloud has Nous Hermes\nMixtral.\n\nAs you see in this live demo that is clearly not pre-recorded at all, I'm\nrunning Mixtral on the cloud.\n\n(Pause)\n\nI'd say it's magic, but it's just science. Computer science! Let's hope any of\nthose citations are real, they pass the sniff test though!\n\nAnd this is how you can get your own private Ollama server in the cloud.\nAdditionally, if you have any applications in your fly network, you can have\nthem poke your Ollama server at your-app.flycast. And then they will turn the\nOllama server on when they need it and the platform will turn it off when they\ndon't.\n\nSuch as this project of mine that generates cryptocurrency investment\nhoroscopes every 12 hours. It pokes an Ollama machine running Mixtral every 12\nhours and gives...someone entertainment. The GPU turns off when it's done,\nsaving me money.\n\nAnd that's about it. I've been Xe Iaso and thank you for having me out here\ngiving you a blatant product demo. If you have any questions, please ask me.\nMy employer paid for me to fly out here and talk with you so please, please\ncome up and ask me some questions. I am more than happy to answer. If I don't\nget to them and you really want an answer, please email productdemo@xeserv.us.\n\nOtherwise, anyone have any questions?\n\nFacts and circumstances may have changed since publication. Please contact me\nbefore jumping to conclusions if something seems wrong or unclear.\n\nTags: ollama, ai, llm, art\n\nView slides\n\nCopyright 2012-2024 Xe Iaso (Christine Dodrill). Any and all opinions listed\nhere are my own and not representative of any of my employers, past, future,\nand/or present.\n\nLike what you see? Donate on Patreon like these awesome people!\n\nServed by xesite v4 (/nix/store/k97rlywj5qw9c7w69jl42gm1afw8vxnz-\nxesite_v4-20240224/bin/xesite) with site version b6478e00 , source code\navailable here.\n\n", "frontpage": false}
