{"aid": "40224233", "title": "LLM-as-Judge: Evaluating and Improving Language Model Performance in Production", "url": "https://segment.com/blog/llm-as-judge/", "domain": "segment.com", "votes": 1, "user": "n2parko", "posted_at": "2024-05-01 15:01:43", "comments": 0, "source_title": "LLM-as-Judge: Evaluating and Improving Language Model Performance in Production", "source_text": "LLM-as-Judge: Evaluating and Improving Language Model Performance in Production | Segment\n\n  * Help Center\n  * Docs\n  * API Status\n  * Contact Us\n\nLog in\n\nSign Up See how it works\n\n  * Help Center\n  * Docs\n  * API Status\n  * Contact Us\n\nSee how it works Log in Sign Up\n\nEngineers & Developers\n\n# LLM-as-Judge: Evaluating and Improving Language Model Performance in\nProduction\n\nHarness the power of Language Models (LLMs) as judges with Twilio Segment to\nimprove audience building, achieving over 90% alignment with human evaluation\nfor ASTs and paving the way for future advancements in AI-driven solutions.\n\nMay 01, 2024\n\nBy James Zhu, Alfredo Lainez Rodrigo, Ankit Awasthi, Salman Ahmed, Kevin\nNiparko\n\nShare article\n\nWant to stay updated on Segment launches, events, and updates? Subscribe below\nto keep in touch.\n\n### Thank you for subscribing!\n\nWe'll share the latest and greatest Segment content, events, and updates\nstraight to your inbox.\n\n#### Want to learn more about Customer Data Platforms?\n\nWatch a Product Tour to see Segment's CDP in action and learn how it can\nimpact your business.\n\nWatch now\n\nKeep updated\n\nLate last year, we had an LLM a-ha moment.\n\nEvery day, thousands of marketers flock to Segment to build new, advanced\naudiences & customer journeys to power their campaigns \u2013 but doing so requires\nmarketers to navigate an advanced UI and understand their data assets.\n\nWe asked ourselves: How can we harness LLM advancements to create\nsophisticated audiences and customer journeys for campaign generation, all\nwhile eliminating the need for marketers to grapple with complex UIs and data\nsubtleties? And what if we could streamline all these audience generation\ntasks into a simple prompt? Might such an advanced way of building audiences\nbe indistinguishable from Magic?\n\nFast-forward to today, CustomerAI audiences are in the wild. We\u2019ve been blown\naway by the results \u2013 customers are experiencing a 3x improvement in median\ntime-to-audience creation, and a whopping 95% feature retention rate IF the\naudience generation works on the first attempt.\n\nIn this blog post, we want to share the behind-the-scenes story of how we went\nfrom proof-of-concept to widely-adopted generative AI capabilities by using\nLLMs to generate, eval and decide. Today we will share behind the scenes\nmachinery that makes evals work \u2013 an emerging development approach referred to\nas LLM-as-Judge .\n\n# The Challenge\n\nLet\u2019s take a look at the problem we were trying to tackle.\n\nTo build an audience in Segment, end users can use the Audience Builder \u2013 a\nsophisticated UI that enables the expression of complex query logic without\ncode. For example, marketers can build an audience of \u201call users who have\nadded a product to cart but not checked out in the last 7 days\u201d. Once the\naudience is saved, the output (e.g. a list of users that meet the criteria)\nget federated into downstream tools like advertising and email marketing\ntools, so every marketing & sales system is operating off of the consistent\naudience definition.\n\nBehind the scenes, the inputs to the audience UI get compiled into an AST\n(abstract syntax tree). For example, here are the potential ASTs for the\naudience \u201cCustomers who have purchased at least 1 time\u201d:\n\nFor complex audiences, AST syntax can get complex, and there are many ways to\nexpress an audience that have the same meaning and an identical result. For\nexample, the same audience above could be defined as \u201cCustomers who have\npurchased more than 0 times but less than 2 times\u201d.\n\nSo as we set out to simplify audience-building via LLMs & natural language, we\nfaced the challenge: how do we evaluate a generative AI system, when there can\nbe an unbounded set of \u201cright answers\u201d?\n\nSolving this evaluation problem would provide the \u201cscoreboard\u201d we would use to\niterate on our architecture:\n\n  * Model Selection: Which model should we use? Anthropic\u2019s Claude or OpenAI\u2019s GPT models?\n\n  * Prompt Optimization: What prompt should I use in the system? If the developers have improved the prompt, how should we make sure the new prompt is better than the old one?\n\n  * RAG & Persistent Memory: If there are other components in the system, such as a vector database, how do we ensure the end user quality?\n\n  * Single v. Multi-Stage:Should we use a multi-stage LLM approach or a single-stage LLM approach?\n\n  * All of those practical questions relied on figuring out our evals\n\n# Give the LLM a Gavel \ud83e\uddd1\u2696\ufe0f\n\nThe general approach we landed on is that of LLM-as-Judge. Rather than define\na strict set of heuristics, LLM-as-Judge asks a model (the \u201cjudge\u201d) to\nevaluate, compare, and score {prompt:output} pairs to a \u201cground truth\u201d to\nassess the quality & accuracy of model inferences.\n\nSeveral recent papers have explored the use of LLMs as judges for various\ntasks, including JudgeLM , Prometheus, Generative Judge for Evaluating\nAlignment , and Calibrating LLM-Based Evaluator . The LLM-SQL-Solver paper was\nparticularly relevant for our use case, as it focuses on determining SQL\nequivalence, which is similar to our goal of evaluating ASTs.\n\n### Ground Truth & Synthetic Eval Generation\n\nThere was one sub-problem we had to solve before our judge could start\njudging. Our judge needs to compare the \u201cground truth\u201d (i.e. the correct AST\ninput by a customer via the UI) to {prompt:output} eval pairs. And while we\nhave a large dataset of ASTs, those ASTs were generated by our UI, not via a\nprompt.\n\nTo solve for the missing prompts, we built an LLM Question Generator Agent\nthat takes a \u201cground truth\u201d AST and generates a prompt. This may seem\ncounterintuitive, as we normally think of prompts as the input into models.\nBut to construct the necessary eval set, we needed to extract prompts from the\nAST. We then take the synthetic prompts as the input into the AST Generator.\n\nThe LLM Judge then compares the output:\n\nOur current architecture consists of several components:\n\n1\\. Real World AST Input from End users: These are the ASTs provided by\ncustomers or labellers, which serve as the ground truth for evaluation.\n\n2\\. LLM Question Generator Agent: This agent generates potential user input\nprompts based on the Real World AST Input.\n\n3\\. LLM AST Generator Agent: This agent takes the generated prompts and\nproduces ASTs using LLMs. This LLM Agent is responsible for generating the\nASTs given customer\u2019s input\n\n4\\. Generated AST: The AST produced by the LLM AST Generator Agent.\n\n5\\. LLM Judge Agent: This agent evaluates the Generated AST and provides a\nscore based on its alignment with the Real World AST Input.\n\n6\\. LLM Agent: The underlying LLM used for generating and evaluating ASTs.\n\n7\\. AST: The AST structure being evaluated.\n\n8\\. User's Prompt: The original prompt provided by the user.\n\nThe LLM Judge Agent evaluates the Generated AST and provides a score based on\nits alignment with the Real World AST Input. This score helps us determine the\nquality of the generated code and make improvements as needed.\n\n# Results\n\nOverall, we\u2019re been impressed with the results. Our LLM Judge Evaluation\nsystem has achieved over 90% alignment with human evaluation for ASTs.\n\nOur experimentation with various models for LLM AST Generator Agent has\nyielded promising results, with the most potent Claude model scoring 4.02 and\nthe gpt-4-32k-0613 model achieving the highest score of 4.55 with full score\n5.0.\n\nThese outcomes highlight the efficacy of our LLM Judge system in assessing and\nenhancing generated code.\n\n  * There is a remarkable similarity in the final scores between the 8K context length version and the 32k context window version which proves the stability of this method.\n\n  * GPT-4 Series performs slightly better than Claude 3\n\nFurther, these baseline scores enable us to compare future iterations &\noptimizations. For example, as we explore adding persistent memory via RAG.\nadopting a new model, or changing our prompting, we can compare the scores to\ndetermine the impact of improvements.\n\n# Privacy\n\nAt Twilio we believe building products using AI has to be built on three core\nprinciples: Transparent, Responsible, Accountable. You can refer to Generative\nAudiences Nutrition Facts Label for more details on how the data is used for\nthis feature.\n\n# Next Steps\n\nWhile we\u2019re excited about the impact so far, there are additional\noptimizations we plan to introduce for our evals:\n\n  1. Improving the correlation between LLM Judge and human scores to ensure that our evaluation process aligns well with human judgment.\n\n  2. Orchestrating different agents using frameworks such as AutoGen, which will allow for better coordination and efficiency in the evaluation process.\n\n  3. Applying LLM Judge to different use cases for CustomerAI to explore its potential in various domains and applications.\n\n# Conclusion\n\nOur mission at Twilio and Segment is to make magic happen by harnessing the\npower of LLMs to revolutionize the way marketers build audiences and customer\njourneys. As we look to the future, we are excited to delve deeper into the\npotential applications and optimizations of our LLM Judge system, with a focus\non concrete advancements and collaborations across the industry.\n\nWe encourage those interested in learning more about our work or exploring\npotential partnerships to reach out to us. In this era of LLMs, we believe\nthat sharing knowledge and learning from one another is crucial to driving\ninnovation and unlocking the full potential of AI-driven solutions.\n\nAs we continue to refine our approach and explore new use cases, we remain\ncommitted to fostering transparent, responsible, and accountable AI-driven\nsolutions that empower businesses and individuals alike. We hope that by\nsharing our experiences and learnings, we can inspire others to embark on\ntheir own LLM journeys and contribute to the collective growth of the AI\ncommunity. Together, we can shape the future of audience building and create a\nworld where the magic of LLMs is accessible to all.\n\n# Appendix\n\nTo provide a more in-depth understanding of the generative audience evaluation\nprocess, let's look at the steps involved.\n\n1\\. A user provides a prompt, such as \"Customers who have purchased at least 1\ntime.\"\n\n2\\. The prompt is converted into an AST, which is a tree-like data structure\nrepresenting the code structure. In our case, the AST is similar to a JSON\nobject.\n\n3\\. We need to evaluate if the generated AST is robust enough to be shipped to\nproduction.\n\nTo accomplish this, we use the \"ground truth\" ASTs provided by customers as a\nreference for evaluation. However, there can be multiple valid ways to\nrepresent the same code structure, making it difficult to determine the best\nAST representation. This is where LLMs can play the role of a judge, helping\nus evaluate and improve the quality of generated code.\n\nHowever, there are some challenges.\n\n1\\. LLMs struggle with continuous scores. For example, if asking LLM to give a\nscore from 0 to 100, LLM tends to only output discrete values such as 0 and\n100. To address this, we ask the model to provide scores on a discrete 1-5\nscale, with 1 being \"very bad\" and 5 being \"perfect.\" This helps us obtain\nmore interpretable results.\n\n2\\. The LLM Judge also needs a Chain of Thought (CoT) to provide reasoning for\nthe scores, which can improve the model's capability and facilitate human\nevaluation. Implementing CoT allows the model to explain its decisions, making\nit easier for engineers to understand and trust the evaluation process. By\nusing CoT, the alignment with human increase from roughly 89% to 92%.\n\n3\\. We adopt OpenAI\u2019s GPT-4 model as the actual LLM Judge model. Surprisingly,\nwe find that using other strong models such as Claude 3 Opus achieve similar\nscores with GPT-4, which shows that the alignment between different models and\nhuman.\n\nLLMs have proven to be a powerful tool for evaluating and improving code\ngeneration in the context of Segment and Twilio engineering. By using LLMs as\njudge, we can enhance the quality of our generated code and provide better\nrecommendations to our users. As we continue to refine our approach and\nexplore new use cases, we expect LLMs to play an increasingly important role\nin our engineering efforts, leading to more efficient and effective solutions.\n\n### Test drive Segment CDP today\n\nIt\u2019s free to connect your data sources and destinations to the Segment CDP.\nUse one API to collect analytics data across any platform.\n\nGet started\n\n## Recommended articles\n\nLoading\n\n## Want to keep updated on Segment launches, events, and updates?\n\n### Thank you for subscribing!\n\nSee how it works\n\n### Products\n\n  * Connections\n  * Protocols\n  * Unify\n  * Twilio Engage\n  * Customer Data Platform\n  * Integrations Catalog\n  * Pricing\n  * Security\n  * GDPR\n\n### For Developers\n\n  * Documentation\n  * Segment API\n  * Build on Segment\n  * Open Source\n  * Engineering Team\n\n### Company\n\n  * Careers\n  * Blogs\n  * Press\n  * Events\n  * Podcast\n  * Growth Center\n  * Data Hub\n\n### Support\n\n  * Help Center\n  * Contact us\n  * Resources\n  * Recipes\n  * Professional Services\n  * Security Bulletins\n  * Documentation\n  * Release Notes\n  * Become a Partner\n  * Guide to Customer Data Platforms\n\n\u00a9 2024 Twilio Inc. All Rights Reserved.\n\n    Privacy policy Terms of Service\n\nWebsite Data Collection\n\n", "frontpage": false}
