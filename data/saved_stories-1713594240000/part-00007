{"aid": "40092128", "title": "A frequentist approach to probability (2013)", "url": "https://blog.jliszka.org/2013/08/12/a-frequentist-approach-to-probability.html", "domain": "jliszka.org", "votes": 1, "user": "jnakayama", "posted_at": "2024-04-19 21:30:15", "comments": 0, "source_title": "A frequentist approach to probability", "source_text": "A frequentist approach to probability\n\nA Gentleman and a Scala\n\n  * Archive\n  * Tags\n\n# A frequentist approach to probability\n\n12 August 2013\n\nOne thing that always confused me in my intro stats classes was the concept of\na random variable. A random variable is not a variable like I\u2019m used to\nthinking about, like a thing that has one value at a time. A random variable\nis instead an object that you can sample values from, and the values you get\nwill be distributed according to some underlying probability distribution.\n\nIn that way it sort of acts like a container, where the only operation is to\nsample a value from the container. In Scala it might look something like:\n\n    \n    \n    trait Distribution[A] { def get: A }\n\nThe idea is that get returns a different value (of type A) from the\ndistribution every time you call it.\n\nI\u2019m going to add a sample method that lets me draw a sample of any size I want\nfrom the distribution.\n\n    \n    \n    trait Distribution[A] { def get: A def sample(n: Int): List[A] = { List.fill(n)(this.get) } }\n\nNow to create a simple distribution. Here\u2019s one whose samples are uniformly\ndistributed between 0 and 1.\n\n    \n    \n    val uniform = new Distribution[Double] { private val rand = new java.util.Random() override def get = rand.nextDouble() }\n\nAnd sampling it gives\n\n    \n    \n    scala> uniform.sample(10).foreach(println) 0.15738645964157327 0.7827120503875181 0.8787176537434814 0.38506604599728245 0.9469681837641953 0.20822217752687067 0.8229649049912187 0.7767540566158817 0.4133782959276152 0.8152378840945975\n\n### Transforming distributions\n\nEvery good container should have a map method. map will transform values\nproduced by the distribution according to some function you pass it.\n\n    \n    \n    trait Distribution[A] { self => // ... def map[B](f: A => B): Distribution[B] = new Distribution[B] { override def get = f(self.get) } }\n\n(Quick technical note: I added a self-type annotation that makes self an alias\nfor this so that it\u2019s easier to refer to in anonymous inner classes.)\n\nNow I can map * 2 over the uniform distribution, giving a uniform distribution\nbetween 0 and 2:\n\n    \n    \n    scala> uniform.map(_ * 2).sample(10).foreach(println) 1.608298200368093 0.14423181179528677 0.31844160650777886 1.6299535560273648 1.0188592816936894 1.9150473071752487 0.9324757358322544 0.5287503566916676 1.35497977515358 0.5874386820078819\n\nmap also lets you create distributions of different types:\n\n    \n    \n    scala> val tf = uniform.map(_ < 0.5) tf: Distribution[Boolean] = <distribution> scala> tf.sample(10) res2: List[Boolean] = List(true, true, true, true, false, false, false, false, true, false)\n\ntf is a Distribution[Boolean] that should give true and false with equal\nprobability. Actually, it would be a bit more useful to be able to create\ndistributions giving true and false with arbitrary probabilities.\n\n    \n    \n    def tf(p: Double): Distribution[Boolean] = { uniform.map(_ < p) }\n\nTrying it out:\n\n    \n    \n    scala> tf(0.8).sample(10) res0: List[Boolean] = List(true, false, true, true, true, true, true, true, true, true)\n\nA very closely related distribution is the Bernoulli distribution, which gives\n0 or 1 with some probability instead of true and false. This can be achieved\nwith a simple map:\n\n    \n    \n    def bernoulli(p: Double): Distribution[Int] = { tf.map(b => if (b) 1 else 0) }\n\nCool. Now I want to measure the probability that a random variable will take\non certain values. This is easy to do empirically by pulling 10,000 sample\nvalues and counting how many of the values satisfy the given predicate.\n\n    \n    \n    trait Distribution[A] { // ... private val N = 10000 def pr(predicate: A => Boolean): Double = { this.sample(N).count(predicate).toDouble / N } }\n    \n    \n    scala> uniform.pr(_ < 0.4) res2: Double = 0.4015\n\nIt works! It\u2019s not exact, but it\u2019s close enough.\n\nNow I need two ways to transform a distribution.\n\n    \n    \n    trait Distribution[A] { // ... def given(predicate: A => Boolean): Distribution[A] = new Distribution[A] { @tailrec override def get = { val a = self.get if (predicate(a)) a else this.get } } def repeat(n: Int): Distribution[List[A]] = new Distribution[List[A]] { override def get = { List.fill(n)(self.get) } } }\n\ngiven creates a new distribution by sampling from the original distribution\nand discarding values that don\u2019t match the given predicate. repeat creates a\nDistribution[List[A]] from a Distribution[A] by producing samples that are\nlists of samples from the original distributions.\n\nOK, now one more distribution:\n\n    \n    \n    def discreteUniform[A](values: Iterable[A]): Distribution[A] = { val vec = values.toVector uniform.map(x => vec((x * vec.length).toInt)) }\n\nLet\u2019s see how all this works.\n\n    \n    \n    scala> val die = discreteUniform(1 to 6) die: Distribution[Int] = <distribution> scala> die.sample(10) res0: List[Int] = List(1, 5, 6, 5, 4, 3, 5, 4, 1, 1) scala> die.pr(_ == 4) res1: Double = 0.1668 scala> die.given(_ % 2 == 0).pr(_ == 4) res2: Double = 0.3398 scala> val dice = die.repeat(2).map(_.sum) dice: Distribution[Int] = <distribution> scala> dice.pr(_ == 7) res3: Double = 0.1653 scala> dice.pr(_ == 11) res4: Double = 0.0542 scala> dice.pr(_ < 4) res5: Double = 0.0811\n\nNeat! This is getting useful.\n\nOK I\u2019m tired of looking at individual probabilities. What I really want is a\nway to visualize the entire distribution.\n\n    \n    \n    scala> dice.hist 2 2.67% ## 3 5.21% ##### 4 8.48% ######## 5 11.52% ########### 6 13.78% ############# 7 16.61% ################ 8 13.47% ############# 9 11.17% ########### 10 8.66% ######## 11 5.64% ##### 12 2.79% ##\n\nThat\u2019s better. hist pulls 10,000 samples from the distribution, buckets them,\ncounts the size of the buckets, and finds a good way to display it. (The code\nis tedious so I\u2019m not going to reproduce it here.)\n\n### Don\u2019t tell anyone it\u2019s a monad\n\nAnother way to represent two die rolls is to sample from die twice and add the\nsamples.\n\n    \n    \n    scala> val dice = die.map(d1 => die.map(d2 => d1 + d2)) dice: Distribution[Distribution[Int]] = <distribution>\n\nBut wait, that gives me a Distribution[Distribution[Int]], which is nonsense.\nFortunately there\u2019s an easy fix.\n\n    \n    \n    trait Distribution[A] { // ... def flatMap[B](f: A => Distribution[B]): Distribution[B] = new Distribution[B] { override def get = f(self.get).get } }\n\nLet\u2019s try it now.\n\n    \n    \n    scala> val dice = die.flatMap(d1 => die.map(d2 => d1 + d2)) dice: Distribution[Int] = <distribution> scala> dice.hist 2 2.71% ## 3 5.17% ##### 4 8.23% ######## 5 11.54% ########### 6 14.04% ############## 7 16.67% ################ 8 13.53% ############# 9 10.97% ########## 10 8.81% ######## 11 5.62% ##### 12 2.71% ##\n\nIt worked!\n\nThe definition of dice can be re-written using Scala\u2019s for-comprehension\nsyntax:\n\n    \n    \n    val dice = for { d1 <- die d2 <- die } yield d1 + d2\n\nThis is really nice. The <\\- notation can be read as sampling a value from a\ndistribution. d1 and d2 are samples from die and both have type Int. d1 + d2\nis a sample from dice, the distribution I\u2019m creating.\n\nIn other words, I\u2019m creating a new distribution by writing code that\nconstructs a single sample of the distribution from individual samples of\nother distributions. This is pretty handy! Lots of common distributions can be\nconstructed this way. (More on that soon!)\n\n### Monty Hall\n\nI think it would be fun to model the Monty Hall problem.\n\n    \n    \n    val montyHall: Distribution[(Int, Int)] = { val doors = (1 to 3).toSet for { prize <- discreteUniform(doors) // The prize is placed randomly choice <- discreteUniform(doors) // You choose randomly opened <- discreteUniform(doors - prize - choice) // Monty opens one of the other doors switch <- discreteUniform(doors - choice - opened) // You switch to the unopened door } yield (prize, switch) }\n\nThis code constructs a distribution of pairs representing the door the prize\nis behind and the door you switched to. Let\u2019s see how often those are the same\ndoor:\n\n    \n    \n    scala> montyHall.pr{ case (prize, switch) => prize == switch } res0: Double = 0.6671\n\nJust as expected. Lots of people have a hard time believing the explanation\nbehind why this is correct, but there\u2019s no arguing with just trying it 10,000\ntimes!\n\n### HTH vs HTT\n\nAnother fun problem: if you flip a coin repeatedly, which pattern do you\nexpect to see earlier, heads-tails-heads or heads-tails-tails?\n\nFirst I need the following method:\n\n    \n    \n    trait Distribution[A] { // ... def until(pred: List[A] => Boolean): Distribution[List[A]] = new Distribution[List[A]] { override def get = { @tailrec def helper(sofar: List[A]): List[A] = { if (pred(sofar)) sofar else helper(self.get :: sofar) } helper(Nil) } } }\n\nuntil samples from the distribution, adding the samples to the front of the\nlist until the list satisfies some predicate. A single sample from the\nresulting distribution is a list that satisfies the predicate.\n\nNow I can do:\n\n    \n    \n    val hth = tf(0.5).until(_.take(3) == List(true, false, true)).map(_.length) val htt = tf(0.5).until(_.take(3) == List(false, false, true)).map(_.length)\n\nLooking at the distributions:\n\n    \n    \n    scala> hth.hist 3 11.63% ########### 4 12.43% ############ 5 9.50% ######### 6 7.82% ####### 7 7.31% ####### 8 6.51% ###### 9 5.41% ##### 10 4.57% #### 11 4.56% #### 12 3.78% ### 13 3.44% ### 14 3.04% ### 15 2.52% ## 16 2.08% ## 17 1.76% # 18 1.70% # 19 1.34% # 20 1.34% # scala> htt.hist 3 12.94% ############ 4 12.18% ############ 5 12.48% ############ 6 11.29% ########### 7 9.88% ######### 8 7.67% ####### 9 6.07% ###### 10 5.32% ##### 11 4.18% #### 12 3.51% ### 13 2.78% ## 14 2.23% ## 15 1.75% # 16 1.40% # 17 1.21% # 18 0.92% 19 0.78% 20 0.60%\n\nEyeballing it, it appears that HTT is likely to occur earlier than HTH. (How\ncan this be? Excercise for the reader!) But I\u2019d like to get a more concrete\nanswer than that. What I want to know is how many flips you expect to see\nbefore seeing either pattern. So let me add a method to compute the expected\nvalue of a distribution:\n\n    \n    \n    trait Distribution[A] { // ... def ev: Double = { Stream.fill(N)(self.get).sum / N } }\n\nHm, that .sum is not going to work for all As. I mean, A could certainly be\nBoolean, as in the case of the tf distribution (what is the expected value of\na coin flip?). So I need to constrain A to Double for the purposes of this\nmethod.\n\n    \n    \n    trait Distribution[A] { // ... def ev(implicit toDouble: A <:< Double): Double = { Stream.fill(N)(toDouble(self.get)).sum / N } }\n    \n    \n    scala> hth.ev <console>:15: error: Cannot prove that Int <:< Double. hth.ev\n\nPerfect. You know, it really bothered me when I first learned that the\nexpected value of a die roll is 3.5. Requiring an explicit conversion to\nDouble before computing the expected value of any distribution makes that fact\na lot more palatable.\n\n    \n    \n    scala> hth.map(_.toDouble).ev res0: Double = 9.9204 scala> htt.map(_.toDouble).ev res1: Double = 7.9854\n\nThere we go, empirical confirmation that HTT is expected to appear after 8\nflips and HTH after 10 flips.\n\nI\u2019m curious. Suppose you and I played a game where we each flipped a coin\nuntil I got HTH and you got HTT. Then whoever took more flips pays the other\nperson the difference. What is the expected value of this game? Is it 2? It\ndoesn\u2019t have to be 2, does it? Maybe the distributions are funky in some way\nthat makes the difference in expected value 2 but the expected difference\nsomething else.\n\nWell, easy enough to try it.\n\n    \n    \n    val diff = for { me <- hth you <- htt } yield me - you\n    \n    \n    scala> diff.map(_.toDouble).ev res3: Double = 1.9976\n\nActually, it does have to be 2. Expectation is linear!\n\n### Unbiased rounding\n\nAt Foursquare we have some code that computes how much our customers owe us,\nand charges them for it. Our payments provider, Stripe, only allows us to\ncharge in whole cents, but for complicated business reasons sometimes a\ncustomer owes us fractional cents. (No, this is not an Office Space or\nSuperman III reference.) So we just round to the nearest whole cent (actually\nwe use unbiased rounding, or banker\u2019s rounding, which rounds 0.5 cents up half\nthe time and down half the time).\n\nBecause we\u2019re paranoid and also curious, we want to know how much money we are\nlosing or gaining due to rounding. Let\u2019s say that during some period of time\nwe saw that we rounded 125 times, and the sum of all the roundings totaled\n+8.5 cents. That kinda seems like a lot, but it could happen by chance. If\nfractional cents are uniformly distributed, what is the probability that you\nwould see a difference that big after 125 roundings?\n\nLet\u2019s find out.\n\n    \n    \n    scala> val d = uniform.map(x => if (x < 0.5) -x else 1.0-x).repeat(125).map(_.sum) d: Distribution[Double] = <distribution> scala> d.hist -10.0 0.02% -9.0 0.20% -8.0 0.57% -7.0 1.32% # -6.0 2.15% ## -5.0 3.75% ### -4.0 5.12% ##### -3.0 7.83% ####### -2.0 10.58% ########## -1.0 11.44% ########### 0.0 12.98% ############ 1.0 11.57% ########### 2.0 10.68% ########## 3.0 7.73% ####### 4.0 5.70% ##### 5.0 3.88% ### 6.0 2.32% ## 7.0 1.21% # 8.0 0.65% 9.0 0.25% 10.0 0.06%\n\nThere\u2019s the distribution. Each instance is either a loss of x if x < 0.5 or a\ngain of 1.0-x. Repeat 125 times and sum it all up to get the total gain or\nloss from rounding.\n\nNow what\u2019s the probability that we\u2019d see a total greater than 8.5 cents? (Or\nless than -8.5 cents \u2014 a loss of 8.5 cents would be equally surprising.)\n\n    \n    \n    scala> d.pr(x => math.abs(x) > 8.5) res0: Double = 0.0098\n\nPretty unlikely, about 1%! So the distribution of fractional cents is probably\nnot uniform. We should maybe look into that.\n\n### The normal distribution\n\nOne last example. It turns out the normal distribution can be approximated\npretty well by summing 12 uniformly distributed random variables and\nsubtracting 6. In code:\n\n    \n    \n    val normal: Distribution[Double] = { uniform.repeat(12).map(_.sum - 6) }\n\nHere\u2019s what it looks like:\n\n    \n    \n    scala> normal.hist -3.50 0.04% -3.00 0.18% -2.50 0.80% -2.00 2.54% ## -1.50 6.62% ###### -1.00 12.09% ############ -0.50 17.02% ################# 0.00 20.12% #################### 0.50 17.47% ################# 1.00 12.63% ############ 1.50 6.85% ###### 2.00 2.61% ## 2.50 0.82% 3.00 0.29% 3.50 0.01% scala> normal.pr(x => math.abs(x) < 1) res0: Double = 0.6745 scala> normal.pr(x => math.abs(x) < 2) res1: Double = 0.9566 scala> normal.pr(x => math.abs(x) < 3) res2: Double = 0.9972\n\nI believe it! One more check though.\n\n    \n    \n    trait Distribution[A] { // ... def variance(implicit toDouble: A <:< Double): Double = { val mean = this.ev this.map(x => { math.pow(toDouble(x) - mean, 2) }).ev } def stdev(implicit toDouble: A <:< Double): Double = { math.sqrt(this.variance) } }\n\nThe variance of a random variable with mean is , and the standard deviation is\njust the square root of the variance.\n\nAnd now:\n\n    \n    \n    scala> normal.stdev res0: Double = 0.9990012220368588\n\nPerfect.\n\nThis is a great approximation and all, but java.util.Random actually provides\na nextGaussian method, so for the sake of performance I\u2019m just going to use\nthat.\n\n    \n    \n    val normal: Distribution[Double] = new Distribution[Double] { override def get = { rand.nextGaussian() } }\n\n### Conclusion\n\nThe frequentist approach lines up really well with my intuitions about\nprobability. And Scala\u2019s for-comprehensions provide a suggestive syntax for\nconstructing new random variables from existing ones. So I\u2019m going to continue\nto explore various concepts in probability and statistics using these tools.\n\nIn later posts I\u2019ll try to model Bayesian inference, Markov chains, the\nCentral Limit Theorem, probablistic graphical models, and a bunch of related\ndistributions.\n\nAll of the code for this is on github.\n\nTweet\n\n  * probability 10\n\nClimbing the probability distribution ladder \u2192\n\nblog comments powered by Disqus\n\nJason Liszka Engineer at Slack NYC, CMU alum, Scala fan, new father\n\n#### Popular posts\n\n  * How traffic actually works\n  * A frequentist approach to probability\n  * Exact numeric nth derivatives\n  * Infinite lazy polynomials\n\n#### Recent posts\n\n  * Between order and chaos\n  * Probability is in the process\n  * The quantum eraser demystified\n  * Is the NBA draft rigged?\n  * Programming with futures: patterns and anti-patterns\n\nFollow @jliszka\n\n\u00a9 2013 Jason Liszka with help from Jekyll Bootstrap and Twitter Bootstrap\n\n", "frontpage": false}
