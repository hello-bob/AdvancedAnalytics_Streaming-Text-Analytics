{"aid": "40138193", "title": "a poem about the Turing halting proof", "url": "http://www.lel.ed.ac.uk/~gpullum/loopsnoop.html", "domain": "ed.ac.uk", "votes": 16, "user": "soferio", "posted_at": "2024-04-23 22:42:12", "comments": 0, "source_title": "Scooping the Loop Snooper \u2014 Geoffrey K. Pullum", "source_text": "Scooping the Loop Snooper \u2014 Geoffrey K. Pullum\n\n# SCOOPING THE LOOP SNOOPER\n\n## A proof that the Halting Problem is undecidable\n\nGeoffrey K. Pullum (School of Philosophy, Psychology and Language Sciences,\nUniversity of Edinburgh)\n\nNo general procedure for bug checks will do. Now, I won\u2019t just assert that,\nI\u2019ll prove it to you. I will prove that although you might work till you drop,\nyou cannot tell if computation will stop.  \n---  \nFor imagine we have a procedure called P that for specified input permits you\nto see whether specified source code, with all of its faults, defines a\nroutine that eventually halts.  \nYou feed in your program, with suitable data, and P gets to work, and a little\nwhile later (in finite compute time) correctly infers whether infinite looping\nbehavior occurs.  \nIf there will be no looping, then P prints out \u2018Good.\u2019 That means work on this\ninput will halt, as it should. But if it detects an unstoppable loop, then P\nreports \u2018Bad!\u2019 \u2014 which means you\u2019re in the soup.  \nWell, the truth is that P cannot possibly be, because if you wrote it and gave\nit to me, I could use it to set up a logical bind that would shatter your\nreason and scramble your mind.  \nHere\u2019s the trick that I\u2019ll use \u2014 and it\u2019s simple to do. I\u2019ll define a\nprocedure, which I will call Q, that will use P\u2019s predictions of halting\nsuccess to stir up a terrible logical mess.  \nFor a specified program, say A, one supplies, the first step of this program\ncalled Q I devise is to find out from P what\u2019s the right thing to say of the\nlooping behavior of A run on A.  \nIf P\u2019s answer is \u2018Bad!\u2019, Q will suddenly stop. But otherwise, Q will go back\nto the top, and start off again, looping endlessly back, till the universe\ndies and turns frozen and black.  \nAnd this program called Q wouldn\u2019t stay on the shelf; I would ask it to\nforecast its run on itself. When it reads its own source code, just what will\nit do? What\u2019s the looping behavior of Q run on Q?  \nIf P warns of infinite loops, Q will quit; yet P is supposed to speak truly of\nit! And if Q\u2019s going to quit, then P should say \u2018Good.\u2019 Which makes Q start to\nloop! (P denied that it would.)  \nNo matter how P might perform, Q will scoop it: Q uses P\u2019s output to make P\nlook stupid. Whatever P says, it cannot predict Q: P is right when it\u2019s wrong,\nand is false when it\u2019s true!  \nI\u2019ve created a paradox, neat as can be \u2014 and simply by using your putative P.\nWhen you posited P you stepped into a snare; Your assumption has led you right\ninto my lair.  \nSo where can this argument possibly go? I don\u2019t have to tell you; I\u2019m sure you\nmust know. We've proved that there just cannot possibly be a procedure that\nacts like the mythical P.  \nYou can never find general mechanical means for predicting the acts of\ncomputing machines; it\u2019s something that cannot be done. So we users must find\nour own bugs. Our computers are losers!  \n  \nAn earlier version of this poetic proof was published in Mathematics Magazine\n(73, no. 4, 319\u2013320) in October 2000. But despite a refereeing process that\ntook nearly a year, it had an unnoticed error. I am grateful to Philip Wadler\n(Informatics, University of Edinburgh) and Larry Moss (Mathematics, Indiana\nUniversity) for helping develop this corrected version.\n\nThrough the kindness of Anuj Dawar, I had the great privilege and pleasure of\nreading this aloud at a conference in honour of the memory of Alan Turing at\nCambridge University in June 2012. (Notice that reading it aloud works best in\nsouthern British standard English: the rhyme of the first two lines of the\nthird stanza call for a non-rhotic dialect.) However, one attendee at that\nconference, Damiano Mazza of the \u00c9cole Polytechnique, pointed out much later\nthat my revised version had another minor fault: it referred to the proof was\na reductio, when it really isn't (it's a proof by assuming something in order\nto show that a contradiction results). So I altered one line on 7 April 2022.\n(I have tried unsuccessfully to comfort myself over this history of\ncarelessness on my part with the thought that Turing's original 1936 paper\nalso had a few errors; he published a short correction in 1937.) The corrected\nversion has been reprinted with my permission in a beautiful textbook by David\nLiben-Nowell, Connecting Discrete Mathematics and Computer Science (Cambridge\nUniversity Press, 2022), p. 196.\n\nMy thanks to the late Dr. Seuss for the style, and to the pioneering work of\nAlan Turing for the content, and to Martin Davis\u2019s nicely understandable\nsimplified presentations of it, and to Philip Wadler, Larry Moss, and Damiano\nMazza for their help.\n\nCopyright \u00a9 2008, 2012, 2022 by Geoffrey K. Pullum. Permission is hereby\ngranted to reproduce or distribute this work in any medium for non-commercial,\neducational purposes relating to the teaching of computer science,\nmathematics, or logic, provided I am informed and the above copyright\nattribution is included.\n\n", "frontpage": true}
