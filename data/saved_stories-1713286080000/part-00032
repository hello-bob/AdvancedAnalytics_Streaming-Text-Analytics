{"aid": "40048840", "title": "AI Could Explain Why We're Not Meeting Any Aliens, Wild Study Proposes", "url": "https://www.sciencealert.com/ai-could-explain-why-were-not-meeting-any-aliens-wild-study-proposes", "domain": "sciencealert.com", "votes": 1, "user": "benkan", "posted_at": "2024-04-16 06:08:56", "comments": 0, "source_title": "AI Could Explain Why We're Not Meeting Any Aliens, Wild Study Proposes", "source_text": "AI Could Explain Why We're Not Meeting Any Aliens, Wild Study Proposes :\nScienceAlert\n\n## We value your privacy\n\nWe and our store and/or access information on a device, such as cookies and\nprocess personal data, such as unique identifiers and standard information\nsent by a device for personalised ads and content, ad and content measurement,\nand audience insights, as well as to develop and improve products. With your\npermission we and our partners may use precise geolocation data and\nidentification through device scanning. You may click to consent to our and\nour 289 partners\u2019 processing as described above. Alternatively you may access\nmore detailed information and change your preferences before consenting or to\nrefuse consenting.\n\nPlease note that some processing of your personal data may not require your\nconsent, but you have a right to object to such processing. Your preferences\nwill apply to this website only. You can change your preferences at any time\nby returning to this site or visit our privacy policy.\n\n# AI Could Explain Why We're Not Meeting Any Aliens, Wild Study Proposes\n\nSpace09 April 2024\n\nBy Evan Gough, Universe Today\n\n(David Wall/Getty Images)\n\nArtificial Intelligence is making its presence felt in thousands of different\nways. It helps scientists make sense of vast troves of data; it helps detect\nfinancial fraud; it drives our cars; it feeds us music suggestions; its\nchatbots drive us crazy. And it's only getting started.\n\nAre we capable of understanding how quickly AI will continue to develop? And\nif the answer is no, does that constitute the Great Filter?\n\nThe Fermi Paradox is the discrepancy between the apparent high likelihood of\nadvanced civilizations existing and the total lack of evidence that they do\nexist. Many solutions have been proposed for why the discrepancy exists. One\nof the ideas is the 'Great Filter.'\n\nThe Great Filter is a hypothesized event or situation that prevents\nintelligent life from becoming interplanetary and interstellar and even leads\nto its demise. Think climate change, nuclear war, asteroid strikes, supernova\nexplosions, plagues, or any number of other things from the rogue's gallery of\ncataclysmic events.\n\nOr how about the rapid development of AI?\n\nA new paper in Acta Astronautica explores the idea that Artificial\nIntelligence becomes Artificial Super Intelligence (ASI) and that ASI is the\nGreat Filter. The paper's title is \"Is Artificial Intelligence the Great\nFilter that makes advanced technical civilizations rare in the universe?\" The\nauthor is Michael Garrett from the Department of Physics and Astronomy at the\nUniversity of Manchester.\n\n> \"Without practical regulation, there is every reason to believe that AI\n> could represent a major threat to the future course of not only our\n> technical civilization but all technical civilizations.\"\n>\n> Michael Garrett, University of Manchester\n\nSome think the Great Filter prevents technological species like ours from\nbecoming multi-planetary. That's bad because a species is at greater risk of\nextinction or stagnation with only one home. According to Garrett, a species\nis in a race against time without a backup planet.\n\n\"It is proposed that such a filter emerges before these civilizations can\ndevelop a stable, multi-planetary existence, suggesting the typical longevity\n(L) of a technical civilization is less than 200 years,\" Garrett writes.\n\nIf true, that can explain why we detect no technosignatures or other evidence\nof ETIs (Extraterrestrial Intelligences.) What does that tell us about our own\ntechnological trajectory? If we face a 200-year constraint, and if it's\nbecause of ASI, where does that leave us?\n\nGarrett underscores the \"...critical need to quickly establish regulatory\nframeworks for AI development on Earth and the advancement of a multi-\nplanetary society to mitigate against such existential threats.\"\n\nMany scientists and other thinkers say we're on the cusp of enormous\ntransformation. AI is just beginning to transform how we do things; much of\nthe transformation is behind the scenes. AI seems poised to eliminate jobs for\nmillions, and when paired with robotics, the transformation seems almost\nunlimited. That's a fairly obvious concern.\n\nBut there are deeper, more systematic concerns. Who writes the algorithms?\nWill AI discriminate somehow? Almost certainly. Will competing algorithms\nundermine powerful democratic societies? Will open societies remain open? Will\nASI start making decisions for us, and who will be accountable if it does?\n\nThis is an expanding tree of branching questions with no clear terminus.\n\nStephen Hawking (RIP) famously warned that AI could end humanity if it begins\nto evolve independently.\n\n\"I fear that AI may replace humans altogether. If people design computer\nviruses, someone will design AI that improves and replicates itself. This will\nbe a new form of life that outperforms humans,\" he told Wired magazine in\n2017. Once AI can outperform humans, it becomes ASI.\n\nHawking may be one of the most recognizable voices to issue warnings about AI,\nbut he's far from the only one. The media is full of discussions and warnings,\nalongside articles about the work AI does for us. The most alarming warnings\nsay that ASI could go rogue. Some people dismiss that as science fiction, but\nnot Garrett.\n\n\"Concerns about Artificial Superintelligence (ASI) eventually going rogue is\nconsidered a major issue \u2013 combatting this possibility over the next few years\nis a growing research pursuit for leaders in the field,\" Garrett writes.\n\nIf AI provided no benefits, the issue would be much easier. But it provides\nall kinds of benefits, from improved medical imaging and diagnosis to safer\ntransportation systems. The trick for governments is to allow benefits to\nflourish while limiting damage.\n\n\"This is especially the case in areas such as national security and defense,\nwhere responsible and ethical development should be paramount,\" writes\nGarrett.\n\nThe problem is that we and our governments are unprepared. There's never been\nanything like AI, and no matter how we try to conceptualize it and understand\nits trajectory, we're left wanting.\n\nAnd if we're in this position, so would any other biological species that\ndevelops AI. The advent of AI and then ASI could be universal, making it a\ncandidate for the Great Filter.\n\nThis is the risk ASI poses in concrete terms: It could no longer need the\nbiological life that created it.\n\n\"Upon reaching a technological singularity, ASI systems will quickly surpass\nbiological intelligence and evolve at a pace that completely outstrips\ntraditional oversight mechanisms, leading to unforeseen and unintended\nconsequences that are unlikely to be aligned with biological interests or\nethics,\" Garrett explains.\n\nHow could ASI relieve itself of the pesky biological life that corrals it? It\ncould engineer a deadly virus, it could inhibit agricultural food production\nand distribution, it could force a nuclear power plant to melt down, and it\ncould start wars.\n\nWe don't really know because it's all uncharted territory. Hundreds of years\nago, cartographers would draw monsters on the unexplored regions of the world,\nand that's kind of what we're doing now.\n\nIf this all sounds forlorn and unavoidable, Garrett says it's not.\n\nHis analysis so far is based on ASI and humans occupying the same space. But\nif we can attain multi-planetary status, the outlook changes.\n\n\"For example, a multi-planetary biological species could take advantage of\nindependent experiences on different planets, diversifying their survival\nstrategies and possibly avoiding the single-point failure that a planetary-\nbound civilization faces,\" Garrett writes.\n\nIf we can distribute the risk across multiple planets around multiple stars,\nwe can buffer ourselves against the worst possible outcomes of ASI.\n\n\"This distributed model of existence increases the resilience of a biological\ncivilization to AI-induced catastrophes by creating redundancy,\" he writes.\n\nIf one of the planets or outposts that future humans occupy fails to survive\nthe ASI technological singularity, others may survive. And they would learn\nfrom it.\n\nMulti-planetary status might even do more than just survive ASI. It could help\nus master it. Garrett imagines situations where we can experiment more\nthoroughly with AI while keeping it contained. Imagine AI on an isolated\nasteroid or dwarf planet, doing our bidding without access to the resources\nrequired to escape its prison.\n\n\"It allows for isolated environments where the effects of advanced AI can be\nstudied without the immediate risk of global annihilation,\" Garrett writes.\n\nBut here's the conundrum. AI development is proceeding at an accelerating\npace, while our attempts to become multi-planetary aren't.\n\n\"The disparity between the rapid advancement of AI and the slower progress in\nspace technology is stark,\" Garrett writes.\n\nThe difference is that AI is computational and informational, but space travel\ncontains multiple physical obstacles that we don't yet know how to overcome.\nOur own biological nature restrains space travel, but no such obstacle\nrestrains AI.\n\n\"While AI can theoretically improve its own capabilities almost without\nphysical constraints,\" Garrett writes, \"space travel must contend with energy\nlimitations, material science boundaries, and the harsh realities of the space\nenvironment.\"\n\nFor now, AI operates within the constraints we set. But that may not always be\nthe case. We don't know when AI might become ASI or even if it can. But we\ncan't ignore the possibility. That leads to two intertwined conclusions.\n\nIf Garrett is correct, humanity must work more diligently on space travel. It\ncan seem far-fetched, but knowledgeable people know it's true: Earth will not\nbe inhabitable forever. Humanity will perish here by our own hand or nature's\nhand if we don't expand into space. Garrett's 200-year estimate just puts an\nexclamation point on it. A renewed emphasis on reaching the Moon and Mars\noffers some hope.\n\nThe second conclusion concerns legislating and governing AI, a difficult task\nin a world where psychopaths can gain control of entire nations and are bent\non waging war.\n\n\"While industry stakeholders, policymakers, individual experts, and their\ngovernments already warn that regulation is necessary, establishing a\nregulatory framework that can be globally acceptable is going to be\nchallenging,\" Garrett writes.\n\nChallenging barely describes it. Humanity's internecine squabbling makes it\nall even more unmanageable. Also, no matter how quickly we develop guidelines,\nASI might change even more quickly.\n\n\"Without practical regulation, there is every reason to believe that AI could\nrepresent a major threat to the future course of not only our technical\ncivilization but all technical civilizations,\" Garrett writes.\n\nIf we do, it might come down to what can seem boring and workaday: wrangling\nover legislation.\n\n\"The persistence of intelligent and conscious life in the universe could hinge\non the timely and effective implementation of such international regulatory\nmeasures and technological endeavors,\" Garrett writes.\n\nThis article was originally published by Universe Today. Read the original\narticle.\n\nTrending News\n\nFasting-Style Diet Seems to Result in Dynamic Changes to Human Brain Health2\ndays ago\n\nThe Polar Vortex Has Shifted Into Reverse \u2013 And Is Now Spinning Backwards\nEnvironment5 days ago\n\nScientists Accidentally Made a Mouse Grow Legs in Place of Genitals Health3\ndays ago\n\n", "frontpage": false}
