{"aid": "40137999", "title": "Adapting nnue-PyTorch's binary position format for Lichess", "url": "https://lichess.org/@/revoof/blog/adapting-nnue-pytorchs-binary-position-format-for-lichess/cpeeAMeY", "domain": "lichess.org", "votes": 1, "user": "jstrieb", "posted_at": "2024-04-23 22:15:00", "comments": 0, "source_title": "Adapting nnue-pytorch's binary position format for Lichess", "source_text": "revoof's Blog \u2022 Adapting nnue-pytorch's binary position format for Lichess \u2022\nlichess.org\n\nlichess.org\n\nDonate\n\nSign in\n\n# Adapting nnue-pytorch's binary position format for Lichess\n\nrevoofApr 19, 20249,387 viewsEnglish (US)\n\nLichessSoftware Development\n\nLichess stores lots of chess positions, and (thanks to Stockfish) now more\nefficiently\n\nGames on Lichess are stored as sequences of compressed moves. In some other\ncontexts its technically required or convenient to directly store positions.\nThus far we were using FEN\n\n6k1/7p/6r1/8/8/8/7P/5BRK b - - 0 1\n\nor a similar custom (but still human readable) format with reduced syntactic\nsugar:\n\n6k17p6r18887P5BRKb\n\nThe requirements for a successor format are:\n\n  * Stores the same information as FEN: A description of a chess position, including piece positions, turn, castling rights, en passant square, optional move counters, but ignoring the move history for three-fold repetitions\n  * Supports all of Lichess's variants\n  * Actually, just support illegal positions, too\n  * Easily reversible (unlike the hash keys of the opening explorer, variable length is acceptable)\n  * Reading/writing not slower than FEN\n\nNow to design a compact binary format for this ...\n\nTurns out that Stockfish developers also have to store lots of positions as\ntraining data for neural networks. Thanks to free open-source software we can\nhave a look and adapt it to our needs.\n\n## The format\n\nThe format is simple and good. The first 64 bits encode which squares are\noccupied. Often, most will be empty, so it's useful to get those out of the\nway first.\n\nThe occupancy mask is followed by a sequence of encoded pieces, in the order\nat which they are to be placed on each occupied square. Half a byte (a nibble)\nis more than enough to encode the 6 possible piece types of each color. By\nsticking to nibbles we can work byte by byte, encoding/decoding two pieces at\na time.\n\nThe 4 bits of a nibble would allow encoding 16 kinds of pieces, but we only\nhave 12. So there's space to add 4 more special piece types:\n\n  * A white rook that can still participate in castling\n  * A black rook that can still participate in castling\n  * A black king with the additional information that it's black to move\n  * A pawn that has just been moved two squares and can be captured en passant (we don't have space for a distinct piece type for each color, but the position of the pawn is enough to infer this)\n\nThat's the original format, and sufficient for every legal position.\n\nSome small tweaks are needed to adapt it to our requirements.\n\nIf required in the current context and not default, we tack on LEB128-encoded\nmove counters (half moves since the last pawn move or capture, and full moves\nplies since the start of the game). In most cases that will be 1 byte each.\n\nIf not standard, we also tack on variant data (variant identifier, Three-check\ncounters, Crazyhouse pockets and promoted piece mask).\n\nThat leaves one final problem: We are using the special black king to encode\nthe side to move, but Antichess positions, Atomic positions after nuclear\nArmageddon, or downright illegal positions may not have one. Note how earlier\nwe sacrificed a bit by appending a ply counter instead of a full move counter!\nSo all that's left to do, is making the move counters mandatory in this case.\n\n## Space savings\n\nAs of this week, we're using the format for cloud evaluation storage. The\naverage syntax-sugar-less FEN here was 42.1 characters long, and is now\nencoded in 18.7 bytes, for 56% savings. Viewed next to the other data stored\nin the cloud evaluation collection (based on MongoDB's avgObjSize) it looks\nless impressive, but it's a good way to test-drive the new format and\nultimately required to retire the previous one.\n\nThe average study chapter carries 10927 bytes of position data, for which a\n63% reduction down to 4054 bytes is expected. Here the savings are\nsignificant, even viewed next to the remaining data.\n\nThe document sizes here are before general purpose Zstandard compression\n(which is expected to reduce but not eliminate the impact).\n\n## Performance ballpark\n\nByte-aligned encoding/decoding leaves a few bits on the table here and there\n(not even beginning to consider entropy coding as used for our game\ncompression), but makes it easier to create a decently fast implementation.\n\nThanks to the recent work on the foundations of scalachess, it's also no\nlonger a performance compromise to implement formats like this directly in the\nlibrary.\n\nIn primitive benchmarks, reading and writing appear to be roughly 50x faster\nthan scalachess's FEN parser (certainly not slower, which was the goal).\n\nThe next step after carefully watching the cloud evaluation storage for a\nwhile will be to migrate the study chapter collection.\n\nDiscuss this blog post in the forum\n\n## View more blog posts by revoof\n\n", "frontpage": false}
