{"aid": "40221345", "title": "Invisible Stitch: Generating Smooth 3D Scenes with Depth Inpainting", "url": "https://research.paulengstler.com/invisible-stitch/", "domain": "paulengstler.com", "votes": 4, "user": "jasondavies", "posted_at": "2024-05-01 10:04:47", "comments": 0, "source_title": "Invisible Stitch: Generating Smooth 3D Scenes with Depth Inpainting", "source_text": "Invisible Stitch: Generating Smooth 3D Scenes with Depth Inpainting\n\n## Invisible Stitch: Generating Smooth 3D Scenes with Depth Inpainting\n\nPaul Engstler |\n\nAndrea Vedaldi |\n\nIro Laina |\n\nChristian Rupprecht\n\nVisual Geometry Group, University of Oxford\n\n@inproceedings{ engstler2024invisible, title={Invisible Stitch: Generating\nSmooth 3D Scenes with Depth Inpainting} author={Paul Engstler and Andrea\nVedaldi and Iro Laina and Christian Rupprecht} year={2024} booktitle={Arxiv} }\n\n## Building 3D Scenes With Depth Inpainting\n\nTo hallucinate scenes beyond known regions and lift images generated by\n2D-based models into three dimensions, current 3D scene generation methods\nrely on monocular depth estimation networks. For this task, it is crucial to\nseamlessly integrate the newly hallucinated regions into the existing scene\nrepresentation. Simple global scale-and-shift operations to the predicted\ndepth map, as used by previous methods, might lead to discontinuities between\nthe scene and its hallucinated extension. We introduce a depth completion\nnetwork that is able to smoothly extrapolate the existing scene depth based on\nan input image.\n\nView a larger version of this figure\n\nOverview of our 3D scene generation method. Starting from an input image I0,\nwe project it to a point cloud based on a depth map predicted by a depth\nestimation network g. To extend the scene, we render it from a new view point\nand query a generative model f to hallucinate beyond the scene's boundary.\nNow, we condition g on the depth of the existing scene and the image of the\nscene extended by f to produce a geometrically consistent depth map to project\nthe hallucinated points. This process may be repeated until a 360-degree scene\nhas been generated.\n\nThe depth completion network learns to inpaint masked depth map regions by\nbeing conditioned on an image and the depth of known regions. We use masks\nthat represent typical occlusion patterns generated by view point changes. To\nretain the model's ability to predict depth if no sparse depth is available,\nthe sparse depth input is occasionally dropped.\n\nView a larger version of this figure\n\nOverview of our training procedure. In this compact training scheme, a depth\ncompletion network g is learned by jointly training depth inpainting as well\nas depth prediction without a sparse depth input (the ratio being determined\nby the task probability p).A teacher network gT is utilized to generate a\npseudo ground-truth depth map D for a given image I. This depth map is then\nmasked with a random mask M, to obtain a sparse depth input D~.\n\n## 360-Degree Scene Results\n\n\"a view of Zion National Park\"\n\n## Evaluating Scene Geometry\n\nWithin the fully generative task of scene generation, evaluating the geometric\nproperties of generated scenes is difficult due to the lack of ground-truth\ndata. Most existing work resorts to image-text similarity scores, which only\nmeasures the global semantic alignment of the generation with a text\ndescription. To evaluate the geometric consistency and quality of the depth\npredictions used to build the scene, we propose a new evaluation benchmark.\nThis benchmark quantifies the depth-reconstruction quality on a partial scene\nwith known ground truth depth.\n\nView a larger version of this figure\n\nOverview of our scene consistency evaluation approach. Assume a scene is\ndescribed by a set of views {v1,v2,...} with associated images, depth maps,\nand camera poses, where the overlap of two views is described by a function\n\u03c6(vi,vj). For a given view pair (vi,vj) with \u03c6(vi,vj)\u2265\u03c4, we generate a\nrepresentation, e.g., a point cloud, from the ground-truth (GT) data for vi.\nThen, we render the representation from the view point of vj. We feed the\ncorresponding ground-truth image and the representation's depth into the model\nunder consideration to extrapolate the missing depth. Finally, we calculate\nthe mean absolute error between the result and the ground-truth depth for vj,\nonly considering those regions that were extrapolated.\n\nIn both, a real-world and a photorealistic setting, our inpainting model\nproduces predictions that are more faithful to the ground-truth than prior\nmethods.\n\n## Abstract\n\n3D scene generation has quickly become a challenging new research direction,\nfueled by consistent improvements of 2D generative diffusion models. Most\nprior work in this area generates scenes by iteratively stitching newly\ngenerated frames with existing geometry. These works often depend on pre-\ntrained monocular depth estimators to lift the generated images into 3D,\nfusing them with the existing scene representation. These approaches are then\noften evaluated via a text metric, measuring the similarity between the\ngenerated images and a given text prompt. In this work, we make two\nfundamental contributions to the field of 3D scene generation. First, we note\nthat lifting images to 3D with a monocular depth estimation model is\nsuboptimal as it ignores the geometry of the existing scene. We thus introduce\na novel depth completion model, trained via teacher distillation and self-\ntraining to learn the 3D fusion process, resulting in improved geometric\ncoherence of the scene. Second, we introduce a new benchmarking scheme for\nscene generation methods that is based on ground truth geometry, and thus\nmeasures the quality of the structure of the scene.\n\n## Acknowledgements\n\nP. E., A. V., I. L., and C.R. are supported by ERC-UNION- CoG-101001212. P.E.\nis also supported by Meta Research. I.L. and C.R. also receive support from\nVisualAI EP/T028572/1.\n\n", "frontpage": true}
