{"aid": "40210739", "title": "GPT-2: 1.5B Release", "url": "https://openai.com/research/gpt-2-1-5b-release", "domain": "openai.com", "votes": 2, "user": "Anon84", "posted_at": "2024-04-30 13:25:26", "comments": 0, "source_title": "GPT-2: 1.5B release", "source_text": "GPT-2: 1.5B release\n\nSkip to main content\n\n# Mobile Navigation\n\nResearch\n\n# GPT-2: 1.5B release\n\nIllustration: Ben Barry\n\nAs the final model release of GPT-2\u2019s staged release, we\u2019re releasing the\nlargest version (1.5B parameters) of GPT-2 along with code and model weights\nto facilitate detection of outputs of GPT-2 models. While there have been\nlarger language models released since August, we\u2019ve continued with our\noriginal staged release plan in order to provide the community with a test\ncase of a full staged release process. We hope that this test case will be\nuseful to developers of future powerful models, and we\u2019re actively continuing\nthe conversation with the AI community on responsible publication.\n\nNovember 5, 2019\n\n### More resources\n\n  * Read paper\n  * GPT-2 model\n  * Detector model\n  * Model card\n\nLanguage, Responsible AI, Community, GPT-2, Publication, Release\n\nWhile there have been larger language models released since August, we\u2019ve\ncontinued with our original staged release plan in order to provide the\ncommunity with a test case of a full staged release process. We hope that this\ntest case will be useful to developers of future powerful models, and we\u2019re\nactively continuing the conversation with the AI community on responsible\npublication.\n\n## Our findings\n\n1\\. Humans find GPT-2 outputs convincing. Our partners at Cornell University\nsurveyed people to assign GPT-2 text a credibility score across model sizes.\nPeople gave the 1.5B model a \u201ccredibility score\u201d of 6.91 out of 10. This is\nmarginally greater than outputs from the 774M model (6.72) and significantly\nabove the medium 355M model (6.07). These results make us more inclined to\nrelease the 1.5B model, as the incremental increase in human-perceived\ncredibility relative to 774M seems low.\n\n2\\. GPT-2 can be fine-tuned for misuse. Our partners at the Middlebury\nInstitute of International Studies\u2019 Center on Terrorism, Extremism, and\nCounterterrorism (CTEC) found that extremist groups can use GPT-2 for misuse,\nspecifically by fine-tuning GPT-2 models on four ideological positions: white\nsupremacy, Marxism, jihadist Islamism, and anarchism. CTEC demonstrated that\nit\u2019s possible to create models that can generate synthetic propaganda for\nthese ideologies. They also show that, despite having low detection accuracy\non synthetic outputs, ML-based detection methods can give experts reasonable\nsuspicion that an actor is generating synthetic text.\n\n3\\. Detection is challenging. We expect that content-based detection of\nsynthetic text is a long-term challenge. To test whether machine learning\napproaches may help today, we conducted in-house detection research and\ndeveloped a detection model that has detection rates of ~95% for detecting\n1.5B GPT-2-generated text.^A[A]\n\nSpecifically, we based a sequence classifier on RoBERTa_BASE (125 million\nparameters) and RoBERTa_LARGE (355 million parameters) and fine-tuned it to\nclassify the outputs from the 1.5B GPT-2 model versus WebText, the dataset we\nused to train the GPT-2 model.\n\nWe believe this is not high enough accuracy for standalone detection and needs\nto be paired with metadata-based approaches, human judgment, and public\neducation to be more effective. We are releasing this model to aid the study\nof research into the detection of synthetic text, although this does let\nadversaries with access better evade detection.\n\nWhile we found detection accuracy depends heavily on the sampling methods used\nin training and testing, we also found detection to be more reliable when\ntraining across a range of sampling techniques. As seen in the figure below,\nwe observed that larger models\u2019 outputs are more difficult to classify, but\ntraining on larger models\u2019 outputs makes detection results more accurate and\nrobust. We expect this trend to continue and that detection will be more\nchallenging with increased model size.\n\n# Transferred model accuracy (nucleus samples)\n\nTrained on| Tested on Small (124M)| Medium (355M)| Large (774M)| XL (1.5B)  \n---|---|---|---|---  \nSmall (124M)| 99.3%| 96.6%| 90.9%| 79.3%  \nMedium (355M)| 99.0%| 98.5%| 96.9%| 91.8%  \nLarge (774M)| 98.4%| 97.9%| 97.9%| 95.7%  \nXL (1.5B)| 96.9%| 96.7%| 96.6%| 96.0%  \n  \n4\\. We\u2019ve seen no strong evidence of misuse so far. While we\u2019ve seen some\ndiscussion around GPT-2\u2019s potential to augment high-volume/low-yield\noperations like spam and phishing, we haven\u2019t seen evidence of writing code,\ndocumentation, or instances of misuse. We think synthetic text generators have\na higher chance of being misused if their outputs become more reliable and\ncoherent. We acknowledge that we cannot be aware of all threats, and that\nmotivated actors can replicate language models without model release.\n\n5\\. We need standards for studying bias. Language models have biases. Working\nout how to study these biases, discuss them, and address them, is a challenge\nfor the AI research community. We\u2019ve approached the challenge of bias in two\nways:\n\n  * Publishing a model card^B[B]\n\nWhich we\u2019ve based on \u201cModel Cards for Model Reporting\u201d by Mitchell et al.\n\nalongside our models on GitHub to give people a sense of the issues inherent\nto language models such as GPT-2.\n\n  * Performing a qualitative, in-house evaluation of some of the biases in GPT-2: We probed GPT-2 for some gender, race, and religious biases, using those findings to inform our model card. These probes are not comprehensive and raise the need for collaboration on bias analysis frameworks.\n\n## Next steps\n\nOur experience with GPT-2 over the past 9 months has given us valuable insight\ninto the challenges and opportunities for creating responsible publication\nnorms in AI. We\u2019re continuing our work on this issue via participation in the\nPartnership on AI\u2019s \u201cResponsible Publication Norms for Machine Learning\u201d\nproject and discussions with our colleagues in the research community.\n\nIf you\u2019d like to develop large-scale AI systems and think about their\nimplications, we\u2019re hiring.\n\n### Footnotes\n\n  1. Specifically, we based a sequence classifier on RoBERTa_BASE (125 million parameters) and RoBERTa_LARGE (355 million parameters) and fine-tuned it to classify the outputs from the 1.5B GPT-2 model versus WebText, the dataset we used to train the GPT-2 model.\u21a9\ufe0e\n\n  2. Which we\u2019ve based on \u201cModel Cards for Model Reporting\u201d by Mitchell et al.\u21a9\ufe0e\n\n## Related research\n\nView all research\n\n  * ### Practices for Governing Agentic AI Systems\n\nDec 14, 2023December 14, 2023\n\n  * ### Confidence-Building Measures for Artificial Intelligence: Workshop proceedings\n\nAug 1, 2023August 1, 2023\n\n  * ### Frontier AI regulation: Managing emerging risks to public safety\n\nJul 6, 2023July 6, 2023\n\n  * ### Language models can explain neurons in language models\n\nMay 9, 2023May 9, 2023\n\n#### Research\n\n  * Overview\n  * Index\n  * GPT-4\n  * DALL\u00b7E 3\n  * Sora\n\n#### API\n\n  * Overview\n  * Pricing\n  * Docs\n\n#### ChatGPT\n\n  * Overview\n  * Team\n  * Enterprise\n  * Pricing\n  * Try ChatGPT\n\n#### Company\n\n  * About\n  * Blog\n  * Careers\n  * Charter\n  * Security\n  * Customer stories\n  * Safety\n\nOpenAI \u00a9 2015 \u2013 2024Terms & policiesPrivacy policyBrand guidelines\n\n#### Social\n\n  * Twitter\n  * YouTube\n  * GitHub\n  * SoundCloud\n  * LinkedIn\n\n", "frontpage": false}
