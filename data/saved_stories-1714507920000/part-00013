{"aid": "40209964", "title": "10M records per second, on-premises to the cloud", "url": "https://blog.spectralcore.com/redefining-the-data-migration-performance/", "domain": "spectralcore.com", "votes": 2, "user": "metadata", "posted_at": "2024-04-30 11:49:53", "comments": 0, "source_title": "10 million records per second, on-premises to the cloud", "source_text": "10 million records per second, on-premises to the cloud\n\n# 10 million records per second, on-premises to the cloud\n\n#### Damir Bulic\n\nApr 30, 2024 \u2022 3 min read\n\nBy now, it is well known that Spectral Core database migration tooling is the\nmost performant on the market, by some margin. Benchmarks would be good to\nback up such bold claims, but benchmarks are tricky and take time so let's use\nthe next best thing - some screenshots.\n\nWhen migrating data from on-premises servers to the cloud, we are typically\ntalking about a lot of data. Tens on terabytes are not uncommon at all, nor\nare thousands of tables in a database.\n\nOmni Loader is designed to remove as much complexity as possible from the\nmigration process. It creates the target schema and loads all the data with no\nneed of option tweaking - even for the largest and most complex projects.\n\nWe fight WAN latency with high parallelism. Multiple tables are loaded in\nparallel - even any table at a time is dynamically sliced into many parts\ncopied in parallel. As disks are much slower than CPU and RAM, we completely\navoid spilling out to disks during the data transformation. Data is processed\ncompletely in-memory.\n\nTo ingest data into warehousing solutions such as Microsoft Fabric, Google\nBigQuery, Snowflake, and others, one needs to prepare data for efficient\ningestion. Parquet is a very good choice for intermediate data format because\nit is columnar and highly compressed. We choose our slice sizes heuristically\nin such a way that data warehouse can quickly ingest that data. As data is\ncompressed on-premises (Omni Loader can run in a walled-garden), only the\ncompressed data travels to the cloud. With columnar data format, compression\nis extremely efficient - typically you will end up with just 20% of the data\nsize. As data upload is a bottleneck (right after CPU), moving 5x less data is\na good thing.\n\nOmni Loader automatically uses all CPU cores. It is even able to scale\nhorizontally by forming a distributed migration cluster - in that case Omni\nLoader acts as an orchestrator to any number of external agent nodes. However,\neven in a single-node scenario, Omni Loader is extremely performant. The\nscreenshots below are all of the Omni Loader in a single-node (standalone)\nmode, running on a 16-core AMD machine (32 logical cores).\n\n10.2 million records per second, moving on-premises SQL Server to Azure Fabric\nLakehouse\n\nWhat we have above is a best-case scenario for a splashy title - a table with\njust five numerical columns, which means that each row contains just a bit of\ndata. However, that table does contain a billion records so we get to show it\ndoesn't slow us down a bit. We are able to move over 10 million records per\nsecond, going from on-prem SQL Server to the Fabric Lakehouse.\n\nNow, what if we have a table with more data?\n\n307 MB per second, moving on-premises SQL Server to Azure Fabric Lakehouse\n\nNow, \"just\" 2 million records per second are moved. However, take a look at\nthe data processed per second - 307 megabytes per second! That means we are\nmigrating over 25 terabytes of data per day.\n\nJust to show that this performance isn't only attainable with Fabric, here is\na screenshot for the BigQuery target.\n\n10.2 million records per second, moving on-premises SQL Server to Google\nBigQuery\n\nOf course, should you want to migrate data exclusively in the cloud, you would\nget far better performance due to easy availability of high-vcpu machines and\nincredible network throughput.\n\nOne more thing - you may notice that throughput numbers on the right side are\nlower than the ones listed in the active agent gadget (lower-right corner).\nSidebar actually displays averages from beginning (including time to establish\nconnections, create the tables etc.) while agent shows the current stats. We\nmay want to unify that and display trailing 10 seconds in the future.\n\nOmni Loader is used in production and is available today. More info is\navailable at omniloader.com.\n\n## De-branding!\n\nSo... After a lot of preparation and with great expectations, Spectral Core\nrebranded to EQUEL last year. Fantastic name and fantastic domain we have used\non and off for some 10 years now. What could go wrong? It turns out - several\nthings. The most important realization is that I\n\nApr 26, 2024 1 min read\n\n## Introducing Hummingbird SQL parser\n\nSQL is notoriously difficult to parse. There is a standard, ANSI SQL - you\nshell out over $2000 to get your hands on the specs but get little value for\nall that money. Each database engine rolls out own extensions to the standard,\nand no database implements the whole standard\n\nJul 6, 2023 3 min read\n\n## Spectral Core becomes Equel!\n\nWe are growing! Our press release follows: Spectral Core Unveils the\nRebranding and Introduces Ultra-performing Products for Data Engineers Pula,\nCroatia \u2013 Spectral Core, a leading, fully independent provider of unique\ndatabase tooling, is thrilled to announce a significant milestone with a\ncomprehensive rebranding effort and a new product launch. Equel,\n\nJul 4, 2023 2 min read\n\nSpectral Core Blog \u00a9 2024\n\nPowered by Ghost\n\n", "frontpage": false}
