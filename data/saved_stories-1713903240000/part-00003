{"aid": "40131268", "title": "What p-values are, what they are not, and how to sell books with bad statistics", "url": "https://briefer.cloud/blog/posts/p-values/", "domain": "briefer.cloud", "votes": 1, "user": "lucasfcosta", "posted_at": "2024-04-23 12:40:14", "comments": 0, "source_title": "Briefer", "source_text": "Blog | Briefer\n\nBriefer\n\nBriefer\n\nAll posts\n\n# What p-values are, what they are not, and how to sell books with bad\nstatistics\n\n9 minsApr 22, 2024Lucas da Costa\n\nIn this blog post, I'll teach you how to sell books by abusing statistics.\nAlong the way, you will learn what p-values are, what they are not, and why\nyou should care about them.\n\n## Step 1: Understanding p-values so you can misuse them\n\nYour first step to riches is to create a technique that you can claim to\nimprove any aspect of your customers' lives and that you can write books\nabout.\n\nIt doesn't matter if your technique actually works. What matters is that you\ncan convince people that it does, and that's what I'm here to teach you.\n\nLet's say you've created a new study technique and you believe it helps\nstudents score better on exams, for example.\n\nTo test your hypothesis, you set up an experiment with two groups of students:\n\n  * Control Group: This group studies using traditional methods.\n  * Experimental Group: This group uses your revolutionary study technique.\n\nThen, you give both groups the same exam and compare their scores. If the\nexperimental group scores significantly higher, some people might conclude\nthat your study technique is effective.\n\nNow, that's where amateurs usually stop. But you're not an amateur, you're a\nbusiness person. You need to convince even the nerdiest of your customers that\nyour technique is backed by science.\n\nThese people like to think they are smart, so they'll definitely ask how do\nyou know whether the difference in scores is real or just due to chance.\n\nThat's where p-values come in. The p-value is a statistical measure that\ndetermines how likely it is that the improvements in scores were due to random\nchance. In other words, p-values represent the chance of the same or more\nextreme scores happening even if your new study technique doesn't actually\nwork.\n\nA small p-value means that a difference in scores as extreme as the one you\nobserved is unlikely to have occurred by mere luck. Thus, a small p-value\nsuggests that your study technique might be effective.\n\nFor example, a p-value of 0.001 means there's only a 0.1% chance that the\nscore improvement was just random. This suggests that the better scores could\nbe due to your new study technique, not just chance.\n\nOn the other hand, a large p-value means the difference in scores could easily\nhave happened by chance, and your study technique might not be effective.\n\nFor example, a p-value of 0.15 means there's a 15% chance that the score\nimprovement was random. Having such extreme results 15% of times would lead\nnerds to say that your results are not \"statistically significant\", because\nthese events are actually not that rare. In other words, they mean that the\ndifference in scores could easily have happened by chance, and your study\ntechnique might not be effective.\n\n> The way that classical statistics works is by comparing study data to what\n> is expected when there is nothing. If the data are not typical of what is\n> seen when there is nothing, there must be something!\n>\n> \u2014 Dallal, Gerard. The Little Handbook of Statistical Practice.\n\nHaving learned what p-values are, you probably figured that what we're after\nis a small p-value. So let's go ahead and make sure we get one.\n\n## Step 2: Ensuring that p is small enough\n\nLet's be honest: the chances of your revolutionary study technique actually\nworking are slim. Consequently, your experiment will probably fail to\ndemonstrate that the new study technique is much different from just doing\nwhatever everyone is already doing.\n\nOnce your study fails, you have two options. The first is to repeat the\nexperiment until it works. The second is to analyze the data in multiple ways\nuntil you find another measure that looks significant. Then you can rebrand\nyour technique as being effective for that measure, like \"improving memory\nretention\" or \"increasing focus\".\n\n  * Pro-tip 1: if none of the measures look significant, you can always try to find a subgroup of the population that benefits from your technique. For example, \"our revolutionary study technique is particularly effective for left-handed students born in the summer\".\n  * Pro-tip 2: you can also find transformations of the data that make it look significant, like the sum of outcomes squared or the log of the difference between scores.\n\nThis step is pretty easy to memorize because both alternatives rely on the\nfact that if you look around enough, you'll eventually find something that\nlooks like a significant result. Another way to put it is that \"the chances of\nwinning the lottery are small, yet often there's a winner\".\n\n### A crucial detail: make sure p is smaller than 0.05\n\nThere's a convention among scientists that says that a p-value below 0.05 is\nsaid to be \"statistically significant\". So that's what you should aim for, at\nleast most of the time.\n\nThere are many theories about why statistical significance is set at 0.05,\nbut, as Gerard Dallal points in his book \"The Little Handbook of Statistical\nPractice\", all of them trace back to Karl Pearson's 1914 book \"Tables for\nStatisticians and Biometricians\".\n\nPearson figured out how likely different outcomes were in statistics. Then\nFisher made it easier by creating simple tables in his book \"Statistical\nMethods for Research Workers\" in 1925. These tables just showed basic\nprobabilities for certain outcomes. Fisher's idea caught on, and he did the\nsame thing with Frank Yates in 1938. People still use Fisher's tables in\nstatistics texts today.\n\nFisher's tables were shorter and simpler than Pearson's. Instead of giving\nlots of details, they just gave rough probabilities. Fisher also came up with\na neat way to show how sure we are about results\u2014using stars. One star meant a\nresult observed about 5% of the time, two stars meant it was observed about 1%\nof the time, and sometimes three stars meant it was observed about 0.1% of the\ntime.\n\nFisher also said that when these extreme results would only happen 5% of the\ntime without an intervention, we should take notice. He thought this level was\ngood because it helped us find real results without getting too many false\nalarms. And this 5% rule has stuck around, shaping how we understand data and\nwhat we consider important in science.\n\nIn one sense, 5% is just a number, and it's not a magic number. But it's a\ngood rule of thumb. Still, some people think it's too high, and that we should\nuse lower levels like 1% or 0.1%.\n\nTo make matters simpler, I'd also recommend that you keep this excellent table\nfrom the scientific website xkcd handy:\n\n## Step 3: Publishing your results\n\nNow that you have a small p-value, you need to call journalists and tell them\nabout your revolutionary study technique. They'll be thrilled to write about\nit, especially if you can provide them with a scientific paper that includes\nthe words \"p-value\" and \"statistically significant\".\n\nThe important thing here is not to mention that you've run your experiment 100\ntimes until you got a significant result, nor that you've written a python\nscript to test all possible variables until you found one that looks\nsignificant.\n\nAnother important thing is to explain p-values to them in a way that doesn't\nsound egregious, but that is not entirely right either.\n\nOne incorrect way to explain p-values is to say that they represent the\nprobability that the \"null hypothesis\" (your study technique not working) is\ntrue.\n\nLet's say you've found a p-value of 0.001, for example. In that case, make\nsure to tell them that there's only a 0.1% chance that your study technique is\nineffective. Although that's incorrect, it's much more convincing than saying\n\"if the new study technique didn't work, we would still see the results we did\napproximately 0.1% of the time\".\n\nAnother technique that you can use is to invert the logic. Instead of saying\nthat a p-value of 0.001 means there's a 0.1% chance that the score improvement\nwas random, they say that there's a 99.9% chance that the new study technique\nis effective. That's not correct either, but it's looks better in a headline.\n\n## Step 4: Rebuking the skeptics\n\nOnce your revolutionary study technique is published, you'll probably face\nsome skepticism. That's normal, and you should be prepared to deal with it.\n\nThe scientists are the most dangerous skeptics. They'll probably try to\nreplicate your experiment and fail to find the same results.\n\nIf it's only one or two scientists, you can always say that \"failing to find\nan effect is different from showing that there is no effect\". They won't be\nable to argue with that because you're technically correct.\n\nNow, if quite a few scientists fail to replicate your results, then you're in\ndeep trouble. That's why it's best to design experiments that are hard to\nreplicate, like those that require a lot of data or that are too vague to be\ndisproven.\n\nIn general, keep in mind that the best way to deal with skepticism is to\nprevent it from happening in the first place.\n\n## Putting it all together\n\n  * P-values are a measure of how likely it is that you'd observe the results you did if your intervention didn't actually work.\n  * The smaller your p-value, the less likely you are to have observed the results you did by mere luck.\n  * Conversely, the larger your p-value, the more likely it is that you'd observe the results you did by mere luck.\n  * A p-value is not the probability that the null hypothesis is true. The opposite is also true: a p-value is not the probability that your intervention is effective.\n  * Just because you haven't observed an effect, it doesn't mean there isn't one.\n  * If you run the same experiment many times, you'll eventually see a p-value below 0.05, even if the null hypothesis is true.\n  * Looking for an effect until you find one is an effective way to make sure you get a small p-value, but it's bad practice unless you're looking for an easy way to sell books.\n  * A p-value below 0.05 is usually considered \"statistically significant\". Although it's not a magic number, it's a rule of thumb that has been around for a while.\n\n## Before we go\n\nIn case you didn't notice, this post is a satire, and any parallels to the\nreal world are, obviously, coincidence (no pun intended).\n\nThe post was inspired by the excellent work of Randall Munroe, in particular\nhis comic xkcd 882:\n\nFinally, if you're a data-scientist who loves jupyter notebooks or data apps,\nbut finds them hard to share your work with others or too much hassle to\ndeploy, you should check out Briefer.\n\n", "frontpage": false}
