{"aid": "40133555", "title": "Cybertruck recall shows how Tesla is faring in electric truck race", "url": "https://www.usatoday.com/story/money/cars/2024/04/22/tesla-cybertruck-recall-electric-truck-sales/73416419007/", "domain": "usatoday.com", "votes": 1, "user": "segasaturn", "posted_at": "2024-04-23 16:06:51", "comments": 0, "source_title": "4,000 Cybertrucks sold: Recall offers glimpse at Tesla's rank in rocky electric truck market", "source_text": "Cybertruck recall shows how Tesla is faring in electric truck race\n\nSkip to main content\n\n  * Home\n  * Personal Finance\n  * Cars\n  * Retirement\n  * Investing\n  * Careers\n  * Consumer Recalls\n  * Lottery\n  * Tax Season\n  * Coupons\n  * Coupons\n\n4,000 Cybertrucks sold: Recall offers glimpse at Tesla's rank in rocky\nelectric truck market\n\n# 4,000 Cybertrucks sold: Recall offers glimpse at Tesla's rank in rocky\nelectric truck market\n\nJames Powel USA TODAY\n\nShow Caption\n\nHide Caption\n\nTesla Cybertruck stops working After car wash\n\nTesla Cybertruck owners have experienced breakdowns after washing their\nvehicles, sparking discussions about the truck's durability.\n\nunbranded - Newsworthy\n\nA recall from Tesla allowed a rare look under the hood of the electric vehicle\nbalance sheet.\n\nNearly 4,000 Cybertrucks were recalled Wednesday over a defect with the\naccelerator causing it to get stuck when pressed, increasing the risk of\ncrash.\n\nThis covers all the trucks Tesla has sold since the vehicle reached market in\nNovember.\n\nThis recall, unlike most that Tesla can fix with over-the-air updates,\nrequires owners to bring their vehicles in for a physical repair.\n\nThe company does not itemize the number of vehicles sold by model in its\nquarterly reports, meaning that the recall offered a rare glimpse into the\nsales volume of Tesla CEO Elon Musk's pet project.\n\nThe recall hit during a tumultuous week at the company that saw layoffs, price\ncutting and a fight over a potential $56 billion pay package for Musk.\n\nUSA TODAY reached out to Tesla on Monday about the recall and did not receive\na response.\n\n## How many Cybertrucks has Tesla sold?\n\nThe 3,878 Cybertrucks covered by the recall represent all of the vehicle\ndeliveries completed by the company since the rollout of the long-awaited\ntruck in late November.\n\nIn July 2023, an unofficial reservation tracker suggested that around 1.9\nmillion reservations were in place at the time, according to InsideEVs. The\nreservations cost $100 and are refundable until delivery if a customer changes\ntheir mind.\n\nThen in December, the company began inviting customers with a reservation to\nplace a $1,000 deposit for the truck.\n\n## How does the Cybertruck sales compare to other electric trucks?\n\nWhile the number of deliveries revealed by the recall may not be eye popping,\nbeing near the 4,000 vehicles sold benchmark puts Tesla as a strong competitor\nin the domestic electric truck market.\n\nEarlier this month, Ford reported that it had sold 7,743 of its F-150\nLightning electric trucks in the U.S. in the first quarter of 2024.\n\nGeneral Motors reported that it sold 1,688 Hummer EVs domestically.\n\nRivian, which sells a truck and an SUV, reported that it delivered 13,588\nvehicles, though it did not provide a breakdown by model. Motor1 reported that\nthe company had sold 2,399 of the R1T truck.\n\n## Cybertruck lynchpin in Musk's vision\n\nMusk has signaled that the electric truck will be a key component of the\ncompany's future.\n\n\"We're likely to do probably a quarter million a year, I think...maybe\nmore...again very much dependent on what the demand is like,\" Musk said at the\ncompany's 2023 annual shareholder meeting, later adding that the company could\ndeliver up to 500,000 trucks.\n\nElektrek reported that suppliers were asked to plan for an annual production\nvolume of at least 375,000 Cybertrucks.\n\nTF International Securities analyst Ming-Chi Kuo wrote on his Medium page in\nOctober that he believes the company could deliver between 240,000 and 260,000\nCybertrucks by the end of 2025.\n\n## Analysts see Cybertruck as 'another black eye'\n\nThe recall is the latest burner of brand equity for the company, analysts\nsaid, adding to a string of negative press.\n\n\"This is another black eye for Tesla, which has added to the chaos going on\nfor Musk,\u201d Dan Ives, senior equity analyst at Wedbush Securities, told The\nGuardian. \u201cCybertruck is the pedestal moment and a recall out of the gates is\na bad look.\u201d\n\nThe chatoic week that surrounded the recall seemed to crystalize the larger\nheadwinds that surround the company.\n\nSome of those headwinds are self-inflicted including: Musk's erratic behavior,\nhis management of the social media platform X, formerly known as Twitter and\nsafety questions and delays around the company's Full Self Driving technology.\n\nTesla's electric vehicle perch is also threatened by the proliferation of\ncompetition, especially from Chinese automakers, and a softer EV market\noverall.\n\nWhile the stumble of the Cybertruck has not helped the perception of the\ncompany, analyst concerns around the business stretch wider than the angular\nvehicle.\n\n\"While it deserves credit for becoming the charging network of choice by many\nEV makers in the U.S., its Energy Generation and Storage sales growth has\nstalled, while its gross margin for Services remains stuck in the low-single-\ndigits,\" Sean Williams wrote for the Motley Fool. \"Tesla's sales and\nprofitability are dependent on its ability to sell and lease EVs \u2013 and that's\na segment that's under serious pressure right now.\"\n\n## Tesla stock price\n\nTesla's stock closed Monday at $142.05, down 3.4%.\n\nThe company will share its first quarter earnings after market close on\nTuesday.\n\nSMS Facebook Twitter Email\n\nShare your feedback to help improve our site!\n\nShare your feedback to help improve our site!\n\n  * Help\n  * Terms of Service\n  * Subscription Terms & Conditions\n  * Privacy Policy\n  * Site Map\n  * Accessibility\n  * Our Ethical Principles\n  * Responsible Disclosure\n  * Your Privacy Choices\n\n\u00a9 Copyright Gannett 2024\n\n## About Your Privacy\n\nClicking on the \"Reject All\" Button retains the default setting of only\nstrictly necessary cookies.\n\nWe and our 71 partners store and/or access information on a device, such as\nunique IDs in cookies to process personal data. You may accept or manage your\nchoices by clicking below, including your right to object where legitimate\ninterest is used, or at any time in the privacy policy page. These choices\nwill be signaled to our partners and will not affect browsing data.For more\ninformation about how we use your data visit our Privacy Policy.\n\n### If you click \"Accept All\", we and our partners may process personal data\nfor the following purposes:\n\nUse precise geolocation data. Actively scan device characteristics for\nidentification. Store and/or access information on a device. Personalised\nadvertising and content, advertising and content measurement, audience\nresearch and services development.\n\n## About Your Privacy\n\nYour Opt Out Preference Signal is Honored\n\n  * ### Your Privacy\n\n  * ### Strictly Necessary\n\n  * ### Opt Out by Cookie Category\n\n  * ### Personalised advertising and content, advertising and content measurement, audience research and services development 64 partners can use this purpose\n\n  * ### Store and/or access information on a device 51 partners can use this purpose\n\n  * ### Use precise geolocation data 23 partners can use this purpose\n\n  * ### Actively scan device characteristics for identification 8 partners can use this purpose\n\n  * ### Ensure security, prevent and detect fraud, and fix errors 43 partners can use this purpose\n\n  * ### Deliver and present advertising and content 39 partners can use this purpose\n\n  * ### Match and combine data from other data sources 34 partners can use this purpose\n\n  * ### Link different devices 30 partners can use this purpose\n\n  * ### Identify devices based on information transmitted automatically 39 partners can use this purpose\n\n#### Your Privacy\n\nWe process your data to deliver content or advertisements and measure the\ndelivery of such content or advertisements to extract insights about our\nwebsite. We share this information with our partners on the basis of consent\nand legitimate interest. You may exercise your right to consent or object to a\nlegitimate interest, based on a specific purpose below or at a partner level\nin the link under each purpose. These choices will be signaled to our vendors\nparticipating in the Transparency and Consent Framework. More information\n\nUser ID: db345089-ae5f-4ae4-afdf-dfae0db99b5c\n\nThis User ID will be used as a unique identifier while storing and accessing\nyour preferences for future.\n\nTimestamp: --\n\n#### Strictly Necessary\n\nThese are necessary for the website or application to function and cannot be\nswitched off in our systems. They also can be set in response to actions made\nby you which amount to a request for services, such as setting your privacy\npreferences, or where applicable, logging in or filling in forms. You can set\nyour browser or mobile device to block these cookies, but some parts of the\nsite may not work properly in that case.\n\n#### Opt Out by Cookie Category\n\n  * #### Functional\n\nlabel\n\nThis enables the website or application to provide enhanced functionality and\npersonalization. They may be set by us or by third party providers whose\nservices we have added to our pages. If you do not allow then some or all of\nthese services may not function properly.\n\n  * #### Performance\n\nlabel\n\nThis allows us to count visits and traffic sources so we can measure and\nimprove the performance of our site. They help us to know which pages are the\nmost and least popular and see how visitors move around the site. All\ninformation collected is aggregated and therefore anonymous. If you do not\nallow this service we will not know when you have visited our site, and will\nnot be able to monitor its performance.\n\n  * #### Targeting\n\nlabel\n\nThese may be set through our site by our content and advertising\nproviders/business partners. They may be used alone and/or in combination with\nother information by those companies to build a profile of your interests and\nshow you relevant adverts on other sites. They do not store directly\npersonally identifiable information (such as your name), but are based on\nuniquely identifying your browser and internet device which may include\nassigning a unique identification code to you. If you do not allow, you will\nstill see advertisements but they will not be targeted to your likes and\ninterests. The chart below shows First Party Cookies placed on our sites to\nserve you targeted advertising and includes links with further information and\nopt out choices. To opt out of Third Party Targeting Cookies and other\ntargeting identifiers, please see \u201cOpting Out of Targeted Advertising\u201d. First\nParty Targeting Cookies on the Site may include:\n\n  * #### Social Media\n\nlabel\n\nThese are set by a range of social media services that we have added to the\nsite or application to enable you to share our content with your friends and\nnetworks. They are capable of tracking your browser or device across other\nsites and building up a profile of your interests. This may impact the content\nand messages you see on other websites you visitor applications. If you do not\nallow, you may not be able to use or see these sharing tools.\n\n#### Personalised advertising and content, advertising and content\nmeasurement, audience research and services development 64 partners can use\nthis purpose\n\n  * #### Use limited data to select advertising 43 partners can use this purpose\n\nAdvertising presented to you on this service can be based on limited data,\nsuch as the website or app you are using, your non-precise location, your\ndevice type or which content you are (or have been) interacting with (for\nexample, to limit the number of times an ad is presented to you).\n\n  * #### Create profiles for personalised advertising 36 partners can use this purpose\n\nInformation about your activity on this service (such as forms you submit,\ncontent you look at) can be stored and combined with other information about\nyou (for example, information from your previous activity on this service and\nother websites or apps) or similar users. This is then used to build or\nimprove a profile about you (that might include possible interests and\npersonal aspects). Your profile can be used (also later) to present\nadvertising that appears more relevant based on your possible interests by\nthis and other entities.\n\n  * #### Use profiles to select personalised advertising 35 partners can use this purpose\n\nAdvertising presented to you on this service can be based on your advertising\nprofiles, which can reflect your activity on this service or other websites or\napps (like the forms you submit, content you look at), possible interests and\npersonal aspects.\n\n  * #### Create profiles to personalise content 16 partners can use this purpose\n\nInformation about your activity on this service (for instance, forms you\nsubmit, non-advertising content you look at) can be stored and combined with\nother information about you (such as your previous activity on this service or\nother websites or apps) or similar users. This is then used to build or\nimprove a profile about you (which might for example include possible\ninterests and personal aspects). Your profile can be used (also later) to\npresent content that appears more relevant based on your possible interests,\nsuch as by adapting the order in which content is shown to you, so that it is\neven easier for you to find content that matches your interests.\n\n  * #### Use profiles to select personalised content 13 partners can use this purpose\n\nContent presented to you on this service can be based on your content\npersonalisation profiles, which can reflect your activity on this or other\nservices (for instance, the forms you submit, content you look at), possible\ninterests and personal aspects, such as by adapting the order in which content\nis shown to you, so that it is even easier for you to find (non-advertising)\ncontent that matches your interests.\n\n  * #### Measure advertising performance 58 partners can use this purpose\n\nInformation regarding which advertising is presented to you and how you\ninteract with it can be used to determine how well an advert has worked for\nyou or other users and whether the goals of the advertising were reached. For\ninstance, whether you saw an ad, whether you clicked on it, whether it led you\nto buy a product or visit a website, etc. This is very helpful to understand\nthe relevance of advertising campaigns.\n\n  * #### Measure content performance 27 partners can use this purpose\n\nInformation regarding which content is presented to you and how you interact\nwith it can be used to determine whether the (non-advertising) content e.g.\nreached its intended audience and matched your interests. For instance,\nwhether you read an article, watch a video, listen to a podcast or look at a\nproduct description, how long you spent on this service and the web pages you\nvisit etc. This is very helpful to understand the relevance of (non-\nadvertising) content that is shown to you.\n\n  * #### Understand audiences through statistics or combinations of data from different sources 34 partners can use this purpose\n\nReports can be generated based on the combination of data sets (like user\nprofiles, statistics, market research, analytics data) regarding your\ninteractions and those of other users with advertising or (non-advertising)\ncontent to identify common characteristics (for instance, to determine which\ntarget audiences are more receptive to an ad campaign or to certain contents).\n\n  * #### Develop and improve services 43 partners can use this purpose\n\nInformation about your activity on this service, such as your interaction with\nads or content, can be very helpful to improve products and services and to\nbuild new products and services based on user interactions, the type of\naudience, etc. This specific purpose does not include the development or\nimprovement of user profiles and identifiers.\n\n  * #### Use limited data to select content 15 partners can use this purpose\n\nContent presented to you on this service can be based on limited data, such as\nthe website or app you are using, your non-precise location, your device type,\nor which content you are (or have been) interacting with (for example, to\nlimit the number of times a video or an article is presented to you).\n\n#### Store and/or access information on a device 51 partners can use this\npurpose\n\nCookies, device or similar online identifiers (e.g. login-based identifiers,\nrandomly assigned identifiers, network based identifiers) together with other\ninformation (e.g. browser type and information, language, screen size,\nsupported technologies etc.) can be stored or read on your device to recognise\nit each time it connects to an app or to a website, for one or several of the\npurposes presented here.\n\n#### Use precise geolocation data 23 partners can use this purpose\n\nWith your acceptance, your precise location (within a radius of less than 500\nmetres) may be used in support of the purposes explained in this notice.\n\n#### Actively scan device characteristics for identification 8 partners can\nuse this purpose\n\nWith your acceptance, certain characteristics specific to your device might be\nrequested and used to distinguish it from other devices (such as the installed\nfonts or plugins, the resolution of your screen) in support of the purposes\nexplained in this notice.\n\n#### Ensure security, prevent and detect fraud, and fix errors 43 partners can\nuse this purpose\n\nYour data can be used to monitor for and prevent unusual and possibly\nfraudulent activity (for example, regarding advertising, ad clicks by bots),\nand ensure systems and processes work properly and securely. It can also be\nused to correct any problems you, the publisher or the advertiser may\nencounter in the delivery of content and ads and in your interaction with\nthem.\n\n#### Deliver and present advertising and content 39 partners can use this\npurpose\n\nCertain information (like an IP address or device capabilities) is used to\nensure the technical compatibility of the content or advertising, and to\nfacilitate the transmission of the content or ad to your device.\n\n#### Match and combine data from other data sources 34 partners can use this\npurpose\n\nInformation about your activity on this service may be matched and combined\nwith other information relating to you and originating from various sources\n(for instance your activity on a separate online service, your use of a\nloyalty card in-store, or your answers to a survey), in support of the\npurposes explained in this notice.\n\n#### Link different devices 30 partners can use this purpose\n\nIn support of the purposes explained in this notice, your device might be\nconsidered as likely linked to other devices that belong to you or your\nhousehold (for instance because you are logged in to the same service on both\nyour phone and your computer, or because you may use the same Internet\nconnection on both devices).\n\n#### Identify devices based on information transmitted automatically 39\npartners can use this purpose\n\nYour device might be distinguished from other devices based on information it\nautomatically sends when accessing the Internet (for instance, the IP address\nof your Internet connection or the type of browser you are using) in support\nof the purposes exposed in this notice.\n\n### Vendors List\n\nConsent Leg.Interest\n\nlabel\n\nlabel\n\nlabel\n\nlabel\n\n", "frontpage": false}
{"aid": "40133558", "title": "Stable Diffusion Benchmarks: 45 Nvidia, AMD, and Intel GPUs Compared (2023)", "url": "https://www.tomshardware.com/pc-components/gpus/stable-diffusion-benchmarks", "domain": "tomshardware.com", "votes": 2, "user": "pantalaimon", "posted_at": "2024-04-23 16:07:11", "comments": 0, "source_title": "Stable Diffusion Benchmarks: 45 Nvidia, AMD, and Intel GPUs Compared", "source_text": "Stable Diffusion Benchmarks: 45 Nvidia, AMD, and Intel GPUs Compared | Tom's Hardware\n\nSkip to main content\n\nWhen you purchase through links on our site, we may earn an affiliate\ncommission. Here\u2019s how it works.\n\n# Stable Diffusion Benchmarks: 45 Nvidia, AMD, and Intel GPUs Compared\n\nFeatures\n\nBy Jarred Walton\n\npublished December 15, 2023\n\nWhich graphics card offers the fastest AI performance?\n\n(Image credit: Tom's Hardware)\n\nJump to:\n\n  * Introduction\n  * 512x512 Benchmarks\n  * 768x768 Benchmarks\n  * Picking an SD Model\n  * Batch Sizes\n  * Test Setup\n  * Theoretical GPU Performance\n\n### Stable Diffusion Introduction\n\nStable Diffusion and other AI-based image generation tools like Dall-E and\nMidjourney are some of the most popular uses of deep learning right now. Using\ntrained networks to create images, videos, and text has become not just a\ntheoretical possibility but is now a reality. While more advanced tools like\nChatGPT can require large server installations with lots of hardware for\ntraining, running an already-trained network for inference can be done on your\nPC, using its graphics card. How fast are consumer GPUs for doing AI inference\nusing Stable Diffusion? That's what we're here to investigate.\n\nWe've benchmarked Stable Diffusion, a popular AI image generator, on the 45 of\nthe latest Nvidia, AMD, and Intel GPUs to see how they stack up. We've been\npoking at Stable Diffusion for over a year now, and while earlier iterations\nwere more difficult to get running \u2014 never mind running well \u2014 things have\nimproved substantially. Not all AI projects have received the same level of\neffort as Stable Diffusion, but this should at least provide a fairly\ninsightful look at what the various GPU architectures can manage with AI\nworkloads given proper tuning and effort.\n\nThe easiest way to get Stable Diffusion running is via the Automatic1111 webui\nproject. Except, that's not the full story. Getting things to run on Nvidia\nGPUs is as simple as downloading, extracting, and running the contents of a\nsingle Zip file. But there are still additional steps required to extract\nimproved performance, using the latest TensorRT extensions. Instructions are\nat that link, and we've previous tested Stable Diffusion TensorRT performance\nagainst the base model without tuning if you want to see how things have\nimproved over time. Now we're adding results from all the RTX GPUs, from the\nRTX 2060 all the way up to the RTX 4090, using the TensorRT optimizations.\n\nFor AMD and Intel GPUs, there are forks of the A1111 webui available that\nfocus on DirectML and OpenVINO, respectively. We used these webui OpenVINO\ninstructions to get Arc GPUs running, and these webui DirectML instructions\nfor AMD GPUs. Our understanding, incidentally, is that all three companies\nhave worked with the community in order to tune and improve performance and\nfeatures.\n\nWhether you're using an AMD, Intel, or Nvidia GPU, there will be a few hurdles\nto jump in order to get things running optimally. If you have issues with the\ninstructions in any of the linked repositories, drop us a note in the comments\nand we'll do our best to help out. Once you have the basic steps down,\nhowever, it's not too difficult to fire up the webui and start generating\nimages. Note that extra functionality (i.e. upscaling) is separate from the\nbase text to image code and would require additional modifications and tuning\nto extract better performance, so that wasn't part of our testing.\n\nAdditional details are lower down the page, for those that want them. But if\nyou're just here for the benchmarks, let's get started.\n\n### Stable Diffusion 512x512 Performance\n\n(Image credit: Tom's Hardware)\n\nThis shouldn't be a particularly shocking result. Nvidia has been pushing AI\ntechnology via Tensor cores since the Volta V100 back in late 2017. The RTX\nseries added the feature in 2018, with refinements and performance\nimprovements each generation (see below for more details on the theoretical\nperformance). With the latest tuning in place, the RTX 4090 ripped through\n512x512 Stable Diffusion image generation at a rate of more than one image per\nsecond \u2014 75 per minute.\n\nAMD's fastest GPU, the RX 7900 XTX, only managed about a third of that\nperformance level with 26 images per minute. Even more alarming, perhaps, is\nhow poorly the RX 6000-series GPUs performed. The RX 6950 XT output 6.6 images\nper minute, well behind even the RX 7600. Clearly, AMD's AI Matrix\naccelerators in RDNA 3 have helped improve throughput in this particular\nworkload.\n\nIntel's current fastest GPU, the Arc A770 16GB, managed 15.4 images per\nminute. Keep in mind that the hardware has theoretical performance that's\nquite a bit higher than the RTX 2080 Ti (if we're looking at XMX FP16\nthroughput compared to Tensor FP16 throughput): 157.3 TFLOPS versus 107.6\nTFLOPS. It looks like the Arc GPUs are thus only managing less than half of\ntheir theoretical performance, which is why benchmarks are the most important\ngauge of real-world performance.\n\nWhile there are differences between the various GPUs and architecture,\nperformance largely scales proportionally with theoretical compute. The RTX\n4090 was 46% faster than the RTX 4080 in our testing, while in theory it\noffers 69% more compute performance. Likewise, the 4080 beat the 4070 Ti by\n24%, and it has 22% more compute.\n\nThe newer architectures aren't necessarily performing substantially faster.\nThe 4080 beat the 3090 Ti by 10%, while offering potentially 20% more compute.\nBut the 3090 Ti also has more raw memory bandwidth (1008 GB/s compared to the\n4080's 717 GB/s), and that's certainly a factor. The old Turing generation\nheld up as well, with the newer RTX 4070 beating the RTX 2080 Ti by just 12%,\nwith theoretically 8% more compute.\n\n### Stable Diffusion 768x768 Performance\n\n(Image credit: Tom's Hardware)\n\nKicking the resolution up to 768x768, Stable Diffusion likes to have quite a\nbit more VRAM in order to run well. Memory bandwidth also becomes more\nimportant, at least at the lower end of the spectrum.\n\nThe relative positioning of the various Nvidia GPUs doesn't shift too much,\nand AMD's RX 7000-series gains some ground with the RX 7800 XT and above,\nwhile the RX 7600 dropped a bit. The 7600 was 36% slower than the 7700 XT at\n512x512, but dropped to being 44% slower at 768x768.\n\nThe previous generation AMD GPUs had an even tougher time. The RX 6950 XT\ndidn't even manage two images per minute, and the 8GB RX 6650 XT, 6600 XT, and\n6600 all failed to render even a single image. That's a bit odd, as the RX\n7600 still worked okay with only 8GB of memory, but some other architectural\ndifference was at play.\n\nIntel's Arc GPUs also lost ground at the higher resolution, or if you prefer,\nthe Nvidia GPUs \u2014 particularly the fastest models \u2014 put some additional\ndistance between themselves and the competition. The 4090 for example was 4.9X\nfaster than the Arc A770 16GB at 512x512 images, and that increased to a 6.4X\nlead with 768x768 images.\n\nWe haven't tested SDXL, yet, mostly because the memory demands and getting it\nrunning properly tend to be even higher than 768x768 image generation.\nTensorRT support is also missing for Nvidia GPUs, and most likely we'd see\nquite a few GPUs struggle with SDXL. It's something we plan to investigate in\nthe future, however, as the results are generally preferable to SD1.5 and\nSD2.1 for higher resolution outputs.\n\nFor now, we know that performance will be lower than our 768x768 results. As\nan example of what to expect, the RTX 4090 doing 1024x1024 images (still using\nSD1.5), managed just 13.4 images per minute. That's less than half the speed\nof 768x768 image generation, which makes sense as the 1024x1024 images have\n78% more pixels and the time required seems to scale somewhat faster than the\nresolution increase.\n\n### Picking a Stable Diffusion Model\n\nImage 1 of 3\n\nDirectly trying for 1920x1080 generation(Image credit: Tom's Hardware)\n\nAnother attempt at 1920x1080 generation(Image credit: Tom's Hardware)\n\nUpscaling via SwinIR_4x from 768x768 to 1920x1080(Image credit: Tom's\nHardware)\n\nDeciding which version of Stable Generation to run is a factor in testing.\nCurrently, you can find v1.4, v1.5, v2.0, and v2.1 models from Hugging Face,\nalong with the newer SDXL. The earlier 1.x versions were mostly trained on\n512x512 images, while 2.x included more training data for up to 768x768\nimages. SDXL targets 768x768 to 1024x1024 images. As noted above, higher\nresolutions also require more VRAM. Different versions of Stable Diffusion can\nalso generate radically different results from the same prompt, due to\ndifferences in the training data.\n\nIf you try to generate a higher resolution image than the training data, you\ncan end up with \"fun\" results like the multi-headed, multi-limbed, multi-eyed,\nor multi-whatever examples shown above. You can try to work around these via\nvarious upscaling tools, but if you're thinking about just generating a bunch\nof 4K images to use as your Windows desktop wallpaper, be aware that it's not\nas straightforward as you'd probably want it to be. (Our prompt for the above\nwas \"Keanu Reeves portrait photo of old warrior chief, tribal panther make up,\nblue on red, side profile, looking away, serious eyes, 50mm portrait\nphotography, hard rim lighting photography\" \u2014 taken from this page if you're\nwondering.)\n\nIt's also important to note that not every GPU has received equal treatment\nfrom the various projects, but the core architectures are also a big factor.\nNvidia has had Tensor cores in all of its RTX GPUs, and our understanding is\nthat the current TensorRT code only uses FP16 calculations, without sparsity.\nThat explains why the scaling from 20-series to 30-series to 40-series GPUs\n(Turing, Ampere, and Ada Lovelace architectures) mostly correlates with the\nbaseline Tensor FP16 rates.\n\nAs shown above, performance on AMD GPUs using the latest webui software has\nimproved throughput quite a bit on RX 7000-series GPUs, while for RX\n6000-series GPUs you may have better luck with using Nod.ai's Shark version \u2014\nand note that AMD has recently acquired Nod.ai. Throughput with SD2.1 in\nparticular was faster with the RDNA 2 GPUs, but then the results were also\ndifferent from SD1.5 and thus can't be directly compared. Nod.ai doesn't have\n\"sharkify\" tuning if you use SD1.5 models either, which resulted in lower\nperformance with our apples to apples testing.\n\nLATEST VIDEOS FROM tomshardware\n\n### Test Setup: Batch Sizes\n\nImage 1 of 14\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\nThe above gallery shows some additional Stable Diffusion sample images, after\ngenerating them at a resolution of 768x768 and then using SwinIR_4X upscaling\n(under the \"Extras\" tab), followed by cropping and resizing. Hopefully we can\nall agree that these results look a lot better than the mangled Keanu Reeves\nattempts from above.\n\nFor testing, we followed the same procedures for all GPUs. We generated a\ntotal of 24 distinct 512x512 and 24 distinct 768x768 images, using the same\nprompt of \"messy room\" \u2014 short, sweet, and to the point. Doing 24 images per\nrun gave us plenty of flexibility, since we could do batches of 3x8 (three\nbatches of eight concurrent images), 4x6, 6x4, 8x3, 12x2, or 24x1, depending\non the GPU.\n\nWe did our best to optimize for throughput, which means running batch sizes\nlarger than one in many cases. Sometimes, the limiting factor in how many\nimages should be generated concurrently is VRAM capacity, but compute (and\ncache) also appear to factor in. As an example, the RTX 4060 Ti 16GB did best\nwith 6x4 batches, just like the 8GB model, while the 4070 did best with 4x6\nbatches.\n\nFor 512x512 image generation, many of Nvidia's GPUs did best generating three\nbatches of eight images each (the maximum batch size is eight), though we did\nfind that 4x6 or 6x4 worked slightly better on some of the GPUs. AMD's RX\n7000-series GPUs all liked 3x8 batches, while the RX 6000-series did best with\n6x4 on Navi 21, 8x3 on Navi 22, and 12x2 on Navi 23. Intel's Arc GPUs all\nworked well doing 6x4, except the A380 which used 12x2.\n\nFor 768x768 images, memory and compute requirements are much higher. Most of\nthe Nvidia RTX GPUs worked best with 6x4 batches, or 8x3 in a few instances.\n(Note that even the RTX 2060 with 6GB of VRAM was still best with 6x4\nbatches.) AMD's RX 7000-series again liked 3x8 for most of the GPUs, though\nthe RX 7600 needed to drop the batch size and ran 6x4. The RX 6000-series only\nworked at 24x1, doing single images at a time (otherwise we'd get garbled\noutput), and the 8GB RX 66xx cards all failed to render anything at the higher\ntarget output \u2014 you'd need to opt for Nod.ai and a different model on those\nGPUs.\n\n### Test Setup\n\nImage 1 of 3\n\n\"Messy Room\" on AMD GPU(Image credit: Tom's Hardware)\n\n\"Messy Room\" on Intel GPU(Image credit: Tom's Hardware)\n\n\"Messy Room\" on Nvidia GPU(Image credit: Tom's Hardware)\n\nStable Diffusion Testbed\n\nIntel Core i9-12900K MSI Pro Z690-A WiFi DDR4 Corsair 2x16GB DDR4-3600 CL16\nCrucial P5 Plus 2TB Cooler Master MWE 1250 V2 Gold Cooler Master PL360 Flux\nCooler Master HAF500 Windows 11 Pro 64-bit (22H2)\n\nOur test PC for Stable Diffusion consisted of a Core i9-12900K, 32GB of\nDDR4-3600 memory, and a 2TB SSD. We tested 45 different GPUs in total \u2014\neverything that has ray tracing hardware, basically, which also tended to\nimply sufficient performance to handle Stable Diffusion. It's possible to use\neven older GPUs, though performance can drop quite a bit if the GPU doesn't\nhave native FP16 support. Nvidia's GTX class cards were very slow in our\nlimited testing.\n\nIn order to eliminate the initial compilation time, we first generated a\nsingle batch for each GPU with the desired settings. Actually, we'd use this\nstep to determine the optimal configuration for batch size. Once we settled on\nthe batch size, we ran four iterations generating 24 images each, discarded\nthe slowest result, and averaged the time taken from the other three runs. We\nthen used this to calculate the number of images per minute that each GPU\ncould generate.\n\nOur chosen prompt was, again, \"messy room.\" We used the Euler Ancestral\nsampling method, 50 steps (iterations), with a CFG scale of 7. Because all of\nthe GPUs were running the same version 1.5 model from Stable Diffusion, the\nresulting images were generally comparable in content. We noticed previously\nthat SD2.1 tended to often generate \"messy rooms\" that weren't actually messy,\nand were sometimes cartoony. SD1.5 also seems to be preferred by many Stable\nDiffusion users as the later 2.1 models removed many desirable traits from the\ntraining data.\n\nThe above gallery shows an example output at 768x768 for AMD, Intel, and\nNvidia. Rest assured, all of the images appeared to be relatively similar in\ncomplexity and content \u2014 though I won't say I looked carefully at every one of\nthe thousands of images that were generated! For reference, the AMD GPUs\nresulted in around 2,500 total images, Nvidia GPUs added another 4,000+\nimages, with Intel only needing about 1,000 images. All of the same style\nmessy room.\n\n### Comparing Theoretical GPU Performance\n\nWhile the above testing looks at actual performance using Stable Diffusion, we\nfeel it's also worth a quick look at the theoretical GPU performance. There\nare two aspects to consider: First is the GPU shader compute, and second is\nthe potential compute using hardware designed to accelerate AI workloads \u2014\nNvidia Tensor cores, AMD AI Accelerators, and Intel XMX cores, as appropriate.\nNot all GPUs have additional hardware, which means they will use GPU shaders.\nLet's start there.\n\n## Stay on the Cutting Edge\n\nJoin the experts who read Tom's Hardware for the inside track on enthusiast PC\ntech news \u2014 and have for over 25 years. We'll send breaking news and in-depth\nreviews of CPUs, GPUs, AI, maker hardware and more straight to your inbox.\n\nBy submitting your information you agree to the Terms & Conditions and Privacy\nPolicy and are aged 16 or over.\n\nTheoretical GPU Shader Compute (Image credit: Tom's Hardware)\n\nFor FP16 compute using GPU shaders, Nvidia's Ampere and Ada Lovelace\narchitectures run FP16 at the same speed as FP32 \u2014 the assumption is that FP16\ncan and should be coded to use the Tensor cores. AMD and Intel GPUs in\ncontrast have double performance on half-precision FP16 shader calculations\ncompared to FP32, and that applies to Turing GPUs as well.\n\nThis leads to some potentially interesting behavior. The RTX 2080 Ti for\nexample has 26.9 TFLOPS of FP16 GPU shader compute, which nearly matches the\nRTX 3080's 29.8 TFLOPS and would clearly put it ahead of the RTX 3070 Ti's\n21.8 TFLOPS. AMD's RX 7000-series GPUs would also end up being much more\ncompetitive if everything were restricted to GPU shaders.\n\nClearly, this look at FP16 compute doesn't match our actual performance much\nat all. That's because optimized Stable Diffusion implementations will opt for\nthe highest throughput possible, which doesn't come from GPU shaders on modern\narchitectures. That brings us to the Tensor, Matrix, and AI cores on the\nvarious GPUs.\n\nImage 1 of 2\n\n(Image credit: Tom's Hardware)\n\n(Image credit: Tom's Hardware)\n\nNvidia's Tensor cores clearly pack a punch, except as noted before, Stable\nDiffusion doesn't appear to leverage sparsity with the TensorRT code. (It\ndoesn't use FP8 either, which could potentially double compute rates as well.)\nThat means, for the most applicable look at how the GPUs stack up, you should\npay attention to the first chart for Nvidia GPUs, which omits sparsity, rather\nthan the second chart that includes sparsity \u2014 also note that the non-TensorRT\ncode does appear to leverage sparsity.\n\nIt's interesting to see how the above chart showing theoretical compute lines\nup with the Stable Diffusion charts. The short summary is that a lot of the\nNvidia GPUs land about where you'd expect, as do the AMD 7000-series parts.\nBut the Intel Arc GPUs all seem to get about half the expected performance \u2014\nnote that my numbers use the boost clock of 2.4 GHz rather than the lower\n2.0GHz \"Game Clock\" (which is a worst-case scenario that rarely comes into\nplay, in my experience).\n\nThe RX 6000-series GPUs likewise underperform, likely because doing FP16\ncalculations via shaders is less efficient than doing the same calculations\nvia RDNA 3's WMMA instructions. Otherwise, the RX 6950 XT and RX 6900 XT\nshould at least manage to surpass the RX 7600, and that didn't happen in our\ntesting. (Again, performance on the RDNA 2 GPUs tends to be better using\nNod.ai's project, if you're using one of those GPUs and want to improve your\nimage throughput.)\n\nWhat's not clear is just how much room remains for further optimizations with\nStable Diffusion. Looking just at the raw compute, we'd think that Intel can\nfurther improve the throughput of its GPUs, and we also have to wonder if\nthere's a reason Nvidia's 30- and 40-series GPUs aren't leveraging their\nsparsity feature with TensorRT. Or maybe they are and it just doesn't help\nthat much? (I did ask Nvidia engineers about this at one point and was told\nit's not currently used, but these things are still a bit murky.)\n\nStable Diffusion, and other text to image generators, are currently one of the\nmost developed and researched areas of AI that are still readily accessible to\nconsumer level hardware. We've looked at some other areas of AI as well, like\nspeech recognition using Whisper and chatbot text generation, but so far\nneither of those seem to be as optimized or used as Stable Diffusion. If you\nhave any suggestions for other AI workloads we should test, particularly\nworkloads that will work on AMD and Intel as well as Nvidia GPUs, let us know\nin the comments.\n\nJarred Walton\n\nJarred Walton is a senior editor at Tom's Hardware focusing on everything GPU.\nHe has been working as a tech journalist since 2004, writing for AnandTech,\nMaximum PC, and PC Gamer. From the first S3 Virge '3D decelerators' to today's\nGPUs, Jarred keeps up with all the latest graphics trends and is the one to\nask about game performance.\n\nMore about gpus\n\nRare GeForce GTX 2070 engineering sample has surfaced \u2014 the unreleased GPU has\n128 fewer CUDA cores than the RTX 2070\n\nNvidia Blackwell and GeForce RTX 50-Series GPUs: Rumors, specifications,\nrelease dates, pricing, and everything we know\n\nLatest\n\nEKWB reportedly plagued with financial disarray \u2014 many employees and suppliers\nwere allegedly left unpaid for as long as four months\n\nSee more latest \u25ba\n\n30 Comments Comment from the forums\n\n  * Bikki\n\nThanks so much for this, truly generative model is consumer gpu next big thing\nbesides gaming. Meta LLama 2 should be next in the pipe\nhttps://huggingface.co/models?other=llama-2\n\nReply\n\n  * -Fran-\n\nI've learned a lot today, Jarred. Thanks a lot for your review and efforts\ninto explaining everything related to gauging performance for the GPUs for\nthese tasks. Fantastic job.\n\nRegards.\n\nReply\n\n  * JarredWaltonGPU\n\n> Bikki said:\n>\n> Thanks so much for this, truly generative model is consumer gpu next big\n> thing besides gaming. Meta LLama 2 should be next in the pipe\n> https://huggingface.co/models?other=llama-2\n\nI've poked at LLaMa stuff previously with text generation, but what I need is\na good UI and method of benchmarking that can run on AMD, Intel, and Nvidia\nGPUs and leverage the appropriate hardware. Last I looked, most (all) of the\nrelated projects were focused on Nvidia, but there are probably some\nalternatives I haven't seen.\n\nWhat I really need is the equivalent across GPU vendor projects that will use\nLLaMa, not the model itself. Running under Windows 11 would be ideal. If you\nhave any suggestions there, let me know.\n\nReply\n\n  * dramallamadingdong\n\n> Admin said:\n>\n> We've tested all the modern graphics cards in Stable Diffusion, using the\n> latest updates and optimizations, to show which GPUs are the fastest at AI\n> and machine learning inference.\n>\n> Stable Diffusion Benchmarks: 45 Nvidia, AMD, and Intel GPUs Compared : Read\n> more\n\nthank you. Very informative.\n\nReply\n\n  * thisisaname\n\nWhich scripts do I have to let run to display the pictures?\n\nReply\n\n  * kfcpri\n\n> Admin said:\n>\n> We've tested all the modern graphics cards in Stable Diffusion, using the\n> latest updates and optimizations, to show which GPUs are the fastest at AI\n> and machine learning inference.\n>\n> Stable Diffusion Benchmarks: 45 Nvidia, AMD, and Intel GPUs Compared : Read\n> more\n\nAs a SD user stuck with a AMD 6-series hoping to switch to Nv cards, I think:\n\n1\\. It is Nov 23 already if people buy a new card with SD in mind now, they\nabsolutely should consider SDXL and even somewhat plan for \"the version after\nSDXL\" and so omitting it in a benchmark report is like wasting your own time\nand effort. Like, making a detailed benchmarking on Counterstrike but not\nCyberpunk in 2023, Of course the old cards can't run it so I think maybe a\nseparate SDXL report on just the >=12GB latest gens cards? The tests should be\na basic 1024x1024 one, plus another larger dimension one that kind of simulate\na potential future SD version. Some other such test elsewhere shows that the\n4060 ti 16GB will be faster than the 4070 in such vram heavy operation, and I\nhave been hoping to see more tests like that to confirm.\n\n2\\. I can understand not mentioning AMD with Olive, which is a quick\noptimizations but with many limitations and required extra preps on the\nmodels. However AMD on Linux with ROCm support most of the stuff now with few\nlimitations and it runs way faster than AMD on Win DirectML, so it should\nworth a mention. (I prefer to switch to Nv soon though)\n\nReply\n\n  * JarredWaltonGPU\n\n> kfcpri said:\n>\n> As a SD user stuck with a AMD 6-series hoping to switch to Nv cards, I\n> think:\n>\n> 1\\. It is Nov 23 already if people buy a new card with SD in mind now, they\n> absolutely should consider SDXL and even somewhat plan for \"the version\n> after SDXL\" and so omitting it in a benchmark report is like wasting your\n> own time and effort. Like, making a detailed benchmarking on Counterstrike\n> but not Cyberpunk in 2023, Of course the old cards can't run it so I think\n> maybe a separate SDXL report on just the >=12GB latest gens cards? The tests\n> should be a basic 1024x1024 one, plus another larger dimension one that kind\n> of simulate a potential future SD version. Some other such test elsewhere\n> shows that the 4060 ti 16GB will be faster than the 4070 in such vram heavy\n> operation, and I have been hoping to see more tests like that to confirm.\n>\n> 2\\. I can understand not mentioning AMD with Olive, which is a quick\n> optimizations but with many limitations and required extra preps on the\n> models. However AMD on Linux with ROCm support most of the stuff now with\n> few limitations and it runs way faster than AMD on Win DirectML, so it\n> should worth a mention. (I prefer to switch to Nv soon though)\n\nToo many things are \"broken\" with SDXL right now to reliably test it on all of\nthe different GPUs, as noted in the text. TensorRT isn't yet available, and\nthe DirectML and OpenVINO forks may also be iffy. I do plan on testing it, but\nit's easy enough to use regular SD plus a better upscaler (SwinIR_4x is a good\nexample) if all you want is higher resolutions. But SDXL will hopefully\nproduce better results as well. Anyway, just because some people have switched\nto SDXL doesn't make it irrelevant, as part of the reason for all these\nbenchmarks is to give a reasonable look at general AI inference performance.\nSD has been around long enough that it has been heavily tuned on all\narchitectures; SDXL is relatively new by comparison.\n\nRegarding AMD with Olive, you do realize that this is precisely what the\nlinked DirectML instructions use, right? I didn't explicitly explain that, as\ninterested parties following the link will have the necessary details. AMD's\nlatest instructions are to use the DirectML fork, and I'd be surprised if ROCm\nis actually much faster at this point. If you look at the theoretical FP16\nperformance, I'm reasonably confident the DirectML version gets most of what\nis available. ROCm also has limitations in which GPUs are supported, at least\nlast I checked (which has been a while).\n\nReply\n\n  * forrmorr134567\n\nhow did you get to 24 images per minute on 2080 super?\n\nReply\n\n  * JarredWaltonGPU\n\n> forrmorr134567 said:\n>\n> how did you get to 24 images per minute on 2080 super?\n\nMaybe read the article?\n\n\"Getting things to run on Nvidia GPUs is as simple as downloading, extracting,\nand running the contents of a single Zip file. But there are still additional\nsteps required to extract improved performance, using the latest TensorRT\nextensions. Instructions are at that link, and we've previous tested Stable\nDiffusion TensorRT performance against the base model without tuning if you\nwant to see how things have improved over time.\"\n\nSo you have to do the extra steps to get the TensorRT extension installed and\nconfigured in the UI, then pre-compile static sizes, normally a batch size of\n8 with 512x512 resolution.\n\nReply\n\n  * Elegant spy\n\nHello i want to ask so Intel Arc gpus works well using the automatic1111\nopenVINO version right ? does Intel Arc gpus still able to run SD using\ndirectml like amd gpus does ? thank you\n\nReply\n\n##### Most Popular\n\nHands-on with Corsair's 2500D Airflow case: Roomy at the back, for rear-\nconnector motherboard cables\n\n120mm AIO Roundup: Testing Be Quiet, Corsair, Cooler Master, and Enermax\nmodels\n\nHands-on with InWin's F5 PC Case: Back-connector motherboard support and wood\nfront panels\n\nThe five best AMD CPUs of all time: From old-school Athlon to brand-new Ryzen\n\nHands-on with Be Quiet\u2019s Dark Base Pro 901: Decibel dampener\n\nThis is the fastest SSD we've ever tested \u2014 Phison E26 Max14um 2TB performance\npreview\n\nHands-On: Cooler Master's NCore 100 Max case stands tall and handles large\nGPUs\n\nI built a PC With MSI's Project Zero Motherboard: Moving all the ports to the\nback for a cleaner, quicker build with better airflow\n\nSurging on Smartphones, Folding Screen Adoption on Laptops Will Take Years to\nGo Mainstream\n\nTesting GPUs with AMD FSR3 and Avatar: Frontiers of Pandora \u2014 16 graphics\ncards and hundreds of benchmarks\n\nHands-On: Lian Li's LCD Screen fans turn heads and are surprisingly\naffordable, but not as configurable as I'd like\n\nTom's Hardware is part of Future US Inc, an international media group and\nleading digital publisher. Visit our corporate site.\n\n\u00a9 Future US, Inc. Full 7th Floor, 130 West 42nd Street, New York, NY 10036.\n\n", "frontpage": false}
{"aid": "40133574", "title": "Supergraph Panel with Hasura, MotherDuck, ClickHouse and Yugabyte", "url": "https://www.youtube.com/watch?v=efiuibN5YIQ", "domain": "youtube.com", "votes": 3, "user": "tristenharr", "posted_at": "2024-04-23 16:08:07", "comments": 0, "source_title": "Supergraph Panel with Hasura, MotherDuck, ClickHouse & Yugabyte", "source_text": "Supergraph Panel with Hasura, MotherDuck, ClickHouse & Yugabyte - YouTube\n\nSupergraph Panel with Hasura, MotherDuck, ClickHouse & Yugabyte\n\n2x\n\nIf playback doesn't begin shortly, try restarting your device.\n\n\u2022\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV\nrecommendations. To avoid this, cancel and sign in to YouTube on your\ncomputer.\n\nUp next\n\nLive\n\nUpcoming\n\nPlay Now\n\nHasura\n\nSubscribe\n\nSubscribed\n\nShare\n\nAn error occurred while retrieving sharing information. Please try again\nlater.\n\n0:00\n\n0:00 / 36:25\u2022Watch full video\n\n\u2022\n\nNaN / NaN\n\nBack\n\n", "frontpage": false}
