{"aid": "40178881", "title": "Scalable Web Scraping with Serverless \u2013 Part 1", "url": "https://lev.engineer/blog/scalable-web-scraping-with-serverless-part-1", "domain": "lev.engineer", "votes": 1, "user": "levgel", "posted_at": "2024-04-27 10:33:06", "comments": 0, "source_title": "Scalable Web Scraping with Serverless - Part 1", "source_text": "Scalable Web Scraping with Serverless - Part 1\n\nAbout Me\n\nServices\n\nProjects\n\nCompanies\n\nResources\n\nBlog\n\nContact me\n\n\u2261\n\n# Blog\n\n## For solo engineers, but not only.\n\n# Scalable Web Scraping with Serverless - Part 1\n\nApril 27, 2024\u2022Lev Gelfenbuim\u202219 min. read\n\nToday I'll show you how to build a scalable infrastructure for web scraping\nusing Serverless Framework.\n\nServerless computing offers a compelling solution by eliminating the need to\nmanage infrastructure, allowing developers to focus solely on code. This model\nis perfect for web scraping tasks that vary in intensity and frequency, as it\nscales automatically to meet demand without any manual intervention. Moreover,\nserverless functions are cost-effective, as you only pay for the compute time\nyou consume, making it an ideal choice for both small-scale projects and\nlarge-scale data extraction tasks.\n\nIn this blog series, we will explore how to leverage serverless technologies\nto build a robust and scalable web scraping infrastructure. We'll use a suite\nof AWS services including Lambda, S3, SQS, and RDS, combined with popular\nNode.js libraries like node-fetch for fetching data, cheerio for parsing HTML,\nand node-postgres to interact with databases.\n\nOur journey will be split into two parts:\n\n  1. Part 1: We'll set up our serverless environment, deploy functions to fetch and store web data, and orchestrate our components using AWS services.\n  2. Part 2: We will transform our raw HTML data into structured JSON and seamlessly load this data into a PostgreSQL database for further analysis.\n\nUnderstanding Serverless Architecture#\n\nWhat is Serverless Computing?#\n\nServerless computing is a cloud computing execution model where the cloud\nprovider manages the setup, capacity planning, and server management for you.\nEssentially, it allows developers to write and deploy code without worrying\nabout the underlying infrastructure. In a serverless setup, you only pay for\nthe compute time you consume\u2014there is no charge when your code is not running.\n\nKey Benefits of Serverless for Web Scraping#\n\n  * Automatic Scaling: Serverless functions automatically scale based on the demand. For web scraping, this means you can handle virtually any number of pages or sites without needing to manually adjust the capacity of servers.\n  * Cost-Effectiveness: With serverless, you pay per execution. You aren\u2019t charged for idle resources. This can be incredibly cost-efficient compared to running dedicated servers 24/7, especially for scraping tasks that might only need to run a few times a day or week.\n  * Reduced Management Overhead: The serverless model offloads much of the operational burden to the cloud provider. Maintenance tasks like server provisioning, patching, and administration are handled by the provider, freeing you to focus on improving your scraping logic and handling data.\n\nHow Serverless Fits Into Web Scraping#\n\nServerless is particularly adept at handling event-driven applications\u2014like\nthose triggered by a scheduled time to scrape data or a new item appearing in\na queue. This fits naturally with the often sporadic and on-demand nature of\nweb scraping, where workloads can be highly variable and unpredictable.\n\nPopular Serverless Platforms#\n\nWhile AWS Lambda is one of the most popular serverless computing services,\nother cloud providers offer similar functionalities:\n\n  * Azure Functions: Microsoft's equivalent to AWS Lambda, offering easy integration with other Azure services.\n  * Google Cloud Functions: A lightweight, event-driven computing solution that can automatically scale based on the workload.\n  * IBM Cloud Functions: Based on Apache OpenWhisk, IBM\u2019s offering also supports serverless computing within their cloud ecosystem.\n\nEach platform has its own strengths and pricing models, but AWS Lambda will be\nour focus due to its maturity, extensive documentation, and seamless\nintegration with other AWS services like S3 and SQS.\n\nConfiguring the Serverless Framework#\n\nAfter gaining a foundational understanding of serverless architecture, our\nnext step involves setting up the Serverless Framework. This powerful\nframework simplifies deploying serverless applications and managing their\nlifecycle. In this section, we'll cover how to configure the Serverless\nFramework to work seamlessly with AWS services, a crucial component for our\nscalable web scraping infrastructure.\n\nIntroduction to the Serverless Framework#\n\nThe Serverless Framework is an open-source CLI that provides developers with a\nstreamlined workflow for building and deploying serverless applications. It\nabstracts much of the complexity associated with configuring various cloud\nservices, making it easier to launch applications across different cloud\nproviders like AWS, Azure, and Google Cloud.\n\nInstalling the Serverless Framework#\n\nTo begin, you need to have Node.js installed on your machine as the Serverless\nFramework runs on it. Once Node.js is set up, you can install the Serverless\nFramework globally using npm:\n\n    \n    \n    1npm install -g serverless\n\nOr, if you prefer using Yarn:\n\n    \n    \n    1yarn global add serverless\n\nConfiguring AWS Credentials#\n\nTo deploy functions to AWS Lambda and manage resources, you need to configure\nyour AWS credentials in the Serverless Framework:\n\n  1. Create an AWS IAM User: Log in to your AWS Management Console and navigate to the IAM service. Create a new user with programmatic access. Attach the AdministratorAccess policy to this user for now, which grants the necessary permissions. For production, you should customize the permissions to follow the principle of least privilege.\n  2. Configure Credentials: After creating your IAM user, you will receive an access key ID and secret access key. Use these credentials to configure the Serverless Framework by running the following command:\n\n    \n    \n    1serverless config credentials --provider aws --key YOUR_ACCESS_KEY_ID --secret YOUR_SECRET_ACCESS_KEY\n\nThis command stores your credentials in a local file, which the Serverless\nFramework uses to interact with your AWS account.\n\nSetting Up serverless.yml#\n\nEvery Serverless Framework project contains a serverless.yml file at its root.\nThis file is crucial as it defines the service configuration, including\nfunctions, events, and resources. Here\u2019s a basic setup for our web scraping\nproject:\n\n    \n    \n    1service: web-scraping-service 2 3provider: 4 name: aws 5 runtime: nodejs14.x 6 region: us-east-1 7 8functions: 9 scrapeSite: 10 handler: handler.scrape 11 events: 12 - schedule: 13 rate: cron(0 */4 * * ? *) # Runs every 4 hours\n\nIn this configuration:\n\n  * service: Defines the name of your serverless application.\n  * provider: Specifies AWS as the provider and sets the runtime environment and region.\n  * functions: Lists the functions to deploy. In this example, scrapeSite is triggered by a scheduled event.\n\nExtending the Serverless Configuration for SQS#\n\nTo fully harness the power of serverless architecture for our web scraping\ninfrastructure, integrating Amazon Simple Queue Service (SQS) is essential.\nSQS will manage the messages related to tasks such as notifying our system\nwhen new data is available for processing. Here's how to extend our existing\nserverless.yml configuration to include SQS resources:\n\nAdding SQS to the Serverless Framework Configuration#\n\nSQS queues can be defined and managed directly within the serverless.yml file,\nwhich allows for seamless integration and management within our AWS ecosystem.\nLet\u2019s add two SQS queues to our setup\u2014one for HTML files and another for JSON\ndata:\n\n    \n    \n    1service: web-scraping-service 2 3provider: 4 name: aws 5 runtime: nodejs14.x 6 region: us-east-1 7 iamRoleStatements: 8 - Effect: Allow 9 Action: 10 - sqs:SendMessage 11 - sqs:ReceiveMessage 12 - sqs:DeleteMessage 13 - sqs:GetQueueAttributes 14 Resource: \"*\" 15 16resources: 17 Resources: 18 HtmlQueue: 19 Type: AWS::SQS::Queue 20 Properties: 21 QueueName: htmlQueue 22 23functions: 24 scrapeSite: 25 handler: handler.scrape 26 events: 27 - schedule: 28 rate: cron(0 */4 * * ? *) # Runs every 4 hours 29 environment: 30 HTML_QUEUE_URL: !Ref HtmlQueue\n\nExplanation of the Configuration:\n\n  * IAM Role Statements: Specifies the permissions for the Lambda functions to interact with SQS, allowing them to send, receive, and delete messages.\n  * Resources: Defines the SQS queue. The HtmlQueue is used to store messages that contain references to HTML files stored in S3.\n  * Functions:\n\n    * scrapeSite: This function is triggered on a schedule to scrape websites and push references to the HtmlQueue.\n\nCreating Your First Scraping Function with AWS Lambda#\n\nWith the Serverless Framework configured and ready, the next step in building\nour scalable web scraping infrastructure is to develop the first AWS Lambda\nfunction. This function will be responsible for fetching web data periodically\nbased on the sitemap of a target website. Let's dive into creating and\ndeploying this essential component.\n\nUnderstanding AWS Lambda#\n\nAWS Lambda is a serverless compute service that runs your code in response to\nevents and automatically manages the compute resources for you. Lambda is\nideal for handling tasks that respond to HTTP requests, process queue\nmessages, or, as in our case, execute tasks on a schedule.\n\nSetting Up the Lambda Function#\n\n  1. Initialize a Serverless Project: Begin by creating a new directory for your project and navigate into it. Use the Serverless CLI to create a new service:\n\n    \n    \n    1serverless create --template aws-nodejs --path my-web-scraper 2cd my-web-scraper\n\nThis command sets up a basic Node.js project with a serverless.yml file and a\nsample handler function in handler.js.\n\n  1. Install Dependencies: For fetching and parsing the sitemap, we'll use node-fetch for HTTP requests and xml2js for converting XML data into a JSON object.\n\n    \n    \n    1npm install node-fetch xml2js\n\nWriting the Lambda Function#\n\nCreate a new function in the handler.js file or whatever your main file is\nnamed. This function will handle the fetching of the sitemap and parsing it to\nget URLs:\n\n    \n    \n    1exports.scrape = async () => { 2 try { 3 const sitemapUrl = 'https://example.com/sitemap.xml'; 4 const response = await fetch(sitemapUrl); 5 if (!response.ok) { 6 throw new Error(`Failed to fetch sitemap: ${response.statusText}`); 7 } 8 const sitemapXml = await response.text(); 9 10 const parsedSitemap = await xml2js.parseStringPromise(sitemapXml); 11 const urls = parsedSitemap.urlset.url.map(u => u.loc[0]); 12 13 for (const url of urls) { 14 const params = { 15 MessageBody: JSON.stringify({ url }), 16 QueueUrl: process.env.HTML_QUEUE_URL, 17 }; 18 19 await sqs.sendMessage(params).promise(); 20 } 21 } catch (error) { 22 console.error('Error during scraping process:', error); 23 // Optionally rethrow or handle error specifically (e.g., retry logic, notification) 24 } 25};\n\nKey Components of the Function#\n\n  * Fetching the Sitemap: The function starts by fetching the sitemap.xml using node-fetch, a lightweight module suited for such tasks.\n  * Parsing XML: The sitemap XML is parsed into a JavaScript object using xml2js, which allows easy access to the URLs listed in the sitemap.\n  * Pushing URLs to SQS: Each URL is then formatted into a message and sent to the SQS queue designated for HTML page processing.\n\nError Handling#\n\nTo enhance the reliability of your Lambda function, especially in a web\nscraping context where external dependencies like network issues or site\nchanges can lead to errors, error handling is a must. Here\u2019s how you can\nimplement comprehensive error handling in your serverless function:\n\nError Monitoring and Logging#\n\nIntegrate error monitoring and logging tools to capture errors for further\nanalysis. Tools like AWS CloudWatch can be configured to monitor logs, which\nhelps in diagnosing issues after deployment.\n\n    \n    \n    1provider: 2 name: aws 3 runtime: nodejs14.x 4 region: us-east-1 5 iamRoleStatements: 6 # IAM permissions here 7 logs: 8 restApi: 9 loggingLevel: ERROR 10 fullExecutionData: true\n\nDead Letter Queues (DLQ)#\n\nConfigure Dead Letter Queues (DLQ) in SQS for messages that cannot be\nprocessed after several attempts. This approach helps in isolating problematic\nmessages and prevents them from clogging your processing pipeline.\n\n    \n    \n    1resources: 2 Resources: 3 HtmlQueue: 4 Type: AWS::SQS::Queue 5 Properties: 6 QueueName: htmlQueue 7 RedrivePolicy: 8 deadLetterTargetArn: !GetAtt HtmlDeadLetterQueue.Arn 9 maxReceiveCount: 3 # Adjust based on your needs 10 11 HtmlDeadLetterQueue: 12 Type: AWS::SQS::Queue 13 Properties: 14 QueueName: htmlDeadLetterQueue\n\nTimeout and Memory Management#\n\nProperly manage timeouts and memory settings in your Lambda function to\nprevent unexpected terminations or performance issues.\n\n    \n    \n    1functions: 2 scrapeSite: 3 handler: handler.scrape 4 timeout: 30 # seconds 5 memorySize: 256 # MB 6 events: 7 - schedule: 8 rate: cron(0 */4 * * ? *) # Runs every 4 hours\n\nTesting and Deployment#\n\nBefore deploying your function, test it locally or in a development\nenvironment to ensure it behaves as expected. The Serverless Framework\nprovides commands for invoking functions locally and deploying them to AWS.\n\n  * Local Testing\n\n    \n    \n    1serverless invoke local --function scrapeSite\n\n  * Deployment\n\n    \n    \n    1serverless deploy\n\nStoring Web Data in AWS S3#\n\nAfter successfully setting up and deploying our Lambda function to scrape web\ndata, the next crucial step in our serverless web scraping pipeline involves\nefficiently storing the retrieved data. Amazon S3 (Simple Storage Service)\noffers a robust solution for this purpose, providing scalability, data\navailability, security, and performance. This section will guide you through\nsetting up S3 buckets and configuring your Lambda function to save scraped\ndata directly to S3.\n\nWhy Choose Amazon S3?#\n\nAmazon S3 is an object storage service that offers industry-leading\nscalability, data availability, security, and performance. This means\ncustomers of all sizes and industries can use it to store and protect any\namount of data for a range of use cases, such as websites, mobile\napplications, backup and restore, archive, enterprise applications, IoT\ndevices, and big data analytics. Some key benefits include:\n\n  * Durability and Availability: S3 provides comprehensive security and compliance capabilities that meet even the most stringent regulatory requirements. It gives you flexibility in the way you manage data for cost optimization, access control, and compliance.\n  * Scalability: Automatically scale your storage without worrying about the underlying infrastructure. This is crucial for web scraping applications where the amount of data can grow unpredictably.\n  * Cost-Effectiveness: With S3, you pay only for the storage you use. There are no minimum fees or setup costs, which makes it a cost-effective solution for storing web scraped data.\n\nSetting Up S3 Buckets#\n\nBefore you can store data, you need to create an S3 bucket in your AWS\naccount. Each bucket's name must be unique across all existing bucket names in\nAmazon S3 (bucket names are shared among all users globally).\n\n  1. Create a Bucket:\n\n     * Go to the AWS Management Console.\n     * Navigate to S3 and select \u201cCreate bucket\u201d.\n     * Provide a unique bucket name, e.g., web-scraping-data-yourname.\n     * Select the AWS region where you want the bucket to reside.\n     * Leave the default settings or configure options like versioning or logging based on your specific requirements.\n  2. Bucket Policy:\n\n     * To define the bucket policy directly in your serverless.yml configuration, you can use AWS CloudFormation resources that the Serverless Framework supports. Here's how to include a bucket policy that specifically allows a Lambda function to put and get objects in an S3 bucket:\n\n    \n    \n    1service: web-scraping-service 2 3provider: 4 name: aws 5 runtime: nodejs14.x 6 region: us-east-1 7 iamRoleStatements: 8 - Effect: \"Allow\" 9 Action: 10 - \"s3:ListBucket\" 11 Resource: \"arn:aws:s3:::web-scraping-data-yourname\" 12 - Effect: \"Allow\" 13 Action: 14 - \"s3:PutObject\" 15 - \"s3:GetObject\" 16 Resource: \"arn:aws:s3:::web-scraping-data-yourname/*\" 17 18resources: 19 Resources: 20 WebScrapingDataBucket: 21 Type: AWS::S3::Bucket 22 Properties: 23 BucketName: web-scraping-data-yourname 24 25 BucketPolicy: 26 Type: AWS::S3::BucketPolicy 27 Properties: 28 Bucket: 29 Ref: WebScrapingDataBucket 30 PolicyDocument: 31 Statement: 32 - Effect: \"Allow\" 33 Principal: 34 AWS: 35 Fn::GetAtt: [LambdaExecutionRole, Arn] 36 Action: 37 - \"s3:PutObject\" 38 - \"s3:GetObject\" 39 Resource: 40 Fn::Join: 41 - \"\" 42 - - \"arn:aws:s3:::\" 43 - Ref: WebScrapingDataBucket 44 - \"/*\"\n\nExplanation:\n\n  * IAM Role Statements: Defines permissions for the Lambda function to interact with S3 at the provider level. This allows listing the bucket contents and reading/writing objects.\n  * WebScrapingDataBucket: This resource block creates the S3 bucket where the data will be stored.\n  * BucketPolicy: This resource applies a bucket policy to the S3 bucket. It specifically allows the Lambda function to put and get objects. The Principal section utilizes the Fn::GetAtt CloudFormation function to dynamically fetch the Amazon Resource Name (ARN) of the Lambda function's execution role.\n  * Resource: The policy applies to all objects within the bucket (/*), ensuring that the Lambda function can interact with any object stored therein.\n\nWriting the Scraping Lambda Function#\n\nUpdating the serverless.yml Configuration#\n\n  1. Define the Scraping Function: Add the new Lambda function that will be triggered by messages from HtmlQueue.\n  2. Permissions: Ensure the function has the necessary permissions to access SQS and S3.\n  3. Event Source: Configure the function to be triggered by new messages in HtmlQueue.\n\nHere is how you could configure it in the serverless.yml:\n\n    \n    \n    1functions: 2 fetchHtmlContent: 3 handler: fetchHtml.handler 4 events: 5 - sqs: 6 arn: 7 Fn::GetAtt: 8 - HtmlQueue 9 - Arn 10 environment: 11 S3_BUCKET: web-scraping-data-yourname 12 13resources: 14 Resources: 15 HtmlQueue: 16 Type: AWS::SQS::Queue 17 Properties: 18 QueueName: htmlQueue\n\nImplementing the Scraping Lambda Function#\n\nThe function will use node-fetch to fetch the web content.\n\n    \n    \n    1const AWS = require('aws-sdk'); 2const fetch = require('node-fetch'); 3const s3 = new AWS.S3(); 4const proxyList = ['http://proxy1.com:port', 'http://proxy2.com:port']; // Example proxy list 5 6exports.handler = async (event) => { 7 for (const record of event.Records) { 8 const { url } = JSON.parse(record.body); 9 const proxy = proxyList[Math.floor(Math.random() * proxyList.length)]; 10 const proxyOptions = { 11 headers: { 12 'Proxy-Authorization': 'Basic ' + Buffer.from('user:password').toString('base64'), // if authentication is needed 13 } 14 }; 15 16 try { 17 const response = await fetch(url, { agent: new HttpsProxyAgent(proxy), ...proxyOptions }); 18 if (!response.ok) { 19 throw new Error(`HTTP error! status: ${response.status}`); 20 } 21 const body = await response.text(); 22 23 const params = { 24 Bucket: process.env.S3_BUCKET, 25 Key: `${new Date().getTime()}.html`, 26 Body: body, 27 ContentType: 'text/html', 28 }; 29 30 await s3.upload(params).promise(); 31 console.log('HTML uploaded successfully:', params.Key); 32 } catch (error) { 33 console.error('Error fetching and uploading HTML:', error); 34 } 35 } 36};\n\nKey Points to Consider#\n\n  * Proxy Rotation: The function uses a simple random selection for proxy rotation. For more sophisticated proxy management, consider using a dedicated proxy rotation service (affiliation).\n  * Error Handling: Robust error handling ensures that failures in fetching URLs or uploading to S3 are logged and managed appropriately.\n  * Security: If you are using proxies that require authentication, ensure that credentials are securely stored, possibly using AWS Secrets Manager or environment variables encrypted using AWS KMS.\n\nConclusion#\n\nWe've taken significant steps in establishing a robust and scalable serverless\nweb scraping system using AWS technologies and the Serverless Framework. By\ncarefully configuring and deploying our serverless functions, we are well on\nour way to creating a fully automated data extraction pipeline that leverages\nthe cloud's power for efficiency and scalability.\n\nKey Achievements#\n\n  1. Understanding Serverless Architecture: We started by exploring what serverless computing entails and how its characteristics\u2014such as automatic scaling, cost-effectiveness, and reduced management overhead\u2014make it an ideal choice for web scraping tasks.\n  2. Setting Up the Serverless Framework: We installed and configured the Serverless Framework, which serves as the backbone for deploying and managing our AWS services. This setup streamlines the process of deploying code and managing infrastructure, allowing us to focus more on our application logic than on server maintenance.\n  3. Creating and Configuring AWS Services: We have successfully set up essential AWS services, including Lambda for running our code, S3 for storing the scraped data, and SQS for managing the messages between different parts of our scraping process. This configuration not only ensures efficient data handling but also robustness through decoupling of components.\n  4. Building the Initial Scraping Function: Our first Lambda function, designed to fetch sitemap.xml from target websites and parse it for URLs, has been implemented. This function is crucial as it populates our SQS queue with tasks for further processing, demonstrating the automated trigger-based nature of our serverless architecture.\n  5. Introduction of a Function for HTML Fetching: We introduced a crucial Lambda function triggered by messages in the HtmlQueue. This function fetches HTML content using rotating proxies to mitigate the risk of IP bans and uploads the fetched HTML to S3. This addition enhances our pipeline by ensuring real-time data fetching and storage, showcasing the power of integrating multiple serverless services.\n  6. Secure and Scalable Data Storage: By integrating Amazon S3, we've ensured that our data is stored securely and is easily accessible for further processing. This setup benefits from S3\u2019s durability and scalability, which are vital for handling potentially large volumes of web data.\n\nLooking Forward#\n\nIn Part 2 of this series, we will delve deeper into transforming the HTML data\ninto a more structured format (JSON), which can be used for various analytical\npurposes. We will implement additional Lambda functions to process the HTML\ndata stored in S3, transform it into JSON, and finally store this structured\ndata in a PostgreSQL database. This will complete our end-to-end data\nprocessing pipeline, showcasing how serverless technologies can be effectively\nutilized for complex data processing tasks in web scraping.\n\nArticle last update: April 27, 2024\n\nScraping\n\nServerless\n\nEthical Hacking\n\n", "frontpage": false}
