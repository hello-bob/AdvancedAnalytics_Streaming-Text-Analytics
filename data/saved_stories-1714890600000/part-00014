{"aid": "40259696", "title": "Llm.c State of the Union", "url": "https://github.com/karpathy/llm.c/discussions/344", "domain": "github.com/karpathy", "votes": 1, "user": "neeleshs", "posted_at": "2024-05-04 19:22:09", "comments": 0, "source_title": "State of the Union [May 3, 2024] \u00b7 karpathy/llm.c \u00b7 Discussion #344", "source_text": "State of the Union [May 3, 2024] \u00b7 karpathy/llm.c \u00b7 Discussion #344 \u00b7 GitHub\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nkarpathy / llm.c Public\n\n  * Notifications\n  * Fork 1.9k\n  * Star 17.9k\n\n# State of the Union [May 3, 2024] #344\n\nkarpathy started this conversation in General\n\nState of the Union [May 3, 2024] #344\n\nkarpathy\n\nMay 3, 2024 \u00b7 3 comments \u00b7 1 reply\n\nReturn to top\n\nDiscussion options\n\n##\n\nkarpathy\n\nMay 3, 2024\n\nMaintainer\n\nOriginal comment in English -\n\n[May 3, 2024]It is day 24 of the llm.c project. We can now do multi-GPU\ntraining, in bfloat16, with flash attention, and it is FAST! \ud83d\ude80Single GPU\ntraining. We are now training GPT-2 (124M) faster than PyTorch nightly by ~7%,\nwith no asterisks. i.e. this is the fastest PyTorch run that I am aware one\ncan configure, for one GPU training on Ampere, that includes all the modern &\nstandard bells-and-whistles: mixed precision training, torch compile and flash\nattention. Compared to the current PyTorch stable release 2.3.0, we are\nactually ~46% faster, but the folks at PyTorch have been busy and merged a\nnumber of changes over the last ~month that happen to greatly speed up the\nGPT-2 training setting (very nice!). Lastly, compared to the last State of the\nUnion on April 22 (10 days ago), this is ~3X speedup. A lot of improvements\nlanded over the last ~week to get us here, the major ones include:\n\n  * \u2705 Mixed precision training (bfloat16), with a parameter master copy kept in fp32\n  * \u2705 Kernel optimizations, across the board, including e.g. a fused classifier that is an algorithmic improvement on what the PyTorch compiler does so far (we do not materialize normalized logits and only evaluate the loss at the label's index)\n  * \u2705 Flash attention (currently the one from cuDNN)\n  * \u2705 Packed128 data structure (a bit like float4 but supports mixed precision) that forces new hardware to utilize 128-bit load (LDG.128) and store (STS.128) instructions to maximize memory bandwidth.\n  * \u2705 Memory savings. Deleted a large amount of unnecessary memory previously used for backward pass gradients, which dramatically lowered the memory needed to train\n\nMulti-GPU training. Achieved a solid version 1:\n\n  * \u2705 First version of multi-gpu training with MPI+NCCL\n  * \u2705 Profiling the full training run for NVIDIA Nsight Compute\n  * PR for stage 1 of ZeRO (optimizer state sharding) merging imminently\n\nFunctional. Outside of training efficiency alone, we are gearing up for a\nproper reproduction of the GPT-2 miniseries of model sizes from 124M all the\nway to the actual 1.6B model. For this we will need additional changes\nincluding gradient accumulation, gradient clipping, init from random weights\ndirect in C, learning rate warmup and schedule, evaluation (WikiText 103?),\nand a modern pretraining dataset (e.g. fineweb?). A lot of these components\nare pending and currently being worked on.Goal. The current goal is to create\na reliable, stable, clean, tested, minimal, hardened and sufficiently\noptimized LLM stack that reproduces the GPT-2 miniseries of all model sizes,\nfrom 124M to 1.6B, directly in C/CUDA. At current pace this feels like\nsomewhere around ~2 weeks out.Lines of code\ud83d\udc4e With more features and\noptimizations can more lines of code. The main code file train_gpt2.cu is now\nat around 3,000 lines of code (LOC). In addition, we split off two new files\ncommon.h (300 LOC) and tokenizer.h (100 LOC) which we now include. This is up\nfrom ~2000 LOC on April 22.Latency\ud83d\udc4e Sad to report some less upbeat\ndevelopments to the compile latency of the projects:\n\n  * nvcc compile latency, i.e. time make train_gpt2cu: 4.3s (up from 2.4s before). So, sadly, this is now about as bad as import torch and we are very interested in how we could decrease this latency.\n  * turning on flash attention, i.e. time make train_gpt2cu USE_CUDNN=1 includes the cudnn flash attention and gives great speedups and memory savings, but sadly bloats up the compile latency to ~1m24s \ud83e\udd26\u2642\ufe0f. This is a major and previously unexpected slowdown coming from our use of cudnn, and we are actively very interested in how we could delete this dependency as a result.\n  * run latency (ENTER to first step) remains mostly unchanged at around ~3s\n\nPeak memory\ud83d\udc4d Our peak memory usage has improved quite a bit recently by being\nvery careful with what memory we allocate and how we use it, especially with\nour Fused Classifier. Training with batch size 32 and sequence length 1024.\nExample invocations for llm.c and PyTorch:\n\n    \n    \n    make train_gpt2cu USE_CUDNN=1 && ./train_gpt2cu -i data/TinyStories -v 250 -s 250 -g 144 -b 32 python train_gpt2.py --write_tensors=0 --num_iterations=1000 --sequence_length=1024 --compile=1 --tensorcores=1 --dtype=bfloat16 --flash=1 --batch_size=32 --input_bin=data/TinyStories_train.bin\n\nllm.c: 16.6 GiB PyTorch: 37.2 GiB(err, honestly the PyTorch number feels a bit\nsuspiciously high in this comparison, todo investigate more and edit)Runtime,\nDRAM traffic, instructions:\ud83d\udc4d run of profile_gpt2cu.py (batch size 24):\n\n    \n    \n    Kernel type summaries: name time frac count ampere_bf16 360.03 55.13% 111 cutlass::Kernel2 109.09 16.70% 36 cudnn_generated_fort_native_sdpa 58.27 8.92% 24 gelu_backward_kernel 20.44 3.13% 12 fused_classifier_kernel3 20.07 3.07% 1 matmul_backward_bias_kernel6 15.49 2.37% 48 layernorm_backward_kernel7 15.28 2.34% 25 adamw_kernel3 14.08 2.16% 1 gelu_forward_kernel2 13.56 2.08% 12 residual_forward_kernel 8.35 1.28% 24 cudnn::fusion::rearrange_n_convert_dq 8.07 1.24% 12 layernorm_forward_kernel3 5.74 0.88% 25 cudnn::fusion::compute_dot_do_o 3.46 0.53% 12 copy_and_cast_kernel 2.86 0.44% 0 encoder_backward_kernel 0.78 0.12% 1 encoder_forward_kernel3 0.26 0.04% 1 cast_and_add_kernel 0.08 0.01% 48 In total, a training step takes 302.5ms, distributed as: 0.2ms (0.1%) in the encoder, 75.0ms (24.8%) in forward blocks, 44.0ms (14.5%) in the classifier part, 183.4ms (60.6%) in backward blocks, and 0.0ms (0.0%) in the optimizer. We read 113.3GiB (374.4GB/s) and write 68.3GiB (225.6GB/s) to DRAM, read 351.5GiB (1161.9GB/s) and write 70.1GiB (231.8GB/s) to L2, and execute 6.6 billion instructions (21.9 GInst/s).\n\nkernels eye candy, stale by 1 day:Nsight Systems timeline eye candy, stale by\n1 day:Contributors\n\n  * \ud83e\uddd9\u2642\ufe0f kernels: @ngc92 @ademeure @ChrisDryden @JaneIllario\n  * \ud83d\udd25 multi-GPU training: @PeterZhizhin @chinthysl\n  * \ud83d\udc8e tooling: @austinvhuang @Ricardicus @dagelf @rosslwheeler @azret @lancerts\n  * \ud83d\ude4f discussions and PyTorch support: @Chillee\n\nIt is worth especially distinguishing @ngc92 @ademeure, who are both very\nactive and have contributed a great amount of code, ideas and expertise to the\nllm.c project.Notable forksThree new notable forks:\n\n  * llm.cpp by @gevtushenko: a port of this project using the CUDA C++ Core Libraries\n    * A presentation this fork was covered in this lecture in the CUDA MODE Discord Server\n  * llm.zig by @saimirbaci: a Zig port of this project\n  * llm.go by @joshcarp: a Go port of this project\n\nFeatured discussionsLLM.c Speed of Light & Beyond (A100 Performance Analysis)\nby @ademeure for a recent profiling run of llm.c and ideas on further steps\nfor optimization.For more llm.c discussions join us on #llmc on nn zero to\nhero Discord, or on the currently more active #llmdotc on CUDA MODE\nDiscord.fp32 CUDA version plansWe also split off the fp32 CUDA code into its\nown file train_gpt2fp32.cu, which will become pure CUDA kernels only (no\ncublas or cudnn or etc), and which we think would make a really nice endpoint\nof a CUDA course. You start with the gpt2.c pure CPU implementation, and see\nhow fast you can make it by the end of the course on GPU, with kernels only\nand no dependencies.Fine printAll measurements done on:\n\n    \n    \n    A100 40GB PCIe GPU on Lambda Ubuntu 22.04.3 LTS NVIDIA driver version 535.129.03 CUDA Version: 12.2\n\nllm.c: at ~167K tok/s (on SOTA PR from this morning that is merging\nimminently), on master at ~160K PyTorch code as is on master runs at ~150K\ntok/s (i.e. we are 167/150 ~= 11% faster) If you manually pad the vocab size\nto 50304, tok/s improves ~150K -> ~156K, reducing llm.c speed improvement to\n~7%. Note that padding the vocab is not a trivial matter for GPT-2 in PyTorch.\nYou have to know that having a vocab size of 50257 is bad, and that it should\nbe e.g. 50304 (which %64 = 0), and then because of the token embedding table\nweight sharing with the classifier weights, you have to be very careful that\nyou mask out or somehow set to -inf the padded dimensions, and that you also\nnever use them during sampling. And you have to model surgery if you init with\nOpenAI weights. The original OpenAI GPT-2 code also did not pad the vocab in\nthis way.AcknowledgementsThank you to the excellent Lambda labs for sponsoring\nthis project with GPUs. Lambda labs our favorite, goto place for cloud GPUs \ud83d\ude4f.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nYou must be logged in to vote\n\n## Replies: 3 comments \u00b7 1 reply\n\nComment options\n\n###\n\nsbmaruf\n\nMay 3, 2024\n\n-\n\nOMG! You are actually doing this. Huge props! Do you plan to implement FSDP or\ntensor/pipeline parallel in future?  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nYou must be logged in to vote\n\n1 reply\n\nComment options\n\n####\n\nkarpathy May 3, 2024\n\nMaintainer Author\n\n-\n\nThe PR that is up and will be merged in the next ~0-2 days is Stage 1 of ZeRO.\nFSDP is ~Stage 3 of ZeRO. Long story short yes.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nComment options\n\n###\n\nregrettable-username\n\nMay 3, 2024\n\n-\n\nBig kudos for giving attention to compile times. This is something that is\noften overlooked and can get way out of hand! What insane progress this\nproject has made in the last 2 weeks!! \ud83d\udc4f\ud83d\udc4f\ud83d\udc4f  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nYou must be logged in to vote\n\n0 replies\n\nComment options\n\n###\n\nTayDa64\n\nMay 3, 2024\n\n-\n\nI recall your post about large language models for space? Are there any plans\nto develop in parallel during this project? Outstanding work BTW.  \n---  \n  \nBeta Was this translation helpful? Give feedback.\n\nYou must be logged in to vote\n\n0 replies\n\nSign up for free to join this conversation on GitHub. Already have an account?\nSign in to comment\n\nCategory\n\nGeneral\n\nLabels\n\nNone yet\n\n4 participants\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
