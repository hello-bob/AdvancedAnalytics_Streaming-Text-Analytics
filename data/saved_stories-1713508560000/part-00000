{"aid": "40079304", "title": "Try Llama-3 in a Colab Notebook", "url": "https://colab.research.google.com/drive/135ced7oHytdxu3N2DNe1Z0kqjyYIkDXp?usp=sharing", "domain": "colab.research.google.com", "votes": 3, "user": "danielhanchen", "posted_at": "2024-04-18 18:35:23", "comments": 0, "source_title": "Google Colaboratory", "source_text": "Alpaca + Llama-3 8b full example.ipynb - Colab\n\nclose close\n\nThis notebook is open with private outputs. Outputs will not be saved. You can\ndisable this in Notebook settings .\n\nAlpaca + Llama-3 8b full example.ipynb_\n\nFile\n\nEdit\n\nView\n\nInsert\n\nRuntime\n\nTools\n\nHelp\n\npeople Share settings\n\nSign in\n\nformat_list_bulleted\n\nsearch\n\nvpn_key\n\nfolder\n\ncode\n\nterminal\n\nCode Text Copy to Drive people settings expand_less expand_more\n\nNotebook\n\nmore_horiz\n\nTo run this, press \"Runtime\" and press \"Run all\" on a free Tesla T4 Google\nColab instance!\n\nJoin Discord if you need help + support us if you can!\n\nTo install Unsloth on your own computer, follow the installation instructions\non our Github page here.\n\nYou will learn how to do data prep, how to train, how to run the model, & how\nto save it (eg for Llama.cpp).\n\n[NEW] Llama-3 8b is trained on a crazy 15 trillion tokens! Llama-2 was 2\ntrillion.\n\n\u21b3 0 cells hidden\n\n%%capture\n\nimport torch\n\nmajor_version, minor_version = torch.cuda.get_device_capability()\n\n# Must install separately since Colab has torch 2.2.1, which breaks packages\n\n!pip install \"unsloth[colab-new] @\ngit+https://github.com/unslothai/unsloth.git\"\n\nif major_version >= 8:\n\n# Use this for new GPUs like Ampere, Hopper GPUs (RTX 30xx, RTX 40xx, A100,\nH100, L40)\n\n!pip install --no-deps packaging ninja einops flash-attn xformers trl peft\naccelerate bitsandbytes\n\nelse:\n\n# Use this for older GPUs (V100, Tesla T4, RTX 20xx)\n\n!pip install --no-deps xformers trl peft accelerate bitsandbytes\n\npass\n\n  * We support Llama, Mistral, CodeLlama, TinyLlama, Vicuna, Open Hermes etc\n  * And Yi, Qwen (llamafied), Deepseek, all Llama, Mistral derived archs.\n  * We support 16bit LoRA or 4bit QLoRA. Both 2x faster.\n  * max_seq_length can be set to anything, since we do automatic RoPE Scaling via kaiokendev's method.\n  * [NEW] With PR 26037, we support downloading 4bit models 4x faster! Our repo has Llama, Mistral 4bit models.\n\n\u21b3 0 cells hidden\n\n    \n    \n    from unsloth import FastLanguageModel import torch max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally! dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+ load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n    \n    # 4bit pre quantized models we support for 4x faster downloading + no OOMs. fourbit_models = [ \"unsloth/mistral-7b-bnb-4bit\", \"unsloth/mistral-7b-instruct-v0.2-bnb-4bit\", \"unsloth/llama-2-7b-bnb-4bit\", \"unsloth/gemma-7b-bnb-4bit\", \"unsloth/gemma-7b-it-bnb-4bit\", # Instruct version of Gemma 7b \"unsloth/gemma-2b-bnb-4bit\", \"unsloth/gemma-2b-it-bnb-4bit\", # Instruct version of Gemma 2b \"unsloth/llama-3-8b-bnb-4bit\", # [NEW] 15 Trillion token Llama-3 ] # More models at https://huggingface.co/unsloth\n    \n    model, tokenizer = FastLanguageModel.from_pretrained( model_name = \"unsloth/llama-3-8b-bnb-4bit\", max_seq_length = max_seq_length, dtype = dtype, load_in_4bit = load_in_4bit, # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf )\n\nWe now add LoRA adapters so we only need to update 1 to 10% of all parameters!\n\n\u21b3 0 cells hidden\n\n    \n    \n    model = FastLanguageModel.get_peft_model( model, r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128 target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",], lora_alpha = 16, lora_dropout = 0, # Supports any, but = 0 is optimized bias = \"none\", # Supports any, but = \"none\" is optimized # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes! use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context random_state = 3407, use_rslora = False, # We support rank stabilized LoRA loftq_config = None, # And LoftQ )\n    \n    \n    Unsloth 2024.4 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n\n### Data Prep\n\nWe now use the Alpaca dataset from yahma, which is a filtered version of 52K\nof the original Alpaca dataset. You can replace this code section with your\nown data prep.\n\n[NOTE] To train only on completions (ignoring the user's input) read TRL's\ndocs here.\n\n[NOTE] Remember to add the EOS_TOKEN to the tokenized output!! Otherwise\nyou'll get infinite generations!\n\nIf you want to use the ChatML template for ShareGPT datasets, try our\nconversational notebook.\n\nFor text completions like novel writing, try this notebook.\n\n\u21b3 1 cell hidden\n\n    \n    \n    alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n    \n    ### Instruction: {}\n    \n    ### Input: {}\n    \n    ### Response: {}\"\"\"\n    \n    EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN def formatting_prompts_func(examples): instructions = examples[\"instruction\"] inputs = examples[\"input\"] outputs = examples[\"output\"] texts = [] for instruction, input, output in zip(instructions, inputs, outputs): # Must add EOS_TOKEN, otherwise your generation will go on forever! text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN texts.append(text) return { \"text\" : texts, } pass\n    \n    from datasets import load_dataset dataset = load_dataset(\"yahma/alpaca-cleaned\", split = \"train\") dataset = dataset.map(formatting_prompts_func, batched = True,)\n\n### Train the model\n\nNow let's use Huggingface TRL's SFTTrainer! More docs here: TRL SFT docs. We\ndo 60 steps to speed things up, but you can set num_train_epochs=1 for a full\nrun, and turn off max_steps=None. We also support TRL's DPOTrainer!\n\n\u21b3 4 cells hidden\n\n    \n    \n    from trl import SFTTrainer from transformers import TrainingArguments\n    \n    trainer = SFTTrainer( model = model, tokenizer = tokenizer, train_dataset = dataset, dataset_text_field = \"text\", max_seq_length = max_seq_length, dataset_num_proc = 2, packing = False, # Can make training 5x faster for short sequences. args = TrainingArguments( per_device_train_batch_size = 2, gradient_accumulation_steps = 4, warmup_steps = 5, max_steps = 60, learning_rate = 2e-4, fp16 = not torch.cuda.is_bf16_supported(), bf16 = torch.cuda.is_bf16_supported(), logging_steps = 1, optim = \"adamw_8bit\", weight_decay = 0.01, lr_scheduler_type = \"linear\", seed = 3407, output_dir = \"outputs\", ), )\n\n### Show current memory stats\n\nedit\n\n    \n    \n    #@title Show current memory stats gpu_stats = torch.cuda.get_device_properties(0) start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3) print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\") print(f\"{start_gpu_memory} GB of memory reserved.\")\n\nShow code\n\n    \n    \n    GPU = Tesla T4. Max memory = 14.748 GB. 5.668 GB of memory reserved.\n    \n    \n    trainer_stats = trainer.train()\n\n### Show final memory and time stats\n\nedit\n\n    \n    \n    #@title Show final memory and time stats used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3) used_memory_for_lora = round(used_memory - start_gpu_memory, 3) used_percentage = round(used_memory /max_memory*100, 3) lora_percentage = round(used_memory_for_lora/max_memory*100, 3) print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\") print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\") print(f\"Peak reserved memory = {used_memory} GB.\") print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\") print(f\"Peak reserved memory % of max memory = {used_percentage} %.\") print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n\nShow code\n\n    \n    \n    424.2005 seconds used for training. 7.07 minutes used for training. Peak reserved memory = 8.982 GB. Peak reserved memory for training = 3.314 GB. Peak reserved memory % of max memory = 60.903 %. Peak reserved memory for training % of max memory = 22.471 %.\n\n### Inference\n\nLet's run the model! You can change the instruction and input - leave the\noutput blank!\n\n\u21b3 3 cells hidden\n\n    \n    \n    # alpaca_prompt = Copied from above FastLanguageModel.for_inference(model) # Enable native 2x faster inference inputs = tokenizer( [ alpaca_prompt.format( \"Continue the fibonnaci sequence.\", # instruction \"1, 1, 2, 3, 5, 8\", # input \"\", # output - leave this blank for generation! ) ], return_tensors = \"pt\").to(\"cuda\")\n    \n    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True) tokenizer.batch_decode(outputs)\n    \n    \n    Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n    \n    \n    ['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nContinue the fibonnaci sequence.\\n\\n### Input:\\n1, 1, 2, 3, 5, 8\\n\\n### Response:\\n13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025']\n\nYou can also use a TextStreamer for continuous inference - so you can see the\ngeneration token by token, instead of waiting the whole time!\n\n\u21b3 0 cells hidden\n\n    \n    \n    # alpaca_prompt = Copied from above FastLanguageModel.for_inference(model) # Enable native 2x faster inference inputs = tokenizer( [ alpaca_prompt.format( \"Continue the fibonnaci sequence.\", # instruction \"1, 1, 2, 3, 5, 8\", # input \"\", # output - leave this blank for generation! ) ], return_tensors = \"pt\").to(\"cuda\")\n    \n    from transformers import TextStreamer text_streamer = TextStreamer(tokenizer) _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n    \n    \n    Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n    \n    \n    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request. ### Instruction: Continue the fibonnaci sequence. ### Input: 1, 1, 2, 3, 5, 8 ### Response: 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025, 121393, 196418, 317811, 514229, 832040, 1346269, 2178309, 3524578, 5702887, 9227465, 14930352, 24157817, 39088169, 632459\n\n### Saving, loading finetuned models\n\nTo save the final model as LoRA adapters, either use Huggingface's push_to_hub\nfor an online save or save_pretrained for a local save.\n\n[NOTE] This ONLY saves the LoRA adapters, and not the full model. To save to\n16bit or GGUF, scroll down!\n\n\u21b3 5 cells hidden\n\n    \n    \n    model.save_pretrained(\"lora_model\") # Local saving # model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n\nNow if you want to load the LoRA adapters we just saved for inference, set\nFalse to True:\n\n\u21b3 0 cells hidden\n\n    \n    \n    if False: from unsloth import FastLanguageModel model, tokenizer = FastLanguageModel.from_pretrained( model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING max_seq_length = max_seq_length, dtype = dtype, load_in_4bit = load_in_4bit, ) FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n    \n    # alpaca_prompt = You MUST copy from above!\n    \n    inputs = tokenizer( [ alpaca_prompt.format( \"What is a famous tall tower in Paris?\", # instruction \"\", # input \"\", # output - leave this blank for generation! ) ], return_tensors = \"pt\").to(\"cuda\")\n    \n    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True) tokenizer.batch_decode(outputs)\n    \n    \n    Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n    \n    \n    [\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is a famous tall tower in Paris?\\n\\n### Input:\\n\\n\\n### Response:\\nOne of the most famous tall towers in Paris is the Eiffel Tower. It is a wrought iron tower located on the Champ de Mars in Paris, France. It was built in 1889 as the entrance to the 1889 World's Fair, and it was designed by the French engineers Gustave Eiff\"]\n\nYou can also use Hugging Face's AutoModelForPeftCausalLM. Only use this if you\ndo not have unsloth installed. It can be hopelessly slow, since 4bit model\ndownloading is not supported, and Unsloth's inference is 2x faster.\n\n\u21b3 0 cells hidden\n\n    \n    \n    if False: # I highly do NOT suggest - use Unsloth if possible from peft import AutoPeftModelForCausalLM from transformers import AutoTokenizer model = AutoPeftModelForCausalLM.from_pretrained( \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING load_in_4bit = load_in_4bit, ) tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")\n\n### Saving to float16 for VLLM\n\nWe also support saving to float16 directly. Select merged_16bit for float16 or\nmerged_4bit for int4. We also allow lora adapters as a fallback. Use\npush_to_hub_merged to upload to your Hugging Face account! You can go to\nhttps://huggingface.co/settings/tokens for your personal tokens.\n\n\u21b3 1 cell hidden\n\n    \n    \n    # Merge to 16bit if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",) if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n    \n    # Merge to 4bit if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",) if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n    \n    # Just LoRA adapters if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",) if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")\n\n### GGUF / llama.cpp Conversion\n\nTo save to GGUF / llama.cpp, we support it natively now! We clone llama.cpp\nand we default save it to q8_0. We allow all methods like q4_k_m. Use\nsave_pretrained_gguf for local saving and push_to_hub_gguf for uploading to\nHF.\n\nSome supported quant methods (full list on our Wiki page):\n\n  * q8_0 - Fast conversion. High resource use, but generally acceptable.\n  * q4_k_m - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n  * q5_k_m - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n\n\u21b3 3 cells hidden\n\n    \n    \n    # Save to 8bit Q8_0 if False: model.save_pretrained_gguf(\"model\", tokenizer,) if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n    \n    # Save to 16bit GGUF if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\") if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n    \n    # Save to q4_k_m GGUF if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\") if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n\nNow, use the model-unsloth.gguf file or model-unsloth-Q4_K_M.gguf file in\nllama.cpp or a UI based system like GPT4All. You can install GPT4All by going\nhere.\n\n\u21b3 0 cells hidden\n\nAnd we're done! If you have any questions on Unsloth, we have a Discord\nchannel! If you find any bugs or want to keep updated with the latest LLM\nstuff, or need help, join projects etc, feel free to join our Discord!\n\nSome other links:\n\n  1. Zephyr DPO 2x faster free Colab\n  2. Llama 7b 2x faster free Colab\n  3. TinyLlama 4x faster full Alpaca 52K in 1 hour free Colab\n  4. CodeLlama 34b 2x faster A100 on Colab\n  5. Mistral 7b free Kaggle version\n  6. We also did a blog with \ud83e\udd17 HuggingFace, and we're in the TRL docs!\n  7. ChatML for ShareGPT datasets, conversational notebook\n  8. Text completions like novel writing notebook\n\nSupport our work if you can! Thanks!\n\n\u21b3 0 cells hidden\n\nColab paid products - Cancel contracts here\n\nmore_horiz\n\nmore_horiz\n\nmore_horiz\n\nLocate in Drive\n\nNew notebook\n\nOpen notebook\n\nUpload notebook\n\nRename\n\nMove\n\nMove to trash\n\nSave a copy in Drive\n\nSave a copy as a GitHub Gist\n\nSave a copy in GitHub\n\nSave\n\nSave and pin revision\n\nRevision history\n\nDownload \u25ba\n\nPrint\n\nDownload .ipynb\n\nDownload .py\n\nUndo\n\nRedo\n\nSelect all cells\n\nCut cell or selection\n\nCopy cell or selection\n\nPaste\n\nDelete selected cells\n\nFind and replace\n\nFind next\n\nFind previous\n\nNotebook settings\n\nClear all outputs\n\ncheck\n\nTable of contents\n\nNotebook info\n\nExecuted code history\n\ncheck\n\nComments sidebar\n\nCollapse sections\n\nExpand sections\n\nSave collapsed section layout\n\nShow/hide code\n\nShow/hide output\n\nFocus next tab\n\nFocus previous tab\n\nMove tab to next pane\n\nMove tab to previous pane\n\nCode cell\n\nText cell\n\nSection header cell\n\nScratch code cell\n\nCode snippets\n\nAdd a form field\n\nRun all\n\nRun before\n\nRun the focused cell\n\nRun selection\n\nRun after\n\nInterrupt execution\n\nRestart session\n\nRestart session and run all\n\nDisconnect and delete runtime\n\nChange runtime type\n\nManage sessions\n\nView resources\n\nView runtime logs\n\nCommand palette\n\nSettings\n\nKeyboard shortcuts\n\nDiff notebooks (opens in a new tab)\n\nFrequently asked questions\n\nView release notes\n\nSearch code snippets\n\nReport a bug\n\nReport Drive abuse\n\nSend feedback\n\n", "frontpage": false}
