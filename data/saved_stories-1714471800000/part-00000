{"aid": "40207152", "title": "W3C's Excessive DTD Traffic (2008)", "url": "https://www.w3.org/blog/2008/w3c-s-excessive-dtd-traffic/", "domain": "w3.org", "votes": 2, "user": "aragonite", "posted_at": "2024-04-30 04:07:55", "comments": 0, "source_title": "W3C's Excessive DTD Traffic", "source_text": "W3C's Excessive DTD Traffic | 2008 | Blog | W3C\n\nSkip to content\n\n# W3C's Excessive DTD Traffic\n\nPart of Systems\n\n## Author(s) and publish date\n\nBy:\n\n    \n\n  * Ted Guild\n\nPublished:\n\n    8 February 2008\n\nSkip to 105 comments\n\nIf you view the source code of a typical web page, you are likely to see\nsomething like this near the top:\n\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\"\n\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\n\nand/or\n\n<html xmlns=\"http://www.w3.org/1999/xhtml\" ...>\n\nThese refer to HTML DTDs and namespace documents hosted on W3C's site.\n\nNote that these are not hyperlinks; these URIs are used for identification.\nThis is a machine-readable way to say \"this is HTML\". In particular, software\ndoes not usually need to fetch these resources, and certainly does not need to\nfetch the same one over and over! Yet we receive a surprisingly large number\nof requests for such resources: up to 130 million requests per day, with\nperiods of sustained bandwidth usage of 350Mbps, for resources that haven't\nchanged in years.\n\nThe vast majority of these requests are from systems that are processing\nvarious types of markup (HTML, XML, XSLT, SVG) and in the process doing\nsomething like validating against a DTD or schema.\n\nHandling all these requests costs us considerably: servers, bandwidth and\nhuman time spent analyzing traffic patterns and devising methods to limit or\nblock excessive new request patterns. We would much rather use these assets\nelsewhere, for example improving the software and services needed by W3C and\nthe Web Community.\n\nA while ago we put a system in place to monitor our servers for abusive\nrequest patterns and send 503 Service Unavailable responses with custom text\ndepending on the nature of the abuse. Our hope was that the authors of\nmisbehaving software and the administrators of sites who deployed it would\nnotice these errors and make the necessary fixes to the software responsible.\n\nBut many of these systems continue to re-request the same DTDs from our site\nthousands of times over, even after we have been serving them nothing but 503\nerrors for hours or days. Why are these systems bothering to request these\nresources at all if they don't care about the response? (For repeat offenders\nwe eventually block the IPs at the TCP level as well.)\n\nWe have identified some of the specific software causing this excessive\ntraffic and have been in contact with the parties responsible to explain how\ntheir product or service is essentially creating a Distributed Denial of\nService (DDoS) attack against W3C. Some have been very responsive, correcting\nthe problem in a timely manner; unfortunately others have been dragging on for\nquite some time without resolution, and a number of sources remain\nunidentified.\n\nWe would like to see this issue resolved once and for all, not just for our\nown needs but also to improve the quality of software deployed on the Web at\nlarge. Therefore we have a number of suggestions for those writing and\ndeploying such software:\n\n  * Pay attention to HTTP response codes\n\nThis is basic good programming practice: check your return codes, otherwise\nyou have no idea when something goes wrong.\n\n  * Honor HTTP caching/expiry information\n\nResources on our site are served in a cache-friendly way: our DTDs and\nschemata generally have explicit expiry times of 90 days or more, so there's\nno reason to request these resources several times a day. (In one case we\nnoticed, a number of IP addresses at one company were requesting DTDs from our\nsite more than three hundred thousand times per day each, per IP address.)\n\nMark Nottingham's caching tutorial is an excellent resource to learn more\nabout HTTP caching.\n\n  * If you implement HTTP in a software library, allow for caching\n\nAny software that makes HTTP requests to other sites should make it\nstraightforward to enable the use of a cache. Applications that use such\nlibraries to contact other sites should clearly document how to enable\ncaching, and preferably ship with caching enabled by default.\n\nMany XML utilities have the ability to use an XML catalog to map URIs for\nexternal resources to a locally-cached copy of the files. For information on\nconfiguring XML applications to use a catalog, see Norman Walsh's Caching in\nwith Resolvers article or Catalog support in libxml.\n\n  * Take responsibility for your outgoing network traffic\n\nIf you install software that interacts with other sites over the network, you\nshould be aware how it works and what kind of traffic it generates. If it has\nthe potential to make thousands of requests to other sites, make sure it uses\nan HTTP cache to prevent inflicting abuse on other sites. If the software\ndoesn't make it straightforward to do so, file a bug report with the vendor,\nseek alternatives, or use an intercepting proxy server with a built-in cache.\n\n  * Don't fetch stuff unless you actually need it\n\nJudging from the response to our 503 errors, much of the software requesting\nDTDs and schemata from our site doesn't even need them in the first place, so\nrequesting them just wastes bandwidth and slows down the application. If you\ndon't need it, don't fetch it!\n\n  * Identify your user agents\n\nWhen deploying software that makes requests to other sites, you should set a\ncustom User-Agent header to identify the software and provide a means to\ncontact its maintainers. Many of the automated requests we receive have\ngeneric user-agent headers such as Java/1.6.0 or Python-urllib/2.1 which\nprovide no information on the actual software responsible for making the\nrequests.\n\nSome sites (e.g. Google, Wikipedia) block access to such generic user-agents.\nWe have not done that yet but may consider doing so.\n\nIt is generally quite easy to set a custom User-Agent with most HTTP software\nlibraries, see for example How to change the User-Agent of Python's urllib.\n\nWe are interested in feedback from the community on what else we can do to\naddress the issue of this excessive traffic. Specifically:\n\n  * Do we need to make our specifications clearer in terms of HTTP caching and best practices for software developers?\n\nYou might think something like \"don't request the same resource thousands of\ntimes a day, especially when it explicitly tells you it should be considered\nfresh for 90 days\" would be obvious, but unfortunately it seems not.\n\nAt the W3C Systems Team's request the W3C TAG has agreed to take up the issue\nof Scalability of URI Access to Resources.\n\n  * Do you have any examples of specific applications that do things right/wrong by default, or pointers to documentation on how to enable caching in software packages that might be affecting us?\n\n  * What do other medium/large sites do to detect and prevent abuse?\n\nWe are not alone in receiving excessive schema and namespace requests, take\nfor example the stir when the DTD for RSS 0.91 disappeared.\n\nFor other types of excessive traffic, we have looked at software to help block\nor rate-limit requests, e.g. mod_cband, mod_security, Fail2ban.\n\nSome of the community efforts in identifying abusive traffic are too\naggressive for our needs. What do you use, and how do you use it?\n\n  * Should we just ignore the issue and serve all these requests?\n\nWhat if we start receiving 10 billion DTD requests/day instead of 100 million?\n\nAuthors: Gerald Oskoboiny and Ted Guild\n\n## Related to this post\n\n### Tags\n\n  * dtd\n\n## Related RSS feed\n\nSubscribe to our blog feed\n\n## Comments (105)\n\n  1. Steve - 16 years ago\n\nI definitely like the idea of defining which DTD to use through something\nother than a straight HTTP URL.\n\nAnother possible idea is that of decentralizing where the DTDs are actually\nstored, coupled with the above idea. A doctype declaration could then just be\nthe name of the actual type of document. The browser (and supporting\nlibraries) could keep track of \"DTD servers\", which could possibly be set up\nby anyone. This solves both problems, both of having a URI for the doctype and\nfor eliminating the strain of rogue applications who suck up W3's bandwidth.\n\n  2. Poul-Henning Kamp - 16 years ago\n\nInstead of rejecting the requests with 503, it might be a much better strategy\nto serve them, but very very slowly.\n\nFailures and error codes are rutinely ignored in code, but things which\nbasically work will happen, and if things are suddenly incredibly slow, people\ntend to notice.\n\nDrip out the respons at around 10 char every other second, fast enough to keep\nthe TCP connection open and the application running, but slow enough to\ntotally wreck any hope of responsetime in the other end.\n\nOne level nastier, and in general one level too nasty, is to return buggy\ncontents for repeat offenders, hoping to make their applications fail with\ninteresting diagnostics.\n\nPoul-Henning\n\n(Who has far more experience with this problem in NTP context than he ever\nwanted)\n\n  3. Ben Strawson - 16 years ago\n\nOne current problem is that all the DTDs are under http://www.w3.org/ which\nmakes it difficult to distribute them around the Internet. Perhaps if new DTDs\nwere placed at http://dtd.w3.org/ then DNS could be used to return the nearest\nserver to the requesting agent - a technique used on a number of other sites\nof course.\n\nIn the short term, there may be little effect as most documents have\nwww.w3.org in them, but over time people would start using the new hostname\ninstead, and of course newer DTDs would not be available anywhere else.\n\n  4. Martin Nicholls - 16 years ago\n\nYou guys are completely right about this - the number of badly behaved robots,\nspiders and other tools around the internet these days is starting to get\nsilly.\n\nWe block a few of the generic ones like libwww-perl and some others, and yet\nthey keep comming back for more.\n\nWe had another spider looking for RSS feeds that didn't exist going round &\nround in circles and eventually putting enough load on our server for it to\nsend me an alert at 4am so I had to get up and block it's IP range (that's\nanother annoying trend - spiders over massive and disparate IP address ranges\nthat won't go way.. *glares at Slurp*).\n\nThese aren't exactly the same thing, but they are kinda of the same ilk -\nbadly behaved robots doing what they shouldn't.\n\nThere's one more thing that drives me nuts that's kinda part of this family of\nannoyances - robots that pick out URLs that don't exist and aren't linked from\nanywhere. The number of 404 requests we get from robots is insane. What I\nreally wish is that robot developers would use the referer header so we can\nfigure out where they got the URL from.\n\nA little off topic I grant you but it's kinda the same thing at the same time,\nit's time robots were written properly so they don't hit servers in a way that\nseems like an attack - then they might not be treated as such all the time?\n\n  5. Gerald Oskoboiny - 16 years ago\n\nTo try to help put these numbers into perspective, this blog post is currently\n#1 on slashdot, #7 on reddit, the top page of del.icio.us/popular , etc; yet\nwww.w3.org is still serving more than 650 times as many DTDs as this blog\npost, according to a 10-min sample of the logs I just checked.\n\n  6. Douglas W. Goodall - 16 years ago\n\nPlease accept my humble observations. While attempting to learn about W3 and\nthe evolving language of the hyperweb, I had certain misconceptions that must\nbe common. I thought you were in the business of providing certain basic\nreference schemas that were critical to writing well formed web pages int the\ntime of XML and xhtml. My first few compliant web pages were cut and pasted\nversions of example pages from W3. Only now after reading about yor problem in\n/. do I even have a clue that it is not so. I see two ways out of this. The\nfirst is education. You must make it clear how important schemas need to live\nsomewhere else, keeping in mind that existing ones may have to last decades\nfor the sake of legacy documents. The other possibility is that you accept\nresponsibility for hosting these keystone documents, but push them out onto\nakamai servers or some other way of not being the bottleneck of the\nhyperverse. Although this is a terrible hack, your most common existing DTDs\ncould be cached in the browser's themselves, not unlike the root certificates.\nIt is a compliment that peope think you are the center of the Document\nuniverse, if only you can survive it. Thank you very much for your time. Doug\n\n     * Ike - 13 years ago\n\nWholeheartedly agree with this. I considered downloading a copy of the schemas\nI'd wanted and storing them in the \"resources\" directory of the website I was\ncurrently building. This way I could change\n\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\" to\n\"http://mywebsite/resourcespath/xhtml1-strict.dtd\" but unfortunately, I\nreceived a 503. Go figure.\n\n     * Christopher - 13 years ago\n\nDownloading the DTD may or may not be a working idea in practice \u2013 it is\ncertainly not recommended, since the purpose of the URI, as the text above\nclearly says, is *not* to give a place to download some file, it is to\nuniquely identify the DTD. This becomes especially important once you start\ncombining documents using different (or maybe not so different) DTDs, but even\nfor single files, is the best chance of n application to check whether it can\nhandle this file.\n\n  7. Greg Hemphill - 16 years ago\n\nWhat about getting the ISPs and major internet backbones involved. For example\nan ISP could scan the URL traffic and return a locally cached version of the\nDTD.\n\nAlso what if DTD requests were handled more like DNS requests? Where you have\na handful of root servers on the back bones and each ISP has one or more DTD\nservers with cached copies of the DTD schemas. The various DTD servers would\nthen only update their caches when the TTL settings of the DTD schema expire.\n\n     * Christopher - 13 years ago\n\nThat would require deep packet inspection (or even transparent proxies), since\nthe ISP usually does not see the URL. I'd rather my ISP didn't do that to my\ntraffic.\n\n  8. Will - 16 years ago\n\nI think the with a problem as massive as this, defeating it will require\nworking from several different angles at once.\n\nFor starters, I'd try working with the people who make the libraries that get\nused by the offending apps to try and convince them to change their API's. If\nthe python urllib API changed so that you had to use a custome user agent to\ninitialize the library, it'd be easier to track down the rogue apps. (Or at\nleast print a mode-deprecation warning to stderr if you don't give a custom\nUser string) Once you find the bad apples, try to get in touch with the\ndevelopers. Ask them what materials they learned from. Was there a specific\nonline tutorial that taught them to do things badly? If so, try and submit\nsome revised text, so that a second printing will lead fewer astray, or a\ntutorial webpage can update its offerings.\n\nMaybe try to dedicate a small group who spend at least one hour every week on\ngiving the problem publicity. Posting a blog about the problem, submitting an\narticle to slashdot, writing a tutorial on the right way to do things,\nactively searching out tutorials that sopmebody might use if they were\nlearning how to write an app that could have these problems, and asking the\ntutorial to include some discussion of the issue, etc.\n\nAnd, others have suggested making things painfully slow when abusive behavior\nis encountered. I agree with that. Break some apps, force the issue. I'd hate\nyou if your broke one of my apps, but it still is probably the correct thing\nto do.\n\n  9. Guillaume Cot\u00c3\u00a9 - 16 years ago\n\nI think you should insite on librairy provider that a catalog is configure in\nthe default setting. I remember last time I had to set one in Java, it was\ncomplex and time customing. I would find it better if a standard catalog was\nconfigure by default and a empty catalog could be use only when specificaly\nrequired.\n\nMost developper don't have the time to play with these thing.\n\n  10. Andrew - 16 years ago\n\nTarpitting the response sounds like a good idea, but it would require the\ncapacity to sustain hundreds of times more connections than the servers do\npresently. That's not free, either.\n\nRate limiting the requests from individual IPs is more appealing -- it would\njust require some decent packet filtering software in front of the servers,\nstill not free but not a significant cost. This will reduce the bandwidth\nyou're paying for, but it probably won't reduce the connection attempts\nsignificantly.\n\nI think the mistake you're making here is that you only returned 503s for\n\"hours or days\". Broken software is often broken due to neglect. Days is\nnothing. Think months, years maybe.\n\nOK, that's contra to the mission of the w3c, I understand.\n\nHere's an alternative, if rate limiting requests turns out, as I suspect, to\nhave zero effect after months:\n\nAnnounce that the URIs are changing. dtd.w3.org is a good idea1. Wait a few\nmonths. Start returning 301s for the old request paths (far less bandwidth,\nand just as fast or faster than returning the DTD). Broken software that\ndoesn't cache results probably won't follow the redirect (try it for a few\nhours to see). Humans that don't follow the w3c news, but do care, will check\nin a regular browser and see what's going on and update their software. If\nbroken software follows the redirect, send them a special error page with\ninstructions on how to clear their src address from the blacklist.\n\nEven if all of the above fails, you'll still have gained by moving to a new\nhost part of the URI -- as mentioned before, you can distribute the load to\nother hosts in different places, and you can put them behind slow connections\nwithout affecting www.w3.org.\n\n1\\. dtd.w3.org (or any three-character nodename to replace \"www\") is important\nbecause the source to some of the software could be lost or otherwise\nunavailable. Replacing \"www\" with \"dtd\" in a binary editor is simple, but\nchanging it to a string of a different length might not be. Obviously don't\nchange the request path portions of the URIs either. :)\n\nPS: Preview appears to be broken -- any changes I made, and then re-previewed,\nwere lost in both the preview display and the textarea. Hopefully the edits\nwill make it through when I 'Send' for real. If they do, this comment will be\nvisible.\n\n  11. Daniel Barkalow - 16 years ago\n\nI think the main source of the problem is that the XML spec (section 5.1)\nimplies that a validating parser without any persistent state is supposed to\ndo this, and so they mostly do. For example, the default Java SAX handler, if\ngiven an XHTML document, will fetch all of the DTD parts, extremely slowly. Of\ncourse, it'll use a User-Agent like Java/1.6.0, because the application author\nprobably doesn't realize that the application will be making any network\nconnections at all.\n\n  12. David - 16 years ago\n\nYes this is a huge problem. The applications that do these requests slow\nthemselves down and the whole network. There is no need. For Java apps, see\n\"Apache XML Commons resolver\"\nhttp://xml.apache.org/commons/components/resolver/ which developed from Norm\nWalsh's work.\n\n  13. ivan - 16 years ago\n\nI think you have to combine solutions Poul-Henning Kamp and Ben Strawson\nwrote.\n\nI think the good solution is to use DNS load-balancing with a separated domain\n(e.g. schema.w3c.org or dtd.w3c.org) and find partners who can operate\nreliable mirrors (I think you can find easily such partners). To solve current\nproblem you should permanently redirect (http 301) requests to the separated\ndomain. You can slow down serving requests for biggest crawlers.\n\nBeside these solutions maybe you have to contact with major xml library\nvendors to ask them to disable validation or enable caching dtds by default\nand write best practices about validation in their documentation.\n\nI can't understand why somebody use validation this way, it's the slowest I\ncan imagine. :-)\n\nBest regards, ivan\n\n  14. Lyle - 16 years ago\n\nI have to agree with a delay. After a user requests more than 100 in an hour,\nput them on a 2 second delay; after 1,000 a 10 second delay.\n\nThat may alert them to the fact that something is wrong.\n\nAnd for those addresses that hit the 10 second delay and don't seem to notice\nor slow their requests after 3 months, cut 'em off after the first 100\nrequests in an hour.\n\nPersonally, I am also curious about all of the new developer tools for\nfirefox, ie, etc. that perform validation on every page. I hope these are\nusing caching mechanisms, but given the ease with which they can be\nimplemented, they could quickly set up a distributed DOS.\n\n  15. Steve - 16 years ago\n\nIt's good that this blog post is getting attention - I'm sure many of the\ndevelopers of various libraries are starting to take notice. IMO, that's the\nbiggest problem right there - libraries, their documentation, and their\ndefault behavior.\n\nIt's a hard sell for a library developer to include a disk based cache\nmechanism as part of the parsing library when something like that really\nshould be the responsibility of the program making use of the library. If\nevery XML parsing library were to include file io as part of the package, that\nwould be overkill. Certainly publishing tutorials about caching for\nperformance reasons including sample code would be a good thing. Also, memory\nbased caching should really be included by default for those libraries that\ncan.\n\nI think the best way to resolve the problem for w3c would be to create some\ntutorials for using some of the more common parsing libraries that exhibit\nundesired behavior by default and as part of those tutorials show performance\nbenchmarks. As those kinds of things start hitting the top pages for google\nsearches, developers will take notice and start to build better solutions.\n\nI think short term fixes like 503 or even 404 errors will end up doing very\nlittle to resolve your issues long term, and probably will have very little\neffect short term as well.\n\nI also think that setting up a subdomain for dtds is a great idea.\n(dtd.w3.org). It at least opens up some potential solutions down the road.\n(geo-responsive dns, etc).\n\n  16. Nils H - 16 years ago\n\nSome cases like \"Java/1.6.0\" user agent may be that it's a Java applet or\napplication using the standard Java library. Changing the user agent string\nhere may not always be possible and blocking it may cause harm to a lot of\napplications on the net.\n\nOne way that was suggested is to limit the throughput to the resource, and\nthat may be fine, but that should be done dynamically to not introduce\nproblems for \"normal\" applications. To make things worse - in some cases a lot\nof requests may appear from a single IP address, but that address may be a NAT\nfirewall for a large company. Of course - such companies should have a web\ncache.\n\nA limitation in throughput may not even be easy for the application developer\nto track down since the delay may be masked in a library and the developer may\nend up hunting problems in all other places than the DTD link.\n\nThe use of a separate DTD serving address may actually be a good idea, since\nit will allow for a distributed load. The downside is that it will take some\ntime before it becomes effective and that it will require some resources. The\ngood side is that the amount of data served is very small and also very\npredictable, so it's not very problematic to set up such a server.\n\n  17. Michael Daconta - 16 years ago\n\nHi Folks, Sorry but I think the W3C is at fault here. If a namespace should\nnot be retrievable than DON'T use an http URL to identify it. If you want to\nfoster or require the use of caching define some kind of optional cache\nidentifier to define a namespace. In general, I think the identity of a\nresource being retrievable is a good thing. It promotes the idea that the\n\"definition\" of an identified thing can be discovered by retrieving it. That\nmakes sense - the only disconnect here is that we are talking about a \"well-\nknown\" thing like HTML where the logic breaks down. However, for not very-\nwell-known tag sets, this makes a lot of sense. So, recommend you make an\nalternative syntax to specify that a local cached copy exists (or must exist).\nThan you can switch the default to that \"cached copy URI\". If such a caching\nURI exists, my apologies for not researching it before posting. Best wishes,\n\n\\- Mike\n\n  18. Jay - 16 years ago\n\nThe standards encourage this behavior. The 4.01 spec says, \"The URI in each\ndocument type declaration allows user agents to download the DTD and any\nentity sets that are needed.\" And the XHTML spec says, \"If the user agent\nclaims to be a validating user agent, it must also validate documents against\ntheir referenced DTDs\". General purpose SGML or XML parsers will not embed the\nHTML DTDs, and even if they have the luxury of a cache it isn't likely to\npersist across process invocations. There is obviously a software component to\nthis problem, and developers need to be aware. But as you point out, the\nproblem is not limited to the W3C. The best way forward will be to improve\ninfrastructure, and in particular, to find sustainable caching strategies. I\nwould like to think that the Scalability TAG will come up with solutions, but\nthe emails are not encouraging.\n\n  19. uv - 16 years ago\n\nTrack the IP to a person (use a court order if necessary) and find out what is\nthe offending software. Then sue them for abuse. One case like that and you\nwould get much more publicity than via slashdot.\n\nTrack the IP to an ISP and ask them to install a transparent proxy for your\nsite, or to contact their user and tell them to configure one.\n\nConvince Sun that the next update of Java 6 (and Apache commons) would install\na local cache. Same for Python urllib2 and Perl's libwww.\n\nI like the idea of slowing down offensive connections, but since that may be\nhard on the server level, you can just return a wrong DTD. Make it have a\nvalid syntax, so the DTD parser would not fail, but contain no real elements.\nIf that fails, use a completely invalid DTD.\n\n  20. Paul Boddie - 16 years ago\n\nIf we ignore the intentional \"dual purpose\" of the URLs concerned (that they\nshould be used as a unique identifier, yet can also be used to consult the\nDTD), probably the biggest reason why the URLs are getting so many hits is\nbecause many parsing toolkits have bad defaults: that for many implementations\nof APIs like SAX, you have to go out of your way to override various\n\"resolver\" classes in order for your application not to go onto the network\nand retrieve stuff. So it's quite possible that most people don't even know\nthat their code is causing network requests until their application seems to\nbe freezing for an unforeseen reason which they then discover to be related to\nslow network communications.\n\nMy first experience with excessive network usage probably arose with various\nJava libraries, but it's true that Python's standard library has similar\nmechanisms, and if you look at tools like xsltproc there are actually command\nswitches called --nonet and --novalid, implying that you'll probably fetch\nDTDs with that software unless you start using these switches.\n\nWho is responsible? Well, I don't think you can put too much blame on the\napplication authors. If using XML technologies has to involve a thorough\nperusal of the manual in order to know which switches, set in the \"on\"\nposition by default, have to be turned off in order for the application to\nbehave nicely, then the people writing the manual have to review their own\ndecisions.\n\nSome clearer language in various specifications would help, rather than having\nto read around the committee hedging their bets on most of the issues the\nwhole time.\n\n  21. Gerald Oskoboiny - 16 years ago\n\nThanks to everyone for your comments. I'll try to reply in more detail later,\njust a quick followup for now:\n\nThe tarpitting idea sounds worth trying; does anyone have specific software to\nrecommend that is able to keep tens of thousands of concurrent connections\nopen on a typical cheap Linux box?\n\n  22. Sander Bos - 16 years ago\n\nAs Arman Sharif says you would say that the W3C would make it easy to download\nthe schema-files they don't want you to access directly. But it's my\nexperience that the W3C does not offer this properly.\n\nI actually wrote some software that reads XHTML documents into XML DOMs. As\nsoon as the XML parser encounters an entity reference the URL will be loaded.\nSo I created a local resolving mechanism with an entity resolver to read the\nDTDs from local, however: - I had to go to all the individual specifications\nand download the individual specs there, and create my own full repository (I\ndon't have the source-code here, but I am quite sure I ended up with over 50\nfiles for 5 or specs). - Create my own mapping file that goes from public\n(handling broken in .Net XML parsers) and system ids to my local files. - And\nthen of course implement entity resolving to actually pick up my local files.\n\nEvery time a developer implements an application that loads html documents\nusing a standard XML parser (a quite common thing I would say), they need to\nperform these steps to alleviate stress on the W3C servers.\n\nWhat I actually naively expected this article (found from slashdot) to contain\nwhen I opened it was a link to an archive with the files for all your stable\nspecifications in one, with a id->path mapping, and some sample resolver code\nfor common parser libraries in various languages. Does it exists?\n\n(and caching it after one request is not usable for many situations, since my\nreason for caching was actually not to lessen W3C's Internet bill but allow\nthe application to run without Internet access)\n\n  23. Peufeu - 16 years ago\n\n130 million requests a day is about 1500 per second. On a few static files. A\n3 years-old laptop running lighttpd can manage that easily (and actually a lot\nmore). On my server which is the cheapest Core2, lighttpd handles 200 req/s\nand it uses a few percent CPU.\n\nJust use lighttpd or nginx (obviously you should forget about Apache !)\n\nNow here is my suggestion to get rid of the spamming.\n\nYou need two servers, a main server and a backup server. Both are machines\nsuitable for serving a few thousands static requests per second, ie. the\ncheapest Core 2 boxes you can get. You will need to adjust the allowed number\nof sockets on both, of course, to allow as many concurrent connections as you\ncan. Don't forget to enable zlib compression.\n\nNow you implement some redirections : - When www.w3.org sees a request for a\nDTD, it redirects to dtd.w3.org which runs on the \"main\" box. - The \"main\" box\nhas a good connection (like 100 Mbits) - When the \"main\" box receives a HTTP\nrequest, it looks up the client IP address. If this address has submitted few\nrequests, it serves the requests. However, if this IP has submitted say, more\nthan 5 requests in a period of a few hours, it redirects it to the \"backup\"\nserver, which is dtd2.w3.org, on a different IP.\n\nTo implement this you will need to code a simple C module for lighttpd.\n\n\\- The \"backup\" server is connected to the internet via a completely separate,\nrather slow (10 Mbits) connection. It just serves static files.\n\nSo, the \"main\" server will always be fast and responsive, and the \"backup\"\nserver will always have its connection horribly saturated.\n\nTherefore, any client will get fast response on the first request from the\n\"main\" server. Well behaving clients will cache it, and it ends there. Badly\nbehaving clients will not cache it and will request again, they will get\nredirected to the \"backup\" server and feel the pain.\n\n  24. Ted Guild - 16 years ago\n\nThank you all for the comments. Scaling is not the long term solution as it\ndoes not address the cause, however it is something we will have to continue\nto do and appreciate suggestions made in this area.\n\nBy making this post we are trying to increase awareness so ideally this gets\nresolved as far upstream at the library level as possible since that will have\nthe broadest effect. Community involvement with their respective development\nplatforms of choice will help as we have had mixed success in identifying and\ncontacting software and library maintainers. Some have been very responsive\nand are implementing caching catalogs or making static catalogs a default\ninstead of an afterthought left to those installing and utilizing the library.\nSome developers have noticed our blocking scheme and have contacted us letting\nus know they have taken corrective steps on their own.\n\nFor those who wondered why these schemata and namespace resources are made\navailable via HTTP to begin with is we intend for them to be derefenced as\nneeded but expected this to be more reasonable given the caching directives in\nthe HTTP spec. The performance cost of going over the network repeatedly for\nthe same thing should be reason enough for developers to cache. Since many of\nthese systems ignore response codes a tarpit solution might very well succeed\nin gaining their attention, plus has some entertainment value. If their\napplication performance suffers substantially enough developers may take\nnotice.\n\nAs mentioned many of these systems only understand a small handful of the\nvarious HTTP responses (200 OK, 302 Found, 401 Unauthorized, 403 Forbidden,\n404 Not Found). We are more than slightly curious how the browser plugins, in\ncases we have for instance suspected a particular large scale ISP's webmail\nplugin is a traffic culprit, would handle HTTP 401 \"Authorization Required\"\nresponses to their requests. Inside the realm part of the WWW-Authenticate\nheader it would be quite tempting to give the technical support phone number\nto the ISP who has not listened to our repeated phone calls and emails on the\nsubject. That would likely get their attention and potentially encourage them\nto correct the plugin.\n\nIdentifying the sources of W3C's abusive DTD traffic can be quite time\nconsuming and difficult depending on the data in the HTTP request. One rather\nodd case we see often has the HTTP Referrer as\n\"file:///C:/WINDOWS/fonts/set.ttf\" and we have so far not found the related\nsoftware. For identifying some we have found resources provided by various\norganizations (eg McAfee SiteAdvisor) that catalog browser plugins, software\nnetwork interactions and viruses quite helpful. We would very much like to\ncollaborate with such organizations or similar community efforts to help us\nidentify more software responsible for this traffic. We have made a couple\nefforts to establish contacts within a couple such organizations but\nunfortunately emails and phone calls have not gone very far. Specific\nsuggestions or contacts for us to follow up with would be appreciated.\n\n  25. Ted Guild - 16 years ago\n\nMartin Nicholls,\n\nI cannot agree more with your sentiment towards poorly behaving bots/crawlers,\nthey are getting out of hand. There has been talk around here at W3C and\nelsewhere on starting an activity for directives governing bot interactions\nwith a website. There have been some scattered conventions which should be\nstandardized and improved upon.\n\nFor instance polite bots could\n\nidentity themselves properly including URI with various information\n\nhow to submit complaints means to authenticate crawler and IP range it is\ncoming from\n\nrespect directives regarding frequency, concurrency, etc. given in response\nheaders of site being crawled advertise peak hours with higher thresholds if\nthe crawler would like to schedule it's return server being crawled could make\navailable data of resources and last modified dates so more intelligent,\nminimal crawls can be made saving both sides resources\n\nThose bots that do not abide by these conventions and overstep the boundaries\nspelled out can be spotted and blocked through automated means.\n\nThose that do could do their indexing in as efficient a manner as is\ncomfortable for the website being crawled.\n\n  26. Joshua - 16 years ago\n\nWhy not change the doctype tag to something like:\n\n<!DOCTYPE PUBLIC html \"-//W3C//DTD XHTML 1.0 Strict//EN\"\n\"dtd://TR/xhtml1/DTD/xhtml1-strict.dtd\">\n\nThen it's known where the dtd is located in case it's actually needed, but\nwould only be used by applications and libraries that would probably need it,\nsince it would require special handling instead of blind handling. Slowing\ntraffic for the http link wouldn't be great, because on a few occasions I've\ndownloaded the DTD's for learning the document types(you do want valid html\ninstead of what most tutorials provide, right?).\n\nStill unique, still locatable, and hard to misinterpret.\n\n  27. Stephen Fletcher - 16 years ago\n\nThere is some good smtp tarpit software in OpenBSD. It isnt hard to change the\nBSD specific c calls to linux based calls using the same functions from Rsync.\nI guess it would need to be modified for http as well however.\n\n  28. Clyde Kessel - 16 years ago\n\nServing requests very very slowly will unduly penalize the innocent\nbystanders. Many of us have no idea where to look when an application has slow\nresponse. Our systems are running software from dozens of vendors, and we will\nhave no idea which of the vendors is running slowly. So we will just suffer.\nOr, we will decide our computer is old and needs to be replaced with a faster\none: One which can hit your website even more frequently.\n\n  29. JR - 16 years ago\n\nI've had similar problems with dumb crawlers that couldn't handle escaped '&'\nentities in URLs and would bombard the server with invalid requests for them.\nSo I have sympathy.\n\nHowever, I don't know if the tarpit solution is a good idea. What's John Q.\nPublic who's running some misbehaving software going to think? \"Oh, this must\nbe what they mean when they say XML is slow. This problem never happened\nbefore I put the DOCTYPE on all my files. That guy who pushed us to adopt\nXHTML is a moron.\" Fix the problem going forward by changing the scheme for\nidentifying DTDs, but think carefully before spreading the pain just to save\nthe W3C some inconvenience.\n\n  30. Martin v. L\u00f6wis - 16 years ago\n\nI would like to point out that, contrary to what Ted says, the system\nidentifier *is* a downloadable resource, not just an idenitification (unlike\nthe namespace URI, which is mere identification). Specifically, XML 1.0,\nsection 4.4.3, says that a parser MUST \"include\" references to parsed entities\nif they are validating (so it's no option not to read the DTD), and it MAY do\nso even if non-validating.\n\nIn particular, reading the DTD is necessary even in non-validating mode in\ncase the document contains entity references.\n\nOf course, to read the DTD, one might be able to use an alternate URL based on\nthe public identifier. Unfortunately, catalogs are not in wide-spread use, and\nW3C does nothing to promote them.\n\n  31. Ted Guild - 16 years ago\n\nMartin,\n\nWhere did I say System Ids are not downloadable resources? This post is about\nthe frequency of the downloads, disregarding HTTP caching directives.\n\n  32. Daniel - 16 years ago\n\nTed, I think Martin was referring to the following excerpt, which does sound\nlike \"these URIs are identifiers, not for download\". Note that these are not\nhyperlinks, these URIs are used for identification. This is a machine-readable\nway to say -this is HTML-. In particular, software does not usually need to\nfetch these resources, and certainly does not need to fetch the same one over\nand over\n\nAs many have pointed, the data downloaded is needed, so it'd be great if W3C\ncould provide basic catalogs/suggestions to be used as the sane default.\n\nYou have to keep in mind that caching is a great solution for many given\nscenarios, but low level tools/libraries cannot be expected to assume\ncaching/catalogs are THE right thing to do when the spec includes checking\nagainst what the URI point to.\n\nYou saying that developers were supposed to implement caching due to their own\nperformance concerns makes me wonder: what if most already do that? What will\nchange when they do?\n\nWhat if most hits you get are from software that scraps \"http://[...]\" from\ndata and follow that? What if library per-process/thread cache is already\nthere but the system forks for each URL? How about distributing batches of\nURLs to visit?\n\nSo IMO the W3C has a chance of simply postponing the issue if no steps are\ntaken towards providing local, reliable catalogs to the community and changing\nthe recommended http:// URIs to something else (like the dtd:// above).\n\n  33. Ted Guild - 16 years ago\n\nDaniel, A misunderstanding then, my apologies for us being ambiguous. We went\nwith that wording to avoid going into the differences between DTDs and\nnamespaces which parsers have no need to dereference as there may not be\nanything of use to them as is the case with\nxmlns=\"http://www.w3.org/1999/xhtml\". DTDs are meant to be downloaded for\nmachine processing but reasonably not incessantly by an application running on\na machine. We are seeing XML processors grab these even when they are not\nusing them.\n\nMaking catalogs available has come up before and we certainly will consider\nit. Catalogs still would need to make their way into the various tools and\nlibraries, many of which do not come with any. We are just one of the many\norganizations and individuals making these sorts of resources (namespaces,\nDTD) available so tool and library developers will still have to collect\nthese.\n\nIt is difficult for tool and library developers to know what markup that will\nrun through their utilities for validations or transformations not to mention\nnew schemata are always being created. Because of this the best solution is\nfor a caching XML Catalog resolver as I understand is part of Glassfish. The\nlibrary will add DTDs to it's cache as it needs them, caching is part of the\nHTTP protocol.\n\n  34. Daniel - 16 years ago\n\nTed,\n\nThanks a lot for the discussion.\n\nI like the caching idea, but believe distributing the load extends it. The\ngeneral issue with caching is related to where to cache. Local (or in memory.\nper process, etc.) caches will be less efficient than shared ones. Shared\n(system, library) caches will have their own load of issues. So you might end\nwith much nicer libraries and still being hammered by requests.\n\nNotice that scaling up and mirroring amounts to an extremely-shared cache. I\nbelieve having the machinery for mirroring in place (checksums, compressed\nsnapshots, change notifications) could lead to lower level mirroring (dtd-\ndaemon, anyone?).\n\nThe benefits would be: Network admins could save resources that lazy\nprogrammers forgot to (and legacy code would automagically stop being so\nnasty).\n\nUsers could get performance boosts by installing software that tricks dumb\napps to fetch DTDs from a local cache, regardless of upstream actions.\n\nLibrary developers (and even dumb programmers) would have a Darn Easy\u00ae\nrecommended route to caching, as local-ish mirroring and checksums would be\ndiscussed all over the place (and faster, cheaper, tastier).\n\nAlso, maybe you should talk to Coral Content Distribution Network regarding\nforwarding traffic. It might be interesting for them to have such a huge\nsource of input to their research.\n\nOn a meta note, I think it could be very useful to have a central location\n(wiki?) to gather resources and discussions on this issue.\n\n  35. Ted Guild - 16 years ago\n\nOne of the arguments against having caching resolvers in XML libraries has\nbeen this is attainable outside of the library, which it certainly is with a\ncaching proxy server for instance. It is a very worthwhile solution and why we\ngive the caching directives in the first place.\n\nWe have seen a number of corporate and large ISP HTTP proxies hammering us\nbecause of some XML application[s] running behind them. Sometimes the network\nadmins would, if they were responsive at all to us, add caching to their proxy\nsetup or less often track down the parties responsible for the software\ncausing the traffic. More often they would refuse to add caching to their\nproxy or any other action citing cost or complexity. Bandwidth is cheaper than\nequipment and admin time I guess.\n\n  36. St\u00e9phane Bortzmeyer - 16 years ago\n\nIt is strange (and probably an indication of the lack of XML knowledge of many\nposters here) that noone mentioned the best solution on the application side:\ncatalogs. They have been part of SGML and XML for many years, so there is good\nsupport for them. Any XML parser should support catalogs and, then, the DTD\nwould be retrieved on the local disk and not through the network.\n\nhttp://www.oasis-open.org/committees/entity/spec-2001-08-06.html\nhttp://www.sagehill.net/docbookxsl/Catalogs.html\n\n(Of course, there are always broken programs and sites where catalogs will not\nbe installed, so there is a still a need for other measures.)\n\n  37. St\u00e9phane Bortzmeyer - 16 years ago\n\nA few people mentioned testing that the file has been changed or not. The HTTP\nprotocol has a If-Modified-Since header for precisely this purpose and W3C's\nserver honors it. You can set it, for instance, with curl:\n\n% curl -v --time-cond 20080201 -o html.dtd\nhttp://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd ... < HTTP/1.0 304 Not\nModified [No download]\n\n% curl -v --time-cond 20000201 -o html.dtd\nhttp://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd ... < HTTP/1.0 200 OK [And\nthe file is actually downloaded]\n\nOf course, this requires a program that sends it and has a local storage to\nkeep the DTD, but recommending this technique may help (among other techniques\nlike HTTP caching, XML catalogs, terminating the offenders, etc).\n\n  38. Jeroen Pulles - 16 years ago\n\nTed,\n\nAdding to what Daniel already said:\n\nI just happen to be stacking together a new flavor of modular XHTML in the\nspirit of XHTML+RDFa for the backend of a new website I'm working on.\n\nI'm using libxml on MacOS X via MacPorts. MacPorts has a package with HTML4\ndtd's, but not XHTML and it does not supply a catalog with the DTD's. I'll\naccept that I'll have to add a new entry to the catalog, but I'll still have\nto get the DTD's in the first place.\n\nI have three options at this point: download each DTD (module) manually\nthrough my webbrowser at /MarkUp/DTD, let wget crawl the DTD directory, or\ndownload Debian's w3c-dtd-xhtml package and rip the files from that package.\nHardly convenient.\n\nI assume that a lot of developers will even ignore the speed problems whilst\ngetting their new apps to work.\n\nI think it would really help if W3C would package its DTD's in a tar.gz, and\nperhaps even pro-actively work with package maintainers to distribute these\nfiles.\n\nObviously, this will not be a quick fix to your bandwidth problems, but I\nthink it does address the core of the problem: Too many developers are not\naware the inner workings of XML validation (or validating parsing) and assume\n'it just works'.\n\nmy 2 cents\n\n  39. kev - 15 years ago\n\nWell, this could be an example of rogue crawlers, bots and spiders causing an\neffect on the website. Large numbers of crawlers which take on pages and\ncatches everything that seems like a link and cannot analyse the HTML and miss\nignoring the links in the dtd and xmlns.\n\nI agree we can't just depend on w3 to persistently follow up as everyone has\nthe responsibility to help.\n\n  40. Jon Leech - 15 years ago\n\nCan anyone comment on the combination of IE7, Docbook-generated XHTML, and the\nDTD http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd that's imbedded in\nsuch XHTML? We've generated a bunch of man pages built using Docbook 4.3 (see\nthe link at top) which are now all failing in IE because (as best we can\nfigure it out) w3.org is rejecting requests from the IE user agent.\n\nWhile I sympathize with the bandwidth concerns discussed above, the question\nis what can we do about it? We're not the source of the offending app,\nMicrosoft is, but we bear the consequences. Even if MS were to turn around a\ncaching patch quickly, it probably wouldn't get widely deployed for years and\nI imagine the W3C admins will not lift the IE ban until then.\n\nAll I can think of to do on our end is to locally cache the DTD (and the\nentity files it references, IE also tries to fetch those) on our server and\npatch all of the documents to refer to those.\n\nBTW, I don't see any reason this isn't affecting the combination of IE with\nevery Docbook-generated XHTML doc in the world, if they're built using the\nstandard stylesheet distribution.\n\nAre there any other options within our control, that don't require cooperation\nfrom W3C or Microsoft?\n\n  41. Ted Guild - 15 years ago\n\nJon,\n\nSo I notice in these man pages, the main frame (eg\nhttp://www.opengl.org/sdk/docs/man/xhtml/glBindAttribLocation.xml ) is already\nXHTML markup but served with HTTP header Content-Type: application/xhtml+xml\n\nIf you serve this [X]HTML markup as HTML instead of XML, MSIE will not call\nit's XML processor which in turn tries to dereference DTD from us. Try serving\nit with .html extension and/or Content-Type: text/html and your problem should\nbe resolved.\n\n  42. Jon Leech - 15 years ago\n\nHi Ted, If we serve our content as HTML instead of XML, as you suggest, then\nIE will not invoke the MathPlayer plugin to render MathML content and the\npages aren't rendered correctly. Getting MathML displayed properly in the man\npages is pretty much the whole point of this exercise, so I don't think that\nwill work for us. We've modified the man pages to refer to a local cache of\nthe DTD, and that seems to work well enough.\n\n  43. Gummistiefel - 15 years ago\n\nSure, they're ignoring the response status, but I'll betcha most of them are\ndoing synchronous requests. If I were solving this problem for W3C, I'd be\ndelaying the abusers by 5 or 6 *minutes*. Maybe respond to the first request\nfrom a given IP/user agent with no or little delay, but each subsequent\nrequest within a certain timeframe incurs triple the previous delay, or the\nthroughput gets progressively throttled-down until you're drooling it out at\n150bps. That would render the really abusive applications immediately\nunusable, and with any luck, the hordes of angry customers would get the\nvendors to fix their broken software.\n\n  44. Ted Guild - 15 years ago\n\nMicrosoft blog article on how to more efficiently invoke MSXML in your\napplications.\n\n  45. Robin Berjon - 15 years ago\n\nIt's not a complete or ideal solution, but have you considered in-place\nediting of the relevant DTDs to make them smaller while maintaining their\nsemantics? It's unpleasant, but Process allows for it.\n\nTake for instance http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd. It is\n25kb. Instead, replace it with a DTD that only contains:\n\n<!ENTITY % x SYSTEM \"http://dtd.w3.org/xhtml1-strict\">%x\n\nThat's 56 bytes, 455 smaller than the raw version you have to serve to those\nstupid libraries that often don't send Accept-Encoding: gzip (even if they\nsupport it), and still 120 times smaller than the gzip version.\n\nNow this assumes that an important subset of the requests that are made don't\nactually do anything useful with the content and so don't make a second\nrequest to the actual content. I suspect it's worth a shot, or at least worth\ntesting.\n\nIt has the additional advantage that using a different DNS means that you\nmight be able to use load distribution tricks not available to you for the\ngeneral website.\n\nAnyway, just a thought!\n\n  46. Matthias Kraft - 15 years ago\n\nPlease tell people from Saxon not to reload the xhtml.dtd everytime you open\nan internet document with the xpath-document() function.\n\n  47. Keith Mashinter - 15 years ago\n\nThe source of the problem is that the URI actually exists. If the URI did not\nexist, then everything would be forced to implement local caches of the\nrequired files and there would be little sustained traffic to w3.org.\n\nIf the URI is really name and not a resource then it should not match to\nresource, e.g. xhtml1-strict could have been\nwww.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd instead of including the http://\nand then this kind of issue would have been forced to resolve itself earlier\nin everyone's development and test cycles.\n\n  48. Dave - 15 years ago\n\nIn case anyone experiences this issue when using Java facelets(!) look at this\nbug report: https://facelets.dev.java.net/issues/show_bug.cgi?id=352\n\n  49. Ted Guild - 15 years ago\n\nWe are now seeing such extreme surges in traffic that our automatic and manual\nmethods simply cannot keep up. Increases in serving capacity are readily\nconsumed by this traffic and our site becomes overwhelmed. As such we are\ntaking some more drastic temporary measures which we hope to be able to back\ndown shortly. We are sorry for the impact this is causing the community. We\ncontinue experimenting with various methods including some of those suggested\nby posters here.\n\nIf you are impacted file a bug report with the developers of the library or\nutility you use asking them to implement a [caching] catalog solution. You may\nalso put a caching proxy in front of your application for immediate remedy to\nyour situation, populating the cache with a user agent we are not blocking DTD\naccess to.\n\nJava based applications and libraries are presently accounting for nearly\n1/4th of our DTD traffic (in the hundred of millions a day). There is also\nanother more substantial source of traffic which the vendor is working to\ncorrect in the hopefully near future.\n\n  50. Dominique Hazael-Massieux - 15 years ago\n\nTo ensure that Saxon doesn't hit W3C Web site when transforming XHTML content,\nsee: http://saxon.wiki.sourceforge.net/XML+Catalogs\n\n  51. Dan Craft - 15 years ago\n\nTrying to write a well-behaved system using Xerces DOM parser. Seem to be two\nthings I need to do: set the UserAgent to indicated I'm not the raw Java libs,\nand manage a local cache.\n\nI can set the System user.agent, which URLConnection then uses in the request.\nIt appends the Java \"Java/vers\" string to the one I provide, giving e.g.\n\"DSS/1.0 Java/1.6.0_13\". I believe this is the correct format and intention\nfor UserAgent, indicating the primary system and version followed by any\nsubsystem.\n\nYou're still denying this request. Are you searching for the Java identifier\n*anywhere* in the string? That precludes any Java-based system (at least, ones\nnot controlling the headers all the way down, i.e. using most any libraries)\nfrom behaving properly and working.\n\nCaveat: My understanding is limited.\n\n  52. Steve Donie - 15 years ago\n\nFor those that (like me) run into this when using Ant (java build tool) xslt\ntask - take a look at the xslt task manual and the section on xmlcatalog. That\nallowed me to keep the dtd files locally and use them from there. I was\ntransforming XHTML, so that also required downloading several entities files\nas well.\n\n  53. Ted Guild - 15 years ago\n\nDan,\n\nChanging the user-agent is commendable especially if you post it somewhere it\ncan be indexed and people can contact you if there is an issue with it.\n\nInstead of writing something to maintain your own cache look to Xerces XML\nCatalog capabilities which I wish were the default instead of an after\nthought.\n\nFor the time being I am also relaxing the filtering based on your suggestion.\nThere is one particular Java UA that prepends a string that is causing 80\nmillion or so hits/day at present. We contacted them after researching the\nuser-agent used.\n\n  54. Ned Nowotny - 15 years ago\n\nI use a variety of mostly Java libraries and tools. The authoring tools and\nIDEs are generally good about using local cached copies of the DTDs. The\nlibraries and tools like Saxon do not. While there are many historical reasons\nfor why we have the implementations and behavior we have today, the best fix\nis for the library maintainers to enable DTD caching by default. After all, it\nis the libraries that are fetching the documents in the first place. Of\ncourse, for that to work, the W3 will have to still serve the documents so\nthat they can be cached, but that will likely continue to cause the current\nproblem given the long delay likely to occur between the time the libraries\nare changed and the time when they largely replace the currently deployed\nlibraries.In the meantime, I am trying to resolve my own problems by\nimplementing the necessary XML catalogs. However, I am now struggling with the\nproblem of assembling all of the DTDs and related documents I need to cache\nlocally. (My difficulty is not an isolated case. See Validating XHTML Basic\n1.1 (http://people.w3.org/~dom/archives/2009/06/validating-xhtml-basic-1-1/)\nfor one other example.) My first attempt at a catalog simply included the\nXHTML DTDs, but then Saxon complained it could not find xhtml-lat1.ent. So\nthen I needed to retrieve the referenced entities documents. Then I needed to\ndo the same for each DTD I required. Facing the tedious prospect of pulling\ndown each document individually, I went looking for an archive containing all\ncurrent DTDs and related documents. After refining several searches and\nrestricting them to the W3 site in the hope of finding an official\u2014or, at\nleast, semi-official\u2014distribution, I finally located the DTD library\n(http://validator.w3.org/sgml-lib.tar.gz) made available as part of the Markup\nValidation Service (http://validator.w3.org/docs/install.html)\ndistribution.That was too difficult and may not have been the best solution in\nany case. However, it does highlight the need to make available an official\narchive or library distribution and to make it clearly available from\nsomewhere on the home page even if it is only listed on a page referenced from\na \u201cDownloads\u201d link. If you want to encourage people to use local cached\ncatalogs, help make it easier to assemble the necessary documents.\n\n  55. Martin Rattigan - 15 years ago\n\nI arrived at the page\n\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\" by following the\nlink in the first sentence of section \"A.1.2. XHTML-1.0-Transitional\" in the\nW3C document \"XHTMLTM 1.0 The Extensible HyperText Markup Language (Second\nEdition)\", where it states, \"The file DTD/xhtml1-transitional.dtd is a\nnormative part of this specification\". The phrase\n\"DTD/xhtml1-transitional.dtd\" links through to the page in question.An\nannotated version of a DTD is available by following the link in following\nsentence in the same section, but, contrary to what is stated, this is clearly\nnot an annotated version of the first file.The first file (the \"normative\"\npart of the specification) is either the phrase \"see http://w3.org/brief/MTE2\"\nor this page, neither of which, as displayed in my browser window, is well\nformed XML (if I understand the W3C XML specification correctly.)Perhaps the\nW3C would suffer fewer problems of the kind discussed above if it maintained\naccurate online documentation.\n\n  56. Petr Gladkikh - 15 years ago\n\nI am desperately sorry but I am to blame for some hundreds of those millions\nof requests. One of projects I did a while ago used PHP, called \"the best tool\nfor web\" by some. And while I am aware of this problem I found no way to\ndisable schema fetching in PHP without messing with PHP core itself. I had\nneither time to mess with it nor permissions to deploy those fixes. So,\nperhaps, you also need to contact authors of PHP XML parsers to persuade them\nto fix it (because I already gave up).\n\n  57. Vladimir Nicolici - 15 years ago\n\nOK, so, how do I parse a XHTML file using only the JDK? This fails with 503:\n\nDocumentBuilderFactory.newInstance().newDocumentBuilder().parse\n(\"http://www.weststats.com/Items/right_arm/\")\n\nAnd how do I parse it within less than 10 lines of code?\n\nAs a developer, all I see is that code that used to work doesn't work anymore,\nbecause the naughty w3c decided to break it.\n\nYou're a bad w3c. Yes you are! :)\n\nNow, I read the whole thread, I understood it, I understand w3c's position,\nbut as an end user (or end programmer, whatever), I still have to wonder, why\ndo hundreds of programmers have to implement complicated caching techniques\nbecause you didn't see this coming and didn't plan in advance?\n\nJust complain to Xerces and Sun, and maybe Operating System manufacturers, and\nask Sun and/or Xerces to cache such resources, at the JRE installation level.\nOr even better, at the operating system level.\n\nAs for me, I will try to stay as far as possible from w3c \"standards\" if at\nall possible.\n\n  58. Vladimir Nicolici - 15 years ago\n\nOK. Problem fixed. How:\n\nInstalled Squid for Windows, from\nhttp://squid.acmeconsulting.it/download/squid-2.7.STABLE7-bin.zip to C:\\squid.\n\nCopied cachemgr.conf.default, mime.conf.default and squid.conf.default from\nC:\\squid\\etc to cachemgr.conf, mime.conf and squid.conf.\n\nModified this line in squid.conf: http_access allow localnet to: http_access\nallow localhost\n\nRun this commands at a command prompt: c:\\squid\\sbin>.\\squid.exe -i\nc:\\squid\\sbin>.\\squid.exe -z c:\\squid\\sbin>net start squid\n\nModified my Java Application, adding this lines:\nSystem.setProperty(\"http.proxyHost\", \"localhost\");\nSystem.setProperty(\"http.proxyPort\", \"3128\"); System.setProperty(\"http.agent\",\n\"Mozilla/5.0 (Windows; U; Win98; en-US; rv:1.7.2) Gecko/20040803\");\n\nYou can also set the properties from command line, with\n\njava -Dhttp.proxyHost=localhost -Dhttp.proxyPort=3128\n-Dhttp.agent=\"Mozilla/5.0 (Windows; U; Win98; en-US; rv:1.7.2) Gecko/20040803\"\nyourClass\n\nAfter you successfully access the DTDs from your application and the cache is\npopulated, you can remove the fake http.agent line, or replace it with\nsomething useful.\n\n  59. Luke - 15 years ago\n\nI am just trying to make a simple application do an XPath on a valid HTML\ndocument and it automatically pings you for the dtd. Don't block IPs. Get sun\nto fix their lousy library. There is no way that an XPath should create a\nconnection to the internet to collect a w3c. And they don't give you any way\nto disable it. If they just had a nice IOC type framework it would be easy to\nfix, but they are just not that smart.\n\n  60. Brant Gurganus - 15 years ago\n\nGo back and reread your specification. The DTD declaration consists of two\nparts, the public identifier and the system identifier. The system identifier\nis supposed to point to where the local system can find the DTD. It may be a\nbad idea using the w3.org address in all the examples and there may be\nmisbehaving user agents out there, but it is correct behavior for user agents\nto look up the DTD.\n\nThe conformance section for XHTML actually uses the w3.org addresses without\nindicating those can technically change. Learn from that mistake for future\nspecifications.\n\n  61. Kevn - 15 years ago\n\nIt looks like you're now blocking attempts by Microsoft's .Net XmlDocument\nobject. To echo Jon Leech's comment back in February, we are at the mercy of\nwhatever is going on at Microsoft.\n\nOur use case is manipulating an html page using the .Net XML objects. Since\nthe html page contains HTML entities, we need the DOCTYPE reference, but\nrecently that has been generating the 503 error.\n\nDoes anyone know of a work around?\n\n  62. Nifty - 15 years ago\n\nXmlReader and XmlDocument classes in Microsoft .Net Framework try to find the\nspecified url, not only Java.\n\nAnyway, I don't like the DTD identification takes 'http://...' form. It's\nquite mistakable, isn't it?\n\n  63. Ted Guild - 14 years ago\n\nMicrosoft releases fix for Microsoft XML Core Services (MSXML).\n\nFull release information\n\nIf you have a Windows platform being blocked access to W3C, ensure you have\nthis upgrade installed.\n\n  64. Dorian Taylor - 14 years ago\n\nI was surprised recently that a valid XML DOCTYPE declaration required a URI\nin addition to an FPI in a public identifier (unlike SGML which I understand\njust needs an FPI). I too have been bitten by the fervent attempts of Web\nsoftware to dereference DTDs (for instance WebKit ostensibly does not have the\nXHTML+RDFa DTD in its catalogue).\n\nI think, however, that it is entirely reasonable to expect to be able to\ndereference a URL (specifically), especially within a framework that affords\nthe dereferencing of URIs. By my reading, \u00a74.2.2 of the XML spec indeed\ndiscusses the dereferencing of URIs in system identifiers:\n\n> An XML processor attempting to retrieve the entity's content may use any\n> combination of the public and system identifiers as well as additional\n> information outside the scope of this specification to try to generate an\n> alternative URI reference. If the processor is unable to do so, it MUST use\n> the URI reference specified in the system literal. Before a match is\n> attempted, all strings of white space in the public identifier MUST be\n> normalized to single space characters (#x20), and leading and trailing white\n> space MUST be removed.\n\nI cannot speak for the HTML equivalent, except to assume it would ideally\nfollow the same rules as SGML in which a URI may be omitted from the public\nidentifier.\n\nXML Namespaces using HTTP URIs should ideally have something present on the\nother end at the very least as a courtesy, but a sane XML processor should not\nattempt to dereference them on sight.\n\nThis problem was also endemic to RDF before Linked Data gathered steam\u2014there\nwould be HTTP URIs used as identifiers everywhere but relatively few would\ncorrespond to live HTTP resources.\n\nDo I think vendors of Web software would serve their customers better if they\nkept DTD catalogues up to date? Indubitably. Do I think they are doing a\ndisservice to high-traffic targets like the W3C by not including their DTDs?\nAbsolutely. Do I think that complying in this manner is the past of least\nresistance? Unfortunately not.\n\nI think if you mint an ostensibly dereferenceable URI, you should expect\nattempts to dereference it. If you are in the business of dereferencing HTTP\nURIs, however, you should make an attempt to comply with their cache\ndirectives.\n\n  65. Rene - 14 years ago\n\nWhen using Internet-Explorer 7 and browse to a HTML document on a misbehaving\nserver the following may happen...\n\nThe server responds with:\n\nHTTP/1.0 200 OK ... http headers .... Content-Type: text/xml;charset=utf-8 ...\nhttp headers ....\n\n<?xml version=\"1.0\" encoding=\"UTF-8\"?> <!DOCTYPE html PUBLIC \"-//W3C//DTD\nXHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\n<html lang=\"nl\" xmlns=\"http://www.w3.org/1999/xhtml\"> .... the actual html\ncontent ..... </html>\n\nThis is a HTML document encoded as XML and sent with a content-type of\ntext/xml. Internet-Explorer 7 interpretes this as XML and tries to resolve the\nURI \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\".\n\nNow see what happens when IE7 tries to resolve this:\n\nGET http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd HTTP/1.1 Accept: */*\nReferer: http://some-address/some-document.html UA-CPU: x86 Accept-Encoding:\ngzip, deflate User-Agent: Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1;\n.NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30; .NET\nCLR 3.0.4506.2152; .NET CLR 3.5.30729) ... other http headers ...\n\nAnd the answer from W3C:\n\nHTTP/1.0 503 Service Unavailable Date: Thu, 07 Jan 2010 13:41:41 GMT Server:\nApache/2 Content-Location: msie7.asis Vary: negotiate,User-Agent TCN: choice\nRetry-After: 864000 Cache-Control: max-age=21600 Expires: Thu, 07 Jan 2010\n19:41:41 GMT P3P: policyref=\"http://www.w3.org/2001/05/P3P/p3p.xml\" Content-\nLength: 28 Content-Type: text/plain ... other Http headers ...\n\nsee http://w3.org/brief/MTE2\n\nOne solution: Tell Microsoft that this URI should be cached or tell Microsoft\nto create a catalog of DTDs inside its IE7 and IE8 browser. I think it should\nbe possible using the standard windows updates to distribute a patch that\nsolves a large part of this problem.\n\nPlease tell Microsoft that they are part of this problem.\n\n  66. Henrik Solgaard - 14 years ago\n\nI am afraid I am guilty of a few unnecessary requests to W3C DTDs. I just\nrealized that a SaxParser in Java downloads the DTD even if the parser is non-\nvalidating, which is the default. I don't think this is very well documented,\nso I post it here to warn others.\n\nThis code will create a parser that doesn't download the DTDs.\n\nSAXParserFactory factory = SAXParserFactory.newInstance();\nfactory.setFeature(\"http://apache.org/xml/features/nonvalidating/load-\nexternal-dtd\", false); SAXParser parser = factory.newSAXParser();\n\n  67. Doug Forester - 14 years ago\n\nI downloaded the Java JDK from Sun/Oracle website a couple days ago. Included\nin the JDK is a class called DocumentBuilder which provides access to the SAX\nxml parser (you see where this is going?). I compiled and ran a Java program\nwhich builds an object tree from the xml. The run failed in the SAX parser\nwhen processing the following xml: . That caused the parser to try to access\nthis website, getting the 503 error. Can somebody please contact Sun about\nreplacing the SAX parser with one which does not cause the website access? I\ndon't have the slightest idea who to talk to. And can I get a copy of the SAX\npackage which doesn't exhibit the problem, as discussed in the previous post\nby Henrik Solgaard? Thanks.\n\n  68. Lucas Holt - 14 years ago\n\nI filed a bug report with Oracle (sun) on this. I included the URL for this\nblog. Hopefully they will react.\n\nWorkaround:\nfactory.setFeature(\"http://apache.org/xml/features/nonvalidating/load-\nexternal-dtd\", false);\n\n  69. Leif Halvard Silli - 14 years ago\n\nIE8 transforms the code of a page if a X-UA-Compatible HTTP header is served\ntogether with the page. The transformation have several effects: Uppercasing\ntag names (always), inserting a the <META content=\"IE=8.0000\" http-\nequiv=\"X-UA-Compatible\"> element (always). And, if the page contains the HTML5\ndoctype - <!DOCTYPE html>, then IE8 even replaces it with a legacy, non-\nofficial HTML4 doctype:<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01\nTransitional//EN\" \"http://www.w3c.org/TR/1999/REC-\nhtml401-19991224/loose.dtd\">he URL of that doctype, only to reach a page which\nsaid \"Forbidden due to abuse\", with a message to contact staff about how I\nreached that page.Some hunches, based on the fact that Ted said that the\nmisuse of these URIs had only increaseed, since he first posted this\narticle:X-UA-Compatible has increased in popularity The HTML5 doctype has also\nbecome more popular IE8 has become more popular. Chrome Frame has become more\npopular. HTML5 forbids non-stnadard META elements, thus one must use the HTTP\nheader to be valid as HTML5. All the above points to factors that could\nexplain why the abuse has increased, provided that IE8 is involved in this.\n(Note that even when a X-UA-Compatible header is used to request Chrome Frame,\nany IE8 without Chrome Frame installed, will be affected, regardless.)\n\n  70. Elliotte Rusty Harold - 14 years ago\n\nThis post seems to be giving some folks the wrong impression. It is not\npossible to correctly parse most XHTML documents using an XML parser without\nreading the DTD. You can cache the DTD so you don't read it more than once.\nYou can read a local copy instead. You can point the DOCTYPE to a different\ncopy of the DTD. But if you leave out the DTD completely and use a generic XML\nparser, you will lose information such as default attribute values (including\nnamespace declarations) and entity definitions. This is arguably a design flaw\nin XML, but it is one we have to deal with.\n\n  71. wcsungod - 14 years ago\n\nI can't believe it, but Dreamweaver CS5 is causing this error. Here's the\nerror text:\n\nAn exception occurred! Type:NetAccessorException, Message:Could not open file:\nhttp://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\n\nSo DW CS5 attempted to open this \"URL\". Amazing that some developer at Adobe\nwas so clueless (and I have a lot of respect for the quality of development\nthat goes on at Adobe).\n\n  72. Richard - 14 years ago\n\nI just found this thread. I read many of the comments but not all. Apologies\nif this duplicates what someone else has said.\n\nAs some commenters have said the XML spec encourages validating parsers to\ndownload the DTD from the location given, and a document might not be\nidentified correctly if the original DTD is not given with the original URI.\n\nOf course software can and should cache regularly accessed documents that are\nunlikely to change. But the other major error most software designers are\nmaking is not properly understanding or using the standalone declaration. If\nstandalone=\"yes\" the DTD probably doesn't need to be read in most cases. But\nthe standalone declaration has to be set correctly in the document too.\n\n  73. Pete - 13 years ago\n\nI only found this issue after several months of my XML schema validation\nworking fine. Basically I was validating the schema in .NET and assumed the\ndefault settings would be set in an appropriate way. Its unfortunate as I\nthink a lot of developers will get caught out by this the first time they use\nthe validator.\n\n  74. David - 13 years ago\n\nIf the stick is to slow the servers, how about a carrot to go with it?\n\nDTDs easier to find and download in zip files, along with a catalog.xml file.\n\nOK, only half the issue, but it'd help. The other half is setting up a catalog\nresolver to read the catalog file.\n\n     * Rolf - 13 years ago\n\nDitto David's comment. Is there a zip file I can get to 'seed' an off-line\nEntityResolver I am building? Something in the form of an XML Catalog would be\ngreat, but just a .zip of the actual dtd files from w3.org would be useful\ntoo.\n\n  75. Matt Hillsdon - 13 years ago\n\nVarious commenters ask for a ZIP of DTDs. It took me a few minutes to figure\nout where to download them from so I thought it worth posting links:\n\nHTML 4: http://www.w3.org/TR/html401/html40.zip (note no top-level directory\nin ZIP)\n\nXHTML 1.0: http://www.w3.org/TR/xhtml1/xhtml1.zip\n\n  76. RampantRab - 13 years ago\n\nWe implemented an application to analyse a fairly large website's XHTML pages\nfor Accessibility. Part of this process involved reading the XHTML into an\nXmlDocument using standard .Net methods. Because the DTD is stated in the way\nthat it is, these methods automatically looked up the web address. This may be\na noob mistake but there will be lots and lots of noobs who won't realise that\nby doing the above, they are going to be calling the web address.\n\nOur application stopped working and it was only after trawling through\nWireshark analysis that we worked out what was going wrong. We had to report\nthe reason for the failure to the customer (government). I'm sure you can\nimagine the surprise and subsequent guffaws that went around the office when a\nhigh level manager of the customer insisted that we get in touch with the head\nof the W3C to resolve this issue. Needless to say, he was ignored and we\ndeveloped a workaround.\n\nAnyway, in my humble opinion, it is not always the fault of the people who\nmake software that uses the specified location of the DTD to read the XHTML.\nPerhaps a more foolproof method is required.\n\"dtd://TR/xhtml1/DTD/xhtml1-strict.dtd\" to be used perhaps?\n\nOr, maybe getting Oracle and Microsoft to make the standard .Net and Java XML\nparsers create the cache for the DTD out of the box would be most efficient.\n\nAnother idea, although maybe not workable, would be to return a page with a\nset of links to standard code that will cache the DTDs instead of returning\n503 responses.\n\n  77. Jim Stanley - 13 years ago\n\nI apologize in advance, but I have a somewhat dissenting opinion.\n\nIf a schema on w3.org's site references other schemas, which in turn reference\nother schemas that redefine still other schemas, it's a little much to expect\nthe coder or application to know which ones to download. I've tried\ndownloading every reference I can find (in the VoiceXML 2.1 schema) and\nloading them for unit testing is still taking several minutes per file, which\nprobably means that despite my best efforts, your site is still being hit.\n\nIf you put your URL at the top, that URL is gonna get hit. Come up with\nanother address, or an alternative attribute at the top of the schema that\ntells the processing file not to hit you.\n\n  78. Martin Sharratt - 12 years ago\n\nThis is all very well and I can see why you would block IP addresses for badly\nwritten code. Howver the authors of the article are making huge assumptions\nabout the people who might be trying to download your resources. I am\nsuffering the 503 response at present. I'm not a web site author or\nprogrammer, I'm a Unix systems administrator trying to implement a local copy\nof your validator with HTML5 support. To do this, I have to install the\nvalidator.nu application. This won't build as it is attemting to download one\nof your DTD's. I work for a reasonably large University with several thousand\nusers who mostly get onto the Web via a (very large) caching proxy. This\neffectively means that we will all appear to have the same IP address.\n\nI don't have the skills to change the validator.nu application builder and I\ndon't have time to learn Python only to forget it when I never use it again.\nSo what do you suggest? I can't change my external IP address and can't change\nthe code of (probably several hundred) students who may very well be writing\npoor code/html.\n\nIf I can successfully install the validator.nu engine then I may take away a\nconsiderable amount of traffic to both your and their website as all\nconformance checking will be done locally.\n\n  79. Allie Ward - 12 years ago\n\n@MATT: thank you for the links. Yes I searched ZIP of DTDs to download them\nfrom dedicated servers with windows.\n\n     * Martin Sharratt - 12 years ago\n\nFor those that don't have the links - there is a patch to the validator.nu\nbuild script that downloads the DTD's as zipfiles - the patch is available at\nhttps://bitbucket.org/validator/build/issue/2/received-error-retrying\n\nThanks to AlanJ for this information\n\n  80. v1nce - 12 years ago\n\nCould someone post a full C# exemple on how to validate a xml against a xsd\n(and/or DTD ?) with local copies of all/some xsd (including recursive\nconsiderations and possible security issues)\n\nmsdn is a mess (as always...)\n\n     * Salim - 11 years ago\n\nThere is a C# library but it doesn't work too.\n\nhttp://sourceforge.net/projects/w3cmarkupvalida/?source=dlp\n\n  81. Rolf - 12 years ago\n\nHi all.\n\nThis particular issue continues to perplex me. There are a number of XML\nSchemas and DTD documents that are required for general purpose XML processing\n(not just HTML, xhtml, etc), and the 'throttling' has impacted me in a number\nof ways. Despite the availability of web catalog systems, and other resources,\nit is 'painful' and inconvenient to manage.\n\nI have decided to start up a new open-source project (in Java) specifically\ntargeting this type of issue. It is intended to be a 'simple' EntityResolver.\nIt respects the Cache-Control HTTP headers in the response headers in order to\nmaintain an on-disk cache of web resources. Only if the cache-entry goes out\nof date will the source server be re-queried. The initial version is a\nfunctional resolver that is suitable for running in a multi-threaded\nenvironment, and in a way where multiple JVM's can share the same cache\nlocation.\n\nIn other words, given that the w3c.org standard cache-control timeout is 90\ndays, it should be possible to set up a single cache folder for all your Java\nprograms and have each document only pulled once every 90 days from w3c.org.\n\nFurther, I anticipate that future versions will be expanded to allow a 'chain'\nof resolvers where 'catalog' type resolvers can be queried, and only if those\nfail, will the caching resolver be used.\n\nThis should allow for a well-behaved, easy to use, efficient EntityResolver\nsystem.\n\nIf anyone is interested in playing with the code you can grab it from github\nat (Apache 2.0 license) https://github.com/rolfl/Resolver , you can contact me\nthrough github, and feel free to offer suggestions, criticisms, etc.\n\n     * Ted Guild - 12 years ago\n\nRolf,\n\nA caching catalog is the ideal solution and you might want to look at this\nearlier start on the subject.\n\nhttp://norman.walsh.name/2007/09/07/treadLightly\n\nIdeally such a resolver would make its way upstream into JDK. Past lobbying\nefforts for that to happen have not been successful. I will gladly try to\nraise attention to your effort.\n\n     * Rolf - 12 years ago\n\nHi Ted.\n\nThanks for the feedback, and encouragement. I don't think this is the right\nplace for a full discussion on the requirements, etc. I have set up an 'issue'\nand a wiki page for the proposal, and I am hoping to maybe have people comment\non it there.\n\nIt would be great to make it 'official' that way (part of the JRE). I guess I\nset my sights low by 'hoping' this could be an future apache-commons type\ntool.... ;-), and if not that, then I could possibly incorporate it in to the\nJDOM project. Right now it is too early though.\n\nSo, please head over to: https://github.com/rolfl/Resolver/issues/1\n\nIt would be great if you could contact me directly on this too. I would love\nto pick your brains on some ideas.\n\nThanks\n\nRolf\n\n  82. Stephan Kreutzer - 10 years ago\n\nxhtml1-strict.dtd states \"Copyright (c) 1998-2002 W3C (MIT, INRIA, Keio), All\nRights Reserved.\" Therefore, a local copy of the DTD can't be obtained from\nanother source than the W3C with legal permission. If the DTD URI would only\nbe used for identification, there would still be a download attempt to just\nanother W3C URL. For non-XHTML-specific, general XML tools, it is quite\nunlikely that they have local copies or know about URLs of all kinds of (even\ncustom) DTDs, which might additionally be not that static as the XHTML DTDs,\nso they just attempt to download them from somewhere if the URI is an HTTP\nURL.\n\n  83. Stephan Kreutzer - 10 years ago\n\nI've written a Java wrapper in order to do XSLT on the command line. Since the\ntool is intended to be usable for all kinds of XML input, it doesn't have\nspecial handling for XHTML. However, to avoid automatic download attempts,\nI've disabled URL resolving completely, because - as said - those URLs are\nintended as identifiers. As Elliotte Rusty Harold said here on 2010-05-08, it\nis impossible to successfully parse XHTML without the corresponding DTD and\nentity definitions. So I added a mechanism to resolve references locally based\non a configuration file, where a user has to obtain the DTD and entity\ndefinition files from you manually (due to legal restrictions, they're not\nredistributable, so I can't pre-package them with my software). Therefore, I\ndon't do any caching of automatic download attempts, because other DTDs than\nthe XHTML DTD may change more frequently or may become inaccessible, and, as\nsaid, the URL is for identification, not for download.\n\nThis is how I've implemented it: xsltransformator1 (released under GNU AGPL 3\nor any later version).\n\nIf you would change your licensing of the DTD and entity files (at least\npermit redistribution, you may still prohibit modifications to the files), you\nwould do both, increase user experience and lower the traffic to your site.\n\n  84. Ted Guild - 10 years ago\n\nStephan,\n\nWe would very much like to see our schemata included in library and tool\ncatalogs and you certainly can include them.\n\nW3C document license\n\n[[Permission to copy, and distribute the contents of this document, or the W3C\ndocument from which this statement is linked, in any medium for any purpose\nand without fee or royalty is hereby granted, provided that you include the\nfollowing on ALL copies of the document, or portions thereof, that you use:]]\n\n     * Stephan Kreutzer - 10 years ago\n\nThank you very much for your reply! I wasn't aware of the W3C Document License\nand I really appreciate the open approach of the W3C regarding licensing.\nWould then this modified header comment make the XHTML 1.0 Strict DTD freely\nredistributable? If so, I would gladly pre-package it with my XHTML-processing\ntools, so that no user or setup would need to download the DTD from W3C\nservers.\n\n     * Ted Guild - 10 years ago\n\nThat is certainly fine and I'm trying to get clarification, hence the delay,\nwhether a file in same directory instead of comment is sufficient in case you\nfind that preferable.\n\nThat DTD hasn't changed since 2002 and is unlikely to change but you may want\nto check in the future.\n\nIdeal would be XML processors that require these to be stored in a caching\ncatalog and go off caching directives given in HTTP. Packaging manually is\ncertainly better than going over the net thousands of times a day/hour\nwhatever for the same resource so thank you for doing so.\n\n     * Stephan Kreutzer - 10 years ago\n\nTed, thank you very much for your help! I won't put a file with only the\nnotification in the pre-packaged DTD directory, but indeed place a\ncorresponding notice as header comment into every single W3C document. The\nlinked one was just for demonstration purposes how it would look in the actual\nDTD.\n\n     * Ted Guild - 10 years ago\n\nI got clarification and indeed as I suspected the license can be in a separate\nfile instead of embedded as comment. Do whichever you prefer.\n\n     * Stephan Kreutzer - 10 years ago\n\nI've just applied the license to the W3C documents I'm using with my tools, so\nhopefully this commit isn't a license violation already, and if it is, please\nlet me know how to comply. Maybe other people might use those license headers\nas well. In any case, thank you very much for your efforts, I really\nappreciate!\n\n  85. Stephan Kreutzer - 10 years ago\n\nReading even the most primitive XHTML 1.1 file with a XML processor requires\nno less than 38 files to be obtained either from W3C or from a local catalog\ndue to modularization. It seems that none of the required W3C files complies\nwith the W3C Document License in the header comment per default, so they're\nnot easy to redistribute and downloading them for each software installation\nmight seem to be an option - if downloading is even possible, since some\nsystem IDs don't provide a URL (and therefore won't be processable at all\nwithout a redistributed local catalog).\n\n     * Stephan Kreutzer - 10 years ago\n\nI've just committed the XHTML 1.1 files, all extended by the W3C Document\nLicense in their header comments. The way the W3C Document License works as a\nredistributable license for free software packages is that I've obtained those\ndocuments from the copyright holder (W3C), which granted me the right to\ndistribute the documents for any purpose (including sublicensing) as long as I\ncomply with the license, so I distribute them as part of a free software\npackage while sublicensing the W3C documents to every recepient, who gets the\nright to distribute the package and the W3C documents from my sublicensing\nwithout the need to become a licensee from W3C directly. As the W3C Document\nLicense isn't particularly freedom protective in itself (allows restrictive\nsublicensing), it's on the other hand free enough (respecting the four\nessential freedoms for software), so I didn't sublicense the W3C documents to\nmy users under the GNU AGPL 3 or later (yet).\n\n  86. Wozza Xing - 9 years ago\n\nI would really like to see the w3c or oasis publish sets of catalogs for\ndownload. This would save hours of time trying to configure each tool.\nNetbeans allows developers to import an existing catalog.\n\n  87. peter - 9 years ago\n\nSorry guys but I am a bit offended by ending up writing this. I have spent\nseveral days now trying to figure out how to write an XML Schema as it ought\nto be written. I have been quite dilligent, and have no idea what I have done\nwrong. It might be, that the java package I am using is broken (I doubt it) It\nmight be that the schema I am trying to import (http://www.w3.org/TR/speech-\ngrammar/grammar.xsd) does something wrong (but I doubt it). In particular I\nhave been working to the w3c document:\nhttp://www.w3.org/TR/xmlschema-0/#SchemaInMultDocs Why is it so complex and\nwhy is it such that silly mistakes (obviously) can have catestrophic\nconsequences. I can't help feeling it is your own fault in some way. I am not\ndoing anything werid;there are thousands trying to do the same and thus the\nproblem I suspect. Please make it easier for us to use schemas the right way.\n\n     * Ted Guild - 9 years ago\n\nI would talk to the people behind the Java package or the library it is based\non. Quite a few development platforms understand the issue, how it is\ninefficient and poor design not to mention potentially abusive to incessantly\nrequest a remote resource ignoring caching directives.\n\nWe see a high percent of Java user-agent strings. I have filled bug reports\nwith some prominent libraries that have gone unanswered. I suspect some of\nthese are not actively maintained.\n\n  88. ruth - 8 years ago\n\nHello, I am totally confused! I have not visited the W3C site for more than 2\nweeks,yet i receive an abuse message telling me that I have attempted to use\nthe site more than 500 times in 10 minutes! I am trying to set up a wordpress\nsite for a local voluntary group. I selected this theme fmedicine because it\nis the only one that states it is w3c valadated! I believe in W3C goals and\nwant to adhere to its high quality levels. COuld it be possible that something\nin the theme is accessing your site? I believe in w3c!\n\n  89. R Wicks - 7 years ago\n\nA heads up:\n\nThe internal XML validator of Java for Linux to validate XML against and XSD\nusing this version of Java:\n\n$ java -version java version \"1.7.0_121\" OpenJDK Runtime Environment (IcedTea\n2.6.8) (7u121-2.6.8-1ubuntu0.14.04.3) OpenJDK 64-Bit Server VM (build\n24.121-b00, mixed mode)\n\nUsing this script, (from this location: https://github.com/amouat/xsd-\nvalidator/blob/master/xsdv.sh)\n\n#!/bin/bash # call xsdv\n\n#First find out where we are relative to the user dir callPath=${0%/*}\n\nif [[ -n \"${callPath}\" ]]; then callPath=${callPath}/ fi\n\necho java -cp ${callPath}build:${callPath}lib/xsdv.jar xsdvalidator.validate\n\"$@\" java -cp ${callPath}build:${callPath}lib/xsdv.jar xsdvalidator.validate\n\"$@\"\n\nAlways makes a request to: \"http://www.w3.org/2001/XMLSchema.xsd\"\n\nWith the XSD I'm using (15118-2, I'm not at liberty to share this XSD)\n\nThis script is very common, and although I'm not that familiar with XML, I\nwouldn't be surprised if a lot of XSD's contain a reference to the\nXMLSchema.xsd. I have a work around for this, I do NOT need (or want) to be\nunbanned either since being banned allows me to identify when I'm accessing an\nexternal website easily.\n\nThis seems fundamental to the Java implementation of Linux - so if you can get\nthat fixed, you might get a lot of requests eliminated.\n\nI have to generate millions of XML messages for the particular project I'm\nworking on. I had no idea that Java was stupid enough to be accessing your\nsite every time I validated a single XML message I generate.\n\n-Rich\n\n  90. Gerald Oskoboiny - 6 years ago\n\nThis article has some good info on how to disable external entities in various\nXML software:\n\nhttps://www.owasp.org/index.php/XML_External_Entity_(XXE)_Prevention_Cheat_Sheet\n\nComments for this post are closed.\n\n  * Home\n  * Contact\n  * Help\n  * Donate\n  * Legal & Policies\n  * Corporation\n  * System Status\n\n  * W3C on Mastodon\n  * W3C on GitHub\n\nCopyright \u00a9 2024 World Wide Web Consortium. W3C^\u00ae liability, trademark and\npermissive license rules apply.\n\n", "frontpage": false}
