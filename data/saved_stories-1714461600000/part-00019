{"aid": "40204574", "title": "Daily bite of C++ #474 \u2013 Optimizing code to run faster for the 1b Row Challenge", "url": "https://simontoth.substack.com/p/daily-bite-of-c-optimizing-code-to", "domain": "simontoth.substack.com", "votes": 1, "user": "davikr", "posted_at": "2024-04-29 21:54:24", "comments": 0, "source_title": "Daily bit(e) of C++ | Optimizing code to run 87x faster", "source_text": "Daily bit(e) of C++ | Optimizing code to run 87x faster\n\n# Daily bit(e) of C++\n\nShare this post\n\n#### Daily bit(e) of C++ | Optimizing code to run 87x faster\n\nsimontoth.substack.com\n\n#### Discover more from Daily bit(e) of C++\n\nRandom C++ knowledge organized in arbitrary order.\n\nOver 1,000 subscribers\n\nContinue reading\n\nSign in\n\n# Daily bit(e) of C++ | Optimizing code to run 87x faster\n\n### Daily bit(e) of C++ #474, Optimizing C++ code to run 87x faster for the\nOne Billion Row Challenge (1brc).\n\n\u0160imon T\u00f3th\n\nApr 29, 2024\n\n1\n\nShare this post\n\n#### Daily bit(e) of C++ | Optimizing code to run 87x faster\n\nsimontoth.substack.com\n\nShare\n\nThe One Billion Row Challenge was initially a challenge for Java developers.\nThe goal was to develop and optimize a parser for a file containing one\nbillion records.\n\nWhile the original challenge was for Java, this challenge is an excellent\nopportunity to demonstrate the optimization of C++ code and the related\nperformance tools.\n\n#\n\nThe challenge\n\nOur input is a file called measurements.txt, which contains temperature\nmeasurements from various measurement stations. The file contains exactly one\nbillion rows with the following format:\n\n    \n    \n    station name;value station name;value\n\nThe station name is a UTF-8 string with a maximum length of 100 bytes,\ncontaining any 1-byte or 2-byte characters (except for \u2018;\u2019 or \u2018\\n\u2019). The\nmeasurement values are between -99.9 and 99.9, all with one decimal digit. The\ntotal number of unique stations is limited to 10\u2019000.\n\nThe output (to stdout) is a lexicographically sorted list of stations, each\nwith the minimum, average and maximum measured temperature.\n\n    \n    \n    {Abha=-23.0/18.0/59.2, Abidjan=-16.2/26.0/67.3, Ab\u00e9ch\u00e9=-10.0/29.4/69.0, ...}\n\n#\n\nBaseline implementation\n\nNaturally, we have to start with a baseline implementation. Our program will\nhave two phases: processing the input and formatting the output.\n\nFor the input, we parse the station name and the measured value and store them\nin a std::unordered_map.\n\nTo produce the output, we collate the unique names, sort them\nlexicographically and then print out the minimum, average and maximum\nmeasurements.\n\nWe are left with a straightforward main function that plugs both parts\ntogether.\n\nThis baseline implementation is very simple, but sadly, it has two major\nproblems: It\u2019s not correct (we will ignore that detail for now), and it's\nvery, very slow. We will be tracking the performance across three machines:\n\n  * Intel 9700K on Fedora 39\n\n  * Intel 14900K on Windows Subsystem for Linux (WSL)\n\n  * Mac Mini M1 (8-core)\n\nThe binaries are compiled using clang 17 on the 9700K, clang 18 on the 14900K\nand Apple clang 15 on the Mac Mini. All three machines use the same\ncompilation flags:\n\n    \n    \n    -O3 -march=native -g0 -DNDEBUG -fomit-frame-pointer\n\nSince we mainly care about the relative performance progression, the\nmeasurements are simply the fastest run on the corresponding machine.\n\n  * 9700K (Fedora 39): 132s\n\n  * 14900K (WSL Ubuntu): 67s\n\n  * Mac Mini M1: 113s\n\n#\n\nEliminating copies\n\nWe don\u2019t need special tooling for the first change. The most important rule\nfor high-performance code is to avoid excessive copies. And we are making\nquite a few.\n\nWhen we process a single line (and remember, there are 1 billion of them), we\nread the station name and the measured value into a std::string. This\ninherently introduces an explicit copy of the data because when we read from a\nfile, the content of the file is already in memory in a buffer.\n\nTo eliminate this issue, we have a couple of options. We can switch to\nunbuffered reads, manually handle the buffer for the istream or take a more\nsystem-level approach, specifically memory mapping the file. For this article,\nwe will go the memory-map route.\n\nWhen we memory-map a file, the OS allocates address space for the file\u2019s\ncontent; however, data is only read into memory as required. The benefit is\nthat we can treat the entire file as an array; the downside is that we lose\ncontrol over which parts of the file are currently available in memory\n(relying on the OS to make the right decisions).\n\nSince we are working with C++, let\u2019s wrap the low-level system logic in RAII\nobjects.\n\nBecause we are wrapping the memory-mapped file in a std::span, we can validate\nthat everything still works by simply swapping std::ifstream for\nstd::ispanstream.\n\nWhile this validates that everything still works, it doesn\u2019t remove any\nexcessive copies. To do that, we have to switch the input processing from\noperating on top of an istream to treating the input as one big C-style\nstring.\n\nWe must adjust our hash map to support heterogeneous lookups because we now\nlook up the stations using a std::string_view. This involves changing the\ncomparator and adding a custom hasher.\n\nThis makes our solution run much faster (we are skipping over M1 for now since\nclang 15 doesn\u2019t support std::from_chars for floating point types).\n\n  * 9700K (Fedora 39): 47.6s (2.8x)\n\n  * 14900K (WSL Ubuntu): 29.4s (2.3x)\n\n#\n\nAnalyzing the situation\n\nTo optimize the solution further, we have to analyze which parts of our\nsolution are the main bottlenecks. We need a profiler.\n\nWhen it comes to profilers, we have to choose between precision and low\noverhead. For this article, we will go with perf, a Linux profiler with\nextremely low overhead that still provides reasonable precision.\n\nTo have any chance to record a profile, we have to inject a bit of debugging\ninformation into our binary:\n\n    \n    \n    -fno-omit-frame-pointer # do not omit the frame pointer -ggdb3 # detailed debugging information\n\nTo record a profile, we run the binary under the perf tool:\n\n    \n    \n    perf record --call-graph dwarf -F999 ./binary\n\nThe callgraph option will allow perf to attribute low-level functions to the\ncorrect caller using the debug information stored in the binary. The second\noption decreases the frequency at which perf captures samples; if the\nfrequency is too high, some samples might be lost.\n\nWe can then view the profile:\n\n    \n    \n    perf report -g 'graph,caller'\n\nHowever, if we run perf on the current implementation, we get a profile that\nisn\u2019t particularly informative.\n\nWe can deduce that the biggest portion of the runtime is spent in the\nstd::unordered_map. However, the rest of the operations are lost in the low-\nlevel functions. For example, you might conclude that parsing out the measured\nvalues only takes 3% (the std::from_chars function); this would be an\nincorrect observation.\n\nThe profile is poor because we have put all the logic into a single tight\nloop. While this is good for performance, we completely lose the sense of the\nlogical operations we are implementing:\n\n  * parse out the station name\n\n  * parse out the measured value\n\n  * store the data in the database\n\nProfile clarity will drastically improve if we wrap these logical operations\ninto separate functions.\n\nNow, we can see that we are spending 62% of our time inserting data into our\ndatabase, 26% parsing the measured values, and 5% parsing the station names.\n\nWe will address the hash map, but before that, let\u2019s deal with parsing the\nvalues. This will also fix a persistent bug in our code (bad rounding).\n\n#\n\nIntegers in disguise\n\nThe input contains measurements from the range -99.9 to 99.9, always with one\ndecimal digit. This means that we are not dealing with floating-point numbers\nin the first place; the measured values are fixed-point numbers.\n\nThe proper way to represent fixed-point values is as an integer, which we can\nmanually parse (for now, in a straightforward way).\n\nThis change also propagates to the record structure.\n\nDatabase insertion can remain the same, but we can take the opportunity to\noptimize the code slightly.\n\nFinally, we have to modify the output formatting code. Since we are now\nworking with fixed-point numbers, we have to correctly convert and round the\nstored integer values back to floating point.\n\nThis change fixes the aforementioned rounding bug and improves the runtime\n(floating-point arithmetic is slow). The implementation is also compatible\nwith the M1 Mac.\n\n  * 9700K (Fedora 39): 35.5s (3.7x)\n\n  * 14900K (WSL Ubuntu): 23.7s (2.8x)\n\n  * Mac Mini M1: 55.7s (2.0x)\n\n#\n\nCustom hash map\n\nThe std::unordered_map from the standard library is notorious for being slow.\nThis is because it uses a node structure (effectively an array of linked lists\nof nodes). We could switch to a flat map (from Abseil or Boost). However, that\nwould be against the original spirit of the 1brc challenge, which prohibited\nexternal libraries.\n\nMore importantly, our input is very constrained. We will have at most 10k\nunique keys for 1B records, leading to an exceptionally high hit ratio.\n\nBecause we are limited to 10k unique keys, we can use a linear probing hash\nmap based on a 16-bit hash that directly indexes a statically sized array. We\nuse the next available slot when we encounter a collision (two different\nstations map to the same hash/index).\n\nThis means that in the worst case (when all stations map to the same\nhash/index), we end up with a linear complexity lookup. However, that is\nexceptionally unlikely, and for the example input using std::hash, we end up\nwith 5M collisions, i.e. 0.5%.\n\nThis change results in a sizeable speedup.\n\n  * 9700K (Fedora 39): 25.6s (5.1x)\n\n  * 14900K (WSL Ubuntu): 18.4s (3.6x)\n\n  * Mac Mini M1: 49.4s (2.3x)\n\n#\n\nMicro-optimizations\n\nWe have cleared up the high-level avenues of optimizations, meaning it is time\nto dig deeper and micro-optimize the critical parts of our code.\n\nLet\u2019s review our current situation.\n\nWe can potentially make some low-level optimizations in hashing (17%) and\ninteger parsing (21%).\n\nThe correct tool for micro-optimizations is a benchmarking framework. We will\nimplement several versions of the targetted function and compare the results\nagainst each other.\n\nFor this article, we will use Google Benchmark.\n\n###\n\nParsing integers\n\nThe current version of integer parsing is (deliberately) poorly written and\nhas excessive branching.\n\nWe can\u2019t properly make use of wide instructions (AVX) since the values can be\nas short as three characters. With wide instructions out of the way, the only\napproach that eliminates branches is a lookup table.\n\nAs we parse the number, we have only two possible situations (ignoring the\nsign):\n\n  * we encounter a digit: multiply the accumulator by 10 and add the digit value\n\n  * we encounter a non-digit: multiply the accumulator by 1 and add 0\n\nWe can encode this as a 2D compile-time generated array that contains\ninformation for all 256 values of a char type.\n\nWe can plug these two versions into our microbenchmark and get a very decisive\nresult.\n\nSadly, the previous sentence is a lie. You cannot just plug the two versions\ninto Google Benchmark. Our implementation is a tight loop over (at most) 5\ncharacters, which makes it incredibly layout-sensitive. We can align the\nfunctions using an LLVM flag.\n\n    \n    \n    -mllvm -align-all-functions=5\n\nHowever, even with that, the results fluctuate heavily (up to 40%).\n\n###\n\nHashing\n\nWe have two opportunities for optimization when it comes to hashing.\n\nCurrently, we first parse out the name of the station, and then later, inside\nlookup_slot, we compute the hash. This means we traverse the data twice.\n\nAdditionally, we compute a 64-bit hash but only need a 16-bit one.\n\nTo mitigate the issues we run into with integer parsing, we will merge the\nparsing into a single step, producing a string_view of the station name, a\n16-bit hash and the fixed-point measured value.\n\nWe use a simple formula to calculate the custom 16-bit hash and rely on\nunsigned overflow instead of modulo.\n\nThis provides a nice speedup (with reasonable stability).\n\nAnd when we plug this improvement into our solution, we get an overall\nspeedup.\n\n  * 9700K (Fedora 39): 19.2s (6.87x)\n\n  * 14900K (WSL Ubuntu): 14.1s (4.75x) (noisy)\n\n  * Mac Mini M1: 46.2s (2.44x)\n\n#\n\nUnleash the threads\n\nIf we investigate the profile now, we will see that we are reaching the limit\nof what is feasible.\n\nThere is very little runtime left outside of the parsing (which we just\noptimized), slot lookup and data insertion. So naturally, the next step is to\nparallelize our code.\n\nThe simplest way to achieve this is to chunk the input into roughly identical\nchunks, process each chunk in a separate thread and then merge the result.\n\nWe can extend our MappedFile type to provide chunked access.\n\nWe can then simply run our existing code per chunk, each in its own thread.\n\nThis gives a fairly nice scaling.\n\nThe following are the best results. Note that these are just best runs for\nrelative comparison, not a rigorous benchmark.\n\n  * 9700K (Fedora 39): 2.6s (50x) (on 8 threads)\n\n  * 14900K (WSL Ubuntu): 0.89s (75x) (on 32 threads)\n\n  * Mac Mini M1: 10.2s (11x) (on 24 threads)\n\n###\n\nDealing with asymmetric processing speeds\n\nThe 9700K scales extremely cleanly, but that is because this processor has 8\nidentical cores that do not support hyperthreading. Once we move on to 14900K,\nthe architecture gets a lot more complicated with performance and efficiency\ncores.\n\nIf we trivially split the input into identical chunks, the efficiency cores\nwill trail behind and slow the overall runtime. So, instead of splitting the\ninput into chunks, one for each thread, let\u2019s have the threads request chunks\nas needed.\n\nAnd the corresponding next_chunk method in our MappedFile.\n\nWhich allows us to squeeze the last bit of performance from the 14900K.\n\n  * 14900K (WSL Ubuntu): 0.77s (87x) (on 32 threads)\n\n#\n\nConclusion\n\nWe have improved the performance of our initial implementation by 87x, which\nis definitely not insignificant. Was it worth it?\n\nWell, that depends. This article has taken me a long time to write and has\ncost me a chunk of my sanity. Notably, the alignment issues when using\nmicrobenchmarks were a huge pain to overcome.\n\nIf I were optimizing a production piece of code, I would likely stop at the\nbasic optimizations (and threads). Micro-optimizations can be worthwhile, but\nthe time investment is steep, and the stability of these optimizations on\nmodern architectures is very poor.\n\nThe full source code is available in this GitHub repository.\n\nLeave a comment\n\nThanks for reading Daily bit(e) of C++! Subscribe for free to receive new\nposts and support my work.\n\n1 Like\n\n1\n\nShare this post\n\n#### Daily bit(e) of C++ | Optimizing code to run 87x faster\n\nsimontoth.substack.com\n\nShare\n\nComments\n\nDaily bit(e) of C++ | Learn Modern C++ 1/N\n\nDaily bit(e) of C++ #83 , A Modern-only C++ course (including C++23), part 1\nof N\n\nMar 25, 2023 \u2022\n\n\u0160imon T\u00f3th\n\n26\n\nShare this post\n\n#### Daily bit(e) of C++ | Learn Modern C++ 1/N\n\nsimontoth.substack.com\n\n10\n\nDaily bit(e) of C++ | Error handling\n\nDaily bit(e) of C++ #6 | Error handling in C++ (with a dash of C++23)\n\nJan 7, 2023 \u2022\n\n\u0160imon T\u00f3th\n\n9\n\nShare this post\n\n#### Daily bit(e) of C++ | Error handling\n\nsimontoth.substack.com\n\n4\n\nDaily bit(e) of C++ | Hardened mode of standard library implementations\n\nDaily bit(e) of C++ #384, Improve your development experience using hardened\nmode of the standard library implementations.\n\nJan 20 \u2022\n\n\u0160imon T\u00f3th\n\n4\n\nShare this post\n\n#### Daily bit(e) of C++ | Hardened mode of standard library implementations\n\nsimontoth.substack.com\n\nReady for more?\n\n\u00a9 2024 \u0160imon T\u00f3th\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
