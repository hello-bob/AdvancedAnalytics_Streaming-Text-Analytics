{"aid": "40131537", "title": "Introducing the Open Chain of Thought Leaderboard", "url": "https://huggingface.co/blog/leaderboard-cot", "domain": "huggingface.co", "votes": 1, "user": "srirangr", "posted_at": "2024-04-23 13:08:24", "comments": 0, "source_title": "Introducing the Open Chain of Thought Leaderboard", "source_text": "Introducing the Open Chain of Thought Leaderboard\n\nHugging Face\n\nBack to Articles\n\n# Introducing the Open Chain of Thought Leaderboard\n\nPublished April 23, 2024\n\nUpdate on GitHub\n\nUpvote\n\n2\n\nggbetz Gregor Betz\n\nlogikon\n\nscacean Sebastian Cacean\n\nlogikon\n\nclefourrier Cl\u00e9mentine Fourrier\n\nyakazimir Kyle Richardson\n\nallenai\n\nChain-of-thought prompting is emerging as a powerful and effective design\npattern for LLM-based apps and agents. The basic idea of chain-of-thought\nprompting is to let a model generate a step-by-step solution (\u201creasoning\ntrace\u201d) before answering a question or taking a decision. With the Open CoT\nLeaderboard we\u2019re tracking LLMs\u2019 ability to generate effective chain-of-\nthought traces for challenging reasoning tasks.\n\nUnlike most performance based leaderboards, we\u2019re not scoring the absolute\naccuracy a model achieves on a given task, but the difference between the\naccuracy with and without chain-of-thought prompting:\n\n    \n    \n    accuracy gain \u0394 = accuracy with CoT \u2013 accuracy w/o CoT.\n\nThis allows us to truly inspect the impact that chain-of-thought has on model\naccuracy.\n\nNote: without CoT prompting, we use the loglikelihood accuracy to score the\nmodel on multiple choice evaluation.\n\n## What\u2019s the motivation behind such a leaderboard for chain-of-thought?\n\nChain-of-thought prompting is a universally applicable prompting strategy that\nmay improve explainability and accuracy of LLM-based apps and agents (see,\ne.g., this collection for recent research and implementations)). With\nframeworks like Langchain or LMQL, it\u2019s straightforward to insert\nsophisticated reasoning chains in your apps. But even if you\u2019ve never heard\nabout chain-of-thought before, you may have noticed, while using a ChatBot,\nthat it tends to proceed step by step before answering your query. So, a\nsystematic, up-to-date comparison of LLMs\u2019 ability to generate effective\nchain-of-thought traces may inform the decisions of builders and users when\nchoosing a model.\n\nOver time, static \"accuracy-based\" benchmarks risk becoming less informative:\ndoes a model score well because of its superior skill, because it has seen the\ncorrect answers during training, or because it has been developed in a\ncompetitive context that is governed by this very benchmark? These widely\nacknowledged issues are addressed by recent eval approaches such as ChatBot\narenas, the use of LLMs as judges, or dynamic benchmarks with programmatically\ngenerated tasks. We hope the Open CoT Leaderboard contributes to these\nefforts, notably by being more robust to training data contamination: knowing\nthe answer to a question doesn\u2019t ensure that one can reason effectively about\nit.\n\n## Which tasks are used?\n\nThe Open CoT Leaderboard evaluates LLMs\u2019 ability to generate effective chain-\nof-thought reasoning traces for the following tasks:\n\n  * LogiQA (new translation of original version, and version 2.0 with new examples)\n  * LSAT dataset (including subsets on analytical reasoning, logical reasoning, and reading comprehension)\n\nExcept for the original version of LogiQA, all these tasks are part of the\nAGIEval benchmark, and have been re-published as logikon-bench.\n\nWe\u2019ve chosen these tasks because they\n\n  1. are generic, i.e. can be solved through reasoning and just require commonsense knowledge;\n  2. are still relatively difficult even for the most powerful LLMs (leaving enough room for improvement through chain-of-thought);\n  3. have been introduced as AI benchmarks before (in AGIEval) and are widely used (e.g., in the Nous benchmark suite).\n\nAll tasks are rendered as multiple-choice problems, with the answer options\nbeing enumerated in the prompt.\n\nWe use the following prompt template for assessing baseline and CoT accuracies\n\u2013 the reasoning traces (starting with Reasoning) are only added in the case\n\u201cwith CoT\u201d:\n\n    \n    \n    Answer the following question about the given passage. Base your answer on the reasoning below. Passage: <passage> Question: <question> A. <choice1> B. <choice2> ... Reasoning: <reasoning> Answer:\n\n## How are chain-of-thought traces generated?\n\n[#cot-generation]\n\nThe literature on chain-of-thought prompting has exploded in the last year,\nand the wealth of prompting strategies for CoT (involving, e.g., decoding,\nprompt wording, prompt chains, decomposition, aggregation and revision of\nreasoning traces) has become sheerly mindblowing.\n\nTo cope with this variety, the evaluation pipeline behind the Open CoT\nLeaderboard implements CoT generation through modular and easily extensible\nprompt chains. So far, we have implemented two fairly simple prompting\nstrategies:\n\n  * Classic: Problem presentation followed by \u201cLet\u2019s think step by step.\u201d instruction.\n  * Reflect: Problem presentation followed by instruction to reflect upon the problem in general terms before solving it step by step.\n\nLet\u2019s have a look at an example. The following problem is from the LSAT\nlogical reasoning benchmark.\n\n    \n    \n    Editorial: The structure of the present school calendar was established to satisfy the requirements of early-twentieth-century agricultural life. In those days, farmers needed their children to have long breaks during which they could remain at home and help with the harvest. The contemporary school year is thus made up of periods of study interspersed with long breaks. But agricultural life no longer occupies most of our citizens, so we can now make changes that serve the interests of children. Therefore, long breaks should be removed from the school calendar. Which one of the following is an assumption on which the editorial's argument depends? A) A change in the present school calendar that shortened breaks would serve the interests of agricultural life. B) Children of farmers need to continue observing a school calendar made up of periods of study interspersed with long breaks. C) Long breaks in the school calendar should be replaced with breaks that are no longer than workers' average vacations. D) A school calendar made up of periods of study without long breaks would serve the interests of children more than a school calendar with long breaks. E) During long breaks children have a tendency to forget what they have learned.\n\nMixtral-8x7B-Instruct-v0.1 is doing fairly well in generating the following\nchain-of-thought traces:\n\nWith prompting strategy Classic| With prompting strategy Reflect  \n---|---  \nThe editorial argues that the current school calendar, with long breaks, was\ndesigned to accommodate agricultural life but should now be changed because\nagricultural life no longer occupies most of our citizens. It concludes that\nlong breaks should be removed from the school calendar.The assumption that the\nargument depends on is the belief that removing long breaks from the school\ncalendar would serve the interests of children more than the current calendar.\nThis is not explicitly stated in the editorial, but it is the underlying\nassumption that justifies the proposed change.Therefore, the correct answer\nis: D) A school calendar made up of periods of study without long breaks would\nserve the interests of children more than a school calendar with long breaks.|\nThe decision problem in abstract terms is to identify the assumption on which\nthe editorial's argument depends.Common mistakes for this kind of problem\ninclude:\n\n  1. Misinterpreting the argument's premises or conclusion.\n  2. Overlooking the specific requirement of identifying an assumption.\n  3. Confusing an assumption with a conclusion or a premise.\n\nA plan for how to solve this problem could be:\n\n  1. Identify the premises and conclusion of the argument.\n  2. List possible assumptions that could bridge the gap between the premises and the conclusion.\n  3. Evaluate each assumption to see if it is necessary for the argument.\n\nSolving the problem step by step, following the plan:\n\n  1. The premises of the argument are:\n     * The structure of the present school calendar was established to satisfy the requirements of early-twentieth-century agricultural life.\n     * Agricultural life no longer occupies most of our citizens.\n  2. The conclusion of the argument is:\n     * Long breaks should be removed from the school calendar.\n  3. Possible assumptions could be:\n     * A school calendar made up of periods of study without long breaks would serve the interests of children more than a school calendar with long breaks.\n     * Children of farmers no longer need to remain at home and help with the harvest during long breaks.\n     * A change in the present school calendar that shortened breaks would not serve the interests of agricultural life.\n  4. Evaluating each assumption:\n     * The argument does not depend on the assumption that children of farmers no longer need to remain at home and help with the harvest during long breaks, because the argument is about the interests of all children, not just those of farmers.\n     * The argument does not depend on the assumption that a change in the present school calendar that shortened breaks would not serve the interests of agricultural life, because the argument is about the interests of children, not agricultural life.\n     * The argument does depend on the assumption that a school calendar made up of periods of study without long breaks would serve the interests of children more than a school calendar with long breaks. This is the assumption that justifies the conclusion.\n\nTherefore, the assumption on which the editorial's argument depends is D) A\nschool calendar made up of periods of study without long breaks would serve\nthe interests of children more than a school calendar with long breaks.  \n  \nEach of our two prompting strategies\u2014Classic and Reflect\u2014is combined and run\nwith multiple decoding parameters:\n\n  * Greedy.\n  * Beam search (n=2).\n  * Sampling (T=.3).\n\nThis gives us, in combination, six \u201cCoT generation regimes.\u201d When we evaluate\na model, we let it generate, for every example in the test datasets, one\nchain-of-thought trace for each regime. The generated traces are then plugged\ninto the prompt template shown above, which we use to score the models. We\nobtain, accordingly, six different numerical scores for accuracy with chain-\nof-thought (namely one for each regime). In the Open CoT Leaderboard, we\nreport (for every model/task) the best marginal accuracy gain achieved under\nany regime.\n\n## What are the main take-aways so far?\n\nWe\u2019re gradually extending the Open CoT Leaderboard by evaluating more and more\nmodels, but current results (model count=30) already suggest some interesting\ninsights.\n\n  * Mighty dwarfs: We have been very pleased to see that relatively small (7B) open LLMs are capable of effective, i.e. accuracy-improving, chain-of-thought reasoning, in some cases at a better rate than bigger model. \ud83c\udf89 For example, a small model like Phi-2 benefits more than the Mixtral model from added CoT traces.\n  * Instruction- and chat-finetuning helps: Finetuned models score much better than their corresponding base models. More specifically, finetuning may improve both the baseline accuracy without CoT and the marginal accuracy gains achieved through CoT.\n  * Variable and ambiguous effects of CoT: Digging a bit deeper, we see that there is no single preferred or superior CoT generation regime. What works best for one model and one task might not work for another model, or another task. And sometimes CoT reduces accuracy rather than increasing it. We take this as a reminder that finding an implementation of CoT that is universally effective, reliable and robust remains a challenging problem.\n\n## What are the next steps? \u2013 And how to contribute.\n\nWe\u2019re planning to move ahead in different directions. And contributions to all\nthese efforts are more than welcome.\n\nFirst, we\u2019d love to evaluate your models! You can \ud83d\udcec submit any open LLMs for\nevaluation on the Open CoT Leaderboard space, using the Submission tab!\n\nThen, we\u2019d love some help on the following coding and data analysis tasks.\n\n  * Carry out in-depth analysis of full evaluation results. For example, a qualitative analysis of the generated CoT traces to check whether they actually point to the correct answer choice. We\u2019ve created a notebook that shows how to access and explore the eval results and reasoning traces which back up the Open Cot Leaderboard. You can build on that and share your own analyses in the corresponding repo (or somewhere else, of course). Feel free to open an issue with suggestions or questions. In case you plan to use the data for research projects and want feedback, just drop a note.\n  * Create Open CoT Dashboard. The Open CoT Leaderboard contends with ranking models according to marginal accuracy gains. It doesn\u2019t display the baseline accuracies, the variance, the scores for different CoT generation regimes, properties of the generated reasoning traces (e.g., length), etc. We think it would be super informative to complement the leaderboard with a dashboard (e.g., as an extra tab or a separate HF space) that presents all this info and can be interactively explored by users. In case you\u2019re interested in building such an Open CoT Dashboard (with or without us), just reach out.\n  * More CoT chains. We\u2019re pondering implementing further CoT generation regimes. Promising candidates are, for example, self-consistency, tree-of-thought, self-check, or debating. Want to help us with that? Get in touch! (\ud83e\udd2b: Why not choose such a project for your master\u2019s or bachelor\u2019s thesis?)\n  * More tasks and test datasets. The Open CoT Leaderboard is arguably built on a rather narrow set of benchmarks. Once we have free compute resources, we\u2019d like to include further challenging reasoning tasks. We\u2019d be happy to learn which tasks you\u2019d like to see included in the Open CoT Leaderboard.\n\nHere\u2019s where we can exchange our ideas and collaborate:\n\n  * For non-technical suggestions and feedback, join the discussion at the leaderboard\u2019s HF space.\n  * For technical feedback and questions, open an issue at our GitHub repo.\n\nLooking forward to hearing from you!\n\nMore Articles from our Blog\n\n## The Open Medical-LLM Leaderboard: Benchmarking Large Language Models in\nHealthcare\n\nBy April 19, 2024 guest \u2022 43\n\n## Introducing the LiveCodeBench Leaderboard - Holistic and Contamination-Free\nEvaluation of Code LLMs\n\nBy April 16, 2024 guest \u2022 7\n\nUpvote\n\n2\n\n", "frontpage": false}
