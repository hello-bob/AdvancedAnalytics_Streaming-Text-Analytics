{"aid": "40257420", "title": "StarCoder2-Instruct: Transparent Self-Alignment for Code Generation", "url": "https://github.com/bigcode-project/starcoder2-self-align", "domain": "github.com/bigcode-project", "votes": 26, "user": "DeathArrow", "posted_at": "2024-05-04 13:13:33", "comments": 5, "source_title": "GitHub - bigcode-project/starcoder2-self-align: StarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment for Code Generation", "source_text": "GitHub - bigcode-project/starcoder2-self-align: StarCoder2-Instruct: Fully\nTransparent and Permissive Self-Alignment for Code Generation\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nbigcode-project / starcoder2-self-align Public\n\n  * Notifications\n  * Fork 6\n  * Star 79\n\nStarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment for Code\nGeneration\n\n### License\n\nApache-2.0 license\n\n79 stars 6 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# bigcode-project/starcoder2-self-align\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n7 Branches\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\ncassanofupdate submodMay 2, 20245cc525e \u00b7 May 2, 2024May 2, 2024\n\n## History\n\n30 Commits  \n  \n### evaluation\n\n|\n\n### evaluation\n\n| refactor(doc): clarification| Apr 29, 2024  \n  \n### prompts\n\n|\n\n### prompts\n\n| refactor: rename data to prompts| Apr 26, 2024  \n  \n### seed_gathering\n\n|\n\n### seed_gathering\n\n| update submods| May 2, 2024  \n  \n### src/star_align\n\n|\n\n### src/star_align\n\n| update submod| May 2, 2024  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| feat: training and data generation pipeline| Apr 26, 2024  \n  \n### .gitmodules\n\n|\n\n### .gitmodules\n\n| added submod| Apr 30, 2024  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Initial commit| Apr 25, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| docker instructions| Apr 30, 2024  \n  \n### pyproject.toml\n\n|\n\n### pyproject.toml\n\n| fix(doc): simplify| Apr 29, 2024  \n  \n### requirements.txt\n\n|\n\n### requirements.txt\n\n| feat: project file and documentation| Apr 29, 2024  \n  \n## Repository files navigation\n\n# StarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment for\nCode Generation\n\n\u2b50\ufe0f About | \ud83d\ude80 Quick start | \ud83d\udcda Data generation | \ud83e\uddd1\ud83d\udcbb Training | \ud83d\udcca Evaluation | \u26a0\ufe0f Limitations\n\n## About\n\nWe introduce StarCoder2-15B-Instruct-v0.1, the very first entirely self-\naligned code Large Language Model (LLM) trained with a fully permissive and\ntransparent pipeline. Our open-source pipeline uses StarCoder2-15B to generate\nthousands of instruction-response pairs, which are then used to fine-tune\nStarCoder-15B itself without any human annotations or distilled data from huge\nand proprietary LLMs.\n\n  * Model: bigcode/starcoder2-15b-instruct-v0.1\n  * Code: bigcode-project/starcoder2-self-align\n  * Dataset: bigcode/self-oss-instruct-sc2-exec-filter-50k\n  * Authors: Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Harm de Vries, Leandro von Werra, Arjun Guha, Lingming Zhang.\n\n## Quick start\n\nHere is an example to get started with StarCoder2-15B-Instruct-v0.1 using the\ntransformers library:\n\n    \n    \n    import transformers import torch pipeline = transformers.pipeline( model=\"bigcode/starcoder2-15b-instruct-v0.1\", task=\"text-generation\", torch_dtype=torch.bfloat16, device_map=\"auto\", ) def respond(instruction: str, response_prefix: str) -> str: messages = [{\"role\": \"user\", \"content\": instruction}] prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False) prompt += response_prefix teminators = [ pipeline.tokenizer.eos_token_id, pipeline.tokenizer.convert_tokens_to_ids(\"###\"), ] result = pipeline( prompt, max_length=256, num_return_sequences=1, do_sample=False, eos_token_id=teminators, pad_token_id=pipeline.tokenizer.eos_token_id, truncation=True, ) response = response_prefix + result[0][\"generated_text\"][len(prompt) :].split(\"###\")[0].rstrip() return response instruction = \"Write a quicksort function in Python with type hints and a 'less_than' parameter for custom sorting criteria.\" response_prefix = \"\" print(respond(instruction, response_prefix))\n\n## Data generation pipeline\n\n> Run pip install -e . first to install the package locally. Check\n> seed_gathering for details on how we collected the seeds.\n\nWe used vLLM's OpenAI compatible server for data generation. So, before\nrunning the following commands, make sure the vLLM server is running, and the\nassociated openai environment variables are set.\n\nFor example, you can start an vLLM server with docker:\n\n    \n    \n    docker run --gpus '\"device=0\"' \\ -v $HF_HOME:/root/.cache/huggingface \\ -p 10000:8000 \\ --ipc=host \\ vllm/vllm-openai:v0.3.3 \\ --model bigcode/starcoder2-15b \\ --tensor-parallel-size 1 --dtype bfloat16\n\nAnd then set the environment variables as follows:\n\n    \n    \n    export OPENAI_API_KEY=\"EMPTY\" export OPENAI_BASE_URL=\"http://localhost:10000/v1/\"\n\n## Training Details\n\n> Run pip install -e . first to install the package locally. And install Flash\n> Attention to speed up the training.\n\n### Hyperparameters\n\n  * Optimizer: Adafactor\n  * Learning rate: 1e-5\n  * Epoch: 4\n  * Batch size: 64\n  * Warmup ratio: 0.05\n  * Scheduler: Linear\n  * Sequence length: 1280\n  * Dropout: Not applied\n\n### Hardware\n\n1 x NVIDIA A100 80GB. Yes, you just need one A100 to finetune StarCoder2-15B!\n\n### Script\n\nThe following script finetunes StarCoder2-15B-Instruct-v0.1 from the base\nStarCoder2-15B model. /path/to/dataset.jsonl is the JSONL format of the 50k\ndataset we generated. You can dump the dataset to JSONL to fit the training\nscript.\n\n## Evaluation on EvalPlus, LiveCodeBench, and DS-1000\n\n> Check evaluation for more details.\n\n## Bias, Risks, and Limitations\n\nStarCoder2-15B-Instruct-v0.1 is primarily finetuned for Python code generation\ntasks that can be verified through execution, which may lead to certain biases\nand limitations. For example, the model might not adhere strictly to\ninstructions that dictate the output format. In these situations, it's\nbeneficial to provide a response prefix or a one-shot example to steer the\nmodel\u2019s output. Additionally, the model may have limitations with other\nprogramming languages and out-of-domain coding tasks.\n\nThe model also inherits the bias, risks, and limitations from its base\nStarCoder2-15B model. For more information, please refer to the StarCoder2-15B\nmodel card.\n\n## About\n\nStarCoder2-Instruct: Fully Transparent and Permissive Self-Alignment for Code\nGeneration\n\n### Topics\n\nlarge-language-models llm llm4code\n\n### Resources\n\nReadme\n\n### License\n\nApache-2.0 license\n\nActivity\n\nCustom properties\n\n### Stars\n\n79 stars\n\n### Watchers\n\n3 watching\n\n### Forks\n\n6 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Contributors 4\n\n  * UniverseFly Yuxiang Wei\n  * cassanof Federico Cassano\n  * lvwerra Leandro von Werra\n  * ganler Jiawei Liu\n\n## Languages\n\n  * Python 100.0%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": true}
