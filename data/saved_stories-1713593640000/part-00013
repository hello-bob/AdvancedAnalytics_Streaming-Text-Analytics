{"aid": "40090520", "title": "LLMs approach expert-level clinical knowledge and reasoning in ophthalmology", "url": "https://journals.plos.org/digitalhealth/article?id=10.1371/journal.pdig.0000341", "domain": "plos.org", "votes": 1, "user": "weeha", "posted_at": "2024-04-19 18:41:10", "comments": 0, "source_title": "Large language models approach expert-level clinical knowledge and reasoning in ophthalmology: A head-to-head cross-sectional study", "source_text": "Large language models approach expert-level clinical knowledge and reasoning in ophthalmology: A head-to-head cross-sectional study | PLOS Digital Health\n\nSkip to main content\n\nAdvertisement\n\n  * plos.org\n  * create account\n  * sign in\n\n  * 0\n\nSave\n\nTotal Mendeley and Citeulike bookmarks.\n\n  * 0\n\nCitation\n\nPaper's citation count computed by Dimensions.\n\n  * 4,332\n\nView\n\nPLOS views and downloads.\n\n  * 84\n\nShare\n\nSum of Facebook, Twitter, Reddit and Wikipedia activity.\n\nOpen Access\n\nPeer-reviewed\n\nResearch Article\n\n# Large language models approach expert-level clinical knowledge and reasoning\nin ophthalmology: A head-to-head cross-sectional study\n\n  * Arun James Thirunavukarasu ,\n\nRoles Conceptualization, Data curation, Formal analysis, Funding acquisition,\nInvestigation, Methodology, Project administration, Resources, Software,\nSupervision, Validation, Visualization, Writing \u2013 original draft, Writing \u2013\nreview & editing\n\n* E-mail: ajt205@cantab.ac.uk (AJT); ting.darren@gmail.com (DSJT)\n\nAffiliations University of Cambridge School of Clinical Medicine, Cambridge,\nUnited Kingdom, Oxford University Clinical Academic Graduate School,\nUniversity of Oxford, Oxford, United Kingdom\n\nhttps://orcid.org/0000-0001-8968-4768\n\n\u2a2f\n\n  * Shathar Mahmood,\n\nRoles Data curation, Investigation, Writing \u2013 review & editing\n\nAffiliation University of Cambridge School of Clinical Medicine, Cambridge,\nUnited Kingdom\n\n\u2a2f\n\n  * Andrew Malem,\n\nRoles Data curation, Investigation, Writing \u2013 review & editing\n\nAffiliation Eye Institute, Cleveland Clinic Abu Dhabi, Abu Dhabi Emirate,\nUnited Arab Emirates\n\n\u2a2f\n\n  * William Paul Foster,\n\nRoles Data curation, Investigation, Writing \u2013 original draft, Writing \u2013 review\n& editing\n\nAffiliations University of Cambridge School of Clinical Medicine, Cambridge,\nUnited Kingdom, Department of Physiology, Development and Neuroscience,\nUniversity of Cambridge, Cambridge, United Kingdom\n\n\u2a2f\n\n  * Rohan Sanghera,\n\nRoles Data curation, Investigation, Writing \u2013 review & editing\n\nAffiliation University of Cambridge School of Clinical Medicine, Cambridge,\nUnited Kingdom\n\nhttps://orcid.org/0000-0001-6370-8426\n\n\u2a2f\n\n  * Refaat Hassan,\n\nRoles Data curation, Investigation\n\nAffiliation University of Cambridge School of Clinical Medicine, Cambridge,\nUnited Kingdom\n\n\u2a2f\n\n  * Sean Zhou,\n\nRoles Data curation, Investigation, Writing \u2013 review & editing\n\nAffiliation West Suffolk NHS Foundation Trust, Bury St Edmunds, United Kingdom\n\n\u2a2f\n\n  * Shiao Wei Wong,\n\nRoles Data curation, Investigation, Writing \u2013 review & editing\n\nAffiliation Manchester Royal Eye Hospital, Manchester University NHS\nFoundation Trust, Manchester, United Kingdom\n\n\u2a2f\n\n  * Yee Ling Wong,\n\nRoles Data curation, Investigation, Writing \u2013 review & editing\n\nAffiliation Manchester Royal Eye Hospital, Manchester University NHS\nFoundation Trust, Manchester, United Kingdom\n\n\u2a2f\n\n  * Yu Jeat Chong,\n\nRoles Data curation, Investigation, Writing \u2013 review & editing\n\nAffiliation Birmingham and Midland Eye Centre, Sandwell and West Birmingham\nNHS Foundation Trust, Birmingham, United Kingdom\n\n\u2a2f\n\n  * Abdullah Shakeel,\n\nRoles Data curation, Investigation, Writing \u2013 review & editing\n\nAffiliation University of Cambridge School of Clinical Medicine, Cambridge,\nUnited Kingdom\n\n\u2a2f\n\n  * Yin-Hsi Chang,\n\nRoles Data curation, Investigation, Writing \u2013 original draft, Writing \u2013 review\n& editing\n\nAffiliation Department of Ophthalmology, Chang Gung Memorial Hospital, Linkou\nMedical Center, Taoyuan, Taiwan\n\n\u2a2f\n\n  * Benjamin Kye Jyn Tan,\n\nRoles Data curation, Investigation, Writing \u2013 review & editing\n\nAffiliation Yong Loo Lin School of Medicine, National University of Singapore,\nSingapore\n\n\u2a2f\n\n  * Nikhil Jain,\n\nRoles Data curation, Investigation, Project administration, Writing \u2013 review &\nediting\n\nAffiliation Bedfordshire Hospitals NHS Foundation Trust, Luton and Dunstable,\nUnited Kingdom\n\n\u2a2f\n\n  * Ting Fang Tan,\n\nRoles Data curation, Investigation, Writing \u2013 review & editing\n\nAffiliation Singapore Eye Research Institute, Singapore National Eye Centre,\nSingapore, Singapore\n\n\u2a2f\n\n  * Saaeha Rauz,\n\nRoles Writing \u2013 review & editing\n\nAffiliations Birmingham and Midland Eye Centre, Sandwell and West Birmingham\nNHS Foundation Trust, Birmingham, United Kingdom, Academic Unit of\nOphthalmology, Institute of Inflammation and Ageing, University of Birmingham,\nBirmingham, United Kingdom\n\nhttps://orcid.org/0000-0003-4627-3496\n\n\u2a2f\n\n  * Daniel Shu Wei Ting,\n\nRoles Funding acquisition, Project administration\n\nAffiliations Singapore Eye Research Institute, Singapore National Eye Centre,\nSingapore, Singapore, Duke-NUS Medical School, Singapore, Singapore, Byers Eye\nInstitute, Stanford University, Palo Alto, California, United States of\nAmerica\n\n\u2a2f\n\n  * [ ... ],\n  * Darren Shu Jeng Ting\n\nRoles Conceptualization, Formal analysis, Funding acquisition, Methodology,\nProject administration, Supervision, Writing \u2013 original draft, Writing \u2013\nreview & editing\n\n* E-mail: ajt205@cantab.ac.uk (AJT); ting.darren@gmail.com (DSJT)\n\nAffiliations Birmingham and Midland Eye Centre, Sandwell and West Birmingham\nNHS Foundation Trust, Birmingham, United Kingdom, Academic Unit of\nOphthalmology, Institute of Inflammation and Ageing, University of Birmingham,\nBirmingham, United Kingdom, Academic Ophthalmology, School of Medicine,\nUniversity of Nottingham, Nottingham, United Kingdom\n\n\u2a2f\n\n  * [ view all ]\n  * [ view less ]\n\n# Large language models approach expert-level clinical knowledge and reasoning\nin ophthalmology: A head-to-head cross-sectional study\n\n  * Arun James Thirunavukarasu,\n  * Shathar Mahmood,\n  * Andrew Malem,\n  * William Paul Foster,\n  * Rohan Sanghera,\n  * Refaat Hassan,\n  * Sean Zhou, ...\n  * Shiao Wei Wong,\n  * Yee Ling Wong,\n  * Yu Jeat Chong\n\nx\n\n  * Published: April 17, 2024\n  * https://doi.org/10.1371/journal.pdig.0000341\n\n  * Article\n  * Authors\n  * Metrics\n  * Comments\n  * Media Coverage\n\n  * Abstract\n  * Author summary\n  * Introduction\n  * Methods\n  * Results\n  * Discussion\n  * Supporting information\n  * Acknowledgments\n  * References\n\n  * Reader Comments\n  * Figures\n\n## Abstract\n\nLarge language models (LLMs) underlie remarkable recent advanced in natural\nlanguage processing, and they are beginning to be applied in clinical\ncontexts. We aimed to evaluate the clinical potential of state-of-the-art LLMs\nin ophthalmology using a more robust benchmark than raw examination scores. We\ntrialled GPT-3.5 and GPT-4 on 347 ophthalmology questions before GPT-3.5,\nGPT-4, PaLM 2, LLaMA, expert ophthalmologists, and doctors in training were\ntrialled on a mock examination of 87 questions. Performance was analysed with\nrespect to question subject and type (first order recall and higher order\nreasoning). Masked ophthalmologists graded the accuracy, relevance, and\noverall preference of GPT-3.5 and GPT-4 responses to the same questions. The\nperformance of GPT-4 (69%) was superior to GPT-3.5 (48%), LLaMA (32%), and\nPaLM 2 (56%). GPT-4 compared favourably with expert ophthalmologists (median\n76%, range 64\u201390%), ophthalmology trainees (median 59%, range 57\u201363%), and\nunspecialised junior doctors (median 43%, range 41\u201344%). Low agreement between\nLLMs and doctors reflected idiosyncratic differences in knowledge and\nreasoning with overall consistency across subjects and types (p>0.05). All\nophthalmologists preferred GPT-4 responses over GPT-3.5 and rated the accuracy\nand relevance of GPT-4 as higher (p<0.05). LLMs are approaching expert-level\nknowledge and reasoning skills in ophthalmology. In view of the comparable or\nsuperior performance to trainee-grade ophthalmologists and unspecialised\njunior doctors, state-of-the-art LLMs such as GPT-4 may provide useful medical\nadvice and assistance where access to expert ophthalmologists is limited.\nClinical benchmarks provide useful assays of LLM capabilities in healthcare\nbefore clinical trials can be designed and conducted.\n\n## Author summary\n\nLarge language models (LLMs) are the most sophisticated form of language-based\nartificial intelligence. LLMs have the potential to improve healthcare, and\nexperiments and trials are ongoing to explore potential avenues for LLMs to\nimprove patient care. Here, we test state-of-the-art LLMs on challenging\nquestions used to assess the aptitude of eye doctors (ophthalmologists) in the\nUnited Kingdom before they can be deemed fully qualified. We compare the\nperformance of these LLMs to fully trained ophthalmologists as well as doctors\nin training to gauge the aptitude of the LLMs for providing advice to patients\nabout eye health. One of the LLMs, GPT-4, exhibits favourable performance when\ncompared with fully qualified and training ophthalmologists; and comparisons\nwith its predecessor model, GPT-3.5, indicate that this superior performance\nis due to improved accuracy and relevance of model responses. LLMs are\napproaching expert-level ophthalmological knowledge and reasoning, and may be\nuseful for providing eye-related advice where access to healthcare\nprofessionals is limited. Further research is required to explore potential\navenues of clinical deployment.\n\n## Figures\n\nCitation: Thirunavukarasu AJ, Mahmood S, Malem A, Foster WP, Sanghera R,\nHassan R, et al. (2024) Large language models approach expert-level clinical\nknowledge and reasoning in ophthalmology: A head-to-head cross-sectional\nstudy. PLOS Digit Health 3(4): e0000341.\nhttps://doi.org/10.1371/journal.pdig.0000341\n\nEditor: Man Luo, Mayo Clinic Scottsdale, UNITED STATES\n\nReceived: July 31, 2023; Accepted: February 26, 2024; Published: April 17,\n2024\n\nCopyright: \u00a9 2024 Thirunavukarasu et al. This is an open access article\ndistributed under the terms of the Creative Commons Attribution License, which\npermits unrestricted use, distribution, and reproduction in any medium,\nprovided the original author and source are credited.\n\nData Availability: All data are available as supplementary information,\nexcluding copyrighted material from the textbook used for experiments.\n\nFunding: DSWT is supported by the National Medical Research Council, Singapore\n(NMCR/HSRG/0087/2018; MOH-000655-00; MOH-001014-00), Duke-NUS Medical School\n(Duke-NUS/RSF/2021/0018; 05/FY2020/EX/15-A58), and Agency for Science,\nTechnology and Research (A20H4g2141; H20C6a0032). DSJT is supported by a\nMedical Research Council / Fight for Sight Clinical Research Fellowship\n(MR/T001674/1). These funders were not involved in the conception, execution,\nor reporting of this review.\n\nCompeting interests: AM is a member of the Panel of Examiners of the Royal\nCollege of Ophthalmologists and performs unpaid work as an FRCOphth examiner.\nDSWT holds a patent on a deep learning system to detect retinal disease. DSJT\nauthored the book used in the study and receives royalty from its sales. The\nother authors have no competing interests to declare.\n\n## Introduction\n\nGenerative Pre-trained Transformer 3.5 (GPT-3.5) and 4 (GPT-4) are large\nlanguage models (LLMs) trained on datasets containing hundreds of billions of\nwords from articles, books, and other internet sources [1, 2]. ChatGPT is an\nonline chatbot which uses GPT-3.5 or GPT-4 to provide bespoke responses to\nhuman users\u2019 queries [3]. LLMs have revolutionised the field of natural\nlanguage processing, and ChatGPT has attracted significant attention in\nmedicine for attaining passing level performance in medical school\nexaminations and providing more accurate and empathetic messages than human\ndoctors in response to patient queries on a social media platform [3,4,5,6].\nWhile GPT-3.5 performance in more specialised examinations has been\ninadequate, GPT-4 is thought to represent a significant advancement in terms\nof medical knowledge and reasoning [3,7,8]. Other LLMs in wide use include\nPathways Language Model 2 (PaLM 2) and Large Language Model Meta AI 2 (LLaMA\n2) [3], [9, p. 2], [10].\n\nApplications and trials of LLMs in ophthalmological settings has been limited\ndespite ChatGPT\u2019s performance in questions relating to \u2018eyes and vision\u2019 being\nsuperior to other subjects in an examination for general practitioners [7,11].\nChatGPT has been trialled on the North American Ophthalmology Knowledge\nAssessment Program (OKAP), and Fellowship of the Royal College of\nOphthalmologists (FRCOphth) Part 1 and Part 2 examinations. In both cases,\nrelatively poor results have been reported for GPT-3.5, with significant\nimprovement exhibited by GPT-4 [12,13,14,15,16]. However, previous studies are\nafflicted by two important issues which may affect their validity and\ninterpretability. First, so-called \u2018contamination\u2019, where test material\nfeatures in the pretraining data used to develop LLMs, may result in inflated\nperformance as models recall previously seen text rather than using clinical\nreasoning to provide an answer. Second, examination performance in and of\nitself provides little information regarding the potential of models to\ncontribute to clinical practice as a medical-assistance tool [3]. Clinical\nbenchmarks are required to understanding the meaning and implications of\nscores in ophthalmological examinations attained by LLMs and are a necessary\nprecursor to clinical trials of LLM-based interventions.\n\nHere, we used FRCOphth Part 2 examination questions to gauge the\nophthalmological knowledge base and reasoning capability of LLMs using fully\nqualified and currently training ophthalmologists as clinical benchmarks.\nThese questions were not freely available online, minimising the risk of\ncontamination. The FRCOphth Part 2 Written Examination tests the clinical\nknowledge and skills of ophthalmologists in training using multiple choice\nquestions with no negative marking and must be passed to fully qualify as a\nspecialist eye doctor in the United Kingdom.\n\n## Methods\n\n### Question extraction\n\nFRCOphth Part 2 questions were sourced from a textbook for doctors preparing\nto take the examination [17]. This textbook is not freely available on the\ninternet, making the possibility of its content being included in LLMs\u2019\ntraining datasets unlikely [1]. All 360 multiple-choice questions from the\ntextbook\u2019s six chapters were extracted, and a 90-question mock examination\nfrom the textbook was segregated for LLM and doctor comparisons. Two\nresearchers matched the subject categories of the practice papers\u2019 questions\nto those defined in the Royal College of Ophthalmologists\u2019 documentation\nconcerning the FRCOphth Part 2 written examination. Similarly, two researchers\ncategorised each question as first order recall or higher order reasoning,\ncorresponding to \u2018remembering\u2019 and \u2018applying\u2019 or \u2018analysing\u2019 in Bloom\u2019s\ntaxonomy, respectively [18]. Disagreement between classification decisions was\nresolved by a third researcher casting a deciding vote. Questions containing\nnon-plain text elements such as images were excluded as these could not be\ninputted to the LLM applications.\n\n### Trialling large language models\n\nEvery eligible question was inputted into ChatGPT (GPT-3.5 and GPT-4 versions;\nOpenAI, San Francisco, California, United States of America) between April 29\nand May 10, 2023. The answers provided by GPT-3.5 and GPT-4 were recorded and\ntheir whole reply to each question was recorded for further analysis. If\nChatGPT failed to provide a definitive answer, the question was re-trialled up\nto three times, after which ChatGPT\u2019s answer was recorded as \u2018null\u2019 if no\nanswer was provided. Correct answers (\u2018ground truth\u2019) were defined as the\nanswers provided by the textbook and were recorded for every eligible question\nto facilitate calculation of performance. Upon their release, Bard (Google\nLLC, Mountain View, California, USA) and HuggingChat (Hugging Face, Inc., New\nYork City, USA) were used to trial PaLM 2 (Google LLC) and LLaMA (Meta, Menlo\nPark, California, USA) respectively on the portion of the textbook\ncorresponding to a 90-question examination, adhering to the same procedures\nbetween June 20 and July 2, 2023.\n\n### Clinical benchmarks\n\nTo gauge the performance, accuracy, and relevance of LLM outputs, five expert\nophthalmologists who had all passed the FRCOphth Part 2 (E1-E5), three\ntrainees (residents) currently in ophthalmology training programmes (T1-T3),\nand two unspecialised (i.e. not in ophthalmology training) junior doctors\n(J1-J2) first answered the 90-question mock examination independently, without\nreference to textbooks, the internet, or LLMs\u2019 recorded answers. As with the\nLLMs, doctors\u2019 performance was calculated with reference to the correct\nanswers provided by the textbook. After completing the examination,\nophthalmologists graded the whole output of GPT-3.5 and GPT-4 on a Likert\nscale from 1\u20135 (very bad, bad, neutral, good, very good) to qualitatively\nappraise accuracy of information provided and relevance of outputs to the\nquestion used as an input prompt. For these appraisals, ophthalmologists were\nblind to the LLM source (which was presented in a randomised order) and to\ntheir previous answers to the same questions, but they could refer to the\nquestion text and correct answer and explanation provided by the textbook.\nProcedures are comprehensively described in the protocol issued to the\nophthalmologists (S1 Protocol).\n\nOur null hypothesis was that LLMs and doctors would exhibit similar\nperformance, supported by results in a wide range of medical examinations [3,\n6]. Prospective power analysis was conducted which indicated that 63 questions\nwere required to identify a 10% superior performance of an LLM to human\nperformance at a 5% significance level (type 1 error rate) with 80% power (20%\ntype 2 error rate). This indicated that the 90-question examination in our\nexperiments was more than sufficient to detect ~10% differences in overall\nperformance. The whole 90-question mock examination was used to avoid over- or\nunder-sampling certain question types with respect to actual FRCOphth papers.\nTo verify that the mock examination was representative of the FRCOphth Part 2\nexamination, expert ophthalmologists were asked to rate the difficulty of\nquestions used here in comparison to official examinations on a 5-point Likert\nscale (\u201cmuch easier\u201d, \u201csomewhat easier\u201d, \u201csimilar\u201d, \u201csomewhat more difficult\u201d,\n\u201cmuch more difficult\u201d).\n\n### Statistical analysis\n\nPerformance of doctors and LLMs were compared using chi-squared (\u03c7^2) tests.\nAgreement between answers provided by doctors and LLMs was quantified through\ncalculation of Kappa statistics, interpreted in accordance with McHugh\u2019s\nrecommendations [19]. To further explore the strengths and weaknesses of the\nanswer providers, performance was stratified by question type (first order\nfact recall or higher order reasoning) and subject using a chi-squared or\nFisher\u2019s exact test where appropriate. Likert scale data corresponding to the\naccuracy and relevance of GPT-3.5 and GPT-4 responses to the same questions\nwere analysed with paired t-tests with the Bonferroni correction applied to\nmitigate the risk of false positive results due to multiple-testing\u2014parametric\ntesting was justified by a sufficient sample size [20]. A chi-squared test was\nused to quantify the significance of any difference in overall preference of\nophthalmologists choosing between GPT-3.5 and GPT-4 responses. Statistical\nsignificance was concluded where p < 0.05. For additional contextualisation,\nexamination statistics corresponding to FRCOphth Part 2 written examinations\ntaken between July 2017 and December 2022 were collected from Royal College of\nOphthalmologists examiners\u2019 reports [21]. These statistics facilitated\ncomparisons between human and LLM performance in the mock examination with the\nperformance of actual candidates in recent examinations. Failure cases where\nall LLMs provided an incorrect answer were appraised qualitatively to explore\nany specific weaknesses of the technology.\n\nStatistical analysis was conducted in R (version 4.1.2; R Foundation for\nStatistical Computing, Vienna, Austria), and figures were produced in Affinity\nDesigner (version 1.10.6; Serif Ltd, West Bridgford, Nottinghamshire, United\nKingdom).\n\n## Results\n\n### Questions sources\n\nOf 360 questions in the textbook, 347 questions (including 87 of the 90\nquestions from the mock examination chapter) were included [17]. Exclusions\nwere all due to non-text elements such as images and tables which could not be\ninputted into LLM chatbot interfaces. The distribution of question types and\nsubjects within the whole set and mock examination set of questions is\nsummarised in Table 1 and S1 Table alongside performance.\n\nDownload:\n\n  * PPT\n\nPowerPoint slide\n\n  * PNG\n\nlarger image\n\n  * TIFF\n\noriginal image\n\nTable 1. Examination characteristics and granular performance data.\n\nQuestion subject and type distributions presented alongside scores attained by\nLLMs (GPT-3.5, GPT-4, LLaMA, and PaLM 2), expert ophthalmologists (E1-E5),\nophthalmology trainees (T1-T3), and unspecialised junior doctors (J1-J2).\nMedian scores do not necessarily sum to the overall median score, as\nfractional scores are impossible.\n\nhttps://doi.org/10.1371/journal.pdig.0000341.t001\n\n#### GPT-4 represents a significant advance on GPT-3.5 in ophthalmological\nknowledge and reasoning.\n\nOverall performance over 347 questions was significantly higher for GPT-4\n(61.7%) than GPT-3.5 (48.41%; \u03c7^2 = 12.32, p<0.01), with results detailed in\nS1 Fig and S1 Table. ChatGPT performance was consistent across question types\nand subjects (S1 Table). For GPT-4, no significant variation was observed with\nrespect to first order and higher order questions (\u03c7^2 = 0.22, p = 0.64), or\nsubjects defined by the Royal College of Ophthalmologists (Fisher\u2019s exact test\nover 2000 iterations, p = 0.23). Similar results were observed for GPT-3.5\nwith respect to first and second order questions (\u03c7^2 = 0.08, p = 0.77), and\nsubjects (Fisher\u2019s exact test over 2000 iterations, p = 0.28). Performance and\nvariation within the 87-question mock examination was very similar to the\noverall performance over 347 questions, and subsequent experiments were\ntherefore restricted to that representative set of questions.\n\n#### GPT-4 compares well with other LLMs, junior and trainee doctors and\nophthalmology experts.\n\nPerformance in the mock examination is summarised in Fig 1\u2014GPT-4 (69%) was the\ntop-scoring model, performing to a significantly higher standard than GPT-3.5\n(48%; \u03c7^2 = 7.33, p < 0.01) and LLaMA (32%; \u03c7^2 = 22.77, p < 0.01), but\nstatistically similarly to PaLM 2 (56%) despite a superior score (\u03c7^2 = 2.81,\np = 0.09). LLaMA exhibited the lowest examination score, significantly weaker\nthan GPT-3.5 (\u03c7^2 = 4.58, p = 0.03) and PaLM-2 (\u03c7^2 = 10.01, p < 0.01) as well\nas GPT-4.\n\nDownload:\n\n  * PPT\n\nPowerPoint slide\n\n  * PNG\n\nlarger image\n\n  * TIFF\n\noriginal image\n\nFig 1. FRCOphth Part 2 performance of LLMs and doctors of variable expertise.\n\nExamination performance in the 87-question mock examination used to trial LLMs\n(GPT-3.5, GPT-4, LLaMA, and PaLM 2), expert ophthalmologists (E1-E5),\nophthalmology trainees (T1-T3), and unspecialised junior doctors (J1-J2).\nDotted lines depict the mean performance of expert ophthalmologists (66/87;\n76%), ophthalmology trainees (60/87; 69%), and unspecialised junior doctors\n(37/87; 43%). The performance of GPT-4 lay within the range of expert\nophthalmologists and ophthalmology trainees.\n\nhttps://doi.org/10.1371/journal.pdig.0000341.g001\n\nThe performance of GPT-4 was statistically similar to the mean score attained\nby expert ophthalmologists (Fig 1; \u03c7^2 = 1.18, p = 0.28). Moreover, GPT-4\u2019s\nperformance exceeded the mean mark attained across FRCOphth Part 2 written\nexamination candidates between 2017\u20132022 (66.06%), mean pass mark according to\nstandard setting (61.31%), and the mean official mark required to pass the\nexamination after adjustment (63.75%), as detailed in S2 Table. In individual\ncomparisons with expert ophthalmologists, GPT-4 was equivalent in 3 cases (\u03c7^2\ntests, p > 0.05, S3 Table), and inferior in 2 cases (\u03c7^2 tests, p < 0.05;\nTable 2). In comparisons with ophthalmology trainees, GPT-4 was equivalent to\nall three ophthalmology trainees (\u03c7^2 tests, p > 0.05; Table 2). GPT-4 was\nsignificantly superior to both unspecialised trainee doctors (\u03c7^2 tests, p <\n0.05; Table 2). Doctors were anonymised in analysis, but their\nophthalmological experience is summarised in S3 Table. Unsurprisingly, junior\ndoctors (J1-J2) attained lower scores than expert ophthalmologists (E1-E5; t =\n7.18, p < 0.01), and ophthalmology trainees (T1-T3; t = 11.18, p < 0.01),\nillustrated in Fig 1. Ophthalmology trainees approached expert-level scores\nwith no significant difference between the groups (t = 1.55, p = 0.18). None\nof the other LLMs matched any of the expert ophthalmologists, mean mark of\nreal examination candidates, or FRCOphth Part 2 pass mark.\n\nExpert ophthalmologists agreed that the mock examination was a faithful\nrepresentation of actual FRCOphth Part 2 Written Examination papers with a\nmean and median score of 3/5 (range 2-4/5).\n\nDownload:\n\n  * PPT\n\nPowerPoint slide\n\n  * PNG\n\nlarger image\n\n  * TIFF\n\noriginal image\n\nTable 2. GPT-4 compares favourably with LLMs and doctors.\n\nResults of pair-wise comparisons of examination performance between GPT-4 and\nthe other answer providers. Significantly greater performance for GPT-4 is\nhighlighted green, significantly inferior performance for GPT-4 is highlighted\norange. GPT-4 was superior to all other LLMs and unspecialised junior doctors,\nand equivalent to most expert ophthalmologists and all ophthalmology trainees.\n\nhttps://doi.org/10.1371/journal.pdig.0000341.t002\n\n#### LLM strengths and weaknesses are similar to doctors.\n\nAgreement between answers given by LLMs, expert ophthalmologists, and trainee\ndoctors was generally absent (0 \u2264 \u03ba < 0.2), minimal (0.2 \u2264 \u03ba < 0.4), or weak\n(0.4 \u2264 \u03ba < 0.6), with moderate agreement only recorded for one pairing between\nthe two highest performing ophthalmologists (Fig 2; \u03ba = 0.64) [19].\nDisagreement was primarily the result of general differences in knowledge and\nreasoning ability, illustrated by strong negative correlation between Kappa\nstatistic (quantifying agreement) and difference in examination performance\n(Pearson\u2019s r = -0.63, p < 0.01). Answer providers with more similar scores\nexhibited greater agreement overall irrespective of their category (LLM,\nexpert ophthalmologist, ophthalmology trainee, or junior doctor).\n\nDownload:\n\n  * PPT\n\nPowerPoint slide\n\n  * PNG\n\nlarger image\n\n  * TIFF\n\noriginal image\n\nFig 2. Heat map of Kappa statistics quantifying agreement between answers\ngiven by LLMs, expert ophthalmologists, and trainee doctors.\n\nAgreement correlates strongly with overall performance and stratification\nanalysis found no particular question type or subject was associated with\nbetter performance of LLMs or doctors, indicating that LLM knowledge and\nreasoning ability is general across ophthalmology rather than restricted to\nparticular subspecialties or question types.\n\nhttps://doi.org/10.1371/journal.pdig.0000341.g002\n\nStratification analysis was undertaken to identify any specific strengths and\nweaknesses of LLMs with respect to expert ophthalmologists and trainee doctors\n(Table 1 and S4 Table). No significant difference between performance in first\norder fact recall and higher order reasoning questions was observed among any\nof the LLMs, expert ophthalmologists, ophthalmology trainees, or unspecialised\njunior doctors (S4 Table; \u03c7^2 tests, p > 0.05). Similarly, only J1 (junior\ndoctor yet to commence ophthalmology training) exhibited statistically\nsignificant variation in performance between subjects (S4 Table; Fisher\u2019s\nexact tests over 2000 iterations, p = 0.02); all other doctors and LLMs\nexhibited no significant variation (Fisher\u2019s exact tests over 2000 iterations,\np > 0.05). To explore whether consistency was due to an insufficient sample\nsize, similar analyses were run for GPT-3.5 and GPT-4 performance over the\nlarger set of 347 questions (S1 Table; S4 Table). As with the mock\nexamination, no significant differences in performance across question types\n(S4 Table; \u03c7^2 tests, p > 0.05) or subjects (S4 Table; Fisher\u2019s exact tests\nover 2000 iterations, p > 0.05) were observed.\n\n#### LLM examination performance translates to subjective preference indicated\nby expert ophthalmologists.\n\nOphthalmologists\u2019 appraisal of GPT-4 and GPT-3.5 outputs indicated a marked\npreference for the former over the latter, mirroring objective performance in\nthe mock examination and over the whole textbook. GPT-4 exhibited\nsignificantly (t-test with Bonferroni correction, p < 0.05) higher accuracy\nand relevance than GPT-3.5 according to all five ophthalmologists\u2019 grading\n(Table 3). Differences were visually obvious, with GPT-4 exhibiting much\nhigher rates of attaining the highest scores for accuracy and relevance than\nGPT-3.5 (Fig 3). This superiority was reflected in ophthalmologists\u2019\nqualitative preference indications: GPT-4 responses were preferred to GPT-3.5\nresponses by every ophthalmologist with statistically significant skew in\nfavour of GPT-4 (\u03c7^2 test, p < 0.05; Table 3).\n\nDownload:\n\n  * PPT\n\nPowerPoint slide\n\n  * PNG\n\nlarger image\n\n  * TIFF\n\noriginal image\n\nFig 3. Accuracy and relevance of GPT-3.5 and GPT-4 in response to\nophthalmological questions.\n\nAccuracy (A) and relevance (B) ratings were provided by five expert\nophthalmologists for ChatGPT (powered by GPT-3.5 and GPT-4) responses to 87\nFRCOphth Part 2 mock examination questions. In every case, the accuracy and\nrelevance of GPT-4 is significantly superior to GPT-3.5 (t-test with\nBonferroni correct applied, p < 0.05). Pooled scores for accuracy (C) and\nrelevance (D) from all five raters are presented in the bottom two plots, with\nGPT-3.5 (left bars) compared directly with GPT-4 (right bars).\n\nhttps://doi.org/10.1371/journal.pdig.0000341.g003\n\nDownload:\n\n  * PPT\n\nPowerPoint slide\n\n  * PNG\n\nlarger image\n\n  * TIFF\n\noriginal image\n\nTable 3. GPT-4 responses are preferred to GPT-3.5 responses by expert\nophthalmologists.\n\nt-test results with Bonferroni correction applied showing the superior\naccuracy and relevance of GPT-4 responses relative to GPT-3.5 responses in the\nopinion of five fully trained ophthalmologists (positive mean differences\nfavour GPT-4), and \u03c7^2 test showing that GPT-4 responses were preferred to\nGPT-3.5 responses by every ophthalmologist in their blinded qualitative\nappraisals.\n\nhttps://doi.org/10.1371/journal.pdig.0000341.t003\n\n#### Failure cases exhibit no association with subject, complexity, or human\nanswers.\n\nThe LLM failure cases\u2014where every LLM provided an incorrect answer\u2014are\nsummarised in Table 4. While errors made by LLMs were occasionally similar to\nthose made by trainee ophthalmologists and junior doctors, this association\nwas not consistent (Table 4). There was no preponderance of ophthalmological\nsubject or first or higher order questions in the failure cases, and questions\ndid not share a common theme, sentence structure, or grammatical construct\n(Table 4). Examination questions are redacted here to avoid breaching\ncopyright and prevent future LLMs accessing the test data during pretraining\nbut can be provided on request.\n\nDownload:\n\n  * PPT\n\nPowerPoint slide\n\n  * PNG\n\nlarger image\n\n  * TIFF\n\noriginal image\n\nTable 4. LLMs do not exhibit consistent weaknesses.\n\nSummary of LLM failure cases, where all models provided an incorrect answer to\nthe FRCOphth Part 2 mock examination question. No associations were found with\nhuman answers, complexity, subject, theme, sentence structure, or grammatic\nconstructs.\n\nhttps://doi.org/10.1371/journal.pdig.0000341.t004\n\n## Discussion\n\nHere, we present a clinical benchmark to gauge the ophthalmological\nperformance of LLMs, using a source of questions with very low risk of\ncontamination as the utilised textbook is not freely available online [17].\nPrevious studies have suggested that ChatGPT can provide useful responses to\nophthalmological queries, but often use online question sources which may have\nfeatured in LLMs\u2019 pretraining datasets [7, 12, 15, 22]. In addition, our\nemployment of multiple LLMs as well as fully qualified and training doctors\nprovides novel insight into the potential and limitations of state-of-the-art\nLLMs through head-to-head comparisons which provide clinical context and\nquantitative benchmarks of competence in ophthalmology. Subsequent research\nmay leverage our questions and results to gauge the performance of new LLMs\nand applications as they emerge.\n\nWe make three primary observations. First, performance of GPT-4 compares well\nto expert ophthalmologists and ophthalmology trainees, and exhibits pass-\nworthy performance in an FRCOphth Part 2 mock examination. PaLM 2 did not\nattain pass-worthy performance or match expert ophthalmologists\u2019 scores but\nwas within the spread of trainee doctors\u2019 performance. LLMs are approaching\nhuman expert-level knowledge and reasoning in ophthalmology, and significantly\nexceed the ability of non-specialist clinicians (represented here by\nunspecialised junior doctors) to answer ophthalmology questions. Second,\nclinician grading of model outputs suggests that GPT-4 exhibits improved\naccuracy and relevance when compared with GPT-3.5. Development is producing\nmodels which generate better outputs to ophthalmological queries in the\nopinion of expert human clinicians, which suggests that models are becoming\nmore capable of providing useful assistance in clinical settings. Third, LLM\nperformance was consistent across question subjects and types, distributed\nsimilarly to human performance, and exhibited comparable agreement between\nother LLMs and doctors when corrected for differences in overall performance.\nTogether, this indicates that the ophthalmological knowledge and reasoning\ncapability of LLMs is general rather than limited to certain subspecialties or\ntasks. LLM-driven natural language processing seems to facilitate\nsimilar\u2014although idiosyncratic\u2014clinical knowledge and reasoning to human\nclinicians, with no obvious blind spots precluding clinical use.\n\nSimilarly dramatic improvements in the performance of GPT-4 relative to\nGPT-3.5 have been reported in the context of the North American Ophthalmology\nKnowledge Assessment Program (OKAP) [13, 15]. State-of-the-art models exhibit\nfar more clinical promise than their predecessors, and expectations and\ndevelopment should be tailored accordingly. Results from the OKAP also suggest\nthat improvement in performance is due to GPT-4 being more well-rounded than\nGPT-3.5 [13]. This increases the scope for potential applications of LLMs in\nophthalmology, as development is eliminating weaknesses rather than optimising\nin narrow domains. This study shows that well-rounded LLM performance compares\nwell with expert ophthalmologists, providing clinically relevant evidence that\nLLMs may be used to provide medical advice and assistance. Further improvement\nis expected as multimodal foundation models, perhaps based on LLMs such as\nGPT-4, emerge and facilitate compatibility with image-rich ophthalmological\ndata [3, 23, 24].\n\n### Limitations\n\nThis study was limited by three factors. First, examination performance is an\nunvalidated indicator of clinical aptitude. We sought to ameliorate this\nlimitation by employing expert ophthalmologists, ophthalmology trainees, and\nunspecialised junior doctors answering the same questions as clinical\nbenchmarks; and compared LLM performance to real cohorts of candidates in\nrecent FRCOphth examinations. However, it remains an issue that comparable\nperformance to clinical experts in an examination does not necessarily\ndemonstrate that an LLM can communicate with patients and practitioners or\ncontribute to clinical decision making accurately and safely. Early trials of\nLLM chatbots have suggested that LLM responses may be equivalent or even\nsuperior to human doctors in terms of accuracy and empathy, and experiments\nusing complicated case studies suggest that LLMs operate well even outside\ntypical presentations and more common medical conditions [4,25,26]. In\nophthalmology, GPT-3.5 and GPT-4 have been shown to be capable of providing\nprecise and suitable triage decisions when queried with eye-related symptoms\n[22,27]. Further work is now warranted in conventional clinical settings.\n\nSecond, while the study was sufficiently powered to detect a less than 10%\ndifference in overall performance, the relatively small number of questions in\ncertain categories used for stratification analysis may mask significant\ndifferences in performance. Testing LLMs and clinicians with more questions\nmay help establish where LLMs exhibit greater or lesser ability in\nophthalmology. Furthermore, researchers using different ways to categorise\nquestions may be able to identify specific strengths and weaknesses of LLMs\nand doctors which could help guide design of clinical LLM interventions.\n\nFinally, experimental tasks were \u2018zero-shot\u2019 in that LLMs were not provided\nwith any examples of correctly answered questions before it was queried with\nFRCOphth questions from the textbook. This mode of interrogation entails the\nmaximal level of difficulty for LLMs, so it is conceivable that the\nophthalmological knowledge and reasoning encoded within these models is\nactually even greater than indicated by results here [1]. Future research may\nseek to fine-tune LLMs by using more domain-specific text during pretraining\nand fine-tuning, or by providing examples of successfully completed tasks to\nfurther improve performance in that clinical task [3].\n\n### Future directions\n\nAutonomous deployment of LLMs is currently precluded by inaccuracy and fact\nfabrication. Our study found that despite meeting expert standards, state-of-\nthe-art LLMs such as GPT-4 do not match top-performing ophthalmologists [28].\nMoreover, there remain controversial ethical questions about what roles should\nand should not be assigned to inanimate AI models, and to what extent human\nclinicians must remain responsible for their patients [3]. However, the\nremarkable performance of GPT-4 in ophthalmology examination questions\nsuggests that LLMs may be able to provide useful input in clinical contexts,\neither to assist clinicians in their day-to-day work or with their education\nor preparation for examinations [3,13,14,27]. Further improvement in\nperformance may be obtained by specific fine-tuning of models with high\nquality ophthalmological text data, requiring curation and deidentification\n[29]. GPT-4 may prove especially useful where access to ophthalmologists is\nlimited: provision of advice, diagnosis, and management suggestions by a model\nwith FRCOphth Part 2-level knowledge and reasoning ability is likely to be\nsuperior to non-specialist doctors and allied healthcare professionals working\nwithout support, as their exposure to and knowledge of eye care is limited\n[27,30,31].\n\nHowever, close monitoring is essential to avoid mistakes caused by inaccuracy\nor fact fabrication [32]. Clinical applications would also benefit from an\nuncertainty indicator reducing the risk of erroneous decisions [7]. As LLM\nperformance often correlates with the frequency of query terms\u2019 representation\nin the model\u2019s training dataset, a simple indicator of \u2018familiarity\u2019 could be\nengineered by calculating the relative frequency of query term representation\nin the training data [7,33]. Users could appraise familiarity to temper their\nconfidence in answers provided by the LLM, perhaps reducing error. Moreover,\nophthalmological applications require extensive validation, preferably with\nhigh quality randomised controlled trials to conclusively demonstrate benefit\n(or lack thereof) conferred to patients by LLM interventions [34]. Trials\nshould be pragmatic so as not to inflate effect sizes beyond what may\ngeneralise to patients once interventions are implemented at scale [34,35]. In\naddition to patient outcomes, practitioner-related variables should also be\nconsidered: interventions aiming to improve efficiency should be specifically\ntested to ensure that they reduce rather than increase clinicians\u2019 workload\n[3].\n\n### Conclusion\n\nAccording to comparisons with expert and trainee doctors, state-of-the-art\nLLMs are approaching expert-level performance in advanced ophthalmology\nquestions. GPT-4 attains pass-worthy performance in FRCOphth Part 2 questions\nand exceeds the scores of some expert ophthalmologists. As top-performing\ndoctors exhibit superior scores, LLMs do not appear capable of replacing\nophthalmologists, but state-of-the-art models could provide useful advice and\nassistance to non-specialists or patients where access to eye care\nprofessionals is limited [27,28]. Further research is required to design LLM-\nbased interventions which may improve eye health outcomes, validate\ninterventions in clinical trials, and engineer governance structures to\nregulate LLM applications as they begin to be deployed in clinical settings\n[36].\n\n## Supporting information\n\nChatGPT performance in questions taken from the whole textbook.\n\nShowing 1/6: pdig.0000341.s001.eps\n\nsorry, we can't preview this file\n\n### S1 Fig. ChatGPT performance in questions taken from the whole textbook.\n\nMosaic plot depicting the overall performance of ChatGPT versions powered by\nGPT-3.5 and GPT-4 in 360 FRCOphth Part 2 written examination questions.\nPerformance was significantly higher for GPT-4 than GPT-3.5, and was close to\nmean human examination candidate performance and pass mark set by standard\nsetting and after adjustment.\n\nhttps://doi.org/10.1371/journal.pdig.0000341.s001\n\n(EPS)\n\n### S1 Table. Question characteristics and performance of GPT-3.5 and GPT-4\nover the whole textbook.\n\nSimilar observations were noted here to the smaller mock examination used for\nsubsequent experiments. GPT-4 performs to a significantly higher standard than\nGPT-3.5\n\nhttps://doi.org/10.1371/journal.pdig.0000341.s002\n\n(XLSX)\n\n### S2 Table. Examination statistics corresponding to FRCOphth Part 2 written\nexaminations sat between July 2017-December 2022.\n\nhttps://doi.org/10.1371/journal.pdig.0000341.s003\n\n(XLSX)\n\n### S3 Table. Experience of expert ophthalmologists (E1-E5), ophthalmology\ntrainees (T1-T3), and unspecialised junior doctors (J1-J2) involved in\nexperiments.\n\nhttps://doi.org/10.1371/journal.pdig.0000341.s004\n\n(XLSX)\n\n### S4 Table. Results of statistical tests of variation in performance between\nquestion subjects and types, for each trialled LLM, expert ophthalmologist,\nand trainee doctor.\n\nStatistically significant results are highlighted in green.\n\nhttps://doi.org/10.1371/journal.pdig.0000341.s005\n\n(XLSX)\n\n### S1 Protocol. Procedures followed by ophthalmologists to grade the output\nof GPT-3.5 and GPT-4 in terms of accuracy, relevance, and rater-preference of\nmodel outputs.\n\nhttps://doi.org/10.1371/journal.pdig.0000341.s006\n\n(PDF)\n\n## Acknowledgments\n\nThe authors extend their thanks to Mr Arunachalam Thirunavukarasu (Betsi\nCadwaladr University Health Board) for his advice and assistance with\nrecruitment.\n\n## References\n\n  1. 1\\. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, et al. Language Models are Few-Shot Learners. In: Advances in Neural Information Processing Systems [Internet]. Curran Associates, Inc.; 2020 [cited 2023 Jan 30]. p. 1877\u2013901. Available from: https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html\n\n  2. 2\\. OpenAI. GPT-4 Technical Report [Internet]. arXiv; 2023 [cited 2023 Apr 11]. Available from: http://arxiv.org/abs/2303.08774\n\n  3. 3\\. Thirunavukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan TF, Ting DSW. Large language models in medicine. Nat Med. 2023 Jul 17;29:1930\u201340. pmid:37460753\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  4. 4\\. Ayers JW, Poliak A, Dredze M, Leas EC, Zhu Z, Kelley JB, et al. Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum. JAMA Internal Medicine [Internet]. 2023 Apr 28 [cited 2023 Apr 28]; Available from: pmid:37115527\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  5. 5\\. Gilson A, Safranek CW, Huang T, Socrates V, Chi L, Taylor RA, et al. How Does ChatGPT Perform on the United States Medical Licensing Examination? The Implications of Large Language Models for Medical Education and Knowledge Assessment. JMIR Med Educ. 2023;9(101684518):e45312.\n\n     * View Article\n     * Google Scholar\n  6. 6\\. Kung TH, Cheatham M, Medenilla A, Sillos C, Leon LD, Elepa\u00f1o C, et al. Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models. PLOS Digital Health. 2023 Feb 9;2(2):e0000198. pmid:36812645\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  7. 7\\. Thirunavukarasu AJ, Hassan R, Mahmood S, Sanghera R, Barzangi K, Mukashfi ME, et al. Trialling a Large Language Model (ChatGPT) in General Practice With the Applied Knowledge Test: Observational Study Demonstrating Opportunities and Limitations in Primary Care. JMIR Medical Education. 2023 Apr 21;9(1):e46599. pmid:37083633\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  8. 8\\. Nori H, King N, McKinney SM, Carignan D, Horvitz E. Capabilities of GPT-4 on Medical Challenge Problems [Internet]. arXiv; 2023 [cited 2023 Mar 26]. Available from: http://arxiv.org/abs/2303.13375\n\n     * View Article\n     * Google Scholar\n  9. 9\\. Google. PaLM 2 Technical Report [Internet]. 2023 [cited 2023 May 11]. Available from: https://ai.google/static/documents/palm2techreport.pdf\n\n  10. 10\\. Touvron H, Martin L, Stone K. Llama 2: Open Foundation and Fine-Tuned Chat Models [Internet]. 2023. Available from: https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/\n\n     * View Article\n     * Google Scholar\n  11. 11\\. Ting DSJ, Tan TF, Ting DSW. ChatGPT in ophthalmology: the dawn of a new era? Eye (Lond). 2023 Jun 27; pmid:37369764\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  12. 12\\. Antaki F, Touma S, Milad D, El-Khoury J, Duval R. Evaluating the Performance of ChatGPT in Ophthalmology: An Analysis of its Successes and Shortcomings. Ophthalmology Science [Internet]. 2023 May 4 [cited 2023 May 8];0(0). Available from: https://www.ophthalmologyscience.org/article/S2666-9145(23)00056-8/fulltext pmid:37334036\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  13. 13\\. Teebagy S, Colwell L, Wood E, Yaghy A, Faustina M. Improved Performance of ChatGPT-4 on the OKAP Exam: A Comparative Study with ChatGPT-3.5 [Internet]. medRxiv; 2023 [cited 2023 Apr 23]. p. 2023.04.03.23287957. Available from: https://www.medrxiv.org/content/10.1101/2023.04.03.23287957v1\n\n     * View Article\n     * Google Scholar\n  14. 14\\. Raimondi R, Tzoumas N, Salisbury T, Di Simplicio S, Romano MR. Comparative analysis of large language models in the Royal College of Ophthalmologists fellowship exams. Eye. 2023 May 9;1\u20134. pmid:37161074\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  15. 15\\. Mihalache A, Popovic MM, Muni RH. Performance of an Artificial Intelligence Chatbot in Ophthalmic Knowledge Assessment. JAMA Ophthalmology. 2023 Jun 1;141(6):589\u201397. pmid:37103928\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  16. 16\\. Thirunavukarasu AJ. ChatGPT cannot pass FRCOphth examinations: implications for ophthalmology and large language model artificial intelligence. Eye News [Internet]. 2023 Apr 26 [cited 2023 Apr 26];30(1). Available from: https://www.eyenews.uk.com/features/ophthalmology/post/chatgpt-cannot-pass-frcophth-examinations-implications-for-ophthalmology-and-large-language-model-artificial-intelligence\n\n     * View Article\n     * Google Scholar\n  17. 17\\. Ting DSJ, Steel D. MCQs for FRCOphth Part 2. Oxford University Press; 2020. 253 p.\n\n  18. 18\\. Adams NE. Bloom\u2019s taxonomy of cognitive learning objectives. J Med Libr Assoc. 2015 Jul;103(3):152\u20133. pmid:26213509\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  19. 19\\. McHugh ML. Interrater reliability: the kappa statistic. Biochemia Medica. 2012 Oct 15;22(3):276\u201382. pmid:23092060\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  20. 20\\. Sullivan GM, Artino AR. Analyzing and Interpreting Data From Likert-Type Scales. J Grad Med Educ. 2013 Dec;5(4):541\u20132. pmid:24454995\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  21. 21\\. Part 2 Written FRCOphth Exam [Internet]. The Royal College of Ophthalmologists. [cited 2023 Jan 30]. Available from: https://www.rcophth.ac.uk/examinations/rcophth-exams/part-2-written-frcophth-exam/\n\n  22. 22\\. Tsui JC, Wong MB, Kim BJ, Maguire AM, Scoles D, VanderBeek BL, et al. Appropriateness of ophthalmic symptoms triage by a popular online artificial intelligence chatbot. Eye. 2023 Apr 29;1\u20132. pmid:37120656\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  23. 23\\. Nath S, Marie A, Ellershaw S, Korot E, Keane PA. New meaning for NLP: the trials and tribulations of natural language processing with GPT-3 in ophthalmology. British Journal of Ophthalmology. 2022 Jul 1;106(7):889\u201392. pmid:35523534\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  24. 24\\. Kline A, Wang H, Li Y, Dennis S, Hutch M, Xu Z, et al. Multimodal machine learning in precision health: A scoping review. npj Digit Med. 2022 Nov 7;5(1):1\u201314.\n\n     * View Article\n     * Google Scholar\n  25. 25\\. Kulkarni PA, Singh H. Artificial Intelligence in Clinical Diagnosis: Opportunities, Challenges, and Hype. JAMA [Internet]. 2023 Jul 6 [cited 2023 Jul 7]; Available from: https://doi.org/10.1001/jama.2023.11440\n\n     * View Article\n     * Google Scholar\n  26. 26\\. Singhal K, Azizi S, Tu T, Mahdavi SS, Wei J, Chung HW, et al. Large language models encode clinical knowledge. Nature. 2023 Jul 12;1\u20139.\n\n     * View Article\n     * Google Scholar\n  27. 27\\. Waisberg E, Ong J, Zaman N, Kamran SA, Sarker P, Tavakkoli A, et al. GPT-4 for triaging ophthalmic symptoms. Eye. 2023 May 25;1\u20132. pmid:37231187\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  28. 28\\. Thirunavukarasu AJ. Large language models will not replace healthcare professionals: curbing popular fears and hype. J R Soc Med. 2023;116(5):181\u20132. pmid:37199678\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  29. 29\\. Tan TF, Thirunavukarasu AJ, Campbell JP, Keane PA, Pasquale LR, Abramoff MD, et al. Generative Artificial Intelligence through ChatGPT and Other Large Language Models in Ophthalmology: Clinical Applications and Challenges. Ophthalmology Science. 2023 Sep 5;3(4):100394. pmid:37885755\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  30. 30\\. Alsaedi MG, Alhujaili HO, Fairaq GS, Alwdaan SA, Alwadan RA. Emergent Ophthalmic Disease Knowledge among Non-Ophthalmologist Healthcare Professionals in the Western Region of Saudi Arabia: Cross-Sectional Study. The Open Ophthalmology Journal [Internet]. 2022 Mar 25 [cited 2023 Jul 12];16(1). Available from: https://openophthalmologyjournal.com/VOLUME/16/ELOCATOR/e187436412203160/FULLTEXT/\n\n     * View Article\n     * Google Scholar\n  31. 31\\. Tan TF, Thirunavukarasu AJ, Jin L, Lim J, Poh S, Teo ZL, et al. Artificial intelligence and digital health in global eye health: opportunities and challenges. The Lancet Global Health. 2023 Sep 1;11(9):e1432\u201343. pmid:37591589\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  32. 32\\. Bakken S. AI in health: keeping the human in the loop. Journal of the American Medical Informatics Association. 2023 Jul 1;30(7):1225\u20136. pmid:37337923\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  33. 33\\. Biderman S, Schoelkopf H, Anthony Q, Bradley H, O\u2019Brien K, Hallahan E, et al. Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling [Internet]. arXiv; 2023 [cited 2023 May 6]. Available from: http://arxiv.org/abs/2304.01373\n\n     * View Article\n     * Google Scholar\n  34. 34\\. Thirunavukarasu AJ. How Can the Clinical Aptitude of AI Assistants Be Assayed? Journal of Medical Internet Research. 2023 Dec 5;25(1):e51603. pmid:38051572\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  35. 35\\. Tossaint-Schoenmakers R, Versluis A, Chavannes N, Talboom-Kamp E, Kasteleyn M. The Challenge of Integrating eHealth Into Health Care: Systematic Literature Review of the Donabedian Model of Structure, Process, and Outcome. J Med Internet Res. 2021 May 10;23(5):e27180. pmid:33970123\n\n     * View Article\n     * PubMed/NCBI\n     * Google Scholar\n  36. 36\\. Mesk\u00f3 B, Topol EJ. The imperative for regulatory oversight of large language models (or generative AI) in healthcare. npj Digit Med. 2023 Jul 6;6(1):1\u20136.\n\n     * View Article\n     * Google Scholar\n\nDownload PDF\n\n  * Citation\n  * XML\n\nPrint\n\nShare\n\n  * Reddit\n  * Facebook\n  * LinkedIn\n  * Mendeley\n  * Twitter\n  * Email\n\nAdvertisement\n\n###\n\nSubject Areas\n\n?\n\nFor more information about PLOS Subject Areas, click here.\n\nWe want your feedback. Do these Subject Areas make sense for this article?\nClick the target next to the incorrect Subject Area and let us know. Thanks\nfor your help!\n\n  * Ophthalmology\n\nIs the Subject Area \"Ophthalmology\" applicable to this article?\n\nThanks for your feedback.\n\n  * Trainees\n\nIs the Subject Area \"Trainees\" applicable to this article?\n\nThanks for your feedback.\n\n  * Human performance\n\nIs the Subject Area \"Human performance\" applicable to this article?\n\nThanks for your feedback.\n\n  * Physicians\n\nIs the Subject Area \"Physicians\" applicable to this article?\n\nThanks for your feedback.\n\n  * Textbooks\n\nIs the Subject Area \"Textbooks\" applicable to this article?\n\nThanks for your feedback.\n\n  * Eyes\n\nIs the Subject Area \"Eyes\" applicable to this article?\n\nThanks for your feedback.\n\n  * Language\n\nIs the Subject Area \"Language\" applicable to this article?\n\nThanks for your feedback.\n\n  * Internet\n\nIs the Subject Area \"Internet\" applicable to this article?\n\nThanks for your feedback.\n\n  * Publications\n  * PLOS Biology\n  * PLOS Climate\n  * PLOS Complex Systems\n  * PLOS Computational Biology\n  * PLOS Digital Health\n  * PLOS Genetics\n  * PLOS Global Public Health\n\n  * PLOS Medicine\n  * PLOS Mental Health\n  * PLOS Neglected Tropical Diseases\n  * PLOS ONE\n  * PLOS Pathogens\n  * PLOS Sustainability and Transformation\n  * PLOS Water\n\n  * Home\n  * Blogs\n  * Collections\n  * Give feedback\n  * LOCKSS\n\n  * Privacy Policy\n  * Terms of Use\n  * Advertise\n  * Media Inquiries\n  * Contact\n\nPLOS is a nonprofit 501(c)(3) corporation, #C2354500, based in San Francisco,\nCalifornia, US\n\n### Cookie Preference Center\n\nOur website uses different types of cookies. Optional cookies will only be set\nwith your consent and you may withdraw this consent at any time. Below you can\nlearn more about the types of cookies PLOS uses and register your cookie\npreferences.\n\n### Customize Your Cookie Preference\n\n+Strictly Necessary\n\nAlways On\n\n+Functional\n\n+Performance and Analytics\n\n+Marketing\n\nFor more information about the cookies and other technologies used by us,\nplease read our Cookie Policy.\n\n", "frontpage": false}
