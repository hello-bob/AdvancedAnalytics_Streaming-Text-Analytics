{"aid": "40246128", "title": "Competing in a data science contest without reading the data (2015)", "url": "http://blog.mrtz.org/2015/03/09/competition.html", "domain": "mrtz.org", "votes": 1, "user": "zebreus", "posted_at": "2024-05-03 10:31:01", "comments": 0, "source_title": "Competing in a data science contest without reading the data", "source_text": "Competing in a data science contest without reading the data\n\nMoody Rd\n\n# Competing in a data science contest without reading the data\n\nMar 9, 2015 \u2022 Moritz Hardt\n\nMachine learning competitions have become an extremely popular format for\nsolving prediction and classification problems of all sorts. The most famous\nexample is perhaps the Netflix prize. An even better example is Kaggle, an\nawesome startup that\u2019s organized more than a hundred competitions over the\npast few years.\n\nThe central component of any competition is the public leaderboard.\nCompetitors can repeatedly submit a list of predictions and see how their\npredictions perform on a set of holdout labels not available to them. The\nleaderboard ranks all teams according to their prediction accuracy on the\nholdout labels. Once the competition closes all teams are scored on a final\ntest set not used so far. The resulting ranking, called private leaderboard,\ndetermines the winner.\n\nPublic leaderboard of the Heritage Health Prize (Source)\n\nIn this post, I will describe a method to climb the public leaderboard without\neven looking at the data. The algorithm is so simple and natural that an\nunwitting analyst might just run it. We will see that in Kaggle\u2019s famous\nHeritage Health Prize competition this might have propelled a participant from\nrank around 150 into the top 10 on the public leaderboard without making\nprogress on the actual problem. The Heritage Health Prize competition ran for\ntwo years and had a prize pool of 3 million dollars. Keep in mind though that\nthe standings on the public leaderboard do not affect who gets the money.\n\nThe point of this post is to illustrate why maintaining a leaderboard that\naccurately reflects the true performance of each team is a difficult and deep\nproblem. While there are decades of work on estimating the true performance of\na model (or set of models) from a finite sample, the leaderboard application\nhighlights some challenges that while fundamental have only recently seen\nincreased attention. A follow-up post will describe a recent paper with Avrim\nBlum that gives an algorithm for maintaining a (provably) accurate public\nleaderboard.\n\nLet me be very clear that my point is not to criticize Kaggle or anyone else\norganizing machine learning competitions. On the contrary, I\u2019m amazed by how\nwell Kaggle competitions work. In my opinion, they have contributed a\ntremendous amount of value to both industry and education. I also know that\nKaggle has some very smart people thinking hard about how to anticipate\nproblems with competitions.\n\n## The Kaggle leaderboard mechanism\n\nAt first sight, the Kaggle mechanism looks like the classic holdout method.\nKaggle partitions the data into two sets: a training set and a holdout set.\nThe training set is publicly available with both the individual instances and\ntheir corresponding class labels. The instances of the holdout set are\npublicly available as well, but the class labels are withheld. Predicting\nthese missing class labels is the goal of the participant and a valid\nsubmission is a list of labels\u2014one for each point in the holdout set.\n\nKaggle specifies a score function that maps a submission consisting of N\nlabels to a numerical score, which we assume to be in [0,1]. Think of the\nscore as prediction error (smaller is better). For concreteness, let\u2019s fix it\nto be the misclassification rate. That is a prediction incurs loss 0 if it\nmatches the corresponding unknown label and loss 1 if it does not match it. We\ndivide by the number of predictions to get a score in [0,1].\n\nKaggle further splits its N private labels randomly into n holdout labels and\nN\u2212n test labels. Typically, n=0.3N. The public leaderboard is a sorting of all\nteams according to their score computed only on the n holdout labels (without\nusing the test labels), while the private leaderboard is the ranking induced\nby the test labels. I will let sH(y) denote the public score of a submission\ny, i.e., the score according to the public leaderboard. Typically, Kaggle\nrounds all scores to 5 or 6 digits of precision.\n\n## The cautionary tale of wacky boosting\n\nImagine your humble blogger in a parallel universe: I\u2019m new to this whole\nmachine learning craze. So, I sign up for a Kaggle competition to get some\nskills. Kaggle tells me that there\u2019s an unknown set of labels y\u2208{0,1}N that I\nneed to predict. Well, I know nothing about y. So here\u2019s what I\u2019m going to do.\nI try out a bunch of random vectors and keep all those that give me a slightly\nbetter than expected score. If we\u2019re talking about misclassification rate, the\nexpected score of a random binary vector is 0.5. So, I\u2019m keeping all the\nvectors with score less than 0.5. Then I recall something about boosting. It\ntells me that I can boost my accuracy by aggregating all predictors into a\nsingle predictor using the majority function. Slightly more formally, here\u2019s\nwhat I do:\n\nAlgorithm (Wacky Boosting):\n\n  1. Choose y1,...,yk\u2208{0,1}N uniformly at random.\n  2. Let I={i\u2208[k]:sH(yi)<0.5}.\n  3. Output y^=majority{yi:i\u2208I}, where the majority is component-wise.\n\nLo and behold, this is what happens:\n\nIn this plot, n=4000 and all numbers are averaged over 5 independent\nrepetitions.\n\nAs I\u2019m only seeing the public score (bottom red line), I get super excited. I\nkeep climbing the leaderboard! Who would\u2019ve thought that this machine learning\nthing was so easy? So, I go write a blog post on Medium about Big Data and\nscore a job at DeepCompeting.ly, the latest data science startup in the city.\nLife is pretty sweet. I pick up indoor rock climbing, sign up for wood working\nclasses; I read Proust and books about espresso. Two months later the\ncompetition closes and Kaggle releases the final score. What an embarrassment!\nWacky boosting did nothing whatsoever on the final test set. I get fired from\nDeepCompeting.ly days before the buyout. My spouse dumps me. The lease\nexpires. I get evicted from my apartment in the Mission. Inevitably, I hike\nthe Pacific Crest Trail and write a novel about it.\n\n### What just happened\n\nLet\u2019s understand what went wrong and how you can avoid hiking the Pacific\nCrest Trail. To start out with, each yi has loss around 1/2\u00b11/n\u2212\u2212\u221a. We\u2019re\nselecting the ones that are biased below a half. This introduces a bias in the\nscore and the conditional expected bias of each selected vector wi is roughly\n1/2\u2212c/n\u2212\u2212\u221a for some positive constant c>0\\. Put differently, each selected yi\nis giving us a guess about each label in the unknown holdout set H\u2286[N] that\u2019s\ncorrect with probability 1/2+\u03a9(1/n\u2212\u2212\u221a). Since the public score doesn\u2019t depend\non labels outside of H, the conditioning does not affect the final test set.\nThe labels outside of H are still unbiased. Finally, we need to argue that the\nmajority vote \u201cboosts\u201d our slightly biased coin tosses into a stronger bias.\nMore formally, we can show that y^ gives us a guess for each label in H that\u2019s\ncorrect with probability\n\n12+\u03a9(k/n\u2212\u2212\u2212\u221a).\n\nHence, the public score of y satisfies\n\nsH(y)<12\u2212\u03a9(k/n\u2212\u2212\u2212\u221a).\n\nOutside of H, however, we\u2019re just random guessing with no advantage. To\nsummarize, wacky boosting gives us a bias of k\u2212\u2212\u221a standard deviations on the\npublic score with k submissions.\n\nWhat\u2019s important is that the same algorithm still \u201cworks\u201d even if we don\u2019t get\nexact answers. All we need are answers that are accurate to an additive error\nof 1/n\u2212\u2212\u221a. This is important since Kaggle rounds its answers to 5 digits of\nprecision. In particular, this attack will work so long as n<1010.\n\n### Why the holdout method breaks down\n\nThe idea behind the holdout method is that the holdout data serve as a fresh\nsample providing an unbiased and well-concentrated estimate of the true loss\nof the classifier on the underlying distribution. Why then didn\u2019t the holdout\nmethod detect that our wacky boosting algorithm was overfitting? The short\nanswer is that the holdout method is simply not valid in the way it\u2019s used in\na competition.\n\nOne point of departure from the classic method is that the participants\nactually do see the data points corresponding to holdout labels which can lead\nto some problems. But that\u2019s not the issue here and even if they we don\u2019t look\nat the holdout data points at all, there\u2019s a fundamental reason why the\nvalidity of the classic holdout method breaks down.\n\nThe problem is that a submission in general incorporates information about the\nholdout labels previously released through the leaderboard mechanism. As a\nresult, there is a statistical dependence between the holdout data and the\nsubmission. Due to this feedback loop, the public score is in general no\nlonger an unbiased estimate of the true score. There is no reason not to\nexpect the submissions to eventually overfit to the holdout set.\n\nThe problem of overfitting to the holdout set is well known. Kaggle\u2019s forums\nare full of anecdotal evidence reported by various competitors. The primary\nway Kaggle deals with this problem is by limiting the rate of re-submission\nand (to some extent) the bit precision of the answers. Of course, this is also\nthe reason why the winners are determined on a separate test set.\n\n### Static vs interactive data analysis\n\nKaggle\u2019s liberal use of the holdout method is just one example of a widespread\ndisconnect between the theory of static data analysis and the practice of\ninteractive data analysis. The holdout method is a static method in that it\nassumes the model to be independent of the holdout data on which it is\nevaluated. However, machine learning competitions are interactive, because\nsubmissions generally incorporate information from the holdout set.\n\nI contend that most real world data analysis is interactive. Unfortunately,\nmost of the theory on model validation and statistical estimation falls into\nthe static setting requiring independence between method and holdout data.\nThis divide is not inherent though. Indeed, my next post deals with some\nuseful theory for the interactive setting.\n\n## The Heritage Health Prize leaderboard\n\nLet\u2019s see how this could\u2019ve been applied to an actual competition. Of course,\nthe Heritage Health Prize competition is long over. We\u2019re about two years too\nlate to the party. Besides, we don\u2019t have the solution file for that\ncompetition. Without it there\u2019s no sure way of knowing how well this approach\nwould\u2019ve worked. Nevertheless, we can make some reasonable modeling of what\nthe holdout labels might look like using information that was released by\nKaggle and see how well we\u2019d be doing against our random model.\n\n### Generalized wacky boosting\n\nBefore we can apply wacky boosting to the Heritage prize, we need to clear two\nobstacles. First, wacky boosting required the domain to be Boolean whereas the\nlabels could be arbitrary positive real numbers. Second, the algorithm only\ngave an advantage over random guessing which might be too far from the top of\nthe leaderboard to start out with. It turns out that both of these issues can\nbe resolved nicely with a simple generalization of the previous algorithm.\nWhat was really happening in the algorithm is that we had two candidate\nsolutions, the all ones vector and the all zeros vector, and we tried out\nrandom coordinate-wise combinations of these vectors. The algorithm ends up\nfinding a coordinate wise combination of the two vectors that improves upon\ntheir mean loss, i.e., one half. This way of looking at it generalizes nicely.\nThe resulting algorithm is just a few lines of Julia code.\n\n    \n    \n    # select coordinate from v1 where v is 1 and from v2 where v is 0 combine(v,v1,v2) = v1 .* v + v2 .* (1-v) function wackyboost(v1,v2,k,score) m = mean([score(v1),score(v2)]) A = rand(0:1,(length(v1),k)) # select columns of A that give better than mean score a = filter(i -> score(combine(A[:,i],v1,v2)) < m,[1:k]) # take majority vote over all selected columns v = float(A[:,a] * ones(length(a)) .> length(a)/2) return combine(v,v1,v2) end\n\nI worked through the fun exercise of applying this algorithm in a separate\nJulia notebook. Here\u2019s one picture that came out of it. Don\u2019t treat the\nnumbers as definitive as they all depend on the modeling assumptions I made.\n\nWe see an improvement from 0.462311 (rank 146) to 0.451868 (rank 6).\n\nThe bottom line is: It seems to work reasonably well (under various semi-\nprincipled modeling assumptions I made). From the looks of it this might have\ngiven you an improvement from rank 150ish to 6ish within 700 submissions. Note\nthere was a single team with 671 submissions. There\u2019s a pretty good gap\nbetween number one on the public leaderboard and the rest. While possible in\nprinciple, it took me a bunch more submissions to get to the top. I should say\nthough that I used the completely generic code from above without any\noptimizations specific to the competition. I didn\u2019t even look at the data\npoints (as I don\u2019t have them). It\u2019s possible that using the data and domain\nknowledge could improve things much further. I chose the Heritage Health\nprize, because it was the highest prized Kaggle competition ever (3 million\ndollars) and it ran for two years with a substantial number of submissions.\n\n## How robust is your benchmark?\n\nThere\u2019s a broad lesson to be learned from this example. As computer scientists\nwe love numerical benchmarks and rankings of all sorts. They look so objective\nand scientific that we easily forget how any benchmark is just a proxy for a\nmore complex question. Every once in a while we should step back and ask: How\nrobust is the benchmark? Do improvements in our benchmark really correspond to\nprogress on the original problem? What I\u2019d love to see in all empirical\nbranches of computer science are adversarial robustness evaluations of various\nbenchmarks. How far can we get by gaming rather than by actually making\nprogress towards solving the problem?\n\nLet me end on a positive note. What excites me is that the serious problems we\nsaw in this post actually do have a fix (both in theory and in practice)! So,\nstay tuned for my next post.\n\nSubscribe to the RSS feed or follow me on Twitter.\n\n## Comments\n\n## Moody Rd\n\n  * Moody Rd\n  * blog@mrtz.org\n\n  * mrtzh\n  * mrtz\n\nA blog about machine learning.\n\n", "frontpage": false}
