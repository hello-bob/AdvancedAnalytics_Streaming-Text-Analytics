{"aid": "40247977", "title": "Ollama v0.1.33", "url": "https://github.com/ollama/ollama/releases/tag/v0.1.33", "domain": "github.com/ollama", "votes": 1, "user": "tosh", "posted_at": "2024-05-03 14:20:50", "comments": 0, "source_title": "Release v0.1.33 \u00b7 ollama/ollama", "source_text": "Release v0.1.33 \u00b7 ollama/ollama \u00b7 GitHub\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nollama / ollama Public\n\n  * Notifications\n  * Fork 4.5k\n  * Star 63.3k\n\n# v0.1.33\n\nLatest\n\nLatest\n\ngithub-actions released this 28 Apr 17:51\n\n\u00b7 12 commits to main since this release\n\nv0.1.33\n\n9164b01\n\n## New models:\n\n  * Llama 3: a new model by Meta, and the most capable openly available LLM to date\n  * Phi 3 Mini: a new 3.8B parameters, lightweight, state-of-the-art open model by Microsoft.\n  * Moondream moondream is a small vision language model designed to run efficiently on edge devices.\n  * Llama 3 Gradient 1048K: A Llama 3 fine-tune by Gradient to support up to a 1M token context window.\n  * Dolphin Llama 3: The uncensored Dolphin model, trained by Eric Hartford and based on Llama 3 with a variety of instruction, conversational, and coding skills.\n  * Qwen 110B: The first Qwen model over 100B parameters in size with outstanding performance in evaluations\n  * Llama 3 Gradient: A fine-tune of Llama 3 the supports a context window of up 1M tokens.\n\n## What's Changed\n\n  * Fixed issues where the model would not terminate, causing the API to hang.\n  * Fixed a series of out of memory errors on Apple Silicon Macs\n  * Fixed out of memory errors when running Mixtral architecture models\n\n## Experimental concurrency features\n\nNew concurrency features are coming soon to Ollama. They are available\n\n  * OLLAMA_NUM_PARALLEL: Handle multiple requests simultaneously for a single model\n  * OLLAMA_MAX_LOADED_MODELS: Load multiple models simultaneously\n\nTo enable these features, set the environment variables for ollama serve. For\nmore info see this guide:\n\n    \n    \n    OLLAMA_NUM_PARALLEL=4 OLLAMA_MAX_LOADED_MODELS=4 ollama serve\n\n## New Contributors\n\n  * @hmartinez82 made their first contribution in #3972\n  * @Cephra made their first contribution in #4037\n  * @arpitjain099 made their first contribution in #4007\n  * @MarkWard0110 made their first contribution in #4031\n  * @alwqx made their first contribution in #4073\n  * @sidxt made their first contribution in #3705\n  * @ChengenH made their first contribution in #3789\n  * @secondtruth made their first contribution in #3503\n  * @reid41 made their first contribution in #3612\n  * @ericcurtin made their first contribution in #3626\n  * @JT2M0L3Y made their first contribution in #3633\n  * @datvodinh made their first contribution in #3655\n  * @MapleEve made their first contribution in #3817\n  * @swuecho made their first contribution in #3810\n  * @brycereitano made their first contribution in #3895\n  * @bsdnet made their first contribution in #3889\n  * @fyxtro made their first contribution in #3855\n  * @natalyjazzviolin made their first contribution in #3962\n\nFull Changelog: v0.1.32...v0.1.33\n\n### Contributors\n\nsecondtruth, swuecho, and 16 other contributors\n\n130 people reacted\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
