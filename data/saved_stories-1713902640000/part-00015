{"aid": "40129943", "title": "PhysDreamer: Physics-Based Interaction with 3D Objects via Video Generation", "url": "https://physdreamer.github.io/", "domain": "physdreamer.github.io", "votes": 2, "user": "jasondavies", "posted_at": "2024-04-23 09:00:48", "comments": 0, "source_title": "PhysDreamer", "source_text": "PhysDreamer\n\n## PhysDreamer: Physics-Based Interaction with 3D Objects via Video Generation\n\n  * Tianyuan Zhang ^1\n  * Hong-Xing \"Koven\" Yu ^2\n  * Rundi Wu ^3\n  * Brandon Y. Feng ^1\n  * Changxi Zheng ^3\n  * Noah Snavely ^4\n  * Jiajun Wu ^2\n  * William T. Freeman ^1\n\n^1Massachusetts Institute of Technology, ^2Stanford University, ^3Columbia\nUniversity, ^4 Cornell University\n\nPaper Code\n\n### Poking an object in a variety of ways\n\nPhysDreamer enables realistic 3D interaction with objects.\n\n### Abstract\n\nRealistic object interactions are crucial for creating immersive virtual\nexperiences, yet synthesizing realistic 3D object dynamics in response to\nnovel interactions remains a significant challenge. Unlike unconditional or\ntext-conditioned dynamics generation, action-conditioned dynamics requires\nperceiving the physical material properties of objects and grounding the 3D\nmotion prediction on these properties, such as object stiffness. However,\nestimating physical material properties is an open problem due to the lack of\nmaterial ground-truth data, as measuring these properties for real objects is\nhighly difficult. We present PhysDreamer, a physics-based approach that endows\nstatic 3D objects with interactive dynamics by leveraging the object dynamics\npriors learned by video generation models. By distilling these priors,\nPhysDreamer enables the synthesis of realistic object responses to novel\ninteractions, such as external forces or agent manipulations. We demonstrate\nour approach on diverse examples of elastic objects and evaluate the realism\nof the synthesized interactions through a user study. PhysDreamer takes a step\ntowards more engaging and realistic virtual experiences by enabling static 3D\nobjects to dynamically respond to interactive stimuli in a physically\nplausible manner.\n\n### PhysDreamer = Physics-Based Simulation + Video Diffusion Prior\n\n(Left) Leveraging and distilling prior knowledge of dynamics from a pre-\ntrained video generation model, we estimate a physical material field for the\nstatic 3D object. (Right) The physical material field allows synthesizing\ninteractive 3D dynamics under arbitrary forces. We show rendered sequences\nfrom two viewpoints, with red arrows indicating force directions.\n\n### Results gallery for interactive motions\n\nWe present an interactive gallery showcasing our results in synthesizing\ninteractive motions for eight objects. The drawn circle and arrow roughly\nindicate the area and direction of the applied force (the circles are for\nillustrative purposes only and not drawn precisely).\n\n  * static viewpoint\n  * moving viewpoint\n\n  * force A\n  * force B\n\n### Comparison with baselines and real captured videos\n\nVisual comparison between our synthesized videos, real captured footage, and\nbaseline methods (DreamGaussian4D and PhysGaussian). Additionally, these\nvideos are the same ones utilized in our user study. Details about the user\nstudy is mentioned in the experiment section of the paper.\n\nReal Capture| Ours| PhysGaussian| DreamGaussian4D  \n---|---|---|---  \n  \nSelect different samples.\n\n  * Sample 0\n  * Sample 1\n  * Sample 2\n  * Sample 3\n\n### Citation\n\n### Acknowledgements\n\nWe would like to thank Peter Yichen Chen, Zhengqi Li, Pingchuan Ma, Minghao\nGuo, Ge Yang, and Shai Avidan for their help and insightful discussions. This\nwork is in part supported by the NSF PHY-2019786 (The NSF AI Institute for\nArtificial Intelligence and Fundamental Interactions), RI #2211258, #2338203,\nONR MURI N00014-22-1-2740, Quanta Computer, and Samsung\n\nThe website template was borrowed from ReconFusion, Ref-NeRF and DMD.\n\n", "frontpage": false}
