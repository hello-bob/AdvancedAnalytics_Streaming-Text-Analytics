{"aid": "40051191", "title": "Loading a trillion rows of weather data into TimescaleDB", "url": "https://aliramadhan.me/2024/03/31/trillion-rows.html", "domain": "aliramadhan.me", "votes": 47, "user": "PolarizedPoutin", "posted_at": "2024-04-16 12:39:07", "comments": 9, "source_title": "Building a weather data warehouse part I: Loading a trillion rows of weather data into TimescaleDB", "source_text": "Building a weather data warehouse part I: Loading a trillion rows of weather\ndata into TimescaleDB\n\n# Ali Ramadhan\n\nBuilding a weather data warehouse part I: Loading a trillion rows of weather\ndata into TimescaleDB\n\n31 March 2024\n\nGitHub Discussion\n\n  1. What are we even doing?\n\n    1. Why build a weather data warehouse?\n    2. What\u2019s the data?\n  2. The insert statement\n\n    1. Starting with just the single-row insert statement\n    2. Multi-valued insert\n  3. The copy statement\n\n    1. Upgrading to the copy statement\n    2. Sustaining copy insert rates\n    3. Parallel copy\n  4. Tools\n\n    1. pg_bulkload and timescaledb-parallel-copy\n    2. Multiple workers with timescaledb-parallel-copy\n  5. Tweaking Postgres settings\n  6. So what\u2019s the best method?\n  7. Appendices\n\n    1. Source code\n    2. Benchmarking methodology\n  8. Footnotes\n\nGlobal snapshot of surface temperature at 2018-12-04 04:00:00 UTC.\n\n# What are we even doing?\n\n## Why build a weather data warehouse?\n\nI think it would be cool to have historical weather data from around the world\nto analyze for signals of climate change we\u2019ve already had rather than think\nabout potential future change.\n\nIf we had a huge weather data warehouse we could query it to figure out\nwhether Jakarta is actually warmer or stormier these days and exactly how is\nit warmer (heat waves, winter highs, etc.). Or whether Chile is warming or\ngetting cloudier as a whole, which could be region-specific. We could do this\nkind of analysis for every city or region on Earth to find out which places\nhave already experienced the most climate change and what kind of change.\n\nBut to do this analysis globally we need to make querying the data warehouse\nfast, and there\u2019s a lot of data. The first step is to load the data into a\ndatabase of some kind. I\u2019m going to try using PostgreSQL here. It should be a\ngood learning experience and using TimescaleDB to speed up time-based queries\nand eventually PostGIS to speed up geospatial queries seems promising.\n\nTo get there though we first need to load all this data into Postgres and this\nis what this post is about. Initial attempts at loading the data seemed slow\nso I wanted to investigate how to do this fast, leading me down a rabbit hole\nand me writing this.^1\n\nAre we building a data warehouse? I think so...? Is a relational database even\nappropriate for gridded weather data? No idea but we\u2019ll find out.\n\n## What\u2019s the data?\n\nWe are not working with actual weather observations. They are great, but can\nbe sparse in certain regions especially in the past. Instead, we will be\nworking with the ERA5 climate reanalysis product^2. It\u2019s our best estimate of\nthe historical state of the Earth\u2019s weather and is widely used in weather and\nclimate research.\n\nThe data is output from a climate model run that is constrained to match\nweather observations. So where we have lots of weather observations, ERA5\nshould match it closely. And where we do not have any weather observations,\nERA5 will be physically consistent and should match the climatology, i.e. the\nsimulated weather\u2019s statistics should match reality. At the top of this page\nis a snapshot of what global surface temperature looks like and below is a\nsnapshot of global precipitation.\n\nGlobal snapshot of precipitation rate at 2018-12-04 04:00:00 UTC.\n\nHere\u2019s what a time series of temperature looks like at one location.\n\nTime series of surface temperature near Durban, South Africa.\n\nERA5 covers the entire globe at 0.25 degree resolution, and stretches back in\ntime to 1940 with hourly resolution. Hourly data stretching back to 1940 is\n727,080 snapshots in time for each variable like temperature, precipitation,\ncloud cover, wind speed, etc. A regularly-spaced latitude-longitude grid at\n0.25 degree resolution has 1,038,240 grid points or locations (1440 longitudes\nand 721 latitudes including both poles). Together that\u2019s 753,836,544,000 or\n~754 billion rows of data if indexed by time and location. That\u2019s a good\namount of data. And as I found out, it\u2019s not trivial to quickly shove this\ndata into a relational database, much less be able to query it quickly.\n\nThe ERA5 data is distributed as NetCDF^3 files. You can query an API for the\ndata or download it from certain providers but generally each file contains\ndata for a day, a month, or a year. This chunking by time makes it quick and\neasy to query the dataset at single points in time, but looking at temporal\npatterns is very slow as many files need to be read to pull out a single time\nseries. It takes like 20~30 minutes to pull out temperature data for one\nlocation to make the plot above! Complex geospatial queries, especially over\ntime, will be slow and difficult to perform. Packages like xarray and dask\n(and efforts by Pangeo) speed things up but it\u2019s still a slow process.\n\nWe\u2019ll just load in temperature, zonal and meridional wind speeds, total cloud\ncover, precipitation, and snowfall for each time and location so we\u2019ll use\nthis table schema:\n\n    \n    \n    create table weather ( time timestamptz not null, location_id int, latitude float4, longitude float4, temperature_2m float4, zonal_wind_10m float4, meridional_wind_10m float4, total_cloud_cover float4, total_precipitation float4, snowfall float4 );\n\nAnd before you mention database normalization, yes I have both a location_id\ncolumn and latitude and longitude columns. It\u2019s for later benchmarking with\nqueries and indexes.\n\n# The insert statement\n\n## Starting with just the single-row insert statement\n\nThe simplest way to load data into a table is by using the insert command to\ninsert a single row. This looks something like\n\n    \n    \n    insert into weather ( time, location_id, latitude, longitude, temperature_2m, zonal_wind_10m, meridional_wind_10m, total_cloud_cover, total_precipitation, snowfall ) values ('1995-03-10 16:00:00+00', 346441, -30, 30.25, 15.466888, -2.0585022, 0.25202942, 0.9960022, 0.007845461, 0);\n\nand so you can just loop over all the data doing this row-by-row.\nUnfortunately it is quite slow as quite a bit goes on behind the scenes here:\n\n  1. Postgres needs to parse the statement, validate table and column names, and plan the best way to execute it.\n  2. Postgres may need to lock the table to ensure data integrity.^4\n  3. The data is written to a buffer as Postgres uses a write-ahead logging^5 (or WAL) system.\n  4. Data from the buffer is actually inserted into the table on disk (which may involve navigating through and updating indexes, but we won\u2019t have any here).\n  5. If the insert statement is part of a transaction^6 that is committed, then the changes are made permanent.\n\nSo there\u2019s a lot of overhead associated with inserting single rows, especially\nif each insert gets its own transaction.\n\nHow many rows can we actually insert per second using single-row inserts?\nAfter loading the data from NetCDF into a pandas dataframe I found three^7\nways to insert the data into Postgres from Python so let\u2019s benchmark all\nthree:\n\n  1. pandas: You can insert data straight from a dataframe using the df.to_sql() function with the chunksize=1 keyword argument to force single-row inserts.\n  2. psycopg3: You can use parameterized queries to protect against SQL injection, not that it\u2019s a risk here (yet) but it\u2019s good to practice safety I guess. All inserts are part of one transaction that is committed at the end.\n  3. SQLAlchemy: You can similarly use named parameters in a parameterized query to prevent SQL injection attacks.\n\nBlue bars show the median insert rate into a regular PostgreSQL table, while\norange bars show the median insert rate into a TimescaleDB hypertable. Each\nbenchmark inserted 20k rows and was repeated 10 times. The error bars show the\nrange of insert rates given by the 10th and 90th percentiles.\n\nI benchmarked inserting into a regular Postgres table and a TimescaleDB\nhypertable^8.\n\nPandas and psycopg3 perform similarly, with a slight edge to psycopg3.\nSQLAlchemy is the slowest even though we\u2019re not using its ORM tool. This may\nbe because it introduces extra overhead with its abstractions around session\nmanagement and compiled SQL expressions.\n\nInserting into a Timescale hypertable is a bit slower. This is maybe because\nrows are being inserted into a hypertable with chunks so there may be some\noverhead there, even if there\u2019s only one chunk.\n\nSo at best we\u2019re only getting ~3000 inserts per second with single-row inserts\nat which rate we\u2019re gonna have to wait ~8 years for all the data to load \ud83e\udda5\nThere must be a faster way.\n\n## Multi-valued insert\n\nYou can insert multiple rows with one insert statement. This is called a\nmulti-valued or bulk insert and looks like this:\n\n    \n    \n    insert into weather ( time, location_id, latitude, longitude, temperature_2m, zonal_wind_10m, meridional_wind_10m, total_cloud_cover, total_precipitation, snowfall ) values ('1995-03-02 04:00:00+00', 346444, -30, 31, 21.54013, 7.1091003, 5.9887085, 1, 2.7820282, 0), ('1995-03-02 05:00:00+00', 346444, -30, 31, 21.596466, 7.0369415, 6.2397766, 0.95751953, 2.1944494, 0), ('1995-03-02 06:00:00+00', 346444, -30, 31, 21.660583, 6.303482, 6.017273, 0.88571167, 1.9253268, 0);\n\nThis is faster for a few reasons. There\u2019s less network overhead as each\nsingle-row insert requires a network round trip for each row inserted.\nPostgres also only has to parse and plan once. Multi-row inserts can also be\nfurther optimized when it comes to updating indexes. It seems that you can\nbulk insert as many rows as you want as long as they fit in memory.\n\nIn pandas it sounds like you can do this by passing the method=\"multi\" keyword\nargument to the df.to_sql() function but I found this to be a bit slower than\nsingle-row inserts with chunksize=1. So I just didn\u2019t set a method or chunk\nsize and supposedly all rows will be written at once, and it was faster. With\npsycopg3 you can construct or stream a list of tuples, one for each row, and\ninsert them all at once. With SQLAlchemy it\u2019s a dict of tuples.\n\nThis time each benchmark inserted 100k rows and was repeated 10 times.\n\nNow there\u2019s a clear winner with psycopg3 at 25~30k inserts/sec. I\u2019m not sure\nwhy psycopg3 is faster but it looks like pandas is using dictionaries to\ninsert which can be slower than just plain tuples. SQLAlchemy might be extra\nslow slow here because of additional overhead like with single-row inserts and\nI also passed it dictionaries.\n\nWith multi-row inserts there\u2019s an order-of-magnitude improvement but at ~30k\ninserts per second, we\u2019re still gonna have to wait ~0.8 years or almost 10\nmonths for all the data to load \ud83d\udc22\n\n# The copy statement\n\n## Upgrading to the copy statement\n\nFor loading in larger amounts of data, Postgres has the copy statement\nallowing us to insert rows from a CSV file or from a binary file.^9 copy is\nfaster than multi-row inserts as Postgres reads data straight from the file\nand optimizes parsing, planning, and WAL usage knowing there is a lot of data\nto load.\n\nOnce you have a CSV file it\u2019s as simple as\n\n    \n    \n    copy weather from some_big.csv delimiter ',' csv header;\n\nWe have the option of saving data from NetCDF files as CSV files then using\ncopy. This honestly feels inefficient as saving timestamps and floating-point\nnumbers as plaintext to disk takes up more space that it should then reading\nit from disk seems like it would be slow, but Postgres seems to have optimized\nthis operation. We also have the option of not saving the data into CSV files\nand streaming it straight into Postgres using psycopg3\u2019s cusor.copy()\nfunction.\n\nWhen benchmarking copy vs. psycopg3.cursor.copy() we are starting with a\npandas dataframe so we must account for the time it takes to save all the data\nto CSV files on disk in the case of copy csv. In the case of cursor.copy() if\nwe stream a list of tuples then the only overhead is creating the cursor and\ntuple generator.\n\nHere the full rate includes overhead (writing CSV files or constructing\ntuples) while the copy rate does not. This time each benchmark inserted\n1,038,240 rows (1 day of ERA5 data) and was repeated 10 times.\n\nWe see that copy can actually insert close to 400k rows per second, but that\nis if you already have the CSV file ready to go. Including overhead, both copy\nand psycopg3 can manage around 100k inserts/second with psycopg3 being a bit\nfaster. For some reason there seems to be no difference between regular table\nand hypertable performance for psycopg3.\n\nAt ~100k inserts/second we\u2019re still talking about ~3 months to load all the\ndata \ud83d\udc0c\n\n## Sustaining copy insert rates\n\nWhen inserting many rows, Postgres may encounter bottlenecks^10 so it\u2019s\nimportant that the insert rate can be sustained. To look at this, we can\ninsert hundreds of millions of rows and watch for fluctuations in the insert\nrate.\n\nFor this benchmark, rows were inserted in 744 batches of 1,038,240 rows for a\ntotal of ~772 million rows. The overall insert rate is plotted. The dots show\nthe insert rate for each batch while the solid lines show a 10-batch rolling\nmean. The straight horizontal lines show the mean insert rate over the entire\nbenchmark. Note that the lines orange and blue straight lines are right on top\nof each other.\n\nIt seems that, at least with one worker, we don\u2019t see huge drops in insert\nrates although copy csv shows frequent drops and seems more susceptible to\nfluctuations. psycopg3 is generally faster and interestingly there isn\u2019t much\nof a difference between copying into a regular table or hypertable.\n\n## Parallel copy\n\nInserting data with copy is fast but can we speed it up by executing multiple\ncopy operations in parallel? Using the joblib package we can execute multiple\ncopy statements or psycopg3 cursors in parallel.\n\nThe overall insert rate is plotted as a function of the number of workers.\nEach benchmark inserted 128 hours of ERA5 data (~133 million rows).\n\nInserting data into a single table is not super parallelizable so it looks\nlike performance generally plateaus after 16 workers.^11\n\n# Tools\n\n## pg_bulkload and timescaledb-parallel-copy\n\nBeyond the copy statement, there are external tools for loading large amounts\nof data into Postgres. I\u2019ll benchmark two of them, pg_bulkload and\ntimescaledb-parallel-copy.\n\nBlue and orange bars show results from benchmarks that inserted 1,038,240 rows\n(1 day of ERA5 data) and were repeated 10 times. The sustained insert rates\nare from benchmarks that inserted 256 hours of ERA5 data (~266 million rows)\ninto a hypertable. In these benchmarks the CSV files were already written to\ndisk so the insert rate corresponds to the \"copy rate\" from the copy\nbenchmarks. The insert rate including overhead accounts for the time it takes\nto write the CSV files to disk.\n\nAt first it would seem that pg_bulkload is much faster, however, this is\nbecause by default it bypasses the shared buffers and skips WAL logging so\ndata recovery following a crash may not be possible while timescaledb-\nparallel-copy does not and does things more safely. On a level playing field\nwith fsync off (see next section for an explanation) timescaledb-parallel-copy\nwith multiple workers beats out pg_bulkload.\n\n## Multiple workers with timescaledb-parallel-copy\n\ntimescaledb-parallel-copy lets you specify the number of workers inserting\ndata in parallel. Let\u2019s see how much performance we can squeeze out with more\nworkers, and if that performance can be sustained.\n\nThe insert rate as a function of the number of rows inserted. In this\nbenchmark the CSV files were already written to disk so the insert rate\ncorresponds to the \"copy rate\" from the copy benchmarks. Each benchmark\ninserted 256 hours of ERA5 data (~266 million rows). Note the vertical log\nscale.\n\nInitial performance looks great! But eventually, before 100 million rows on my\nsystem, a bottleneck is reached and the insert rate tanks before picking back\nup in waves. The maximum sustained insert rate is around 600~700k inserts/sec\nfor regular tables and ~300k for hypertables.\n\npg_bulkload doesn\u2019t let you specify the number of threads or workers, but does\nhave a writer=parallel option which uses multiple threads to do data reading,\nparsing and writing in parallel. We\u2019ll look at its insert rate later.\n\n# Tweaking Postgres settings\n\nThere are a couple of other things we can try to speed up inserts, but are\nbasically some form of tweaking Postgres\u2019 non-durable settings.\n\nSome extra performance can be squeezed out of tweaking non-durable settings\nspecifically for loading data following suggestions by Craig Ringer on\nStackOverflow. Some of the settings can be dangerous for database integrity in\nthe event of a crash though. The main settings to change are turning off fsync\nto avoid flushing data to disk and also turning off full_page_writes to avoid\nguarding against partial page writes.\n\nYou can also insert data into an unlogged table that generates no WAL and gets\ntruncated upon crash recovery but is faster to write into. While inserting\ninto an unlogged table might be fast, you still have to convert it to a\nregular logged table afterwards which can be a slow single-threaded process.\nAnd hypertables cannot be unlogged, so if you want a hypertable you need to\nfurther convert/migrate the regular logged table to a hypertable which can\nalso be slow.\n\n# So what\u2019s the best method?\n\nShort answer: Use psycopg3 to directly copy data into a hypertable. If you\nalready have CSV files then use timescaledb-parallel-copy. For parallelization\nthe sweet spot seems to be 12~16 workers on my system.\n\nWe want to end up with a hypertable but it seems like inserting into a regular\ntable is faster. So is it faster to insert into a regular table then convert\nit to a hypertable? Or is it faster to just insert data straight into a\nhypertable?\n\nBlue bars show the wall clock time taken to insert data into the table and the\norange bar shows the time taken to convert the regular table into a\nhypertable.\n\nA quick test with inserting ~772 million rows with psycopg3\u2019s copy and 16\nworkers shows that inserting data into a hypertable is faster as it takes\nroughly 80% of the time in this case. This may not always be the case but\ninserting into a regular table then converting it to a hypertable and\nmigrating the data will probably always be slower as the conversion/migration\nprocess is not super fast and seems to be single-threaded.\n\nNow that we\u2019ve concluded we want to be inserting data into a hypertable, let\u2019s\ntake a look at the all hypertable insert rates we\u2019ve considered in one plot.\n\nSustained hypertable insert rates including overhead (writing CSV files) for\ndifferent insertion methods. Here \"tpc\" is short for timescaledb-parallel-copy\nand \"pgb\" is short for pg_bulkload. \"32W\" means 32 workers were used for that\nbenchmark.\n\nFor pg_bulkload with a single worker the the writer=buffered option was used.\nFor multiple workers, the writer=buffered and multi_process=yes options were\nused. Then for multiple workers with fsync off, the writer=parallel option was\nused.\n\nSo what can we conclude?\n\n  1. At least on my hardware it seems there\u2019s a ceiling of ~140k sustained inserts/sec with overhead when using a single worker with protections on. pg_bulkload wins here by quite a bit.\n  2. You can use multiple workers to increase the sustained insert rate up to ~250k inserts/sec with psycopg3\u2019s copy cursor while still being protected.\n  3. The insertion process is not very parallelizable so the sweet spot is 4-16 workers. The benchmarks used 32 workers to maximize insert rates.\n  4. If you\u2019re okay living a bit dangerously you can turn off fsync and sustain an insert rate of ~462k inserts/sec with psycopg3! You\u2019ll also squeeze out a bit more performance out of timescaledb-parallell-copy.\n  5. Be careful when using pg_bulkload as it disables fsync by default.\n  6. These conclusions assume you need to do extra work to convert data to CSV files which is why psycopg3 was the clear winner, although it does seem pretty fast. If you\u2019re starting with CSV files timescaledb-parallel-copy is probably faster (and quicker to set up).\n\nSome closing thoughts:\n\n  1. Want even faster inserts? You should probably upgrade your hardware. A nice enterprise-grade NvME SSD and lots of high-speed DDR5 RAM will help a lot. I used hardware that is roughly 5 years old so newer hardware should be able to easily beat these benchmarks.\n  2. I know the general wisdom is to just dump this data into Snowflake or BigQuery and get fast analytics for relatively cheap. But I like working with my own hardware and learning this way. Plus I have no real budget for this project.\n  3. I\u2019d be curious how ClickHouse performs on these benchmarks. My impression is that it would probably be faster out of the box. But I want to learn PostgreSQL and like the fact that TimescaleDB is just a Postgres extension so I went with TimescaleDB.\n\nAt a sustained ~462k inserts per second, we\u2019re waiting ~20 days for our ~754\nbillion rows which is not bad I guess \ud83d\udc28 It\u2019s less time than it took me to\nwrite this post.\n\n# Appendices\n\n## Source code\n\nThe code used to download the ERA5 data, create the tables, insert/copy data,\nrun benchmarks, and plot figures is at the timescaledb-insert-benchmarks\nrepository.\n\n## Benchmarking methodology\n\nTo ensure a consistent environment for benchmarking, a new Docker container\nwas spun up for each individual benchmark. No storage was persisted between\nDocker containers. Data including NetCDF and CSV files were read from a HDD\nand the database was stored on an NvME SSD.\n\nHardware:\n\n  * CPU: 2x 12-core Intel Xeon Silver 4214\n  * RAM: 16x 16 GiB Samsung M393A2K40CB2-CTD ECC DDR4 2666 MT/s\n  * SSD: Intel SSDPEKNW020T8 2 TB NvME\n  * HDD: Seagate Exos X16 14TB 7200 RPM 256MB Cache\n\nSoftware:\n\n  * Ubuntu 20.04 with Linux kernel 5.15\n  * PostgreSQL 15.5\n  * TimescaleDB 2.13.0\n  * pg_bulkload 3.1.20\n\nPostgres configuration chosen by timescaledb-tune:\n\n    \n    \n    shared_buffers = 64144MB effective_cache_size = 192434MB maintenance_work_mem = 2047MB work_mem = 13684kB timescaledb.max_background_workers = 16 max_worker_processes = 67 max_parallel_workers_per_gather = 24 max_parallel_workers = 48 Recommendations based on 250.57 GB of available memory and 48 CPUs for PostgreSQL 15 wal_buffers = 16MB min_wal_size = 512MB default_statistics_target = 100 random_page_cost = 1.1 checkpoint_completion_target = 0.9 max_locks_per_transaction = 512 autovacuum_max_workers = 10 autovacuum_naptime = 10 effective_io_concurrency = 256\n\nFor benchmarking I set the WAL size:\n\n    \n    \n    min_wal_size = 4GB max_wal_size = 16GB\n\nAnd for the fsync off benchmarks I set:\n\n    \n    \n    max_wal_size = 32GB fsync = off full_page_writes = off\n\n# Footnotes\n\n  1. The Postgres documentation does have a nice list of performance tips for populating a database but I wanted some benchmarks and to consider some external tools as well. \u21a9\n\n  2. ERA5 is the latest climate reanalysis product produced by the ECMWF re-analysis project. ECMWF is the European Centre for Medium-Range Weather Forecasts. \u21a9\n\n  3. NetCDF (Network Common Data Form) files are ubiquitous in distributing model output in climate science, atmospheric science, and oceanography. They typically store multi-dimensional arrays for each variable output along with enough metadata that you don\u2019t need to refer to external documentation to understand and use the data. The good ones do this at least. \u21a9\n\n  4. Postgres may need to perform a full table lock for some operations that modify the entire table. But for row-level operations no locking is necessary as Postgres uses multiversion concurrency control (MVCC) to allow multiple transactions to operate on the database concurrently. Each transaction sees a version of the database as it was when the transaction began. \u21a9\n\n  5. Write-ahead logging is how Postgres ensures data integrity and database recovery after crashes. All committed transactions are recorded in a WAL file before being applied to the database. In the event of a crash or power failure, the database can recover all committed transactions from the WAL file so the database can always be brought back to a consistent, uncorrupted state. \u21a9\n\n  6. Postgres transactions execute multiple operations as a single atomic unit of work so that either all operations execute successfully (and are written to WAL on disk), or none are applied. This ensures the database is always in a consistent state even if something goes wrong in the middle of a transaction. \u21a9\n\n  7. I wanted to try a fourth method using SQLAlchemy\u2019s Object Relational Mapper (ORM) which maps rows in a database to Python objects. But, the ORM requires a primary key and Timescale hypertables do not support primary keys. This is because the underlying data must be partitioned to several physical PostgreSQL tables and partitioned lookups cannot support a primary key. ORM was probably going to be the slowest at inserting due to extra overhead anyways. \u21a9\n\n  8. TimescaleDB hypertables automatically partition data by time into chunks and have extra features that make working with time-series data easier and faster. \u21a9\n\n  9. Binary is usually a more compact representation for floats and timestamps than plaintext so I was hoping to also benchmark copy with the binary format thinking it might be much faster. Unfortunately the format Postgres expects seems non-trivial and I couldn\u2019t easily find a library that would give me the binary format I needed. And Nick Babcock actually found that binary is no faster than csv, so it didn\u2019t seem worth trying. For reference, 31 days of ERA5 data takes up 7.8 GiB in NetCDF form and 71 GiB in CSV form. \u21a9\n\n  10. Bottlenecks include the disk being overloaded with writes, usually made worse when the WAL and row insertion are competing for disk I/O. Autovacuuming, which removes dead rows, can also compete for I/O although when populating a database we can ensure that there won\u2019t be any duplicates so this could potentially be turned off. Postgres also periodically performs checkpoints to flush all outstanding WAL data to disk. Heavy writes can lead to more checkpoints and more competitinon for I/O. \u21a9\n\n  11. I probably should have run the parallel copy benchmarks using more rows to measure a better sustained rate. I wish there was an easy way to keep track of the total number of rows when inserting many rows in parallel but querying the row count seems very slow as Postgres is busy. I guess I could log a timestamp every time a worker inserted a batch of rows. Also wish I repeated this benchmark but it takes quite a while to run. \u21a9\n\n  * \u00a9 Ali Ramadhan.\n  * Design: HTML5 UP\n\n", "frontpage": true}
