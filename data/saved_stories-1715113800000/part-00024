{"aid": "40285344", "title": "Beware of -ffast-math and -Ofast", "url": "https://simonbyrne.github.io/notes/fastmath/", "domain": "simonbyrne.github.io", "votes": 1, "user": "fanf2", "posted_at": "2024-05-07 13:42:04", "comments": 0, "source_title": "Beware of fast-math", "source_text": "Beware of fast-math\n\nSimon's notes\n\n# Beware of fast-math\n\nOne of my more frequent rants, both online and in person, is the danger posed\nby the \"fast-math\" compiler flag. While these rants may elicit resigned\nacknowledgment from those who already understand the dangers involved, they do\nlittle to help those who don't. So given the remarkable paucity of writing on\nthe topic (including the documentation of the compilers themselves), I decided\nit would make a good inaugural topic for this blog.\n\n## So what is fast-math?\n\nIt's a compiler flag or option that exists in many languages and compilers,\nincluding:\n\n  * -ffast-math (and included by -Ofast) in GCC and Clang\n\n  * -fp-model=fast (the default) in ICC\n\n  * /fp:fast in MSVC\n\n  * \\--math-mode=fast command line option or @fastmath macro in Julia.\n\nSo what does it actually do? Well, as the name said, it makes your math\nfaster. That sounds great, we should definitely do that!\n\n> I mean, the whole point of fast-math is trading off speed with correctness.\n> If fast-math was to give always the correct results, it wouldn\u2019t be fast-\n> math, it would be the standard way of doing math.\n\n\u2014 Mos\u00e8 Giordano\n\nThe rules of floating point operations are specified in the IEEE 754 standard,\nwhich all popular programming languages (mostly) adhere to; compilers are only\nallowed to perform optimizations which obey these rules. Fast-math allows the\ncompiler to break some of these rules: these breakages may seem pretty\ninnocuous at first glance, but can have significant and occasionally\nunfortunate downstream effects.\n\nIn GCC, -ffast-math (or -Ofast) enables the following options: -fno-math-\nerrno, -funsafe-math-optimizations, -ffinite-math-only, -fno-rounding-math,\n-fno-signaling-nans, -fcx-limited-range and -fexcess-precision=fast. Note that\n-funsafe-math-optimizations is itself a collection of options -fno-signed-\nzeros, -fno-trapping-math, -fassociative-math and -freciprocal-math, plus some\nextra ones, which we will discuss further below.\n\nNow some of these are unlikely to cause problems in most cases: -fno-math-\nerrno^[1], -fno-signaling-nans, -fno-trapping-math disable rarely-used (and\npoorly supported) features. Others, such as -freciprocal-math can reduce\naccuracy slightly, but are unlikely to cause problems in most cases.\n\nKrister Walfridsson gives a very nice (and somewhat more objective)\ndescription of some of these, but I want to focus on three in particular.\n\n## -ffinite-math-only\n\n> Allow optimizations for floating-point arithmetic that assume that arguments\n> and results are not NaNs or +-Infs.\n\nThe intention here is to allow the compiler to perform some extra\noptimizations that would not be correct if NaNs or Infs were present: for\nexample the condition x == x can be assumed to always be true (it evaluates\nfalse if x is a NaN).\n\nThis sounds great! My code doesn't generate any NaNs or Infs, so this\nshouldn't cause any problems.\n\nBut what if your code doesn't generate any intermediate NaNs only because it\ninternally calls isnan to ensure that they are correctly handled?\n\n\u2014 based on an example from John Regehr\n\n(to explain what this is showing: the function is setting the return register\neax to zero, by xor-ing it with itself, which means the function will always\nreturn false)\n\nThat's right, your compiler has just removed all those checks.\n\nDepending on who you ask, this is either obvious (\"you told the compiler there\nwere no NaNs, so why does it need to check?\") or ridiculous (\"how can we\nsafely optimize away NaNs if we can't check for them?\"). Even compiler\ndevelopers can't agree.\n\nThis is perhaps the single most frequent cause of fast-math-related\nStackOverflow questions and GitHub bug reports, and so if your fast-math-\ncompiled code is giving wrong results, the very first thing you should do is\ndisable this option (-fno-finite-math-only).\n\n## -fassociative-math\n\n> Allow re-association of operands in series of floating-point operations.\n\nThis allows the compiler to change the order of evaluation in a sequence of\nfloating point operations. For example if you have an expression (a + b) + c,\nit can evaluate it instead as a + (b + c). While these are mathematically\nequivalent with real numbers, they aren't equivalent in floating point\narithmetic: the errors they incur can be different, in some cases quite\nsignificantly so:\n\n    \n    \n    julia> a = 1e9+1; b = -1e9; c = 0.1; julia> (a+b)+c 1.1 julia> a+(b+c) 1.100000023841858\n\n### Vectorization\n\nSo why would you want to do this? One primary reason is that it can enable use\nof vector/SIMD instructions:\n\nFor those who aren't familiar with SIMD operations (or reading assembly), I'll\ntry to explain briefly what is going on here (others can skip this part).\nSince raw clock speeds haven't been getting much faster, one way in which\nprocessors have been able to increase performance is through operations which\noperate on a \"vector\" (basically, a short sequence of values contiguous in\nmemory).\n\nIn this case, instead of performing a sequence of floating point additions\n(addss), it is able to make use of a SIMD instruction (addps) which takes\nvector floats (4 in this case, but it can be up to 16 with AVX 512\ninstructions), and adds them element-wise to another vector in one operation.\nIt does this for the whole array, followed by a final reduction step where to\nsum the vector to a single value. This means that instead of evaluating\n\n    \n    \n    s = arr[0] + arr[1]; s = s + arr[2]; s = s + arr[3]; ... s = s + arr[255];\n\nit is actually doing\n\n    \n    \n    s0 = arr[0] + arr[4]; s1 = arr[1] + arr[5]; s2 = arr[2] + arr[6]; s3 = arr[3] + arr[7]; s0 = s0 + arr[8]; s1 = s1 + arr[9]; s2 = s2 + arr[10]; s3 = s3 + arr[11]); ... s0 = s0 + arr[252]; s1 = s1 + arr[253]; s2 = s2 + arr[254]; s3 = s3 + arr[255]); sa = s0 + s1; sb = s2 + s3; s = sa + sb;\n\nwhere each line corresponds to one floating point instruction.\n\nThe problem here is that the compiler generally isn't allowed to make this\noptimization: it requires evaluating the sum in a different association\ngrouping than was specified in the code, and so can give different\nresults^[2]. Though in this case it is likely harmless (or may even improve\naccuracy^[3]), this is not always the case.\n\n### Compensated arithmetic\n\nCertain algorithms however depend very strictly on the order in which floating\npoint operations are performed. In particular compensated arithmetic\noperations make use of it to compute the error that is incurred in\nintermediate calculations, and correct for that in later computations.\n\nThe most well-known algorithm which makes use of this is Kahan summation,\nwhich corrects for the round off error incurred at addition step in the\nsummation loop. We can compile an implementation of Kahan summation with\n-ffast-math, and compare the result to the simple loop summation above:\n\nIt gives exactly the same assembly as the original summation code above. Why?\n\nIf you substitute the expression for t into c, you get\n\n    \n    \n    c = ((s + y) - s) - y);\n\nand by applying reassociation, the compiler will then determine that c is in\nfact always zero, and so may be completely removed. Following this logic\nfurther, y = arr[i] and so the inside of the loop is simply\n\n    \n    \n    s = s + arr[i];\n\nand hence it \"optimizes\" identically to the simple summation loop above.\n\nThis might seem like a minor tradeoff, but compensated arithmetic is often\nused to implement core math functions, such as trigonometric and exponential\nfunctions. Allowing the compiler to reassociate inside these can give\ncatastrophically wrong answers.\n\n## Flushing subnormals to zero\n\nThis one is the most subtle, but by far the most insidious, as it can affect\ncode compiled without fast-math, and is only cryptically documented under\n-funsafe-math-optimizations:\n\n> When used at link time, it may include libraries or startup files that\n> change the default FPU control word or other similar optimizations.\n\nSo what does that mean? Well this is referring to one of those slightly\nannoying edge cases of floating point numbers, subnormals (sometimes called\ndenormals). Wikipedia gives a decent overview, but for our purposes the main\nthing you need to know is (a) they're very close to zero, and (b) when\nencountered, they can incur a significant performance penalty on many\nprocessors^[4].\n\nA simple solution to this problem is \"flush to zero\" (FTZ): that is, if a\nresult would return a subnormal value, return zero instead. This is actually\nfine for a lot of use cases, and this setting is commonly used in audio and\ngraphics applications. But there are plenty of use cases where it isn't fine:\nFTZ breaks some important floating point error analysis results, such as\nSterbenz' Lemma, and so unexpected results (such as iterative algorithms\nfailing to converge) may occur.\n\nThe problem is how FTZ actually implemented on most hardware: it is not set\nper-instruction, but instead controlled by the floating point environment:\nmore specifically, it is controlled by the floating point control register,\nwhich on most systems is set at the thread level: enabling FTZ will affect all\nother operations in the same thread.\n\nGCC with -funsafe-math-optimizations enables FTZ (and its close relation,\ndenormals-are-zero, or DAZ), even when building shared libraries. That means\nsimply loading a shared library can change the results in completely unrelated\ncode, which is a fun debugging experience.\n\n## What can programmers do?\n\nI've joked on Twitter that \"friends don't let friends use fast-math\", but with\nthe luxury of a longer format, I will concede that it has valid use cases, and\ncan actually give valuable performance improvements; as SIMD lanes get wider\nand instructions get fancier, the value of these optimizations will only\nincrease. At the very least, it can provide a useful reference for what\nperformance is left on the table. So when and how can it be safely used?\n\nOne reason is if you don't care about the accuracy of the results: I come from\na scientific computing background where the primary output of a program is a\nbunch of numbers. But floating point arithmetic is used in many domains where\nthat is not the case, such as audio, graphics, games, and machine learning.\nI'm not particularly familiar with requirements in these domains, but there is\nan interesting rant by Linus Torvalds from 20 years ago, arguing that overly\nstrict floating point semantics are of little importance outside scientific\ndomains. Nevertheless, some anecdotes suggest fast-math can cause problems, so\nit is probably still useful understand what it does and why. If you work in\nthese areas, I would love to hear about your experiences, especially if you\nidentified which of these optimizations had a positive or negative impact.\n\n> I hold that in general it\u2019s simply intractable to \u201cdefensively\u201d code against\n> the transformations that -ffast-math may or may not perform. If a\n> sufficiently advanced compiler is indistinguishable from an adversary, then\n> giving the compiler access to -ffast-math is gifting that enemy nukes. That\n> doesn\u2019t mean you can\u2019t use it! You just have to test enough to gain\n> confidence that no bombs go off with your compiler on your system.\n\n\u2014 Matt Bauman\n\nIf you do care about the accuracy of the results, then you need to approach\nfast-math much more carefully and warily. A common approach is to enable fast-\nmath everywhere, observe erroneous results, and then attempt to isolate and\nfix the cause as one would usually approach a bug. Unfortunately this task is\nnot so simple: you can't insert branches to check for NaNs or Infs (the\ncompiler will just remove them), you can't rely on a debugger because the bug\nmay disappear in debug builds, and it can even break printing.\n\nSo you have to approach fast-math much more carefully. A typical process might\nbe:\n\n  1. Develop reliable validation tests\n\n  2. Develop useful benchmarks\n\n  3. Enable fast-math and compare benchmark results\n\n  4. Selectively enable/disable fast-math optimizations^[5] to identify\n\na. which optimizations have a performance impact,\n\nb. which cause problems, and\n\nc. where in the code those changes arise.\n\n  5. Validate the final numeric results\n\nThe aim of this process should be to use the absolute minimum number of fast-\nmath options, in the minimum number of places, while testing to ensure that\nthe places where the optimizations are used remain correct.\n\nAlternatively, you can look into other approaches to achieve the same\nperformance benefits: in some cases it is possible to rewrite the code to\nachieve the same results: for example, it is not uncommon to see expressions\nlike x * (1/y) in many scientific codebases.\n\nFor SIMD operations, tools such as OpenMP or ISPC provide constructions to\nwrite code that is amenable to automatic SIMD optimizations. Julia provides\nthe @simd macro, though this also has some important caveats on its use. At\nthe more extreme end, you can use SIMD intrinsics: these are commonly used in\nlibraries, often with the help of code generation (FFTW uses this appraoch),\nbut requires considerably more effort and expertise, and can be difficult to\nport to new platforms.\n\nFinally, if you're writing an open source library, please don't hardcode fast-\nmath into your Makefile.\n\n## What can language and compilers developers do?\n\nI think the widespread use of fast-math should be considered a fundamental\ndesign failure: by failing to provide programmers with features they need to\nmake the best use of modern hardware, programmers instead resort to enabling\nan option that is known to be blatantly unsafe.\n\nFirstly, GCC should address the FTZ library issue: the bug has been open for 9\nyears, but is still marked NEW. At the very least, this behavior should be\nmore clearly documented, and have a specific option to disable it.\n\nBeyond that, there are 2 primary approaches: educate users, and provide finer\ncontrol over the optimizations.\n\nThe easiest way to educate users is to give it a better name. Rather than\n\"fast-math\", something like \"unsafe-math\". Documentation could also be\nimproved to educate users on the consequences of these choices (consider this\npost to be my contribution to toward that goal). Linters and compiler warnings\ncould, for example, warn users that their isnan checks are now useless, or\neven just highlight which regions of code have been impacted by the\noptimizations.\n\nSecondly, languages and compilers need to provide better tools to get the job\ndone. Ideally these behaviors shouldn't be enabled or disabled via a compiler\nflag, which is a very blunt tool, but specified locally in the code itself,\nfor example\n\n  * Both GCC and Clang let you enable/disable optimizations on a per-function basis: these should be standardized to work with all compilers.\n\n  * There should be options for even finer control, such as a pragma or macro so that users can assert that \"under no circumstances should this isnan check be removed/this arithmetic expression be reassociated\".\n\n  * Conversely, a mechanism to flag certain addition or subtraction operations which the compiler is allowed to reassociate (or contract into a fused-multiply-add operation) regardless of compiler flags.^[6]\n\nThis still leaves open the exact question of what the semantics should be: if\nyou combine a regular + and a fast-math +, can they reassociate? What should\nthe scoping rules be, and how should it interact with things like inter-\nprocedural optimization? These are hard yet very important questions, but they\nneed to be answered for programmers to be able to make use of these features\nsafely.\n\nFor more discussion, see HN.\n\n## Updates\n\nA few updates since I wrote this note:\n\n  * Brendan Dolan-Gavitt wrote a fantastic piece about FTZ-enabling libraries in Python packages: it also has some nice tips on how to find out if your library was compiled with fast-math.\n\n    * He also has a nice proof-of-concept buffer overflow vulnerability.\n\n  * It turns out Clang also enables FTZ when building shared libraries with fast-math: but only if you have a system GCC installation. I've opened an issue.\n\n  * MSVC doesn't remove isnan checks, but instead generates what looks like worse code when compiling with fast-math.\n\n  * The FTZ library issue will be fixed in GCC 13!\n\n[1]| Apparently -fno-math-errno in GCC can affect malloc, so may not be quite\nso harmless.  \n---|---  \n[2]| In fact, it possible to construct array such that taking the sum in\ndifferent ways can produce almost any floating point value.  \n---|---  \n[3]| One important result in numerical analysis is that the error bound on\nsummation is proportional to the sum of the absolute values of the\nintermediate sums. SIMD summation splits the accumulation over multiple\nvalues, so will typically give smaller intermediate sums.  \n---|---  \n[4]| A good description of why subnormals incur performance penalties.  \n---|---  \n[5]| As mentioned above, -fno-finite-math-only should be the first thing you\ntry.  \n---|---  \n[6]| Rust provides something like this via experimental intrinsics, though I'm\nnot 100% clear on what optimzations are allowed.  \n---|---  \n  \n\u00a9 Simon Byrne. Last modified: April 06, 2024.\n\n", "frontpage": false}
