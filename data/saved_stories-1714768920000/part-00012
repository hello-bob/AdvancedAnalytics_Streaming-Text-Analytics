{"aid": "40247364", "title": "FIFO Is Better Than LRU: The Power of Lazy Promotion and Quick Demotion", "url": "https://blog.jasony.me/system/cache/2023/06/24/fifo-lru", "domain": "jasony.me", "votes": 2, "user": "thunderbong", "posted_at": "2024-05-03 13:20:05", "comments": 0, "source_title": "FIFO is Better than LRU: the Power of Lazy Promotion and Quick Demotion", "source_text": "FIFO is Better than LRU: the Power of Lazy Promotion and Quick Demotion\n\n# Juncheng's blog\n\n# FIFO is Better than LRU: the Power of Lazy Promotion and Quick Demotion\n\nComputer Systems\n\nCache\n\n\u00b7\n\nJune 24, 2023\n\n\u00b7\n\nby\n\nJuncheng Yang\n\n> TL;DR Historically FIFO-based algorithms are thought to be less efficient\n> (having higher miss ratios) than LRU-based algorithms. In this blog, we\n> introduce two techniques, lazy promotion, which promotes objects only at\n> eviction time, and quick demotion, which removes most new objects quickly.\n> We will show that\n>\n>   * Conventional-wisdom-suggested \u201cweak LRUs\u201d, e.g., FIFO-Reinsertion, is\n> actually more efficient (having lower miss ratios) than LRU;\n>   * Simply evicting most new objects can improve state-of-the-art\n> algorithm\u2019s efficiency.\n>   * Eviction algorithms can be designed like building LEGOs by adding lazy\n> promotion and quick demotion on top of FIFO.\n>\n\n# Background\n\nCaching is a well-known and widely deployed technique to speed up data access,\nreduce repeated computation and data transfer. A core component of a cache is\nthe eviction algorithm, which chooses the objects stored in the limited cache\nspace. Two metrics describe the performance of an eviction algorithm:\nefficiency measured by the miss ratio and throughput measured by the number of\nrequests served per second.\n\nThe study of cache eviction algorithms has a long history, with a majority of\nthe work centered around LRU (that is, to evict the least-recently-used\nobject). LRU maintains a doubly-linked list, promoting objects to the head of\nthe list upon cache hits and evicting the object at the tail of the list when\nneeded. Belady and others found that memory access patterns often exhibit\ntemporal locality \u2014 \u201cthe most recently used pages were most likely to be\nreused in the immediate future\u201d. Thus, LRU using recency to promote objects\nwas found to be better than FIFO.\n\nMost eviction algorithms designed to achieve high efficiency start from LRU.\nFor example, many algorithms, such as ARC, SLRU, 2Q, MQ, and multi-\ngenerational LRU, use multiple LRU queues to separate hot and cold objects.\nSome algorithms, e.g., LIRS and LIRS2, maintain an LRU queue but use different\nmetrics to promote objects. While other algorithms, e.g., LRFU, EE-LRU, LeCaR,\nand CACHEUS, augment LRU\u2019s recency with different metrics. In addition, many\nrecent works, e.g., Talus, improve LRU\u2019s ability to handle scan and loop\nrequests.\n\nBesides efficiency, there have been fruitful studies on enhancing the cache\u2019s\nthroughput performance and thread scalability. Each cache hit in LRU promotes\nan object to the head of the queue, which requires updating at least six\npointers guarded by locks. These overheads are not acceptable in many\ndeployments that need high performance. Thus, performance-centric systems\noften use FIFO-based algorithms to avoid LRU\u2019s overheads. For example, FIFO-\nReinsertion and variants of CLOCK have been developed, which serve as LRU\napproximations. It is often perceived that these algorithms trade miss ratio\nfor better throughput and scalability.\n\nIn this blog, I am going to show that FIFO is in-fact better than LRU not only\nbecause it is faster, more scalable, but also more efficient and effective\n(having lower miss ratios).\n\n# Why FIFO and What it needs\n\nFIFO has many benefits over LRU. For example, FIFO has less metadata and\nrequires no metadata update on each cache hit, and thus is faster and more\nscalable than LRU. In contrast, LRU requires updating six pointers on each\ncache hit, which is not friendly for modern computer architecture due to\nrandom memory accesses. Moreover, FIFO is always the first choice when\nimplementing a flash cache because it does not incur write amplification.\nAlthough FIFO has throughput and scalability benefits, it is common wisdom\nthat FIFO is less effective (having higher miss ratio) than LRU.\n\n#### A cache can be viewed as a logically ordered queue with four operations:\ninsertion, removal, promotion and demotion. Most eviction algorithms are\npromotion algorithms.\n\nTo understand the various factors that affect the miss ratio, we introduce a\ncache abstraction. A cache can be viewed as a logically total-ordered queue\nwith four operations: insertion, removal, promotion, and demotion. Objects in\nthe cache can be compared and ordered based on some metric (e.g., time since\nthe last request), and the eviction algorithm evicts the least valuable object\nbased on the metric. Insertion and removal are user-controlled operations,\nwhere removal can either be directly invoked by the user or indirectly via the\nuse of time-to-live (TTL). Promotion and demotion are internal operations of\nthe cache used to maintain the logical ordering between objects.\n\nWe observe that most eviction algorithms use promotion to update the ordering\nbetween objects. For example, all the LRU-based algorithms promote objects to\nthe head of the queue on cache hits, which we call eager promotion. Meanwhile,\ndemotion is performed implicitly: when an object is promoted, other objects\nare passively demoted. We call this process passive demotion, a slow process\nas objects need to traverse through the cache queue before being evicted.\nHowever, we will show that instead of eager promotion and passive demotion,\neviction algorithms should use lazy promotion and quick demotion.\n\n# Lazy Promotion\n\nTo avoid popular objects from being evicted while not incurring much\nperformance overhead, we propose adding lazy promotion on top of FIFO (called\nLP-FIFO), which promotes objects only when they are about to be evicted. lazy\npromotion aims to retain popular objects with minimal effort. An example is\nFIFO-Reinsertion (note that FIFO-Reinsertion, 1-bit CLOCK, and Second Chance\nare different implementations of the same eviction algorithm): an object is\nreinserted at eviction time if it has been requested while in the cache.\n\nLP-FIFO has several benefits over eager promotion (promoting on every access)\nused in LRU-based algorithms.\n\n  * First, LP-FIFO inherits FIFO\u2019s throughput and scalability benefits because few metadata operations are needed when an object is requested. For example, FIFO-Reinsertion only needs to update a Boolean field upon the first request to a cached object without locking.\n  * Second, performing promotion at eviction time allows the cache to make better decisions by accumulating more information about the objects, e.g., how many times an object has been requested.\n\nTrace| approx time| #trace| cache type| #req (millions)| #obj (millions)  \n---|---|---|---|---|---  \nMSR| 2007| 13| block| 410| 74  \nFIU| 2008| 9| block| 514| 20  \nCloudphysics| 2015| 106| block| 2,114| 492  \nMajor CDN| 2018| 219| object| 3,728| 298  \nTencent Photo| 2018| 2| object| 5,650| 1,038  \nWiki CDN| 2019| 3| object| 2,863| 56  \nTencent CBS| 2020| 4030| block| 33,690| 551  \nAlibaba| 2020| 652| block| 19,676| 1702  \nTwitter| 2020| 54| KV| 195,441| 10,650  \nSocial Network| 2020| 219| KV| 549,784| 42,898  \n  \nTo understand LP-FIFO\u2019s efficiency, we performed a large-scale simulation\nstudy on 5307 production traces from 10 data sources, which include open-\nsource and proprietary datasets collected between 2007 and 2020. The 10\ndatasets contain 814 billion (6,386 TB) requests and 55.2 billion (533 TB)\nobjects, and cover different types of caches, including block, key-value (KV),\nand object caches. We further divide the traces into block and web (including\nMemcached and CDN). We choose small/large cache size as 0.1%/10% of the number\nof unique objects in the trace.\n\nWe compare the miss ratios of LRU with two LP-FIFO algorithms: FIFO-\nReinsertion and 2-bit CLOCK. 2-bit CLOCK tracks object frequency up to three,\nand an object\u2019s frequency decreases by one each time the CLOCK hand scans\nthrough it. Objects with frequency zero are evicted.\n\nCommon wisdom suggests that these two LP-FIFO examples are LRU approximations\nand will exhibit higher miss ratios than LRU. However, we found that LP-FIFO\noften exhibits miss ratios lower than LRU.\n\n#### Comparison of FIFO-Reinsertion, 2-bit CLOCK and LRU on 10 datasets with\n5307 traces. Left two: small cache, right two: large cache. A longer bar means\nthe algorithm is more efficient (having lower miss ratios on more traces).\nNote that we do not consider the overhead of LRU metadata in all the\nevaluations.\n\nThe figure above shows that FIFO-Reinsertion and 2-bit CLOCK are better than\nLRU on most traces. Specifically, FIFO-Reinsertion is better than LRU on 9 and\n7 of the 10 datasets using a small and large cache size, respectively.\nMoreover, on half of the datasets, more than 80% of the traces in each dataset\nfavor FIFO-Reinsertion over LRU at both sizes. On the two social network\ndatasets, LRU is better than FIFO-Reinsertion (especially at the large cache\nsize). This is because most objects in these two datasets are accessed more\nthan once, and using one bit to track object access is insufficient.\nTherefore, when increasing the one bit in FIFO-Reinsertion (CLOCK) to two bits\n(2-bit CLOCK), we observe that the number of traces favoring LP-FIFO increases\nto around 70%. Across all datasets, 2-bit CLOCK is better than FIFO on all\ndatasets at the small cache size and 9 of the 10 datasets at the large cache\nsize.\n\n#### FIFO-Reinsertion demotes new objects faster than LRU because objects\nrequested before the new object also pushes it down the queue.\n\nTwo reasons contribute to LP-FIFO\u2019s high efficiency. First, lazy promotion\noften leads to quick demotion. For example, under LRU, a newly-inserted object\nG is pushed down the queue only by (1) new objects and (2) cached objects that\nare requested after G. However, besides the objects requested after G, the\nobjects requested before G (but have not been promoted, e.g., B, D) also push\nG down the queue when using FIFO-Reinsertion. Second, compared to promotion at\neach request, object ordering in LP-FIFO is closer to the insertion order,\nwhich we conjecture is better suited for many workloads that exhibit\npopularity decay \u2014 old objects have a lower probability of getting a request.\n\nWhile LP-FIFO surprisingly wins over LRU in miss ratio, it cannot outperform\nstate-of-the-art algorithms. We next discuss another building block that\nbridges this gap.\n\n# Quick Demotion\n\nEfficient eviction algorithms not only need to keep popular objects in the\ncache but also need to evict unpopular objects fast. In this section, we show\nthat quick demotion (QD) is critical for an efficient eviction algorithm, and\nit enables FIFO-based algorithms to achieve state-of-the-art efficiency.\n\nBecause demotion happens passively in most eviction algorithms, an object\ntypically traverses through the cache before being evicted. Such traversal\ngives each object a good chance to prove its value to be kept in the cache.\nHowever, cache workloads often follow Zipf popularity distribution, with most\nobjects being unpopular. This is further exacerbated by (1) the scan and loop\naccess patterns in the block cache workloads, and (2) the vast existence of\ndynamic and short-lived data, the use of versioning in object names, and the\nuse of short TTLs in the web cache workloads. We believe the opportunity cost\nof new objects demonstrating their values is often too high: the object being\nevicted at the tail of the queue may be more valuable than the objects\nrecently inserted.\n\n#### An example of quick demotion: adding a small FIFO to filter most new\nobjects that do not have a request soon after insertion.\n\nTo illustrate the importance of quick demotion, we add a simple QD technique\non top of state-of-the-art eviction algorithms. The QD technique consists of a\nsmall probationary FIFO queue storing cached data and a ghost FIFO queue\nstoring metadata of objects evicted from the probationary FIFO queue. The\nprobationary FIFO queue uses 10% of the cache space and acts as a filter for\nunpopular objects: objects not requested after insertion are evicted early\nfrom the FIFO queue. The main cache runs a state-of-the-art algorithm and uses\n90% of the space. And the ghost FIFO stores as many entries as the main cache.\nUpon a cache miss, the object is written into the probationary FIFO queue\nunless it is in the ghost FIFO queue, in which case, it is written into the\nmain cache. When the probationary FIFO queue is full, if the object to evict\nhas been accessed since insertion, it is inserted into the main cache.\nOtherwise, it is evicted and recorded in the ghost FIFO queue.\n\nWe add this FIFO-based QD technique to five state-of-the-art eviction\nalgorithms, ARC, LIRS, CACHEUS, LeCaR, and LHD. We used the open-source LHD\nimplementation from the authors, implemented the others following the\ncorresponding papers, and cross-checked with open-source implementations. We\nevaluated the QD-enhanced and original algorithms on the 5307 traces. Because\nthe traces have a wide range of miss ratios, we choose to present each\nalgorithm\u2019s miss ratio reduction from FIFO calculated as (mr_FIFO - mr_algo) /\nmr_FIFO.\n\n#### On the block traces, quick demotion can improve most state-of-the-art\nalgorithm's efficiency. Left: small cache, right: large cache.\n\n#### On the web traces, quick demotion can improve all state-of-the-art\nalgorithm's efficiency. Left: small cache, right: large cache.\n\nThe figures above show that the QD-enhanced algorithms further reduce the miss\nratio of each state-of-the-art algorithm on almost all percentiles. For\nexample, QD-ARC (QD-enhanced ARC) reduces ARC\u2019s miss ratio by up to 59.8% with\na mean reduction of 1.5% across all workloads on the two cache sizes, QD-LIRS\nreduces LIRS\u2019s miss ratio by up to 49.6% with a mean of 2.2%, and QD-LeCaR\nreduces LeCaR\u2019s miss ratio by up to 58.8% with a mean of 4.5%. Note that\nachieving a large miss ratio reduction on a large number of diverse traces is\nnon-trivial. For example, the best state-of-the-art algorithm, ARC, can only\nreduce the miss ratio of LRU 6.2% on average.\n\nThe gap between the QD-enhanced algorithm and the original algorithm is wider\n(1) when the state-of-the-art is relatively weak, (2) when the cache size is\nlarge, and (3) on the web workloads. With a weaker state-of-the-art, the\nopportunity for improvement is larger, allowing QD to provide more prominent\nbenefits. For example, QD-LeCaR reduces LeCaR\u2019s miss ratios by 4.5% on\naverage, larger than the reductions on other state-of-the-art algorithms. When\nthe cache size is large, unpopular objects spend more time in the cache, and\nquick demotion becomes more valuable. For example, QD-ARC and ARC have similar\nmiss ratios on the block workloads at the small cache size. But QD-ARC reduces\nARC\u2019s miss ratio by 2.3% on average at the large cache size. However, when the\ncache size is too large, e.g., 80% of the number of objects in the trace,\nadding QD may increase the miss ratio (not shown). At last, QD provides more\nbenefits on the web workloads than the block workloads, especially when the\ncache size is small. We conjecture that web workloads have more short-lived\ndata and exhibit stronger popularity decay, which leads to a more urgent need\nfor quick demotion. While quick demotion improves the efficiency of most\nstate-of-the-art algorithms, for a small subset of traces, QD may increase the\nmiss ratio when the cache size is small because the probationary FIFO is too\nsmall to capture some potentially popular objects.\n\nAlthough adding the probationary FIFO improves efficiency, it further\nincreases the complexity of the already complicated state-of-the-art\nalgorithms. To reduce complexity, we add the same QD technique on top of 2-bit\nCLOCK and call it QD-LP-FIFO. QD-LP-FIFO uses two FIFO queues to cache data\nand a ghost FIFO queue to track evicted objects. It is not hard to see QD-LP-\nFIFO is simpler than all state-of-the-art algorithms \u2014 it requires at most one\nmetadata update on a cache hit and no locking for any cache operation.\nTherefore, we believe it will be faster and more scalable than all state-of-\nthe-art algorithms. Besides enjoying all the benefits of simplicity, QD-LP-\nFIFO also achieves lower miss ratios than state-of-the-art algorithms. For\nexample, compared to LIRS and LeCaR, QD-LP-FIFO reduces miss ratio by 1.6% and\n4.3% on average, respectively, across the 5307 traces. While the goal of this\nwork is not to propose a new eviction algorithm, QD-LP-FIFO illustrates how we\ncan build simple yet efficient eviction algorithms by adding quick demotion\nand lazy promotion techniques to a simple base eviction algorithm such as\nFIFO.\n\n# Discussion\n\nWe have demonstrated reinsertion as an example of LP and the use of a small\nprobationary FIFO queue as an example of QD. However, these are not the only\ntechniques. For example, reinsertion can leverage different metrics to decide\nwhether the object should be reinserted. Besides reinsertion, several other\ntechniques are often used to reduce promotion and improve scalability, e.g.,\nperiodic promotion, batched promotion, promoting old objects only, and\npromoting with try-lock. Although these techniques do not fall into our strict\ndefinition of lazy promotion (promotion on eviction), many of them effectively\nretain popular objects from being evicted. On the quick demotion side, besides\nthe small probationary FIFO queue, one can leverage other techniques to define\nand discover unpopular objects such as Hyperbolic and LHD. Moreover, admission\nalgorithms, e.g., TinyLFU, Bloom Filter, probabilistic, and ML-based admission\nalgorithms, can be viewed as a form of QD \u2014 albeit some of them are too\naggressive at demotion (rejecting objects from entering the cache).\n\nNote that QD bears similarity with some generational garbage collection\nalgorithms, which separately store short-lived and long-lived data in young-\ngen and old-gen heaps. Therefore, ideas from garbage collection may be\nborrowed to strengthen cache eviction algorithms.\n\nThe design of QD-LP-FIFO opens the door to designing simple yet efficient\ncache eviction algorithms by innovating on LP and QD techniques. And we\nenvision future eviction algorithms can be designed like building LEGO \u2014\nadding lazy promotion and quick demotion on top of a base eviction algorithm.\n\n# Acknowledgement\n\nThere are many people I would like to thank, including but not limited to my\nco-authors, Carnegie Mellon University Parallel Data Lab (and our sponsors),\nand Cloudlab. I would also like to give a big shoutout to the people and\norganizations that open-sourced the traces, without which, this work is not\npossible and we will NEVER know that CLOCK is better than LRU on every aspect!\n\nThis work is published at HotOS\u201923, more details can be found here, and the\nslides can be found here.\n\nJuncheng Yang 2024\n\n\u00b7\n\n", "frontpage": false}
