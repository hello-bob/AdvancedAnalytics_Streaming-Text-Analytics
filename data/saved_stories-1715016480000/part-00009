{"aid": "40271582", "title": "Large Language Models, Open API, and BFFs", "url": "https://johnnyreilly.com/large-language-models-view-models-backend-for-frontend", "domain": "johnnyreilly.com", "votes": 1, "user": "johnny_reilly", "posted_at": "2024-05-06 06:00:31", "comments": 0, "source_title": "Large Language Models, Open API, View Models and the Backend for Frontend Pattern", "source_text": "Large Language Models, Open API, View Models and the Backend for Frontend Pattern | johnnyreilly\n\nSkip to main content\n\n# Large Language Models, Open API, View Models and the Backend for Frontend\nPattern\n\nMay 3, 2024 \u00b7 8 min read\n\nJohn Reilly\n\nOSS Engineer - TypeScript, Azure, React, Node.js, .NET\n\nOf late, I've been involved in work to integrate APIs into LLM interactions,\nusing Semantic Kernel. This post is something of a brain dump on the topic.\nGiven how fast this space is moving, I expect what is written here to be out\nof date, possibly even before I hit publish. But nevertheless, I hope it's\nuseful.\n\n## Swagger / Open API and Semantic Kernel\n\nAPIs are awesome. Imagine LLMs could interact with APIs to allow us to chat\ndirectly to our data. This is what function calling provides. It allows us to\ntake some kind of API and integrate it with our LLM. This is a powerful\nconcept, but it's not without its challenges.\n\nAPIs are often documented in Swagger / Open API. This is a great way to\ndocument APIs, but it's not always the best way to interact with them from an\nLLM point of view. We'll go into more detail on the problems it can present in\na moment, but first let's look at how we can use Semantic Kernel to integrate\nwith APIs.\n\nIt's completely possible to plug an LLM into an Open API / Swagger spec\ndescribed API using Semantic Kernel. Here's an example of how we might do that\nfrom the Semantic Kernel GitHub repository:\n\n    \n    \n    var apiPluginRawFileURL = new Uri(\"https://raw.githubusercontent.com/microsoft/PowerPlatformConnectors/dev/certified-connectors/JIRA/apiDefinition.swagger.json\"); jiraFunctions = await kernel.ImportPluginFromOpenApiAsync( \"jiraPlugin\", apiPluginRawFileURL, new OpenApiFunctionExecutionParameters( httpClient, tokenProvider.AuthenticateRequestAsync, serverUrlOverride: new Uri(serverUrl) ) );\n\nThe code above is creating a Jira plugin from an Open API spec. Brilliant! We\ndidn't have to do any work; Semantic Kernel has done the heavy lifting for us.\nIt's created a plugin that we can use to interact with Jira. Are you ready for\nthe but?\n\n## The problem with Swagger / Open API and LLMs\n\nThe example above illustrates the simplicity of integrating. But what it\ndoesn't reveal is the unfortunate reality that LLMs are not great at ignoring\ninformation. They will mention information we explicitly tell them not to.\nJust to spite us.\n\nLet's take the Jira plugin as an example. When using direct Swagger / Open API\nintegration I have found myself writing prompts like this:\n\n> Please tell me about stories that are assigned to me. Please never refer to\n> the stories by their ids - use titles instead.\n\nOnly to find that in the responses the LLM will still refer to the stories by\ntheir ids.\n\nIt's a bit like having a child who you've told not to do something, only to\nfind they've done it anyway. The LLM may even cheekily say something like \"I\nknow you told me not too, but I included the id for reference\". The scallywag.\n\nOr perhaps, given the variety of endpoints that are available in an API, the\nLLM will call one that we didn't want it to. Or perhaps our Swagger / Open API\nspec is poorly documented, and the LLM doesn't think it has an endpoint it can\ncall.\n\n## View models and the BFFs to the rescue\n\nA useful framing for this problem is remembering when ORMs started to automate\naccess to databases. We could take our ORM, and host it in a web service and,\nhey presto, our database was now accessible over HTTP. So let's take our React\napp (or whatever) and have it talk directly to our database.\n\nExcept, of course, that's a terrible idea. We don't want our front end talking\nto our database. There's a number of reasons why:\n\n  * Too much information going backwards and forward between client and server (perhaps including information we'd never like clients to see).\n  * Security; why are we exposing our database to updates directly from the internet? Is that wise?\n\nYou get the picture. We tend not to integrate our databases directly with our\nfront ends with good reason.\n\nA common approach to tackle these issues is employing the back end for front\nends (BFF) pattern; having something that sits between our front end and our\ndatabase. One of the things the BFF does is to provide a view of the data that\nis appropriate for the client. So for example, exposing a view model in the\nback end to serve the front end. It's a way to ensure that only the necessary\ninformation is exposed to the client.\n\nWe can take this idea and apply it to building integrations with APIs and\nLLMs. So rather than plugging a Swagger / Open API spec into Semantic Kernel,\ninstead build a custom plugin that manages access to our API, and have it\nexpose view models for providing data to LLMs.\n\nThat way we can ensure that only the necessary information is given to the\nLLM, and our answers do not contain data we would rather not see.\n\nSo rather than giving an LLM a data structure like this:\n\n    \n    \n    { \"stories\": [ { \"id\": 1, \"title\": \"Story 1\", \"description\": \"This is the first story\" //... MANY MORE FIELDS }, { \"id\": 2, \"title\": \"Story 2\", \"description\": \"This is the second story\" //... MANY MORE FIELDS } ] }\n\nWe give it the trimmed down equivalent:\n\n    \n    \n    { \"stories\": [ { \"title\": \"Story 1\", \"description\": \"This is the first story\" }, { \"title\": \"Story 2\", \"description\": \"This is the second story\" } ] }\n\nThis has the combined benefit of reducing our token usage / cost (as we're\nsending less data to the LLM) and reducing the risk of exposing data we'd\nrather not.\n\nIt also has the advantage of allowing us to steer the LLM towards the\nfunctions we want it to call. If we only expose the functions we want the LLM\nto call, then we can ensure that it doesn't call functions we'd rather it\ndidn't.\n\n## \"But integrating with APIs is a lot of work!\"\n\nA common, and quite reasonable, complaint is that integrating with an API\ninvolves a lot of work. We have to write some code to interact with the API,\nand then we have to write the types that we'll use to pass data around.\nFortunately there are tools like NSwag that use the Swagger / Open API spec to\nautomate creating a client with types to manage API interaction. If we're\nautogenerating our API clients, then the work of integrating an LLM with an\nAPI is significantly reduced.\n\nWith Semantic Kernel it effectively is reduced to creating a plugin; and\nthat's quite simple to do. There is guidance on how to create a plugin here.\nSo to create a BFF plugin for an API, we'd need to create that plugin,\nexposing the functions we want to be called. Those functions will internally\ncall into the APIs using the auto-generated Swagger clients and then map that\nto the view models which we want to expose to the LLM. Imagine something like\nthis:\n\n    \n    \n    public record JiraStory( string Title, string Description );\n    \n    [KernelFunction] [Description(\"Provides stories for a given user\")] [return: Description(\"Jira user stories for the given user\")] public async Task<JiraStory[]> GetUsersJiraStories( Kernel kernel,\n    \n    [Description(\"Email of user to filter by\")] string userEmail ) { var stories = await _jiraClient.GetStories(userEmail);\n    \n    return stories .Select(story => new JiraStory( title: story.Title, description: story.Description )) .ToArray(); }\n\nThe code above exposes a well defined function to the LLM, which will return\nthe stories for a given user. The function internally calls into the Jira API,\nand then maps the large amount of data returned from the API to a much slimmer\nview model that is appropriate for the LLM. As we can see, this was very\nlittle work indeed!\n\n## Conclusion\n\nThe integrated support for consuming Open API / Swagger specs is definitely\ngoing to improve over time, both in Semantic Kernel and in the wider\necosystem. However, it's possible that there is fundamental issue that needs\nto be solved, and that BFFs for LLMs may solve it. It's a way to ensure that\nonly the necessary information is exposed to LLMs, and that the answers they\ngive are appropriate for the context in which they are being used.\n\nI'm not aware of a specific name for this pattern as yet. My colleague, Ryan\nsuggested \"Frontend for Language Models\" (FLM) which is less of a mouthful\nthan \"Backend for Frontends for Language Models\". Naming things is hard.\n\nAnother colleague (Rick), suggested that perhaps the BFF for LLMs could be\nbuilt directly into APIs. So rather than having to implement a custom plugin\nthat manages the interaction with API, we could still perhaps use the Swagger\n/ Open API approach and avoid the custom plugin implementation. This is an\ninteresting idea.\n\nMany thanks to David Rosevear, George Karsas, Rick Roch\u00e9, Ryan Cook and Ali\nSomer whose thoughts, ideas and experimentation have fed into the thinking in\nthis post.\n\nTags:\n\n  * azure\n  * c#\n  * ai\n\nEdit this page\n\n  * Swagger / Open API and Semantic Kernel\n  * The problem with Swagger / Open API and LLMs\n  * View models and the BFFs to the rescue\n  * \"But integrating with APIs is a lot of work!\"\n  * Conclusion\n\n  * TypeScript\n\n    * TypeScript vs JSDoc JavaScript\n    * Type annotations proposal: strong types, weakly held\n\nAzure\n\n    * Azure Container Apps: Easy Auth and .NET\n    * Azure Static Web Apps: dynamic redirects with Azure Functions\n\nASP.NET\n\n    * ESLint your C# with Roslyn Analyzers\n    * ASP.NET, Serilog and Application Insights\n\nReact\n\n    * Structured data and React\n    * React: storing state in URL with URLSearchParams\n\n  * Notable articles\n\n    * The history of Definitely Typed\n    * TypeScript: the documentary\n    * The definitive guide to migrating from Blogger to Docusaurus\n    * How we fixed my SEO\n\nPopular articles\n\n    * ASP.NET, Serilog and Application Insights\n    * ESLint your C# with Roslyn Analyzers\n    * dotnet-format: Prettier your C# with lint-staged & husky\n\nRecently updated\n\n    * Large Language Models, Open API, View Models and the Backend for Frontend Pattern\n    * XML: read and write with Node.js\n    * Under the Duck: An Afternoon in Open Source\n\nLearn more / support me\n\n  * Blog source code on GitHub\n  * Blog categories\n  * RSS feed\n  * Atom feed\n  * Privacy Policy\n\nCopyright \u00a9 2012 - 2024 John Reilly. Built with Docusaurus.\n\n", "frontpage": false}
