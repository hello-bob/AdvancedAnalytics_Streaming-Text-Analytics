{"aid": "40130506", "title": "Developing Rapidly with Generative AI", "url": "https://discord.com/blog/developing-rapidly-with-generative-ai", "domain": "discord.com", "votes": 2, "user": "makaimc", "posted_at": "2024-04-23 11:10:26", "comments": 0, "source_title": "Developing Rapidly with Generative AI", "source_text": "Developing Rapidly with Generative AI\n\nEngineering & DevelopersDeveloping Rapidly with Generative AI\n\nShannon Phu\n\nApril 12, 2024\n\nTags\n\nGenerative AI is attracting attention as the technology has progressed in\nleaps and bounds in recent years, offering fresh ways to solve user problems.\nSince it's a relatively new area in terms of its practical application,\nfiguring out how to start building with LLMs (large language models) can be\nchallenging. We're excited to share our approach for solving problems with\ngenerative AI, along with insights on rapidly launching new features\nleveraging this technology.\n\nWe break down the process of building with LLMs into a few stages. Starting\nwith product ideation and defining requirements, we first need to figure out\nwhat we\u2019re building and how it can benefit users. Next, we develop a prototype\nof our idea, learn from small-scale experiments, and repeat that process until\nour feature is in a good state. Finally, we fully launch and deploy our\nproduct at scale. In this post, we will dive deeper into each stage of this\nprocess.\n\nThe different stages of building an LLM-powered feature\n\n## How we identify use cases for generative AI\n\nWe start by having empathy for our users and for our staff - what are the\nopportunities that generative AI can help address? Like machine learning in\ngeneral, generative AI is a tool \u2014 and one that shouldn\u2019t be applied when\nother tools are a better fit. When it comes to identifying where generative AI\ncan make an impact, we dig into challenges that commonly:\n\n  * Involve analysis, interpretation, or review of unstructured content (e.g. text) at scale\n  * Require massive scaling that may be otherwise prohibitive due to limited resources\n  * Would be challenging for rules-based or traditional ML approaches\n\n## Defining product requirements\n\nOnce we've identified a potential use case for a generative AI application,\nthe next step involves defining the product requirements. This phase requires\na thoughtful analysis to select the best-suited LLM and to frame our problem\nas a prompt to an LLM.\n\nWe consider these aspects of our problem:\n\n  * Latency: How fast does the system need to respond to user input?\n  * Task Complexity: What level of understanding is required from the LLM? Is the input context and prompt super domain-specific?\n  * Prompt Length: How much context needs to be provided for the LLM to do its task?\n  * Quality: What is the acceptable level of accuracy for the generated content?\n  * Safety: How important is it to sanitize user input or prevent the generation of harmful content and prompt hacking?\n  * Language Support: Which languages does the application need to support?\n  * Estimated QPS: What throughput does our system eventually need to handle?\n\nSeveral factors, such as complexity, prompt length, and quality, often\nconflict with the need for low latency, primarily because a bigger, more\ncapable LLM usually delivers better outcomes but operates more slowly during\ninference owing to the model\u2019s larger size. Consequently, if minimizing\nresponse time is critical, we can consider either incurring higher costs (e.g.\nby having more available compute) or accepting a drop in quality by using\nsmaller models.\n\n## Prototyping AI applications: From Idea to MVP\n\nThe product requirements we define then play into our selection of which off-\nthe-shelf LLM we'll use for our prototype. We generally lean towards picking\nmore advanced commercial LLMs to quickly validate our ideas and obtain early\nfeedback from users. Although they may be expensive, the general idea is that\nif problems can't be adequately solved with state-of-the-art foundational\nmodels like GPT-4, then more often than not, those problems may not be\naddressable using current generative AI tech. If an off-the-shelf LLM can\naddress our problem, then we can step into the learning stage and concentrate\non iterating on our product rather than diverting engineering resources\ntowards building and maintaining machine learning infrastructure.\n\n### Evaluating Prompts\n\nThe key step at this stage is to create the right prompt. We start with a\nbasic prompt that tells ChatGPT (or whatever LLM we selected for our\nprototype) what we want it to do. Then, we make adjustments to this prompt,\nchanging the wording to make the task clearer. However, after a lot of\nadjustments, it's often difficult to tell if these changes are actually\nimproving our results. That's where evaluating the prompts becomes crucial. By\nusing metrics to guide our changes, we know we are moving the needle on the\nquality of our results.\n\nTo do this, we employ a technique known as AI-assisted evaluation, alongside\ntraditional metrics for measuring performance. This helps us pick the prompts\nthat lead to better quality outputs, making the end product more appealing to\nusers. AI-assisted evaluation uses best-in-class LLMs (like GPT-4) to\nautomatically critique how well the AI's outputs match what we expected or how\nthey score against a set of criteria. This method uses GPT-4 in a way that\u2019s\nsimilar to the critic model found in the actor-critic algorithm in\nreinforcement learning where a separate model is used to evaluate how well the\nmodel used for inference performed. Automating evaluation allows us to quickly\nsee what's working well and what needs to be tweaked in our prompts, without\nhaving to manually check everything. When evaluating, we design prompts that\nask for simple yes or no answers or rate the outputs on a scale, making the\nevaluation process straightforward.\n\nAI-assisted evaluation consists of 2 separate prompts: one for your task and\nanother to evaluate your results. The task prompt is passed to the inference\nmodel whereas the critic prompt is passed to the more advanced critic model.\n\n### Launch and Learn\n\nOnce we are sufficiently confident in the quality of the results our prompt\ngenerates, we roll out a limited release (e.g. A/B test) of our product and\nobserve the system\u2019s performance in situ. The exact metrics we use depend on\nthe application \u2014 our main goal is to understand how users use the feature and\nquickly make improvements to better meet their needs. For internal\napplications, this might mean measuring efficiency and sentiment. For\nconsumer-facing applications, we similarly focus on measures of user\nsatisfaction - direct user feedback, user engagement measures, etc. This\nfeedback is critical to identify areas for improvement, including highlighting\nincorrect answers or instances where LLM hallucinations might be causing a\nstrange user experience.\n\nBeyond user satisfaction, we also pay attention to system health metrics, such\nas response speed (latency), throughput (tokens per second), and error rates.\nLLMs sometimes have trouble generating output in a consistently structured\nformat, which is crucial for minimizing data parsing errors and ensuring the\noutput is robustly usable in our services. Insights here can inform how much\npost-hoc processing might be needed to fully productionize this capability at\nscale.\n\nKeeping an eye on costs is equally important for understanding how much we\nwill spend when we fully scale up the feature. We look at how many tokens per\nsecond we're using in our initial limited release to predict the costs of a\ncomplete launch if we were to use the same technology that\u2019s powering our\nprototype.\n\nAll of the above information is critical to understanding if our product is\nworking as intended and providing value to users. If it is, then we can\nproceed to the next step: deploying at scale. If not, then we look to take our\nlearnings, iterate on the system, and try again.\n\n## Deploying at Scale\n\n### LLM Application Architecture\n\nA high-level architecture for an LLM application\n\nThe basic setup for apps using LLMs consists of several essential parts.\nInputs to the inference server are prepared into a prompt that we\u2019ve tested\nand evaluated on a robust set of examples. At the heart of the architecture\nlies the LLM inference server, tasked with the job of operating the LLM to\nproduce answers from the inputs it gets. Examples of such servers commercially\ninclude ChatGPT or other OpenAI GPT APIs, which are specialized in generating\ncontent with low latency.\n\nBecause we care deeply about the user experience, privacy, and safety, we work\nwith cross-functional partners like Legal and other Safety teams to ensure\nwe\u2019ve implemented thoughtful mitigations, while adhering to privacy principles\nsuch as data minimization. For example, we chose to incorporate content safety\nfilters to the output of the inference server to identify undesired material\nbefore it reaches the user. We can leverage in-house or third-party trust and\nsafety ML models to detect inappropriate content.\n\nAll these elements put together form a system that taps into the capabilities\nof LLMs efficiently while monitoring the content's quality and safety to\nensure that we\u2019re delivering a quality end product.\n\n### Self-hosted LLMs\n\nWhen we're thinking about adding a feature that uses LLMs, we consider many\ntradeoffs when designing our LLM inference server such as balancing the costs\nand the amount of engineering effort. Using commercial LLMs is great because\nit gives us access to top-notch models and we don't have to worry about\nsetting up the tech ourselves, but the expenses can add up quickly. For\nprivacy reasons, we may also prefer to process full-scale data completely in-\nhouse. A solution is to self-host an open-sourced or custom fine-tuned LLM.\nOpting for a self-hosted model can reduce costs dramatically - but with\nadditional development time, maintenance overhead, and possible performance\nimplications. Considering self-hosted solutions requires weighing these\ndifferent trade-offs carefully.\n\nRecent open-source models, like Llama and Mistral, are making high-quality\nresults possible right out of the gate, even for complex tasks that\ntraditionally required a model to be trained specifically for them. However,\nfor domain-specific or complex tasks, we might still need to fine-tune the\nmodel to achieve excellent performance. We've found it's best to start with\nsmaller models and only move up to bigger ones if needed for quality reasons.\n\nSetting up the necessary machine learning infrastructure to run these big\nmodels is another challenge. We need a dedicated model server for running\nmodel inference (using frameworks like Triton or vLLM), powerful GPUs to run\neverything robustly, and configurability in our servers to make sure they're\nhigh throughput and low latency. Tuning the inference servers for optimal\nperformance is task-specific - the best configuration depends on the models\nwe\u2019re using, as well as the input and output token lengths, and ultimately\nimpacts how efficiently the server can batch input requests to maximize\nthroughput.\n\nSelf-hosted inference server\n\n## Closing Thoughts\n\nLooking ahead, there\u2019s little doubt that generative AI will only grow more\nimportant as a means of solving massive-scale business-critical problems.\nBalancing cost, engineering effort, and performance will remain challenging,\nand we\u2019re excited to see (and contribute to) the rapid development of novel\ntechnology and tools to more effectively do so in the coming years!\n\nContents\n\nIntroHow we identify use cases for generative AIDefining product\nrequirementsPrototyping AI applications: From Idea to MVPDeploying at\nScaleClosing Thoughts\n\nTHE AUTHOR\n\nShannon Phu\n\nSenior Machine Learning Engineer, Applied Machine Learning.\n\nMORE FROM\n\nEngineering & Developers\n\nEngineering & Developers\n\nIt\u2019s time to Pitch Your Discord Apps!!\n\nEngineering & Developers\n\nHeaded to GDC 2024? Check Out the Latest News for Discord Game Devs\n\nEngineering & Developers\n\nHow It All Goes Live: An Overview of Discord\u2019s Streaming Technology\n\nEnglish, USA\n\n\u010ce\u0161tina\n\nDansk\n\nDeutsch\n\nEnglish\n\nEnglish (UK)\n\nEspa\u00f1ol\n\nEspa\u00f1ol (Am\u00e9rica Latina)\n\nFran\u00e7ais\n\nHrvatski\n\nItaliano\n\nlietuvi\u0173 kalba\n\nMagyar\n\nNederlands\n\nNorsk\n\nPolski\n\nPortugu\u00eas (Brasil)\n\nRom\u00e2n\u0103\n\nSuomi\n\nSvenska\n\nTi\u1ebfng Vi\u1ec7t\n\nT\u00fcrk\u00e7e\n\n\u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac\n\n\u0431\u044a\u043b\u0433\u0430\u0440\u0441\u043a\u0438\n\n\u0420\u0443\u0441\u0441\u043a\u0438\u0439\n\n\u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430\n\n\u0939\u093f\u0902\u0926\u0940\n\n\u0e44\u0e17\u0e22\n\n\ud55c\uad6d\uc5b4\n\n\u4e2d\u6587\n\n\u4e2d\u6587(\u7e41\u9ad4)\n\n\u65e5\u672c\u8a9e\n\nEnglish\n\nProduct\n\nDownloadNitroStatusApp DirectoryNew Mobile Experience\n\nCompany\n\nAboutJobsBrandNewsroom\n\nResources\n\nCollegeSupportSafetyBlogFeedbackStreamKitCreatorsCommunityDevelopersGamingQuestsOfficial\n3rd Party Merch\n\nPolicies\n\nTermsPrivacyCookie SettingsGuidelinesAcknowledgementsLicensesModeration\n\nSign up\n\n## Discord\n\nBy clicking \u201cAccept All Cookies\u201d, you agree to the storing of cookies on your\ndevice to enhance site navigation, analyze site usage, and assist in our\nmarketing efforts.Privacy Notice\n\n", "frontpage": false}
