{"aid": "40275273", "title": "Developing a RAG Knowledge Base with DuckDB", "url": "https://motherduck.com/blog/search-using-duckdb-part-2/", "domain": "motherduck.com", "votes": 1, "user": "polyrand", "posted_at": "2024-05-06 14:43:23", "comments": 0, "source_title": "Developing a RAG Knowledge Base with DuckDB", "source_text": "Developing a RAG Knowledge Base with DuckDB\n\nBuilding data-driven components and applications doesn't have to be so ducking\nhard\n\nmotherduck wasm sdk\n\nGO BACK TO BLOG\n\n# Developing a RAG Knowledge Base with DuckDB\n\n2024/05/06\n\nBY\n\nAdithya Krishnan\n\nSubscribe to MotherDuck Blog\n\nThis blog is the second in a series of three on search using DuckDB. It builds\non knowledge from the first blog on AI-powered search, which shows how\nrelevant textual information is retrieved using cosine similarity.\n\nDifferent facets of our work and our lives are documented in different places,\nfrom note-taking apps to PDFs to text files, code blocks, and more. AI\nassistants that use large language models (LLMs) can help us navigate this\nmountain of information by answering contextual questions based on it. But how\ndo AI assistants even get this knowledge?\n\nRetrieval Augmented Generation (RAG) is a technique to feed LLMs relevant\ninformation for a question based on stored knowledge. A knowledge base is a\ncommonly used term that refers to the source of this stored knowledge. In\nsimple terms, it\u2019s a database that contains information from all the documents\nwe feed into our model.\n\nOne common method of storing this data is to take documents and chunk up the\nunderlying text into smaller parts (e.g., a group of four sentences) so these\n\u2018chunks\u2019 can be stored along with their vector embeddings. These blocks of\ntext can later be retrieved based on their cosine similarity. At its simplest,\na RAG can retrieve relevant information as text and feed it to an LLM, which\nin turn will output an answer to a question. For example, if we asked a\nquestion, we would retrieve the top 3 relevant chunks of text from our\nknowledge base and feed them to an LLM to generate an answer. Lots of research\nhas been done in the field, from pioneering new, better ways to chunk\ninformation, store it, and retrieve it based on a variety of techniques. That\nsaid, information retrieval in RAG is typically based on semantic similarity.\n\nHow cool would it be to build your own AI-powered personal assistant? In this\nblog post, we walk through a step-by-step example of how to build an AI-\npowered knowledge base and use it as a foundation to answer end users\u2019\nquestions by running embedding and language models.\n\n## Building an AI Assistant with a Local Knowledge Base\n\nBuilding an AI assistant consists of three parts: the embedding model, the\nknowledge base, and the LLM that uses relevant information to form the answer.\nIn our example, we use Llama-Index, a Python data framework for building LLM\napplications to put all the pieces of the AI assistant together.\n\nTo kick things off, let\u2019s install the dependencies for our project:\n\n    \n    \n    pip install llama-index pip install llama-index-embeddings-huggingface pip install llama-index-vector-stores-duckdb pip install llama-index-llms-ollama\n\n## Embedding Model with HuggingFace and SentenceTransformers\n\nHuggingFace provides access to a large repository of embedding models. The\nHuggingFace-LlamaIndex integration makes it easier to download an embedding\nmodel from HuggingFace and run embedding models using the Python package\nSentenceTransformers. In this project, we will use \u201cBAAI/bge-small-en-v1.5,\u201d a\nsmall model that generates a vector embedding of 384 dimensions with a maximum\ninput tokens limit of 512. This means that the maximum chunk size of the text\nwill be 512 tokens.\n\nThe following code will download and run the model:\n\n    \n    \n    from llama_index.embeddings.huggingface import HuggingFaceEmbedding # loads BAAI/bge-small-en-v1.5, embed dimension: 384, max token limit: 512 embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n\nNow, let\u2019s test the model by generating vector embeddings for the following\ntext: \u201cknowledge base\u201d\n\n    \n    \n    test_embeddings = embed_model.get_text_embedding(\"knowledge base\") print(len(test_embeddings)) >>> 384\n\nWhen we print the length of the generated embeddings, we get 384, the\ndimension size of the vector for this model that we mentioned above.\n\n## The Knowledge Base\n\nDuckDB provides a convenient fixed array size and list (variable size) data\ntype to store vector embeddings. LlamaIndex has a DuckDB integration that\nhelps you store your compiled knowledge base and save it to disk for future\nuse.\n\nNext, let\u2019s build our knowledge base by importing the necessary dependencies:\n\n    \n    \n    # Imports for loadings documents and building knowledge from llama_index.core import ( StorageContext, ServiceContext, VectorStoreIndex, SimpleDirectoryReader, ) # DuckDB integration for storing and retrieving from knowledge base from llama_index.vector_stores.duckdb import DuckDBVectorStore\n\nIn this project, we will load documents from a folder called local_documents\nusing the \u2018SimpleDirectoryReader.\u2019\n\nBy using the \u2018ServiceContext\u2019 object, we can define the chunking strategy for\nthe text in the documents:\n\n    \n    \n    # Load the files in the folder 'papers' and store them as Llama-Index Document object documents = SimpleDirectoryReader(\"./local_documents\").load_data() # Set the size of the chunk to be 512 tokens documents_service_context = ServiceContext.from_defaults(chunk_size=512)\n\nIt\u2019s finally time to build our knowledge base. When we initialize the\nDuckDBVectorStore and pass it to the StorageContext, LlamaIndex learns that\nDuckDB should be used for storage and retrieval. The initialization process\nalso tells LlamaIndex how to use DuckDB.\n\nBy passing the embedding model, DuckDB storage context, and documents\u2019 context\nto the VectorStoreIndex object, we can create our knowledge base.\n\nIn the following code snippet, the DuckDBVectorStore is initialized by passing\na directory location to use to persist your knowledge base:\n\n    \n    \n    vector_store = DuckDBVectorStore( database_name=\"knowledge_base\", persist_dir=\"./\", embed_dim=384, ) storage_context = StorageContext.from_defaults(vector_store=vector_store) knowledge_base = VectorStoreIndex.from_documents( documents, storage_context=storage_context, embed_model=embed_model, service_context=documents_service_context, )\n\nThis means that a database file with the specified database name\n\u2018knowledge_base\u2019 will be created in the listed directory. It\u2019s important to\nnote that our database file can be reused, which means you can add new\ndocuments to it. You can learn more about this here.\n\nNote: It is important to specify the dimensions of the vector embeddings used,\nas this information will be required for the embedding field data type when we\ncreate the table to store the embeddings.\n\n## The Large Language Model (LLM)\n\nOne benefit of Ollama is that it lets you run language models on your system.\nLlamaIndex has a convenient integration for Ollama, enabling you to connect\nany of your data sources to your LLMs. In this project, we use the \u2018llama2\u2019\nmodel, but there are plenty of other models in its library, which you can find\nhere.\n\nLet\u2019s begin by initializing the model:\n\n    \n    \n    from llama_index.llms.ollama import Ollama llm = Ollama(model=\"llama2\", request_timeout=60.0)\n\nNote: a request timeout has been configured to cancel the request if a\nresponse is not obtained within the specified time frame.\n\n## Query Answer Engine\n\nAlthough small, the model has captured decent knowledge of the world. To\nprovide better context for questions, we can pass relevant knowledge from our\nknowledge base to the model and generate answers. We do this by building a\nquery engine with our knowledge base and passing the LLM object to the query\nengine.\n\nWith this query engine, you can ask questions, and it will fetch the relevant\ninformation from the knowledge base and generate an answer:\n\n    \n    \n    # The query engine query_engine = knowledge_base.as_query_engine(llm=llm) # Run a query answer = query_engine.query(\"...fill in your question...\")\n\n## Conclusion\n\nTurning documents into a knowledge base for AI with DuckDB is incredibly\nexciting because you can run this workflow directly on your computer. The\npossibilities created by having a personalized AI assistant that can browse\nyour documents and answer questions on demand are still emerging, and we can\u2019t\nwait to see what the future has in store.\n\nUsing DuckDB, you can store your knowledge, persist it on disk, and retrieve\nthe relevant information for your AI assistants. As we\u2019ve seen above, the\nLlama-Index integration is easy to integrate with the other parts of an AI\nassistant, like the LLM and embedding model.\n\n###### CONTENT\n\n  1. Building an AI Assistant with a Local Knowledge Base\n\n  2. Embedding Model with HuggingFace and SentenceTransformers\n\n  3. The Knowledge Base\n\n  4. The Large Language Model\n\n  5. Query Answer Engine\n\n  6. Conclusion\n\nSubscribe to MotherDuck Blog\n\n## NEXT POSTS\n\n2024/04/29 - Vasilije Markovic\n\n## Structured memory management for AI Applications and AI Agents with DuckDB\n\nLearn how to optimize Retrieval-Augmented Generation (RAG) systems with\nDuckDB, dlt, and Cognee to streamline data management and workflows for\naccurate LLM outputs.\n\n2024/04/30 - Luciano Galv\u00e3o Filho\n\n## This Month in the DuckDB Ecosystem: April 2024\n\nDuckDB Monthly: Dr. Qiusheng Wu, CSVs, 1 Billion Row challenge, duckplyr, and\nmore\n\nView all\n\nMotherDuck\n\nBlog\n\nDocs\n\nProduct\n\nOverview\n\nFor Data Teams\n\nFor App Devs\n\nEcosystem\n\nPricing\n\nCompany\n\nAbout us\n\nCareers\n\nQuacking\n\nCommunity\n\nEvents\n\nDuckDB News\n\nDuckDB Snippets\n\nDuckDB Docs\n\nFree DuckDB Book\n\nMotherDuck is powered by\n\nDuckDB\n\nTerms of Use\n\nPrivacy Policy\n\n", "frontpage": false}
