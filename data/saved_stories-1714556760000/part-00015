{"aid": "40217178", "title": "Matrix Multiplications on GPUs Run Faster When Given Predictable Data", "url": "https://www.thonking.ai/p/strangely-matrix-multiplications", "domain": "thonking.ai", "votes": 3, "user": "jasondavies", "posted_at": "2024-04-30 22:26:40", "comments": 0, "source_title": "Strangely, Matrix Multiplications on GPUs Run Faster When Given \"Predictable\" Data! [short]", "source_text": "Strangely, Matrix Multiplications on GPUs Run Faster When Given \"Predictable\"\nData! [short]\n\n# Thonk From First Principles\n\nShare this post\n\n#### Strangely, Matrix Multiplications on GPUs Run Faster When Given\n\"Predictable\" Data! [short]\n\nwww.thonking.ai\n\n#### Discover more from Thonk From First Principles\n\nML Systems from first principles. Aims to be better than a ChatGPT summary.\n\nContinue reading\n\nSign in\n\n# Strangely, Matrix Multiplications on GPUs Run Faster When Given\n\"Predictable\" Data! [short]\n\n### Great minds discuss flops per watt.\n\nHorace He\n\nApr 29, 2024\n\n23\n\nShare this post\n\n#### Strangely, Matrix Multiplications on GPUs Run Faster When Given\n\"Predictable\" Data! [short]\n\nwww.thonking.ai\n\n5\n\nShare\n\nIt\u2019s 2022. I check out this cool new project, CUTLASS, with very fast matmuls.\nI take a large matmul, 8192 x 8192 x 8192, and benchmark it in PyTorch, which\ncalls CuBLAS.\n\n    \n    \n    python mm_bench.py > CuBLAS: 258 Teraflops\n\nNot bad, 83% flop utilization. Now let\u2019s check out Cutlass\u2019s performance using\ntheir profiler.\n\n    \n    \n    ./cutlass_profiler --operation=Gemm --m=8192 --n=8192 --k=8192 > CUTLASS: 288 Teraflops\n\n!!! 10% higher perf? That\u2019s incredible. CuBLAS is highly optimized for large\ncompute-bound matmuls, and somehow CUTLASS + autotuning is outperforming it by\n10%? We gotta start using these matmuls yesterday.\n\nThe next step is to bind the CUTLASS kernels into Python and compare against\nCuBLAS using my previous script.\n\n    \n    \n    python cutlass_mm_bench.py > CuBLAS: 258 Teraflops > CUTLASS: 257 Teraflops\n\nSomehow, in the light of Python, all of CUTLASS\u2019s performance gains disappear.\nThis in of itself is not shocking - it\u2019s notoriously difficult to ensure\nconsistent benchmarking across setups.\n\nI tediously ablate the two benchmark scripts, until finally, I find that\nCUTLASS\u2019s profiler, by default, actually initializes the values in a fairly\nstrange way - it only initializes the inputs with integers. Confused about\nwhether this matters, I try:\n\n    \n    \n    zero_inputs = torch.zeros(N, N) randn_inputs = torch.randn(N, N) benchmark(zero_inputs) # 295 Teraflops benchmark(randn_inputs) # 257 Teraflops\n\nWhat? How could the values of the matrix affect the runtime of the model? I\nknow Nvidia has some weird data compression thing on A100s, but I wouldn\u2019t\nhave expected that to be on in matmuls. Let\u2019s try some other data\ndistributions, like an uniform distribution [0,1].\n\nThis was ... confusing, to say the least. Somehow, the actual content of the\ntensors being multiplied is leading to different matmul performance.\n\nThere certainly are cases where the runtime depends on the content of the\ntensor \u2014 indirect indexing (e.g. A[b]), or things like sparsity.\n\nBut matrix multiplications have nothing like that at all! No matter what the\ncontents of the matrix contain, the matrix multiplication kernel will 1.\nperform the same number of computations, 2. perform the same computations in\nthe same order, 3. access the same memory addresses, and 4. access the same\nmemory addresses in the same order.\n\nNowhere did my mental model of matrix multiplications and GPU hardware allow\nfor the values in the matrix to influence matmul performance. And yet, here we\nare.\n\nAs it turns out, the culprit is ....... dynamic/switching power in\nsemiconductors!\n\nThonk From First Principles is a reader-supported publication. To receive new\nposts and support my work, consider becoming a free or paid subscriber.\n\n#\n\nPower Usage in Semiconductors\n\nAn Nvidia A100 GPU has a power limit of 400W1. However, as the phrase \u201cpower\nlimit\u201d may hint, the GPU doesn\u2019t always use all 400W. For example, when the\nGPU is fully idle, nvidia-smi tells me that it\u2019s only pulling 88W of power.\n\nBut when the GPU is running under load, that power usage will spike\nconsiderably, typically to around the power limit.\n\nIn order to stay under the power limit, a piece on the chip called the Voltage\nRegular Module reduces the voltage supplied to the GPU, \u2014 throttling the clock\nfrequency and reducing its performance.\n\nIn other words, if our GPU ends up using enough power to hit the power limit,\nour performance will become capped.\n\nMost of us take it for granted that \u201cGPU does something, power consumption\ngoes up\u201d. But there are actually two distinct mechanisms through which power\ngets consumed.\n\nDynamic/switching power on the left, static/leakage power on the right. Taken\nfrom https://semiengineering.com/knowledge_centers/low-power/low-power-\ndesign/power-consumption/\n\nThe first one is static/leakage power. You can think of this as the power that\ninevitably gets lost by just flowing power through your circuits. The amount\nof static power used is proportional the amount of silicon that is powered. As\nGPUs don\u2019t do much power gating, this is essentially the amount of power used\nat idle (88W in the above photo).\n\nHowever, the second one, dynamic (or switching) power, is the culprit.\nSpecifically, a small amount of power is consumed whenever a transistor\nswitches states. If the transistor never needs to switch states, it doesn\u2019t\nconsume any extra power. On the other hand, if it\u2019s rapidly flipping, then it\nconsumes a ton of dynamic/switching power. Multiply that by the billions of\ntransistors in your GPU, and you get the overall increase in power\nconsumption.\n\nIn other words, the reason why matrix multiplications are faster when passed\nzeros is that this reduces the \u201cflipping\u201d of enough transistors in the chip to\nstay under the power limit!\n\nSo, this (mostly) explains what we saw previously2. All zeros are probably the\nfastest since every single bit of each computation is a zero and the\naccumulator remains at zero. All ones is probably still quite fast since every\nsingle tensor-core instruction results in exactly the same values. The uniform\ndistribution probably is a little bit faster than the normal distribution\nsince the accumulator never needs to flip-flop between positive and negative.\nThe normal distribution probably has the worst performance since it leads to\npretty high randomness among all transistors involved in the computation(?).\n\nHere\u2019s the results on a number of fun distributions I tried:\n\n  1. Randn: Normally distributed\n\n  2. Checkerboard: Normal distribution, but we have zeros in a checkerboard shape.\n\n  3. Rand: Uniform distribution\n\n  4. Sparse: Normal distribution, but a (random) 75% of the elements are masked.\n\n  5. Ternary: Every value is 1, -1, or 0.\n\n  6. One Bit: Only one bit set in every single value (the 4th bit)\n\n  7. All Pies: Every single value is the mathematical constant PI.\n\n  8. Twos: Every value in the matrices are 2\n\n  9. Zeros: Every value in the matrices are 0\n\nWho says unstructured sparsity isn\u2019t efficient with tensor-cores? :)\n\n#\n\nHow power limit and clock speed affects this\n\nHere\u2019s another piece of evidence that dynamic/switching power is responsible.\n\nRoughly, the power we\u2019re using is proportional to the clock speed multiplied\nby amount of transistor flips we\u2019re doing.\n\npower ~= clock speed * \u201ctransistor flips per clock\u201d.\n\nWe run into throttling when the power we use surpasses the power limit we\u2019ve\nset. Thus:\n\n  1. If we reduce our power limit we exacerbate this effect.\n\n  2. If we reduce the clock speed we reduce this effect.\n\nLet\u2019s show that in action! To do so, I\u2019ll compare the relative performance of\na very predictable input (zeros) vs a very unpredictable input (randn).\n\nAs expected, we see that as the power limit decreases from 330W down to 100W\n(the minimum), the relevant performance improvement from using predictable\ninputs increases. Interestingly, at the lowest power limit (100W), the trend\nreverses. I\u2019m guessing that the GPU is so power constrained that even using\nall zeros still results in too much power usage. Remember that the switching\npower is coming from every transistor in the GPU, not just the ones holding\ndata! So that includes the transistors for say, keeping track of the current\nprogram counter, keeping track of how many loop iterations you need to\nperform, the ones signaling other transistors to perform operations, pretty\nmuch everything that a GPU can possibly be doing.\n\nNow, to test the effect of GPU clocks on using predictable vs. unpredictable\ninputs, I\u2019ll use a a power limit of 200 and vary the GPU clock limit.\n\nWe see that at the top range of the clock frequency, there is nearly no change\nin the ratio, as presumably even with predictable inputs, we\u2019re still getting\nthrottled. Then, as we decrease the clock speed, the relative gap shrinks as\npredictable inputs are affected by the clock speed limit, but not\nunpredictable inputs. Finally, at the very left of the chart, both using\npredictable and unpredictable inputs have identical performance, as both\nbecome completely limited by our manual clock speed limit and don\u2019t do any\npower throttling.\n\nAnother interesting thing we can test is, for a given input and power limit,\nwhat is the maximum clock speed the GPU can sustain for a matmul?\n\n#\n\nMarketing vs \u201cReal\u201d Performance\n\nThis observation that GPUs are unable to sustain their peak clock speed due to\npower throttling is one of the primary factors that separates \u201creal\u201d matmul\nperformance from Nvidia\u2019s marketed specs.\n\nThe figure that Nvidia provides for marketing is:\n\nFor example, on an H100, there are 528 tensor cores per GPU (4 per SM), the\nmax clock speed for these is 1.830 Ghz, and the FLOP per tensor-core\ninstruction is 1024. Thus, we have 1.830e9 * 528 * 1024 = 989 TFLOPS, exactly\nNvidia\u2019s listed number.\n\nHowever, you can only achieve this number by sustaining 1.83 Ghz clocks, and\nas we\u2019ve seen above, the GPU just doesn\u2019t have enough power to do that!\n\nDo note that in both of these cases, the GPUs have a higher power limit than I\ncan test (400W on A100 and 700W on H100 respectively), so it\u2019s able to sustain\na higher clock speed than what\u2019s charted here. But we can see that, especially\non the H100, the max sustainable clock speed is much lower than the\ntheoretical one! In other words, matmuls on the H100 are primarily not compute\nor bandwidth limited, they are power limited.\n\nAs many have noted, power is increasingly a crucial constraint. So, although\nthe H100 theoretically had 3x more FLOPS than the A100, its \u201creal\u201d performance\nhas usually been closer to 2x due to the power throttling we\u2019ve been\ndiscussing, and its \u201cflops per watt\u201d is even less than that.\n\n#\n\nConclusion\n\nAll of this should make you exceedingly curious to see the actual performance\nimprovement on the B100, which has a 1.75x theoretical increase in FLOPS with\nthe same power usage as the H100. I have a Manifold market about guessing the\nmax FLOPS utilization on a B100.\n\nI\u2019ll leave you with a slightly modified version of this tweet from roon.\n\nThanks to Sophia Wisdom, Vijay Thakkar, Philippe Tillet, Dylan Patel, and\nNatalia Gimelshein who have helped me understand this phenomenon.\n\nAlso, be aware that you need to set the `scale` parameter on the CUTLASS\nprofiler if you wanna compare its FLOPS numbers to other benchmarks!\n\nThonk From First Principles is a reader-supported publication. To receive new\nposts and support my work, consider becoming a free or paid subscriber.\n\n1\n\nThe A100 GPU I\u2019m testing this on happens to have a power limit of 330W.\n\n2\n\nIt\u2019s hard for me to say for sure, since I can\u2019t count how many times each\nindividual transistor flips :)\n\n23 Likes\n\n\u00b7\n\n2 Restacks\n\n23\n\nShare this post\n\n#### Strangely, Matrix Multiplications on GPUs Run Faster When Given\n\"Predictable\" Data! [short]\n\nwww.thonking.ai\n\n5\n\nShare\n\n5 Comments\n\nDylan PatelSemiAnalysisApr 29\u00b7edited Apr 29Liked by Horace HeBenchmark\nperformance by input type chart for H100, H200, MI300X pls. Make it something\nI gotta pay for lol. It would be cool to see FLOPS/Watt across all those HW\ntoo, cause rn all we see is some random MFU across a whole model which isnt\napples to apple or theoretical peaks.Expand full commentLike (3)ReplyShare  \n---  \n  \n2 replies\n\nAmit24 hrs agoLiked by Horace HeVery nice article, Horace.It\u2019s something many\nof us have long suspected, but this is the first time I am seeing actual\nexperimental results showing that peak flops are really just pie in the sky \u2014\neven if memory bandwidth was infinite.Expand full commentLike (2)ReplyShare  \n---  \n  \n1 reply by Horace He\n\n3 more comments...\n\nWhat Shapes Do Matrix Multiplications Like? [medium]\n\nDivining order from the chaos\n\nApr 1 \u2022\n\nHorace He\n\n20\n\nShare this post\n\n#### What Shapes Do Matrix Multiplications Like? [medium]\n\nwww.thonking.ai\n\nSolutions: What Shapes Do Matrix Multiplications Like?\n\nCompanion to https://www.thonking.ai/p/what-shapes-do-matrix-multiplications\n\nApr 8 \u2022\n\nHorace He\n\n6\n\nShare this post\n\n#### Solutions: What Shapes Do Matrix Multiplications Like?\n\nwww.thonking.ai\n\nSupporting Mixtral in gpt-fast through torch.compile [short]\n\nLong-form version of this tweet thread:\nhttps://twitter.com/cHHillee/status/1762269069351461196\n\nFeb 26 \u2022\n\nHorace He\n\nand\n\nYanbo Liang\n\n9\n\nShare this post\n\n#### Supporting Mixtral in gpt-fast through torch.compile [short]\n\nwww.thonking.ai\n\n2\n\nReady for more?\n\n\u00a9 2024 Horace He\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
