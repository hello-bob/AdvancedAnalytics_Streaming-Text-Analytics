{"aid": "40173630", "title": "Non-Intrusive Load Monitoring Based on Multiscale Attention Mechanisms", "url": "https://www.mdpi.com/1996-1073/17/8/1944", "domain": "mdpi.com", "votes": 2, "user": "PaulHoule", "posted_at": "2024-04-26 20:08:11", "comments": 0, "source_title": "Non-Intrusive Load Monitoring Based on Multiscale Attention Mechanisms", "source_text": "Energies | Free Full-Text | Non-Intrusive Load Monitoring Based on Multiscale Attention Mechanisms\n\nLoading [MathJax]/jax/output/HTML-CSS/jax.js\n\n  * Consent\n  * Details\n  * [#IABV2SETTINGS#]\n  * About\n\n## This website uses cookies\n\nWe use cookies to personalise content and ads, to provide social media\nfeatures and to analyse our traffic. We also share information about your use\nof our site with our social media, advertising and analytics partners who may\ncombine it with other information that you\u2019ve provided to them or that they\u2019ve\ncollected from your use of their services.\n\nShow details\n\n  * Necessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies.\n\n    * Cookiebot\n\n1\n\nLearn more about this provider\n\n1.gifUsed to count the number of sessions to the website, necessary for\noptimizing CMP product delivery.\n\nExpiry: SessionType: Pixel\n\n    * Crazyegg\n\n2\n\nLearn more about this provider\n\n_ce.cchStores the user's cookie consent state for the current domain\n\nExpiry: SessionType: HTTP\n\nce_successful_csp_checkDetects whether user behaviour tracking should be\nactive on the website.\n\nExpiry: PersistentType: HTML\n\n    * Google\n\n1\n\nLearn more about this provider\n\ntest_cookieUsed to check if the user's browser supports cookies.\n\nExpiry: 1 dayType: HTTP\n\n    * LinkedIn\n\n2\n\nLearn more about this provider\n\nli_gcStores the user's cookie consent state for the current domain\n\nExpiry: 180 daysType: HTTP\n\nbscookieThis cookie is used to identify the visitor through an application.\nThis allows the visitor to login to a website through their LinkedIn\napplication for example.\n\nExpiry: 1 yearType: HTTP\n\n    * commenting.mdpi.com\n\n2\n\nSESS#Preserves users states across page requests.\n\nExpiry: SessionType: HTTP\n\nXSRF-TOKENEnsures visitor browsing-security by preventing cross-site request\nforgery. This cookie is essential for the security of the website and visitor.\n\nExpiry: SessionType: HTTP\n\n    * commenting.mdpi.com consent.cookiebot.com\n\n2\n\nCookieConsent [x2]Stores the user's cookie consent state for the current\ndomain\n\nExpiry: 1 yearType: HTTP\n\n    * matomo.mdpi.com\n\n1\n\n_pk_testcookie_domainThis cookie determines whether the browser accepts\ncookies.\n\nExpiry: 1 dayType: HTTP\n\n    * mdpi.com\n\n3\n\n__cfruidThis cookie is a part of the services provided by Cloudflare -\nIncluding load-balancing, deliverance of website content and serving DNS\nconnection for website operators.\n\nExpiry: SessionType: HTTP\n\ncf_clearanceThis cookie is used to distinguish between humans and bots.\n\nExpiry: 1 yearType: HTTP\n\nMDPIPHPSESSIDPending\n\nExpiry: SessionType: HTTP\n\n    * mdpi.com mdpi.org mdpi-res.com sciprofiles.com\n\n4\n\n__cf_bm [x4]This cookie is used to distinguish between humans and bots. This\nis beneficial for the website, in order to make valid reports on the use of\ntheir website.\n\nExpiry: 1 dayType: HTTP\n\n    * www.jisc.ac.uk\n\n2\n\nAWSALBRegisters which server-cluster is serving the visitor. This is used in\ncontext with load balancing, in order to optimize user experience.\n\nExpiry: 7 daysType: HTTP\n\nAWSALBCORSRegisters which server-cluster is serving the visitor. This is used\nin context with load balancing, in order to optimize user experience.\n\nExpiry: 7 daysType: HTTP\n\n    * www.mdpi.com\n\n9\n\ncf_chl_1This cookie is a part of the services provided by Cloudflare -\nIncluding load-balancing, deliverance of website content and serving DNS\nconnection for website operators.\n\nExpiry: 1 dayType: HTTP\n\ncf_chl_rc_mThis cookie is a part of the services provided by Cloudflare -\nIncluding load-balancing, deliverance of website content and serving DNS\nconnection for website operators.\n\nExpiry: 1 dayType: HTTP\n\ncf_use_obUsed to detect if the website is inaccessible, in case of maintenance\nof content updates - The cookie allows the website to present the visitor with\na notice on the issue in question.\n\nExpiry: 1 dayType: HTTP\n\niconify0Used by the website's content management system (CMS) to determine how\nthe website's menu-tabs should be displayed.\n\nExpiry: PersistentType: HTML\n\niconify1This cookie is set to ensure proper product displays on the website.\n\nExpiry: PersistentType: HTML\n\niconify2Used by the website's content management system (CMS) to determine how\nthe website's menu-tabs should be displayed.\n\nExpiry: PersistentType: HTML\n\niconify3Determines the device used to access the website. This allows the\nwebsite to be formatted accordingly.\n\nExpiry: PersistentType: HTML\n\niconify-countUsed by the website's content management system (CMS) to\ndetermine how the website's menu-tabs should be displayed.\n\nExpiry: PersistentType: HTML\n\niconify-versionUsed by the website's content management system (CMS) to\ndetermine how the website's menu-tabs should be displayed.\n\nExpiry: PersistentType: HTML\n\n  * Preference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in.\n\n    * LinkedIn\n\n1\n\nLearn more about this provider\n\nlidcRegisters which server-cluster is serving the visitor. This is used in\ncontext with load balancing, in order to optimize user experience.\n\nExpiry: 1 dayType: HTTP\n\n    * www.mdpi.com\n\n2\n\nmdpi_layout_typeThis cookie is used to store user setting of using fixed\ndesktop layout instead of the default responsive layout\n\nExpiry: 1 yearType: HTTP\n\nsettingsThis cookie is used to determine the preferred language of the visitor\nand sets the language accordingly on the website, if possible.\n\nExpiry: PersistentType: HTML\n\n  * Statistic cookies help website owners to understand how visitors interact with websites by collecting and reporting information anonymously.\n\n    * Crazyegg\n\n8\n\nLearn more about this provider\n\n_ce.clock_dataCollects data on the user\u2019s navigation and behavior on the\nwebsite. This is used to compile statistical reports and heatmaps for the\nwebsite owner.\n\nExpiry: 1 dayType: HTTP\n\n_ce.clock_eventCollects data on the user\u2019s navigation and behavior on the\nwebsite. This is used to compile statistical reports and heatmaps for the\nwebsite owner.\n\nExpiry: 1 dayType: HTTP\n\n_ce.gtldHolds which URL should be presented to the visitor when visiting the\nsite.\n\nExpiry: SessionType: HTTP\n\n_ce.sCollects data on the user\u2019s navigation and behavior on the website. This\nis used to compile statistical reports and heatmaps for the website owner.\n\nExpiry: 1 yearType: HTTP\n\ncebsTracks the individual sessions on the website, allowing the website to\ncompile statistical data from multiple visits. This data can also be used to\ncreate leads for marketing purposes.\n\nExpiry: SessionType: HTTP\n\ncebsp_Collects data on the user\u2019s navigation and behavior on the website. This\nis used to compile statistical reports and heatmaps for the website owner.\n\nExpiry: SessionType: HTTP\n\nce_fvdCollects data on the user\u2019s navigation and behavior on the website. This\nis used to compile statistical reports and heatmaps for the website owner.\n\nExpiry: PersistentType: HTML\n\ncetabidSets a unique ID for the session. This allows the website to obtain\ndata on visitor behaviour for statistical purposes.\n\nExpiry: SessionType: HTML\n\n    * Google\n\n5\n\nLearn more about this provider\n\ncollectUsed to send data to Google Analytics about the visitor's device and\nbehavior. Tracks the visitor across devices and marketing channels.\n\nExpiry: SessionType: Pixel\n\n_gaRegisters a unique ID that is used to generate statistical data on how the\nvisitor uses the website.\n\nExpiry: 2 yearsType: HTTP\n\n_ga_#Used by Google Analytics to collect data on the number of times a user\nhas visited the website as well as dates for the first and most recent visit.\n\nExpiry: 2 yearsType: HTTP\n\n_gatUsed by Google Analytics to throttle request rate\n\nExpiry: 1 dayType: HTTP\n\n_gidRegisters a unique ID that is used to generate statistical data on how the\nvisitor uses the website.\n\nExpiry: 1 dayType: HTTP\n\n    * Hotjar\n\n5\n\nLearn more about this provider\n\nhjActiveViewportIdsThis cookie contains an ID string on the current session.\nThis contains non-personal information on what subpages the visitor enters \u2013\nthis information is used to optimize the visitor's experience.\n\nExpiry: PersistentType: HTML\n\nhjViewportIdSaves the user's screen size in order to adjust the size of images\non the website.\n\nExpiry: SessionType: HTML\n\n_hjSession_#Collects statistics on the visitor's visits to the website, such\nas the number of visits, average time spent on the website and what pages have\nbeen read.\n\nExpiry: 1 dayType: HTTP\n\n_hjSessionUser_#Collects statistics on the visitor's visits to the website,\nsuch as the number of visits, average time spent on the website and what pages\nhave been read.\n\nExpiry: 1 yearType: HTTP\n\n_hjTLDTestRegisters statistical data on users' behaviour on the website. Used\nfor internal analytics by the website operator.\n\nExpiry: SessionType: HTTP\n\n    * LinkedIn\n\n1\n\nLearn more about this provider\n\nAnalyticsSyncHistoryUsed in connection with data-synchronization with third-\nparty analysis service.\n\nExpiry: 30 daysType: HTTP\n\n    * Twitter Inc.\n\n1\n\nLearn more about this provider\n\npersonalization_idThis cookie is set by Twitter - The cookie allows the\nvisitor to share content from the website onto their Twitter profile.\n\nExpiry: 400 daysType: HTTP\n\n    * matomo.mdpi.com\n\n3\n\n_pk_id#Collects statistics on the user's visits to the website, such as the\nnumber of visits, average time spent on the website and what pages have been\nread.\n\nExpiry: 1 yearType: HTTP\n\n_pk_ref#Used by Piwik Analytics Platform to identify the referring website\nfrom which the visitor has come.\n\nExpiry: 6 monthsType: HTTP\n\n_pk_ses#Used by Piwik Analytics Platform to track page requests from the\nvisitor during the session.\n\nExpiry: 1 dayType: HTTP\n\n    * www.mdpi.com\n\n1\n\nsentryReplaySessionRegisters data on visitors' website-behaviour. This is used\nfor internal analysis and website optimization.\n\nExpiry: SessionType: HTML\n\n  * Marketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers.\n\n    * Meta Platforms, Inc.\n\n3\n\nLearn more about this provider\n\nlastExternalReferrerDetects how the user reached the website by registering\ntheir last URL-address.\n\nExpiry: PersistentType: HTML\n\nlastExternalReferrerTimeDetects how the user reached the website by\nregistering their last URL-address.\n\nExpiry: PersistentType: HTML\n\n_fbpUsed by Facebook to deliver a series of advertisement products such as\nreal time bidding from third party advertisers.\n\nExpiry: 3 monthsType: HTTP\n\n    * Google\n\n2\n\nLearn more about this provider\n\npagead/1p-user-list/#Tracks if the user has shown interest in specific\nproducts or events across multiple websites and detects how the user navigates\nbetween sites. This is used for measurement of advertisement efforts and\nfacilitates payment of referral-fees between websites.\n\nExpiry: SessionType: Pixel\n\ntdRegisters statistical data on users' behaviour on the website. Used for\ninternal analytics by the website operator.\n\nExpiry: SessionType: Pixel\n\n    * LinkedIn\n\n4\n\nLearn more about this provider\n\nbcookieUsed by the social networking service, LinkedIn, for tracking the use\nof embedded services.\n\nExpiry: 1 yearType: HTTP\n\nli_sugrCollects data on user behaviour and interaction in order to optimize\nthe website and make advertisement on the website more relevant.\n\nExpiry: 3 monthsType: HTTP\n\nUserMatchHistoryEnsures visitor browsing-security by preventing cross-site\nrequest forgery. This cookie is essential for the security of the website and\nvisitor.\n\nExpiry: 30 daysType: HTTP\n\nli_adsIdCollects data on user behaviour and interaction in order to optimize\nthe website and make advertisement on the website more relevant.\n\nExpiry: PersistentType: HTML\n\n    * Twitter Inc.\n\n3\n\nLearn more about this provider\n\ni/adsct [x2]The cookie is used by Twitter.com in order to determine the number\nof visitors accessing the website through Twitter advertisement content.\n\nExpiry: SessionType: Pixel\n\nmuc_adsCollects data on user behaviour and interaction in order to optimize\nthe website and make advertisement on the website more relevant.\n\nExpiry: 400 daysType: HTTP\n\n    * YouTube\n\n22\n\nLearn more about this provider\n\n#-#Pending\n\nExpiry: SessionType: HTML\n\niU5q-!O9@$Registers a unique ID to keep statistics of what videos from YouTube\nthe user has seen.\n\nExpiry: SessionType: HTML\n\nLAST_RESULT_ENTRY_KEYUsed to track user\u2019s interaction with embedded content.\n\nExpiry: SessionType: HTTP\n\nLogsDatabaseV2:V#||LogsRequestsStorePending\n\nExpiry: PersistentType: IDB\n\nnextIdUsed to track user\u2019s interaction with embedded content.\n\nExpiry: SessionType: HTTP\n\nremote_sidNecessary for the implementation and functionality of YouTube video-\ncontent on the website.\n\nExpiry: SessionType: HTTP\n\nrequestsUsed to track user\u2019s interaction with embedded content.\n\nExpiry: SessionType: HTTP\n\nServiceWorkerLogsDatabase#SWHealthLogNecessary for the implementation and\nfunctionality of YouTube video-content on the website.\n\nExpiry: PersistentType: IDB\n\nTESTCOOKIESENABLEDUsed to track user\u2019s interaction with embedded content.\n\nExpiry: 1 dayType: HTTP\n\nVISITOR_INFO1_LIVETries to estimate the users' bandwidth on pages with\nintegrated YouTube videos.\n\nExpiry: 180 daysType: HTTP\n\nVISITOR_PRIVACY_METADATAStores the user's cookie consent state for the current\ndomain\n\nExpiry: 180 daysType: HTTP\n\nYSCRegisters a unique ID to keep statistics of what videos from YouTube the\nuser has seen.\n\nExpiry: SessionType: HTTP\n\nyt.innertube::nextIdRegisters a unique ID to keep statistics of what videos\nfrom YouTube the user has seen.\n\nExpiry: PersistentType: HTML\n\nytidb::LAST_RESULT_ENTRY_KEYStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: PersistentType: HTML\n\nYtIdbMeta#databasesUsed to track user\u2019s interaction with embedded content.\n\nExpiry: PersistentType: IDB\n\nyt-remote-cast-availableStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-cast-installedStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-connected-devicesStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: PersistentType: HTML\n\nyt-remote-device-idStores the user's video player preferences using embedded\nYouTube video\n\nExpiry: PersistentType: HTML\n\nyt-remote-fast-check-periodStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-session-appStores the user's video player preferences using embedded\nYouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-session-nameStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\n    * cdn.pbgrd.com\n\n2\n\npagead/gen_204Collects data on visitor behaviour from multiple websites, in\norder to present more relevant advertisement - This also allows the website to\nlimit the number of times that they are shown the same advertisement.\n\nExpiry: SessionType: Pixel\n\ncsiCollects data on visitors' preferences and behaviour on the website - This\ninformation is used make content and advertisement more relevant to the\nspecific visitor.\n\nExpiry: SessionType: Pixel\n\n    * pub.mdpi-res.com\n\n1\n\nOAIDRegisters a unique ID that identifies a returning user's device. The ID is\nused for targeted ads.\n\nExpiry: 1 yearType: HTTP\n\n  * Unclassified cookies are cookies that we are in the process of classifying, together with the providers of individual cookies.\n\n    * Crazyegg\n\n1\n\nLearn more about this provider\n\n_ce.irvPending\n\nExpiry: SessionType: HTTP\n\n    * www.mdpi.com\n\n2\n\nmdpi_layout_type_v2Pending\n\nExpiry: 1 yearType: HTTP\n\nsettings_cachedPending\n\nExpiry: SessionType: HTTP\n\nCross-domain consent[#BULK_CONSENT_DOMAINS_COUNT#] [#BULK_CONSENT_TITLE#]\n\nList of domains your consent applies to: [#BULK_CONSENT_DOMAINS#]\n\nCookie declaration last updated on 4/24/24 by Cookiebot\n\n## [#IABV2_TITLE#]\n\n[#IABV2_BODY_INTRO#]\n\n[#IABV2_BODY_LEGITIMATE_INTEREST_INTRO#]\n\n[#IABV2_BODY_PREFERENCE_INTRO#]\n\n[#IABV2_BODY_PURPOSES_INTRO#]\n\n[#IABV2_BODY_PURPOSES#]\n\n[#IABV2_BODY_FEATURES_INTRO#]\n\n[#IABV2_BODY_FEATURES#]\n\n[#IABV2_BODY_PARTNERS_INTRO#]\n\n[#IABV2_BODY_PARTNERS#]\n\nCookies are small text files that can be used by websites to make a user's\nexperience more efficient.\n\nThe law states that we can store cookies on your device if they are strictly\nnecessary for the operation of this site. For all other types of cookies we\nneed your permission.\n\nThis site uses different types of cookies. Some cookies are placed by third\nparty services that appear on our pages.\n\nYou can at any time change or withdraw your consent from the Cookie\nDeclaration on our website.\n\nLearn more about who we are, how you can contact us and how we process\npersonal data in our Privacy Policy.\n\nPlease state your consent ID and date when you contact us regarding your\nconsent.\n\nPowered by Cookiebot by Usercentrics\n\nNext Article in Journal\n\nAssessment of Spent Nuclear Fuel in Ukrainian Storage System: Inventory and\nPerformance\n\nPrevious Article in Journal\n\nCharacteristics of Low-Temperature Gasification Products from Wheat Straw in a\nFluidized Bed Based on Cement Production Process\n\n## Journals\n\nActive Journals Find a Journal Proceedings Series\n\n## Topics\n\n## Information\n\nFor Authors For Reviewers For Editors For Librarians For Publishers For\nSocieties For Conference Organizers\n\nOpen Access Policy Institutional Open Access Program Special Issues Guidelines\nEditorial Process Research and Publication Ethics Article Processing Charges\nAwards Testimonials\n\n## Author Services\n\n## Initiatives\n\nSciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS\nProceedings Series\n\n## About\n\nOverview Contact Careers News Press Blog\n\nSign In / Sign Up\n\n## Notice\n\nclear\n\n## Notice\n\nYou are accessing a machine-readable page. In order to be human-readable,\nplease install an RSS reader.\n\nContinue Cancel\n\nclear\n\nAll articles published by MDPI are made immediately available worldwide under\nan open access license. No special permission is required to reuse all or part\nof the article published by MDPI, including figures and tables. For articles\npublished under an open access Creative Common CC BY license, any part of the\narticle may be reused without permission provided that the original article is\nclearly cited. For more information, please refer to\nhttps://www.mdpi.com/openaccess.\n\nFeature papers represent the most advanced research with significant potential\nfor high impact in the field. A Feature Paper should be a substantial original\nArticle that involves several techniques or approaches, provides an outlook\nfor future research directions and describes possible research applications.\n\nFeature papers are submitted upon individual invitation or recommendation by\nthe scientific editors and must receive positive feedback from the reviewers.\n\nEditor\u2019s Choice articles are based on recommendations by the scientific\neditors of MDPI journals from around the world. Editors select a small number\nof articles recently published in the journal that they believe will be\nparticularly interesting to readers, or important in the respective research\narea. The aim is to provide a snapshot of some of the most exciting work\npublished in the various research areas of the journal.\n\nOriginal Submission Date Received: .\n\n  * Journals\n\n    *       * Active Journals\n      * Find a Journal\n      * Proceedings Series\n\n  * Topics\n  * Information\n\n    *       * For Authors\n      * For Reviewers\n      * For Editors\n      * For Librarians\n      * For Publishers\n      * For Societies\n      * For Conference Organizers\n\n      * Open Access Policy\n      * Institutional Open Access Program\n      * Special Issues Guidelines\n      * Editorial Process\n      * Research and Publication Ethics\n      * Article Processing Charges\n      * Awards\n      * Testimonials\n\n  * Author Services\n  * Initiatives\n\n    *       * Sciforum\n      * MDPI Books\n      * Preprints.org\n      * Scilit\n      * SciProfiles\n      * Encyclopedia\n      * JAMS\n      * Proceedings Series\n\n  * About\n\n    *       * Overview\n      * Contact\n      * Careers\n      * News\n      * Press\n      * Blog\n\nSign In / Sign Up Submit\n\nJournals\n\nEnergies\n\nVolume 17\n\nIssue 8\n\n10.3390/en17081944\n\nSubmit to this Journal Review for this Journal Propose a Special Issue\n\n\u25ba \u25bc Article Menu\n\n## Article Menu\n\n  * Academic Editor\n\nAhmed Abu-Siada\n\n  * Subscribe SciFeed\n  * Recommended Articles\n  * Related Info Link\n\n    * Google Scholar\n\n  * More by Authors Links\n\n    * on DOAJ\n\n      * Yao, L.\n      * Wang, J.\n      * Zhao, C.\n\n    * on Google Scholar\n\n      * Yao, L.\n      * Wang, J.\n      * Zhao, C.\n\n    * on PubMed\n\n      * Yao, L.\n      * Wang, J.\n      * Zhao, C.\n\n/ajax/scifeed/subscribe\n\nArticle Views 391\n\n  * Table of Contents\n\n    * Abstract\n    * Introduction\n    * Load Characterization and Datasets\n    * Smart Home Modeling Using Non-Intrusive Load Decomposition\n    * Algorithm Analysis\n    * Conclusions\n    * Author Contributions\n    * Funding\n    * Data Availability Statement\n    * Conflicts of Interest\n    * References\n\nAltmetric share Share announcement Help format_quote Cite question_answer\nDiscuss in SciProfiles thumb_up\n\n...\n\nEndorse textsms\n\n...\n\nComment\n\n## Need Help?\n\n### Support\n\nFind support for a specific problem in the support section of our website.\n\nGet Support\n\n### Feedback\n\nPlease let us know what you think of our products and services.\n\nGive Feedback\n\n### Information\n\nVisit our dedicated information section to learn more about MDPI.\n\nGet Information\n\nclear\n\n## JSmol Viewer\n\nclear\n\nfirst_page\n\nDownload PDF\n\nsettings\n\nOrder Article Reprints\n\nFont Type:\n\nArial Georgia Verdana\n\nFont Size:\n\nAa Aa Aa\n\nLine Spacing:\n\nColumn Width:\n\nBackground:\n\nOpen AccessArticle\n\n# Non-Intrusive Load Monitoring Based on Multiscale Attention Mechanisms\n\nby\n\nLei Yao\n\nLei Yao\n\nSciProfiles Scilit Preprints.org Google Scholar\n\n^^,\n\nJinhao Wang\n\nJinhao Wang\n\nSciProfiles Scilit Preprints.org Google Scholar\n\n^ *^ and\n\nChen Zhao\n\nChen Zhao\n\nSciProfiles Scilit Preprints.org Google Scholar\n\n^^\n\nDepartment of Electrical Engineering, University of Shanghai for Science and\nTechnology, Shanghai 200093, China\n\n^*\n\nAuthor to whom correspondence should be addressed.\n\nEnergies 2024, 17(8), 1944; https://doi.org/10.3390/en17081944\n\nSubmission received: 19 March 2024 / Revised: 8 April 2024 / Accepted: 17\nApril 2024 / Published: 19 April 2024\n\n(This article belongs to the Topic Advanced Operation, Control, and Planning\nof Intelligent Energy Systems)\n\nDownload keyboard_arrow_down\n\nDownload PDF Download PDF with Cover Download XML Download Epub\n\nBrowse Figures\n\nVersions Notes\n\nArticle Views\n\nCitations -\n\n## Abstract\n\nWith the development of smart grids and new power systems, the combination of\nnon-intrusive load identification technology and smart home technology can\nprovide users with the operating conditions of home appliances and equipment,\nthus reducing home energy loss and improving users\u2019 ability to demand a\nresponse. This paper proposes a non-intrusive load decomposition model with a\nparallel multiscale attention mechanism (PMAM). The model can extract both\nlocal and global feature information and fuse it through a parallel multiscale\nnetwork. This improves the attention mechanism\u2019s ability to capture feature\ninformation over long time periods. To validate the model\u2019s decomposition\nability, we combined the PMAM model with four benchmark models: the Long\nShort-Term Memory (LSTM) recurrent neural network model, the Time Pooling-\nbased Load Disaggregation Model (TPNILM), the Extreme Learning Machine (ELM),\nand the Load Disaggregation Model without Parallel Multi-scalar Attention\nMechanisms (UNPMAM). The model was trained on the publicly available UK-DALE\ndataset and tested. The models\u2019 test results were quantitatively evaluated\nusing a confusion matrix. This involved calculating the F1 score of the load\ndecomposition. A higher F1 score indicates better model decomposition\nperformance. The results indicate that the PMAM model proposed in this paper\nmaintains an F1 score above 0.9 for the decomposition of three types of\nelectrical equipment under the same household user, which is 3% higher than\nthat of the other benchmark models on average. In the cross-household test,\nthe PMAM also demonstrated a better decomposition ability, with the F1 score\nmaintained above 0.85, and the mean absolute error (MAE) decreased by 5.3% on\naverage compared with that of the UNPMAM.\n\nKeywords:\n\nnon-intrusive load monitoring; smart home; parallel multiscale attention\nmechanisms; smart grid; machine learning\n\n## 1\\. Introduction\n\nWith the rapid development of the electrical power industry, the smart grid is\nbeing highly valued in the electrical power industry of various countries,\nwhich adopts intelligent control means to unite all aspects of the traditional\nelectric power grid. Smart grids integrate the three fields of information\ninteraction, power coordination, and business initiatives and, through the\nintegration of these three fields, they achieve intelligence and constitute a\nmodern power grid with good interaction measures. In the context of \u201ccarbon\nneutral, carbon peak\u201d energy requirements, in order to achieve a carbon peak\nand carbon neutrality, countries are accelerating the intelligent\ntransformation of the power grid in order to build a new power system with new\nenergy as the theme [1]. In recent years, more and more scholars have devoted\ntheir attention to the study of smart grids and new power systems, aiming to\nachieve more efficient and beneficial energy management initiatives through\nintelligent facilities and solutions [2,3]. Each household user, as both a\nconsumer and a demander of energy, constitutes an important part of the total\nenergy consumption of the country [4]. The composition of the smart grid is\nshown in Figure 1 below.\n\nFigure 1. Smart grid architecture.\n\nIn addition, with the access to distributed energy resources and the\nincreasing installed capacity of new renewable energy sources, such as\nphotovoltaic power generation, distribution grids require faster and more\naccurate demand-side response capabilities [5]. For each power plant and\nenergy company that provides energy, user-side energy management is an\nimportant part of building a new power system under the smart grid. If the\nuser side can obtain real-time information about the energy consumed by\ndifferent electrical devices in the home environment, i.e., the amount of\nelectricity consumed by each electrical device and the current price of\nelectricity, it can greatly improve each home user\u2019s awareness about saving\nenergy and electricity consumption, so that the user can join the optimization\nstrategy of the energy demand-side response on their own initiative and, thus,\nachieve the purpose of reducing the non-essential energy expenditure of the\nhome and reducing the non-essential energy consumption [6,7]. In the\nliterature [8,9], some scholars have proposed that, if each household user\u2019s\nelectricity consumption and tariff information is provided to them in a timely\nmanner, the household energy consumption can be greatly reduced (by about\n5\u201320%). And, according to the benefits of the power plants and energy\ncompanies themselves, if we can grasp the electrical energy consumption of\neach household in real time and analyze the electricity consumption of each\nuser, we can improve the accuracy and reliability of the established grid load\nmodel and, at the same time, greatly improve the security of the grid\u2019s\noperation and the economy of the project planning [10]. At present, real-time\nload detection is one of the most effective means of enhancing home users\u2019\nunderstanding of the operating status of household electrical equipment and\nenergy consumption.\n\nReal-time load detection can be classified as intrusive or non-intrusive\ndepending on the method of detection. Intrusive load detection involves adding\nan information collection device in front of each household electrical device\nto collect the switching status, operating voltage, current conditions, power\nfactor, and other electrical characteristics. This method of load detection\ncan accurately collect the electrical data on the equipment and provide an\nunderstanding of its operating status. In practical applications, users have a\nvariety of electrical equipment. Intrusive load detection methods require the\ninstallation of multiple information collection devices, which significantly\nincreases the user\u2019s costs. Additionally, maintaining these collection devices\nposes a major challenge. In contrast to intrusive load monitoring, the non-\nintrusive load monitoring (NILM) method requires only the connection of an\ninformation collection device to the total power line of the household\u2019s\nelectrical equipment. By analyzing the electrical data on the bus and\ncombining them with the unique electrical characteristics of different pieces\nof electrical equipment, NILM can break down the information on the power\nconsumption of each device. Figure 2 below illustrates the physical\narchitecture of both intrusive and non-intrusive load detection.\n\nFigure 2. Physical architecture of intrusive load detection and non-intrusive\nload detection.\n\nNon-intrusive load monitoring (NILM) disaggregates the data from the power\nbus, and its main tasks can be divided into detecting the operating state of\neach electrical device and decomposing the electrical energy consumed by the\nelectrical device. In 1992, Professor Hart from the United States proposed the\nconcept of NILM [11,12], and he proposed a method for solving the power\ndecomposition problem using a finite state machine, which establishes a two-\ndimensional coordinate system of P\u2013Q based on the changes in the active and\nreactive power of loads for cluster identification [13]. Although the\nidentification method proposed by Prof. Hart is less accurate in identifying\nloads, more and more scholars began to study NILM based on it, giving rise to\nthe classical models of load decomposition algorithms such as the Hidden\nMarkov Model (HMM) and the Factorial Hidden Markov Model (FHMM) [14,15,16,17].\nAs they neglect the different electrical devices and their operating states,\nthe load decomposition results of the models are affected when dealing with\nenergy data with multiple operating states, leading to an increase in the\ncommonly used evaluation metric of the mean absolute error (MAE) [18]. An\nadaptive NILM algorithm for constructing a feature library of V-I trajectories\nbased on the voltage and current parameters of the load has been proposed in\n[19]. The transient characteristics of electrical equipment are used in [20]\nto match the power curve changes generated by the equipment at the instant the\nestablished feature library of household load data is switched. The NILM\ndecomposition algorithm that combines the sequence-to-sequence (Seq2Seq) model\nwith the attention mechanism proposed in [21] greatly improves the deep mining\nof the load power sequence data features.\n\nAs more and more scholars conduct research on NILM algorithms, more and more\ntypes of NILM algorithms are developed. According to the different technical\nsolutions adopted by NILM algorithms, NILM algorithms are divided into two\nmain categories based on event detection and non-event detection. The main\ndifference between the two is that the former uses the input and output events\nof electrical equipment to classify the events and thus achieve load\nidentification, while the latter uses the sequence features or other features\nof the entire power line of the electrical equipment and directly predicts the\nelectrical equipment to be identified or speculates on the combination of\nelectrical equipment that may be activated at that time to achieve load\ndecomposition.\n\nIn summary, in order to achieve the accurate identification of household\nelectrical appliances and provide users with detailed information about power\nloss, we adopted a load decomposition model with a parallel multiscale\nattention mechanism (PMAM), which solves the problem of traditional attention\nmechanisms being too focused on a single representation and too difficult to\nuse to characterize long sequences. The PMAM model can also capture both\nglobal and local contextual information on the input sequences. In order to\nverify the load performance of the PMAM model, we trained and tested the model\non the publicly available UK-DALE dataset. The decomposition results were\nevaluated quantitatively using the confusion matrix and mean absolute error\n(MAE). A smaller MAE indicates that the error between the model score and the\nactual score is smaller. From the confusion matrix, advanced model evaluation\nindexes, such as the accuracy, recall, and F1 score, can be obtained. The F1\nscore is commonly used to measure the accuracy of the detection of the working\nstate of electrical equipment, taking into account both the accuracy and\nrecall of the model. The higher the F1 score, the better the model\u2019s\ndecomposition performance. The research contributions of this paper are as\nfollows:\n\n  * We discuss the combination of smart home technology and non-intrusive load monitoring (NILM) technology under the development of smart grids and new power systems in order to provide users with a clear picture of the operating status of household electrical appliances based on NILM, reduce household energy loss, and save money.\n\n  * We propose a non-intrusive load disaggregation model with a parallel multiscale attention mechanism that has a high degree of readiness for load disaggregation and was validated on the publicly available UK-DALE dataset. First, the feature extraction network was formed using a four-layer, one-dimensional convolutional block and a pooling layer. This improved the model\u2019s global perception of sequence data through the convolution of multiple layers. Subsequently, the feature extraction network was enhanced with a parallel multiscale attention mechanism and a parallel feature fusion network in order to extract deep load features. The PMAM model\u2019s decomposition performance was validated using the publicly available UK-DALE dataset.\n\n  * In the case of load detection using the same dataset, other machine learning algorithms, such as the Long Short-Term Memory (LSTM) recurrent neural network, Time Pooling-Based Load Disaggregation Model (TPNILM), Extreme Learning Machine (ELM), and Load Disaggregation Model without Parallel Multi-scale Attention Mechanisms (UNPMAM), were applied, and the results were compared and analyzed. To verify the generalizability of the PMAM model, we trained the model using two different household users and compared its decomposition performance across households.\n\nThe paper is structured as follows. Section 2 outlines the steady state and\ntransient characteristics of load profiles as well as the publicly available\ndatasets. Section 3 explains the parallel multiscale attention mechanism used\nin this paper. Section 4 presents the analytical validation of the model on\npublicly available data using arithmetic cases. Finally, Section 5 provides\nthe conclusions of this paper and suggests possible future research\ndirections.\n\n## 2\\. Load Characterization and Datasets\n\n#### 2.1. Load Characteristics\n\nIn non-intrusive load monitoring (NILM), the detection device is connected to\nthe power input bus of the household\u2019s electrical equipment and detects the\npower information of the power input bus. The total power can be expressed as\nfollows:\n\nwhere denotes the total power of the input bus at time , denotes the power of\nelectrical appliance in the house at time , and denotes the noise interference\nat the time of sampling. Since each electrical device does not work in real\ntime and there are interruptions in the working time, the working state of the\ndevice can be set as a variable (). The value of this variable takes 1 to\nindicate that the device is in the input state and vice versa for the device\nremoval state. The total power can be expressed as follows:\n\nAs shown in Figure 3 below, for a user within 7 days of the total power\nconsumed by household electrical appliances and the power consumed by each\nelectrical appliance individually, the total power is superimposed by the\npower consumed by each individual electrical appliance. The essence of the\nNILM algorithm is actually based on the identification of different loads\nduring steady state operation or the switching operating state when the\ndifferent electrical features can be classified into steady state features and\ntransient features, which are also divided into non-traditional features in\n[15,22]. The commonly used load feature library classification and feature\nextraction methods are shown in Table 1 below.\n\nFigure 3. Power curve.\n\nTable 1. Commonly used load feature library classification and feature\nextraction methods.\n\nCompared with transient features, steady state features are the most commonly\nused feature variables in NILM. This is due to the fact that most transient\nfeatures occur at the moment the load state changes and during load switching,\nwhich requires a high-frequency sampling device to capture the state at the\nmoment the transient feature occurs. Although high-frequency sampling devices\ncan capture a range of load information, they are expensive, have a large\namount of data, and require the use of high-performance computing equipment,\nwhich results in increased user costs and a long computing time. Therefore, it\nis more appropriate to set up a load information acquisition device in the\nhome environment to extract the steady state characteristics of the load.\n\n#### 2.1.1. Homeostatic Characteristics\n\nThe characteristics of the current, active power, and reactive power in\nsteady-state operation are different for different electrical appliances.\nFigure 4 below shows the current, active power, and reactive power waveforms\nof a laptop computer, a water dispenser, a microwave oven, and a laser printer\nin a user\u2019s home during steady-state operation.\n\nFigure 4. (a) Laptop computer; (b) Water dispenser; (c) Microwave oven; (d)\nLaser printer. Current, active power, and reactive power waveforms during\nsteady-state operation.\n\nFrom the four sets of waveforms shown in Figure 4, it can be seen intuitively\nthat different loads have their own unique current and active/reactive power\ncharacteristics. The current characteristics can be symbolized by the mean ,\nthe root-mean-square , and the maximum values of the current , which are\nquantified by the following equations:\n\nwhere is the duration of the state.\n\nWith the broad application of deep learning algorithms in the field of image\nprocessing in recent years, ref. [23] proposed that the V-I trajectory\ncharacteristics of the load, in combination with a convolutional neural\nnetwork, be used to weight-pixelize the V-I trajectory map as the input of the\nnetwork. In addition, in order to extract more load information from the V-I\ntrajectories, ref. [24] color-coded the V-I trajectory images to maximize the\nclassification ability of the images and transform the V-I trajectories into\nvisual features. The authors of [25] proposed a machine learning algorithm\nusing semi-supervised learning to solve the problem of the inability of a load\noperating state that has diverse V-I trajectories to correctly identify the\ncorresponding load. In this paper, we used the information on steady-state\nload collected by a home user\u2019s acquisition device to extract the voltage and\ncurrent data over one cycle corresponding to four types of electrical\nequipment in order to plot the V-I trajectory.\n\nWhen plotting the V-I curve, in order to avoid the influence of the\ndifferences in the amplitude of the voltage and current during the steady-\nstate operation of different pieces of electrical equipment, we unified the\nvoltage and current data for data preprocessing. The two parameters were\nnormalized to values between 0 and 1 to reduce the influence of the different\nmagnitudes of the two parameters. The normalized formula is as follows:\n\nAfter processing the voltage\u2013current data using Equation (6), we plotted the\nV-I curve, which is shown in Figure 5 below.\n\nFigure 5. V-I curves: (a) Laptop computer; (b) Water dispenser; (c) Microwave\noven; (d) Laser printer.\n\nFrom the V-I trajectories of the four electrical appliances shown in Figure 5\nabove, it can be seen that different electrical appliances have different V-I\ntrajectory characteristics, and each V-I trajectory map contains different\ncharacteristic information for the identification of electrical appliances.\nWhen using V-I trajectories for NILM, the main concern is the information on\nstress, such as the curvature, total area, asymmetry, and slope of the V-I\ntrajectories. The formula for the slope of the curve is as follows\n\nwhere denotes the voltage span, denotes the current span, denotes device , and\ndenotes the state circumference of the device.\n\n#### 2.1.2. Transient Characteristics\n\nDuring the transition of electrical equipment from one state to another, such\nas from startup to stable operation or from stable operation to shutdown,\nthere is a short period of switching known as the transient characteristics.\nThese characteristics require a high sampling frequency for the sampling\ndevice due to the quick nature of the transient process. The NILM algorithm\naims to improve the load identification accuracy by considering both steady\nstate and transient load characteristics. For instance, ref. [26] suggests\nthat the load data be pre-screened using the time and power jump values of the\ntransient process between two state jumps. This method uses a support vector\nmachine (SVM) to identify the load. The commonly used transient features\ninclude the transient transition time, inrush power multiplier, and\nactive/reactive power jump values. These are shown in Figure 6 below, which\nillustrates the transition process of electrical equipment from startup to\nsteady state.\n\nFigure 6. Examples of transient characteristics.\n\nThe time required for an electrical device to transition from its current\nstate to the next state is represented by the transient transition time . The\nformula for this is expressed as follows:\n\nThe active power jump variable represents the difference between the active\npower of the load in the previous steady state and the active power when it\nreaches the new steady state after passing through the transient process,\nwhich is expressed by the following formula:\n\nThe reactive power jump variable represents the difference between the\nreactive power of the load at the previous steady state and the reactive power\nwhen the load reaches the new steady state after passing through the transient\nprocess, which is expressed by the following formula:\n\nThe impact power multiplier indicates the relationship between the peak value\nreached by the power of the electrical equipment in the transient process and\nthe power when it reaches the next steady state, which is expressed by the\nfollowing formula:\n\nThe difference between the peak active power in the transient state and the\nactive power after the transient state is denoted . The difference between the\npeak reactive power and the reactive power after the transient state is\ndenoted . The formula is expressed as follows:\n\n#### 2.2. Dataset\n\nRegardless of whether the steady state or transient characteristics of loads\nare chosen, the implementation of NILM must rely on household load data\nacquired in real scenarios. In recent years, with the growing interest in load\nidentification, there has been a gradual increase in NILM-related datasets,\nand there are already many high-quality datasets available for researchers in\nthis field [27,28]. We introduce several commonly used datasets below.\n\nThe UK-DALE dataset [29], provided by Jack Kelly in the UK, offers information\non the electrical energy consumption of all electrical equipment in the homes\nof five householders over a period of time. The data were recorded every 6 s\nby a collection device for each household. To provide data with richer high-\nfrequency load information, the voltage and current of the entire house were\nrecorded at 16 kHz for three of the five households. Additionally, the dataset\nincludes energy consumption data for one home over a period of 655 days. The\nREDD dataset, provided by Kolter and Johnson in the U.S. [30], offers both\nhigh-frequency and low-frequency data. The low-frequency data contain\ninformation on six households. The collection device recorded energy\nconsumption data for each electrical device in each household, as well as the\noverall energy consumption data, at a sampling rate of 1 Hz. The high-\nfrequency data were sampled at 15 kHz, and load energy data were collected for\ntwo households. Anderson et al. [31] provided the BLUED dataset, which\nincludes the electricity usage and state transition information on each\nelectrical device in a U.S. household for one week at a sampling frequency of\n12 kHz.\n\nThese datasets are commonly used to validate NILM algorithms. The UK-DALE\ndataset is preferred due to its rich information, the appropriate amount of\nuser data captured, and the length of time compared with the other two\ndatasets. In this study, we used the UK-DALE data to divide the training and\nvalidation samples. We validated the non-intrusive load decomposition model\nwith the parallel multiscale attention mechanism proposed in this paper. In\norder to facilitate subsequent cross-household experiments with the model, we\nselected electrical devices owned by both Household User 1 and Household User\n2 in the UK-DALE dataset. In this study, washing machines, dishwashers, and\nrefrigerators were selected as target loads. The total power and the power\ncurves of the three types of electrical appliances for Household User 1 and\nHousehold User 2 in one day are shown in Figure 7 below.\n\nFigure 7. (a) Household User 1; (b) Household User 2. Total power and power\ncurves for three types of electrical appliances.\n\nAs can be seen in Figure 7, the power curves of the refrigerator and\ndishwasher are simpler, with only two states (on and off) present during the\noperation of the appliance. However, the power curve of the washing machine\n[32] has multiple load fluctuation patterns, meaning there are switching\noperating states. For Household User 1, the maximum power of the refrigerator\nis 253.78 W, the maximum power of the dishwasher is 2419.01 W, and the maximum\npower of the washing machine is 2004.73 W. For Household User 2, the maximum\npower of the refrigerator is 102.58 W, the maximum power of the dishwasher is\n2041.4 W, and the maximum power of the washing machine is 2242.19 W.\n\n## 3\\. Smart Home Modeling Using Non-Intrusive Load Decomposition\n\n#### 3.1. Model Structure\n\nThe smart home model based on non-intrusive recognition designed in this study\nconsists of a decision control part, an execution control part, and an\ninformation perception part. The overall model framework is shown in Figure 8.\n\nFigure 8. A framework for smart home modeling based on non-intrusive\nrecognition.\n\nThe execution control part contains most of the electrical devices used within\nthe home, and the user can expand the entire system by increasing the size of\nthe lower computer module. The upper computer module communicates with each\nlower computer module using Zigbee technology in order to reduce the layout\ndifficulties caused by the line connection. The information perception part\nconsists of two subparts: the traditional collection module and the\ninformation collection device, which is dedicated to non-invasive load\nrecognition. The model monitors the environmental conditions in the home in\nreal time by means of traditional collection modules, such as temperature\ndetection sensors, humidity detection sensors, and smoke concentration\ndetection sensors, and the monitored data are summarized and sent to the\ndecision-making control part. The non-intrusive load recognition model adopts\na special voltage and current collection device, with a set sampling frequency\nof 1/6 Hz, and puts the data collected on power consumption into the parallel\nmultiscale information collection device proposed in this paper. In the\nproposed load decomposition model with the parallel multiscale attention\nmechanism, the energy consumption data used by the corresponding electrical\nequipment over a period of time are extracted. The decision control part sends\ncontrol commands to the executive control part using the data transmitted from\nthe information perception part. It then summarizes the data and provides to\nthe user through the visualization interface specific information on the home\nenvironment and electrical equipment.\n\n#### 3.2. A Non-Intrusive Load Decomposition Model Based on a Parallel\nMultiscale Attention Mechanism\n\nNon-intrusive load identification is a typical time series problem that aims\nto decompose the energy consumed by individual electrical devices from the\ntotal power consumption. Equation (1) shows that the total power consumption\nof household loads at a certain moment is not only determined by the power of\nall operating devices at that moment but also by the operating status of the\ndevices at that moment. Commonly used methods for solving time series problems\ninclude the sequence-to-sequence model (Seq2Seq), the Long Short-Term Memory\n(LSTM) recurrent neural network, and the convolutional neural network (CNN).\nIn this study, we used a sequence-to-point (Seq2Point) learning model, which\nhas shown a greater ability to decompose the load compared with the Seq2Seq\nmodel, while also requiring less computation time [33]. In this paper, we\npresent a non-intrusive load decomposition model that incorporates a parallel\nmultiscale attention mechanism (PMAM) based on the existing model.\n\n#### 3.2.1. Sequence-to-Point Learning Model\n\nThe sequence-to-point (Seq2Point) learning model maps the input time series\ndata to the output sequence. Unlike the Seq2Seq model, the output of the\nSeq2Point model becomes a single prediction for each midpoint element of the\ntarget device, rather than an average of the predicted data across the entire\noutput window, which helps to reduce the amount of time required to train the\nmodel and produces more accurate predictions.\n\nAfter the raw total power data are input into the Seq2Point model, after\nencoding the raw data, the model maps the input total power data in the\nsliding window fragments to the corresponding output window at the midpoint .\nThe mapping relation function can be expressed by Equation (14):\n\nwhere denotes a sliding time window of length starting from and denotes random\nGaussian noise. In order to make the predicted value of the model close to the\ntrue value, the loss function of the model can be defined as:\n\nwhere is the parameter of the training network.\n\nIn the learning model of the sequence problem, in order to ensure that the\nacquired input sequence features contain rich feature information rather than\nfocus too much on a single representation, we adopted a parallel multiscale\nattention mechanism to simultaneously learn the contextual representation\ninformation at different scales and combined the self-attention mechanism with\na convolution to learn the sequence at different scales.\n\n#### 3.2.2. Parallel Multiscale Attention Mechanism\n\nThe self-attention mechanism has a huge advantage when using local\ninformation. When faced with information containing multiple features, it can\nselectively focus on a certain feature while ignoring the other features, and\nit can simulate very long dependency relationships. In order to obtain both\nlocal and global feature information, we adopted a parallel multi-channel\nattention mechanism, the structure of which is shown in Figure 9.\n\nFigure 9. A non-intrusive decomposition model based on a parallel multi-\nchannel attention mechanism.\n\nAs the collected load information is characterized by high dimensionality,\nredundant information, and a large amount of information, we pooled the data\nin order to simplify the complexity of the network. Pooling not only expands\nthe perception of information but also has invariance. The characteristics of\nthe data, such as translation, rotation, and scale, do not change after\npooling, which both speeds up the calculation and prevents overfitting.\nMaximum pooling is one of the most-adopted pooling methods and is able to\nselect the largest value in each pooling window as the output in order to\nachieve feature reduction and extraction. In performing non-invasive load\ndecomposition, in addition to focusing on the most significant features, it is\nalso necessary to focus on the overall data features. For this reason, we\nadopted both maximum pooling and average pooling. The most significant data\nfeatures were obtained through maximum pooling, and the overall data features\nwere obtained through average pooling. The two datasets were fused into the\ninput of the next level of the parallel multiscale attention mechanism.\n\nThe parallel multiscale attention mechanism is the key part of the non-\ninvasive decomposition model, which has three main parts: the self-attention\nmechanism that can capture the global feature information, the depth-separable\nconvolution that acquires the local feature information, and the position\nfeed-forward network that captures the labeled features [34]. The relationship\nbetween the output of the model and the output can be expressed as follows:\n\nwhere denotes the sequence data input into the layer, denotes the self-\nattention mechanism function, denotes the depth-separable convolution\nfunction, and denotes the position feed-forward network function.\n\nIn the self-attentive mechanism function, the input sequence , is first\nlinearly transformed to obtain the desired Query (), Key (), and Value ()\nvariables. The linear transformation is shown in Equation (17):\n\nSecondly, the attention score is obtained after calculating the similarity of\nand . The obtained attention score is normalized to a probability distribution\nusing the function. Finally, the final attention value is obtained by a\nweighted summation with based on the normalized result. The calculation\nformula is shown in Equation (18):\n\nwhere denotes the variance in the normalized probability distribution. Since\nthe distribution of the probability distribution obtained by the function is\nrelated to the variance , the probability distribution obtained by the\nfunction needs to be decoupled from the variance during the computation to\nensure that the gradient value of the model remains stable during the training\nprocess.\n\nIn contrast to the conventional convolution operation, the depth-separable\nconvolution used in this paper employs a convolution kernel that is\nresponsible for only one channel, rather than multiple channels. As a result,\nthe number of convolution kernels and the number of channels in the previous\nlayer are the same, and each channel is calculated independently of the\nothers. We utilized a position feed-forward network in conjunction with the\ndepth-separable convolution to provide richer feature information for the\nmodel. This approach addresses the issue of the model\u2019s inability to fully\nexplore the feature information at the same spatial location.\n\n#### 3.2.3. Feature Extraction Networks\n\nThe feature extraction network aims to filter the input long-term sequence\ndata and remove redundant information to obtain the important features of the\nsequence data. The PMAM decomposition model\u2019s input layer employs four sets of\none-dimensional convolutional blocks and pooling layers to form the feature\nextraction network. The one-dimensional convolutional block has four\ncomponents: a one-dimensional convolution (Conv1D), a RELU activation\nfunction, batch normalization (BN), and Dropout. Figure 10 provides a visual\nrepresentation.\n\nFigure 10. Structure of the feature extraction network.\n\nIn the feature extraction network, the local features of sequence data are\nextracted by multiple 1D convolutional blocks, and, as the layers of 1D\nconvolutions are further stacked, more and more feature information can be\ncaptured, which improves the ability of the model to globally sense the input\nlong-term sequence data [35]. The decomposition performance of the PMAM model\nis also affected by the hyperparameters, such as the number of convolution\nkernels in the 1D convolutional block, the convolution kernel\u2019s dimensions,\nand the activation function settings. The model\u2019s decomposition effect varies\ndepending on the hyperparameter settings [36]. In this study, the PMAM model\nused four groups of 1D convolutional blocks for feature extraction. Each group\nhad a different number of convolutional kernels (32, 61, 128, and 320).\nAdditionally, the number of feature channels in each group of convolutional\nblocks was increased from 1 to 128.\n\nThe one-dimensional convolutional block uses the Rectified Linear Unit (RELU)\nas its activation function to improve the nonlinear relationship between the\nlayers in the network. The RELU activation function is effective in solving\nthe problem of gradient vanishing in non-negative intervals due to its\nconstant gradient. Additionally, it sparsifies the network by resulting in\nsome outputs being 0. This allows the model to better identify relevant\nfeatures and fit the training data.\n\n#### 3.2.4. Parallel Multiscale Feature Fusion\n\nIn a fixed deep learning network architecture, the network\u2019s learning scale is\ntypically unchangeable and challenging to adjust to the task objectives. This\nlimitation restricts the model to only obtaining information on a fixed scale\nduring the learning process. To enable the model to learn multiscale\ninformation, we propose a multiscale network architecture.\n\nTwo frequently used multiscale network structures are serial networks and\nparallel multibranch networks [37]. Serial networks use layer-hopping\nconnections to combine different scale features, while parallel multibranch\nnetworks use various convolutional kernels with different sizes, null\nconvolutions, and pooling to obtain feature information on different scales\nwithin the same layer. Finally, they pass the feature fusion on to the next\nlayer. In comparison with serial networks, parallel networks can significantly\nenhance the model\u2019s computational ability and reduce the required computing\ntime. Therefore, we employed a parallel multiscale network in the temporary\npooling layer of the PMAM model to merge the load features at seven different\nscales, as illustrated in Figure 11 below.\n\nFigure 11. Parallel multiscale network structure.\n\nIn a parallel multiscale network, each pooling block has five components: an\naverage pooling layer, a 1D convolution, the RELU activation function, a BN\nlayer, and Dropout. The input data for feature information in the parallel\nmultiscale network are transformed into load features of varying sizes through\nthe average pooling layer in different pooling blocks. Subsequently, the size\nof the output load feature is made consistent by using 1D convolution. The\nsize of the convolution kernel used for 1D convolution is 64. The feature\ninformation from the seven sets of pooling blocks is combined with the feature\ninformation inputted into the parallel multiscale network to obtain the final\nload feature. The seven pooled blocks are then fused with the input data of\nthe parallel multiscale network to obtain the final load features.\n\n#### 3.3. Model Parameter Selection\n\nIn the model parameter settings, we set the number of data samples (Batchsize)\nused in a single pass to train the model to 1470, and we set the number of\ntimes the model was trained to 100. As overfitting may occur during the model\ntraining process, we used the optimizer to manage and update the values of the\nlearnable parameters in the model, which is conducive to a closer match\nbetween the training results of the model and the real values. Figure 12 shows\nthe results of the loss values of the model training by the different\noptimizers used in this study.\n\nFigure 12. Model training loss values under different optimizers.\n\nAs can be seen in Figure 12, the Adam optimizer is able to converge quickly\nand stabilize the loss values of the model compared with the other optimizers\nwhen the model is trained. For example, the Adadelta optimizer does not\nconverge even after being trained 100 times, and the convergence performance\nis not good. Therefore, in this study, the Adam optimizer was used to train\nthe model. In order to verify the convergence performance of the Adam\noptimizer on the model at different learning rates, we tested the loss value\nof the model during training at learning rates of 0.0001, 0.001, 0.005, and\n0.01. The test results are shown in Figure 13 below, according to which it can\nbe found that the model is better trained at a learning rate of 0.005.\n\nFigure 13. Model training loss values at different learning rates.\n\n## 4\\. Algorithm Analysis\n\n#### 4.1. Hardware and Software Platform\n\nThis study\u2019s hardware environment consisted of a 64-bit computer with an\nIntel(R) Core(TM) i7-11800H @ 2.30 GHz processor and 16.0 GB of RAM. The\nsoftware platform used was the Windows 10 Professional operating system,\nPython 3.7.4, and the Tensorflow-gpu version 2.1 deep learning framework. The\nmodel structure shown in Figure 8 was used to train the UK-DALE data.\n\n#### 4.2. Load Decomposition Evaluation Metrics\n\nTo assess the performance of the training model, we used several evaluation\nmetrics, including the F1 score, precision rate, recall rate, accuracy rate,\nMatthews correlation coefficient (MCC), and mean absolute error (MAE). Among\nthem, the confusion matrix is the most commonly used performance evaluation\nmatrix in the field of machine learning, and we calculated the required\nevaluation indexes, such as the F1 score, precision rate, and recall rate,\naccording to the results of the confusion matrix. The structure of the\nconfusion matrix is shown in Figure 14.\n\nFigure 14. Confusion matrix structure.\n\nIn the confusion matrix, means that the result of the model decomposition is\nconsistent with the actual working state of the electrical equipment, Positive\nmeans that the electrical equipment is in the working state, and means that\nthe electrical equipment is in the non-working state. The formulas for\nPrecision, Accuracy, Recall, F1 score, and MCC are as follows:\n\nMAE denotes the mean absolute value of the difference between the model\u2019s\npredicted value and the true value over a period of time and is expressed in\nthe following equation:\n\n#### 4.3. Experimental Results and Analysis\n\nIn conducting experiments on the PMAM load decomposition model, the\nexperimental process was divided into data collection, hyperparameter setting,\nmodel training, and model evaluation. For this study, we selected the total\npower data on Household User 1 and the electrical data on each electrical\ndevice from the publicly available UK-DALE dataset, covering the period from\nApril 2013 to December 2014. The collected data have been preprocessed. The\nmodel was trained using the preprocessed data and the set hyperparameters.\nSubsequently, the load decomposition effect of the PMAM model was verified\nusing each load decomposition evaluation index proposed in Section 4.2. The\nexperimental flow is illustrated in Figure 15.\n\nFigure 15. Experimental process.\n\nIn this study, the data on dishwasher and washing machine loads and the\ncorresponding operating states in the UK-DALE data were selected. After we\napplied the PMAM load decomposition model proposed in this paper, the\ndecomposition results of their operating states were compared with the real\noperating states of the electrical equipment as shown in Figure 16 below.\n\nFigure 16. (a) dishwashers; (b) washing machine. Work status prediction\nresults.\n\nFrom the work state prediction results shown in Figure 16, it can be seen that\nthe model has a good load decomposition ability and can more accurately\nachieve the extraction of the target load work state from the bus data\ncontaining the work state of multiple devices. In order to better validate the\nperformance of this model, we used the electrical appliance data on Household\nUser 1 from the same UK-DALE dataset to train the Long Short-Term Memory\n(LSTM) recurrent neural network model, the Time Pooling-Based Load\nDisaggregation Model (TPNILM) [38], the Extreme Learning Machine (ELM) [39],\nand the same network structure as in this paper, but without the parallel\nmulti-scalar attention mechanism. The training set for Household User 1\nconsisted of data from a specific period of time, while the data used to train\nthe model were taken from a different period than that of the test. After the\nmodel training was completed, we obtained the performance evaluation index of\neach model, which is shown in Table 2 below.\n\nTable 2. Comparison of the load-splitting performance evaluation metrics for\nHousehold User 1.\n\nAs can be seen from Table 2, the PMAM model proposed in this paper outperforms\nthe other four decomposition models in most of the load decomposition\nperformance metrics. The F1 scores of the PMAM model for decomposing the three\ntypes of loads, namely dishwasher, washing machine, and refrigerator loads,\nremain above 0.9 and are better than those of the other load decomposition\nmodels. The F1 scores were also improved by an average of 8.5 percent compared\nwith those of the ELM model, which showed a good ability to decompose the\nloads. The PMAM model is optimal in the recall evaluation index for all types\nof electrical equipment except for dishwashers, where its performance is\nslightly lower than that of the ELM model. The PMAM model also improves the\nrecall by 3% compared with the TPNILM model. Although the ELM model performs\nthe best in the MAE decomposition performance index, the PMAM model proposed\nin this paper outperforms the other models in terms of overall decomposition\nperformance. Under the same network, compared with the Load Disaggregation\nModel without Parallel Multi-scalar Attention Mechanisms (UNPMAM), the average\nabsolute errors of both appliances were reduced, and they were also slightly\nbetter than the UNPMAM in terms of the F1 score.\n\nCompared with the F1 evaluation metrics of the UNPMAM in the washing machine,\nthe F1 metrics of the PMAM model proposed in this paper are significantly\nbetter than those of the UNPMAM in both the dishwasher and the refrigerator.\nThe power of the washing machine fluctuates greatly during operation and its\nworking state transitions frequently, as shown in Figure 7. In contrast, the\nrefrigerator has a clear periodicity during operation, and the power required\nfor each cycle is relatively stable with distinct load characteristics. It is\nimportant to note these differences in power consumption between the two\nappliances. Table 2 shows that all models have a significant deviation from\nthe actual washing machine decomposition results. Additionally, Household User\n1 uses the washing machine less frequently, resulting in a smaller number of\nrequired training samples. As a result, the PMAM model cannot effectively\nextract the load characteristics. Therefore, to enhance the model\u2019s ability to\nextract deep load features, further research will be conducted to extract the\nload features of electrical appliances that frequently switch operating\nstates. Additionally, the super-parameters of the feature extraction network\nwill be optimized.\n\nIn order to validate the generalizability of the PMAM model proposed in this\npaper, data from Household User 2 in the UK-DALE dataset were selected for\ncross-household experiments. The model\u2019s training set was selected from\nHousehold User 1\u2019s data between April 2013 and December 2014, while the test\nset was selected from Household User 2\u2019s data for a one-month period in order\nto conduct load decomposition tests. The results of the tests are presented in\nTable 3 below.\n\nTable 3. Comparison of the load-splitting performance evaluation metrics for\nHousehold User 2.\n\nTable 3 shows that the PMAM model maintains good decomposition performance\neven when tested with data from untrained Household User 2. The model\noutperforms the other models in most performance evaluation metrics. In the\nperformance evaluation index of the PMAM model\u2019s decomposition of the\nrefrigerator data, the model\u2019s MAE is the lowest. Although the F1 score of the\nELM model is higher than that of the PMAM model, the difference is only 1.6%.\nHowever, the MAE of the PMAM model is 31% lower than that of the ELM model.\nTherefore, it appears that the PMAM model proposed in this paper is superior\noverall. In the performance evaluation of the PMAM model on the washing\nmachine data, the PMAM model outperforms the other models with higher F1\nscores (an 11% increase compared with the TPNILM model). The results in Table\n2 and Table 3 demonstrate the superior generality of the PMAM model proposed\nin this paper.\n\nWhen implementing smart grid technology, it is important to consider the\nmodel\u2019s generalization and load decomposition capabilities as well as its\ntraining time and computational requirements. To achieve this, we compared the\nnumber of model parameters and the training time required by different models,\nas shown in Table 4.\n\nTable 4. Model training time and number of parameters for each model.\n\nFrom Table 4, it can be seen that the PMAM model requires more computations\ncompared with the other models, which is due to the fact that the PMAM model\nuses a parallel multiscale feature fusion network, which captures both global\nand local features through a parallel multiscale attention mechanism,\nresulting in an increase in the number of model parameters. Compared with the\nother models, the PMAM model proposed in this paper has a smaller difference\nin the training time per epoch. It also has a comprehensive decomposition\ncapability that can still be applied to NILM. However, there is still room to\noptimize the number of computations that the model requires when deployed in\nreal smart grid projects. The complexity of the model can be optimized while\nmaintaining the decomposition performance.\n\n## 5\\. Conclusions\n\nThis paper presented a non-intrusive load decomposition model with a parallel\nmultiscale attention mechanism that combines non-intrusive load identification\ntechnology with smart home technology to improve the energy demand-side\nresponse under the development of smart grids and new power systems.\nIncorporating the multiscale attention mechanism in the training network\ncaptures both global and local feature information, expanding the perceptual\nfield of the model. Additionally, the model\u2019s decomposition performance is\nimproved to a certain extent by the multiscale feature fusion network. After\nvalidating the PMAM model with two household users from the UK-DALE dataset,\nwe proposed a model with better generalizability and improved accuracy and F1\nscore values compared with the other models. The experimental results indicate\nthat the PMAM model maintains F1 scores above 0.9 for refrigerator,\ndishwasher, and washing machine data under the same household user. On\naverage, the F1 scores improved by 2.1% and the MAE decreased by 6.84%\ncompared with the UNPMAM. This suggests that the model\u2019s decomposition\nperformance was better after the parallel multiscale attention mechanism was\nadded. In the cross-family test, the PMAM model demonstrated a superior\ndecomposition ability, maintaining an F1 score above 0.85. On average,\ncompared with the other models, the F1 score increased by 15.34%, indicating\nbetter generalizability. The PMAM model allows for the decomposition of data\non household electrical equipment, providing users with a clear understanding\nof the working status of each device in the household. Users can utilize the\ninformation on the operational status of their loads to establish a home\nenergy management system. This system can optimize the scheduling of household\nelectrical equipment, resulting in reduced energy loss and cost savings while\nmaintaining user comfort.\n\nIn future research, the PMAM model\u2019s decomposition results will be combined\nwith a home energy management system to optimize equipment scheduling by\nconsidering the working status of each electrical device. Additionally,\ndepending on the energy suppliers\u2019 buying and selling policies, home users\nwith energy storage devices or electric vehicles can achieve better energy\nreturns. Simultaneously, information on electrical equipment obtained from\ndecomposition can be linked to the user\u2019s behavior, allowing for the\nprediction of the user\u2019s next action based on their use of a particular\nelectrical device. This may enhance the intelligence of household equipment\nand improve the user\u2019s experience.\n\n## Author Contributions\n\nConceptualization and methodology, J.W.; software and validation, J.W.; formal\nanalysis and investigation, J.W.; data curation and writing\u2014original draft\npreparation, J.W.; writing\u2014review and editing, C.Z.; visualization and\nsupervision, L.Y. All authors have read and agreed to the published version of\nthe manuscript.\n\n## Funding\n\nThis research was funded by the National Natural Science Foundation of China\n(No. 51707121).\n\n## Data Availability Statement\n\nThe original contributions presented in the study are included in the article,\nfurther inquiries can be directed to the corresponding author.\n\n## Conflicts of Interest\n\nThe authors declare no conflicts of interest.\n\n## References\n\n  1. Lu, J.; Zhao, R.; Liu, B.; Yu, Z.; Zhang, J.; Xu, Z. An Overview of Non-Intrusive Load Monitoring Based on VI Trajectory Signature. Energies 2023, 16, 939. [Google Scholar] [CrossRef]\n  2. Aboulian, A.; Green, D.H.; Switzer, J.F.; Kane, T.J.; Bredariol, G.V.; Lindahl, P.; Donnal, J.S.; Leeb, S.B. NILM dashboard: A power system monitor for electromechanical equipment diagnostics. IEEE Trans. Ind. Inform. 2018, 15, 1405\u20131414. [Google Scholar] [CrossRef]\n  3. Li, J.; Xu, W.; Zhang, X.; Feng, X.; Chen, Z.; Qiao, B.; Xue, H. Control method of multi-energy system based on layered control architecture. Energy Build. 2022, 261, 111963. [Google Scholar] [CrossRef]\n  4. Koseleva, N.; Ropaite, G. Big data in building energy efficiency: Understanding of big data and main challenges. Procedia Eng. 2017, 172, 544\u2013549. [Google Scholar] [CrossRef]\n  5. He, X.; Dong, H.; Yang, W.; Hong, J. A Novel Denoising Auto-Encoder-Based Approach for Non-Intrusive Residential Load Monitoring. Energies 2022, 15, 2290. [Google Scholar] [CrossRef]\n  6. Gopinath, R.; Kumar, M.; Joshua, C.P.C.; Srinivas, K. nergy management using non-intrusive load monitoring techniques\u2014State-of-the-art and future research directions. Sustain. Cities Soc. 2020, 62, 102411. [Google Scholar] [CrossRef]\n  7. Armel, K.C.; Gupta, A.; Shrimali, G.; Albert, A. Is disaggregation the holy grail of energy efficiency? The case of electricity. Energy Policy 2013, 52, 213\u2013234. [Google Scholar] [CrossRef]\n  8. Desley, V.; Laurie, B.; Peter, M. The effectiveness of energy feedback for conservation and peak demand: A literature review. Open J. Energy Effic. 2013, 2013, 28957. [Google Scholar]\n  9. Bonfigli, R.; Principi, E.; Fagiani, M.; Severini, M.; Squartini, S.; Piazza, F. Non-intrusive load monitoring by using active and reactive power in additive Factorial Hidden Markov Models. Appl. Energy 2017, 208, 1590\u20131607. [Google Scholar] [CrossRef]\n  10. Qu, L. Research and Application of Non-Intrusive Load Monitoring Technology Based on Event Detection. Ph.D. Thesis, Hangzhou University of Electronic Science and Technology, Hangzhou, China, 2023. [Google Scholar]\n  11. Hart, G.W. Prototype Nonintrusive Appliance Load Monitor; Technical Report 2; MIT Energy Laboratory and Electric Power Research Institute: Concorde, MA, USA, 1985. [Google Scholar]\n  12. Hart, G.W. Nonintrusive appliance load monitoring. Proc. IEEE 1992, 80, 1870\u20131891. [Google Scholar] [CrossRef]\n  13. Xu, X.H.; Zhao, S.T.; Cui, K.B. A non-intrusive load decomposition algorithm based on convolutional block attention model. Grid Technol. 2021, 45, 3700\u20133706. [Google Scholar]\n  14. Kelly, J.; Knottenbelt, W. Neural nilm: Deep neural networks applied to energy disaggregation. In Proceedings of the 2nd ACM International Conference on Embedded Systems for Energy-Efficient Built Environments, New York, NY, USA, 4\u20135 November 2015; pp. 55\u201364. [Google Scholar]\n  15. Batra, N.; Singh, A.; Whitehouse, K. Neighbourhood nilm: A big-data approach to household energy disaggregation. arXiv 2015, arXiv:1511.02900. [Google Scholar]\n  16. Nalmpantis, C.; Vrakas, D. Machine learning approaches for non-intrusive load monitoring: From qualitative to quantitative comparation. Artif. Intell. Rev. 2019, 52, 217\u2013243. [Google Scholar] [CrossRef]\n  17. Yang, F.; Liu, B.; Luan, W.; Zhao, B.; Liu, Z.; Xiao, X.; Zhang, R. FHMM based industrial load disaggregation. In Proceedings of the 2021 6th Asia Conference on Power and Electrical Engineering (ACPEE), Chongqing, China, 8\u201311 April 2021; pp. 330\u2013334. [Google Scholar]\n  18. Jiao, X.; Chen, G.; Liu, J. A non-intrusive load monitoring model based on graph neural networks. In Proceedings of the 2023 IEEE 2nd International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA), Changchun, China, 24\u201326 February 2023; pp. 245\u2013250. [Google Scholar]\n  19. Kang, J.S.; Yu, M.; Lu, L.; Wang, B.; Bao, Z. Adaptive non-intrusive load monitoring based on feature fusion. IEEE Sens. J. 2022, 22, 6985\u20136994. [Google Scholar] [CrossRef]\n  20. Gao, Y.; Yang, H. Domestic load identification based on transient feature closeness matching. Power Syst. Autom. 2013, 37, 54\u201359. [Google Scholar]\n  21. Wang, K.; Zhong, H.; Yu, N.; Xia, Q. Non-intrusive load decomposition for residential users based on seq2seq and Attention mechanism. Chin. J. Electr. Eng. 2019, 39, 75\u201383+322. [Google Scholar]\n  22. Shi, Y.T. Research and Application of Non-Intrusive Load Identification Method Based on Deep Learning. Ph.D. Thesis, Hangzhou University of Electronic Science and Technology, Hangzhou, China, 2023. [Google Scholar]\n  23. De Baets, L.; Ruyssinck, J.; Develder, C.; Dhaene, T.; Deschrijver, D. Appliance classification using VI trajectories and convolutional neural networks. Energy Build. 2018, 158, 32\u201336. [Google Scholar] [CrossRef]\n  24. Wang, S.; Chen, H.; Guo, L.; Xu, D. Non-intrusive load identification based on the improved voltage-current trajectory with discrete color encoding background and deep-forest classifier. Energy Build. 2021, 244, 111043. [Google Scholar] [CrossRef]\n  25. Han, Y.; Li, K.; Feng, H.; Zhao, Q. Non-intrusive load monitoring based on semi-supervised smooth teacher graph learning with voltage\u2013current trajectory. Neural Comput. Appl. 2022, 34, 19147\u201319160. [Google Scholar] [CrossRef]\n  26. Mou, K.; Yang, H. A non-intrusive load monitoring method based on PLA-GDTW support vector machine. Grid Technol. 2019, 43, 4185\u20134193. [Google Scholar]\n  27. Huber, P.; Ott, M.; Friedli, M.; Rumsch, A.; Paice, A. Residential power traces for five houses: The iHomeLab RAPT dataset. Data 2020, 5, 17. [Google Scholar] [CrossRef]\n  28. Klemenjak, C.; Kovatsch, C.; Herold, M.; Elmenreich, W. A synthetic energy dataset for non-intrusive load monitoring in households. Sci. Data 2020, 7, 108. [Google Scholar] [CrossRef] [PubMed]\n  29. Kelly, J.; Knottenbelt, W. The UK-DALE dataset, domestic appliance-level electricity demand and whole-house demand from five UK homes. Sci. Data 2015, 2, 150007. [Google Scholar] [CrossRef]\n  30. Kolter, J.Z.; Johnson, M.J. REDD: A public data set for energy disaggregation research. In Proceedings of the Workshop on Data Mining Applications in Sustainability (SIGKDD), San Diego, CA, USA, 21\u201324 August 2011; Volume 25, pp. 59\u201362. [Google Scholar]\n  31. Filip, A. Blued: A fully labeled public dataset for event-based nonintrusive load monitoring research. In Proceedings of the 2nd Workshop on Data Mining Applications in Sustainability (SustKDD), Beijing, China, 12\u201316 August 2012. [Google Scholar]\n  32. Cui, P.F. Research on Non-Intrusive Household Electric Load Decomposition and Application Based on Deep Learning. Ph.D. Thesis, Xi\u2019an University of Architecture and Technology, Xi\u2019an, China, 2023. [Google Scholar]\n  33. Zhang, C.; Zhong, M.; Wang, Z.; Goddard, N.; Sutton, C. Sequence-to-point learning with neural networks for non-intrusive load monitoring. In Proceedings of the AAAI Conference on Artificial Intelligence, New Orleans, LA, USA, 2\u20137 February 2018; Volume 32. [Google Scholar]\n  34. Zhao, G.; Sun, X.; Xu, J.; Zhang, Z.; Luo, L. Muse: Parallel multi-scale attention for sequence to sequence learning. arXiv 2019, arXiv:1911.09483. [Google Scholar]\n  35. Alhussein, M.; Aurangzeb, K.; Haider, S.I. Hybrid CNN-LSTM model for short-term individual household load forecasting. IEEE Access 2020, 8, 180544\u2013180557. [Google Scholar] [CrossRef]\n  36. Shao, X.; Kim, C.S.; Sontakke, P. Accurate deep model for electricity consumption forecasting using multi-channel and multi-scale feature fusion CNN\u2013LSTM. Energies 2020, 13, 1881. [Google Scholar] [CrossRef]\n  37. Xu, R.; Liu, D. Non-intrusive load monitoring based on multi-scale feature fusion and multi-head self-attention mechanism. Sci. Technol. Eng. 2024, 24, 2385\u20132395. [Google Scholar]\n  38. Massidda, L.; Marrocu, M.; Manca, S. Non-intrusive load disaggregation by convolutional neural network and multilabel classification. Appl. Sci. 2020, 10, 1454. [Google Scholar] [CrossRef]\n  39. Salerno, V.M.; Rabbeni, G. An extreme learning machine approach to effective energy disaggregation. Electronics 2018, 7, 235. [Google Scholar] [CrossRef]\n\nFigure 1. Smart grid architecture.\n\nFigure 2. Physical architecture of intrusive load detection and non-intrusive\nload detection.\n\nFigure 3. Power curve.\n\nFigure 4. (a) Laptop computer; (b) Water dispenser; (c) Microwave oven; (d)\nLaser printer. Current, active power, and reactive power waveforms during\nsteady-state operation.\n\nFigure 5. V-I curves: (a) Laptop computer; (b) Water dispenser; (c) Microwave\noven; (d) Laser printer.\n\nFigure 6. Examples of transient characteristics.\n\nFigure 7. (a) Household User 1; (b) Household User 2. Total power and power\ncurves for three types of electrical appliances.\n\nFigure 8. A framework for smart home modeling based on non-intrusive\nrecognition.\n\nFigure 9. A non-intrusive decomposition model based on a parallel multi-\nchannel attention mechanism.\n\nFigure 10. Structure of the feature extraction network.\n\nFigure 11. Parallel multiscale network structure.\n\nFigure 12. Model training loss values under different optimizers.\n\nFigure 13. Model training loss values at different learning rates.\n\nFigure 14. Confusion matrix structure.\n\nFigure 15. Experimental process.\n\nFigure 16. (a) dishwashers; (b) washing machine. Work status prediction\nresults.\n\nTable 1. Commonly used load feature library classification and feature\nextraction methods.\n\nFeature Type| Specific Characteristics| Feature Extraction Method  \n---|---|---  \nHomeostatic Characteristics| voltage/current characteristics| statistical\nanalysis  \nactive/reactive power characteristics| statistical analysis  \nharmonic characteristic| Fast Fourier Transform (FFT)  \nperipheral V-I characteristics| relevant indicators used to characterize\ntrajectories  \nTransient Characteristics| prompt power| power spectral envelope estimation,\nwaveform vectors  \ninstantaneous current| Fast Fourier Transform (FFT)  \nvoltage noise| spectral analysis  \n  \nTable 2. Comparison of the load-splitting performance evaluation metrics for\nHousehold User 1.\n\nElectrical Equipment| Modeling| Evaluation Indicators  \n---|---|---  \nF1| Precision| Recall| Accuracy| MAE| MCC  \nDishwasher| PMAM| 0.966| 0.967| 0.965| 0.998| 20.44| 0.965  \nUNPMAM| 0.933| 0.908| 0.959| 0.997| 21.37| 0.932  \nLSTM| 0.06| 0.03| 0.63| 0.35| 130| -  \nTPNILM| 0.930| 0.942| 0.919| 0.997| 20.41| 0.928  \nELM| 0.93| 0.89| 0.99| 0.98| 19| -  \nWashing machine| PMAM| 0.989| 0.987| 0.991| 0.998| 41.48| 0.988  \nUNPMAM| 0.984| 0.985| 0.982| 0.998| 41.51| 0.982  \nLSTM| 0.09| 0.05| 0.62| 0.31| 133| -  \nTPNILM| 0.978| 0.975| 0.982| 0.997| 41.97| 0.977  \nELM| 0.84| 0.73| 0.99| 0.76| 27| -  \nRefrigerators| PMAM| 0.900| 0.903| 0.898| 0.910| 12.66| 0.818  \nUNPMAM| 0.867| 0.879| 0.856| 0.881| 15.09| 0.760  \nLSTM| 0.06| 0.03| 0.63| 0.35| 130| -  \nTPNILM| 0.867| 0.875| 0.859| 0.880| 15.25| 0.759  \nELM| 0.83| 0.88| 0.80| 0.88| 20| -  \n  \nTable 3. Comparison of the load-splitting performance evaluation metrics for\nHousehold User 2.\n\nElectrical Equipment| Modeling| Evaluation Indicators  \n---|---|---  \nF1| Precision| Recall| Accuracy| MAE| MCC  \nDishwasher| PMAM| 0.899| 0.907| 0.892| 0.994| 30.84| 0.896  \nUNPMAM| 0.740| 0.679| 0.813| 0.984| 35.91| 0.735  \nLSTM| 0.08| 0.04| 0.87| 0.30| 168| -  \nTPNILM| 0.809| 0.788| 0.835| 0.989| 33.07| 0.805  \nELM| 0.55| 0.35| 1.00| 1.00| 22| -  \nWashing machine| PMAM| 0.973| 0.977| 0.969| 0.999| 9.32| 0.972  \nUNPMAM| 0.955| 1.00| 0.915| 0.999| 9.50| 0.956  \nLSTM| 0.03| 0.01| 0.73| 0.23| 109| -  \nTPNILM| 0.863| 0.858| 0.869| 0.997| 8.31| 0.862  \nELM| 0.43| 0.10| 1.00| 0.84| 21| -  \nRefrigerators| PMAM| 0.874| 0.829| 0.924| 0.908| 15.87| 0.805  \nUNPMAM| 0.847| 0.785| 0.919| 0.886| 17.53| 0.763  \nLSTM| 0.74| 0.72| 0.77| 0.81| 36| -  \nTPNILM| 0.871| 0.892| 0.851| 0.905| 17.03| 0.796  \nELM| 0.89| 0.90| 0.92| 0.94| 23| -  \n  \nTable 4. Model training time and number of parameters for each model.\n\nModel| Number of Modeling Parameters| Training Time/(s/epoch)  \n---|---|---  \nPMAM| 549,902| 3.17  \nUNPMAM| 496,259| 0.76  \nTPNILM| 327,619| 1.62  \nELM| -| 2.46  \n  \nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in\nall publications are solely those of the individual author(s) and\ncontributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s)\ndisclaim responsibility for any injury to people or property resulting from\nany ideas, methods, instructions or products referred to in the content.  \n---  \n\u00a9 2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an\nopen access article distributed under the terms and conditions of the Creative\nCommons Attribution (CC BY) license\n(https://creativecommons.org/licenses/by/4.0/).\n\n## Share and Cite\n\nMDPI and ACS Style\n\nYao, L.; Wang, J.; Zhao, C. Non-Intrusive Load Monitoring Based on Multiscale\nAttention Mechanisms. Energies 2024, 17, 1944.\nhttps://doi.org/10.3390/en17081944\n\nAMA Style\n\nYao L, Wang J, Zhao C. Non-Intrusive Load Monitoring Based on Multiscale\nAttention Mechanisms. Energies. 2024; 17(8):1944.\nhttps://doi.org/10.3390/en17081944\n\nChicago/Turabian Style\n\nYao, Lei, Jinhao Wang, and Chen Zhao. 2024. \"Non-Intrusive Load Monitoring\nBased on Multiscale Attention Mechanisms\" Energies 17, no. 8: 1944.\nhttps://doi.org/10.3390/en17081944\n\nNote that from the first issue of 2016, this journal uses article numbers\ninstead of page numbers. See further details here.\n\n## Article Metrics\n\nYes\n\n### Citations\n\nNo citations were found for this article, but you may check on Google Scholar\n\nNo\n\n### Article Access Statistics\n\nFor more information on the journal statistics, click here.\n\nMultiple requests from the same IP address are counted as one view.\n\nZoom | Orient | As Lines | As Sticks | As Cartoon | As Surface | Previous Scene | Next Scene\n\n## Cite\n\nExport citation file: BibTeX | EndNote | RIS\n\nMDPI and ACS Style\n\nYao, L.; Wang, J.; Zhao, C. Non-Intrusive Load Monitoring Based on Multiscale\nAttention Mechanisms. Energies 2024, 17, 1944.\nhttps://doi.org/10.3390/en17081944\n\nAMA Style\n\nYao L, Wang J, Zhao C. Non-Intrusive Load Monitoring Based on Multiscale\nAttention Mechanisms. Energies. 2024; 17(8):1944.\nhttps://doi.org/10.3390/en17081944\n\nChicago/Turabian Style\n\nYao, Lei, Jinhao Wang, and Chen Zhao. 2024. \"Non-Intrusive Load Monitoring\nBased on Multiscale Attention Mechanisms\" Energies 17, no. 8: 1944.\nhttps://doi.org/10.3390/en17081944\n\nNote that from the first issue of 2016, this journal uses article numbers\ninstead of page numbers. See further details here.\n\nclear\n\nEnergies, EISSN 1996-1073, Published by MDPI\n\nRSS Content Alert\n\n### Further Information\n\nArticle Processing Charges Pay an Invoice Open Access Policy Contact MDPI Jobs\nat MDPI\n\n### Guidelines\n\nFor Authors For Reviewers For Editors For Librarians For Publishers For\nSocieties For Conference Organizers\n\n### MDPI Initiatives\n\nSciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS\nProceedings Series\n\n### Follow MDPI\n\nLinkedIn Facebook Twitter\n\n\u00a9 1996-2024 MDPI (Basel, Switzerland) unless otherwise stated\n\nDisclaimer\n\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in\nall publications are solely those of the individual author(s) and\ncontributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s)\ndisclaim responsibility for any injury to people or property resulting from\nany ideas, methods, instructions or products referred to in the content.\n\nTerms and Conditions Privacy Policy\n\nWe use cookies on our website to ensure you get the best experience. Read more\nabout our cookies here.\n\nAccept\n\n## Share Link\n\nCopy\n\nclear\n\n## Share\n\nclear\n\nBack to TopTop\n\n", "frontpage": false}
