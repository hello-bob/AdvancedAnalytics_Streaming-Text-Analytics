{"aid": "40220982", "title": "How to Enhance Database Performance Testing Using Custom SQL Scripts in PgBench", "url": "https://www.yugabyte.com/blog/pgbench-custom-sql-scripts/", "domain": "yugabyte.com", "votes": 3, "user": "franckpachot", "posted_at": "2024-05-01 08:56:32", "comments": 0, "source_title": "Improve Database Performance Testing with Custom SQL Scripts in pgbench", "source_text": "Improve Database Performance Testing with Custom SQL Scripts in pgbench\n\nSkip to content\n\n  * Sign In\n  * Get Started\n\n    * Try YugabyteDB Managed FreeCloud database managed by us\n    * Download YugabyteDBFree open source database\n    * Book a DemoPersonalized demo\n\nBack to Blog Home\n\n# How to Enhance Database Performance Testing Using Custom SQL Scripts in\nPgBench\n\nPostgreSQL Tips and Tricks Series\n\nFranck Pachot\n\nApril 25, 2024\n\npgbench is a popular tool for testing database performance, but it\u2019s default\n\u2018TPC-B like\u2019 workload, which involves many roundtrips and context switches,\nmay not effectively reflect true performance as it could skew results. To\novercome this, users can employ custom SQL within pgbench to design a load\ntest that more accurately simulates real-world scenarios and identifies\npotential bottlenecks.\n\nThis applies to PostgreSQL and Postgres-compatible databases. I\u2019ll demo on\nYugabyteDB. You can skip to \u201cStep 3: Create the schema\u201d if you already have a\ndatabase.\n\n## #1: Start a YugabyteDB cluster\n\nTo begin, I will use my yb-compose Docker Compose configuration to start a 3\nnodes cluster.\n\nPlain text\n\nCopy to clipboard\n\nOpen code in new window\n\nEnlighterJS 3 Syntax Highlighter\n\ngit clone https://github.com/FranckPachot/yb-compose.git\n\ncd yb-compose\n\ndocker compose up -d\n\ngit clone https://github.com/FranckPachot/yb-compose.git cd yb-compose docker\ncompose up -d\n\n    \n    \n    git clone https://github.com/FranckPachot/yb-compose.git cd yb-compose docker compose up -d\n\nThis starts a cluster with 3 nodes and replication factor 3, which you can\nscale further with:\n\nPlain text\n\nCopy to clipboard\n\nOpen code in new window\n\nEnlighterJS 3 Syntax Highlighter\n\ndocker compose up --scale yb=6 -d\n\ndocker compose up --scale yb=6 -d\n\n    \n    \n    docker compose up --scale yb=6 -d\n\nUsing the service I\u2019ve declared in the docker-compose, you can connect with\nthe PostgreSQL client from a container, or from the host through the forwarded\nports. I\u2019ll explain both.\n\n## #2a: Connect from a container\n\nIf you connect from another container, preferably from one where you can use\nthe service name 'yb' which will connect to any node. For example, you can\nstart a shell from the \u2018pg\u2018 service declared in the docker-compose.yaml and\nset the environment to connect to the \u2018yb\u2018 service:\n\nPlain text\n\nCopy to clipboard\n\nOpen code in new window\n\nEnlighterJS 3 Syntax Highlighter\n\ndocker compose run -it pg bash\n\nexport PGLOADBALANCEHOSTS=random\n\nexport PGUSER=yugabyte\n\nexport PGDATABASE=yugabyte\n\nexport PGPASSWORD=yugabyte\n\nexport PGPORT=5433\n\nexport PGHOST=yb\n\ndocker compose run -it pg bash export PGLOADBALANCEHOSTS=random export\nPGUSER=yugabyte export PGDATABASE=yugabyte export PGPASSWORD=yugabyte export\nPGPORT=5433 export PGHOST=yb\n\n    \n    \n    docker compose run -it pg bash export PGLOADBALANCEHOSTS=random export PGUSER=yugabyte export PGDATABASE=yugabyte export PGPASSWORD=yugabyte export PGPORT=5433 export PGHOST=yb\n\nTo verify, this connection will show a different address each time you run it:\n\nPlain text\n\nCopy to clipboard\n\nOpen code in new window\n\nEnlighterJS 3 Syntax Highlighter\n\npsql -c \"show listen_addresses\"\n\npsql -c \"show listen_addresses\"\n\n    \n    \n    psql -c \"show listen_addresses\"\n\n## #2b: Connect from the host via forwarded posts\n\nIf you connect through forwarded ports, you can list them with:\n\nPlain text\n\nCopy to clipboard\n\nOpen code in new window\n\nEnlighterJS 3 Syntax Highlighter\n\ndocker compose ps yb\n\ndocker compose ps yb\n\n    \n    \n    docker compose ps yb\n\nYou need the PostgreSQL client; I will use version 16 to utilize features like\nload balancing. Here is an example of installing it in Alma8, to get psql and\npgbench:\n\nPlain text\n\nCopy to clipboard\n\nOpen code in new window\n\nEnlighterJS 3 Syntax Highlighter\n\nsudo dnf install -y\n\nhttps://download.postgresql.org/pub/repos/yum/reporpms/EL-8-x86_64/pgdg-\nredhat-repo-latest.noarch.rpm\n\nsudo dnf -qy module disable postgresql\n\nsudo dnf install -y postgresql16 jq\n\nalias psql=/usr/pgsql-16/bin/psql\n\nalias pgbench=/usr/pgsql-16/bin/pgbench\n\nsudo dnf install -y\nhttps://download.postgresql.org/pub/repos/yum/reporpms/EL-8-x86_64/pgdg-\nredhat-repo-latest.noarch.rpm sudo dnf -qy module disable postgresql sudo dnf\ninstall -y postgresql16 jq alias psql=/usr/pgsql-16/bin/psql alias\npgbench=/usr/pgsql-16/bin/pgbench\n\n    \n    \n    sudo dnf install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-8-x86_64/pgdg-redhat-repo-latest.noarch.rpm sudo dnf -qy module disable postgresql sudo dnf install -y postgresql16 jq alias psql=/usr/pgsql-16/bin/psql alias pgbench=/usr/pgsql-16/bin/pgbench\n\nWith jq installed, here is how I set PGHOST and PGPORT from the list of ports\nto enable load balancing:\n\nPlain text\n\nCopy to clipboard\n\nOpen code in new window\n\nEnlighterJS 3 Syntax Highlighter\n\nexport PGLOADBALANCEHOSTS=random\n\nexport PGUSER=yugabyte\n\nexport PGDATABASE=yugabyte\n\nexport PGPASSWORD=yugabyte\n\nexport PGPORT=$( docker compose ps yb --format json | jq -r '[ .[].Publishers[]|select(.TargetPort==5433)|.PublishedPort ] | join(\",\")' )\n\nexport PGHOST=$( echo \"$PGPORT\" | sed -e 's/[^,]*/localhost/g' )\n\nset | grep ^PG\n\nexport PGLOADBALANCEHOSTS=random export PGUSER=yugabyte export PGDATABASE=yugabyte export PGPASSWORD=yugabyte export PGPORT=$( docker compose ps yb --format json | jq -r '[ .[].Publishers[]|select(.TargetPort==5433)|.PublishedPort ] | join(\",\")' ) export PGHOST=$( echo \"$PGPORT\" | sed -e 's/[^,]*/localhost/g' ) set | grep ^PG\n    \n    \n    export PGLOADBALANCEHOSTS=random export PGUSER=yugabyte export PGDATABASE=yugabyte export PGPASSWORD=yugabyte export PGPORT=$( docker compose ps yb --format json | jq -r '[ .[].Publishers[]|select(.TargetPort==5433)|.PublishedPort ] | join(\",\")' ) export PGHOST=$( echo \"$PGPORT\" | sed -e 's/[^,]*/localhost/g' ) set | grep ^PG\n\nHere is an example of settings. Docker Compose assigns ports in the range\ndefined:\n\nPlain text\n\nCopy to clipboard\n\nOpen code in new window\n\nEnlighterJS 3 Syntax Highlighter\n\n-bash-4.2# set | grep ^PG\n\nPGHOST=localhost,localhost,localhost,localhost,localhost,localhost\n\nPGLOADBALANCEHOSTS=random\n\nPGDATABASE=yugabyte\n\nPGPASSWORD=yugabyte\n\nPGUSER=yugabyte\n\nPGPORT=5437,5438,5439,5443,5444,5441\n\n-bash-4.2# set | grep ^PG PGHOST=localhost,localhost,localhost,localhost,localhost,localhost PGLOADBALANCEHOSTS=random PGDATABASE=yugabyte PGPASSWORD=yugabyte PGUSER=yugabyte PGPORT=5437,5438,5439,5443,5444,5441\n    \n    \n    -bash-4.2# set | grep ^PG PGHOST=localhost,localhost,localhost,localhost,localhost,localhost PGLOADBALANCEHOSTS=random PGDATABASE=yugabyte PGPASSWORD=yugabyte PGUSER=yugabyte PGPORT=5437,5438,5439,5443,5444,5441\n\nTo verify, this connection will show a different address each time you run it:\n\nPlain text\n\nCopy to clipboard\n\nOpen code in new window\n\nEnlighterJS 3 Syntax Highlighter\n\npsql -c \"show listen_addresses\"\n\npsql -c \"show listen_addresses\"\n\n    \n    \n    psql -c \"show listen_addresses\"\n\nNow that the PostgreSQL client is ready, we can use psql and pgbench.\n\n## #3: Create the schema\n\nLet\u2019s create the schema we will be running. I\u2019m setting something very simple\nhere:\n\nPlain text\n\nCopy to clipboard\n\nOpen code in new window\n\nEnlighterJS 3 Syntax Highlighter\n\npsql <<'\\q'\n\n\\set ON_ERROR_STOP on\n\n\\timing on\n\ndrop table if exists demo;\n\ncreate table demo (k bigint, v int, primary key(k));\n\ninsert into demo select generate_series(1,10000) k, 0 v;\n\n\\q\n\npsql <<'\\q' \\set ON_ERROR_STOP on \\timing on drop table if exists demo; create\ntable demo (k bigint, v int, primary key(k)); insert into demo select\ngenerate_series(1,10000) k, 0 v; \\q\n\n    \n    \n    psql <<'\\q' \\set ON_ERROR_STOP on \\timing on drop table if exists demo; create table demo (k bigint, v int, primary key(k)); insert into demo select generate_series(1,10000) k, 0 v; \\q\n\nNote: You can already use PgBench to run those statements. I\u2019ll explain the\narguments later:\n\nPlain text\n\nCopy to clipboard\n\nOpen code in new window\n\nEnlighterJS 3 Syntax Highlighter\n\npgbench --transactions 1 --report-per-command --verbose-errors --no-vacuum\n--file=/dev/stdin <<'\\q'\n\ndrop table if exists demo;\n\ncreate table demo (k bigint, v int, primary key(k));\n\ninsert into demo select generate_series(1,10000) k, 0 v;\n\n\\q\n\npgbench --transactions 1 --report-per-command --verbose-errors --no-vacuum\n--file=/dev/stdin <<'\\q' drop table if exists demo; create table demo (k\nbigint, v int, primary key(k)); insert into demo select\ngenerate_series(1,10000) k, 0 v; \\q\n\n    \n    \n    pgbench --transactions 1 --report-per-command --verbose-errors --no-vacuum --file=/dev/stdin <<'\\q' drop table if exists demo; create table demo (k bigint, v int, primary key(k)); insert into demo select generate_series(1,10000) k, 0 v; \\q\n\nPut your script in a file. I use STDIN and a HEREDOC for easy copy/paste.\n\nAn example of output:\n\npgbench output showing the latencies of the DDL statements in the script\n\n## #4: Run transactions from multiple clients\n\nThe script provided to PgBench can use some functions for example to generate\nrandom values into variables. All is documented and can be found on\nPostgresql.org documentation.\n\nI will run PgBench with the following arguments:\n\n\\--no-vacuum: doesn\u2019t run vacuum on the default pgbench tables, which we don\u2019t\nuse there. NOTE that VACUUM is not needed in YugabyteDB\n\n\\--clients: this is the number of connections to the server. With PostgreSQL\nclient 16 and PGLOADBALANCEHOSTS=random it will open connections in a round-\nrobin fashion to the hosts/port listed in PGHOST/PGPORT\n\n\\--job: By default, those connections are opened by one pgbench thread, which\nprobably works fine since the requests are sent asynchronously. Increasing the\nnumber of jobs distributes those connections from multiple pgbench threads.\nThe names are misleading: --client is about the number of server processes\n(backend) and --job is about the number of client (application) threads.\n\n\\--file=/dev/stdin: the script to run will be taken from standard input, which\nI provide with a bash HEREDOC, but you can use a real file of course\n\n\\--transactions: the number of script executions per connection. The total\nnumber of transactions is --transactions multiplied by --clients. You may\nprefer to run as many transactions as can be run in a time window, with\n--timein seconds.\n\n\\--report-per-command: shows individual latency for each statement in the\nscript, which is important to understand which statements may need tuning.\n\n\\--max-tries 5: in case of retriable error (SQLSTATE 40001 for serializable\nerrors or 40P01 for deadlocks) pgBench can retry them and report the number of\nretries. It is a best practice to have a retry logic, especially in a\ndistributed SQL database where some rare cases of clock skew may need retry,\neven in Read Committed isolation levels, as well as some online schema\nchanges.\n\nExample #1\n\nHere is a simple example:\n\nPlain text\n\nCopy to clipboard\n\nOpen code in new window\n\nEnlighterJS 3 Syntax Highlighter\n\npgbench --client=2 --transactions=10 --max-tries 5 --report-per-command --no-\nvacuum --file=/dev/stdin <<'\\q'\n\n\\set my_value random(1,3)\n\nbegin isolation level serializable;\n\nselect * from demo where k=:my_value;\n\nupdate demo set v=v+1 where k=:my_value;\n\nselect pg_sleep(1);\n\ncommit;\n\n\\q\n\npgbench --client=2 --transactions=10 --max-tries 5 --report-per-command --no-\nvacuum --file=/dev/stdin <<'\\q' \\set my_value random(1,3) begin isolation\nlevel serializable; select * from demo where k=:my_value; update demo set\nv=v+1 where k=:my_value; select pg_sleep(1); commit; \\q\n\n    \n    \n    pgbench --client=2 --transactions=10 --max-tries 5 --report-per-command --no-vacuum --file=/dev/stdin <<'\\q' \\set my_value random(1,3) begin isolation level serializable; select * from demo where k=:my_value; update demo set v=v+1 where k=:my_value; select pg_sleep(1); commit; \\q\n\npgbench output showing the latencies of the DML statements in the script\n\nI used a serializable isolation level with a long transaction to show the\nretries.\n\nExample #2\n\nHere is another example running for 15 minutes and using prepared statements:\n\nPlain text\n\nCopy to clipboard\n\nOpen code in new window\n\nEnlighterJS 3 Syntax Highlighter\n\npgbench --progress=10 --protocol=prepared --client=10 --time=1200 --max-tries\n5 --report-per-command --no-vacuum --file=/dev/stdin <<'\\q'\n\n\\set my_value random(1,10000)\n\nbegin isolation level read committed;\n\nupdate demo set v=v+1 where k=:my_value;\n\ncommit;\n\n\\q\n\npgbench --progress=10 --protocol=prepared --client=10 --time=1200 --max-tries\n5 --report-per-command --no-vacuum --file=/dev/stdin <<'\\q' \\set my_value\nrandom(1,10000) begin isolation level read committed; update demo set v=v+1\nwhere k=:my_value; commit; \\q\n\n    \n    \n    pgbench --progress=10 --protocol=prepared --client=10 --time=1200 --max-tries 5 --report-per-command --no-vacuum --file=/dev/stdin <<'\\q' \\set my_value random(1,10000) begin isolation level read committed; update demo set v=v+1 where k=:my_value; commit; \\q\n\nThe load is well-balanced across the three nodes:\n\nUI showing CPU usage in YugabyteDB\n\nI\u2019ve added the --progress=10 option that displays the statistics every 10\nseconds:\n\npgbench output showing the progress\n\nSince those are single statement transactions, the transaction per second\n(tps) reported by pgbench matches the distributed write operations per second\n(ops/s) reported by YugabyteDB:\n\nDistributed Operations/Sec and Average Latency from YugabyteDB UI\n\nTo set YSQL session options, you can use PGOPTIONS, like in this example, to\nallow reads for Raft followers in read-only transactions:\n\nPlain text\n\nCopy to clipboard\n\nOpen code in new window\n\nEnlighterJS 3 Syntax Highlighter\n\nPGOPTIONS=\"-c default_transaction_read_only=on -c yb_read_from_followers=on -c\nyb_follower_read_staleness_ms=15000\" pgbench --progress=10 --protocol=prepared\n--client=10 --time=1200 --max-tries 5 --report-per-command --no-vacuum\n--file=/dev/stdin <<'\\q'\n\n\\set my_value random(1,10000)\n\nselect * from demo where k=:my_value;\n\n\\q\n\nPGOPTIONS=\"-c default_transaction_read_only=on -c yb_read_from_followers=on -c\nyb_follower_read_staleness_ms=15000\" pgbench --progress=10 --protocol=prepared\n--client=10 --time=1200 --max-tries 5 --report-per-command --no-vacuum\n--file=/dev/stdin <<'\\q' \\set my_value random(1,10000) select * from demo\nwhere k=:my_value; \\q\n\n    \n    \n    PGOPTIONS=\"-c default_transaction_read_only=on -c yb_read_from_followers=on -c yb_follower_read_staleness_ms=15000\" pgbench --progress=10 --protocol=prepared --client=10 --time=1200 --max-tries 5 --report-per-command --no-vacuum --file=/dev/stdin <<'\\q' \\set my_value random(1,10000) select * from demo where k=:my_value; \\q\n\nExample #3\n\nHere is an example comparing yb_read_from_followers set to on (5500 reads per\nsecond at 1.7 milliseconds) and set to off (4230 reads per second at 2.2\nmilliseconds):\n\nPerformance with follower reads enabled and disabled\n\nBefore interpreting any benchmark result, it\u2019s important to fully understand\nthe data and verify it against different metrics across different layers. For\nexample, I compared the transactions accounted for by the client and the write\noperations accounted for by the database. During a database stress test,\nespecially with distributed SQL databases, ensure that the correct resources\nare being stressed and that the workload is evenly distributed, as\ndemonstrated by monitoring CPU usage.\n\nApril 25, 2024\n\nDatabase PerformancepgbenchPostgreSQL CompatibilityPostgreSQL Tips and Tricks\n\nFranck Pachot\n\nApril 25, 2024\n\n#### Related Posts\n\n### YugabyteDB 2.20 Unveiled: Elevating Change Data Capture for Cloud Native\nApplications\n\n5 months ago\n\n### Scaling Sequences with Server-Level Caching in YugabyteDB\n\n7 months ago\n\n### Integrate YugabyteDB Logs into Splunk for Comprehensive Monitoring and\nAnalysis\n\n8 months ago\n\nGet your free report to discover YugabyteDB and the latest in cloud database\ntechnology.\n\nGet Report\n\n## Popular Topics\n\n### PostgreSQL Compatibility\n\n### Database Migration\n\n### Database Sharding\n\n### Geo-Distribution\n\n### Customer Stories\n\n### Change Data Capture\n\n## Categories\n\nACID TransactionsAmazon AuroraAmazon DynamoDBAmazon Web ServicesApache\nCassandraCareersCloud ProvidersCockroachDBCommunity NewsCompany NewsCompare\nand contrastContainersCustomersDatabase ArchitectureDatabase\nMigrationDatabasesDistributed SQLDistributed SQL SummitDockerEcosystem\nIntegrationsGeo-DistributionGoogle Cloud PlatformGoogle SpannerGraphQLHow It\nWorksHow ToIntegrationsJepsen TestsKafkaKubernetesMicrosoft AzureMicrosoft\nAzure Cosmos DBMongoDBOn-PremisesOpen SourcePartnersPerformance\nBenchmarksPivotal Container Service (PKS)PostgreSQLRelease\nAnnouncementsRetailSecuritySpringStreaming IndustryUse CasesWeek in\nReviewYugabyteDBYugabyteDB AnywhereYugabyteDB ManagedYugabyteDB\nTrainingYugabyteDB Voyager\n\n## Explore Distributed SQL and YugabyteDB in Depth\n\nDiscover the future of data management.\n\nLearn at Yugabyte University\n\nGet Started\n\nBrowse Yugabyte Docs\n\nExplore docs\n\nPostgreSQL For Cloud Native World\n\nRead for Free\n\n  *     * Product\n\n      * YugabyteDB\n      * YugabyteDB Voyager\n      * Deployment Options\n      * Pricing\n    * Company\n\n      * About Yugabyte\n      * Press\n      * Careers\n      * Trust Center\n      * Contact\n      * Legal\n  *     * Solutions\n\n      * BY USE CASE\n\n        * Database Modernization\n        * Cloud Native Apps\n        * Edge and Streaming Apps\n      * BY INDUSTRY\n\n        * Financial Services\n        * Retail and eCommerce\n        * Telecommunications\n      * BY CLOUD\n\n        * AWS\n        * Google Cloud\n        * Microsoft Azure\n  *     * Community\n\n      * Get Involved\n      * Events\n      * YugabyteDB Friday Tech Talks\n      * Distributed SQL Summit\n      * Slack\n      * GitHub\n    * Resources\n\n      * Developer Hub\n      * Docs\n      * Yugabyte University\n      * Key Concepts\n      * Support\n      * Forum\n      * Success Stories\n      * Blog\n      * Content Library\n      * FAQ\n\n\u00a9 2024 YUGABYTE, INC. All rights reserved.\n\nTerms of Service Privacy Policy Cookie Policy Your California Privacy Choices\n\n", "frontpage": false}
