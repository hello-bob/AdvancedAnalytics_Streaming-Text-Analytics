{"aid": "40140002", "title": "Maxtext: A simple, performant and scalable Jax LLM", "url": "https://github.com/google/maxtext", "domain": "github.com/google", "votes": 6, "user": "zerojames", "posted_at": "2024-04-24 03:00:46", "comments": 0, "source_title": "GitHub - google/maxtext: A simple, performant and scalable Jax LLM!", "source_text": "GitHub - google/maxtext: A simple, performant and scalable Jax LLM!\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\ngoogle / maxtext Public\n\n  * Notifications\n  * Fork 183\n  * Star 991\n\nA simple, performant and scalable Jax LLM!\n\n### License\n\nApache-2.0 license\n\n991 stars 183 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# google/maxtext\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n471 Branches\n\n1 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nmaxtext authorsMerge pull request #615 from google:mattdavidow-move-aqtp-\npinApr 23, 2024718d9e7 \u00b7 Apr 23, 2024Apr 23, 2024\n\n## History\n\n1,136 Commits  \n  \n### .github\n\n|\n\n### .github\n\n| adding script to fix the style and adding modified/fixed files with l...|\nApr 16, 2024  \n  \n### .vscode\n\n|\n\n### .vscode\n\n| Streamline params usage| Apr 2, 2024  \n  \n### MaxText\n\n|\n\n### MaxText\n\n| Explicitly set AQT Freezer mode in MaxText.| Apr 23, 2024  \n  \n### assets\n\n|\n\n### assets\n\n| add gemma tokenizer| Feb 21, 2024  \n  \n### end_to_end\n\n|\n\n### end_to_end\n\n| Share GCS path between Gemma-7b tests| Apr 12, 2024  \n  \n### getting_started\n\n|\n\n### getting_started\n\n| Update Run_MaxText_via_xpk.md| Apr 19, 2024  \n  \n### pedagogical_examples\n\n|\n\n### pedagogical_examples\n\n| adding script to fix the style and adding modified/fixed files with l...|\nApr 16, 2024  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| Prebuild and install Transformer Engine package| Mar 25, 2024  \n  \n### AUTHORS\n\n|\n\n### AUTHORS\n\n| Added some basics| Dec 16, 2022  \n  \n### CONTRIBUTING.md\n\n|\n\n### CONTRIBUTING.md\n\n| Adding License, CONTRIBUTING.md, fixing pylintrc (#84)| Mar 28, 2023  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Initial commit| Dec 16, 2022  \n  \n### PREFLIGHT.md\n\n|\n\n### PREFLIGHT.md\n\n| Add network setting and numa binding recommendation| Feb 17, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| Add functionality to automatically upload logs to Vertex Tensorboard| Apr 9,\n2024  \n  \n### code_style.sh\n\n|\n\n### code_style.sh\n\n| adding script to fix the style and adding modified/fixed files with l...|\nApr 16, 2024  \n  \n### constraints_gpu.txt\n\n|\n\n### constraints_gpu.txt\n\n| Move aqtp pin up| Apr 23, 2024  \n  \n### docker_build_dependency_image.sh\n\n|\n\n### docker_build_dependency_image.sh\n\n| Merge remote-tracking branch 'origin/main' into yiinho-prebuilt-te| Apr 12,\n2024  \n  \n### docker_upload_runner.sh\n\n|\n\n### docker_upload_runner.sh\n\n| Add pinned build to daily build workflow| Apr 3, 2024  \n  \n### download_dataset.sh\n\n|\n\n### download_dataset.sh\n\n| Fix creation of \"/\" directory in gs bucket.| Jan 24, 2024  \n  \n### gpu_multi_process_run.sh\n\n|\n\n### gpu_multi_process_run.sh\n\n| Support GPU runs via XPK| Mar 28, 2024  \n  \n### maxtext_dependencies.Dockerfile\n\n|\n\n### maxtext_dependencies.Dockerfile\n\n| Move apt install from rto_setup.sh to setup.sh| Apr 17, 2024  \n  \n### maxtext_gpu_dependencies.Dockerfile\n\n|\n\n### maxtext_gpu_dependencies.Dockerfile\n\n| Mark nvidia devtools repo as trusted| Apr 22, 2024  \n  \n### maxtext_libtpu_path.Dockerfile\n\n|\n\n### maxtext_libtpu_path.Dockerfile\n\n| unify WORKDIR to /deps| Apr 12, 2024  \n  \n### maxtext_runner.Dockerfile\n\n|\n\n### maxtext_runner.Dockerfile\n\n| Merge remote-tracking branch 'origin/main' into yiinho-prebuilt-te| Apr 12,\n2024  \n  \n### maxtext_transformerengine_builder.Dockerfile\n\n|\n\n### maxtext_transformerengine_builder.Dockerfile\n\n| Fix typo| Apr 11, 2024  \n  \n### multihost_job.py\n\n|\n\n### multihost_job.py\n\n| Add a flag in multihost_job.py to enable Autocheckpoint (#197)| Oct 17, 2023  \n  \n### multihost_runner.py\n\n|\n\n### multihost_runner.py\n\n| multihost_runner and multihost_job assert zone is set (#123)| Aug 24, 2023  \n  \n### preflight.sh\n\n|\n\n### preflight.sh\n\n| Move all dependencies from preflight.sh to setup.sh| Feb 20, 2024  \n  \n### pylintrc\n\n|\n\n### pylintrc\n\n| adding script to fix the style and adding modified/fixed files with l...|\nApr 16, 2024  \n  \n### requirements.txt\n\n|\n\n### requirements.txt\n\n| Add functionality to automatically upload logs to Vertex Tensorboard| Apr 9,\n2024  \n  \n### rto_setup.sh\n\n|\n\n### rto_setup.sh\n\n| Move apt install from rto_setup.sh to setup.sh| Apr 17, 2024  \n  \n### setup.sh\n\n|\n\n### setup.sh\n\n| Move apt install from rto_setup.sh to setup.sh| Apr 17, 2024  \n  \n### setup_gcsfuse.sh\n\n|\n\n### setup_gcsfuse.sh\n\n| add grain instructions| Mar 26, 2024  \n  \n### setup_with_retries.sh\n\n|\n\n### setup_with_retries.sh\n\n| Retry setup (#108)| Sep 1, 2023  \n  \n### unit_test_and_lint.sh\n\n|\n\n### unit_test_and_lint.sh\n\n| Add support to XPK for v4 (#200)| Oct 12, 2023  \n  \n## Repository files navigation\n\n# Overview\n\nMaxText is a high performance, highly scalable, open-source LLM written in\npure Python/Jax and targeting Google Cloud TPUs and GPUs for training and\ninference. MaxText achieves high MFUs and scales from single host to very\nlarge clusters while staying simple and \"optimization-free\" thanks to the\npower of Jax and the XLA compiler.\n\nMaxText aims to be a launching off point for ambitious LLM projects both in\nresearch and production. We encourage users to start by experimenting with\nMaxText out of the box and then fork and modify MaxText to meet their needs.\n\nWe have used MaxText to demonstrate high-performance, well-converging training\nin int8 and scale training to ~51K chips.\n\nKey supported features:\n\n  * TPUs and GPUs (in preview)\n  * Training and Inference (in preview)\n  * Models: Llama2, Mistral and Gemma\n\n# Table of Contents\n\n  * Getting Started\n  * Runtime Performance Results\n  * Comparison To Alternatives\n  * Development\n  * Features and Diagnostics\n\n# Getting Started\n\nFor your first time running MaxText, we provide specific instructions.\n\nMaxText supports training and inference of various open models. Follow user\nguides in the getting started folder to know more.\n\nSome extra helpful guides:\n\n  * Gemma: a family of open-weights Large Language Model (LLM) by Google DeepMind, based on Gemini research and technology. You can run decode and finetuning using these instructions.\n  * Llama2: a family of open-weights Large Language Model (LLM) by Meta. You can run decode and finetuning using these instructions.\n\nIn addition to the getting started guides, there are always other MaxText\ncapabilities that are being constantly being added! The full suite of end-to-\nend tests is in end_to_end. We run them with a nightly cadence. They can be a\ngood source for understanding MaxText Alternatively you can see the continuous\nunit tests which are run almost continuously.\n\n# Runtime Performance Results\n\nMore details on reproducing these results can be found in\nMaxText/configs/README.md.\n\n## TPU v5p\n\nNo. of params| Accelerator Type| TFLOP/chip/sec| Model flops utilization (MFU)  \n---|---|---|---  \n32B| v5p-128| 3.28e+02| 71.47%  \n64B| v5p-128| 3.23e+02| 70.31%  \n128B| v5p-256| 3.15e+02| 68.68%  \n128B| v5p-512| 3.15e+02| 68.53%  \n256B| v5p-1024| 3.16e+02| 68.82%  \n512B| v5p-1024| 2.94e+02| 63.99%  \n1024B| v5p-2048| 2.49e+02| 64.05%  \n1024B| v5p-4096| 2.97e+02| 64.80%  \n1160B| v5p-7680| 2.95e+02| 64.27%  \n1160B| v5p-12288| 3.04e+02| 66.23%  \n  \n## TPU v5e\n\nFor 16B, 32B, 64B, and 128B models. See full run configs in\nMaxText/configs/v5e/ as 16b.sh, 32b.sh, 64b.sh, 128b.sh.\n\nHardware| 16B TFLOP/sec/chip| 16B MFU| 32B TFLOP/sec/chip| 32B MFU| 64B\nTFLOP/sec/chip| 64B MFU| 128B TFLOP/sec/chip| 128B MFU  \n---|---|---|---|---|---|---|---|---  \n1x v5e-256| 120| 61.10%| 132| 66.86%| 118| 59.90%| 110| 56.06%  \n2x v5e-256| 117| 59.37%| 128| 64.81%| 112| 56.66%| 110| 55.82%  \n4x v5e-256| 117| 59.14%| 126| 64.10%| 110| 55.85%| 108| 54.93%  \n8x v5e-256| 115| 58.27%| 125| 63.67%| 108| 54.96%| 104| 52.93%  \n16x v5e-256| 111| 56.56%| 123| 62.26%| 105| 53.29%| 100| 50.86%  \n32x v5e-256| 108| 54.65%| 119| 60.40%| 99| 50.18%| 91| 46.25%  \n  \n# Comparison to Alternatives\n\nMaxText is heavily inspired by MinGPT/NanoGPT, elegant standalone GPT\nimplementations written in PyTorch and targeting Nvidia GPUs. MaxText is more\ncomplex, supporting more industry standard models and scaling to tens of\nthousands of chips. Ultimately MaxText has an MFU more than three times the\n17% reported most recently with that codebase, is massively scalable and\nimplements a key-value cache for efficient auto-regressive decoding.\n\nMaxText is more similar to Nvidia/Megatron-LM, a very well tuned LLM\nimplementation targeting Nvidia GPUs. The two implementations achieve\ncomparable MFUs. The difference in the codebases highlights the different\nprogramming strategies. MaxText is pure Python, relying heavily on the XLA\ncompiler to achieve high performance. By contrast, Megatron-LM is a mix of\nPython and CUDA, relying on well-optimized CUDA kernels to achieve high\nperformance.\n\nMaxText is also comparable to Pax. Like Pax, MaxText provides high-performance\nand scalable implementations of LLMs in Jax. Pax focuses on enabling powerful\nconfiguration parameters, enabling developers to change the model by editing\nconfig parameters. By contrast, MaxText is a simple, concrete implementation\nof various LLMs that encourages users to extend by forking and directly\nediting the source code.\n\n# Features and Diagnostics\n\n## Collect Stack Traces\n\nWhen running a Single Program, Multiple Data (SPMD) job on accelerators, the\noverall process can hang if there is any error or any VM hangs/crashes for\nsome reason. In this scenario, capturing stack traces will help to identify\nand troubleshoot the issues for the jobs running on TPU VMs.\n\nThe following configurations will help to debug a fault or when a program is\nstuck or hung somewhere by collecting stack traces. Change the parameter\nvalues accordingly in MaxText/configs/base.yml:\n\n  1. Set collect_stack_trace: True to enable collection of stack traces on faults or when the program is hung. This setting will periodically dump the traces for the program to help in debugging. To disable this, set collect_stack_trace: False.\n  2. Set stack_trace_to_cloud: False to display stack traces on console. stack_trace_to_cloud: True will create a temporary file in /tmp/debugging in the TPUs to store the stack traces. There is an agent running on TPU VMs that will periodically upload the traces from the temporary directory to cloud logging in the gcp project. You can view the traces in Logs Explorer on Cloud Logging using the following query:\n\n    \n    \n    logName=\"projects/<project_name>/logs/tpu.googleapis.com%2Fruntime_monitor\" jsonPayload.verb=\"stacktraceanalyzer\"\n\n  3. stack_trace_interval_seconds signifies the duration in seconds between each stack trace collection event. Setting stack_trace_interval_seconds: 600 will collect the stack traces every 600 seconds (10 minutes).\n\nHere is the related PyPI package: https://pypi.org/project/cloud-tpu-\ndiagnostics.\n\n## Ahead of Time Compilation (AOT, tpu-only)\n\nTo compile your training run ahead of time, we provide a tool\ntrain_compile.py. This tool allows you to compile the main train_step in\ntrain.py for target hardware (e.g. a large number of v5e devices) without\nusing the target hardware, and instead you may use only a CPU or a single VM\nfrom a different family. This compilation helps with two main goals:\n\n  * It will flag any out of memory (OOM) information, such as when the per_device_batch_size is set too high, with an identical OOM stack trace as if it was compiled on the target hardware.\n\n  * The ahead of time compilation can be saved and then loaded for fast startup and restart times on the target hardware.\n\nThe tool train_compile.py is tightly linked to train.py and uses the same\nconfiguration file configs/base.yml. Although you don't need to run on a TPU,\nyou do need to install jax[tpu] in addition to other dependencies, so we\nrecommend running setup.sh to install these if you have not already done so.\n\n### Example AOT 1: Compile ahead of time basics\n\nAfter installing the dependencies listed above, you are ready to compile ahead\nof time:\n\n    \n    \n    # Run the below on a single machine, e.g. a CPU python3 MaxText/train_compile.py MaxText/configs/base.yml compile_topology=v5e-256 compile_topology_num_slices=2 \\ global_parameter_scale=16 per_device_batch_size=4\n\nThis will compile a 16B parameter MaxText model on 2 v5e pods.\n\n### Example AOT 2: Save compiled function, then load and run it\n\nHere is an example that saves then loads the compiled train_step, starting\nwith the save:\n\nStep 1: Run AOT and save compiled function\n\n    \n    \n    # Run the below on a single machine, e.g. a CPU export LIBTPU_INIT_ARGS=\"--xla_enable_async_all_gather=true\" python3 MaxText/train_compile.py MaxText/configs/base.yml compile_topology=v5e-256 \\ compile_topology_num_slices=2 \\ compiled_trainstep_file=my_compiled_train.pickle global_parameter_scale=16 \\ per_device_batch_size=4 steps=10000 learning_rate=1e-3\n\nStep 2: Run train.py and load the compiled function\n\nTo load the compiled train_step, you just need to pass\ncompiled_trainstep_file=my_compiled_train.pickle into train.py:\n\n    \n    \n    # Run the below on each host of the target hardware, e.g. each host on 2 slices of v5e-256 export LIBTPU_INIT_ARGS=\"--xla_enable_async_all_gather=true\" python3 MaxText/train.py MaxText/configs/base.yml run_name=example_load_compile \\ compiled_trainstep_file=my_compiled_train.pickle \\ global_parameter_scale=16 per_device_batch_size=4 steps=10000 learning_rate=1e-3 \\ base_output_directory=gs://my-output-bucket dataset_path=gs://my-dataset-bucket\n\nIn the save step of example 2 above we included exporting the compiler flag\nLIBTPU_INIT_ARGS and learning_rate because those affect the compiled object\nmy_compiled_train.pickle. The sizes of the model (e.g. global_parameter_scale,\nmax_sequence_length and per_device_batch) are fixed when you initially compile\nvia compile_train.py, you will see a size error if you try to run the saved\ncompiled object with different sizes than you compiled with. However a subtle\nnote is that the learning rate schedule is also fixed when you run\ncompile_train - which is determined by both steps and learning_rate. The\noptimizer parameters such as adam_b1 are passed only as shaped objects to the\ncompiler - thus their real values are determined when you run train.py, not\nduring the compilation. If you do pass in different shapes (e.g.\nper_device_batch), you will get a clear error message reporting that the\ncompiled signature has different expected shapes than what was input. If you\nattempt to run on different hardware than the compilation targets requested\nvia compile_topology, you will get an error saying there is a failure to map\nthe devices from the compiled to your real devices. Using different XLA flags\nor a LIBTPU than what was compiled will probably run silently with the\nenvironment you compiled in without error. However there is no guaranteed\nbehavior in this case; you should run in the same environment you compiled in.\n\n## Automatically Upload Logs to Vertex Tensorboard\n\nMaxText supports automatic upload of logs collected in a directory to a\nTensorboard instance in Vertex AI. Follow user guide to know more.\n\n## About\n\nA simple, performant and scalable Jax LLM!\n\n### Topics\n\ngpt large-language-models llm\n\n### Resources\n\nReadme\n\n### License\n\nApache-2.0 license\n\n### Code of conduct\n\nCode of conduct\n\n### Security policy\n\nSecurity policy\n\nActivity\n\nCustom properties\n\n### Stars\n\n991 stars\n\n### Watchers\n\n19 watching\n\n### Forks\n\n183 forks\n\nReport repository\n\n## Releases\n\n1 tags\n\n## Packages 0\n\nNo packages published\n\n## Contributors 48\n\n\\+ 34 contributors\n\n## Languages\n\n  * Python 78.4%\n  * Shell 21.0%\n  * Dockerfile 0.6%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": true}
