{"aid": "40291780", "title": "Intent-tuned LLM router that selects the best LLM for a user query", "url": "https://github.com/pulzeai-oss/knn-router/tree/main/deploy/pulze-intent-v0.1", "domain": "github.com/pulzeai-oss", "votes": 10, "user": "fbnbr", "posted_at": "2024-05-07 21:35:04", "comments": 0, "source_title": "knn-router/deploy/pulze-intent-v0.1 at main \u00b7 pulzeai-oss/knn-router", "source_text": "knn-router/deploy/pulze-intent-v0.1 at main \u00b7 pulzeai-oss/knn-router \u00b7 GitHub\n\nSkip to content\n\n## Navigation Menu\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\npulzeai-oss / knn-router Public\n\n  * Notifications\n  * Fork 1\n  * Star 12\n\n/\n\n# pulze-intent-v0.1\n\n/\n\n## Directory actions\n\n## More options\n\n## Directory actions\n\n## More options\n\n## Latest commit\n\njeevb\n\nUpdate README.md (#8)\n\nMay 6, 2024\n\n71f796a \u00b7 May 6, 2024May 6, 2024\n\n## History\n\nHistory\n\n/\n\n# pulze-intent-v0.1\n\n/\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n### parent directory\n\n..  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| Add pulze-intent-v0.1 example (#7)| May 4, 2024  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update README.md (#8)| May 6, 2024  \n  \n### docker-compose.yml\n\n|\n\n### docker-compose.yml\n\n| Add pulze-intent-v0.1 example (#7)| May 4, 2024  \n  \n### k8s.yaml\n\n|\n\n### k8s.yaml\n\n| Add pulze-intent-v0.1 example (#7)| May 4, 2024  \n  \n## README.md\n\n# pulze-intent-v0.1 (model, dataset)\n\nIntent-tuned LLM router that selects the best LLM for a user query.\n\n## Usage\n\n### Local\n\nFetch artifacts from Huggingface:\n\n    \n    \n    huggingface-cli download pulze/intent-v0.1 --local-dir .dist --local-dir-use-symlinks=False\n\nStart the services:\n\n    \n    \n    docker compose up -d --build\n    \n    \n    curl -s 127.0.0.1:8888/ \\ -X POST \\ -d '{\"query\":\"give me instructions for making ramen at home\"}' \\ -H 'Content-Type: application/json' | jq .\n\nOutput:\n\n    \n    \n    { \"hits\": [ { \"id\": \"0c571369-e985-41e1-b14b-3620c4bb40b5\", \"category\": \"writing_cooking_recipe\", \"similarity\": 0.8069034 }, { \"id\": \"9f44d3c0-95f5-43cc-a881-6e23adf9c68b\", \"category\": \"writing_cooking_recipe\", \"similarity\": 0.778615 }, { \"id\": \"21292586-73f3-4bf7-9ada-ae6917d4cd74\", \"category\": \"writing_cooking_recipe\", \"similarity\": 0.77417636 }, { \"id\": \"edd39535-f9b7-4188-a56e-055846d0ba23\", \"category\": \"writing_cooking_recipe\", \"similarity\": 0.772714 }, { \"id\": \"3cc563e1-1816-4ff1-8d2e-60fb9392c6de\", \"category\": \"writing_cooking_recipe\", \"similarity\": 0.76833653 }, { \"id\": \"15c9e10f-d217-418a-b367-a16e3cd3a541\", \"category\": \"writing_cooking_recipe\", \"similarity\": 0.76015425 }, { \"id\": \"ad33a141-269f-4a88-b99c-456cf67d9221\", \"category\": \"writing_cooking_recipe\", \"similarity\": 0.75983727 }, { \"id\": \"d6ee2a78-3b7a-44d6-9778-adc1e1f9a3db\", \"category\": \"writing_cooking_recipe\", \"similarity\": 0.75918543 }, { \"id\": \"afa1f32e-e69e-4d75-9a73-7a11b0259a24\", \"category\": \"writing_cooking_recipe\", \"similarity\": 0.7565732 }, { \"id\": \"5a569973-3934-49f6-8901-11f6b490a6cd\", \"category\": \"writing_cooking_recipe\", \"similarity\": 0.7564193 } ], \"scores\": [ { \"target\": \"gpt-3.5-turbo-0125\", \"score\": 0.83 }, { \"target\": \"command-r-plus\", \"score\": 0.93 }, { \"target\": \"llama-3-70b-instruct\", \"score\": 0.95 }, { \"target\": \"gpt-4-turbo-2024-04-09\", \"score\": 0.96 }, { \"target\": \"dbrx-instruct\", \"score\": 0.91 }, { \"target\": \"mixtral-8x7b-instruct\", \"score\": 0.91 }, { \"target\": \"mistral-small\", \"score\": 0.9 }, { \"target\": \"mistral-large\", \"score\": 0.91 }, { \"target\": \"mistral-medium\", \"score\": 0.89 }, { \"target\": \"claude-3-opus-20240229\", \"score\": 0.91 }, { \"target\": \"claude-3-sonnet-20240229\", \"score\": 0.9 }, { \"target\": \"command-r\", \"score\": 0.88 }, { \"target\": \"claude-3-haiku-20240307\", \"score\": 0.89 } ] }\n\n### Kubernetes\n\nSee this example.\n\n## Models\n\n  * claude-3-haiku-20240307\n  * claude-3-opus-20240229\n  * claude-3-sonnet-20240229\n  * command-r\n  * command-r-plus\n  * dbrx-instruct\n  * gpt-3.5-turbo-0125\n  * gpt-4-turbo-2024-04-09\n  * llama-3-70b-instruct\n  * mistral-large\n  * mistral-medium\n  * mistral-small\n  * mixtral-8x7b-instruct\n\n## Data\n\n### Prompts and Intent Categories\n\nPrompt and intent categories are derived from the GAIR-NLP/Auto-J scenario\nclassification dataset.\n\nCitation:\n\n    \n    \n    @article{li2023generative, title={Generative Judge for Evaluating Alignment}, author={Li, Junlong and Sun, Shichao and Yuan, Weizhe and Fan, Run-Ze and Zhao, Hai and Liu, Pengfei}, journal={arXiv preprint arXiv:2310.05470}, year={2023} }\n\n### Response Evaluation\n\nCandidate model responses were evaluated pairwise using\nopenai/gpt-4-turbo-2024-04-09, with the following prompt:\n\n    \n    \n    You are an expert, impartial judge tasked with evaluating the quality of responses generated by two AI assistants. Think step by step, and evaluate the responses, <response1> and <response2> to the instruction, <instruction>. Follow these guidelines: - Avoid any position bias and ensure that the order in which the responses were presented does not influence your judgement - Do not allow the length of the responses to influence your judgement - a concise response can be as effective as a longer one - Consider factors such as adherence to the given instruction, helpfulness, relevance, accuracy, depth, creativity, and level of detail - Be as objective as possible Make your decision on which of the two responses is better for the given instruction from the following choices: If <response1> is better, use \"1\". If <response2> is better, use \"2\". If both answers are equally good, use \"0\". If both answers are equally bad, use \"0\". <instruction> {INSTRUCTION} </instruction> <response1> {RESPONSE1} </response1> <response2> {RESPONSE2} </response2>\n\nEach pair of models is subject to 2 matches, with the positions of the\nrespective responses swapped in the evaluation prompt. A model is considered a\nwinner only if it wins both matches.\n\nFor each prompt, we then compute Bradley-Terry scores for the respective\nmodels using the same method as that used in the LMSYS Chatbot Arena\nLeaderboard. Finally, we normalize all scores to a scale from 0 to 1 for\ninteroperability with other weighted ranking systems.\n\n## Model\n\nThe embedding model was generated by first fine-tuning BAAI/bge-base-en-v1.5\nwith the intent categories from the dataset above, using contrastive learning\nwith cosine similarity loss, and subsequently merging the resultant model with\nthe base model at a 3:2 ratio.\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": true}
