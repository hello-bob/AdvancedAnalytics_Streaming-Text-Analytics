{"aid": "40131954", "title": "Picking the Right Vector Index for Your Project", "url": "https://thesequence.substack.com/p/guest-post-choosing-the-right-vector", "domain": "thesequence.substack.com", "votes": 1, "user": "stephen37", "posted_at": "2024-04-23 13:50:54", "comments": 0, "source_title": "\ud83d\udcdd Guest Post: Choosing the Right Vector Index For Your Project*", "source_text": "\ud83d\udcdd Guest Post: Choosing the Right Vector Index For Your Project*\n\n# TheSequence\n\nShare this post\n\n#### \ud83d\udcdd Guest Post: Choosing the Right Vector Index For Your Project*\n\nthesequence.substack.com\n\n#### Discover more from TheSequence\n\nThe best source to stay up-to-date with the developments in the machine\nlearning, artificial intelligence, and data science world. Trusted by 165,000\nprofessionals from the main AI labs, universities, and enterprises\n\nOver 169,000 subscribers\n\nContinue reading\n\nSign in\n\n# \ud83d\udcdd Guest Post: Choosing the Right Vector Index For Your Project*\n\nJun 26, 2023\n\n27\n\nShare this post\n\n#### \ud83d\udcdd Guest Post: Choosing the Right Vector Index For Your Project*\n\nthesequence.substack.com\n\nShare\n\nIn this post, Frank Liu. ML Architect at Zilliz, discusses vector databases\nand different indexing strategies for approximate nearest neighbor search. The\noptions mentioned include brute-force search, inverted file index, scalar\nquantization, product quantization, HNSW, and Annoy. Liu emphasizes the\nimportance of considering application requirements when choosing the\nappropriate index.\n\nVector databases are purpose-built databases meant to conduct approximate\nnearest neighbor search across large datasets of high-dimensional vectors\n(typically over 96 dimensions and sometimes over 10k). These vectors are meant\nto represent the semantics of unstructured data, i.e. data that cannot be fit\ninto traditional databases such as relational databases, wide-column stores,\nor document databases.\n\nConducting efficient similarity search requires a data structure known as a\nvector index. These indexes enable efficient traversal of the entire database;\nrather than having to perform brute-force search with each vector. There are a\nnumber of in-memory vector search algorithms and indexing strategies available\nto you on your vector search journey. Here's a quick summary of each:\n\n###\n\nBrute-force search (`FLAT`)\n\nBrute-force search, also known as \"flat\" indexing, is an approach that\ncompares the query vector with every other vector in the database. While it\nmay seem naive and inefficient, flat indexing can yield surprisingly good\nresults for small datasets, especially when parallelized with accelerators\nlike GPUs or FPGAs.\n\n###\n\nInverted file index (`IVF`)\n\nIVF is a partition-based indexing strategy that assigns all database vectors\nto the partition with the closest centroid. Cluster centroids are determined\nusing unsupervised clustering (typically k-means). With the centroids and\nassignments in place, we can create an inverted index, correlating each\ncentroid with a list of vectors in its cluster. IVF is generally a solid\nchoice for small- to medium-size datasets.\n\n###\n\nScalar quantization (`SQ`)\n\nScalar quantization converts floating-point vectors (typically float32 or\nfloat64) into integer vectors by dividing each dimension into bins. The\nprocess involves:\n\n  * Determining the maximum and minimum values of each dimension.\n\n  * Calculating start values and step sizes.\n\n  * Performing quantization by subtracting start values and dividing by step sizes.\n\nThe quantized dataset typically uses 8-bit unsigned integers, but lower values\n(5-bit, 4-bit, and even 2-bit) are also common.\n\n###\n\nProduct quantization (`PQ`)\n\nScalar quantization disregards distribution along each vector dimension,\npotentially leading to underutilized bins. Product quantization (PQ) is a more\npowerful alternative that performs both compression and reduction: high-\ndimensional vectors are mapped to low-dimensional quantized vectors assigning\nfixed-length chunks of the original vector to a single quantized value. `PQ`\ntypically involves splitting vectors, applying k-means clustering across all\nsplits, and converting centroid indices.\n\n###\n\nHierarchical Navigable Small Worlds (`HNSW`)\n\nHNSW is the most commonly used vectoring indexing strategy today. It combines\ntwo concepts: skip lists and Navigable Small Worlds (NSWs). Skip lists are\neffectively layered linked lists for faster random access (O(log n)` for skip\nlists vs. O(n) for linked lists). In HNSW, we create a hierarchical graph of\nNSWs. Searching in HNSW involves starting at the top layer and moving toward\nthe nearest neighbor in each layer until we find the closest match. Inserts\nwork by finding the nearest neighbor and adding connections.\n\n###\n\nApproximate Nearest Neighbors Oh Yeah (`Annoy`)\n\nAnnoy is a tree-based index that uses binary search trees as its core data\nstructure. It partitions the vector space recursively to create a binary tree,\nwhere each node is split by a hyperplane equidistant from two randomly\nselected child vectors. The splitting process continues until leaf nodes have\nfewer than a predefined number of elements. Querying involves iteratively the\ntree to determine which side of the hyperplane the query vector falls on.\n\nDon't worry if some of these summaries feel a bit obtuse. Vector search\nalgorithms can be fairly complex but are often easier to explain with\nvisualizations and a bit of code.\n\n##\n\nPicking a vector index\n\nSo how exactly do we choose the right vector index? This is a fairly open-\nended question, but one key principle to remember is that the right index will\ndepend on your application requirements. For example: are you primarily\ninterested in query speed (with a static database), or will your application\nrequire a lot of inserts and deletes? Do you have any constraints on your\nmachine type, such as limited memory or CPU? Or perhaps the domain of data\nthat you'll be inserting will change over time? All of these factors\ncontribute to the most optimal index type to use.\n\nHere are some guidelines to help you choose the right index type for your\nproject:\n\n100% recall: This one is fairly simple - use FLAT search if you need 100%\naccuracy. All efficient data structures for vector search perform\n_approximate_ nearest neighbor search, meaning that there's going to be a loss\nof recall once the index size hits a certain threshold.\n\nindex_size < 10MB: If your total index size is tiny (fewer than 5k\n512-dimensional `float32` vectors), just use FLAT search. The overhead\nassociated with index building, maintenance, and querying is simply not worth\nit for a tiny dataset.\n\n10MB < index_size < 2GB: If your total index size is small (fewer than 100k\n512-dimensional float32 vectors), my recommendation is to go with a standard\ninverted-file index (e.g. IVF). An inverted-file index can reduce the search\nscope by around an order of magnitude while still maintaining fairly high\nrecall.\n\n2GB < index_size < 20GB: Once you reach a mid-size index (fewer than 10M\n512-dimensional float32 vectors), you'll want to start considering other PQ\nand HNSW index types. Both will give you reasonable query speed and\nthroughput, but PQ allows you to use significantly less memory at the expense\nof low recall, while HNSW often gives you 95%+ recall at the expense of high\nmemory usage - around 1.5x the total size of your index. For dataset sizes in\nthis range, composite IVF indexes (IVF_SQ, IVF_PQ) can also work well, but I\nwould use them only if you have limited compute resources.\n\n20GB < index_size < 200GB: For large datasets (fewer than 100M 512-dimensional\nfloat32 vectors), I recommend the use of _composite indexes_: IVF_PQ for\nmemory-constrained applications and HNSW_SQ for applications that require high\nrecall. A composite index is an indexing technique combining multiple vector\nsearch strategies into a single index. This technique effectively combines the\nbest of both indexes; HNSW_SQ, for example, retains most of HNSW's base query\nspeed and throughput but with a significantly reduced index size. We won't\ndive too deep into composite indexes here, but FAISS's documentation provides\na great overview for those interested.\n\nOne last note on Annoy - we don't recommend using it simply because it fits\ninto a similar category as HNSW since, generally speaking, it is less\nperformant. Annoy is the most uniquely named index, so it gets bonus points\nthere.\n\n##\n\nA word on disk indexes\n\nAnother option we haven't dove into explicitly in this blog post is disk-based\nindexes. In a nutshell, disk-based indexes leverage the architecture of NVMe\ndisks by colocating individual search subspaces into their own NVMe page. In\nconjunction with zero seek latency, this enables efficient storage of both\ngraph- and tree-based vector indexes.\n\nThese index types are becoming increasingly popular since they enable the\nstorage and search of billions of vectors on a single machine while\nmaintaining a reasonable performance level. The downside to disk-based indexes\nshould be obvious as well. Because disk reads are significantly slower than\nRAM reads, disk-based indexes often experience increased query latencies,\nsometimes by over 10x! If you are willing to sacrifice latency and throughput\nfor the ability to store billions of vectors at minimal cost, disk-based\nindexes are the way to go. Conversely, if your application requires high\nperformance (often at the expense of increased compute costs), you'll want to\nstick with IVF_PQ or HNSW_SQ.\n\n##\n\nWrapping up\n\nIn this post, we covered some of the vector indexing strategies available.\nGiven your data size and compute limitations, we provided a simple flowchart\nto help determine the optimal strategy. Please note that this flowchart is a\ngeneral guideline, not a hard-and-fast rule. Ultimately, you'll need to\nunderstand the strengths and weaknesses of each indexing option, as well as\nwhether a composite index can help you squeeze out the last bit of performance\nyour application needs. All of these index types are freely available to you\nin Milvus, so you can experiment as you see fit. Go out there and experiment!\n\nTo learn more, register to join the upcoming Zilliz webinar: Vector Search\nBest Practices on July 13 where we will cover more details on vector index\nselection. See you then!\n\nSAVE YOUR SPOT\n\n###### *This post was written by Frank Liu, ML Architect at Zilliz,\nexclusively for TheSequence. We thank Zilliz for their ongoing support of\nTheSequence.\n\n27 Likes\n\n27\n\nShare this post\n\n#### \ud83d\udcdd Guest Post: Choosing the Right Vector Index For Your Project*\n\nthesequence.substack.com\n\nShare\n\nComments\n\nInside Orca 2: Microsoft's Small Language Model that Outperforms Models 10x\nLarger in Reasoning Capabilities\n\nThe model innovating in the training procedures to improve reasoning abilities\nin small language models.\n\nDec 28, 2023\n\n274\n\nShare this post\n\n#### Inside Orca 2: Microsoft's Small Language Model that Outperforms Models\n10x Larger in Reasoning Capabilities\n\nthesequence.substack.com\n\nEdge 361: LLM Reasoning with Graph of Thoughts\n\nNot chains or trees but graph structures for LLM reasoning.\n\nJan 16\n\n217\n\nShare this post\n\n#### Edge 361: LLM Reasoning with Graph of Thoughts\n\nthesequence.substack.com\n\nEdge 357: Understanding Chain-of-Thought Prompting\n\nA deep dive into the most popular LLM reasoning technique.\n\nJan 2\n\n206\n\nShare this post\n\n#### Edge 357: Understanding Chain-of-Thought Prompting\n\nthesequence.substack.com\n\nReady for more?\n\n\u00a9 2024 Jesus Rodriguez\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
