{"aid": "40173708", "title": "Building a team of internal R packages", "url": "https://www.emilyriederer.com/post/team-of-packages/", "domain": "emilyriederer.com", "votes": 1, "user": "asiftr", "posted_at": "2024-04-26 20:14:07", "comments": 0, "source_title": "Building a team of internal R packages", "source_text": "Building a team of internal R packages | Emily Riederer\n\nWe use cookies\n\nWe use cookies and other tracking technologies to improve your browsing\nexperience on our website, to show you personalized content and targeted ads,\nto analyze our website traffic, and to understand where our visitors are\ncoming from.\n\n# Building a team of internal R packages\n\nOn the jobs-to-be-done and design principles for internal tools\n\nrstats\n\npkgdev\n\njtbd\n\nAuthor\n\nEmily Riederer\n\nPublished\n\nJanuary 21, 2021\n\nNote: this post is a written version of my rstudio::global 2020 talk on the\nsame topic. Please see the link for the slides and video version. I do\nelaborate on a few points here that I cut from the talk; if you\u2019ve already\nwatched the talk and just want the delta, please see the sections in blue\n\nMore and more organizations are beginning to write their own internal R\npackages. These internal tools have great potential to improve an\norganization\u2019s code quality, promote reproducible analysis frameworks, and\nenhance knowledge management. Developers of such tools are often inspired by\ntheir favorite open-source tools and, consequently, rely on the design\npatterns and best practices they have observed. Although this is a good\nstarting position, internal packages have unique challenges (such as a smaller\ndeveloper community) and opportunities (such as an intimate understanding of\nthe problem space and over-arching organizational goals). Internal packages\ncan realize their full potential by engineering to embrace these unique\nfeatures. In this post, I explore the jobs of internal packages and the types\nof different design decisions these jobs can inspire \u2013 from API design and\nerror handling to documentation to training and testing. If you\u2019d rather just\nread the main ideas instead of the full essay, skip to the tl;dr\n\n## What differentiates internal packages\n\nTo begin, think about the last time that you joined a new organization. There\nwas so much you had to learn before you could get started \u2013 accessing data,\nintuiting what problems are worth tackling, understanding team norms, and so\nmuch more. Thankfully, we only have to descend this learning curve once.\nHowever, the off-the-shelf tools we use can\u2019t preserve this context and\nculture. Every day is like their first day at work.\n\nIn open-source, this high level of abstraction is a feature, not a bug.\nStripping context from code enables reuse and collaboration across the globe.\nThe survival package, for example, need not care whether one is modeling the\ntime-to-death in a clinical trial or the time-to-attrition of a Netflix\nsubscriber. In contrast, internal packages can drive additional value if they\nact more like a colleague and embrace institutional knowledge.\n\nContrasting internal and external packages on two dimensions illuminates this\ndifference.\n\nFirst, internal packages can target more domain-specific and concrete problems\nthan open-source packages. Inherently, within an institution, such tools can\nhave a narrower problem definition without loss of useful generality. This\nmeans our functions can be more tailored to the contours and corner cases of\nour problems and our documentation can use more relevant examples.\n\nSecond, and perhaps less intuitively, internal packages can actually span a\nbroader solution space, as measured by the number of steps in the workflow\nthat they span. For example, an open source package might offer many different\nmethods to model time-to-event but remain agnostic to where the input data\ncame from or what the analyst does with the results. In contrast, an internal\npackage might cater to answering a narrower question but apply its insight\ninto our organization to cover more of the steps in the workflow of answering\nthat questions \u2013 such as pulling data, engineering features, and communicating\noutcomes.\n\nBecause of these two factors, internal packages can make large contributions\nto our organization by addressing different needs such as:\n\n  * Utilities: Providing an abstraction layer over internal infrastructure, such as connecting to databases, APIs, servers or enabling proxies\n  * Analysis: Guiding analysts through a curated set of methods and packages to answer common questions\n  * Developer Tools: Helping analysts produce better outputs faster with building blocks like ggplot themes, R Markdown templates, and Shiny modules\n\nBut it\u2019s not just what internal packages do that distinguishes them from open-\nsource tools. Even more critical is how these packages are designed to do\nthese tasks. In the rest of this post, I\u2019ll illustrate how internal packages\ncan be conscientiously engineered to act more like a veteran than a new hire.\n\n## The theory of jobs-to-be-done\n\nTo motivate the design goals and decisions we\u2019ll discuss, it\u2019s useful to think\nabout Harvard Business School Professor Clayton Christensen\u2019s jobs-to-be-done\ntheory of disruptive innovation. This asserts that\n\n> we hire a product to do a job that helps us make progress towards a goal\n\nand sometimes^1 the theory further asserts that\n\n> these jobs can have functional, social, and emotional components\n\nIn the world of product development, that focus on the progress a customer is\na critical distinction from the conventional understanding of competition and\nindustry makeup. For example, if I ask you who Twitter\u2019s competitors are, you\nmight first think of Facebook, Instagram, or TikTok \u2013 other entrants in the\ncategory of \u201cmobile apps for social networking\u201d. However, if we think about\nthe jobs that I \u201chire\u201d Twitter to do, I might \u201chire\u201d Twitter to help me pass\nthe time while I wait in a long line or (back in \u201cthe olden days\u201d) ride my\nlong commute to work. Through that lens, it not longer matters what industry\nTwitter is nominally in; this is about me and my needs - not Twitter. So,\nTwitter is \u201ccompeting\u201d for the position of my travel companion against all\nsorts of other potential hires like Spotify, podcasts, books, or even an extra\n20 minutes nap on the train.\n\nIn relation to R packages, open source packages know a task we want done (e.g.\n\u201cfit a Cox proportional hazard model\u201d) which is somewhat like a product\ncategory. But what really sets internal packages apart is that they can use\ntheir knowledge of our organization to cater to the kind of progress that we\ntruly want to make.\n\nIt\u2019s hard for passionate R users^2 to imagine, but no one actually wants a\npackage. I\u2019d even go as far to say that no one even wants data (blasphemy!).\nOrganizations need progress \u2013 they need strategies, fueled by decisions,\nfueled by answers to questions, and fueled, in turn and in part, by data sets\nand tools. Since internal tools have a better sense of what that key bit of\nprogress is, they can be more focused on helping us get there.\n\nI claim that we can make internal tools more useful and more appealing by\ntargeting them towards the specific jobs of an organization. To complete the\nanalogy, I\u2019ll restate the initial theory with a few alterations\n\n> let\u2019s build a team of packages that can do the jobs that help our\n> organization make answer impactful questions with efficient workflows\n\nSo how does jobs-to-be-done inform our package design? To explore this, we\u2019ll\nconsider what makes good teammates and how we can encode those traits in our\ntools.\n\n## The IT Guy - Abstraction\n\nFirst, let\u2019s meet the IT Guy. IT and DevOps colleagues complement the skills\nof data analysts by handling all of the things that analysts are generally not\nthe most skilled or experienced at doing independently \u2013 production systems,\nservers, deployments, etc. In that abstraction process, they also take on\nadditional responsibilities to promote good practices like data security and\ncredential management. Ideally, they can save us time and frustration in\nnavigating organization-specific roadblocks than no amount of searching on\nStackOverflow can help.\n\nPut another way, the IT Guy fills the jobs of\n\n  * Functional: Abstracting away confusing quirks of infrastructure\n  * Social: Promoting or enforcing good practices\n  * Emotional: Avoiding frustrating or stress of lost time or \u201csilly\u201d questions\n\nWe can emulate these characteristics in internal packages by including utility\nfunctions, taking an opinionated stance on design, and providing helpful and\ndetailed error messages.^3\n\nAs an example, let\u2019s consider a simple function to connect to an\norganization\u2019s internal database.^4 First, we might start out with a rather\nboilerplate piece of code using the DBI package. We take in the username and\npassword; hard-code the driver name, server location, and port; and return a\nconnection object. (Note that the content of this example doesn\u2019t matter. The\njobs of the IT Guy prototype are abstracting away things we don\u2019t need to\nthink about and protecting us from anti-patterns \u2013 not just connecting to\ndatabases. The design patterns shown will apply to the more general problem.)\n\n    \n    \n    get_database_conn <- function(username, password) { conn <- DBI::dbConnect( drv = odbc::odbc(), driver = \"driver name here\", server = \"server string here\", UID = username, PWD = password, port = \"port number here\" ) return(conn) }\n\n### Opinionated Design\n\nNow, let\u2019s suppose our organization has strict rules against putting secure\ncredentials in plain text. (Let\u2019s actually hope they have such rules!) To\nenforce this, we can remove username and password from the function header,\nand use Sys.getenv() inside of the function to retrieve specifically named\nenvironment variables (DB_USER and DB_PASS) containing these quantities.^5\n\nIn an open source package, I would not presume to force users\u2019 hand to use one\nspecific system set-up. However, in this case, we can make strong assumptions\nbased on our knowledge of an organization\u2019s rules and norms. And this sort of\nfunction can be great leverage to incentivize users to do it right (like\nstoring their credentials in environment variables) because there\u2019s only one\nway it can work.\n\n    \n    \n    get_database_conn <- function() { conn <- DBI::dbConnect( drv = odbc::odbc(), driver = \"driver name here\", server = \"server string here\", UID = Sys.getenv(\"DB_USER\"), PWD = Sys.getenv(\"DB_PASS\"), port = \"port number here\" ) return(conn) }\n\n### Helpful Error Messages\n\nOf course, opinionated design is only helpful if we communicate those opinions\nto our users. Otherwise, they\u2019ll get an error that DB_PASS is missing and find\nnothing useful online to help them troubleshoot since this is a specific\ninternal choice that we made.\n\nSo, we can enhance the function with extremely custom and prescriptive error\nmessages explaining what went wrong and either how to fix it (e.g. setting an\nenvironment variable, applying for an access) or where one can get more\ninformation (in a vignette, asking someone on a specific support team,\nchecking out a specific document in a team wiki).\n\nSuch support messages can be made even more approachable with the use of the\ncli package which makes console messages more aesthetic, user-friendly, and\nclear in intent.\n\n    \n    \n    get_database_conn <- function() { if (any(Sys.getenv(c(\"DB_USER\", \"DB_PASS\")) == \"\")) { stop( \"DB_USER or DB_PASS environment variables are missing.\", \"Please read set-up vignette to configure your system.\" ) } conn <- DBI::dbConnect( drv = odbc::odbc(), driver = \"driver name here\", server = \"server string here\", UID = Sys.getenv(\"DB_USER\"), PWD = Sys.getenv(\"DB_PASS\"), port = \"port number here\" ) return(conn) }\n\n### Proactive Problem Solving\n\nOf course, even better than explaining errors is preventing them from\noccurring. We might also know at our specific organization, non alphanumeric\ncharacters are required in passwords and that DBI::dbConnect() does not\nnatively encode these correctly when passing them to the database. Instead of\ntroubling the users and telling them how to pick a password, we can instead\nhandle this silently by running the password retrieved from the DB_PASS\nenvironment variable through the URLencode() function.\n\n    \n    \n    get_database_conn <- function() { if (any(Sys.getenv(c(\"DB_USER\", \"DB_PASS\")) == \"\")) { stop( \"DB_USER or DB_PASS environment variables are missing.\", \"Please read set-up vignette to configure your system.\" ) } conn <- DBI::dbConnect( drv = odbc::odbc(), driver = \"driver name here\", server = \"server string here\", UID = Sys.getenv(\"DB_USER\"), PWD = URLencode(Sys.getenv(\"DB_PASS\"), reserved = TRUE), port = \"port number here\" ) return(conn) }\n\nAt this point, you might be wondering: \u201cIsn\u2019t proactive troubleshooting and\nhelpful error messages good standard practice?\u201d And, yes, these are things\nthat both external and internal packages should strive for. The differences\nwith internal R packages are that you have a tighter feedback loop with your\nusers and their systems. This has two advantages.\n\nFirst, you\u2019ll be quicker to learn about the spectrum of issues your users are\nencountering (trust me, they\u2019ll find you!), and proactively explaining or\nsolving those issues in the package saves you both a lot of time.\n\nSecondly, in a constrained environment, there are more predictable ways things\ncan go wrong, so you attempts at proactive problem solving and helpful error\nmessages have a better chance at spanning the spectrum of commonly occurring\nproblems.\n\n## The Junior Analyst - Proactive Problem-Solving\n\nOf course, strong opinions and complete independence don\u2019t always make a great\nteammate. Other times, we might want someone more like a junior analyst. They\nknow a good bit about our organization, and we can trust them to execute\ncalculations correctly and make reasonable assumptions. At the same time, we\nwant them to be responsive to feedback and willing to try out things in more\nways than one.\n\nMore concretely, a Junior Analyst fills the jobs:\n\n  * Functional: Performs rote tasks effectively and makes reasonable assumptions where needed\n  * Social: Communicates assumptions and eagerly adapts to feedback\n  * Emotional: Builds trust so you can focus on other things\n\nTo capture these jobs in our package, we can build proactive yet responsive\nfunctions by using default arguments, reserved keywords, and the ellipsis.\n\nTo illustrate, now let\u2019s imagine a basic visualization function that wraps\nggplot2 code but allows users to input their preferred x-axis, y-axis and\ngrouping variables to draw cohort curves. (Again, note that the content of\nthis example doesn\u2019t matter. The jobs of the junior analyst prototype are\nperforming rote tasks efficiently and flexibly \u2013 not visualizing cohort\ncurves, per say. The design patterns shown will apply to the general problem\nof assisting in analysis without overbearing assumptions.)\n\n    \n    \n    viz_cohort <- function(data, time, metric, group) { gg <- ggplot(data) + aes(x = .data[[time]], y = .data[[metric]], group = .data[[group]]) + geom_line() + my_org_theme() return(gg) }\n\nThis function is alright, but we can probably draw on institutional knowledge\nto make our junior analyst a bit more proactive.\n\n### Default Function Arguments\n\nIf we relied on the same opinionated design as the IT Guy, we might consider\nhard-coding some of the variables inside of the function. Here, though, that\nis not a great approach. The junior analyst\u2019s job is not to give edicts. We\nmight know what the desired x-axis will be 80% of the time, but hard-coding\nhere is a too-strong assumption even for an internal package and decreases the\nusefulness of the function in the other 20% of applications.\n\n    \n    \n    viz_cohort <- function(data, metric, group) { gg <- ggplot(data) + aes(x = .data[[\"MONTHS_SUBSCRIBED\"]], y = .data[[metric]], group = .data[[group]]) + geom_line() + my_org_theme() return(gg) }\n\nInstead, we can put our best-guess 80-percent-right names as the default\narguments in the function header \u2013 ordered by decreasing likelihood to\noverride. This means, when users do not provide their own value, a default\nvalue is used (that\u2019s the junior analyst\u2019s best-guess!), but users retain\ncomplete control to change it as they see fit.\n\n    \n    \n    viz_cohort <- function(data, metric = \"IND_ACTIVE\", time = \"MONTHS_SUBSCRIBED\", group = \"COHORT\") { gg <- ggplot(data) + aes(x = .data[[time]], y = .data[[metric]], group = .data[[group]]) + geom_line() + my_org_theme() return(gg) }\n\n### Reserved Keywords\n\nThis approach becomes even more powerful if we can abstract out a small set of\ncommonly occuring assumed variable names or other values. Then, we can define\nand document a set of \u201ckeywords\u201d or special variable names that span all of\nour internal packages. Following the example above, we might define a handful\nof reserved keywords such as TIME_SUBSCRIBED, CUSTOMER_COHORT, and\nCUSTOMER_SEGMENT.\n\nIf these are well-known and well-documented, users will then get into the\nhabit of shaping their data so it \u201cplays nice\u201d with the package ecosystem and\nsave a lot of manual typing. This type of \u201cincentive\u201d to standardize field\nnames can have other convenient consequences in making data extracts more\nsharable and code more readable.\n\nIn the spirit of the IT Guy\u2019s proactive problem-solving, we can also help\nusers catch potential errors caused by missing keywords. A few strategies here\nare to provide a function like validate_{package name}_data() to check that\nany required names exist or report out what is missing or to provide similar\non-the-fly validation inside of functions that expect reserved keyword\nvariable names to exist.\n\n    \n    \n    validate_mypackage_data <- function(vbl_names) { # validate variable names ---- required <- c(\"TIME_SUBSCRIBED\", \"CUSTOMER_COHORT\", \"CUSTOMER_SEGMENT\") not_present <- setdiff(required, vbl_names) # report missing / required names ---- if (length(not_present) == 0) {return(TRUE)} message(\"The following required variable(s) are missing: \", paste(not_present, collapse = \", \")) return(FALSE) }\n\n(An alternative approach to reserved keywords is to let users specify\nimportant variables by using package options(). However, I do not prefer this\napproach because it does not have the benefits of driving consistency, and it\nrequires more documentation and R knowledge. Most any user can grasp \u201c)\n\n### Ellipsis\n\nFinally, one other nice trick in making our functions responsive to feedback\nis the ellipsis (..., also called \u201cpassing the dots\u201d). This allows users to\nprovide any number of additional, arbitrary arguments beyond what was\nspecified by the developer and to plug them in at a designated place in the\nfunction body.\n\n    \n    \n    viz_cohort <- function(data, time = \"MONTHS_SUBSCRIBED\", metric = \"IND_ACTIVE\", group = \"COHORT\", ...) { gg <- ggplot(data) + aes(x = .data[[time]], y = .data[[metric]], group = .data[[group]]) + geom_line(aes(...)) + my_org_theme() return(gg) }\n\nThis way, users can extend functions based on needs that the developer could\nnot have anticipated, like customizing the color, size, and line type with\ncommands such as:\n\n    \n    \n    viz_cohort(my_org_data) viz_cohort(my_org_data, color = COHORT, linetype = COHORT)\n\nNote that the benefits of the ellipsis are very similar to the benefits of\nfunctional programming, in general, with small, modular, and composable\nfunctions that do not preserve state. This also provides users flexibility\nbecause they can continue to add on to our functions from the outside without\nhaving to modify the internals. For example, if users wanted to make our plot\nseparate by the quarter that different cohorts began, they could simply create\nan additional column with this information before calling our function and add\na facet_grid() call outside of viz_cohort().\n\n    \n    \n    my_org_data %>% mutate(COHORT_QUARTER = quarter(DATE_COHORT_START)) %>% viz_cohort(my_org_data) + facet_grid(rows = vars(COHORT_QUARTER))\n\n## The Tech Lead - Knowledge Management\n\nSo far, we have mostly focused on that first dimension of differentiation\nbetween internal and open-source tools \u2013 making our package teammates targeted\nto solving specific internal problems. But there\u2019s just as much value in that\nsecond dimension: using internal packages as a way to ship not just\ncalculations but workflows and share an understanding of how the broader\norganization operates. This allows our packages to play leadership and\nmentorship roles in our org. To illustrate this, consider our intrepid tech\nlead.\n\nWe value this type of teammate because they can draw from a breadth of past\nexperience and institutional knowledge to help you weigh trade-offs, learn\nfrom collected wisdom, and inspire you to do your best work.\n\n  * Functional: Coach you through weighing alternatives and debugging issues based on vast experience\n  * Social: Shares collected \u201ctribal\u201d knowledge\n  * Emotional: Breaks down barriers to help you do your best work\n\nOf course, all of that is a pretty tall order for a humble R package! But\nconscientious use of vignettes, R Markdown templates, and project templates\ncan do a lot to help us toward this goal.\n\n### Vignettes\n\nVignettes often help introduce the basic functionality of a package with a toy\nexample, as found in the dplyr vignette, or less commonly may discuss a\nstatistical method that\u2019s implemented, as done in the survival package.\nVignettes of internal R packages can do more diverse and numerous jobs by\ntackling comprehensive knowledge management. These vignettes can accumulate\nthe hard-won experience and domain knowledge like an experienced tech leads\u2019\nphotographic memory and hand-deliver them to anyone currently working on a\nrelated analysis.\n\nJust as a few examples, vignettes of an internal package might cover some of\nthe following topics:\n\n  * Conceptual Overview: Explain the problem(s) the package is trying to solve and how they help the organization make progress\n  * Workflow: Describe the workflow for answering the question For example, does it require data collection or use of existing data? what sort of models will be employed? what are the steps in in what order?\n  * Key Questions: What sort of questions should an analyst be asking themselves throughout the analysis? What assumptions are the methods making that they should verify? What are common \u201cgotchas\u201d?\n  * Process Documentation: What are organizational challenges to this problem (as opposed to quantiative ones)? What other systems might someone have to set up or teams to contact (e.g. if need to deploy a model or add features to a library)?\n  * Technical Documentation: How does the conceptual workflow map to different functions in the package? What sort of inputs do these functions require and what sort of outputs do they produce? Where should these inputs and outputs be stored?\n  * Method Comparison: When the package offers different options for solving specific problems, how should users evaluate these options and decide which is best? Are there quantitative measures? Examples from past experience?\n  * Lessons Learned: What has gone wrong when solving this type of problem before? What can be done better?\n  * Past Examples: Previous case studies of successful use of the package and the organizational progress it drove\n\nThese examples of potential vignettes further illustrate how our internal\npackages can help us better achieve progress. No one in an organization wants\na model for the sake of having a model; they want is to answer a question or\nsuggest an action. Open-source packages are relatively \u201cblind\u201d to this, so\ntheir vignettes will focus on getting to the model; internal packages can\npartner with us in understanding that \u201cmoment before\u201d and \u201cmoment after\u201d with\ntips on everything from featuring engineering to assessing the ethics and bias\nin a certain output.\n\nOf course, any organization can document their processes, and this\ndocumentation doesn\u2019t strictly have to live inside an R package. However, many\norganizations struggle with knowledge management. Including documentation in a\nrelated R package ensures that it\u2019s easily discoverable to the right people\n(those doing a related project), at the right time (when they are working on\nthat project). It also ensures that documentation won\u2019t be lost when any one\nperson changes teams, leaves the organization, or gets a new computer.\n\n### Package Websites\n\nIn fact, all of that context may be so helpful, even people who are not direct\nusers of your package may wish to seek its mentorship.\n\nIn this case, you can use the pkgdown package to automatically (honestly, you\njust run pkgdown::build_site()) create a package website to share these\nvignettes with anyone who needs to learn about a specific problem space \u2013\nwhether or not they will ultimately be the data analyst using your package to\nsolve the computational part of this quest for organizational progress. And,\nunlike their human counterparts, the packaged tech lead can always find time\nfor another meeting.\n\nLike other R Markdown output types, pkgdown creates basic HTML output. This\nmeans that you can host it on an internal server or (if you organization uses\nit) GitHub Pages in GitHub enterprise. Alternatively, you may even simply\nstore the HTML files in an internal shared drive and any users may open the\nsite on their own computer in their favorite browser.\n\n### R Markdown Templates\n\nSimilar to vignettes, embedded R Markdown templates take on a more important\nand distinctive role for internal packages.\n\nTemplates are a way to include prepopulated R Markdowns within your package,\nwhich users may access through File > New File > R Markdown > From Template in\nthe RStudio IDE or with the rmarkdown::draft() function. In open-source\npackages, R Markdown templates provide a pre-populated file instead of the\ndefault. This is most commonly used to demonstrate proper formatting syntax.\nFor example, the flexdashboard package use a template to show users how to set\nup their YAML metadata and section headers.\n\nInstead, internal packages can use templates to coach users through workflows\nbecause they understand the problem users are facing and the progress they\nhope to achieve. Internal packages can mentor users and structure their work\nin two different ways:\n\nProcess walk-throughs can serve as interactive notebooks that \u201ccoach\u201d users\nthrough common analyses. As an example, if a type of analysis requires manual\ndata cleaning and curation, a template notebook could guide users to ask the\nright questions of their data and generate the common views they need to\nanalyze.\n\nWe can also include full end-to-end analysis outlines which include\nplaceholder text, commentary, and code if the type of analysis that a package\nsupports usually results in a specific report.\n\n### Project Templates\n\nSimilarly, our package can include RStudio project templates. These templates\ncan predefine a standard file-structure and boilerplate set of files for a new\nproject to give users a helping hand and drive the kind of consistency across\nprojects that any tech lead dreams of when doing a code review. When any\npackage user is beginning a new analysis, they may click File > New Project in\nRStudio and select your custom project template among a number of options in\nthe IDE.\n\nFor example, one flexible file structure might be something like:\n\n  * analysis: RMarkdown files that constitute the final narrative output\n  * src: R scripts that contain useful helper functions or other set-up tasks (e.g. data pulls)\n  * data: Raw data - this folder should be considered \u201cread only\u201d!\n  * output: Intermediate data objects created in the analysis\n  * doc: Any long form documentation or set-up instructions\n  * ext: Any miscellaneous external files or presentation material collected or created throughout the analysis\n\nAdditionally, for a given problem that our package attempts to solve, we could\npre-populate this file structure with certain guaranteed files such as\ntemplate scripts for querying and accessing data, configuration files, some of\nthe template R Markdowns we mentions above, etc. The more that directories and\nfiles are standardized for a given problem type, the easier it is for anyone\nelse in our organization that has previously used our tool to easily\nunderstand our project.\n\nThe idea of a standardized file structure is one of the 10 principles called\nout in the wonderful Good Enough Practices for Scientific Computing and\npotentially one of the single highest leverage practices I\u2019ve found for\ndriving consistency and preserving sanity when building collaboratively on\nlarge teams.\n\n## The Project Manager - Coordination\n\nNow, we discussed how a package filling the \u201ctech lead\u201d role can help\nstructure our work with templates and how this can help collaboration between\nR users. But to think about collaboration more broadly, we must turn to our\nfinal teammate \u2013 the project manager.\n\nOne of the biggest differences between task-doing open-source packages versus\nproblem-solving internal packages is understanding the whole workflow and\nhelping coordinate projects across different components.\n\nIn short, a package can help project manage by addressing the jobs of\n\n  * Functional: Integrate work across colleagues and components\n  * Social: Help mediate conflict and find common ground\n  * Emotional: Meet individual teammates where they are at and remove barriers\n\nSpecifically, when writing open-source packages, we rightly tend to assume our\ntarget audience is R users, but on a true cross-functional team not everyone\nwill be, so we can intentionally modularize the workflow, define specific\ntasks, and augment RStudio\u2019s IDE to make sure our tools work well with all of\nour colleagues.\n\n### Modularization\n\nModularizing allows us to isolate parts of our workflow that do not really\nrequire R code to provide a lower barrier of entry for more of our colleagues\nto contribute.\n\nFor example, in the templates we just discussed, we could actually make\nseparate template components for parts that require R code and for commentary.\nThe commentary files could be plain vanilla markdown files that any\ncollaborator could edit without even having R installed, and main R Markdown\nfile can pull in this plaintext output using child documents. This approach is\nmade even easier with advances in the RStudio IDE like the visual markdown\neditor provides great, graphical UI support for word processing in markdown.\n\n### Project Planning\n\nPieces of code are not the only thing we can modularize for our users. Often,\na good bit of work in the early stages of a project is figuring out what the\nright steps are. This is emphasized in the project management book Shape Up\nwhich illustrates the flow of a project with \u201chill diagrams\u201d:\n\nOur \u201ctech lead\u201d documentation can already help document some of the learned\nproject steps and help speed up the \u201chill climbing\u201d. Additionally, a package\ncould play the role of the project manager and make those steps more tangible\nby embedding a project plan. One option for this is the projmgr package^6\nwhich allows for specification of main steps as a project as in a YAML file\nwhich can then be bulk uploaded to GitHub issues and milestones.\n\nTechnically, this would mean writing a project plan, saving it in the inst/\nfolder of your package, and perhaps writing a custom function to wrap\nsystem.file() to help users access the plan. Your function might look\nsomething like:\n\n    \n    \n    retrieve_{package name}_plan <- function(proj_type = c(\"a\", \"b\", \"c\")) { match.arg(proj_type) system.file(\"plan.yml\", package = \"{package name}\") }\n\nAlternatively, you could automate similar project plans yourself using the\npublic APIs for Jira, Trello, GitLab, or whatever project management solution\nyour organization uses.\n\n### IDE Support\n\nWe can also use RStudio Add-Ins to extend RStudio interface and ship\ninteractive widgets (imagine writing a \u201cbaby Shiny app\u201d) in our internal\npackages. For example, the esquisse package uses add-ins to ship a point-and-\nclick plot coding assistant. Add-ins should be used judiciously because they\nrequire more investment upfront, but they are much easier to maintain than a\nfull application and can help slowly convert more teammates to R users over\ntime. Besides, a good project manager is willing to go the extra mile to\nsupport their team.\n\nOne other way to \u201cgo the extra mile\u201d could also be to include an interactive\nlearnr tutorial as another way to help onboard new users. These tutorials are\nnow also viewable through the RStudio IDE\u2019s Tutorial pane.\n\n## Collaboration\n\nNow, speaking of collaboration, we\u2019ve talked about how packages can act like\nteam members such as the IT Guy, Analyst, Tech Lead, or Project Manager. But\nbeing a good teammate is about more than doing individual jobs; organizational\nprogress is made by working together. Another major opportunity when building\na suite of internal tools is that we have the unique opportunity to think\nabout how multiple packages on our team can interact most effectively.\n\nWe want teammates that are clear communicators, have defined responsibilities,\nand keep their promises. We can help our packages be good teammates with\nnaming conventions, clearly defined scopes, and careful attention to\ndependencies and testing.\n\n### Clear Communication - Naming Conventions\n\nClear function naming conventions and consistent method signatures help\npackages effectively communicate with both package and human collaborators.\n\nInternally, we can give our suite of internal packages a richer language by\ndefine a consistent set of prefix name stubs that indicate how each function\nis used. One approach is that each function prefix can denote the type of\nobject being returned (like \u201cviz\u201d for a ggplot object).^7\n\nThis sort of convention can reduce the cognitive load when working with any\npackages; it\u2019s not particularly unique to internal tools. However, while the\npractice is general, the benefits scale significantly for internal packages.\nSince we can drive this consistent standard across a collection of packages,\npast experience working with any one of our internal packages gives a user\nintuition when working with the next.\n\n### Defined Roles - Curation\n\nAnother aspect of good team communication is having clearly defined roles and\nresponsibilities. Again, since we own our whole internal stack, we have more\nfreedom in how we chose to divide functionality across packages.\n\nOpen source packages inevitably have overlapping functionality which forces\nusers to compare alternatives and decide which is best. The need to curate all\nof the potentially useful tools creates a meta-job for analysts of researching\nand weighing alternatives, seeking their colleague\u2019s advice, and piecing\ndisparate pieces of a pipeline together.\n\nBut internally, we can use some amount of central planning to ensure each\npackage teammate has a clearly defined role \u2013 whether that be to provide a\nhorizontal utility or to enable progress on a specific workstream. And just\nlike one team ought to work well with another, that central planning can\ninclude curation and promotion of open-source tools that \u201cplay nicely\u201d with\nour internal ones. After all, no one team can go it alone!\n\n### Definded Roles - Dependencies\n\nWhen assigning these roles and responsibilities to our team of packages, we\nshould consider how to manage the dependencies between them when different\nfunctionality needs to be shared.\n\nPackages often have direct dependencies where a function in one package calls\na function in another. This is not necessarily bad, but especially with\ninternal packages which might sometimes have a shorter shelf-life and few\ndevelopers, this can create a domino effect. If one package is deprecated (or\ndecides to retire or take a vacation), we don\u2019t want the rest of our ecosystem\naffected.\n\nAlternative, we can use the fact that both packages A and B are under our\ncontrol and see if we can eliminate explicit dependencies by promoting a clean\nhand-off. We can see if a function in A can produce an output that B can\nconsume instead of directly calling A\u2019s function internally.\n\nAdditionally, because we own the full stack, we may also consider if there are\nshared needs in A and B that should be be extracted into a common \u201cbuilding\nblock\u201d package C. For instance, a set of common visualization function\n\u201cprimitives\u201d. This was, we at least have a clear hierarchy of dependencies\ninstead of a spiderweb and can identify a small number of truly essential\nones.\n\n### Delivering Reliably - Testing\n\nRegardless of the type of dependencies we end up with, we can use tests to\nmake sure that our packages are reliable teammates who do what they promised.\n\nTypically, if I write a package B that depends on package A, I can only\ncontrol package B\u2019s tests, so I could write tests to see if A continues to\nperform as B is expecting.\n\nThis is a good safeguard, but it means that we will only detect problems after\nthey have already been introduced in A. There\u2019s nothing in place to actually\nstop package A from getting distracted in the first place.\n\nInstead, we\u2019d prefer that both A and B be conscientious of the promises that\nthey have made and stay committed to working towards their shared goals.\n\nWe can formalize that shared vision with integration tests. That is, we can\nadd tests to both the upstream and downstream packages to ensure that they\ncontinue to check in with each other and inquire if any changes that they are\nplanning could be disruptive. Now, just imagine having such rigorous and\nongoing communication and prioritization with your actual teammates!\n\nThis type of two-way testing can also be deployed to more robustly test\nexternal and cross-platform dependencies. For example, if your internal\npackage requires connects to external resources like APIs or databases, you\ncould even write tests that connect to external resources and verify that\ncertain API endpoints, database schemas, or even columns still exist.\n\nIn open source packages, we generally employ \u201cmocking\u201d to avoid testing\nexternal dependencies since these fail when run by unauthenticated users or\nprograms (such as CRAN checks.) However, once again, internally we can get\ndifferential value and safety by making different decisions.\n\n## Final Thoughts\n\nIn summary, we all know the the joy of working with a great team, and, if\nyou\u2019ve made it this far, I suspect you know the pleasure of cracking open a\nnew R package. By taking advantage of unique opportunities when designing\ninternal packages, we can truly achieve the best of both worlds. We can share\nthe fun of working with good tools with the teammates we care about, and\nelevate those same tools to full-fledged teammates by giving them the skills\nto succeed.\n\n## Resources\n\nReady to get started? Check out my posts on:\n\n  * R Markdown Driven Development to learn the technical side of converting existing analysis scripts into reusable tools and packages\n  * my Rtistic project for a template to make your organization\u2019s first package (for themes and color palettes)\n  * my round-up of cool examples of R packages in industry.^8\n\nFor general resources on building R packages, check out:\n\n  * the R Packages book\n  * Malcolm Barrett and Rich Iannone\u2019s My Organization\u2019s First Package training from rstudio::conf 2020\n  * Hadley Wickham\u2019s Building Tidy Tools training from rstudio::conf 2020\n\nFor opinionated guides to package design, check out:\n\n  * rOpenSci Packages: Development, Maintenance, and Peer Review\n  * How to develop good R packages for open science by Ma\u00eblle Salmon\n\n## Too Long Didn\u2019t Read\n\nThe following summarizes the key principles and practices discussed in this\narticle.\n\nAbstraction & Enforcement: In an organization, there are more things that we\nabsolutely know to be true (e.g. the location of a server) or required (e.g.\nthe right way to authenticate.) Thus, for internal tools, sometimes less\nflexible functions are useful; these don\u2019t require users to input things that\nwill not change or that they may not completely understand (e.g. encryption).\nSo, internal packages could include functions that:\n\n  * Provide utilities such as database connections objects\n  * Take an opinionated stance to be more efficient and enforce best practices (by what they chose not to parameterize)\n  * Provide extremely helpful and prescriptive error messages not only explaining failure but explaining what organizational resources are needed to fix (e.g. perhaps someone needs to access a DBA for access to a schema). Bonus points for using the cli package to make these errors easy to read\n  * Continually learn from user challenges to silently handle more potential errors and corner cases internally\n\nInformed & Flexible: Because we know our organization\u2019s problem domain, there\nare some things we probably know to be true 80% of the time. When designing\nfunctions, we can make our functions informed enough to save user\u2019s time and\ntyping by handling this 80% of cases more automatically but still providing\ncomplete flexibility to override with:\n\n  * Default arguments which allow us to provide a likely value for a function argument that is used unless overridden by the user\n  * Reserved keywords that provide at the package or package-ecosystem level a small set of variable names that will be repeatedly used for these defaults\n  * Ellipsis to allow users to insert additional arguments and customize functions in ways we did not foresee\n  * Functional programming which, like the ellipsis, makes our functions seamless to extend\n\nProviding Context & Coaching: Internal packages can shared knowledge learned\nfrom past experience and nudge users towards better coding standards by\ninclude:\n\n  * Vignettes that package the accumulated knowledge about solving a certain type of problem within the bounds of an organization\n  * pkgdown sites to share all of the qualitative information contained in vignettes as part of institutional knowledge management\n  * R Markdown templates to encode an interactive workflow or provide an analysis template\n  * R Project templates to drive consistency in how a project using that package is structured\n\nImproving accessibility and coordination: Internal packages can become more\nintegral to how our organization works if they help all of our coworkers\nengage \u2013 even non-R users. Some strategies for this are:\n\n  * Modularizing parts of the workflow that do and do not require programming. One way to do this is with R Markdown child documents\n  * Defining project plans with projmgr (or other APIs) and automatically publishing them to one\u2019s prefered project management platform\n  * Promoting use of the RStudio Visual Markdown Editor\n  * Providing RStudio Add-ins to help new users get some amount of graphical support in performing simple tasks with out package\n\nCollaboration: When we write multiple internal R package, we also have the\nunique ability to think about how our set of packages can interact well with\neach other. To fully capitalize on this, we can:\n\n  * Define a naming convention to use across all packages and help make \u201cperformance guarantees\u201d so knowledge transfers between packages\n  * Carefully rationalizing the functions in each package to avoid confusing redunancy and promote a curated set of (internal and open) tools\n  * Tackle refactoring across all our packages instead of one-by-one to attempt to unneeded, potentially brittle dependencies\n  * Set up two-way unit tests (or \u201cintegration tests\u201d) where we have dependencies to ensure that the intent of dependent packages doesn\u2019t drift apart\n\n## Footnotes\n\n  1. Many practitioners have built off of Christensen\u2019s initial theory with sometimes passionately conflicting opinions on the nuances\u21a9\ufe0e\n\n  2. Particularly users interested enough to read this post =)\u21a9\ufe0e\n\n  3. The usethis package also has many great examples of \u201cIT Guy\u201d-like qualities. One that I don\u2019t discuss in this post is explicitly informing users what it intends to do and then seeking permission\u21a9\ufe0e\n\n  4. Note that any functions I show in the post are psuedo-code and simplified to illustrate my key points.\u21a9\ufe0e\n\n  5. This is not a recommendation for the best way to store credentials! There are many other options such as keychains and password managers that your organization may prefer. I use a relatively simple example here that I could implement without adding any bulky code or additional packages to illustrate the point of opinionated design - not to give advice on security.\u21a9\ufe0e\n\n  6. Disclaimer, this is a shameless promotion of my own package.\u21a9\ufe0e\n\n  7. rOpenSci has a nice discussion on function naming in their package style guide\u21a9\ufe0e\n\n  8. This round-up also illustrates some great examples of when you should not use an R package for your organization and when other types of end-products could be more beneficial.\u21a9\ufe0e\n\n2016-2023 Emily Riederer Licensed under Creative Commons CC BY 4.0\n\nCookie Preferences\n\nMade with and Quarto View the source at GitHub\n\n", "frontpage": false}
