{"aid": "40162118", "title": "Maglev \u2013 V8's Fastest Optimizing JIT", "url": "https://v8.dev/blog/maglev", "domain": "v8.dev", "votes": 1, "user": "vicek22", "posted_at": "2024-04-25 19:45:21", "comments": 0, "source_title": "V8", "source_text": "Maglev - V8\u2019s Fastest Optimizing JIT \u00b7 V8\n\n# V8\n\nShow navigation\n\n# Maglev - V8\u2019s Fastest Optimizing JIT\n\nPublished 05 December 2023 \u00b7 Tagged with JavaScript\n\nIn Chrome M117 we introduced a new optimizing compiler: Maglev. Maglev sits\nbetween our existing Sparkplug and TurboFan compilers, and fills the role of a\nfast optimizing compiler that generates good enough code, fast enough.\n\n# Background #\n\nUntil 2021 V8 had two main execution tiers: Ignition, the interpreter; and\nTurboFan, V8\u2019s optimizing compiler focused on peak performance. All JavaScript\ncode is first compiled to ignition bytecode, and executed by interpreting it.\nDuring execution V8 tracks how the program behaves, including tracking object\nshapes and types. Both the runtime execution metadata and bytecode are fed\ninto the optimizing compiler to generate high-performance, often speculative,\nmachine code that runs significantly faster than the interpreter can.\n\nThese improvements are clearly visible on benchmarks like JetStream, a\ncollection of traditional pure JavaScript benchmarks measuring startup,\nlatency, and peak performance. TurboFan helps V8 run the suite 4.35x as fast!\nJetStream has a reduced emphasis on steady state performance compared to past\nbenchmarks (like the retired Octane benchmark), but due to the simplicity of\nmany line items, the optimized code is still where most time is spent.\n\nSpeedometer is a different kind of benchmark suite than JetStream. It\u2019s\ndesigned to measure a web app\u2019s responsiveness by timing simulated user\ninteractions. Instead of smaller static standalone JavaScript apps, the suite\nconsists of full web pages, most of which are built using popular frameworks.\nLike during most web page loads, Speedometer line items spend much less time\nrunning tight JavaScript loops and much more executing a lot of code that\ninteracts with the rest of the browser.\n\nTurboFan still has a lot of impact on Speedometer: it runs over 1.5x as fast!\nBut the impact is clearly much more muted than on JetStream. Part of this\ndifference results from the fact that full pages just spend less time in pure\nJavaScript. But in part it\u2019s due to the benchmark spending a lot of time in\nfunctions that don\u2019t get hot enough to be optimized by TurboFan.\n\nWeb performance benchmarks comparing unoptimized and optimized execution\n\nAll the benchmark scores in this post were measured with Chrome 117.0.5897.3\non a 13\u201d M2 Macbook Air.\n\nSince the difference in execution speed and compile time between Ignition and\nTurboFan is so large, in 2021 we introduced a new baseline JIT called\nSparkplug. It\u2019s designed to compile bytecode to equivalent machine code almost\ninstantaneously.\n\nOn JetStream, Sparkplug improves performance quite a bit compared to Ignition\n(+45%). Even when TurboFan is also in the picture we still see a solid\nimprovement in performance (+8%). On Speedometer we see a 41% improvement over\nIgnition, bringing it close to TurboFan performance, and a 22% improvement\nover Ignition + TurboFan! Since Sparkplug is so fast, we can easily deploy it\nvery broadly and get a consistent speedup. If code doesn\u2019t rely solely on\neasily optimized, long-running, tight JavaScript loops, it\u2019s a great addition.\n\nWeb performance benchmarks with added Sparkplug\n\nThe simplicity of Sparkplug imposes a relatively low upper limit on the\nspeedup it can provide though. This is clearly demonstrated by the large gap\nbetween Ignition + Sparkplug and Ignition + TurboFan.\n\nThis is where Maglev comes in, our new optimizing JIT that generates code\nthat\u2019s much faster than Sparkplug code, but is generated much faster than\nTurboFan can.\n\n# Maglev: A Simple SSA-Based JIT compiler #\n\nWhen we started this project we saw two paths forward to cover the gap between\nSparkplug and TurboFan: either try to generate better code using the single-\npass approach taken by Sparkplug, or build a JIT with an intermediate\nrepresentation (IR). Since we felt that not having an IR at all during\ncompilation would likely severely restrict the compiler, we decided to go with\na somewhat traditional static single-assignment (SSA) based approach, using a\nCFG (control flow graph) rather than TurboFan's more flexible but cache\nunfriendly sea-of-nodes representation.\n\nThe compiler itself is designed to be fast and easy to work on. It has a\nminimal set of passes and a simple, single IR that encodes specialized\nJavaScript semantics.\n\n## Prepass #\n\nFirst Maglev does a prepass over the bytecode to find branch targets,\nincluding loops, and assignments to variables in loop. This pass also collects\nliveness information, encoding which values in which variables are still\nneeded across which expressions. This information can reduce the amount of\nstate that needs to be tracked by the compiler later.\n\n## SSA #\n\nA printout of the Maglev SSA graph on the command line\n\nMaglev does an abstract interpretation of the frame state, creating SSA nodes\nrepresenting the results of expression evaluation. Variable assignments are\nemulated by storing those SSA nodes in the respective abstract interpreter\nregister. In the case of branches and switches, all paths are evaluated.\n\nWhen multiple paths merge, values in abstract interpreter registers are merged\nby inserting so-called Phi nodes: value nodes that know which value to pick\ndepending on which path was taken at runtime.\n\nLoops can merge variable values \u201cback in time\u201d, with the data flowing\nbackwards from the loop end to the loop header, in the case when variables are\nassigned in the loop body. That\u2019s where the data from the prepass comes in\nhandy: since we already know which variables are assigned inside loops, we can\npre-create loop phis before we even start processing the loop body. At the end\nof the loop we can populate the phi input with the correct SSA node. This\nallows the SSA graph generation to be a single forward pass, without needing\nto \"fix up\" loop variables, while also minimizing the amount of Phi nodes that\nneed to be allocated.\n\n## Known Node Information #\n\nTo be as fast as possible, Maglev does as much as possible at once. Instead of\nbuilding a generic JavaScript graph and then lowering that during later\noptimization phases, which is a theoretically clean but computationally\nexpensive approach, Maglev does as much as possible immediately during graph\nbuilding.\n\nDuring graph building Maglev will look at runtime feedback metadata collected\nduring unoptimized execution, and generate specialized SSA nodes for the types\nobserved. If Maglev sees o.x and knows from the runtime feedback that o always\nhas one specific shape, it will generate an SSA node to check at runtime that\no still has the expected shape, followed by a cheap LoadField node which does\na simple access by offset.\n\nAdditionally, Maglev will make a side node that it now knows the shape of o,\nmaking it unnecessary to check the shape again later. If Maglev later\nencounters an operation on o that doesn't have feedback for some reason, this\nkind of information learned during compilation can be used as a second source\nof feedback.\n\nRuntime information can come in various forms. Some information needs to be\nchecked at runtime, like the shape check previously described. Other\ninformation can be used without runtime checks by registering dependencies to\nthe runtime. Globals that are de-facto constant (not changed between\ninitialization and when their value is seen by Maglev) fall into this\ncategory: Maglev does not need to generate code to dynamically load and check\ntheir identity. Maglev can load the value at compile time and embed it\ndirectly into the machine code; if the runtime ever mutates that global, it'll\nalso take care to invalidate and deoptimize that machine code.\n\nSome forms of information are \u201cunstable\u201d. Such information can only be used to\nthe extent that the compiler knows for sure that it can\u2019t change. For example,\nif we just allocated an object, we know it\u2019s a new object and we can skip\nexpensive write barriers entirely. Once there has been another potential\nallocation, the garbage collector could have moved the object, and we now need\nto emit such checks. Others are \"stable\": if we have never seen any object\ntransition away from having a certain shape, then we can register a dependency\non this event (any object transitioning away from that particular shape) and\ndon\u2019t need to recheck the shape of the object, even after a call to an unknown\nfunction with unknown side effects.\n\n## Deoptimization #\n\nGiven that Maglev can use speculative information that it checks at runtime,\nMaglev code needs to be able to deoptimize. To make this work, Maglev attaches\nabstract interpreter frame state to nodes that can deoptimize. This state maps\ninterpreter registers to SSA values. This state turns into metadata during\ncode generation, providing a mapping from optimized state to unoptimized\nstate. The deoptimizer interprets this data, reading values from the\ninterpreter frame and machine registers and putting them into the required\nplaces for interpretation. This builds on the same deoptimization mechanism as\nused by TurboFan, allowing us to share most of the logic and take advantage of\nthe testing of the existing system.\n\n## Representation Selection #\n\nJavaScript numbers represent, according to the spec, a 64-bit floating point\nvalue. This doesn't mean that the engine has to always store them as 64-bit\nfloats though, especially since In practice many numbers are small integers\n(e.g. array indices). V8 tries to encode numbers as 31-bit tagged integers\n(internally called \u201cSmall Integers\u201d or \"Smi\"), both to save memory (32bit due\nto pointer compression), and for performance (integer operations are faster\nthan float operations).\n\nTo make numerics-heavy JavaScript code fast, it\u2019s important that optimal\nrepresentations are chosen for value nodes. Unlike the interpreter and\nSparkplug, the optimizing compiler can unbox values once it knows their type,\noperating on raw numbers rather than JavaScript values representing numbers,\nand rebox values only if strictly necessary. Floats can directly be passed in\nfloating point registers instead of allocating a heap object that contains the\nfloat.\n\nMaglev learns about the representation of SSA nodes mainly by looking at\nruntime feedback of e.g., binary operations, and propagating that information\nforwards through the Known Node Info mechanism. When SSA values with specific\nrepresentations flow into Phis, a correct representation that supports all the\ninputs needs to be chosen. Loop phis are again tricky, since inputs from\nwithin the loop are seen after a representation should be chosen for the phi \u2014\nthe same \"back in time\" problem as for graph building. This is why Maglev has\na separate phase after graph building to do representation selection on loop\nphis.\n\n## Register Allocation #\n\nAfter graph building and representation selection, Maglev mostly knows what\nkind of code it wants to generate, and is \"done\" from a classical optimization\npoint of view. To be able to generate code though, we need to choose where SSA\nvalues actually live when executing machine code; when they're in machine\nregisters, and when they're saved on the stack. This is done through register\nallocation.\n\nEach Maglev node has input and output requirements, including requirements on\ntemporaries needed. The register allocator does a single forward walk over the\ngraph, maintaining an abstract machine register state not too dissimilar from\nthe abstract interpretation state maintained during graph building, and will\nsatisfy those requirements, replacing the requirements on the node with actual\nlocations. Those locations can then be used by code generation.\n\nFirst, a prepass runs over the graph to find linear live ranges of nodes, so\nthat we can free up registers once an SSA node isn\u2019t needed anymore. This\nprepass also keeps track of the chain of uses. Knowing how far in the future a\nvalue is needed can be useful to decide which values to prioritize, and which\nto drop, when we run out of registers.\n\nAfter the prepass, the register allocation runs. Register assignment follows\nsome simple, local rules: If a value is already in a register, that register\nis used if possible. Nodes keep track of what registers they are stored into\nduring the graph walk. If the node doesn\u2019t yet have a register, but a register\nis free, it\u2019s picked. The node gets updated to indicate it\u2019s in the register,\nand the abstract register state is updated to know it contains the node. If\nthere\u2019s no free register, but a register is required, another value is pushed\nout of the register. Ideally, we have a node that\u2019s already in a different\nregister, and can drop this \"for free\"; otherwise we pick a value that won\u2019t\nbe needed for a long time, and spill it onto the stack.\n\nOn branch merges, the abstract register states from the incoming branches are\nmerged. We try to keep as many values in registers as possible. This can mean\nwe need to introduce register-to-register moves, or may need to unspill values\nfrom the stack, using moves called \u201cgap moves\u201d. If a branch merge has a phi\nnode, register allocation will assign output registers to the phis. Maglev\nprefers to output phis to the same registers as its inputs, to minimize moves.\n\nIf more SSA values are live than we have registers, we\u2019ll need to spill some\nvalues on the stack, and unspill them later. In the spirit of Maglev, we keep\nit simple: if a value needs to be spilled, it is retroactively told to\nimmediately spill on definition (right after the value is created), and code\ngeneration will handle emitting the spill code. The definition is guaranteed\nto \u2018dominate\u2019 all uses of the value (to reach the use we must have passed\nthrough the definition and therefore the spill code). This also means that a\nspilled value will have exactly one spill slot for the entire duration of the\ncode; values with overlapping lifetimes will thus have non-overlapping\nassigned spill slots.\n\nDue to representation selection, some values in the Maglev frame will be\ntagged pointers, pointers that V8\u2019s GC understands and needs to consider; and\nsome will be untagged, values that the GC should not look at. TurboFan handles\nthis by precisely keeping track of which stack slots contain tagged values,\nand which contain untagged values, which changes during execution as slots are\nreused for different values. For Maglev we decided to keep things simpler, to\nreduce the memory required for tracking this: we split the stack frame into a\ntagged and an untagged region, and only store this split point.\n\n## Code Generation #\n\nOnce we know what expressions we want to generate code for, and where we want\nto put their outputs and inputs, Maglev is ready to generate code.\n\nMaglev nodes directly know how to generate assembly code using a \u201cmacro\nassembler\u201d. For example, a CheckMap node knows how to emit assembler\ninstructions that compare the shape (internally called the \u201cmap\u201d) of an input\nobject with a known value, and to deoptimize the code if the object had a\nwrong shape.\n\nOne slightly tricky bit of code handles gap moves: The requested moves created\nby the register allocator know that a value lives somewhere and needs to go\nelsewhere. If there\u2019s a sequence of such moves though, a preceding move could\nclobber the input needed by a subsequent move. The Parallel Move Resolver\ncomputes how to safely perform the moves so that all values end up in the\nright place.\n\n# Results #\n\nSo the compiler we just presented is both clearly much more complex than\nSparkplug, and much simpler than TurboFan. How does it fare?\n\nIn terms of compilation speed we\u2019ve managed to build a JIT that\u2019s roughly 10x\nslower than Sparkplug, and 10x faster than TurboFan.\n\nCompile time comparison of the compilation tiers, for all functions compiled\nin JetStream\n\nThis allows us to deploy Maglev much earlier than we\u2019d want to deploy\nTurboFan. If the feedback it relied upon ended up not being very stable yet,\nthere\u2019s no huge cost to deoptimizing and recompiling later. It also allows us\nto use TurboFan a little later: we\u2019re running much faster than we\u2019d run with\nSparkplug.\n\nSlotting in Maglev between Sparkplug and TurboFan results in noticeable\nbenchmark improvements:\n\nWeb performance benchmarks with Maglev\n\nWe have also validated Maglev on real-world data, and see good improvements on\nCore Web Vitals.\n\nSince Maglev compiles much faster, and since we can now afford to wait longer\nbefore we compile functions with TurboFan, this results in a secondary benefit\nthat\u2019s not as visible on the surface. The benchmarks focus on main-thread\nlatency, but Maglev also significantly reduces V8\u2019s overall resource\nconsumption by using less off-thread CPU time. The energy consumption of a\nprocess can be measured easily on an M1- or M2-based Macbook using taskinfo.\n\nBenchmark| Energy Consumption  \n---|---  \nJetStream| -3.5%  \nSpeedometer| -10%  \n  \nMaglev isn\u2019t complete by any means. We've still got plenty more work to do,\nmore ideas to try out, and more low-hanging fruit to pick \u2014 as Maglev gets\nmore complete, we\u2019ll expect to see higher scores, and more reduction in energy\nconsumption.\n\nMaglev is now available for desktop Chrome now, and will be rolled out to\nmobile devices soon.\n\nPosted by Toon Verwaest, Leszek Swirski, Victor Gomes, Olivier Fl\u00fcckiger,\nDarius Mercadier, and Camillo Bruni \u2014 not enough cooks to spoil the broth.\n\nExcept as otherwise noted, any code samples from the V8 project are licensed\nunder V8\u2019s BSD-style license. Other content on this page is licensed under the\nCreative Commons Attribution 3.0 License. For details, see our site policies.\n\n", "frontpage": false}
