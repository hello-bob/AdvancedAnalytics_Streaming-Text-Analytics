{"aid": "40251701", "title": "Golf Course Rankings Are Mostly Useless", "url": "https://golfcoursewiki.substack.com/p/golf-course-rankings-are-mostly-useless", "domain": "golfcoursewiki.substack.com", "votes": 1, "user": "scoofy", "posted_at": "2024-05-03 19:57:06", "comments": 0, "source_title": "Golf Course Rankings are Mostly Useless", "source_text": "Golf Course Rankings are Mostly Useless\n\n# Wigs on the Green\n\nShare this post\n\n#### Golf Course Rankings are Mostly Useless\n\ngolfcoursewiki.substack.com\n\n# Golf Course Rankings are Mostly Useless\n\nMatthew Schoolfield\n\nMay 02, 2024\n\nShare this post\n\n#### Golf Course Rankings are Mostly Useless\n\ngolfcoursewiki.substack.com\n\nShare\n\nEach year or so, the new golf course rankings are announced and I inevitably\nscramble to see if anything has happened. Almost every time, I walk away from\nreading them grumbling about how bullshit they are. Oh, look, Pine Valley is\non top, maybe I should ready my private yacht and sail to New Jersey. Looks\nlike this year I should phone up my monocled business associate in Monterey\nand have him pencil me in for a round or two at Monterey CC and Cypress? I\u2019d\nlove to kvetch about a compendium of courses I\u2019ll never be invited to, but to\nbe fair, it has been fun to see Team Keiser creep up the lists in the last\ndecade. But again, I don\u2019t need anyone to tell me I should play at Bandon.\n\nThat\u2019s at least the surface reason I love to hate golf course rankings. But\nall that aside, there is a more fundamental issue with rankings at play here\nthat makes them not just annoying to me, but largely unhelpful to the masses.\nSo let\u2019s talk about why the act of ranking courses in the first place makes\nthe lists mostly useless, and how the ranking system could evolve over time to\nproduce lists that are actually meaningful to most people.\n\n#\n\nGolf Courses Are Ranked (for Some Reason)\n\nGolf is unique in its ranked lists,1 but you might not even notice it until\nsomeone points it out. While restaurants, films, and even National Parks are\nreviewed with star-rating systems,2 golf course rankings are much more\nreminiscent of awards shows. Other areas where golf rankings parallel awards\nis that they are generally presented once a year, the voting is anonymous, and\nthe results are presented with an air of authoritativeness. This framing means\nthere is often no individual to disagree with; it is a consensus opinion.\nHowever, there are too many courses for any one individual to review each\nyear, so the results are typically a hodgepodge at best.\n\nThe reasons for this quirk of golf are likely historical, but it is arguable\nthat shifting golf reviews to a score-based system would improve their\nusefulness to the people who actually read these lists. There are some\nbenefits and drawbacks to ranking vs scoring systems,3 and the de facto\npurpose of the review probably has a lot to do with which system gets used.4\nAnd while there are some organizations that are changing things,5 if you\u2019re\nlooking at a golf course review, it\u2019s most likely attached to a ranking.\n\nPart of the fun of rankings (or a drawback depending on your stance on them)\nis that lots of people typically don\u2019t agree with rankings when they come out.\nOften, they can\u2019t even agree on what qualities of the rankings are even worth\ncaring about.6 Values vary from stunning views, to playability, to artistic\nmerit. This gets complicated when these virtues happen to be contradictory.7\n\n#\n\nWhat If Everyone\u2019s Taste is Just Different?\n\nThe entire framework for authoritative rankings assumes that there actually is\na best golf course. Despite all the disagreement, we read these rankings and\nassume that a groups\u2019 opinion for something being the best makes sense. There\nis obviously the case for deferring to experts, but what if the experts still\ndisagree? What if all this disagreement can be explained by the fact that\npeople just like different things generally? To illustrate this argument,\nlet\u2019s look at soda pop; something so immediately accessible that even most\nchildren have strongly held, informed opinions on the subject.\n\n###\n\nCoca-Cola vs Cherry Vanilla Dr Pepper\n\nSo, what is the \u201cbest\u201d soda? I think most people would say Coca-Cola, but I\u2019m\nsure there is a strong contingent for Pepsi, Dr Pepper, or even Sprite. If by\n\u201cbest\u201d soda, we mean \u201cbest selling\u201d or \u201csoda that the most people vote for\u201d\nthen the answer would definitely be Coca-Cola. However, to illustrate the\nshortcomings of a system like this, we need to understand why Cherry Vanilla\nDr Pepper was created. You read that right: this isn\u2019t just Dr Pepper. I\u2019m\nreferring to a very obscure alternative: Cherry Vanilla Dr Pepper. The work of\nHoward Moskowitz, who created the obscure soda, can show us why something\nbeing the most popular doesn\u2019t mean it\u2019s the best.\n\nMoskowitz was credited for popularizing the concept of inter-market\nvariability in the food industry.8 He argues that we should reject the idea\nthat people would prefer a consensus \u201cbest\u201d version of something. Instead, he\nsuggests that people just have inherently different tastes for things. His\nwork shows that products can be micro-targeted at individuals by creating lots\nof variations, rather than offering them the single popular version.\n\nMoskowitz argues that, when it comes to taste preferences, there isn\u2019t one\n\u201cbest soda.\u201d Rather, it comes down to taste preferences, and everyone\u2019s\nopinions are equally valid. So instead of trying to find \u201cthe best soda\u201d to\nenjoy, we should be trying to find \u201cour favorite soda.\u201d This isn\u2019t a total\nrejection of expertise (I\u2019ll come back to that later), but it basically means\nthat there are, generally, clusters of people who share similar taste\npreferences. This taste clustering means that, while it might not be practical\nto make a soda for every individual, we can get very close to creating ideal\nsodas by targeting these clusters of preferences. Following this reasoning,\nwhen we rank things based on general popularity, we should expect the winners\nto be something people like, but allowing for niche variations in rankings\nshould lead to multiple winners, but winners that these smaller groups love.\n\nIf put to a popular vote, Cherry Vanilla Dr Pepper wouldn\u2019t rank in the top\n100 of best sodas. It probably wouldn\u2019t rank in the top 1000. However,\nMoskowitz\u2019s idea is that there are people out there who do love Cherry Vanilla\nDr Pepper, and think it\u2019s the best soda, period. Even if only one person in\n10,000 does, that still ends up being a large number of people. Thus, if\nrating systems are simply popularity contests, they won\u2019t create the most\nvalue for the most people.\n\n#\n\nThe Tomatometer and the Critic\u2019s Page\n\nNow, Moskowitz\u2019s results do not mean that there is no value to listening to\ncritics, only that their opinions exist on a plane alongside other, equally\nvalid opinions. Professional critics have the time, resources, and education\nto appreciate the nuance to things. So even though critics don\u2019t have better\nopinions, per se, they do have highly informed opinions. Thus, ratings/reviews\nby critics are still extremely useful to discover things we love. There are\ntwo ways we generally encounter critical reviews: through aggregating them or\nthrough looking at them individually.\n\nRotten Tomatoes is a film site that aggregates critics\u2019 reviews of films. Its\nTomatometer is a good way to see if most professionals have a favorable\nopinion of a film.9 However, this only means most professional critics think\nthe film is okay. It\u2019s a good way to avoid films you will probably hate, but\nit\u2019s not a great way to find films you will love.\n\nThe site, however, has critic pages as well. Critic pages show all reviews of\nindividual critics.10 If a film-lover can find an individual film critic they\nusually agree with, they can use that film critic\u2019s opinions as a proxy for\nthe taste clustering in Moskowitz\u2019s research. When that happens, it\u2019s very\neasy to find films they will love, and avoid films they will hate. Nothing is\nperfect, but knowing that you will have a high probability of finding\nsomething you will love, that\u2019s made for your kind of quirk, is extremely\nvaluable for most people.\n\nMoskowitz\u2019s Cherry Vanilla Dr Pepper only really appeals to the people who\nlove it. In the same way, the film critic who really loves bizarre, niche\nfilms can point eccentric film lovers in the direction of their favorite\ncherry vanilla versions of film.\n\n#\n\nCollaborative Filtering: the Holy Grail of Dynamic Rating Systems\n\nThere aren\u2019t nearly as many professional critics in golf as there are in film,\nso finding one that would match an individual's views would be challenging, if\nnot impossible. Still, there is a way to stretch the available information by\nusing collaborative filtering. Collaborative filtering is a type of machine\nlearning algorithm best known as the logic behind Netflix\u2019s recommendation\nengine.11 It uses peoples\u2019 existing ratings, and very carefully compares them\nto other peoples ratings, to make surprisingly accurate predictions about the\nlikes or dislikes of things they\u2019ve never tried. While it\u2019s fair to say these\nalgorithms can be crude and are certainly not a replacement for professional\ncritic\u2019s opinions, they can be incredibly helpful. Unfortunately, for\ncollaborative filtering to work, the reviews that get processed need to be\nscored, rather than ranked.\n\n#\n\nWhat Does This Mean For Golf Courses?\n\nI cannot overstate how much I think a clustering-based recommendation engine\nwould benefit the golf world. The parameters that affect the enjoyment of a\ncourse vary dramatically from player to player.12 However, establishing such a\nsystem would be difficult. As it stands, the historical ranked-rating systems\nare not much use in creating any system like this, nor is there much way to\nseek out a reviewer who shares unique views.\n\nThe golf world would immediately benefit if all of the ranking publications\nused score-based rankings, and published the scores of each rater, even\nanonymously. While I don\u2019t think this will ever happen, I would hope that the\nnewer publications, currently using score-based systems, make that a norm over\ntime.\n\nEliminating authoritativeness from our golf course rankings would be the\neasiest change. Siskel and Ebert demonstrated how to do this perfectly with\ntheir television show At the Movies. Framing reviews with at least two\nopinions allows for disagreement to exist. That disagreement creates healthy\ndifferentiation the audience can use. Even if it\u2019s a hassle for multiple\npeople to write in-depth reviews, I would hope that publishers at least ask\nall the critics to provide their opinion even if they can only manage a number\nor a quick blurb. Finding a good critic you match with is a huge bonus.\n\n\u2014\n\nAll of this has created a bit of a quandary for me over at GolfCourse.wiki. I\nhave thought pretty hard about whether or not to include a rating system for\ngolf courses there, and so far, I\u2019ve decided against it. However, I do think\nbuilding up a collaborative filtering system to help people find golf courses\nthey might enjoy could really benefit the community. As it stands, it would be\nchallenging to implement, and would take a lot of time. I think it would also\nbe difficult to verify user ratings to prevent people from gaming (ruining)\nthe system. I think if I do institute something like that, there will not be\nany rating posted on course pages. Instead, only a recommendation engine built\non collaborative filtering. That way trying to game the system makes little\nsense. However, I\u2019m still not sure what the best approach would be.\n\nThank you for reading Wigs on the Green. Sharing this post helps me a lot.\n\nShare\n\nThere are many golf course reviewers out there, but there isn\u2019t a central\ndatabase for reviews like Rotten Tomatoes. Getting one started would be\ndifficult, especially starting one that doesn\u2019t already have ties to vested\ninterest in the industry. The world of golf has a pretty significant blind\nspot when it comes to reviewing the more humble, but quality courses people\nhave in their communities. People often still need word-of-mouth\nrecommendations. I think this blind spot can be alleviated somewhat with the\nwiki, but creating a robust database of professional reviews would do a lot of\ngood, and move us away from the mostly unhelpful award ceremonies we see\npublished each year.\n\nLeave a comment\n\n1\n\nTop 10 \u201cTop 100\u201d lists:\n\n  1. Golf Magazine: the classic.\n\n  2. Golf Digest: the response to the classic.\n\n  3. Golfweek: they have a top 50 list for any topic.\n\n  4. Golf Monthly: the regional list.\n\n  5. Planet Golf: the no nonsense list, and lists of lists.\n\n  6. Golf World: the Todays Golfer list.\n\n  7. Golf Course Architecture: the scholarly list.\n\n  8. Top 100 Golf Courses: the website list.\n\n  9. Links Magazine: the top 10 list lists.\n\n  10. NBC\u2019s Golf Pass: a user aggregated list.\n\n2\n\nThese scoring systems are generally \u201cstar rating\u201d systems, typically in a four\nor five star range. Sometimes ranking thinks some score out of 10, or\noccasionally some score out of 100.\n\nThe National Park reviews are from Yelp, but made headlines when the cognitive\ndissonance of low ratings of National Parks made headlines:\nhttps://www.washingtonpost.com/travel/2024/02/21/national-parks-one-star-\nreviews/\n\nThese scoring systems have become so ubiquitous in society that it seems\nextremely odd that golf is unique in its tradition of ranking. My favorite\nexample of a self-reflection on the star-rating\u2019s ubiquity on our world is,\nquasi-satire book The Anthropocene Reviewed, that is obviously poking fun at\nrating systems, while at the same time presenting beautiful and personal\nstories that go along with a cliche five star rating.\n\n3\n\nBenefits of Ranking:\n\nOne benefit of ranking is that it removes the need to address imperfection.\nRankers need not justify why points are deducted. Ranking allows the\ncelebration of excellence without the need to address weaknesses, and here,\nrankers can effectively equivocate. If Course A is \u201cflawless\u201d and Course B is\nalso \u201cflawless,\u201d ranking one course higher, can effectively imply that that\ncourse is more flawless. Even though this makes little sense logically, there\nis little need to justify the details of the preference, since this type of\nranked preference isn\u2019t held to some platonic perfection standard.\n\nDrawbacks of Ranking:\n\nThe biggest drawback of ranking is how limited the information is.\n\nBenefits of Scoring:\n\nIndividual scores allow for more information to be conveyed to the audience.\nScores allow for hierarchical ranking, but also show the distance between\nthose ranks. Importantly, scored data facilitates sophisticated data use,\nmachine learning models, suggestion algorithms, etc.\n\nDrawbacks of Scoring:\n\nDifferent types of scoring systems have different qualities. Effectively\ndiscrete scoring will not communicate nuance. Effectively contentious scoring\ncan leave confusion about what a zero and max score mean. Often the max score\nis set to a platonic ideal of what is being scored, which can making their\nusefulness limited\n\nGeneralizing multiple scores to reach a consensus, however, can be\nchallenging.\n\n4\n\nRatings as celebrations of excellence (primary audience is typically industry\ninsiders/experts):\n\nWhen we look at institutional awards: the Academy Awards, the Grammys, Tony\nAwards, Emmy Awards, etc. We see institutions celebrating the most successful\nartists of the year. These awards typically take the form of rankings, with\none winner, and unranked non-winning nominees.\n\nThese awards generally are not given to help consumers decide which films they\nwould like to see, as we should probably see all of them, as all the nominees\nare generally considered excellent in their field. In addition, the works are\noften in contrast to each other, where different genres or subgenres are vying\nfor prestige.\n\nRatings for informing the consumer (primary audience is typically the end\nconsumer, data capture):\n\nWhen we look at product reviews, we generally see ratings as scores. These\ntake the form of film reviews, with their star-rating systems or their thumbs\nup or down. These types of ratings generally cover a broad swath of the\navailable product.\n\nRatings as Marketing (primary audience is the end consumer, or possible owners\nin the case of private clubs):\n\nNow the inner cynic in me comes out. At the end of the day, golf is a\nbusiness, and the business of golf is buoyed by the new course being the one\nthat everyone wants to play. Even private clubs, which seem like they would\nhave no need for marketing, open up their doors to the right people at the\nright time to get on the cover of the right magazines, seemingly under the\nguise of celebrations of excellence.\n\nWhile I noted earlier that awards shows are typically for the industry\ninsiders, the reason why I note that it is a de facto purpose is that it\u2019s\nwell documented that studios will campaign strongly for their films to get\nawards as a form of marketing. There have been some fairly serious allegations\nin the last year that this is happening in the golf world as well.\n\n5\n\nProfessionals that use scoring systems rather than rankings:\n\n  * Tom Doak: https://www.doakgolf.com/confidential-guide-to-golf-courses/\n\n  * The Fried Egg: https://thefriedegg.com/fried-egg-golf-course-ranking-system/\n\nThere are quite a few site that aggregate scored user reviews including:\n\n  * UK Golf Guide: https://ukgolfguide.com\n\n  * Greenskeeper.org: https://greenskeeper.org\n\n  * NBC\u2019s Golf Pass (but they still typically present the ratings in ranking form): https://www.golfpass.com/travel-advisor/articles/golf-advisors-top-100-courses-thru-the-first-five-years-of-ratings\n\nSome of the ranking publications provide aggregated scores, but these scores\ncannot be tied to individual critics (as far as I know):\n\n  * Golf Digest\n\n  * Golfweek\n\n6\n\nNote here that historically, Golf Digest has emphasized the challenging nature\nof a course, Golf Magazine, by contrast, has a much more loose way of choosing\ntheir top courses.\n\n7\n\nA prime example of contradictory values, which has led to many arguments, is\noverseeding bermuda with rye. Dormant bermuda is an excellent firm-and-fast\nplaying surface, which is extremely desirable in golf, however, it is yellow,\nnot green. This has led many, many courses to plant and grow rye grass during\nthe dormant periods because they care much more about the verdant appearance\nof their course, even if it\u2019s a softer, slower playing surface, generally\nregarded as less interesting. This type of contradiction affects even the\nhighest ranked courses in the world.\n\n8\n\nHoward Moscowitz rose to prominence in the food industry when he used this\nmethod to develop thick and chunky tomato sauce. Malcom Gladwell has profiled\nhis career and why it was so influential, both in the New Yorker and at TED:\n\n9\n\nThe breakdown for Tomatometer scores are as follows:\n\n  * Certified Fresh:\n\n    * Greater than 75% of professional film reviews are positive.\n\n    * Five or more professional reviews from top critics.\n\n    * At least 80 reviews for wide release, and 40 reviews for limited releases.\n\n    * It must maintain these stats consistently over time.\n\n  * Fresh: Greater than 60% of professional film reviews are positive.\n\n  * Rotten: Less than 60% of professional film reviews are positive.\n\nTomatometer explainer:\nhttps://www.rottentomatoes.com/about#whatisthetomatometer\n\n10\n\nFor reference, the Rotten Tomatoes Critic\u2019s page for The Atlantic film critic\nand Blank Check host, David Sims:\nhttps://www.rottentomatoes.com/critics/david-sims/movies\n\n11\n\nNetflix used to host a contest for data scientists to improve their\ncollaborative filtering algorithms called the Netflix Prize. While most major\ntech companies utilize recommendation engines, Netflix and Spotify are the two\nmost prominent companies with engines dedicated solely to aesthetics. It\u2019s\nworth noting here that collaborative filtering is an implementation of cluster\nanalysis, which is directly related to Moskowitz's bliss point clustering\nresearch.\n\n12\n\nDozens of parameters exist regarding players\u2019 preferences that can affect\nenjoyment of a course. To list a few briefly:\n\n  * Conditioning:\n\n    * General presentation\n\n    * Green speeds\n\n    * Turf firmness\n\n    * Turf coverage\n\n    * Turf Imperfections\n\n    * Grass types\n\n  * Architecture:\n\n    * Dominant school of architecture:\n\n      * Penal, Strategic, Heroic, etc.\n\n    * Land movement:\n\n      * Walkability\n\n      * Routing\n\n      * Lie angles\n\n      * Forced carries\n\n    * Surface contours:\n\n      * Relationship between contour and green speeds\n\n    * Hazards:\n\n      * Types of hazards\n\n      * \u201cFairness\u201d of hazards\n\n      * Number of hazards\n\n      * Options for avoiding hazards\n\n    * Course style\n\n      * Links vs parkland vs heathland, etc.\n\n      * Testing courses vs fun courses vs risk-reward match play courses, etc.\n\n      * Open or narrow corridors\n\n  * Stewardship:\n\n    * Historical accuracy/preservation\n\n    * Environmental concerns\n\n    * Native flora/fauna\n\n  * Culture:\n\n    * Pace-of-play concerns\n\n    * Play style preferences\n\n      * Casual vs competitive\n\n      * Stroke play vs match play vs foursomes, etc.\n\n    * Ethical concerns\n\n    * Access concerns\n\n    * How the course fits in with the local area\n\n  * Cost:\n\n    * Is the course a good value\n\n    * Is the course exceptional regardless of being expensive\n\nAll of these things could be parameters that are considered in a collaborative\nfiltering program. In fact, clustering algorithms can find parameters that are\npreviously not thought of.\n\n### Subscribe to Wigs on the Green\n\nBy Matthew Schoolfield \u00b7 Launched 2 years ago\n\nThe name denotes a fight between gentlemen. Golf culture is divided and avoids\ntough conversations. The goal here is constructive criticism without\nflinching, with a focus on nuance, incentives, and ideals that can make golf\nbetter for everyone.\n\nShare this post\n\n#### Golf Course Rankings are Mostly Useless\n\ngolfcoursewiki.substack.com\n\nShare\n\nComments\n\nThe Four-Hour Round is Bullshit\n\nAnd the group in front of you isn\u2019t slowing you down \u2014 the course is.\n\nOct 10, 2022 \u2022\n\nMatthew Schoolfield\n\n3\n\nShare this post\n\n#### The Four-Hour Round is Bullshit\n\ngolfcoursewiki.substack.com\n\n6\n\nNorth Korea Has One Operating Golf Course\n\n(Updated) The hermit kingdom's only course is a deeply imperfect place that\nshould remind us that there is more to life than golf.\n\nNov 29, 2022 \u2022\n\nMatthew Schoolfield\n\n1\n\nShare this post\n\n#### North Korea Has One Operating Golf Course\n\ngolfcoursewiki.substack.com\n\n1\n\nWhat Even Is a Private Golf Club and Why Are Most So Difficult to Play?\n\nHow the IRS is driving a wedge between the golfing world. Part one of a series\non private clubs and their impact on golf\n\nMar 1, 2023 \u2022\n\nMatthew Schoolfield\n\nShare this post\n\n#### What Even Is a Private Golf Club and Why Are Most So Difficult to Play?\n\ngolfcoursewiki.substack.com\n\nReady for more?\n\n\u00a9 2024 Matthew Schoolfield\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
