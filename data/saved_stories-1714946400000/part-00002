{"aid": "40263607", "title": "Can LLM benchmarks, evaluate React coding skills?", "url": "https://Kshitij-Banerjee.github.io/2024/05/04/evaluating-llm-benchmarks-for-react/", "domain": "kshitij-banerjee.github.io", "votes": 1, "user": "kshitij_libra", "posted_at": "2024-05-05 10:08:42", "comments": 0, "source_title": "Evaluating LLM Benchmarks for React", "source_text": "Evaluating LLM Benchmarks for React | KiloBytes by KB\n\n# Evaluating LLM Benchmarks for React\n\nMay 4, 2024\n\n# Introduction#\n\nI previously wrote about writing react code with Deepseek-coder 33b model, and\nwhether we could improve some of these shortcomings with the latest research\nin the LLM space\n\nBut to really measure and mark progress, it would require the build of a\nbenchmark to test various hypothesis around it.\n\nSo in this post, I\u2019m going to evaluate existing benchmarks that specifically\nmeasures LLM capabilities on coding capabilities.\n\nMy goal is to be able to build a benchmark that can test their\nReact/Typescript coding capabilities.\n\n## What we need#\n\n### Unit Test Evaluations#\n\nIn this method, we\u2019ll require that the LLM write the code, and then we will\nrun unit tests to measure the outcome.\n\nWe then will evaluate pass@1, pass@k, and strict-accuracy metrics.\n\n### Visual verification#\n\nIn this method, we want to test style replication and ask the LLM to produce a\ncomponent with some given specifications.\n\nWe\u2019ll then verify it\u2019s output against a known ground-truth of correct visual\noutput.\n\n### Ease of writing, and similarity to real-life#\n\nI\u2019d also want this to be similar to how we write code practically.\n\nA file where some code is written, and a corresponding .test file that imports\nthe code and runs a set of evaluations.\n\n# How the rest of the post is structured#\n\n### Review of existing benchmarks and how they are setup#\n\n  1. OpenAI Evals\n\n  2. APPS benchmark\n\n  3. HumanEval\n\n  4. CanAiCode\n\n  5. MultiPL-E\n\n  6. RepoBench\n\n### In a future post, I intent to cover#\n\nDetails on Test based method\n\nDetails on Visual verification\n\nBenchmark Results for 3 open source LLM models.\n\n# 1) OpenAI Evals#\n\nThis is probably the most renowned of all evaluation frameworks.\nhttps://github.com/openai/evals\n\nHowever, they don\u2019t accept \u201cCustom code\u201d Evals. Meaning, only simple matches\n(Exact, Includes, Fuzzy Match) are possible test evaluations to run.\n\nEven though OpenAI doesn\u2019t accept these evals. It\u2019s worth noting that we can\nsimply fork the repo and write our own custom evals\n\nThe framework allows to build a custom eval, as well as a custom completion\nfunction. It also comes with a nice cookbook tutorial.\n\n### Pros#\n\n  1. Mature framework.\n\n  2. A ton of existing sample benchmarks. Once this is set up, it will allow one to find results on other interesting benchmarks.\n\n  3. Enables custom evals and custom completions\n\n### Cons#\n\n  1. Doesn\u2019t accept new custom evals.\n\n  2. It\u2019s a bit heavy to setup, with git LFS and lots of dependencies that are added over time\n\n  3. Doesn\u2019t have many code related benchmarks\n\n### Verdict#\n\n\ud83d\udc4d - This could work for building a react benchmark. It might be a bit hard to\nget off the ground though, and may limit customization.\n\n# 2) APPS#\n\nPaper: Measuring Coding Challenge Competence With APPS\n\nRepository: https://github.com/hendrycks/apps\n\n10,000 code generation problems of varying difficulties. Covers simple\nintroductory problems, interview-level problems, and coding competition\nchallenges\n\n## Pros#\n\n  1. Simple code base. See evaluation guide here\n\n  2. A ton of Coding specific evaluations, with multiple difficulty levels.\n\n## Cons#\n\n  1. Most of the code benchmarks are python. So it may not work too well for other languages.\n\n  2. Isn\u2019t written with extensibility in mind, and mostly coded for testing python codebases.\n\n## Verdict#\n\n  * \ud83d\udc4e - Not something to use for custom real world \u201capp\u201d related benchmarking\n\n# 3) HumanEval#\n\nFrom OpenAI again, hand-written set of evaluations\n\nRepo: https://github.com/openai/human-eval\n\nPaper: Evaluating LLMs\n\n> We evaluate functional correctness on a set of 164 handwritten programming\n> problems, which we call the HumanEval dataset. Each problem includes a\n> function signature, docstring, body, and several unit tests, with an average\n> of 7.7 tests per problem\n\n## Pros#\n\n  1. Pretty simple codebase, and good examples\n\n## Cons#\n\n  1. Mostly python evaluations\n\n## Verdict#\n\nIf not testing python, this one is a \ud83d\udc4e\n\n# 4) CanAiCode#\n\nRepo: https://github.com/the-crypt-keeper/can-ai-\ncode/blob/main/prompts/codellama-input-v2.txt\n\nLeaderboard: https://huggingface.co/spaces/mike-ravkine/can-ai-code-results\n\n## Pros#\n\n  1. Supports Javascript, and not just python test cases.\n\n  2. Template based generation of test cases. See template prompt for starcoder\n\n    \n    \n    {% if language == \"python\" %}<fim_prefix>def {{Signature}}: '''a function {{Input}} that returns {{Output}}{% if Fact %} given {{Fact}}{% endif %}''' <fim_suffix> # another function{% endif %} {% if language == \"javascript\" %}<fim_prefix>// a function {{Input}} that returns {{Output}}{% if Fact %} given {{Fact}}{% endif %} function {{Signature}} { <fim_suffix> } // another function{% endif %}<fim_middle>\n\n  3. Combined with yaml for tests\n\n    \n    \n    .Checks: &Checks FactorialZeroShot: Signature: \"factorial(n)\" Input: \"with input n\" Output: \"the factorial of n using iteration\" Description: \"See if the model can implement a well known function\" Checks: one_argument: assert: \"len(f.args)\" eq: 1 returns_list: assert: \"isinstance(f.call(1),int)\" eq: true value_0: assert: \"f.call(1)\" eq: 1 value_5: assert: \"f.call(5)\" eq: 120\n\n## Cons#\n\n1 - Unfortunately, it is not customizable beyond simple input-output testing.\n\n# 5) MultiPL-E#\n\nMeant to tests code LLMs on multiple programming languages.\n\n> A system for translating unit test-driven neural code generation benchmarks\n> to new languages. We have used MultiPL-E to translate two popular Python\n> benchmarks (HumanEval and MBPP) to 18 other programming languages.\n\nExamples shown here: https://nuprl.github.io/MultiPL-E/\n\nPaper: MultiPL-E\n\nRepo: https://github.com/nuprl/MultiPL-E\n\n# Pros#\n\n  1. Examples on running JS tests: https://github.com/nuprl/MultiPL-E/blob/main/prompts/humaneval-js-keep.jsonl\n\n  2. Enables writing tests as a function, so not just simple input output comparisons.\n\n  3. Adding new tests seems simple: See https://nuprl.github.io/MultiPL-E/new_benchmark.html\n\n# Cons#\n\n  1. While the tutorial makes it sound like writing the test cases is really simple. This doesn\u2019t seem to be the case\n\n  2. https://github.com/nuprl/MultiPL-E/blob/main/prompts/humaneval-r-remove.jsonl\n\n  3. Each of the test case needs to be decoded in a particular jsonl format, with escape characters fixed etc.\n\n## Verdict#\n\n\ud83d\udc4d - This could work for building a react benchmark. But may not be easy to add\nnew test cases to.\n\n# 6) RepoBench#\n\nPaper: RepoBench\n\nRepo: https://github.com/Leolty/repobench\n\nValidates LLM on 3 tasks\n\n1 - Retrieval Task: Ability to retrieve the right contextual files.\n\n2 - Completion Task: Ability to complete next line, given the context files.\n\n3 - Combined Task: Retrieval + Completion\n\nSome interesting points noted in the paper:\n\n> Python Retrieval Shows Higher Accuracy Than Java: The language-specific\n> results show that Python tasks typically show higher accuracy than Java\n> across all retrieval methods. This discrepancy might be attributed to\n> Python\u2019s simpler syntax and less verbose nature, potentially reducing the\n> variability of similar code snippets.\n\n> Pronounced Performance Differences in Java for RepoBenchC-2k: The evaluation\n> on Java showcases a marked differentiation in model performance: Codex\n> notably stands out as the superior model, followed by StarCoder, while\n> CodeGen largely lags behind.\n\nWhile there are some intuitive reasons cited, this clearly shows that\nbenchmarks on Python may not directly apply to React / Typescript codebases.\n\n### Interesting bits#\n\nThe project is easy to read and some interesting files are\n\n1 - Metrics: ExactMatch, Similarity, and Accuracy@K\nhttps://github.com/Leolty/repobench/blob/main/evaluation/metrics.py. Note:\ntheir accuracy@k is not a probabilistic calculation like the pass@k metric\nintroduced in HumanEval, and refers to the number of accurate codes retrieved\nout of correct codes.\n\n2 - Retriever:\nhttps://github.com/Leolty/repobench/blob/main/retriever/retriever.py\n\n3 - Similarity (Jaccard, Edit, Cosine):\nhttps://github.com/Leolty/repobench/blob/main/retriever/similarity.py\n\n4 - Promp constructor:\nhttps://github.com/Leolty/repobench/blob/main/data/utils.py\n\n## Pros#\n\n1 - Easy to understand\n\n2 - Repo level context understanding.\n\n3 - Usage of Google drive for dataset.\n\n4 - Multiple languages supported with various similarity metrics on next line.\n\n## Cons#\n\nThe question this benchmark is trying to answer is different from what we\nneed.\n\nWe require unit-test and visual accuracy, assuming the right context is\nalready given.\n\n## Verdict#\n\nNot applicable.\n\n# Conclusion#\n\nSo far, the only ones that meet what I\u2019m looking for are the open-ai evals,\nand the MultiPL-E benchmark.\n\nIdeally, if these benchmarks were easier to prepare and mimicked the way we\nactually write code / test cases, then it would be much easier to extend.\n\nSo after this research, I believe the best answer is to build a new\n\u201cReactBench\u201d - a benchmark that mimics how React code is structured and is\ngeared towards accuracy on Typescript / React with unit-testing and\nsnapshotting.\n\n  * Machine-Learning\n  * AI\n\n\u00a9 2024 KiloBytes by KB Powered by Hugo & PaperMod\n\n", "frontpage": false}
