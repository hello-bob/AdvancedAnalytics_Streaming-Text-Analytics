{"aid": "40123029", "title": "The mainframe era in CERN (2004)", "url": "https://cerncourier.com/a/computing-at-cern-the-mainframe-era/", "domain": "cerncourier.com", "votes": 1, "user": "ngcc_hk", "posted_at": "2024-04-22 22:22:49", "comments": 0, "source_title": "Computing at CERN: the mainframe era \u2013 CERN Courier", "source_text": "Computing at CERN: the mainframe era \u2013 CERN Courier\n\nHome\n\n  * Physics\n\n    * Latest in Physics\n    * Antimatter\n    * Astrophysics and cosmology\n    * Dark universe\n    * Flavour physics\n    * Higgs and electroweak\n    * Neutrinos\n    * Searches for new physics\n    * Strong interactions\n    * Theory\n  * Technology\n\n    * Latest in Technology\n    * Accelerators\n    * Applications\n    * Computing\n    * Detectors\n  * Community\n\n    * Latest in Community\n    * Culture and history\n    * Education and outreach\n    * People\n    * Careers\n    * Policy\n    * Scientific practice\n    * Webinars\n    * Events\n    * Meeting reports\n  * In focus\n  * Magazine\n  * Jobs\n  * Follow CERN Courier on Linkedin\n  * Follow CERN Courier on Twitter\n\n  * Jobs\n  * Follow CERN Courier on Linkedin\n  * Follow CERN Courier on Twitter\n\n### Topics\n\n  * Nuclear Physics\n\n  * IOP Publishing\n\n  * Jobs\n\n#### Reset your password\n\n#### Registration complete\n\nThank you for registering If you'd like to change your details at any time,\nplease visit My account\n\nClose\n\n  * Share on Facebook\n  * Share on Twitter\n  * Share on Linkedin\n  * Share via email\n  * Print this article\n\n  * Computing\n  * Feature\n\n# Computing at CERN: the mainframe era\n\n5 September 2004\n\nChris Jones takes a look back at the heyday of the computer mainframe through\na selection of \u201cmemory bytes\u201d.\n\nIn June 1996 computing staff at CERN turned off the IBM 3090 for the last\ntime, so marking the end of an era that had lasted 40 years. In May 1956 CERN\nhad signed the purchasing contract for its first mainframe computer \u2013 a\nFerranti Mercury with a clock cycle 200,000 times slower than modern PCs. Now,\nthe age of the mainframe is gone, replaced by \u201cscalable solutions\u201d based on\nUnix \u201cboxes\u201d and PCs, and CERN and its collaborating institutes are in the\nprocess of installing several tens of thousands of PCs to help satisfy\ncomputing requirements for the Large Hadron Collider.\n\nThe Mercury was a first-generation vacuum tube (valve) machine with a 60\nmicrosecond clock cycle. It took five cycles \u2013 300 microseconds \u2013 to multiply\n40-bit words and had no hardware division, a function that had to be\nprogrammed. The machine took two years to build, arriving at CERN in 1958,\nwhich was a year later than originally foreseen. Programming by users was\npossible from the end of 1958 with a language called Autocode. Input and\noutput (I/O) was by paper tape, although magnetic tape units were added in\n1962. Indeed, the I/O proved something of a limitation, for example when the\nMercury was put to use in the analysis of paper tape produced by the\ninstruments used to scan and measure bubble-chamber film. The work of the fast\nand powerful central processing unit (CPU) was held up by the sluggish I/O. By\n1959 it was already clear that a more powerful system was needed to deal with\nthe streams of data coming from the experiments at CERN.\n\nA panoramic view of CERN\u2019s computer centre in the mid-1980s, during the era of\nthe combined service provided by IBM and Siemens. (Courtesy Chris Jones.)\n\nThe 1960s arrived at the computing centre initially in the form of an IBM 709\nin January 1961. Although it was still based on valves, it could be programmed\nin FORTRAN, read instructions written on cards, and read and write magnetic\ntape. Its CPU was four to five times faster than that of the Mercury, but it\ncame with a price tag of 10 millions Swiss francs (in 1960 prices!). Only two\nyears later it was replaced by an IBM 7090, a transistorized version of the\nsame machine with a 2.18 microsecond clock cycle. This marked the end for the\nvalve machines, and after a period in which it was dedicated to a single\nexperiment at CERN (the Missing Mass Spectrometer), the Mercury was given to\nthe Academy of Mining and Metallurgy in Krakow. With the 7090 the physicists\ncould really take advantage of all the developments that had begun on the 709,\nsuch as on-line connection to devices including the flying spot digitizers to\nmeasure film from bubble and spark chambers. More than 300,000 frames of\nspark-chamber film were automatically scanned and measured in record time with\nthe 7090. This period also saw the first on-line connection to film-less\ndetectors, recording data on magnetic tape.\n\nCERN\u2019s IBM 709 computer is unloaded at Geneva\u2019s Cointrin Airport in 1961,\nunder the watchful eye of a Swiss customs officer, at right.\n\nIn 1965 the first CDC machine arrived at CERN \u2013 the 6600 designed by computer\npioneer Seymour Cray, with a CPU clock cycle of 100 ns and a processing power\n10 times that of the IBM 7090. With serial number 3, it was a pre-production\nseries machine. It had disks more than 1 m in diameter \u2013 which could hold 500\nmillion bits (64 megabytes) and subsequently made neat coffee tables \u2013 tape\nunits and a high-speed card reader. However, as Paolo Zanella, who became\ndivision leader from 1976 until 1988, recalled, \u201cThe introduction of such a\ncomplex system was by no means trivial and CERN experienced one of the most\npainful periods in its computing history. The coupling of unstable hardware to\nshaky software resulted in a long traumatic effort to offer a reliable\nservice.\u201d Eventually the 6600 was able to realise its potential, but only\nafter less-powerful machines had been brought in to cope with the increasing\ndemands of the users. Then in 1972 it was joined by a still more powerful\nsibling, the CDC 7600, the most powerful computer of the time and five times\nfaster than the 6600, but again there were similar painful \u201cteething\nproblems\u201d.\n\nWith a speed of just over 10 Mips (millions of instructions per second) and\nsuperb floating-point performance, the 7600 was, for its time, a veritable\n\u201cFerrari\u201d of computing. But it was a Ferrari with a very difficult running-in\nperiod. The system software was again late and inadequate. In the first months\nthe machine had a bad ground-loop problem causing intermittent faults and\neventually requiring all modules to be fitted with sheathed rubber bands. It\nwas a magnificent engine for its time whose reliability and tape handling just\ndid not perform to the levels needed, in particular by the electronic\nexperiments. Its superior floating-point capabilities were valuable for\nprocessing data from bubble-chamber experiments with their relatively low data\nrates, but for the fast electronic experiments the \u201clog jam\u201d of the tape\ndrives was a major problem.\n\nSo a second revolution occurred with the reintroduction of an IBM system, the\n370/168, in 1976, which was able to meet a wider range of users\u2019 requirements.\nNot only did this machine bring dependable modern tape drives, it also\ndemonstrated that computer hardware could work reliably and it ushered in the\nheyday of the mainframe, with its robotic mass storage system and a laser\nprinter operating at 19,000 lines per minute. With a CPU cycle of 80 ns, 4\nmegabytes (later 5) of semiconductor memory and a high-speed multiply unit, it\nbecame the \u201cCERN unit\u201d of physics data-processing power, corresponding to 3-4\nMips. Moreover, the advent of the laser printer, with its ability to print\nbitmaps rather than simple mono-spaced characters, heralded the beginning of\nscientific text processing and the end for the plotters with their coloured\npens (to say nothing of typewriters).\n\nThe arrival of the IBM 370/168 in 1976 ushered in the heyday of the mainframe\nand the \u201cCERN unit\u201d of physics data processing.\n\nThe IBM also brought with it the MVS (Multiple Virtual Storage) operating\nsystem, with its pedantic Job Control Language, and it provided the\nopportunity for CERN to introduce WYLBUR, the well-loved, cleverly designed\nand friendly time-sharing system developed at SLAC, together with its\nbeautifully handwritten and illustrated manual by John Ehrman. WYLBUR was a\nmasterpiece of design, achieving miracles with little power (at the time)\nshared amongst many simultaneous users. It won friends with its accommodating\ncharacter and began the exit of punch-card machinery as computer terminals\nwere introduced across the lab. It was also well interfaced with the IBM Mass\nStore, a unique file storage device, and this provided great convenience for\nfile handling and physics data sample processing. At its peak WYLBUR served\naround 1200 users per week.\n\nThe IBM 370/168 was the starting point for the IBM-based services in the\ncomputer centre and was followed by a series of more powerful machines: the\n3032, the 3081, several 3090s and finally the ES/9000. In addition, a sister\nline of compatible machines from Siemens/Fujitsu was introduced and together\nthey provided a single system in a manner transparent to the users. This\nservice carried the bulk of the computer users, more than 6000 per week, and\nmost of the data handing right up to the end of its life in 1996. At its peak\naround 1995 the IBM service provided a central processor power around a\nquarter of a top PC today, but the data-processing capacity was outstanding.\n\nDuring this period CERN\u2019s project for the Large Electron Positron (LEP)\ncollider brought its own challenges, together with a planning review in 1983\nof the computing requirements for the LEP era. Attractive alternative systems\nto the mainframe began to appear over the horizon, presenting computing\nservices with some difficult choices. The DEC VAX machines, used by many\nphysics groups \u2013 and subsequently introduced as a successful central facility\n\u2013 were well liked for the excellent VMS operating system. On another scale the\ntechnical jump in functionality that was appearing on the new personal\nworkstations, for example from Apollo \u2013 such as a fully bit-mapped screen and\na \u201cwhole half a megabyte of memory\u201d for a single user \u2013 were an obvious major\nattraction for serious computer-code developers, albeit at a cost that was not\nyet within the reach of many. It is perhaps worth reflecting that in 1983 the\nPC used the DOS operating system and a character-based screen, whilst the\nMacintosh had not yet been announced, so bit-mapped screens were a major step\nforward. (To put that in context, another recommendation of the above planning\nreview was that CERN should install a local-area network and that Ethernet was\nthe best candidate for this.)\n\nThe user-friendly nature of the WYLBUR time-sharing system, which was\ndeveloped at SLAC, was reflected in its beautifully handwritten and\nillustrated manual by John Ehrman.\n\nThe future clearly held exciting times, but some pragmatic decisions about\nfinances, functionality, capacity and tape handling capacity had to be made.\nIt was agreed that for the LEP era the IBM-based services would move to the\ntruly interactive VM/CMS operating system as used at SLAC. (WYLBUR was really\na clever editor submitting jobs to batch processing.) This led to a most\nimportant development, the HEPVM collaboration. It was possible and indeed\ndesirable to modify the VM/CMS operating system to suit the needs of the user\ncommunity. All the high-energy physics (HEP) sites running VM/CMS were setting\nout to do exactly this, as indeed they had done with many previous operating\nsystems. To some extent each site started off as if it were their sovereign\nright to do this better than the others. In order to defend the rights of the\nitinerant physicist, in 1983 Norman McCubbin from the Rutherford Appleton\nLaboratory made the radical but irresistible proposal: \u201cdon\u2019t do it better, do\nit the same!\u201d\n\nThe HEPVM collaboration comprised most of the sites who ran VM/CMS as an\noperating system and who had LEP physicists as clients. This ranged from large\ndedicated sites such as SLAC, CERN and IN2P3, to university sites where the\nphysicists were far from being the only clients. It was of course impossible\nto impose upon the diverse managements involved, so it was a question of\ndiscussion and explanation and working at the issues. Two important products\nresulted from this collaboration. A HEPVM tape was distributed to more than 30\nsites, containing all the code necessary for producing a unified HEP\nenvironment, and the \u201cconcept of collaboration between sites\u201d was established\nas a normal way to proceed. The subsequent off-spring, HEPiX and HEPNT, have\ncontinued the tradition of collaboration and it goes without saying that such\ncollaboration will have to take a higher level again in order to make Grid\ncomputing successful.\n\n### The era of the supercomputer\n\nThe 1980s also saw the advent of the supercomputer. The CRAY X-MP\nsupercomputer, which arrived at CERN in January 1988, was the logical\nsuccessor to Seymour Cray\u2019s CDC 7600 at CERN, and a triumph of price\nnegotiation. The combined scalar performance of its four processors was about\na quarter of the largest IBM installed at CERN, but it had strong vector\nfloating-point performance. Its colourful presence resolved the question as to\nwhether the physics codes could really profit from vector capabilities, and\nprobably the greatest benefit to CERN from the CRAY was to the engineers whose\napplications, for example in finite element analysis and accelerator design,\nexcelled on this machine. The decision was also taken to work together with\nCRAY to pioneer Unix as the operating system, and this work was no doubt of\nuse to later generations of machines running Unix at CERN.\n\nThe CRAY X-MP brought vector capabilities \u2013 and a colourful presence \u2013 to\nCERN\u2019s computer centre with its arrival in 1988.\n\nThroughout most of the mainframe period the power delivered to users had\ndoubled approximately every 3.5 years \u2013 the CDC 7600 lasted an astonishing 12\nyears. The arrival of the complete processor on a CMOS chip, which conformed\nto Moore\u2019s law of doubling speed every 18 months, was an irresistible force\nthat sounded the eventual replacement of mainframe systems, although a number\nof other issues had to be solved first, including notably the provision of\nreliable tape-handling facilities. The heyday of the mainframe thus eventually\ncame to an inevitable end.\n\nOne very positive feature of the mainframe era at CERN was the joint project\nteams with the major manufacturers, in particular those of IBM and DEC. The\npresence of around say 20 engineers from such a company on-site led to\nextremely good service, not only from the local staff but also through direct\ncontacts to the development teams in America. It was not unknown for a\ncritical bug, discovered during the evening at CERN, to be fixed overnight by\nthe development team in America and installed for the CERN service the next\nmorning, a sharp contrast to the service available in these days of commodity\ncomputing. The manufacturers on their side saw the physicists\u2019 use of their\ncomputers as pushing the limits of what was possible and pointing the way to\nthe needs of other more straightforward customers in several years time. Hence\ntheir willingness to install completely new products, sometimes from their\nresearch laboratories, and often free of charge, as a way of getting them\nused, appraised and de-bugged. The requirements from the physicists made their\nway back into products and into the operating systems. This was one particular\nand successful way for particle physics to transfer its technology and\nexpertise to the world at large. In addition, the joint projects provided a\nframework for excellent pricing, allowing particle physics to receive much\nmore computer equipment than they could normally have paid for.\n\n### Further reading\n\nThis article has been a personal look at some of the aspects of mainframe\ncomputing at CERN, and could not in the space available provide anything more\nthan a few snapshots. It has benefited from some of the details contained in\nthe more in-depth look at the first 30 years of computing at CERN written for\nthe CERN Computing Newsletter by Paolo Zanella in 1990 and available on the\nWeb in three parts at: http://cnlart.web.cern.ch/cnlart/2001/002/cern-\ncomputing; http://cnlart.web.cern.ch/cnlart/2001/003/comp30;\nhttp://cnlart.web.cern.ch/cnlart/2001/003/comp30-last.\n\nChris Jones, CERN.\n\n### New-issue alert: sign up today\n\nRead previous\n\n  * Neutrinos\n  * Feature\n\n#### The beta-decay route to a high-flux neutrino source\n\nRead next\n\n  * Higgs and electroweak\n  * Feature\n\n#### The discovery of the weak neutral currents\n\n### more from Cern Courier\n\nRead article 'Advances in cosmology'\n\n    * Astrophysics and cosmology\n    * Review\n\n## Advances in cosmology\n\nRead article 'Extremely Brilliant Source illuminates Paganini\u2019s favourite\nviolin'\n\n    * Culture and history\n    * News\n\n## Extremely Brilliant Source illuminates Paganini\u2019s favourite violin\n\nRead article 'New CERNs for a fractured world'\n\n    * Policy\n    * Opinion\n\n## New CERNs for a fractured world\n\n### CERN Courier Jobs\n\n  * United States | Brookhaven National Laboratory\n\n#### Director of the Scientific Data and Computing Center (SDCC)\n\n  * United States | Lawrence Livermore National Laboratory\n\n#### Computational Physicist\n\n  * Germany | GSI Helmholzzentrum fuer Schwerionenforschung GmbH\n\n#### Scientific Managing Director (m/f/d)\n\n### Events\n\n  * Accelerators | Conference IPAC 2024 19\u201424 May 2024 | Nashville, US\n  * Flavour physics | Conference FPCP 2024 27\u201431 May 2024 | Bangkok, Thailand\n  * Strong interactions | Conference SQM 2024 3\u20147 June 2024 | Strasbourg, France\n\nCopyright \u00a9 2024 by CERN\n\n#### Explore Cern Courier\n\n  * About CERN Courier\n  * Advertising options\n  * Feedback\n  * Sign up for alerting\n\n#### MORE INFORMATION\n\n  * Copyright\n  * Terms and Conditions\n  * Privacy and Cookies\n  * Modern Slavery Act\n  * Disclaimer\n\n#### OUR MISSION\n\nCERN Courier is essential reading for the international high-energy physics\ncommunity. Highlighting the latest research and project developments from\naround the world, CERN Courier offers a unique record of the ongoing endeavour\nto advance our understanding of the basic laws of nature.\n\nProduced for CERN by IOP Publishing\n\n", "frontpage": false}
