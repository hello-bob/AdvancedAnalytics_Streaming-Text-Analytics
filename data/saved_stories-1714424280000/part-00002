{"aid": "40199147", "title": "Moravec's Paradox", "url": "https://en.wikipedia.org/wiki/Moravec%27s_paradox", "domain": "wikipedia.org", "votes": 1, "user": "simonebrunozzi", "posted_at": "2024-04-29 14:57:05", "comments": 0, "source_title": "Moravec's paradox", "source_text": "Moravec's paradox - Wikipedia\n\nJump to content\n\nSearch\n\n# Moravec's paradox\n\n  * Afrikaans\n  * Catal\u00e0\n  * \u010ce\u0161tina\n  * Deutsch\n  * Espa\u00f1ol\n  * Fran\u00e7ais\n  * \ud55c\uad6d\uc5b4\n  * \u0540\u0561\u0575\u0565\u0580\u0565\u0576\n  * Italiano\n  * \u05e2\u05d1\u05e8\u05d9\u05ea\n  * Magyar\n  * \u65e5\u672c\u8a9e\n  * Polski\n  * Portugu\u00eas\n  * \u0420\u0443\u0441\u0441\u043a\u0438\u0439\n  * Svenska\n  * \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430\n  * \u4e2d\u6587\n\nEdit links\n\nFrom Wikipedia, the free encyclopedia\n\nObservation that perception requires more computation than reasoning\n\nMoravec's paradox is the observation in artificial intelligence and robotics\nthat, contrary to traditional assumptions, reasoning requires very little\ncomputation, but sensorimotor and perception skills require enormous\ncomputational resources. The principle was articulated by Hans Moravec, Rodney\nBrooks, Marvin Minsky and others in the 1980s. Moravec wrote in 1988, \"it is\ncomparatively easy to make computers exhibit adult level performance on\nintelligence tests or playing checkers, and difficult or impossible to give\nthem the skills of a one-year-old when it comes to perception and\nmobility\".^[1]\n\nSimilarly, Minsky emphasized that the most difficult human skills to reverse\nengineer are those that are below the level of conscious awareness. \"In\ngeneral, we're least aware of what our minds do best\", he wrote, and added\n\"we're more aware of simple processes that don't work well than of complex\nones that work flawlessly\".^[2] Steven Pinker wrote in 1994 that \"the main\nlesson of thirty-five years of AI research is that the hard problems are easy\nand the easy problems are hard.\"^[3]\n\nBy the 2020s, in accordance to Moore's law, computers were hundreds of\nmillions of times faster than in the 1970s, and the additional computer power\nwas finally sufficient to begin to handle perception and sensory skills, as\nMoravec had predicted in 1976.^[4] In 2017, leading machine learning\nresearcher Andrew Ng presented a \"highly imperfect rule of thumb\", that\n\"almost anything a typical human can do with less than one second of mental\nthought, we can probably now or in the near future automate using AI.\"^[5]\nThere is currently no consensus as to which tasks AI tends to excel at.^[6]\n\n## The biological basis of human skills[edit]\n\nOne possible explanation of the paradox, offered by Moravec, is based on\nevolution. All human skills are implemented biologically, using machinery\ndesigned by the process of natural selection. In the course of their\nevolution, natural selection has tended to preserve design improvements and\noptimizations. The older a skill is, the more time natural selection has had\nto improve the design. Abstract thought developed only very recently, and\nconsequently, we should not expect its implementation to be particularly\nefficient.\n\nAs Moravec writes:\n\n> Encoded in the large, highly evolved sensory and motor portions of the human\n> brain is a billion years of experience about the nature of the world and how\n> to survive in it. The deliberate process we call reasoning is, I believe,\n> the thinnest veneer of human thought, effective only because it is supported\n> by this much older and much more powerful, though usually unconscious,\n> sensorimotor knowledge. We are all prodigious olympians in perceptual and\n> motor areas, so good that we make the difficult look easy. Abstract thought,\n> though, is a new trick, perhaps less than 100 thousand years old. We have\n> not yet mastered it. It is not all that intrinsically difficult; it just\n> seems so when we do it.^[7]\n\nA compact way to express this argument would be:\n\n  * We should expect the difficulty of reverse-engineering any human skill to be roughly proportional to the amount of time that skill has been evolving in animals.\n  * The oldest human skills are largely unconscious and so appear to us to be effortless.\n  * Therefore, we should expect skills that appear effortless to be difficult to reverse-engineer, but skills that require effort may not necessarily be difficult to engineer at all.\n\nSome examples of skills that have been evolving for millions of years:\nrecognizing a face, moving around in space, judging people's motivations,\ncatching a ball, recognizing a voice, setting appropriate goals, paying\nattention to things that are interesting; anything to do with perception,\nattention, visualization, motor skills, social skills and so on.\n\nSome examples of skills that have appeared more recently: mathematics,\nengineering, games, logic and scientific reasoning. These are hard for us\nbecause they are not what our bodies and brains were primarily evolved to do.\nThese are skills and techniques that were acquired recently, in historical\ntime, and have had at most a few thousand years to be refined, mostly by\ncultural evolution.\n\n## Historical influence on artificial intelligence[edit]\n\nIn the early days of artificial intelligence research, leading researchers\noften predicted that they would be able to create thinking machines in just a\nfew decades (see history of artificial intelligence). Their optimism stemmed\nin part from the fact that they had been successful at writing programs that\nused logic, solved algebra and geometry problems and played games like\ncheckers and chess. Logic and algebra are difficult for people and are\nconsidered a sign of intelligence. Many prominent researchers^[a] assumed\nthat, having (almost) solved the \"hard\" problems, the \"easy\" problems of\nvision and commonsense reasoning would soon fall into place. They were wrong\n(see also AI winter), and one reason is that these problems are not easy at\nall, but incredibly difficult. The fact that they had solved problems like\nlogic and algebra was irrelevant, because these problems are extremely easy\nfor machines to solve.^[b]\n\nRodney Brooks explains that, according to early AI research, intelligence was\n\"best characterized as the things that highly educated male scientists found\nchallenging\", such as chess, symbolic integration, proving mathematical\ntheorems and solving complicated word algebra problems. \"The things that\nchildren of four or five years could do effortlessly, such as visually\ndistinguishing between a coffee cup and a chair, or walking around on two\nlegs, or finding their way from their bedroom to the living room were not\nthought of as activities requiring intelligence.\"^[9]\n\nIn the 1980s, this would lead Brooks to pursue a new direction in artificial\nintelligence and robotics research. He decided to build intelligent machines\nthat had \"No cognition. Just sensing and action. That is all I would build and\ncompletely leave out what traditionally was thought of as the intelligence of\nartificial intelligence.\"^[9] He called this new direction \"Nouvelle AI\".^[10]\n\n## Reception[edit]\n\nLinguist and cognitive scientist Steven Pinker considers this the main lesson\nuncovered by AI researchers. In his 1994 book The Language Instinct, he wrote:\n\n> The main lesson of thirty-five years of AI research is that the hard\n> problems are easy and the easy problems are hard. The mental abilities of a\n> four-year-old that we take for granted \u2013 recognizing a face, lifting a\n> pencil, walking across a room, answering a question \u2013 in fact solve some of\n> the hardest engineering problems ever conceived... As the new generation of\n> intelligent devices appears, it will be the stock analysts and petrochemical\n> engineers and parole board members who are in danger of being replaced by\n> machines. The gardeners, receptionists, and cooks are secure in their jobs\n> for decades to come.^[11]\n\n## See also[edit]\n\n  * AI effect\n  * Embodied cognition\n  * History of artificial intelligence\n  * Subsumption architecture\n\n## Notes[edit]\n\n  1. ^ Anthony Zador wrote in 2019: \"Herbert Simon, a pioneer of artificial intelligence (AI), famously predicted in 1965 that \u201cmachines will be capable, within twenty years, of doing any work a man can do\u201d \u2014 to achieve [human-level] general AI.\"^[8]\n  2. ^ These are not the only reasons that their predictions did not come true: see History of artificial intelligence \u00a7 Problems.\n\n## References[edit]\n\n  1. ^ Moravec 1988, p. 15.\n  2. ^ Minsky 1986, p. 2.\n  3. ^ Pinker 2007, p. 190.\n  4. ^ Moravec 1976.\n  5. ^ Lee 2017.\n  6. ^ Brynjolfsson & Mitchell 2017.\n  7. ^ Moravec 1988, pp. 15\u201316.\n  8. ^ Zador 2019.\n  9. ^ Jump up to: ^a ^b Brooks (2002), quoted in McCorduck (2004, p. 456)\n  10. ^ Brooks 1986.\n  11. ^ Pinker 2007, pp. 190\u201391.\n\n## Bibliography[edit]\n\n  * Brooks, Rodney (1986), Intelligence Without Representation, MIT Artificial Intelligence Laboratory\n  * Brooks, Rodney (2002), Flesh and Machines, Pantheon Books\n  * Brynjolfsson, Erik; Mitchell, Tom (22 December 2017). \"What can machine learning do? Workforce implications\". Science. 358 (6370): 1530\u20131534. Bibcode:2017Sci...358.1530B. doi:10.1126/science.aap8062. Retrieved 7 May 2018.\n  * Lee, Amanda (14 June 2017). \"Will your job still exist in 10 years when the robots arrive?\". South China Morning Post. Retrieved 7 May 2018.\n  * Minsky, Marvin (1986), The Society of Mind, Simon and Schuster, p. 29\n  * Moravec, Hans (1976), The Role of Raw Power in Intelligence, archived from the original on 3 March 2016, retrieved 16 October 2008\n  * Moravec, Hans (1988), Mind Children, Harvard University Press\n  * McCorduck, Pamela (2004), Machines Who Think (2nd ed.), Natick, MA: A. K. Peters, Ltd., ISBN 1-56881-205-1, p. 456.\n  * Pinker, Steven (September 4, 2007) [1994], The Language Instinct, Perennial Modern Classics, Harper, ISBN 978-0-06-133646-1\n  * Zador, Anthony (2019-08-21). \"A critique of pure learning and what artificial neural networks can learn from animal brains\". Nature Communications. 10 (1): 3770. Bibcode:2019NatCo..10.3770Z. doi:10.1038/s41467-019-11786-6. PMC 6704116. PMID 31434893.\n\n## External links[edit]\n\n  * Explanation of the XKCD comic about Moravec's paradox\n\nRetrieved from\n\"https://en.wikipedia.org/w/index.php?title=Moravec%27s_paradox&oldid=1213589842\"\n\nCategories:\n\n  * Philosophy of artificial intelligence\n  * Paradoxes\n\nHidden categories:\n\n  * Articles with short description\n  * Short description matches Wikidata\n\n  * This page was last edited on 13 March 2024, at 23:32 (UTC).\n  * Text is available under the Creative Commons Attribution-ShareAlike License 4.0; additional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\n\n  * Privacy policy\n  * About Wikipedia\n  * Disclaimers\n  * Contact Wikipedia\n  * Code of Conduct\n  * Developers\n  * Statistics\n  * Cookie statement\n  * Mobile view\n  * Edit preview settings\n\n", "frontpage": false}
