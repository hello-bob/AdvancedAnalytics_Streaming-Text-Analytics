{"aid": "40234228", "title": "Introduction to Vectorization", "url": "https://thepalindrome.org/p/an-introduction-to-vectorization", "domain": "thepalindrome.org", "votes": 2, "user": "sujayk_33", "posted_at": "2024-05-02 09:16:59", "comments": 0, "source_title": "An introduction to vectorization", "source_text": "An introduction to vectorization - by Tivadar Danka\n\n# The Palindrome\n\nShare this post\n\n#### An introduction to vectorization\n\nthepalindrome.org\n\n#### Discover more from The Palindrome\n\nMathematics for engineers, scientists, and other curious minds. Explaining\nthings like your teachers should have, but probably never did.\n\nOver 15,000 subscribers\n\nContinue reading\n\nSign in\n\n# An introduction to vectorization\n\n### Making expressions do more, faster\n\nTivadar Danka\n\nMay 01, 2024\n\n26\n\nShare this post\n\n#### An introduction to vectorization\n\nthepalindrome.org\n\nShare\n\nThere\u2019s a pattern in machine learning that blows my mind every time, even\nthough I\u2019ve seen it more than I can count.\n\nLook at this expression:\n\nYes, I know. You are more than familiar with linear regression; we are not\nhere to discuss that. I want to share a wonderful mathematical principle with\nyou, learning through the example of linear regression.\n\nDepending on what we understand by a, x, b, +, and \u00b7, the expression \u201cax + b\u201d\ncan either be the very first machine learning model a student encounters or\nthe main component of a powerful neural network.\n\nIts evolution from basic to state-of-the-art was shaped by the two great\nforces of mathematics:\n\n  * generalizing the meaning of simple symbols such as + and \u00b7 to make them do more,\n\n  * and then abstracting the complex symbols into a-s, b-s, and x-es to keep them simple.\n\nThis dance of generalization and abstraction is the essence of mathematics;\nit\u2019s why we can treat functions as vectors, use matrices as exponents, build\nthe foundations of mathematics by drawing dots and arrows, and many more.\n\nLet\u2019s see the profound lesson that ax + b can teach us.\n\nThe Palindrome is a reader-supported publication. To receive new posts and\nsupport my work, consider becoming a free or paid subscriber.\n\n#\n\nFitting a line to a cluster of points\n\nThe fundamental problem of machine learning: predicting one variable from\nanother. Mathematically speaking, we are looking for a function y = h(x) that\ndescribes the relation between the predictor variable x and the target\nvariable y.\n\nOn the ground floor of machine learning, the simplest idea is to assume that\nthe relation is linear; that is, y = ax + b for some parameters a and b.\n\nIn other words, if x is the predictor and a quantifies its effect on the\ntarget variable y, then the unwritten \u201c\u00b7\u201c operation calculates x\u2019s total\ncontribution to y.\n\nIn yet another word, we are fitting a line to a cluster of points.\n\nThis is called linear regression. You know all about it, but let\u2019s recap two\nessential facts: the parameter a quantifies how the target variable y changes\nwhen the predictor variable x moves, and b describes the bias.\n\nHow will ax + b become the ubiquitous building block of neural networks?\n\nLet\u2019s kickstart that cycle of mathematical generalization and abstraction by\nstepping out from the plane into the space.\n\n#\n\nLaunching into higher dimensions\n\nIs ax + b a good model?\n\nNot in most cases. One of the first things that comes to mind is its failure\nto deal with multiple predictor variables. Say, y describes the USD/m2 price\nof real estate or the lactic acid production of a microbial culture.\n\nDo we only have a single predictor variable?\n\nNo. Real estate prices are influenced by several factors, and hundreds of\nvarious metabolites drive lactic acid-producing microbial culture processes.\nLife is complex, and it\u2019s extremely rare that an effect only has a singular\ncause. Instead of a lonely x, we have a sequence: x1, x2, ..., xn.\n\nAs each predictor variable has a different effect on the target, the simplest\nis to compute the individual effects aixi for some parameter ai, then mix all\nthe effects by summing them together, obtaining\n\nNow that we have generalized our model, it\u2019s time for abstraction.\n\nMathematically speaking, our predictor variable is stored in the vector\n\nwhere each xi describes a feature; and similarly, we have a vector of\nparameters\n\nIf we define the operation \u201c\u00b7\u201d between two vectors by\n\nthen our model takes the form\n\nwhich is the very same expression as the previous one, with an overloaded\nmultiplication operator. Recall what I claimed a few lines above:\n\n> if x is the predictor and a quantifies its effect on the target variable y,\n> then the unwritten \u201c\u00b7\u201c operation calculates x\u2019s total contribution to y.\n\nWe generalized \u201c\u00b7\u201c to keep this specific property. The result is the famous\ndot product, which is the cornerstone of mathematics, full of useful\nproperties and geometric interpretations. Check out this post:\n\n## How to measure the angle between two functions\n\nTivadar Danka\n\n\u00b7\n\nDecember 8, 2022\n\nThis will surprise you: sine and cosine are orthogonal to each other. Even the\nnotion of the enclosed angle between sine and cosine is unclear, let alone its\nexact value. How do we even define the angle between two functions? This is\nnot a trivial matter. What\u2019s intuitive for vectors in the Euclidean plane is a\nmystery for objects such as functions.\n\nRead full story\n\nThe process of abstracting multiple scalars into a vector is called\nvectorization.\n\nVectorization has two layers: a mathematical and a computational one. So far,\nwe\u2019ve seen the mathematical part, overloading the symbols of the expression ax\n+ b to do more.\n\nHowever, there\u2019s another side of the coin. Computationally speaking,\nvectorized code is much faster than their scalar counterparts because it is\n\n  * massively parallelizable,\n\n  * and the array data structure enables contiguous memory allocation.\n\n(Contiguous memory allocation means that the elements of the array are stored\nnext to each other in the memory. This makes a huge difference in performance,\nas you don\u2019t have to jump around to access the elements.)\n\nVectorized code makes machine learning possible!\n\n#\n\nHigh dimensions \u2192 high dimensions\n\nIs the model\n\ngood enough?\n\nIt\u2019s getting better, but there\u2019s still something missing: we usually have\nmultiple target variables. Think about a microbial colony in a biofuel factory\nthat takes in a bunch of metabolites, producing a different set of molecules.\nThere, we don\u2019t just want to predict ethanol production; we want to think in\nsystems and processes, so we track all the produced metabolites.\n\nIn other words, instead of a single target variable y, we have a bunch: y1,\ny2, ..., ym. Thus, we are jumping from a vector-scalar model to a vector-\nvector model.\n\nWe can think about the vector-vector function h as a vector of vector-scalar\nfunctions:\n\nIf each hj is linear, then we h(x) takes the form\n\nwhere each aj is an n-dimensional parameter vector.\n\nThis was the generalization part. Now, let\u2019s see that abstraction. Is there a\nway to simplify this? First of all, we can form a vector from the biases and\nmove the addition outside the vector:\n\nHowever, there\u2019s an even more important pattern there. Instead of working with\nn and m-dimensional vectors, let\u2019s shift our perspective and use matrices\ninstead! If we treat an n-dimensional vector as a 1 \u00d7 n matrix, our updated\nmodel can be written as\n\nWe are one step away from the final form.\n\nTo make that step, notice that the aiT-s are column vectors that we can\nhorizontally concatenate into a matrix A:\n\nWith this we have reached the final form\n\nCompare this one with the starting point:\n\nIt\u2019s almost the same (except the order in which a and x appears), but the\nvectorized one is light-years ahead of the starting point.\n\n#\n\nComputing everything at once\n\nThere is one more step before we reach the final form.\n\nData usually comes in the form of tables, batches, and other bundles. In other\nwords, we have a sequence of data samples x1, x2, ..., xs, each forming a row\nvector, which we stack on top of each other to obtain the data matrix X,\ndefined by\n\nStacking s number of n-dimensional vectors yields an s \u00d7 n matrix. It\u2019s an\nabuse of notations, but a good one.\n\nWith another abuse of notation, we might define the function h on the matrix X\nby\n\nThis looks quite repetitive, so there should be a pattern. Must we apply h(x)\nrow by row? First, notice that adding the row vector b can be moved outside of\nthe matrix via\n\nThe matrix with x-es and A-s look familiar! Indeed, this is by definition, the\nproduct of X and A:\n\nFeel free to check it by hand. If you are confused by matrix multiplication,\nhere\u2019s my earlier post to guide you:\n\n## Epsilons, no. 2: Understanding matrix multiplication\n\nTivadar Danka\n\n\u00b7\n\nMarch 13, 2023\n\nMatrix multiplication is not easy to understand. Even looking at the\ndefinition used to make me sweat, let alone trying to comprehend the pattern.\nYet, there is a stunningly simple explanation behind it. Let's pull back the\ncurtain!Understanding mathematics is a superpower. Subscribing to The\nPalindrome will instantly unlock it for you. For sure. (Or at least help you\nget there, step by step.)\n\nRead full story\n\nOne thing that sticks out like a sore thumb is that matrix of b-s. Can we get\nrid of them? Sure.\n\nHere\u2019s that typical math trick: overload the operations. Instead of defining\nanother matrix B via stacking the b-s, let\u2019s define the addition between an s\n\u00d7 m matrix and a 1 \u00d7 m vector by\n\nmaking our model to be\n\nwhich is as clear as it gets.\n\nThis last trick of blowing up row vectors is called broadcasting, and it\u2019s\nbuilt-in to tensor frameworks such as NumPy. Check it out:\n\nSource: vectorization.py\n\n(The symbol @ denotes matrix multiplication in NumPy.)\n\nOur loop of generalization and abstraction becomes complete: the linear\nregression model is vectorized. We have gained\n\n  * the ability to handle multiple features and target variables,\n\n  * no additional complexity in the model,\n\n  * and a massively parallel implementation that uses efficient data structures.\n\nIf you are familiar with neural networks, you know that the simple expression\nXA + b is the so-called linear layer, one of the most frequent building\nblocks. (Even the convolutional layer is often implemented as an extremely\nsparse linear layer.)\n\nEven though this post was about generalization and abstraction, the\ncomputational aspects are so interesting that they are worth an entire post.\n\nSee you next time with a deep dive into how vectors and matrices work in\nsilico!\n\nThanks for reading! If you liked this post, share it with your friends! I\nregularly post deep dives on mathematics and machine learning.\n\nShare\n\n### Subscribe to The Palindrome\n\nBy Tivadar Danka \u00b7 Hundreds of paid subscribers\n\nMathematics for engineers, scientists, and other curious minds. Explaining\nthings like your teachers should have, but probably never did.\n\n26 Likes\n\n\u00b7\n\n1 Restack\n\n26\n\nShare this post\n\n#### An introduction to vectorization\n\nthepalindrome.org\n\nShare\n\nComments\n\nMatrices and graphs\n\nThe single most undervalued fact of linear algebra: matrices are graphs, and\ngraphs are matrices\n\nJan 9, 2023 \u2022\n\nTivadar Danka\n\n106\n\nShare this post\n\n#### Matrices and graphs\n\nthepalindrome.org\n\n19\n\nHow large that number in the Law of Large Numbers is?\n\nAll large numbers are large, but some are larger than others\n\nSep 4, 2023 \u2022\n\nTivadar Danka\n\nand\n\nLevi\n\n35\n\nShare this post\n\n#### How large that number in the Law of Large Numbers is?\n\nthepalindrome.org\n\nThe Maximum Likelihood Estimation\n\n(is the foundation of machine learning)\n\nJan 2 \u2022\n\nTivadar Danka\n\n34\n\nShare this post\n\n#### The Maximum Likelihood Estimation\n\nthepalindrome.org\n\nReady for more?\n\n\u00a9 2024 Tivadar Danka\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
