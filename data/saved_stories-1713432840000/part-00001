{"aid": "40071773", "title": "How Perfectly Can Reality Be Simulated?", "url": "https://www.newyorker.com/magazine/2024/04/22/can-the-world-be-simulated", "domain": "newyorker.com", "votes": 4, "user": "pseudolus", "posted_at": "2024-04-18 00:43:30", "comments": 0, "source_title": "How Perfectly Can Reality Be Simulated?", "source_text": "How Perfectly Can Reality Be Simulated? | The New Yorker\n\nSkip to main content\n\nTo revisit this article, select My Account, then View saved stories\n\nSign In\n\nSearch\n\nFind anything you save across the site in your account\n\nOnward and Upward with Technology\n\n# How Perfectly Can Reality Be Simulated?\n\nVideo-game engines were designed to mimic the mechanics of the real world.\nThey\u2019re now used in movies, architecture, military simulations, and efforts to\nbuild the metaverse.\n\nBy Anna Wiener\n\nApril 15, 2024\n\nThe C.E.O. of Epic Games said that, in our lifetimes, computers will make\nimages that are \u201ccompletely indistinguishable from reality.\u201dIllustration by\nJon Han\n\nOn a warm afternoon last fall, Steven Caron, a technical artist at the video-\ngame company Quixel, stood at the edge of a redwood grove in the Oakland\nHills. \u201cCross your eyes, kind of blur your eyes, and get a sense for what\u2019s\nhere,\u201d he instructed. There was a circle of trees, some logs, and a wooden\nfence; two tepee-like structures, made of sticks, slumped invitingly. Quixel\ncreates and sells digital assets\u2014the objects, textures, and landscapes that\ncompose the scenery and sensuous elements of video games, movies, and TV\nshows. It has the immodest mission to \u201cscan the world.\u201d In the past few years,\nCaron and his co-workers have travelled widely, creating something like a\ndigital archive of natural and built environments as they exist in the early\ntwenty-first century: ice cliffs in Sweden; sandstone boulders from the\nshrublands of Pakistan; wooden temple doors in Japan; ceiling trim from the\nBo\u017ck\u00f3w Palace, in Poland. That afternoon, he just wanted to scan a redwood\ntree. The ideal assets are iconic, but not distinctive: in theory, any one of\nthem can be repeated, like a rubber stamp, such that a single redwood could\ncompose an entire forest. \u201cThink about more generic trees,\u201d he said, looking\naround. We squinted the grove into lower resolution.\n\nQuixel is a subsidiary of the behemoth Epic Games, which is perhaps best known\nfor its blockbuster multiplayer game Fortnite. But another of Epic\u2019s core\nproducts is its \u201cgame engine\u201d\u2014the software framework used to make games\u2014called\nUnreal Engine. Video games have long bent toward realism, and in the past\nthirty years engines have become more sophisticated: they can now render near-\nphotorealistic graphics and mimic real-world physics. Animals move a lot like\nanimals, clouds cast shadows, and snow falls more or less to expectations.\nSound bounces, and moves more slowly than light. Most game developers rely on\nthird-party engines like Unreal and its competitors, including Unity.\nIncreasingly, they are also used to build other types of imaginary worlds,\nbecoming a kind of invisible infrastructure. Recent movies like \u201cBarbie,\u201d \u201cThe\nBatman,\u201d \u201cTop Gun: Maverick,\u201d and \u201cThe Fabelmans\u201d all used Unreal Engine to\ncreate virtual sets. In 2022, Epic gave the Sesame Workshop a grant to scan\nthe sets for \u201cSesame Street.\u201d Architects now make models of buildings in\nUnreal. NASA uses it to visualize the terrain of the moon. Some Amazon\nwarehouse workers are trained in part in gamelike simulations; most virtual-\nreality applications rely on engines. \u201cIt\u2019s really coming of age now,\u201d Tim\nSweeney, the founder and C.E.O. of Epic Games, told me. \u201cThese little \u2018game\nengines,\u2019 as we called them at the time, are becoming simulation engines for\nreality.\u201d\n\nQuixel got its start helping artists create the textures for digital models, a\npractice that historically relied on sleight of hand. (Online, a small\nsubculture has formed around \u201ctexture archaeology\u201d: for Super Mario 64,\nreleased in 1996, reflective surfaces would have been too inefficient to\nrender, so a metal hat worn by Mario was made with a low-resolution fish-eye\nphotograph of flowers against a blue sky, which created an illusion of\nshininess.) It soon became clear that the best graphics would be created with\nhigh-resolution photographs. In 2011, Quixel began capturing 3-D images of\nreal-world objects and landscapes\u2014what the company calls \u201cmegascans.\u201d \u201cWe\nhave, to a great extent, mastered our ability to digitize the real world,\u201d\nTeddy Bergsman Lind, who co-founded Quixel, said. He particularly enjoyed\ndigitizing Iceland. \u201cVast volcanic landscapes, completely barren, desolate,\nalienlike, shifting from pitch-black volcanic rock to the most vivid reds I\u2019ve\never seen in an environment to completely moss-covered areas to glaciers,\u201d he\nsaid. \u201cThere\u2019s just so much to scan.\u201d\n\nA still from The Matrix Awakens, a video game made with Unreal Engine.Source:\nEpic Games\n\nDigitizing the real world involves the tedium of real-world processes. Three-\ndimensional models are created using lidar and photogrammetry, a technique in\nwhich hundreds or thousands of photographs of a single object are stitched\ntogether to produce a digital reproduction. In the redwood grove, as Caron set\nup his equipment, he told me that he had spent the past weekend inside, under,\nand atop a large \u201cdebris box\u201d\u2014crucially, not a branded Dumpster, which might\nnot pass legal review\u2014scanning it from all angles. The process required some\nnine thousand photographs. (\u201cI had to do it fast,\u201d he said. \u201cPeople illegally\ndump their stuff.\u201d) Plants and leaves, which are fragile, wavery, and have a\nshort shelf life, require a dedicated vegetation scanner. Larger elements,\nlike cliff faces, are scanned with drones. Reflective objects, such as swords,\ndemand lasers. Lind told me that he loved looking at textures up close. \u201cWhen\nyou scan it, a metal is actually pitch-black,\u201d he said. \u201cIt holds no color\ninformation whatsoever. It becomes this beautiful canvas.\u201d But most of\nQuixel\u2019s assets are created on treks that require permits and months of\nplanning, by technical artists rucking wearable hard drives, cameras, cables,\nand other scanning equipment. Caron had travelled twice to the I\u2019on Swamp, a\nformer rice paddy on the outskirts of Charleston, South Carolina, to scan\ncypress-tree knees\u2014spiky, woody growths that rise out of the water like\nstalagmites. \u201cThey look creepy,\u201d he said. \u201cIf you want to make a spooky swamp\nenvironment, you need cypress knees.\u201d\n\nThe company now maintains an enormous online marketplace, where digital\nartists can share and download scans of props and other environmental\nelements: a banana, a knobkerrie, a cluster of sea thrift, Thai coral, a\nsmattering of horse manure. A curated collection of these elements labelled\n\u201cAbattoir\u201d includes a handful of rusty and sullied cabinets, chains, and\ncrates, as well as twenty-seven different bloodstains (puddle, archipelago,\n\u201chigh velocity splatter\u201d). \u201cMedieval Banquet\u201d offers, among other sundries, an\naggressively roasted turnip, a rack of lamb ribs, wooden cups, and several\npork pies in various sizes and stages of consumption. The scans are detailed\nenough that when I examined a roasted piglet\u2014skin leathered with heat and torn\nat the elbow\u2014it made me feel gut-level nausea.\n\nAssets are incorporated into video games, architectural renderings, TV shows,\nand movies. Quixel\u2019s scans make up the lush, dappled backgrounds of the live-\naction version of \u201cThe Jungle Book,\u201d from 2016; recently, watching the series\n\u201cThe Mandalorian,\u201d Caron spotted a rock formation that he had scanned in Moab.\nDistinctive assets run the risk of being too conspicuous: one Quixel scan of a\ndenuded tree has become something of a meme, with gamers tweeting every time\nit appears in a new game. In Oakland, Caron considered scanning a wooden\nfence, but ruled out a section with graffiti (\u201cDAN\u201d), deeming it too unique.\n\nEpic creates detailed simulations of people as part of a project called\nMetaHumans.Source: Epic Games\n\nAfter a while, he zeroed in on a qualified redwood. Working in visual effects\nhad given him a persnickety lens on the world. \u201cYou\u2019re just trained to look at\nthings differently,\u201d he said. \u201cYou can\u2019t help but look at clouds when you\u2019ve\ndone twenty cloudscapes. You\u2019re hunting for the perfect cloud.\u201d He crouched\ndown to inspect the ground cover beneath the tree and dusted a branch of\nneedles\u2014distractingly green\u2014out of the way. Caron\u2019s colleagues sometimes trim\ngrass, or snap a branch off a tree, in pursuit of an uncluttered image. But\nCaron, who is in his late thirties and grew up exploring the woods of South\nCarolina, prefers a leave-no-trace approach. He hoisted one of the scanning\nrigs onto his back, clipped in a hip belt to steady it, and picked up a large\ndigital camera. After making a series of tweaks\u2014color calibration, scale,\nshooting distance\u2014he began to slowly circle the redwood, camera snapping like\na metronome. An hour passed, and the light began to change, suboptimally. On\nthe drive home, I considered the astonishing amount of labor involved in\ncreating set pieces meant to go unnoticed. Who had baked the pork pies?\n\nSweeney, Epic\u2019s C.E.O., has the backstory of tech-founder lore\u2014college\ndropout, headquarters in his parents\u2019 basement, posture-ruining work ethic\u2014and\nthe stage presence of a spelling-bee contestant who\u2019s dissociating. He is\nfifty-three years old, and deeply private. He wears seventies-style aviator\neyeglasses, and dresses in corporate-branded apparel, like an intern. He is\nmild and soft-spoken, uses the word \u201cawesome\u201d a lot, and tweets in a way that\nsuggests either the absence of a communications strategist or a profound\nunderstanding of his audience. (\u201cElon Musk is going to Mars and here I am\ndebugging race conditions in single-threaded JavaScript code.\u201d) He likes fast\ncars and Bojangles chicken. Last year, he successfully sued Google for\nviolating antitrust laws. Epic, which is privately held, is currently valued\nat more than twenty-two billion dollars; Sweeney reportedly is the controlling\nshareholder.\n\nWhen we spoke, earlier this spring, he was at home, in Raleigh, North\nCarolina, wearing an Unreal Engine T-shirt and drinking a soda from Popeyes.\nBehind him were two high-end Yamaha keyboards. We were on video chat, and the\nlighting in the room was terrible. During our conversation, he vibrated\ngently, as if shaking his leg; I wondered if it was the soda. \u201cIt\u2019s probably\ngoing to be in our lifetime that computers are going to be able to make images\nin real time that are completely indistinguishable from reality,\u201d Sweeney told\nme. The topic had been much discussed in the industry, during the company\u2019s\nearly days. \u201cThat was foreseeable at the time,\u201d he said. \u201cAnd it\u2019s really only\nstarting to happen now.\u201d\n\nSweeney grew up in Potomac, Maryland, and began writing little computer games\nwhen he was nine. After high school, he enrolled at the University of Maryland\nand studied mechanical engineering. He stayed in the dorms but spent some\nweekends at his parents\u2019 house, where his computer lived. In 1991, he created\nZZT, a text-based adventure game. Players could create their own puzzles and\npay for add-ons, which Sweeney shipped to them on floppy disks. It was a\nsleeper hit. By then, he had started a company called Potomac Computer\nSystems. (It took its name from a consulting business he had wanted to start,\nfor which he had already purchased stationery.) It operated out of his\nparents\u2019 house. His father, a cartographer for the Department of Defense, ran\nits finances. Sweeney renamed the company Epic MegaGames\u2014more imposing, to his\near\u2014and hired a small team, including the game designer Cliff Bleszinski, who\nwas still a teen-ager. \u201cIn many ways, Tim Sweeney was a father figure to me,\u201d\nBleszinski told me. \u201cHe showed me the way.\u201d\n\nCartoon by Suerynn Lee\n\nLink copied\n\nSweeney\u2019s lodestar was a company called id Software. In 1993, id released\nDoom, a first-person shooter about a husky space marine battling demons on the\nmoons of Mars and in Hell. Doom was gory, detailed, and, crucially, fast: its\ndevelopers had drawn on military research, among other things. But id also\ntook the unusual step of releasing what it called Doom\u2019s \u201cengine\u201d\u2014the\nfoundational code that made the game work. Previously, games had to be built\nfrom scratch, and companies kept their code proprietary: even knowing how to\nmake a character crouch or jump gave them an edge. Online, Doom \u201cmods\u201d\nproliferated, and game studios built new games atop Doom\u2019s architecture.\nStructurally, they weren\u2019t a huge departure. Heretic was a fantastical first-\nperson shooter about fighting the undead; Strife was a fantastical first-\nperson shooter about fighting robots. But they were proofs of concept for a\nnew method and philosophy of game-making. As Henry Lowood, a video-game\nhistorian at Stanford, told me, \u201cThe idea of the game engine was \u2018We\u2019re just\nproducing the technology. Have at it.\u2019 \u201d\n\nVideo From The New Yorker\n\nSweeney thought that he could do better. He soon began building his own first-\nperson shooter, which he named Unreal. He recalled looking through art\nreference books and photographs to better understand shadows and light. When\nyou spend hours thinking about computer graphics, he told me, the subject\n\u201ctends to be unavoidable in your life. You\u2019re walking through a dark scene\noutdoors at night, and it\u2019s rainy, and you\u2019re seeing the street light bounce\noff of the road, and you\u2019re seeing all these beautiful fringes of color, and\nyou realize, Oh, I should be able to render this.\u201d Unreal looked impressive.\nWater was transparent, and flames flickered seemingly at random. After\nscreenshots of the game were published, before its release, developers began\ncontacting Sweeney, asking to use his engine for their own games.\n\nIn 1999, the company moved to North Carolina. Soon, Unreal Engine was being\nused in a Harry Potter PC game and in America\u2019s Army, a multiplayer game\ncreated by the U.S. military as a recruitment tool. \u201cThe plan was to license\nthe engine to anyone and everyone,\u201d Bleszinski later wrote in a memoir. \u201cIt\nwould provide Epic with unlimited new revenue streams . . . Ka-ching! \u201d Over\ntime, the company improved rendering times and lighting capabilities.\nCharacters cast shadows. A new system for creating waterfalls allowed for\ndroplets of mist to leap from a fall\u2019s base. Bubbles, once popped, didn\u2019t\ndisappear but exploded into little fragments. (The same principles applied in\ncollisions between characters and weapons\u2014instead of disappearing, fatally\nwounded characters collapsed or shattered\u2014and gore became gorier.)\n\nFor game developers, polygons are a major preoccupation. These shapes, usually\ntriangles, are the building blocks of almost all 3-D graphics. The more\npolygons a rendering has, the more detailed it is. Updates to the engine made\nit possible to include textures and characters with exponentially more\npolygons. \u201cIt\u2019s quite an incredible feeling to realize, I might be the only\nperson on earth who understands that this is possible,\u201d Sweeney said of the\nbreakthroughs. \u201cAnd I\u2019m looking at it right now on my computer screen,\nworking. And in a few years everybody\u2019s going to know this, and have it. It\nwill eventually be taught in textbooks.\u201d Today, Unreal Engine\u2019s user interface\nlooks a little like a piece of photo- or video-editing software; it offers\ntemplates such as \u201cthird-person shooter\u201d and \u201csidescroller.\u201d\n\nMany movies and TV shows are now shot in an L.E.D. \u201cvolume,\u201d which displays\nbackdrops created in a video-game engine.Source: Epic Games\n\nLast year, at the annual Game Developers Conference, in San Francisco, Epic\ngave a presentation on new updates to Unreal Engine. Sweeney delivered opening\nremarks, wearing a black Ralph Lauren x Fortnite hoodie. Then executives\nshowed off the engine\u2019s new capabilities\u2014including near-photorealistic foliage\nand updated fluid simulations\u2014using an interactive scene of an electric truck\noff-roading through a verdant forest. Birds chirped as the vehicle rumbled\nthrough a ravine, its engine emitting a thin whine. The tires bounced in\naccordance with the truck\u2019s suspension system. Leaves, brushed away, snapped\nback. Rocks were shunted to the side. People clapped at waving foliage. As the\ntruck navigated through a puddle, water gushed over the tires. The man sitting\nnext to me gasped.\n\nEpic\u2019s demos are so system-intensive that they would slow to a stutter on the\naverage laptop. Most games, including Fortnite, remain stylized. \u201cThe holy\ngrail of all this is to cross the uncanny valley\u2014to make a C.G. human that you\ncan\u2019t tell is fake,\u201d Bleszinski told me. \u201cIn some ways, Tim Sweeney and his\nteam are playing God, you know? I\u2019m an atheist myself, but I believe that, if\nthere is a God, the way we can honor them is by creating, because God was the\noriginal creator, allegedly.\u201d He recited some lyrics from the musical \u201cRent\u201d:\n\u201cThe opposite of war isn\u2019t peace, it\u2019s creation.\u201d \u201cMaybe Tim Sweeney has a God\ncomplex,\u201d he said. But he suggested that it might just be that Sweeney is\nhyper-focussed. \u201cI don\u2019t think he ever married, and I think Epic is his\nfamily, and Epic is his journey,\u201d Bleszinski said. \u201cHe\u2019s not much of a gamer,\neven. Fortnite is crushing it. But, you know, I think the engine is his\nendgame.\u201d\n\nEpic\u2019s headquarters, in Cary, North Carolina, are furnished with dangling\nSupply Llamas\u2014a purple pi\u00f1atalike character from Fortnite\u2014and a large metal\nplayground slide that terminates in the lobby, at the base of a nearly two-\nstory reproduction of a character from the Unreal series, clad in a beret and\na futuristic suit of armor. (Last year, the company laid off sixteen per cent\nof its workforce; Sweeney cited a pattern of spending more money than the\ncompany was bringing in.) Today, some major game studios, such as Activision\nBlizzard, which makes Call of Duty, still use their own proprietary engines.\nBut most rely on Unity, Unreal, and others. A number of big-budget\ntitles\u2014including Halo, Tomb Raider, and Final Fantasy\u2014have recently traded\ntheir own engines for Unreal. \u201cIt\u2019s a zero-sum game,\u201d Ben Irving, an executive\nproducer at Crystal Dynamics, which makes Tomb Raider, told me. \u201cDo we want to\nbe an engine company? Or do we want to be a game-making company?\u201d\n\nA key part of many game engines is the physics engine, which mathematically\nmodels everything we\u2019ve learned about the physical world. A strong wind can be\nsimulated using velocity. An animated bubble might take into account surface\ntension. Last year, Epic released Lego Fortnite, a family-friendly mode in\nwhich players can build\u2014and destroy with dynamite\u2014their own Lego\nconstructions. The game is cartoonish, but its mechanics are grounded in\nreality. \u201cWhen the building falls, everybody knows what that\u2019s supposed to\nlook like,\u201d Saxs Persson, an executive at Epic, told me. \u201cIt looks good\nbecause they got the mass right. They got the collision volumes right. They\ngot the gravity right. They got friction, which is really hard. They got wind,\nterrain. All of it has to be perfect.\u201d Even the precise tension of pulling\nLegos apart, a common muscle memory, has been simulated. \u201cIt\u2019s all math,\u201d he\nsaid.\n\nYet certain things remain hard to simulate. There are multiple types of water\nrenderers\u2014an ocean demands a kind of simulation different from that of a river\nor a swimming pool\u2014but buoyancy is challenging, as are waves and currents.\n\u201cThe Navier-Stokes equation for fluid simulation is one of the remaining six\nMillennium Prize Problems in mathematics\u2014it\u2019s unsolved,\u201d Vladimir Mastilovi\u0107,\nEpic\u2019s vice-president of digital-humans technology, told me, referring to a\nset of math problems that have been impervious to human effort. Clouds are\ntricky. Fabric, which stretches, bends, wrinkles, and billows, often in\nunpredictable ways, is notoriously difficult to get right. It\u2019s hard to\nsimulate chain reactions. \u201cIf I chop down a tree in a forest, there\u2019s a chance\nthat it hits another tree and knocks over another tree, and that splinters and\nbreaks,\u201d Kim Libreri, Epic\u2019s chief technology officer, said. \u201cGetting that\nlevel of simulation is very, very hard right now.\u201d Even the smallest human\ngestures can be headaches. \u201cPutting your hand through your hair\u2014that\u2019s an\nunbelievably complicated problem to solve,\u201d Libreri said. \u201cWe have physics\nsimulation to make it wobble and stuff, but it\u2019s almost at the molecular\nlevel.\u201d (In some games, hair is simulated by using cloth sheets with hairlike\ntexture.)\n\n\u201cNo, son. Let him finish.\u201d\n\nCartoon by Edward Steed\n\nLink copied\n\nThis is just one of the reasons that it\u2019s incredibly difficult to\nrealistically simulate humans. \u201cThe solution to fluid dynamics and to fire and\nto all these other phenomena we see in the real world is just brute-force\nmath,\u201d Sweeney said. \u201cIf we have enough computing power to throw at the\nequations, we can solve them.\u201d But humans have an intuitive sense of how\nothers should look, sound, and move, which is based on our evolution and\ncognition. \u201cWe don\u2019t even know the equations we need to solve in order to\nsimulate humans,\u201d Sweeney said. \u201cNobody\u2019s invented them yet.\u201d Epic\u2019s MetaHuman\nCreator, billed as \u201chigh-fidelity digital humans made easy,\u201d is a tool for\nmaking photorealistic animated avatars. \u201cWe go to some extreme lengths to\ncapture all the data,\u201d Mastilovi\u0107 said. To create one model, Epic\u2019s\nresearchers gave an actor a full-body MRI, to scan his bones and muscles, then\nput him on a stage surrounded by several hundred cameras to capture the\nenveloping tissue. To simulate his facial movements, the researchers put\nsensors on the actor\u2019s tongue and teeth, placed his head in an electromagnetic\nfield, and collected data on the ways his mouth moved while he talked.\n\nMetaHuman Creator draws on a database of scanned humans, a kind of anthropoid\nslurry, to create highly detailed virtual models of people, often used for\nsecondary characters in games. Currently, the avatars\u2019 movements are not quite\nright: they\u2019re overly smooth and a little slippery; their mouths move oddly;\nthey struggle to make eye contact, which is unsettling. When I launched the\napplication recently, a default MetaHuman named Rosemary emerged on my\ncomputer screen, blinking and gently twisting her head back and forth.\nRosemary was white, with blue eyes and slightly yellowing under-eye bags. She\nappeared not to have got much sun lately; I touched her up with a little\nblush. Using sliders, I adjusted her eyes\u2014color, iris size, limbus\ndarkness\u2014lengthened her teeth, dialled up the plaque, and gave her freckles. I\nselected \u201chappy\u201d from a list of emotional states. Rosemary smiled and tilted\nher head in different directions, like a royal in a coronation procession. I\nchanged her hair style to a Pennywise coiffure. My husband came into the room.\n\u201cWhat the fuck is that?\u201d he said.\n\nFortnite, which is made with Unreal, is a cultural phenomenon, with about a\nhundred million monthly players. Its most popular mode is Battle Royale, in\nwhich players blast one another with weapons. But there are also more social\nmodes. There are now live concerts in Fortnite, attended by millions of\npeople. There are shops, where real people spend real money to buy virtual\ngoods for their virtual avatars. (Epic settled a lawsuit with the F.T.C. last\nyear, for violating minors\u2019 privacy and manipulating them into unwanted\npurchases, and paid more than half a billion dollars.) There is a comedy club\nrun by Trevor Noah, and a Holocaust museum. In February, Disney invested $1.5\nbillion in Epic, for a nine-per-cent stake in the company; Bob Iger, the\nC.E.O. of Disney, has said that he plans to create a Disney universe in\nFortnite, in which players can interact with the company\u2019s intellectual\nproperty. Epic views Fortnite as a \u201cplatform,\u201d and encourages players to\ncreate, sell, and build with their own digital assets. Photogrammetry apps can\nbe used to make assets of everyday items\u2014a chair, a blouse, a bowl of noodles.\nIn theory, a person could make a digital asset of her orthodontic retainer,\nsell it to other \u201ccreators,\u201d then place it on her virtual bedside table and\nforget about it.\n\nMastilovi\u0107 suggested that MetaHumans could one day be used to create\nautonomous characters. \u201cSo it will not be a set of prerecorded animations\u2014it\nwill be a simulation of somebody\u2019s personality,\u201d he said. He suggested that a\nsimulation of Dwayne (the Rock) Johnson could be fun, and that people could\ncreate digital copies of themselves and then license and monetize them.\nMastilovi\u0107\u2019s team often talked about a concept called Magic Mirror: a way to\nvisualize, alter, and explore oneself virtually. \u201cWhat if I was ten kilos\nmore, ten kilos less?\u201d he said. \u201cWhat if I was more confident? What if I was\nolder or younger? How would this look on me?\u201d He added, \u201cWhen things become\ntruly real, photo-real, and truly interactive, that is so much more than the\nmedium we have right now. That\u2019s not a game. That\u2019s a simulation of alternate\nreality.\u201d\n\nThe video-game industry and Hollywood have long been symbiotic, but the lines\nhave become increasingly blurred. When James Cameron began making \u201cAvatar,\u201d in\nthe mid-two-thousands, he announced that he would replace traditional green\nscreens with a new technology that he called a \u201cvirtual camera.\u201d Actors\nperformed on a motion-capture soundstage, wearing bodysuits covered in\nsensors, while video of their bodies and faces was fed into proprietary\nsoftware similar to a game engine. Using a specialized camera, Cameron was\nable to see the actors, in real time, moving around the computer-generated\nworld of Pandora, a lush, vegetal moon. \u201cIf I want to fly through space, or\nchange my perspective, I can,\u201d he told the Times, in 2007. \u201cI can turn the\nwhole scene into a living miniature and go through it on a 50 to 1 scale.\u201d The\ncrew called the technique \u201cvirtual production.\u201d Behind-the-scenes footage\nshowed actors, faces freckled with sensors, aiming bows and arrows on a\nstarkly lit soundstage. On Cameron\u2019s screen, the actors were sleek and blue,\ntails gently bobbing.\n\nAt the time, using virtual reality for filmmaking was prohibitively expensive.\nBut in the past ten years a confluence of factors, including cheaper L.E.D.\nscreens and better commercial game engines, has brought it into wider use. The\nterm \u201cvirtual production\u201d is now used for a number of filmmaking techniques.\nThe most prominent application is as an alternative to a green screen. Actors\nwork on a soundstage called a \u201cvolume,\u201d which has a curved back wall and is\ncovered in thousands of L.E.D. panels; the ceiling is panelled, too. The\npanels display backdrops\u2014a mountaintop, a desert, a hostile planet\u2014that are\nmade in a game engine, and can be adjusted in real time. (The effect is\nsomething like an updated version of rear projection, an early-twentieth-\ncentury technique in which film was projected behind an actor.) Unlike with a\ngreen screen, actors can see the world that they\u2019re meant to inhabit. They are\nlit from the L.E.D. panels, and thus don\u2019t assume the telltale green-screen\ntint. The camera and the engine are in communication, so when the camera moves\nthe virtual world can move with it\u2014much like in a video game. (Baz Idoine, the\ncinematographer for \u201cThe Mandalorian,\u201d called this effect, known as parallax,\na \u201cgame changer.\u201d) There are now several hundred virtual-production stages in\noperation, including at Cinecitt\u00e0, the landmark Italian film studio. Julie\nTurnock, a film-studies professor at the University of Illinois Urbana-\nChampaign, said that such virtual methods were likely to become \u201cthe dominant\nform of production.\u201d This year, N.Y.U. will begin offering a master\u2019s in\nvirtual production, at a new facility funded by George Lucas and named in\nhonor of Martin Scorsese. Both Epic and Unity have received Emmy Awards for\ntheir game engines.\n\nA still from Unreal Engine.Source: Epic Games\n\nIn 2021, Epic set up shop in a large warehouse in El Segundo, California, that\nholds two L.E.D. volumes. When I visited, last year, the company was hosting a\nworkshop for cinematographers. People milled about, exploring virtual\nenvironments using V.R. goggles. Up on one volume was a hundred-and-eighty-\ndegree view of a Himalayan plateau, where it was daybreak. The light was clear\nand cold. I stood in the middle of the stage, surrounded by virtual mountains,\nand considered the inclusion of two virtual tents: Whose were they? The\nthought suggested that I was having an immersive experience. But did this view\nexist, or was it a collage\u2014Himalay-ish? Clouds rushed overhead, and a string\nof multicolored flags flapped in a breeze that we could not feel. Several\nyards away, a technician entered a few commands into a computer, and the\nclouds and the stage glowed coral\u2014golden hour. \u201cI\u2019m going to move some\nmountains for you,\u201d he said, and a snow-covered mesa floated across the set.\n\nOn-set virtual production is often cheaper and safer than sending a cast to\nfar-flung places. \u201cThere have been cases where we just travel to a real-world\nlocation, like Iceland or Brazil, to scan some really large terrain pieces,\u201d\nAsad Manzoor, an executive at Pixomondo, a virtual-production and visual-\neffects company owned by Sony Pictures, said. \u201cWe take those scans into\nUnreal, reshape them, cobble them together to create an alien planet.\u201d The\nidea is to create something \u201cphoto-real but alien.\u201d For multi-season series,\nlike \u201cStar Trek: Strange New Worlds,\u201d it\u2019s cheaper to store sets in the cloud\nthan in a hangar. Virtual characters, like MetaHumans, can now be used for\ncrowd scenes\u2014a sticking point in negotiations between studios and the Screen\nActors Guild last year. Several people I spoke with brought up the convenience\nof not having to worry about the weather: it is, after all, always sunny in\nBarbie Land. For that movie, a scene parodying \u201c2001: a Space Odyssey,\u201d in\nwhich a cadre of little girls in a desert encounters a humongous doll wearing\na kicky bathing suit, was shot in an L.E.D. volume; it would have been\nchallenging to schlep child actors to Monument Valley, where the original was\nfilmed.\n\nGame engines can also be used for previsualization, including virtual\nscouting, which relies on 3-D mockups of sets. The virtual models are often\ncreated before the sets are built. \u201cEverything that was happening in Barbie\nLand we technically had a real-time version of, for scouting,\u201d Kaya Jabar, the\nfilm\u2019s virtual-production supervisor, said. The effects department put up\nmodel environments on a small L.E.D. volume for the movie\u2019s director, Greta\nGerwig, and its cinematographer, Rodrigo Prieto, to explore. \u201cThey would walk\naround with a viewfinder, with the correct lens and everything, and just get a\nsense: Does this feel right? Are the palm trees too tall?\u201d Jabar said.\nSometimes the models are of real-world locations: during preproduction for\n\u201cDune: Part Two,\u201d the cinematographer planned shots using a model of the Wadi\nRum desert, populated with MetaHumans, before filming in Jordan. When I spoke\nwith Sweeney, he reflected on how widespread the use of scans had become.\n\u201cThey\u2019re all isolated into their own separate projects, kept in some\ncorporation\u2019s vault,\u201d he said. \u201cBut if you take all of the 3-D terrain data\nand all of the 3-D object data and you combined it together, right now I bet\nyou\u2019d have about ten per cent of the whole world.\u201d\n\nNonetheless, virtual production presents difficulties. It\u2019s hard to establish\ndistance between actors, since a volume can be only so large. Some directors\nresort to digi-doubles\u2014animated scans of an actor. Aligning a digital set with\na physical stage can be a challenge. \u201cWe have a game at the studio that we\ncall Find the Seam,\u201d Manzoor told me. \u201cThat\u2019s always the dead giveaway.\u201d Then\nthere is the opposite problem: on the fourth season of \u201cStar Trek: Discovery,\u201d\nManzoor said, \u201cit got to a point where it was so seamless that we had an actor\nnearly walk into the wall just because they thought it was a continuation of\nthe practical set.\u201d Idoine told me, \u201cThe volume is the right tool for the\nright job. It\u2019s not the right tool for all jobs.\u201d\n\nDecisions about lighting, scenery, and visual effects have to be made in\nadvance, rather than in postproduction. In 2019, the director Francis Ford\nCoppola announced that he was developing \u201cMegalopolis,\u201d a project he first\nconceived of in 1979, and that the movie would be made using virtual\nproduction. The film is a science-fiction epic about an architect, played by\nAdam Driver, who wants to rebuild New York after a catastrophe. For one scene,\nthe team made a physical replica of the top of the Chrysler Building,\noverlooking a version of the city built in Unreal. Mark Russell, the visual-\neffects supervisor, said, of Driver, \u201cHe was standing up on this platform that\nwas just a couple of feet off the ground but surrounded by a view of New York,\nand it was beautiful. Just to watch him kind of live in that environment was\npretty spectacular.\u201d But last year Coppola fired the visual-effects department\nand traded the volume for a classic green-screen approach. \u201cFrancis wasn\u2019t\nprepared to define what Megalopolis was,\u201d Russell said. \u201cHe\u2019d been developing\nthis idea for forty-some years, and he still was not willing to choose a\ndirection as to what the future would look like.\u201d\n\nVirtual production still works best when dealing with fantasy worlds, for\nwhich viewers have no direct references. \u201cWe all have an intuitive\nunderstanding of how things move in the real world, and creating that sense of\nreality is tough,\u201d Paul Franklin, a principal at the visual-effects house\nDNEG, said. Turnock, the film-studies professor, noted that visual realism\nisn\u2019t always about imitating \u201cwhat the eye sees in real life.\u201d She brought up\nfilmmakers\u2019 use of visual elements like shaky camerawork and shafts of light\nglittering with dust motes\u2014gestures toward realism whose presence is sometimes\ngratuitous, or even defies logic. (These have also become common in video\ngames: there are no cameras in video games, but there are lens flares.)\nTurnock traces this back to efforts in the seventies and eighties to make the\nearly \u201cStar Wars\u201d films look gritty and naturalistic. \u201cThere\u2019s a whole series\nof norms that have grown up around what makes things look realistic,\u201d she\nsaid. Some attempts at realism, it struck me, were so realistic that they\ncould only be fake.\n\nA still from Unreal Engine.Source: Epic Games\n\nIn El Segundo, Epic has an office in a large, loftlike area above one of the\nstages. When I visited, the space looked as if it had been furnished using a\ndrop-down menu: a couple of gray sofas; a mid-century-style leather chair; a\nbookcase holding some awards and tchotchkes alongside \u201cThe Illusion of Life,\u201d\na history of Disney animation, and \u201cThinking of You. I Mean Me. I Mean You,\u201d\nby the artist Barbara Kruger. Next to the bookcase was a plastic fiddle-leaf\nfig, which complemented a nearby bowl of plastic moss. \u201cMore and more, it\u2019s\nbecome digital-first,\u201d James Chinlund, the production designer for \u201cThe\nBatman\u201d and the live-action remake of \u201cThe Lion King,\u201d told me, sitting in the\nloft. He pointed to a poster on the wall, an evening cityscape from an Epic-\nproduced game that was based on the \u201cMatrix\u201d franchise. The rendering was so\ndetailed that gray ceiling tiles could be seen through the windows of the\noffice buildings. \u201cIf we had to actually build that space, it\u2019d be\noverwhelming,\u201d he said.\n\nStill, Chinlund wondered if the digital turn might produce \u201clazy filmmaking.\u201d\n\u201cThe audience is going to get bored with having all the candy delivered to\nthem,\u201d he said. He suggested that there might eventually be a backlash to the\nindustry\u2019s technological advancements. \u201cI have fantasies about the idea that\ncraft is going to come swinging back\u2014this punk-rock aesthetic,\u201d he said.\nLately, there has been an emphasis on more analog techniques: the director\nChristopher Nolan has touted the absence of C.G.I. in his film \u201cOppenheimer.\u201d\n(The detonation of the atomic bomb was simulated using explosives and drums of\nfuel.) Chinlund framed the issue as a matter of creative storytelling. \u201cIf you\ndon\u2019t have it available to you to put in the mountain belching lava with\ndragons flying around, then how do you communicate what the knight is seeing\nwhen he\u2019s standing on the cliff?\u201d he said. \u201cNow the fact is that you can do\nthat in a fully accurate, photo-real way. And is that better? Often not.\u201d\n\nToday, it\u2019s polygons all the way down. Tesla uses a game engine for its in-\nvehicle display, which shows a real-time visualization of the car on the road,\nfrom above. BMW trains its autonomous-driving software on data gathered from\nsimulated scenarios between virtual cars in virtual environments. The\nVancouver Airport Authority uses a \u201cdigital twin,\u201d made with Unity, to test\nsafety and operational scenarios, incorporating real-time data from the\nphysical airport. Disneyland features a \u201cStar Wars\u201d ride in which visitors fly\nthe Millennium Falcon through a galaxy that is responsive in real time,\nbecause it\u2019s built with Unreal Engine. Last year, the country musician Blake\nShelton, on his Back to the Honky Tonk tour, performed against a virtual\nbackdrop that evoked a honky-tonk bar\u2014also Unreal\u2014with simulated neon marquees\nand highway signs. A South Korean entertainment company recently unveiled\nMAVE, a K-pop group of MetaHumans. In music videos, MAVE\u2019s four members\u2014lithe\nyoung women with long torsos, glossy hair, and unblemished skin\u2014bounce around\nsynthetic cityscapes doing synchronized dances. Their movement is a little\nstiff, as if they were overthinking the choreography. Still, human dancing has\nseen worse.\n\n\u201cYou can\u2019t compare apples and oranges, because oranges have longer legs.\u201d\n\nCartoon by Elisabeth McNair\n\nLink copied\n\nDigital artists can now use any number of marketplaces to shop for assets. One\nof Epic\u2019s, called ArtStation, includes boxy leather jackets (\u201cstreetwear\u201d), a\nthirty-pack of mutant-skin templates, and a collection of images titled \u201c910+\nFemale Casual Morning Poses Reference Pictures,\u201d which show a naked woman\nstretching, reading a book called \u201cEmotional Intelligence,\u201d and shadowboxing\nwith a pillow\u2014just casual morning stuff. Carmakers and product designers use\ngame engines to create mockups, because it\u2019s cheaper than building prototypes.\nLast fall, the Sphere, an enormous L.E.D.-covered entertainment venue in Las\nVegas, appeared to have been doused in waves of Coca-Cola\u2014part of a promotion\nfor Coke Y3000, a new flavor \u201cco-created with human and artificial\nintelligence.\u201d To pitch the ad, the agency behind the promotion, PHNTM,\nmodelled the Sphere and its surrounding neighborhood in Unreal. \u201cIt\u2019s very\neasy to see how it\u2019s going to look from the Wyndham golf course, from this or\nthat area,\u201d Gabe Fraboni, the agency\u2019s founder, told me. Last summer, the\nagency installed an L.E.D. volume in its office.\n\nIn 2020, Zaha Hadid Architects used Unreal to model a proposed luxury\ndevelopment in Pr\u00f3spera, a controversial private city\u2014and a special economic\nzone, marketed as a haven for cryptocurrency traders\u2014on an island in Honduras.\n(Locals oppose it, fearing displacement, surveillance, and infrastructural\ndependence on a libertarian political project.) Prospective homeowners could\nscout plots of land and personalize their own residences with curved thatched\nroofs and rounded terraces. \u201cWant to check out the view from the balcony?\u201d\nmarketing materials asked, seductively. Last year, Epic worked with Safdie\nArchitects to create an elaborate model of a completed Habitat 67, Moshe\nSafdie\u2019s unfinished utopian development in Montreal, which never got the\nauthorizations necessary to realize Safdie\u2019s vision. The brutalist\narchitecture looks gorgeous in the virtual light. Chris Mulvey, a partner with\nthe firm, said that when Safdie saw it his response was bittersweet: \u201cHe was\njust, like, \u2018If I\u2019d had this, I could have convinced them to build the whole\nproject.\u2019 \u201d\n\nThe same tools have been used to archive disappearing aspects of the physical\nworld. Shortly after Russia invaded Ukraine, in 2022, Virtue Worldwide, an ad\nagency, began working on Backup Ukraine, an ad campaign for UNESCO and\nPolycam, a photogrammetry company. The campaign asked volunteers to create\ndigital assets of antiquities, monuments, and everyday artifacts that were\nunder threat, including sculptures, classical busts, and headstones. (\u201cHow do\nyou save what you can\u2019t physically protect?\u201d it asked.) The original idea was\nto use the assets as blueprints for future reconstruction, if necessary\u2014a\nprofessional team of scanners created meticulous models of churches in Kyiv\nand Lviv\u2014but people also began uploading scans of everyday objects from their\nown lives. Alongside models of an exploded tank, a burnt-out car, and\ndestroyed apartment buildings are assets of a toy Yoda and a pair of worn-out\nChuck Taylors.\n\nWithin a decade, Sweeney told me, most smartphones will likely be able to\nproduce high-detail scans. \u201cEverybody in humanity could start contributing to\na database of everything in the world,\u201d he told me. \u201cWe could have a 3-D map\nof the entire world, with a relatively high degree of fidelity. You could go\nanywhere in it and see a mix of the virtual world and the real world and any\ncombination of real and simulated scenarios you want there.\u201d In recent years,\nSweeney has started talking up the metaverse: a vision of the future in which\npeople can move seamlessly between virtual environments, taking their\nidentities, assets, and friends with them. In 2021, when metaverse chatter\nreached a fever pitch, the idea was sometimes discussed as a replacement for\nthe white-collar office in a world of remote work. That year, Facebook\nrebranded itself as Meta, then released a demo of its own metaverse: a spooky,\nsqueaky-clean office space in which legless avatars floated around virtual\nconference tables. Sweeney sees the metaverse more as a space for\nentertainment and socializing, in which games and experiences can be linked on\none enormous platform. A person could theoretically go with her friends to the\nmovies, interact with MetaHuman avatars of the film\u2019s actors, drop in on an\nEminem concert, then commit an act of ecoterrorism in Pr\u00f3spera, all without\nchanging her mutant skin.\n\nBy this point in our conversation, Sweeney had stopped vibrating, and seemed\nmore relaxed. He described the metaverse as an \u201cenhancer\u201d\u2014not a replacement\nfor in-person social experiences but better than hanging out alone. \u201cThe\nmemories you have about these times, and the dreams you have, are the same\nthings that you would have if you had been in the real world,\u201d he said. \u201cBut,\nyou know, the real world wasn\u2019t available at that time.\u201d He joked that the\n\u201clight source to the outdoor world\u201d\u2014the sun\u2014is available only half the time.\nBut the metaverse is always switched on. Sweeney and some of his friends from\nthe gaming industry, most of whom live in different cities, have their own\nFortnite squad, and often get together in the evening to maraud around and\ntalk business. It hadn\u2019t occurred to me that the people making Fortnite would\nalso hang out there. I wondered if he had ever inadvertently picked off one of\nhis employees.\n\nIn the fall, videos of military conflict, purporting to be from Gaza, began\ncirculating on social media. \u201cNEW VIDEO: Hamas fighters shooting down Israel\nwar helicopter in Gaza,\u201d one tweet read. But similar videos had circulated\nsome months prior, purporting to be footage from Ukraine. In fact, they were\nfrom Arma 3, a video game. Bohemia Interactive, the game\u2019s developer, released\na disinformation explainer that read a bit like an advertisement: \u201cWhile it\u2019s\nflattering that Arma 3 simulates modern war conflicts in such a realistic way,\nwe are certainly not pleased that it can be mistaken for real-life combat\nfootage.\u201d (In 2004, a defense contractor modified the game to create a\ntraining simulation for the U.S. military.)\n\nThe military has experimented with using games as training tools since the\nseventies, and has been integral to the development of computer graphics and\ntactical simulators. As game engines have become more sophisticated, so have\ntheir police and military applications. The N.Y.P.D. has held active-shooter\ndrills in game-engine simulations of high schools and of the plaza at One\nWorld Trade Center, in which animated characters lie bleeding on the ground.\nRaytheon has used game engines to simulate the deployment of new military\ntechnologies, including autonomous drone swarms and fleets of unmanned ground\nvehicles in dense cities. Boeing is using Unreal to create virtual models of\nits new B-52 bombers. (The models are also available on the Unreal Engine\nmarketplace.) In many cases, simulators are less focussed on photorealism and\nmore concerned with physical, mechanical, and even sonic realism. They can be\npersonalized, and the data they produce can be used to customize an\nindividual\u2019s training. Munjeet Singh, who works on immersive technologies for\nthe defense contractor Booz Allen Hamilton, told me that the company uses EEGs\nto monitor pilots\u2019 emotional responses to flight simulations created in Unity,\nin which, say, their engine fails or their plane gets shot at. \u201cWe can see if\nthat alpha brain wave is active, the beta brain wave, and then we can\ncorrelate that to focussed attention, attention drifts, sometimes emotional\nstates,\u201d Singh said. Members of the military who have P.T.S.D. from real-life\nconflicts\u2014for which they may have trained in virtual reality\u2014are now treated\nfor P.T.S.D. in virtual reality.\n\nNaval Air Station Corpus Christi is a military base on a small, squat\npeninsula on the Texas coast. Between the base and the Gulf of Mexico is\nMustang Island, a popular destination for vacationing families, who visit the\n\u201cTexas Riviera\u201d for its affordable condominiums, dolphin cruises, deep-sea-\nfishing tours, and wobbly gobs of ice cream scooped from industrial-sized\ntubs. The main drag displays a grab bag of architectural references: Ionic\ncolumns under a gable roof; a private residence with a castle turret and\nbarrel-tile roofing. As in many other coastal areas of the United States, the\ncars are trucks; the lawns house motorboats. The local mixed-martial-arts gym\nis named Weapons at Hand. I visited in midsummer, during a weather pattern\nthat was still being referred to as a heat wave, possibly a world-historical\neuphemism. On the morning of my visit to the base, I stood in the lobby of a\nBest Western, waiting for a cab and watching television. On the Weather\nChannel, a broadcaster stood in a virtual set, designed in Unreal Engine,\ntalking about the dangers of getting trapped in a vehicle during hot weather.\nBeside him was a life-size digital asset of a sedan, the inside of which\nappeared to be on fire. A dial labelled \u201cSCORCHING CAR SCALE\u201d leaned three-\nquarters of the way to the right.\n\nAn architectural rendering, made in Unreal Engine, of Habitat 67, an\nunfinished utopian development in Montreal.Source: Epic Games\n\nOn the base, which is dedicated mostly to aviation training, people walked\naround quietly and with good posture, their flight suits swishing. The\ntraining buildings contained a variety of simulators. There is a national\npilot shortage, and a number of people had mentioned that they were excited to\nbring attention to the Navy\u2019s use of technology, which they hoped would\nattract a new generation of digital natives to military aviation. (In the\npast, the military has recruited at video-game conventions.) Joshua Calhoun, a\nNavy commander, led me to a virtual-reality sled designed to resemble the\nsmall cockpit of a kind of two-seater prop plane that I\u2019d seen outside. I\nclimbed inside and put on a V.R. headset. When I looked down at my lap, my\nlegs were gone. The simulation environment, made in Unity, was a model of the\nbase\u2019s airstrip outside. \u201cWhere they\u2019re operating\u2014active duty\u2014right now,\ntoday, is here in Corpus Christi,\u201d Calhoun explained. \u201cI could probably create\na scenario where they\u2019re operating in Iraq or Afghanistan, but what\u2019s the\ntraining value for that?\u201d The virtual wind, visibility, and weather could all\nbe adjusted, but for me the day was clear\u2014easy mode.\n\n\u201cI\u2019m going to put you up over the bay,\u201d Calhoun said, and skipped ahead in the\nsimulator. \u201cYou may want to close your eyes.\u201d A few seconds later, I found\nmyself above Nueces Bay, alone in the sky. The traffic, marvellous in its way,\nmoved mathematically below. Boats drifted in the water. On base that day, it\nhad been mentioned, multiple times, that a perk of living in Texas is that\nit\u2019s legal to drive on the beach. It was strange to see Corpus Christi broken\ndown into assets, when the things that made the city interesting were\ncontextual: the region\u2019s industrial history and changing climate; the\ncoexistence of tourist, oil-and-gas, and military infrastructure; the massage\nparlors around the base. \u201cI\u2019m not super interested in ground detail,\u201d Calhoun\nsaid.\n\nI flew over a Citgo plant, thinking about the entanglement between the video-\ngame industry and the armed forces, a dynamic sometimes referred to as the\nmilitary-entertainment complex. Games were repurposed as military training\ntools; military training tools were repurposed as games. The latter were\npopular in part because they had a certain legitimacy: the industry aspired to\nrealism, after all. Among the many Doom modifications in the nineties was\nMarine Doom, a military simulator created to train marines in tactical\ndecision-making; a version of Doom has since been used to train an A.I. system\nintegrated into a new model of tank used by the Israel Defense Forces. I heard\nCalhoun say something about altitude control, but I wasn\u2019t paying attention. I\ncouldn\u2019t see the forest for the trees\u2014too distracted by their polygon count.\n\n\u201cYou\u2019re going to hit the water,\u201d he said.\n\nA few weeks after I chatted with Sweeney, I went hiking with a friend in the\nOakland Hills. It had rained overnight, and the air was cool and mossy; the\ntrails were slick with heavy mud. I had spent the previous few days at this\nyear\u2019s Game Developers Conference, wandering around the basement of a\nconvention center and watching people stumble about in V.R. goggles and eat\nfat chocolate-chip cookies from Epic\u2019s two-story pavilion. It was a relief to\nnow be aboveground. We walked slowly along the edge of the trail, our sneakers\nfeeling for traction.\n\nIn the past decade, Sweeney has become one of the largest private landowners\nin North Carolina, buying up thousands of acres for conservation. Land\nconservation struck me as an interesting project for someone in the business\nof immersive indoor entertainment\u2014incongruous enough that I found it kind of\nmoving. (In a 2007 MTV documentary, Sweeney showed off his garnet collection,\nsome of which was acquired on eBay, and a \u201cclimbing tree\u201d in his yard.) When\nwe spoke, I asked Sweeney whether working in games had made him see nature\ndifferently. \u201cNatural scenes tend to be the hardest to simulate,\u201d he said.\n\u201cWhen you\u2019re standing on a mountaintop, looking out into the distance, you\u2019re\nseeing the effect of trillions of leaves of trees. In the aggregate, they\ndon\u2019t behave as ordinary solid objects. At a certain distance, trees become\nsort of transparent. When you look at the real world and see all the areas\nwhere computer graphics are falling short of the real world, you tend to\nrealize we have a lot of work yet to do.\u201d He speculated that an efficient,\nrealistic simulation of a forest would require a \u201cgeology simulator\u201d and an\n\u201cecology simulator,\u201d each with its own complex sets of rules.\n\nIn the hills, I thought about what it would take to make a digital version of\nthe landscape we were moving through: the way the mud swallowed the yellow\nleaves and frail sticks; the silty puddles reflecting strips of sunlight; scum\naccumulating against the rocks in the creek; the checkered pattern of light\nacross the bark of a redwood; the drainage pipe at the edge of a clearing\u2014a\nreminder that this environment was engineered, too. (We\u2019d split an edible.)\nThe creation of virtual worlds seemed to require paying an incredible amount\nof attention to the natural one: when I\u2019d asked Michael Lentine, Epic\u2019s lead\nphysics engineer, what it took to simulate a tidal wave, his answer began with\nan overview of Eulerian and Lagrangian physics. In gaming, there\u2019s a concept\ncalled immersion breaking, which occurs when something snaps a player out of\nthe narrative flow\u2014permeable walls, characters who float rather than walk. The\nfoliage matters.\n\nMy friend and I talked about Big Basin, a state park that was home to some of\nCalifornia\u2019s oldest redwoods. A few years ago, it suffered a terrible\nwildfire. I toured the park shortly after the fires, and found it devastating.\nBut the trees were now trucking along. There was an archival impulse to\nscanning that I found appealing, even as I wondered if there was an anxiety to\nit, too. Was there something bleak about creating virtual facsimiles of the\nnatural world while we as a species were in the process of destroying it?\nLind, the Quixel co-founder, told me that he had gone on a scanning trip to\nMalibu in 2018. His team spent a week scanning the Santa Monica Mountains,\ncapturing the texture of the landscape. Two weeks later, the Woolsey fire\nburned almost a hundred thousand acres of land in the area. \u201cThat was actually\nfairly emotional,\u201d Lind said. \u201cEvery scanning expedition, you develop a\ncertain connection with that place.\u201d Still, they had the scans. Today, those\nimages could be scattered across games and movies, in jumbled pastiches of the\nreal thing.\n\nWe reached a part of the creek that was shrouded with ferns. Ribbons of foam\nformed in the water. My friend offered me his baseball cap, the crown still\nwarm. We were on the downslope, inescapably, and it had started to rain. \u2666\n\nPublished in the print edition of the April 22 & 29, 2024, issue, with the\nheadline \u201cGet Real.\u201d\n\n## New Yorker Favorites\n\n  * Why facts don\u2019t change our minds.\n\n  * The tricks rich people use to avoid taxes.\n\n  * The man who spent forty-two years at the Beverly Hills Hotel pool.\n\n  * How did polyamory get so popular?\n\n  * The ghostwriter who regrets working for Donald Trump.\n\n  * Snoozers are, in fact, losers.\n\n  * Fiction by Jamaica Kincaid: \u201cGirl\u201d\n\nSign up for our daily newsletter to receive the best stories from The New\nYorker.\n\nAnna Wiener is a contributing writer to The New Yorker, covering Silicon\nValley, startup culture, and technology. She is the author of \u201cUncanny\nValley.\u201d\n\n### Weekly\n\nEnjoy our flagship newsletter as a digest delivered once a week.\n\nRead More\n\nAftermath\n\nDervish\u2019s Post-St. Patrick\u2019s Day Pilgrimage\n\nAn Irish band crisscrosses the Bronx to pay respects at the grave of Michael\nColeman, a Sligo fiddler who revived the genre.\n\nBy Robert Sullivan\n\nSketchbook\n\nA Millionth-Anniversary Surprise\n\nWhen one has been married forever, one sometimes feels that there is nothing\nnew one will ever discover about one\u2019s person, however . . .\n\nBy Roz Chast\n\nOur Local Correspondents\n\nWaking Up to a New York City Earthquake\n\nAfter the most powerful quake in more than a century, the city was full of\nstories, arm-waving, and whispers of California.\n\nBy Sarah Larson\n\nLondon Postcard\n\nHearing the Voices of Grenfell Tower\n\nThe survivors of the deadly 2017 London fire speak in a theatre piece opening\nat St. Ann\u2019s Warehouse.\n\nBy Rebecca Mead\n\n\u00a9 2024 Cond\u00e9 Nast. All rights reserved. The New Yorker may earn a portion of\nsales from products that are purchased through our site as part of our\nAffiliate Partnerships with retailers. The material on this site may not be\nreproduced, distributed, transmitted, cached or otherwise used, except with\nthe prior written permission of Cond\u00e9 Nast. Ad Choices\n\n## We Care About Your Privacy\n\nWe and our 167 partners store and/or access information on a device, such as\nunique IDs in cookies to process personal data. You may accept or manage your\nchoices by clicking below or at any time in the privacy policy page. These\nchoices will be signaled to our partners and will not affect browsing\ndata.More Information\n\n### We and our partners process data to provide:\n\nUse precise geolocation data. Actively scan device characteristics for\nidentification. Store and/or access information on a device. Personalised\nadvertising and content, advertising and content measurement, audience\nresearch and services development.\n\n", "frontpage": true}
